{"title": "ShareBoost: Efficient Multiclass Learning with Feature Sharing", "tag": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "abstract": "Multiclass prediction is the problem of classifying an object into a relevant target class. We consider the problem of learning a multiclass predictor that uses only few features, and in particular, the number of used features should increase sub-linearly with the number of possible classes. This implies that features should be shared by several classes. We describe and analyze the ShareBoost algorithm for learning a multiclass predictor that uses few shared features. We prove that ShareBoost efficiently finds a predictor that uses few shared features (if such a predictor exists) and that it has a small generalization error. We also describe how to use ShareBoost for learning a non-linear predictor that has a fast evaluation time. In a series of experiments with natural data sets we demonstrate the benefits of ShareBoost and evaluate its success relatively to other state-of-the-art approaches.", "text": "multiclass prediction problem classifying object relevant target class. consider problem learning multiclass predictor uses features particular number used features increase sub-linearly number possible classes. implies features shared several classes. describe analyze shareboost algorithm learning multiclass predictor uses shared features. prove shareboost eﬃciently ﬁnds predictor uses shared features small generalization error. also describe shareboost learning non-linear predictor fast evaluation time. series experiments natural data sets demonstrate beneﬁts shareboost evaluate success relatively state-of-the-art approaches. learning classify object relevant target class surfaces many domains document categorization object recognition computer vision advertisement. multiclass learning problems training examples learn classiﬁer later used accurately classifying objects. typically classiﬁer ﬁrst calculates several features input object classiﬁes start predictors based linear combinations features. later section show framework enables learning highly non-linear predictors embedding non-linearity construction features. requiring classiﬁer depend features therefore equivalent sparseness linear weights features. recent years problem learning sparse vectors linear classiﬁcation regression given signiﬁcant attention. while general ﬁnding accurate sparse predictor known hard main approaches proposed overcoming hardness result. ﬁrst approach uses norm surrogate sparsity compressed sensing literature second approach relies forward greedy selection features machine learning literature orthogonal matching pursuit signal processing community popular model multiclass predictors maintains weight vector classes. case even weight vector associated class sparse overall number used features might grow number classes. since number classes rather large goal learn model overall small number features would like weight vectors share features non-zero weights much possible. organizing weight vectors classes rows single matrix equivalent requiring sparsity columns matrix. loss smooth. reason need loss function convex smooth follows. function convex ﬁrst order approximation point gives lower bound function point. function also smooth ﬁrst order approximation gives lower upper bounds value function point. shareboost uses gradient loss function current solution make greedy choice column update. ensure greedy choice indeed yields signiﬁcant improvement must know ﬁrst order approximation indeed close actual loss function need lower upper bounds quality ﬁrst order approximation. given training average training loss matrix approximately smoothness guarantees )−∇f therefore approximate approximation error upper bounded diﬀerence paper describe analyze eﬃcient algorithm learning multiclass predictor whose corresponding matrix weights small number non-zero columns. formally prove exists accurate matrix number non-zero columns grows sub-linearly number classes algorithm also learn matrix. apply algorithm natural multiclass learning problems demonstrate advantages previously proposed state-of-the-art methods. algorithm generalization forward greedy selection approach sparsity columns. alternative approach recently studied generalizes norm based approach relies mixednorms. discuss advantages greedy approach mixed-norms section matrix maps d-dimensional feature vector k-dimensional score vector actual prediction index maximal element score vector. maximizer unique break ties arbitrarily. non-convex solving optimization problem eqn. np-hard overcome hardness result shareboost algorithm follow forward greedy selection approach. algorithm comes formal generalization sparsity guarantees makes shareboost attractive multiclass learning engine eﬃciency accuracy. centrality multiclass learning problem spurred development various approaches tackling task. perhaps straightforward approach reduction multiclass binary e.g. one-vs-rest pairs constructions. direct approach choose particular multiclass predictors form given eqn. extensively studied showed great success practice example alternative construction abbreviated singlevector model shares single weight vector classes paired class-speciﬁc feature mappings. construction common generalized additive models multiclass versions boosting popularized lately role prediction structured output number classes exponentially large approach yield predictors rather mild dependency required features contrast paper tackle general multiclass prediction problems like object recognition document classiﬁcation straightforward even plausible would construct a-priori good class speciﬁc feature mappings class predictors form given eqn. trained using frobenius norm regularization using regularization entries however pointed regularizers might yield matrix many nonzeros columns hence lead predictor uses many features. viewed convex approximation objective advantageous optimization point view global optimum convex problem remains unclear well convex program approximates original goal. example section show cases mix-norm regularization yield sparse solutions shareboost yield sparse solution. despite fact shareboost tackles nonconvex program thus limited local optimum solutions prove theorem mild conditions shareboost guaranteed accurate sparse solution whenever solution exists generalization error bounded shown theorem note several recent papers established exact recovery guarantees mixed norms seem stronger guarantee given theorem however assumptions much stronger assumptions theorem particular strong noise assumptions group like assumption contrast impose restrictions. would like stress many generic practical cases assumptions hold. example using decision stumps features highly correlated violate assumption another advantage shareboost parameter desired number non-zero columns furthermore obtaining whole-regularizationpath shareboost curve accuracy function sparsity performed single relies heuristics ﬁnding boosting step. contrast shareboost allows columns real numbers thus allowing soft sharing classes. therefore shareboost expressive power comparing jointboost. moreover shareboost automatically identiﬁes relatedness classes without rely exhaustive search. shareboost also fully corrective sense extracts information selected features adding ones. leads higher accuracy using less features shown experiments image classiﬁcation. lastly shareboost comes theoretical guarantees. finally mention feature sharing merely transferring information across classes several alternative ways proposed literature target embedding shared hidden structure shared prototypes sharing underlying metric shareboost forward greedy selection approach solving eqn. usually greedy approach update weight feature time. update column time choose column maximizes norm corresponding column gradient loss since matrix matrix partial derivatives denote r’th column vector shareboost much easier obtaining whole regularization path convex relaxation eqn. last least shareboost work even initial number features large long eﬃcient choose next feature. example features constructed using decision stumps extremely large shareboost still implemented eﬃciently. contrast extremely large mixnorm regularization techniques yield challenging optimization problems. mentioned before shareboost follows forward greedy selection approach tackling hardness solving eqn. greedy approach widely studied context learning sparse predictors linear regression. however multiclass problems needs sparsity groups variables shareboost generalizes fully corrective greedy selection procedure given case selection groups variables analysis follows similar techniques. obtaining group sparsity greedy methods also recently studied indeed shareboost shares similarities works. diﬀer analysis impose strong assumptions shareboost applies much wider array applications. addition speciﬁc criterion choosing next feature diﬀerent. ratio diﬀerence objective diﬀerent costs used. shareboost norm gradient matrix used. multiclass problem loss criterion shareboost much easier compute especially large scale problems. suggested many selection rules geared toward squared loss optimal loss function multiclass problems. another related method jointboost algorithm original presentation seems rather diﬀerent type predictors describe eqn. possible show jointboost fact learns matrix additional constraints. particular features assumed decision stumps column classes subset jointboost chooses shared decision stumps greedy manner applying gentleboost algorithm presentation. major disadvantage jointboost pure form exhaustively search among situations features divided groups runtime calculating single feature group almost runtime calculating features group. cases makes sense choose groups features iteration shareboost. easily done simply choosing group features maximizes analysis implies signiﬁcantly larger shareboost overﬁt. case incorporate regularization objective shareboost order prevent overﬁtting. simple objective function frobenius norm regij regularization parameter. easy verify smooth convex function therefore easily adapt shareboost deal regularized objective. also possible rely norms norm mixed-norm. however technicality fact norms smooth. overcome problem deﬁning smooth approximations norms. main idea ﬁrst note rewrite aforementioned norms using operations. then replace expression soft-max counterpart obtain smooth version overall norm function. example smooth version norm controls tradeoﬀ quality runtime shareboost follows. steps requires step convex optimization problem variables performed using various methods. experiments used nesterov’s accelerated gradient method whose runtime smooth objective desired accuracy. therefore overall runtime interesting compare runtime complexity minimizing mixed-norm regularization objective given eqn. since objective longer smooth runtime using nesterov’s accelerated method would much larger shareboost chooses feature maximizes norm r-th column gradient matrix. analysis shows choice leads suﬃcient decrease objective function. however easily develop ways choosing feature potentially lead even larger decrease objective. example choose feature minimizes matrices support class piece-wise linear functions good candidate approximation function example illustration fig. fact easy verify smooth functions approximated piece-wise linear functions general express piece-wise linear vector-valued functions demonstrate shareboost used learning non-linear predictors. main idea similar approach taken boosting svm. construct non-linear predictor ﬁrst mapping original features higher dimensional space learning linear predictor space corresponds non-linear predictor original feature space. illustrate idea present concrete mappings. ﬁrst decision stumps method widely used boosting algorithms. second approach shows shareboost learning piece-wise linear predictors inspired super-vectors construction recently described object feature-vector contains possible decision stumps. naturally dimensionality large calculating step shareboost take forever. luckily simple trick yields eﬃcient solution. first note stump features corresponding values training size therefore sort values examples training calculate value right-hand side eqn. possible values total time thus shareboost implemented eﬃciently decision stumps. dim) matrix. shareboost used learning furthermore apply variant shareboost described section learn piecewise linear model pieces practice ﬁrst deﬁne large candidate centers applying clustering method training examples second deﬁne possible radiuses taking values quantiles training examples. then train shareboost choose multiclass predictor pairs advantage using shareboost learns non-linear model model linear pieces advantageous terms test runtime well terms generalization performance. section provide formal guarantees shareboost algorithm. proofs deferred appendix. ﬁrst show algorithm managed matrix small number non-zero columns small training error generalization error also small. bound next analyze sparsity guarantees shareboost. mentioned previously exactly solving eqn. known hard. following main theorem gives interesting approximation guarantee. tells exists accurate solution small norm shareboost algorithm good sparse solution. section present illustrative examples showing whenever strong feature sharing possible shareboost competitive methods might fail produce solutions small number features. ﬁrst example present shows exponential number features required shareboost number features required regularization methods. consider examples example form rlog+k {±}log binary representation number alphabet vector zero everywhere except y’th coordinate. example ﬁrst matrix deconsider noted matrix whose equals second matrix denoted matrix whose equals ey]. clearly number features used number features used value objective smaller value fact hard show optimal solution takes form therefore matter regularization parameter solution regularized problem features even though exists rather good solution relies shared features. contrast using corollary know stop shareboost poly) iterations produce matrix uses poly) features small loss. similarly possible show appropriate regularization parameter mixalso fail produce sparse solution shareboost still guaranteed learn sparse solution. integer consider examples composed blocks ﬁrst type block equals bin. second type generate example ﬁrst type zero blocks noted bin]/s. note addition second type expectation respect choice block zero. since loss function strictly convex follows jensen’s inequality thus shown using mix-norm regularization prefer matrix fact possible show minimizer form since number blocks arbitrarily large since shareboost guaranteed learn matrix poly) non-zero columns conclude substantial mixnorm regularization shareboost. advantage shareboost example follows ability break ties naturally aforementioned examples synthetic capture extreme situations. however experiments show shareboost performs better mixed-norm regularization natural data sets well. section demonstrate merits shareboost comparing alternative algorithms diﬀerent scenarios. ﬁrst experiment exempliﬁes feature sharing property shareboost. perform experiments data demonstrate mild growth number features number classes grows second experiment compares shareboost mixed-norm regularization jointboost algorithm follow experimental setup main ﬁnding shareboost outperforms mixed-norm regularization method output predictor needs sparse mixed-norm regularization better regime rather dense predictors. also show shareboost faster accurate jointboost. third ﬁnal experiments mnist handwritten digit dataset demonstrate state-of-the-art accuracy extremely eﬃcient runtime performance. main motivation deriving shareboost algorithm need multiclass predictor uses features particular number features increase slowly number classes. demonstrate property shareboost experimented chark data consists images digits letters. trained shareboost number classes varying classes classes corresponding digits capital letters. calculated many features required achieve certain ﬁxed accuracy function number classes. description feature space described section compared shareboost -vs-rest approach latter trained binary classiﬁer using mechanism used shareboost. namely minimize binary logistic loss using greedy algorithm. methods constructing sparse predictors using greedy approach. diﬀerence methods shareboost selects features shared manner -vs-rest approach selects features binary problem separately. fig. plot overall number features required methods achieve ﬁxed accuracy test function number classes. easily seen increase number required features mild shareboost signiﬁcant -vs-rest approach. calculated whole regularization path mixed-norm regularization running algorithm many values regularization parameter fig. plot results three datasets statlog pendigits isolet. number classes datasets respectively. original dimensionality datasets high therefore following expanded features taking products ordered pairs features. transformation number features respectively. fig. displays results. seen shareboost decreases error much faster mixednorm regularization therefore preferable goal rather sparse solution. features allowed shareboost starts overﬁt. surprising since sparsity mean controlling complexity learned classiﬁer. prevent overﬁtting eﬀect variant shareboost incorporates regularization— section compare shareboost jointboost algorithm section description jointboost. previous experiment followed experimental setup jointboost using published code additional implementation heuristic pruning space class-subsets described paper. fig. displays results. used stump features algorithms since needed jointboost. seen shareboost decreases error much faster therefore preferable goal rather sparse solution. previous experiment observe features allowed shareboost starts overﬁt. again surprising prevented adding additional regularization. training runtime shareboost also much shorter jointboost goal experiment show shareboost achieves state-of-the-art performance constructing fast predictors. experimented mnist digit dataset consists training digits represented centered size-normalized fig. examples). mnist dataset extensively studied considered standard test multiclass classiﬁcation handwritten digits. error rate achieved advanced algorithms test sense challenge involved mnist dataset consider straightforward -nearest-neighbor approach test example represented vector entries matched entire training using distance x−xj. classiﬁcation decision majority class label three nearest training examples. naive approach achieves error rate run-time unwieldy proportions. going produce better error rate. advanced shape-similarity measures could improve performance naive approach heavier run-time cost. example shape context similarity measure introduced uses bipartite matching algorithm descriptors computed along points image. using shape-context similarity achieves error rate high run-time cost. challenge mnist dataset therefore design multiclass algorithm small error rate eﬃcient run-time performance. mnist performer uses feed-forward neural-net million connections roughly translates million multiplyaccumulate operations run-time well. training geometrically distorted versions original examples generated order expand training following introduced warping scheme purpose. performance error rate stands run-time cost million test example. table summarizes discussion including performance shareboost. error-rate shareboost rounds stands using original training expanded training examples generated adding deformed instances original example figure shareboost compared mixed-norm regularization jointboost several datasets. horizontal axis feature sparsity vertical axis test error rate. rounds. run-time test examples around leading mnist performer. error rate better reported used -vs-all -degree polynomial kernel expanded training examples. number support vectors giving rise run-time -fold compared shareboost. describe details shareboost implementation mnist dataset. maximum going image locations. prediction argmaxy∈y. fig. shows ﬁrst templates chosen shareboost corresponding spatial masks. example ﬁrst templates matches digit part along image eleventh template matches horizontal stroke near image forth. fig. shows weights ﬁrst templates model produced best results. example eleventh template encodes horizontal line close expected digit digit fig. shows misclassiﬁed samples rounds shareboost fig. displays convergence curve error-rate function number rounds. figure convergence shareboost mnist dataset reaches errors. expanded deformed versions input using method since deformations fairly strong training error higher test. zoomed version shown right. improvements shareboost mnist dataset possible extending training using deformations increasing pool features type descriptors pursued here. point desired make shareboost achieve competitive performance mnist performers accuracy run-time little eﬀort feature space design exhibiting great eﬃciency training time well. might wonder stellar performance shareboost maybe patch-based features designed. section remove doubt using shareboost training piece-wise linear predictor described section mnist using generic features. show shareboost comes close error rate gaussian kernels requiring anchor points well number support-vectors needed kernel-svm. underscores point shareboost extremely fast predictor without sacriﬁcing stateof-the-art performance level. position mask. corresponding columns entries column represents sharing among classes pattern. example eleventh template encodes horizontal line close expected digits digit figure test accuracy shareboost mnist dataset function number rounds using generic piece-wise linear construction. blue train accuracy. test accuracy. dashed gaussian kernel accuracy. anchor points radiuses together corresponding linear classiﬁers. context worthwhile compare classiﬁcation performance gaussian kernels applied -vs-all framework. kernel-svm also selects subset training corresponding weight coeﬃcients thus mechanistic point view piece-wise linear predictor shares principles kernelsvm. pca. pool anchor points taken reduced training means clustering clusters range radius values anchor point taken discrete values. taken together round shareboost selected anchor point radius search space size fig. shows error-rate shareboost rounds. seen shareboost comes close error rate requiring anchor points well number supportvectors needed kernel-svm. underscores point shareboost extremely fast predictor without sacriﬁcing state-of-the-art performance level.", "year": 2011}