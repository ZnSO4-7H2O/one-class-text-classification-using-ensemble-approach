{"title": "Teaching Categories to Human Learners with Visual Explanations", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We study the problem of computer-assisted teaching with explanations. Conventional approaches for machine teaching typically only provide feedback at the instance level e.g., the category or label of the instance. However, it is intuitive that clear explanations from a knowledgeable teacher can significantly improve a student's ability to learn a new concept. To address these existing limitations, we propose a teaching framework that provides interpretable explanations as feedback and models how the learner incorporates this additional information. In the case of images, we show that we can automatically generate explanations that highlight the parts of the image that are responsible for the class label. Experiments on human learners illustrate that, on average, participants achieve better test set performance on challenging categorization tasks when taught with our interpretable approach compared to existing methods.", "text": "study problem computer-assisted teaching explanations. conventional approaches machine teaching typically provide feedback instance level e.g. category label instance. however intuitive clear explanations knowledgeable teacher signiﬁcantly improve student’s ability learn concept. address existing limitations propose teaching framework provides interpretable explanations feedback models learner incorporates additional information. case images show automatically generate explanations highlight parts image responsible class label. experiments human learners illustrate that average participants achieve better test performance challenging categorization tasks taught interpretable approach compared existing methods. computer-assisted teaching offers promise personalized curricula tailored ability level interests every individual. providing open access kinds high-quality teaching currently available small percentage world’s population potential transform education. date subject areas mathematics language learning beneﬁted automated teaching systems. however problem best teach visual expertise ﬁne-grained domains medical diagnosis species identiﬁcation comparatively less well explored. addition beneﬁts human learners better teaching methods automated systems could also take advantage improvements. access expert time often limited result need better techniques train crowd workers image annotation tasks. workers effectively learned task provide higher quality labeled training data. ‘closing loop’ enable take advantage humans’ ability generalize across different domains cope nuisance factors pose lighting changes. figure majority existing machine teaching algorithms visual categories give feedback learner form ground truth class label. much informative display discriminative regions help determine categories present images. here explanations system highlighting blank band viceroy butterﬂy white spots queen butterﬂy’s wings. ﬁeld markings commonly used identify species. existing approaches teaching visual knowledge typically pose problem choosing informative subset images show much larger possible options. major limitations existing work give limited feedback students form class label e.g. fig. example label feedback. providing ground truth class label limited amount feedback compared rich explanations receive human teacher. human teacher would likely teach student speciﬁc parts attributes image discriminative particular class fig. hypothesis students taught interpretable feedback learn effectively receive label feedback. speciﬁcally propose novel teaching algorithm selects images representative categories interest explanations easily interpreted. explanations come form feedback teaching indicating parts image important successful classiﬁcation. show possible generate explanations existing labeled datasets thus minimizing need additional annotations. experiments real human learners show joint selection informative images interpretable explanations results better student learning improved generalization test time. goal machine teaching design algorithms systems teach humans efﬁciently automatically. date variety different approaches explored modeling teaching students assuming perfect learners heuristic-based approaches bayesian models recurrent neural networks reinforcement learning based methods machine teaching successfully deployed online tutoring systems feature highly structured knowledge e.g. mathematics teaching challenging visual concepts human learners less explored. teach binary visual classiﬁcation tasks modeling student stochastically switching different hypotheses learning. model attempts select teaching examples ofﬂine best discount incorrect hypotheses guide student towards ground truth classiﬁcation function. propose interactive approach choice future images show based individual’s past responses. however must update model’s parameters online user making computationally difﬁcult scale large numbers simultaneous users real-world settings. major limitation existing approaches feedback provide student fully informative. student shown sequence images asked estimate object category ﬁnite list believe present image. respond simply told correct answer informed parts image discriminative identifying particular object thus making learning problem artiﬁcially hard student. overcome limitation propose novel teaching algorithm selects images provides interpretable explanations resulting understandable efﬁcient learning student. complementary work recently introduced explanation based teaching algorithm binary tasks. explanations provided pre-existing semantically meaningful features interpretability given explanation modeled. using clear understandable instructional material dramatically improve student’s ability learn concept. shown highlighting informative regions image help improve novice classiﬁcation performance guiding student’s attention another example human teacher unavailable common novices learn species identiﬁcation consulting expertly curated ﬁeld guides. ﬁeld guides typically books apps contain descriptive text example images highlighting important features classifying different species e.g. attempts made automate creation guides using highlighted part annotations automatic generation image speciﬁc text descriptions gamiﬁcation however addition image level class labels majority approaches require collection expert annotations form text descriptions anatomical part locations visual attributes expensive time consuming obtain large image collections furthermore efﬁcacy annotations teaching visual identiﬁcation skills evaluated real human subjects. alternative approach requires less additional annotations learn human interpretable models data e.g. context computer vision evidence suggest deep models commonly used large-scale image classiﬁcation tasks adapted generate features semantically meaningful humans recently outlined approach incorporating additional supervised data users call ‘feature feedback’. addition class level labels typically provided human annotators training supervised classiﬁers allow annotators provide information value speciﬁc feature dimensions. contrast approach instead gives explanations learner importance different image regions models incorporate information updating belief. concerned selecting teaching examples associated interpretable explanations best teach noisy human learners. practice explanations generated additional time-consuming human annotation show possible extract meaningful explanations using existing image level labels. section outline model teaching categorization problems human learners present efﬁcient algorithm selecting interpretable teaching examples. discuss approach context non-adaptive model visual teaching algorithm beneﬁt interpretable explanations. represents consistently learner responds according currently adopted hypothesis. extreme hypotheses inconsistent ground truth label immediately discarded. unrealistic setting represents perfect learner interpretable feedback feedback presented learner strict model comprised true ground truth label teaching example. learner tasked learning mapping image ground truth label explanations provided i.e. must determine regions parts image responsible given class label. real-world teaching scenarios teacher often access much richer information utilized accelerate teaching process. approach explain gives feedback learner form explanations models incorporate information updating belief. slight abuse notation extend denote labeled teaching images along explanations. updated model learner introduces additional discount terms account interpretability explanation representativeness image here diff represents difﬁcult given explanation image large values indicate challenging explanations. intuitively learner conﬁdent discounting inconsistent hypotheses presented easier-tounderstand explanations. information could crowdsourced later describe method teach human learner target classiﬁcation function maps images corresponding ground truth class labels instance single image could picture bird associated label could name species. however cannot directly impart parameters human learner instead must teach showing example images. given images associated labels classes goal select informative subset referred teaching best convey ground truth i.e. human teacher’s classiﬁcation function acknowledging images informative others learning want waste effort teaching unrepresentative images. learning human teacher even textbook student receives feedback form correct answer also explanation describes given answer correct. addition images labels assume access explanation image case images explanations could heatmaps highlight informative regions particular class given image. non-visual scenarios could piece text describing relationship image learner model adopt stochastic strict algorithm model learners adapt images shown teacher. model originally proposed teaching binary classiﬁcation functions. learners modeled carrying random walk ﬁnite hypothesis space element function maps image score context binary classiﬁcation label predicted hypothesis image sgn) magnitude indicates conﬁdence prediction. concretely image represented feature vector hypothesis could linear classiﬁer weights ensure possible teach learner assume also contains ground truth hypothesis beginning teaching learner randomly picks hypothesis according prior distribution teaching presented sequence images along correct class label. receiving image learner stick current hypothesis ground truth label consistent prediction hypothesis. otherwise randomly switches according current posterior hypotheses teaching images ground truth labels seen far. updating posterior light information hypotheses disagree true labels images shown teacher figure strict prone selecting outliers. here numbers represent order examples selected teaching. favoring instances representative others select teaching examples dense regions feature space. here lines represent different hypotheses teachers hypothesis. thickness lines represent probability associated hypothesis showing teaching examples learner. system like could assume multiclass hypothesis made combination linear classiﬁers number classes. results |hl|/ possible combinations becomes prohibitively large exhaustively cover even small numbers hypotheses classes. instead model separate posterior classes. allows reuse hypotheses across different classes. naturally enables model situation learner accurately group images based visual similarity unable associate correct class label groups learn correct mapping. teaching binary concepts corresponds single posterior modeling existing strict model. given model learner describe select teaching choice based desire direct learner towards distribution possible hypotheses results smallest number mistakes possible. multiclass setting classes one-versus-all scheme class maintain different posterior distributions hypotheses intuitively order drive learner’s error probability predicting multiclass labels sufﬁcient make sure learner performs well binary classiﬁcation tasks. class deﬁne error one-vs-all hypothesis possible images yc)| note quality explanation measured image difﬁculty. image difﬁculty strict implicitly encoded image’s location feature space. images close decision boundaries assumed ambiguous learner compared decision boundaries. however images equal difﬁculty different explanations goal bias selection teaching images towards easier understand. teaching would like select representative teaching images concept conveyed learner easily generalized remaining unseen images. practice however observe linear hypothesis space used strict tendency select outlier images selecting teaching strict attempts greedily optimize expected reduction error hypothesis result selecting images optimal reducing error necessarily informative learner. overcome limitation inspired approaches active learning include additional discount factor favors representative examples practice compute mean distance example examples class. teacher still aims select informative examples allows ensure also representative. setting results existing strict model. many real word teaching problems feature multiple different categories interest. presented strict limited binary class setting. approach handling multiple classes change hypothesis space individual hypothesis multiclass classiﬁer e.g. softmax classiﬁer individual hypotheses one-versus-all classiﬁers. however generating multiclass hypothesis space challenging. instance access linear classiﬁers e.g. crowdsourced here denotes feature value pixel location output channel weight values biases ﬁnal fully connected layer associated ground truth class used weight feature map. finally normalize explanations spans range number pixels explanation. captures preference localized discrete highlighted regions whose values either practice weight regularization applied training ensures explanations uniformly high classiﬁcation training objective ensures non-zero entries feature maps correctly predict class label. bias towards classes entropy average subtract mean difﬁculty class resnet trained scratch dataset backbone extract features output second residual block resulting downsampling factor this append additional convolutional layer ﬁlters size ﬁnal fully connected layer number classes output. ﬁnetune entire model image crops batch size epochs using adam training random ﬂips crops color augmentations. start initial learning rate decay factor epochs. generate ﬁnal explanation input image extract explanation center crop resize input resolution. hypotheses space generation require access representative hypothesis space perform teaching. space span different possible linear classiﬁers learners using. represents different biases relation teaching task hand. however generating space multiple ﬁne-grained categories trivial. alternative generating embeddings crowd annotations e.g. propose features ﬁnetuned previous section. extracting multiclass teaching problem combined proxy error probability i.e. expected error learner. given teaching budget would like teaching upon observing images labels associated explanations learner would achieve maximal reduction expected error. formally instead directly optimizing challenging combinatorial problem greedy submodular approach outlined start empty teaching greedily single image time. selection next teaching image show amounts choosing example unseen image require explanation tells learner image given label best generate explanations still open question time-consuming acquire explanations would expert manually label informative regions image. alternatively could crowdsourced result noisy explanations e.g. instead propose ground truth class label provided image automatically generate visual explanations. exploit fact modern convolutional neural networks used image classiﬁcation often produce semantically meaningful features. class activation mapping approach automatically generate explanations image existing methods generating explanations also easily used dimensional penultimate feature vector modiﬁed resnet gives representation encodes visual similarity. unlikely representation used learners advantage generated additional annotation cost. generate candidate linear classiﬁers using features ﬁrst clustering features class subsets training linear separate rest data. also train linear separate class classes representing best possible hypothesis next group pair classes train additional linear models separate remaining classes. linear classiﬁers hypothesis additional random linear classiﬁer bring total number hypotheses dataset split training test sets. images training possible correctly classify optimal hypothesis removed. efﬁcient optimization optimizing involves searching unseen images measuring reduction error hypothesis across classes. pre-computation reduce updates simple matrix operations. simplicity present terms binary strict model formulation holds multiclass explain algorithm including additional discount factors classes. element-wise multiplication. vector size contains error associated hypothesis computed ofﬂine. vector size represents current posterior hypotheses update time step matrix computed ofﬂine. entry encodes conﬁdence places example column vector associated selected teaching example multiclass setting simply maintain separate class choose next teaching example results biggest reduction across classes. existing datasets explanations form annotated visual attributes could used teach visual categories human learners. however found many cases e.g. provided attributes coarse-grained useful teaching highly similar categories. alternatives noisy informative e.g. present interesting avenue future human-in-the-loop explanation generation. instead selected three diverse datasets span range different teaching cases. example images explanations dataset seen fig. butterﬂies ﬁrst dataset represents teaching naturalists wish identify different species plants animals wild. contains images different species butterﬂies captured large variety real-world situations varying image quality inaturalist dataset species monarch viceroy similar appearance third queen looks underneath. ﬁnal species admiral cabbage white distinct appearance. extracted bounding around butterﬂy manually discarded images caterpillars. total dataset contains images close uniformly distributed across species. eyes second dataset mimics teaching trainee ophthalmologist tasked learning identify different retinal diseases images. consists image slices retina obtained optical coherence tomography non-invasive imaging technique uses light waves take cross-sectional images retina. commonly used ophthalmology order million scans performed around world annually here learner’s goal classify images three classes normal contains macular edema contains subretinal fluid. diabetic macular edema leading causes vision loss people diabetes many examples challenging require subtle inspection correctly identify condition. dataset contains images ground truth annotations provided retina specialists. chinese characters finally dataset three different chinese characters extracted also used here exploring scenario nonnative chinese speaker attempting learn identify characters. images different handwritten example following three characters grass mound stem. images vary difﬁculty large variety handwriting quality style different individuals contributed dataset. conducted experiments real human participants crowdsourcing platform mechanical turk using methodology crowd workers i.e. learners randomly assigned dataset baseline teaching strategy. average received participants strategy dataset combination. assumed learners motivated perform well acknowledge might always case. first learners shown brief tutorial illustrating web-based teaching interface worked. teaching began presented sequence images shown time selected assigned teaching algorithm. viewing image turn asked select list options indicating class believed image contain. depending assigned teaching strategy given feedback either form correct class label correct class label corresponding visual explanation. visual explanations displayed input images alternating input image explanation every seconds. upon receiving feedback learners wait minimum seconds could proceed next teaching image. dataset showed teaching images followed randomly selected testing images feedback provided testing. random sequence test images different learner. longer teaching sequences would likely lead better test performance trade learner’s attention span accuracy. shufﬂed order response buttons depicting class labels learner teaching began ensure bias towards particular button position. table average test time accuracies mechanical turk learners across three different test datasets. compared standard image baseline rand_im inclusion explanations rand_exp results better test time performance across datasets. explain model results superior learners three datasets generated hypothesis spaces. compared full explain model three baseline algorithms rand_im random selection images rand_exp random selection visual explanations multiclass version strict note last baseline include density weighting explanations. explain strict generated ﬁxed teaching sequence ofﬂine used learners hypothesis space both. noise level learners explanation representativeness parameters table report average test time accuracy learner different baseline teaching strategies datasets. fig. displays histograms accuracies illustrating strategies include explanations tend better overall. butterﬂies dataset greater percentage learners high scores taught explain fig. challenging dataset observe many learners remain confused three similar species teaching perhaps necessitating longer teaching sequences. eyes images likely unfamiliar learners compared datasets. however images well aligned contain large plane view point changes confusing background texture. enables generate high-quality explanations localize characteristic morphologies different retinal diseases. rand_im baseline fig. difference learner performance explained images selected intrinsic motivation. fig. eyes confusion figure test time classiﬁcation performance human learners. show learners binned test time accuracy. horizontal axes represent average test scores larger numbers indicate higher accuracy. vertical axes number learners. test time learner confusion matrices eyes dataset. explanation based strategies result smaller off-diagonal entries. average test time confusion matrices across learners dataset. clear explain results less cross-category confusion i.e. smaller values off-diagonal entries. learners tend make fewer mistakes identifying subretinal fluid relatively distinct confusion macular edema normal. chinese characters dataset represents interesting failure case explain using generated hypothesis space fig. fig. average performance teaching learners ﬁrst teaching images selected explain. explain selects particularly difﬁcult image fourth teaching example. happens class previous image visually different. inspecting test time confusion matrix early difﬁcult example potentially biases learners lower performance grass class. random based strategies advantage generating different teaching sequences learner uniformly sampling input space thus introducing additional robustness. table rand_exp performs well dataset indicating performance explain result automatically generated hypothesis space explanation interpretability scores rather quality explanations themselves. test hypothesis assigned manual interpretability scores generated separate embedding space closely aligned human notions similarity soliciting pairwise similarity estimates mechanical turk construct embedding space teaching embedding space results best test time performance overall last table introduced explain algorithm teaching multiple visual categories human learners interpretable feedback. interpretable feedback applied existfigure average accuracy teaching chinese characters dataset using generated hypothesis space random guessing strategies observe general improvement learners’ ability time. ﬁrst teaching images selected explain. fourth image difﬁcult instance resulting vast majority learners guessing wrong category. visual teaching algorithm. provides cues learner teaching form visual explanations explicitly models incorporate information updating belief. enables generate informative teaching sequences resulting improved test time performance i.e. better generalization learners unseen images. experiments featuring real human participants show approach superior existing methods provide weaker class label feedback. future plan investigate extending approach interactive teaching setting model updates online based learner’s responses e.g. acknowledgments would like thank google gift visipedia project research credits bloomberg northrop grumman swiss early mobility postdoctoral fellowship. thanks also kareem moussa providing dataset helping generate crowd embeddings.", "year": 2018}