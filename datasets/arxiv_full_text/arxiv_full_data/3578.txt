{"title": "Data Augmentation by Pairing Samples for Images Classification", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Data augmentation is a widely used technique in many machine learning tasks, such as image classification, to virtually enlarge the training dataset size and avoid overfitting. Traditional data augmentation techniques for image classification tasks create new samples from the original training data by, for example, flipping, distorting, adding a small amount of noise to, or cropping a patch from an original image. In this paper, we introduce a simple but surprisingly effective data augmentation technique for image classification tasks. With our technique, named SamplePairing, we synthesize a new sample from one image by overlaying another image randomly chosen from the training data (i.e., taking an average of two images for each pixel). By using two images randomly selected from the training set, we can generate $N^2$ new samples from $N$ training samples. This simple data augmentation technique significantly improved classification accuracy for all the tested datasets; for example, the top-1 error rate was reduced from 33.5% to 29.0% for the ILSVRC 2012 dataset with GoogLeNet and from 8.22% to 6.93% in the CIFAR-10 dataset. We also show that our SamplePairing technique largely improved accuracy when the number of samples in the training set was very small. Therefore, our technique is more valuable for tasks with a limited amount of training data, such as medical imaging tasks.", "text": "data augmentation widely used technique many machine learning tasks image classiﬁcation virtually enlarge training dataset size avoid overﬁtting. traditional data augmentation techniques image classiﬁcation tasks create samples original training data example ﬂipping distorting adding small amount noise cropping patch original image. paper introduce simple surprisingly effective data augmentation technique image classiﬁcation tasks. technique named samplepairing synthesize sample image overlaying another image randomly chosen training data using images randomly selected training generate samples training samples. simple data augmentation technique signiﬁcantly improved classiﬁcation accuracy tested datasets; example top- error rate reduced ilsvrc dataset googlenet cifar- dataset. also show samplepairing technique largely improved accuracy number samples training small. therefore technique valuable tasks limited amount training data medical imaging tasks. machine learning tasks image classiﬁcation machine translation text-to-speech synthesis amount samples available training critically important achieve high performance better generalization. data augmentation applying small mutation original training data synthetically creating samples widely used virtually increase amount training data szegedy fadaee wide variety approaches synthesizing samples original training data; example traditional data augmentation techniques include ﬂipping distorting input image adding small amount noise cropping patch random position. data augmentation mostly norm winning image classiﬁcation contests. paper introduce simple surprisingly effective data augmentation technique image classiﬁcation tasks. technique named samplepairing create sample image overlaying another image randomly picked training data using images randomly selected training generate samples training samples. even overlay another image label ﬁrst image correct label overlaid sample. simple data augmentation technique gives signiﬁcant improvements classiﬁcation accuracy tested datasets ilsvrc cifar- cifar- svhn. example top- error rate reduced ilsvrc dataset googlenet cifar- dataset simple -layer convolutional network. also conducted data augmentation overlaying image picked outside training set; approach also gives improvements technique picks image overlay training gives signiﬁcant gain. show samplepairing technique often gives larger beneﬁts number samples training data smaller conducted evaluations reducing number samples used training cifar-. results showed technique yields larger improvements accuracy number samples smaller full cifar- dataset. used samples label classiﬁcation error rate reduced technique. based results believe technique valuable tasks limited number training datasets available medical image classiﬁcation tasks. model trained small training trained model tends overly samples training results poor generalization. data augmentation widely used avoid overﬁtting problem enlarging size training set; thus allows larger network without signiﬁcant overﬁtting. example krizhevsky employ couple data augmentation techniques alexnet paper using random numbers randomly cropping patches pixels original image pixels horizontally ﬂipping extracted patches changing intensity channels. many submissions imagenet large scale visual recognition challenge employed ﬁrst techniques give increase dataset size. samplepairing technique another enlarge dataset size. since creates pairs images gives samples original samples. also technique orthogonal data augmentation techniques; detailed later implementation employs basic data augmentation techniques used alexnet epoch. case synthesize samples total. many ways avoid overﬁtting data augmentation. dropout variants famous techniques used avoid overﬁtting randomly disabling connections training. batch normalization intends solve problem internal covariate shift also known effective prevent overﬁtting. samplepairing data augmentation orthogonal techniques employ techniques achieve better generalization performance. technique randomly picks image overlay training creates pairs synthesize sample. technique ﬁrst create sample randomly picked original samples; existing work chawla wang perez also create sample samples. smote synthetic minority over-sampling technique chawla well-known technique imbalanced dataset; i.e. dataset consists normal samples small number abnormal samples. smote over-sampling minority class synthesizing sample randomly picked samples feature space. comparing technique smote basic idea pairing samples synthesize sample common techniques; however apply pairing entire dataset instead minority class. wang perez conducted data augmentation pairing images using augmentation network separate convolutional network create sample. compared work simply average intensity channel pixels images create sample need create augmentation network. also technique yields signiﬁcant improvements compared reported results. paper technique called mixup zhang submitted conference although developed totally independently mixup also mixes randomly selected samples create training data paper. differences samplepairing mixup include mixup overlaying sample labels well label sample weighted average overlay samples tested tasks image classiﬁcation including speech recognition generative adversarial network label used training husz´ar pointed using label sample without overlaying labels examples resulted learning result. simpler blending labels also suitable semi-supervised setting. also tested overlaying labels samples samplepairing differences results larger random number generator section describes samplepairing data augmentation technique. basic idea technique simple synthesize image images randomly picked training done existing techniques chawla wang perez synthesize image take naive approach averaging intensity images pixel. figure depicts overview sample pairing technique. training epoch samples network training randomized order. technique randomly picked another image training took average images mixed image associated label ﬁrst image network training. hence essentially samplepairing randomly creates pairs samples training synthesizes sample them. label second image used training. since images equally weighted mixed image classiﬁer cannot correctly predict label ﬁrst image mixed image unless label label label. training loss cannot become zero even using huge network training accuracy cannot surpass average; n-class classiﬁcation maximum training accuracy classes number samples training set. even though training accuracy high samplepairing training accuracy validation accuracy quickly improve stop samplepairing ﬁnal ﬁne-tuning phase training. ﬁne-tuning network trained samplepairing achieve much higher accuracy network trained without technique empirically show paper. network trained samplepairing shows higher training error rate training loss network trained without samplepairing even tuning since samplepairing strong regularizer. types data augmentation employed addition technique apply image mixing ﬁnal image training. data augmentation incurs additional time prepare input image done executing training back propagation. therefore additional execution time visibly increase total training time image. phase intermittently disable samplepairing. ilsvrc enable samplepairing images disable next images. datasets enable samplepairing epochs disable next epochs. evaluated various training processes; example intermittently disabling samplepairing granularity batch without intermittently disabling although samplepairing yielded improvements even training processes empirically observed process gave highest accuracy trained model. section investigate effects samplepairing data augmentation using various image classiﬁcation tasks ilsvrc cifar- cifar- street view house numbers datasets. ilsvrc dataset used googlenet network architecture trained network using stochastic gradient descent momentum optimization method batch size samples. datasets used network convolutional layers batch normalization followed fully connected layers dropout used network architecture except number neurons output layer. trained network using adam optimizer batch size training used data augmentations extracting patch random position input image using random horizontal ﬂipping baseline regardless whether samplepairing shown figure paper ensemble predictions results. validation extracted patch center position classiﬁer without horizontal ﬂipping. implemented algorithm using chainer framework. show improvements accuracy samplepairing data augmentation table ilsvrc dataset well full dataset classes tested shrinked dataset ﬁrst classes. datasets evaluated samplepairing reduced classiﬁcation error rates validation sets training sets samplepairing increased training errors datasets avoiding overﬁtting. comparing ilsvrc classes classes case classes much lower training error rate potentially suffers overﬁtting severely. therefore beneﬁts samplepairing data augmentation signiﬁcant case classes case classes results show samplepairing yields better generalization performance achieves higher accuracy. figure figure illustrate validation error rates training error rates cifar ilsvrc datasets. figure much better ﬁnal accuracy trade longer training time. disabled samplepairing data augmentation intermittently training error rates validation error rates ﬂuctuated signiﬁcantly. network training samplepairing enabled validation error rate training error rate quite poor. particular training error rate expected; theoretical best error rate -class classiﬁcation task even tuning networks trained samplepairing show higher training error rate cifar- higher ilsvrc. validation however disabled samplepairing validation error rates became much lower baseline employ samplepairing tuning already shown table also show changes training valiadation losses ilsvrc dataset figure losses match training validation error rates. without samplepairing achieved figure shows samplepairing data augmentation causes improvements number samples available training limited; change number training samples cifar. cafar- dataset provides samples classes training set. gradually reduced number images used training samples samples class. shown figure largest gain samplepairing used samples class; error rate reduced used samples class second largest reduction achieved. error rate using samples samplepairing actually better error rate using samples without samplepairing. didn’t images training also evaluated effect overlaying image randomly chosen image training set. example used images training made pool images selected unused images randomly picked image overlay pool. results shown figure pairing non-training sample. method also reduced error rates samplepairing picks image training yielded signiﬁcant improvements regardless training size. samplepairing require images training easier implement also gives larger gains compared naively overlaying another image. paper disabled samplepairing intermittently. figure shows ratio withsamplepairing enabled affects ﬁnal classiﬁcation error cifar-. conﬁguration used paper slightly better ﬁnal results compared case without disabling samplepairing. however differences signiﬁcant enabled samplepairing half epochs; hence tuning rate disable samplepairing sensitive. paper presents data augmentation technique named samplepairing. samplepairing quite easy implement; simply randomly picked images classiﬁer training. surprisingly simple technique gives signiﬁcant improvements classiﬁcation accuracy avoiding overﬁtting especially number samples available training limited. therefore technique valuable tasks limited number samples medical image classiﬁcation tasks. paper discussed empirical evidences without theoretical explanations proof. future work like provide theoretical background samplepairing helps generalization much maximize beneﬁts data augmentation. another important future work applying samplepairing machine learning tasks especially generative adversarial networks nitesh chawla kevin bowyer lawrence hall philip kegelmeyer. smote synthetic minority over-sampling technique. journal artiﬁcial intelligence research goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial networks. arxiv. geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutdinov. improving neural networks preventing co-adaptation feature detectors. arxiv. alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. annual conference neural information processing systems christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. conference computer vision pattern recognition matthew zeiler sixin zhang yann lecun fergus. regularization neural networks using dropconnect. international conference machine learning iii– –iii– samplepairing take label sample discard label another figure shows validation error cifar- labels synthesized sample i.e. assigned softmax target samples. figure difference label labels signiﬁcant. result consistent husz´ar pointed mixup", "year": 2018}