{"title": "Probabilistic Active Learning of Functions in Structural Causal Models", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We consider the problem of learning the functions computing children from parents in a Structural Causal Model once the underlying causal graph has been identified. This is in some sense the second step after causal discovery. Taking a probabilistic approach to estimating these functions, we derive a natural myopic active learning scheme that identifies the intervention which is optimally informative about all of the unknown functions jointly, given previously observed data. We test the derived algorithms on simple examples, to demonstrate that they produce a structured exploration policy that significantly improves on unstructured base-lines.", "text": "consider problem learning functions computing children parents structural causal model underlying causal graph identiﬁed. sense second step causal discovery. taking probabilistic approach estimating functions derive natural myopic active learning scheme identiﬁes intervention optimally informative unknown functions jointly given previously observed data. test derived algorithms simple examples demonstrate produce structured exploration policy signiﬁcantly improves unstructured base-lines. large parts literature causality concerned learning causal graph system random variables also known causal discovery causal inference problem motivated realistic problems science biologist wish discover genes responsible regulating genes cell; public health researcher wish know whether certain habits population inﬂuence certain health outcomes starting point paper consider done causal graph system variables already identiﬁed known variables functions variables precise functional relationships still unknown. thus although understand causal relationships coarse sense able accurately predict result intervention system possibly implications decision making. instance suppose cell gene up-regulates gene gene down-regulates gene know reducing expression gene lead decrease expression gene which turn lead increase expression gene however without precise knowledge relationships genes unable quantitatively predict effect applying drug reduces expression gene similarly goal reduce overall levels lung cancer population knowing smoking causes cancer insufﬁcient know best public health policy would better could persuade smokers stop smoking completely persuade every smoker reduce consumption passive agent supplied data generated system learning functional relationships parents children reduces simply separate regression problems—one unknown function—once causal graph known. many knowledge acquisition problems however phrased sequential decision making process data received next point time affected decision made based data already observed. biologist blindly perform series costly time-consuming experiments looking generated data last over; data experiment would analysed next performed thus informing experiment would best perform next. figure even simple setting three variables whose graph chain non-trivial trade-off information expects gain performing different interventions below formalise problem using language structural causal models also known structural equation models essence consists functions connecting child variables causal parents equipped notion intervention variable externally forced take particular value interventions idealised mathematical representation performing experiment. take bayesian approach estimating functional relationships parents children naturally gives rise active learning algorithm decide next ‘experiment’ perform. approach works causal graphs arbitrary dags non-triviality problem apparent even simple case three variables whose causal graph chain goal situation sense made precise later learn functions point time must decide whether perform possible interventions passively observe system different action cost. make passive observation learn something though areas distributions probability mass. small sample setting unlikely learn anything functions areas probability inputs. intervene choose value decide precisely want learn also learn region although would uncertain where. intervene learn precise aspect learn nothing decide action pick given already know problem considered paper approach solving close connection ideas bayesian optimization bayesian optimization goal extremum function expensive evaluate possibly exploiting known structure speed search. expense information obtained evaluation used efﬁciently. contrast however setting simply interested ﬁnding extremum unknown function rather learn entire function sense. tong koller considered similar setting treated discrete variables. below begin formal deﬁnition structural causal models section states precise problem tackling. section provides bayesian formulation inference setting complemented active learning scheme guide exploration section provides empirical evaluations synthetic examples. structural causal models formally deﬁne structural causal models learning study remainder. notational simplicity deﬁnition deviates wider literature including interventions modelled scm. deﬁnition suppose variables taking value respectively. write n=...n vector variables n=...n domain similarly structural causal model tuple consisting following three quantities structural equations vector variables m∈pa. call causal parents interventions. intervention mathematical operation replaces subset equations equations setting variables speciﬁc constants write intervention intervenes subset variables xvar setting values xval xval) instead). write resulting equations. consider acyclic scms. case given ﬁxed value variables unique value structural equation satisﬁed; thus induces distribution unique solutions. refer observational distribution write intervention resulting intervened structural equations also acyclic. follows reasoning therefore parameters model implies distribution ﬁxed implies distributions indexed intervention {pdo assume empty intervention element important note graphical structure ﬁxed parameters model functions distribution variables make non-trivial assumption structural equations non-linear additive gaussian noise meaning equation form fn)+en zero-mean gaussian random variable. setting comes results identiﬁability causal graph example consider following represented figure diagonal. assume graphical structure known functions not. simplicity assume known. given data drawn observational variety interventional distributions element tuple independent draw interested separate tasks. ˆfn) ‘close’ true model sense. part problem deﬁne sensible notion closeness scms. problem active learning. select intervention cost observe single draw incorporating datum close possible? rest section devoted deﬁning risk functional provide notion closeness approximate function practitioner visually interpret relationship parents children predict result intervention cannot practically carried potentially this assumption could relaxed bayesian approach involving prior covariance matrix would reduce running algorithm derived following averaging results posterior. m∈pa supplied probability measure specifying importance learning pointwise value input xpa. puts large amounts mass areas learn precisely small amounts mass areas learn approximately zero mass areas care learning all. function risk functional below. weighted according importance function gives rise total risk n=...n n=...n vectors functions respectively. assume simplicity also known mean integrated squared error case coincides typical objective would minimised classical non-parametric statistical learning setting worth considering possible risk functionals could used since presence measures arguably somewhat arbitrary consider. learning parameters statistical model commonly used objective many separate justiﬁcations minimise divergence true data distribution implied learned model ˆpx. scms imply single distribution variables rather family distributions intervention {pdo therefore wish consider separate loss interventional distribution example uniformly bound losses subset interventions interest. alternatively could replace divergence different divergence measure metric distributions. instance could maximum mean discrepancy corresponding characteristic kernel probabilistic approach learning taking bayesian approach learning vector unknown functions problem reduced series independent regression problems input output domains common choice prior learning functions gaussian process function assume zero mean prior kernel domain recall given dataset consisting elements collection marginal observations drawn distribution intervened upon. since assumption distribution known element represents evaluation input point corrupted gaussian noise known variance. performing regression using speciﬁc assumptions made choice freely chosen incorporate prior knowledge functions parentless unknown constant assume -dimensional gaussian prior zero mean variance µfn|dn kfn|dn explicitly written terms data procedure applied independently giving posterior distribution vector functions chosen given posterior problem demands single choice variable expectation random variable calculated expressed terms posterior covariance mean functions lemma choosing posterior mean minimises expected total risk. uncertain distribution directly yields estimate total risk optimal µf|d chosen which prior ﬁxed purely function data denote active learning section myopic active learning algorithm derived based belief functions expected total risk described above. step time select intervention cost observe single draw distribution goal select intervention reduce expected total risk much possible taking account cost problem non-trivial main reasons. consider ﬁrst issue above. expected total risk change intervention chosen observation expected single observation drawn total risk r}). deﬁne value intervention expected reduction performing intervention divided cost goal intervention largest value since unknown possible calculate right-hand term numerator possible however estimate replacing expectation belief distribution based uncertain estimates next parts section different methods proposed estimate expected total risk performing intervention. ﬁrst uses sampling requires brute-force search interventions. appropriate possible interventions small enough feasible. second uses form dynamic programming enabling search larger interventions efﬁciently. derived algorithm however makes speciﬁc assumptions graph interventions. since drawing factors amounts drawing gaussian distribution possible efﬁciently sample entire joint distribution. replacing equation above arrive estimate expected total risk estimated using interventions consideration exhibits structure coincides causal graph appropriately possible estimate value many interventions simultaneously. speciﬁc example provided done case causal graph chain intervention intervenes variable everything upstream similar example chains interventions intervene exactly variable provided appendix. crux approach fact posterior covariance function gaussian process function inputs conditioning data outputs. writing kn|dn contribution expected total risk estimating function writing |dn|} follows actually function consider intervention sets variables upstream arbitrary values. observation made provides information functions since intervenes curr current contributions expected total risk. deﬁne following shorthand contribution expected total risk function made observation input point made similar discretely approximating continuous domains possible reduce evaluating right hand side series matrix multiplications additions series operations vectorised allow calculation many simultaneously. moreover many initial calculations cached used speed calculation different values algorithm experiments figure shows results running proposed methods synthetic example scms graphs given figures respectively. case structural equations consisting sines cosines parent variables ﬁxed giving sets structural equations noise variables scms ﬁxed variance interventions chain chosen interventions form satisﬁes conditions description dynamic programming algorithm. interventions non-chain deﬁned interventions single variables total risk function experiment chosen setting uniform distribution domain kernels used radial basis function kernels bandwidth parameter costs assumed equal. experiment learning initiated data. learning proposed algorithms compared strategies drawing observational distribution selecting intervention uniformly random. since dynamic programming algorithm could used learning could test sampling algorithm. uniform discretisations intervention sets made total size each. used sets interventions search using sampling algorithm. following quantities used evaluate performance algorithms point time true total risk incurred choosing vector posterior means; maximum calculated intervention median divergences discretised intervention sets; maximum median mmds mmdl calculated intervention figure actively choosing informative interventions speeds learning. proposed methods outperform observing randomly intervening metric evaluation. bottom show results experiments learning respectively. experimental details described section experiment performed many times; faded lines represent results single trials bold lines represent averages single trials. many ways work presented could incrementally furthered possible efﬁcient ways search interventions subject less restrictive assumptions made algorithm different distributions noise variables could considered case approximation need made regressing posterior distribution could relax additive noise assumption altogether. natural extensions include estimating value intervention based reasoning multiple steps future considering implications constrained budget. although proposed algorithms seem perform well synthetic examples considered remains seen whether method suitably extended would similarly perform well convincing real-world problem. hard suitable real-world problems convincing ground truth exists believe sensible assay methods synthetic data performance accurately measured. perhaps fundamental issue raised tackled fact clear best even deﬁne means learn parameters thereof. proposed suitable interventions potentially supi∈i principled objectives minimise total risk functional considered here. interestingly derived algorithms reduce quantities experiments considered though explicit objective. future directions research include trying understand whether these objectives give rise tractable methods parameter estimation causal models selecting interventions active settings assumptions mathematical guarantees made. jongh druzdzel. comparison structural distance measures causal bayesian network models. recent advances intelligent information systems challenging problems science computer science series pages mooij peters janzing zscheischler schölkopf. distinguishing cause effect using observational data methods benchmarks. journal machine learning research http//jmlr.org/papers/volume/-/-.pdf. peters mooij janzing schölkopf. identiﬁability causal graphs using functional models. cozman pfeffer editors conference uncertainty artiﬁcial intelligence pages corvallis auai press. sriperumbudur gretton fukumizu lanckriet schölkopf. injective hilbert space embeddings probability measures. servedio zhang editors proceedings annual conference learning theory pages madison omnipress. tong koller. active learning parameter estimation bayesian networks. leen dietterich tresp editors advances neural information processing systems pages similar reasoning used derive dynamic programming scheme calculate estimated total risk proposed intervention many simultaneously. summarised algorithm pre-compute vectors discrete approximations conditional probability distributions calculate expected loss interventions variable turn return vectors giving estimated expected risks interventions interventions", "year": 2017}