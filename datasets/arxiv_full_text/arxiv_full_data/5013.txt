{"title": "Deep Graph Attention Model", "tag": ["cs.LG", "cs.AI"], "abstract": "Graph classification is a problem with practical applications in many different domains. Most of the existing methods take the entire graph into account when calculating graph features. In a graphlet-based approach, for instance, the entire graph is processed to get the total count of different graphlets or sub-graphs. In the real-world, however, graphs can be both large and noisy with discriminative patterns confined to certain regions in the graph only. In this work, we study the problem of attentional processing for graph classification. The use of attention allows us to focus on small but informative parts of the graph, avoiding noise in the rest of the graph. We present a novel RNN model, called the Graph Attention Model (GAM), that processes only a portion of the graph by adaptively selecting a sequence of \"interesting\" nodes. The model is equipped with an external memory component which allows it to integrate information gathered from different parts of the graph. We demonstrate the effectiveness of the model through various experiments.", "text": "graph representation) follows iterative process recomputes node’s attribute vector concatenating hashing attributes neighboring nodes ﬁnal graph representation computed using attributes nodes graph. another popular technique random walk graph kernel computes number common walks pair graphs measure graph similarity done product graph graphs entire graphs considered. entire graph processed usually costly infeasible compute representations large real-world graphs signiﬁcant computational cost incurred processing entire graph also negative impact overall performance model graph classiﬁcation. particularly true signiﬁcant subgraph patterns given task sparse conﬁned small neighborhoods within graph. since rest graph contain anything help identify graph label processing entire graph inadvertently cause noise introduced. example illustrated figure address issues mentioned above study model uses attention actively select region graph process. using attention focus informative parts graph able improve model’s performance keeping computation cost low. particularly true graphs signal-tonoise ratio signiﬁcant since attention allows ignore noisy parts graph. instance studying interaction networks complex diseases researchers found often beneﬁcial focus speciﬁc subnetworks associated disease inspired recent success recurrent neural networks attention vision-related tasks explore model built-in attention mechanism graph-structured data. many different techniques proposed solve graph classiﬁcation problem. popular approach graph kernel measure similarity different graphs similarity measured considering vargraph classiﬁcation problem practical applications many different domains. existing methods take entire graph account calculating graph features. graphlet-based approach instance entire graph processed total count different graphlets subgraphs. real-world however graphs large noisy discriminative patterns conﬁned certain regions graph only. work study problem attentional processing graph classiﬁcation. attention allows focus small informative parts graph avoiding noise rest graph. present novel model called graph attention model processes portion graph adaptively selecting sequence interesting nodes. model equipped external memory component allows integrate information gathered different parts graph. demonstrate effectiveness model various experiments. graph-structured data arise naturally wide variety applications including bioinformatics chemoinformatics social network analysis urban computing cyber-security many cases primary task identifying class labels graphs dataset. chemoinformatics instance molecules represented graphs nodes correspond atoms edge signiﬁes presence chemical bond pair atoms. task predict label graph instance anti-cancer activity toxicity molecule. solve problem usual strategy calculate certain graph statistics help discriminating different types graphs. expect graphs belonging particular class exhibit common behavior typically observed among graphs. currently existing solutions entire graph taken account calculating statistics. morgan algorithm calculating circular ﬁngerprints association advancement artiﬁcial intelligence rights reserved. figure processing entire graph count various subgraph patterns often limited counting relatively simple patterns since total number patterns grow exponentially size patterns given large noisy graph relatively complex patterns approach fails. attention used allow focus informative parts graph only helping uncover complex useful patterns. allows compute graph representations better discriminate positive negative samples. ious structural properties like shortest paths nodes occurrence certain graphlets subgraphs even structure graph different scales recently several methods generalize previous approaches introduced. methods deep learning framework learn datadriven representations thing common among approaches entire graph processed compute ﬁnal representation. contrast model study processes portion graph attention used determine parts graph focus deep learning frameworks equipped attentional processing shown perform well variety tasks. attention used allow model attend subset source words language translation task. meanwhile used attention help model gaze salient objects image captioning applied attention image classiﬁcation task. hand used attention guide focus relevant objects visual question answering task. although attentional processing applied successfully many problems existing work computer vision natural language processing domains. recently model introduced explores attentional processing medical ontology graphs however work signiﬁcantly different latter model speciﬁcally designed medical ontologies work directed acyclic graphs explore attention mechanism general attributed graphs. best knowledge ﬁrst work explores attention general graph-structured data. finally also experiment architecture simple external memory allow multiple agents integrate information various parts graph. sense conceptually similar memory networks although proposed framework general adopted variety tasks choose frame discussion context graph classiﬁcation attributed graphs. formally given attributed graphs )··· goal learn function input space graphs graph labels. graph comprised adjacency matrix nni×ni attribute matrix rni×d number nodes graph number attributes. graph also corresponding label work formulate problem applying attention graph-structured data decision process goal-directed agent traversing along input attributed graph. agent starts random node graph time step moves neighboring node. information available agent limited node chooses explore. since global information graph unavailable agent needs integrate information time help determine parts graph explore further. ultimate goal agent collect enough information allow make correct prediction label graph. agent explore small portion graph attention mechanism guiding exploration. graph large also initialize multiple agents different nodes graph parallel. deploying multiple agents help improve performance model since agent explore different part graph attention helping steer agent’s exploration along local neighborhood. allows figure step network given labeled graph current node stochastic rank vector step module takes step current node neighbors prioritizing whose type higher rank rt−. attribute vector extracted similarly mapped using another linear layer mapped hidden space using linear layer parameterized parameterized produce step embedding vector represents information captured current step took. architecture core component model; particular long short-term memory variant time step core network takes step embedding internal representation model’s history previous step input produces history vector history vector thought representation summary information we’ve aggregated exploration graph thus far. rank network uses decide types nodes interesting thus prioritized future exploration. likewise classiﬁcation network uses make prediction graph label. proposed model core shown figure time step core network processes information step taken integrates internal representation together information retained previous steps. uses information predict label input graph decide areas graph prioritize exploration next time step. step module time step step module considers one-hop neighborhood current node picks neighbor take step towards. step module biased towards picking neighbors whose types labels higher rankings rank vector rt−. attribute vector chosen node extracted together produce step representation step representation information available core lstm network time step. step algorithm summarized algorithm node type label assign types nodes allows bias exploration towards certain nodes different stages exploration. depending application node type simple discrete value something elaborate like category derived log-binning several attributes capture local structure node. give simple example latter case. suppose agent wants visit carbon nodes adjacent cannot differentiate nodes ﬁrst node typing strategy. second method node type calculated based statistics encoded k-hop neighborhood node allows differentiate carbon nodes. using complex node typing strategies however increase number node types substantially look reinforcement learning strategies work well discrete action space large history core lstm network maintains history vector summary information obtained agent exploration graph thus far. time step information becomes available form step took history vector updated allows core network setting dct+ reward given model classiﬁed graph correctly otherwise. hence formulation considered partially observable markov decision process setting obtain partial information graph environment interactions time step. goal learn policy parameters maps sequence past interactions environment ˆl··· ˆlt− distribution actions current time step words given history past interactions summarized history vector classiﬁcation network rank network policy networks learn generate actions maximize reward. training together core lstm network step network rank network work conjunction form policy agent. learn parameters networks maximize total reward agent expect obtain. since speciﬁc policy agent induces distribution possible interaction sequences want train policy maximize reward generated distribution non-trivial task maximize exactly dealing large possibly inﬁnite number possible interaction sequences. however since frame problem pomdp able obtain sample approximation gradient using technique introduced shown given si’s interaction sequences running agent current policy episodes discount factor allows attribute signiﬁcance actions performed closer time prediction made function maps node type. intuition behind equation also known reinforce rule follows. agent current policy obtain samples interaction sequences. parameters adjusted increase log-probability rank type nodes frequently selected episodes resulted correct prediction. training policy allows increase chance agent choose take step towards particular type node next time ﬁnds similar state. compute ∇θlog simply compute gradient network time step done using standard backpropagation note adjust logprobabilities since rank vector last step longer used. procedure algorithm procedure pick neighbor move algorithm biased towards picking neighbors whose types higher ranks rt−. here represents element-wise multiplication denotes start comment. lstm architecture superior simple rnns capturing long-range dependencies. even though lstms sophisticated memory model compared simple rnns shown still trouble remembering information inputted past this large graphs better deploy multiple agents agent exploring relatively small neighborhood rather agent traverse graph long period. integrate information augment architecture shared external memory additionally network conditioned current history vector trained allow model selectively save information memory. allow model store information useful graph classiﬁcation actions given history vector captures agent seen agent performs actions time step. first predicts label input graph softmax outˆ classiﬁcation network conditioned second uses rank network generate rank vector help steer exploration next step ranking importance different types nodes. primarily rank vector’s encode importance different types nodes. however augment include additional actions deciding stop exploration agent conﬁdent enough information classify graph correctly. another possible action allows model transfer current internal information memory component. reward typical reinforcement learning setting agent receives information environment reward signal taking action time step goal agent maximize reward receives usually quite sparse delayed instead. provides estimate equal expectation original formulation possibly lower variance captures cumulative reward expect advantage choosing action allows increase logprobability actions resulted much larger expected cumulative reward decrease log-probability actions resulted reverse. train parameter reducing mean squared error finally cross entropy loss train classiﬁcation network maximizing true label input graph hybrid loss formulation rank network trained time step using reinforce classiﬁcation network baseline network trained using classical approach supervised learning. space complexity node degree graph dimension node attribute vector. since agent moves current node’s neighbors time step need store matrix containing attributes neighboring nodes given time. taking step node attribute matrix neighbors fetched disk. ignoring space needed store parameters model constant negligible model space complexity ogd) quite small practice. initialization instance initialize start vertex selecting random node input graph rank vector initialized uniform distribution. attention memory predicting label input graph choose average softmax output several runs initializing multiple agents different starting locations graph. case view agent classiﬁer ensemble predict voting. averaging predictions several agents certainly improve classiﬁcation performance model still disadvantage methods integrate information entire graph. agent makes prediction independently using information gathered local area within graph. figure rank values time generated rank vector rank network given encoding information initial step onto node higher rank value signiﬁes importance. stores information local memory component combined form shared memory classiﬁcation network uses make single prediction. simplest case means ﬁnal history vector agent’s local memory. however parts agent’s walk graph yield equally important information. allow model retain information useful task softmaxed output decides useful particular piece memory words weighted pooling obtain local memory. viewed another form attention. finally integrate information multiple agents simply modiﬁcation allows integrate information various regions graph especially helpful graph large take small number steps note agent’s exploration still guided attention mechanism proposed earlier. various modiﬁcations made architecture. instance choose condition output rank network local memory even shared external memory. additional actions also introduced allow model modify rewrite shared memory. work however choose test simplest version demonstrate efﬁcacy. space limitations show diagram model memory instead include supplemental material. table summary experimental results average accuracy ave. rank column shows average rank method. lower average rank better overall performance method. motivating example consider details main experimental setup introduce simple motivating example shows attention used guide agent towards relevant regions graph. example generated small dataset random graphs. embedded several patterns subgraphs generated graphs -paths former pattern embedded primarily onto positive samples latter included negative samples. figure show output rank network time given history vector capturing initial step onto node type interesting note that initially rank network assigns less equal importance types nodes. however time learns prioritize nodes types guarantees agent prioritize exploration right direction giving model enough information classify graphs correctly small number steps. experimental setup data evaluated proposed method binary classiﬁcation task using molecular graph datasets nci- nci- nci- nci-. since molecular structures datasets encoded using smiles format used rdkit package convert string corresponding graph. used package extract following information node node attributes atom element node degree total number attached hydrogens implicit valence atom aromaticity. atom element used label assign types nodes. graph class labels indicate anti-cancer property molecule. datasets highly imbalanced negative samples positive ones. like previous work balanced datasets. experiments conducted balanced subsets containing randomly selected graphs. compared methods order demonstrate effectiveness proposed approach compare several baseline methods utilize entire graph feature extraction. best knowledge ﬁrst work attention graphs compare baselines observe entire graph puts model disadvantage since partial observability. compared methods summarized below. agg-attr given attributed graph simple construct feature vector component-wise average attribute vectors nodes graph. agg-wl ﬁrst approach captures information node attributes. however completely ignores graph’s structural information. second method uses weisfeiler-lehman algorithm calculate node attributes capture local neighborhood node. algorithm works iteratively assigning attribute node computing hash attributes neighboring nodes. simply average attributes running algorithm feature vector used prediction. kernel-sp compare shortest path kernel measures similarity pair graphs comparing distance shortest paths nodes graphs. since attributed graphs label nodes graph concatenating categorical attributes. kernel-gr also compare graphlet kernel measures graph similarity counting number different graphlets. here evaluate -graphlet kernel nodes labeled above. proposed approach uses attention gam-mem proposed approach external memory. used logistic regression classiﬁer ﬁrst baselines. reduce overﬁtting applied regularization used grid search select ideal regularization penalty. furthermore also grid search number iterations algorithm tested fair comparison limited classiﬁcation network methods single softmax layer make equivalent also limited number hidden layers networks model single layer whenever possible. table performance baselines restrict setting given randomly selected partial snapshots graph predict voting. column full indicates performance entire graph seen partial shows performance parts graph seen. diff. difference performance means performance deteriorated partial information available shows increase performance. graph-kernel based approaches used classiﬁer using precomputed kernel generated approach. here grid search used vector agg-wl limited size lstm history vector size well. particular tried size also tried following sizes ﬁrst second hidden layers respectively step network since noticeable change performance increasing following parameters ﬁxed values. number steps number samples also number agents graph prediction. gam-mem grid search adam algorithm optimization initial ﬁnal learning rates respectively. also discounted reward noticeable gain setting finally limit training methods epochs applied early stopping using validation set. classiﬁcation results table shows average classiﬁcation accuracy fold cross-validation compared methods. results proposed model always among top- terms performance tested datasets. particular attention model memory performs best four datasets comes second ﬁfth dataset every single case gammem outperforms shows adding external memory integrate information various locations beneﬁcial. however still performs respectably compared baselines fact comes second tested datasets. also outperforms agg-attr agg-wl almost every single case remarkable since agent access portion graph latter access entire graph. experiments ﬁrst baselines perform worst almost always performing worst datasets. kernel-based approaches better graphlet-based approach being superior. able outperform slightly. however gam-mem consistently best performer datasets tested. applying random attention experiments show attention model competitive baselines observe entire graph model limited seeing portion graph. demonstrate effectiveness attention further another experiment restrict ﬁrst baselines setting gam. straightforward modiﬁcation since methods also graph attribute vectors. however baselines concept attention random attention sample subgraphs graph using randomwalk length limits information available baselines available since ﬁxed table shows result baselines observe random portion graph. clear performance deteriorates methods agg-wl showing marked difference performance. exception agg-attr nci-. fact performance agg-wl drops drastically performs almost better random guessing four datasets shows attention help examine parts graph relevant. parameter study study effect varying step sizes performance gam-mem. datasets ﬁxed parameters ones yielded best results varied cases accuracy increased increased number steps giving fairly good performance datasets methods. surprisingly found models already performed relatively well cases worse best accuracy. molecular graphs fairly small size. found gam-mem general beneﬁts increase size fact using weighted pooling history vectors model support longer walks. interesting directions future work. intend study model using expressive node typing strategies. would also like experiment extension model sophisticated external memory finally would interesting test ﬂexible architectures lstm like tree-lstms seem natural graphs.", "year": 2017}