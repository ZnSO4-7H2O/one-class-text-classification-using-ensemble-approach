{"title": "A Regularized Framework for Sparse and Structured Neural Attention", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "Modern neural networks are often augmented with an attention mechanism, which tells the network where to focus within the input. We propose in this paper a new framework for sparse and structured attention, building upon a smoothed max operator. We show that the gradient of this operator defines a mapping from real values to probabilities, suitable as an attention mechanism. Our framework includes softmax and a slight generalization of the recently-proposed sparsemax as special cases. However, we also show how our framework can incorporate modern structured penalties, resulting in more interpretable attention mechanisms, that focus on entire segments or groups of an input. We derive efficient algorithms to compute the forward and backward passes of our attention mechanisms, enabling their use in a neural network trained with backpropagation. To showcase their potential as a drop-in replacement for existing ones, we evaluate our attention mechanisms on three large-scale tasks: textual entailment, machine translation, and sentence summarization. Our attention mechanisms improve interpretability without sacrificing performance; notably, on textual entailment and summarization, we outperform the standard attention mechanisms based on softmax and sparsemax.", "text": "modern neural networks often augmented attention mechanism tells network focus within input. propose paper framework sparse structured attention building upon smoothed operator. show gradient operator deﬁnes mapping real values probabilities suitable attention mechanism. framework includes softmax slight generalization recently-proposed sparsemax special cases. however also show framework incorporate modern structured penalties resulting interpretable attention mechanisms focus entire segments groups input. derive efﬁcient algorithms compute forward backward passes attention mechanisms enabling neural network trained backpropagation. showcase potential drop-in replacement existing ones evaluate attention mechanisms three large-scale tasks textual entailment machine translation sentence summarization. attention mechanisms improve interpretability withsacriﬁcing performance; notably textual entailment summarization outperform standard attention mechanisms based softmax sparsemax. modern neural network architectures commonly augmented attention mechanism tells network look within input order make next prediction. attentionaugmented architectures successfully applied machine translation speech recognition image caption generation textual entailment sentence summarization name examples. heart attention mechanisms mapping function converts real values probabilities encoding relative importance elements input. case sequence-to-sequence prediction time step generating output sequence attention probabilities produced conditioned current state decoder network. used aggregate input representation single vector relevant current time step. vector ﬁnally decoder network produce next element output sequence. process repeated end-of-sequence symbol generated. importantly architectures trained end-to-end using backpropagation. alongside empirical successes neural attention—while necessarily correlated human attention—is increasingly crucial bringing interpretability neural networks helping explain individual input elements contribute model’s decisions. however commonly used attention mechanism softmax yields dense attention weights elements input always make least small contribution decision. overcome limitation sparsemax recently proposed using euclidean projection onto simplex sparse alternative figure attention weights produced proposed fusedmax compared softmax sparsemax sentence summarization. input sentence summarized along x-axis. bottom shows attention distributed producing word summary. rows grey background corresponds exactly adjacent positions exactly equal weight separated borders. fusedmax pays attention contiguous segments text equal weight; segments never occur softmax sparsemax. addition enhancing interpretability show fusedmax outperforms softmax sparsemax task terms rouge scores. softmax. compared softmax sparsemax outputs interpretable attention weights illustrated task textual entailment. principle parsimony states simple explanations preferred complex ones however limited sparsity remains open whether attention mechanisms designed beneﬁt structural prior knowledge. contributions. success sparsemax motivates explore attention mechanisms output sparse weights take advantage structural properties input modern sparsity-inducing penalties. make following contributions propose general framework builds upon operator regularized strongly convex function. show operator differentiable gradient deﬁnes mapping real values probabilities suitable attention mechanism. framework includes special cases softmax slight generalization sparsemax. show incorporate fused lasso framework derive attention mechanism named fusedmax encourages network attention contiguous segments text making decision. idea illustrated figure sentence summarization. cases contiguity assumption strict show incorporate oscar penalty derive attention mechanism named oscarmax encourages network equal attention possibly non-contiguous groups words. order attention mechanisms deﬁned framework autodiff toolkit problems must addressed evaluating attention computing jacobian. however attention mechanisms require solving convex optimization problem generally enjoy simple analytical expression unlike softmax. computing jacobian solution optimization problem called argmin/argmax differentiation currently area active research references therein). algorithmic contributions show compute jacobian general framework well fused lasso oscar. showcase potential attention mechanisms drop-in replacement existing ones show empirically attention mechanisms enhance interpretability achieving comparable better accuracy three diverse challenging tasks textual entailment machine translation sentence summarization. notation. denote denote -dimensional probability simplex euclidean projection onto miny∈∆d given function convex conjugate deﬁned supy∈dom ytx−f given norm dual deﬁned supy≤ ytx. denote subdifferential function elements subdifferential called subgradients differentiable contains single element gradient denoted denote jacobian function rd×d hessian function rd×d. figure proposed maxω operator constant proposed mapping illustrated case maxω relu-like function sigmoid-like function. framework recovers softmax sparsemax special cases. also introduce three attention mechanisms sq-pnorm-max fusedmax oscarmax except softmax never exactly reaches mappings shown right encourage sparse outputs. motivate proposal ﬁrst show section subgradients maximum operator deﬁne mapping mapping highly unsuitable attention mechanism. maximum operator function deﬁned equality r.h.s comes fact supremum linear form simplex always achieved vertices i.e. standard basis vectors {ei}d moreover hard check solution supremum precisely subgradient maxi∈ xi}. subgradients mapping puts probability mass onto single element max. however behavior undesirable resulting mapping discontinuous function amenable optimization gradient descent. r.h.s. holds combining maxω max∗ ∂maxω ∂maxω {∇maxω} since unique solution. therefore gradient mapping. illustrate maxω various choices figure appendix importance strong convexity. β-strong convexity assumption plays crucial role underestimated. recall function β-strongly convex w.r.t. norm -smooth w.r.t. dual norm conjugate -smooth words sufﬁcient ensure maxω differentiable everywhere gradient training backpropagation. order neural network trained backpropagation problems must addressed regularizer ﬁrst forward computation evaluate i.e. solve optimization problem second backward computation evaluate jacobian equivalently hessian maxω. contributions presented show solve problems general differentiable well structured regularizers fused lasso oscar. negative entropy. conjugate negative entropy restricted simplex moreover γg∗. therefore closed-form expression maxω exi/γ. since negative entropy -strongly convex w.r.t. -smooth w.r.t. obtain classical softmax ex/γ evaluated element-wise. note authors also call maxω soft max. although really soft follow popular terminology. checked maxω reduces softplus sigmoid. sparsemax. choose operator theory since w.r.t. addition easy verify mapping introduced named sparsemax fact sparse alternative softmax. derivation thus gives slight generalization controls sparsity could tuned; experiments however follow literature euclidean projection onto simplex computed exactly following jacobian indicates nonzero elements since lipschitz continuous rademacher’s theorem implies differentiable almost everywhere. points differentiable take arbitrary matrix clarke’s generalized jacobians convex hull jacobians form xt→x tackling structured regularizers address section case general differentiable regularizer involves maximizing concave function simplex computed globally using off-the-shelf projected gradient solver. therefore main challenge compute jacobian address next proposition. proposition jacobian differentiable assume differentiable maxy∈∆d computed. jacobian denoted obtained solving system deﬁned shorthands jp∆d γ∇ω) γhω). proof given appendix unlike recent work tackling argmin differentiation matrix differential calculus karush–kuhn–tucker conditions proof technique relies differentiating ﬁxed point iteration compute arbitrary required backpropagation directly solve show appendix system solved efﬁciently thanks structure squared p-norms. useful example differentiable function simplex consider choice squared p-norms known squared p-norm strongly convex w.r.t. implies maxω smooth w.r.t. call induced mapping function sq-pnorm-max exponentiation performed element-wise. sq-pnorm-max recovers sparsemax like sparsemax encourages sparse outputs. however seen zoomed figure transition smoother throughout experiments fusedmax. cases input sequential order meaningful case many natural languages propose fusedmax attention mechanism based fused lasso also known total variation fusedmax encourages paying attention contiguous segments equal weights within one. expressed framework choosing |yi+ i.e. strongly convex term penalty. easy verify choice yields mapping oscarmax. situations contiguity assumption strict propose oscarmax based oscar penalty encourage attention weights merge clusters value regardless position sequence. accomplished replacing penalty fusedmax ∞-norm penalty pair attention weights i.e. forward computation. constraint computing fusedmax/oscarmax seem trivial ﬁrst sight. next proposition shows without iterative method. proposition computing fusedmax oscarmax here posc indicate proximal operators oscar computed exactly respectively. remind reader denotes euclidean projection onto simplex computed exactly using proposition shows compute fusedmax oscarmax using composition functions exact noniterative algorithms exist. surprising result since proximal operator functions general composition proximal operators function. proof follows showing indicator function satisﬁes conditions groups induced posc. optimal solution posc. formally denote group adjacent elements value minimal maximal indices indices elements absolute j|}. value fusedmax/oscarmax either shift group’s common value elements zero. controls trade-off fusion elements fused single trivial group. tuning improve performance observe sensible defaults work well across tasks report results using them. backward computation. already know jacobian sparsemax then proposition know compute jacobians posc obtain jacobians fusedmax oscarmax straightforward application chain rule. however although posc computed exactly lack analytical expressions. next show nonetheless compute jacobians efﬁciently without needing solve system. proposition jacobians posc proof given appendix clearly structure jacobians permits efﬁcient jacobian-vector products; discuss computational complexity implementation details appendix note posc differentiable everywhere except points groups change. points remark sparsemax applies clarke’s jacobian. showcase performance attention mechanisms three challenging natural language tasks textual entailment machine translation sentence summarization. rely available well-established neural architectures demonstrate simple drop-in replacement softmax structured sparse attention; quite likely newer task-speciﬁc models could lead improvement. textual entailment task deciding given text hypothesis whether human reading likely infer true stanford natural language inference dataset collection english sentence pairs. pair consists sentence hypothesis manually labeled labels entailment contradiction neutral. variant neural attention–based classiﬁer proposed dataset follow methodology terms implementation hyperparameters grid search. employ implementation provided simply replace sparsemax fusedmax/oscarmax; observe training time epoch essentially four attention mechanisms table shows that task fusedmax reaches highest accuracy oscarmax slightly outperforms softmax. furthermore fusedmax results interpretable feature groupings figure shows weights neural network’s attention text considering hypothesis dancing. case four models correctly predicted text band playing stage concert attendants dancing music denoted along x-axis contradicts hypothesis although attention weights differ. notably fusedmax identiﬁes meaningful segment band playing. sequence-to-sequence neural machine translation recently become strong contender machine translation attention weights seen alignment source translated words. demonstrate potential attention mechanisms experiments language pairs. build opennmt-py based pytorch default hyperparameters simply replacing softmax proposed opennmt-py softmax attention optimized gpu. since sparsemax fusedmax oscarmax rely sorting operations implement computations simplicity keeping rest pipeline gpu. however observe that even context switching number tokens processed second within softmax pipeline. sq-pnorm-max observe projected gradient solver used forward pass unlike linear system solver used backward pass could become computational bottleneck. mitigate effect tolerance solver’s stopping criterion quantitatively compared attention mechanisms always within bleu score point best mechanism suggests structured sparsity restrict accuracy. however illustrated figure fusedmax oscarmax often lead interpretable attention alignments well qualitatively different translations. figure attention weights french english translation using conventions figure within weights grouped oscarmax cluster denoted here oscarmax ﬁnds slightly natural english translation. visulizations given appendix attention mechanisms recently explored sentence summarization generate sentence-summary pairs cost authors proposed title news article noisy summary article’s leading sentence. collected million pairs gigaword dataset showed seemingly simplistic approach leads models generalize surprisingly well. follow experimental setup able reproduce comparable results opennmt using softmax attention. models evaluation follows standard dataset randomly held-out subset gigaword released report results rouge- rouge- rouge-l. results table indicate fusedmax best nearly metrics always outperforming softmax. addition figure exemplify enhanced interpretability provide detailed results appendix smoothed operators. replacing operator differentiable approximation based exploited numerous works. regularizing operator squared -norm less frequent used obtain smoothed multiclass hinge loss smoothed linear programming relaxations maximum a-posteriori inference work differs mainly aspects. first less interested operator gradient mapping second since mapping neural networks trained backpropagation study compute mapping’s jacobian contrast previous works. interpretability structure sparsity neural networks. providing interpretations alongside predictions important accountability error analysis exploratory analysis among reasons. toward goal several recent works relying visualizing hidden layer activations potential interpretability provided attention mechanisms noted multiple works work aims fulﬁll potential providing uniﬁed framework upon interpretable attention mechanisms designed using well-studied tools ﬁeld structured sparse regularization. selecting contiguous text segments model interpretations explored explanation generator network proposed justifying predictions using fused lasso penalty. however network attention mechanism parameters learned. furthemore sidesteps need backpropagate fused lasso unlike work using stochastic training approach. constrast attention mechanisms deterministic drop-in replacements existing ones. consequence mechanisms coupled recent research builds softmax attention example order incorporate soft prior knowledge alignment attention penalties attention weights different approach incorporating structure attention uses posterior marginal probabilities conditional random ﬁeld attention weights approach takes account structural correlations marginal probabilities generally dense different other. proposed mechanisms produce sparse clustered attention weights visible beneﬁt interpretability. idea deriving constrained alternatives softmax independently explored differentiable easy-ﬁrst decoding finally sparsity-inducing penalties used obtain convex relaxations neural networks compress models works differ ours sparsity enforced network parameters approach produce sparse structured outputs neural attention layers. proposed paper uniﬁed regularized framework upon attention mechanisms designed. enable mechanisms used neural network trained backpropagation demonstrated carry forward backward computations general differentiable regularizers. developed structured attention mechanisms fusedmax oscarmax demonstrated enhance interpretability achieving comparable better accuracy three diverse challenging tasks textual entailment machine translation summarization. usefulness differentiable mapping real values simplex sparse structured outputs goes beyond attention mechanisms. expect framework useful sample categorical distributions using gumbel trick well conditional computation differentiable neural computers plan explore future work. grateful andré martins takuma otsuka fabian pedregosa antoine rolet suzuki justine zhang helpful discussions. thank anonymous reviewers valuable feedback. graves wayne reynolds harley danihelka grabska-barwi´nska colmenarejo grefenstette ramalho agapiou hybrid computing using neural network dynamic external memory. nature koehn hoang birch callison-burch federico bertoldi cowan shen moran zens dyer bojar constantin herbst. moses open source toolkit statistical machine translation. proc. proof outline. posc. optimality conditions respectively posc order express explicit function then obtaining jacobians posc follows application chain rule expressions. discuss proof points posc differentiable; nondifferentiable points take clarke’s generalized gradients jacobian ptv. lemma optimum deﬁned then words unknowns already uniquely determined. emphasize known introduce leaving unknown rearranging optimality conditions obtain recursion λtj+ applying calculation segment yields desired result. proof proposition follows applying chain rule noting groups constant within neighborhood therefore computing forward backward pass sparsemax compositional building block fusedmax oscarmax well general case; reason discuss others. forward pass. problem exactly euclidean projection simplex computed exactly worst-case required sort expected time using pivot algorithm similar median ﬁnding implementation based sorting. backward pass. result jacobian-vector product sparsity pattern denote number nonzero elements thus jacobian-vector product itself computed o)). implement fusedmax composition fused lasso proximal operator sparsemax. forward pass. need solve proximal operator fused lasso. algorithm worst case strong performance realistic benchmarks close backward pass. structure jacobian locality fused groups jacobian-vector products computed using simple algorithm iterates output vector simultaneously averaging elements whose indices fused elements since consecutive elements fused amounts resetting group soon encounter index implement oscarmax composition oscar proximal operator sparsemax. forward pass. proximal operator oscar penalty computed particular case ordered weighted proximal operator using algorithm involving sort followed isotonic regression backward pass. algorithm similar spirit fusedmax groups reach across non-adjacent indices single pass sufﬁcient. information backward pass computed using stable sort followed linear-time pass ﬁnding groups. optimization possible group indices saved forward pass. forward pass. general projected gradient solver; choose fista iteration requires projection onto simplex; case sq-pnorm-max dominates every iteration leading complexity number iterations performed. backward pass. compute jacobian-vector products solve linear system proposition system ﬁrst sight suggests complexity however structure solve efﬁciently. matrix deﬁned jp∆d sparsemax jacobian rowcolumn-sparse uniquely deﬁned sparsity pattern. splitting system equations corresponding zero nonzero rows obtain solution must sparsity pattern row-sparsity therefore need solve subset system. ﬁxed-point iteration row-sparsity sparsity forward pass solution backward pass complexity thus figure visualization several proposed existing mappings sq-pnorm-max resembles sparsemax smoother transitions. proposed structured attention mechanisms fusedmax oscarmax exhibit plateaus ridges areas weights become fused together. experimental setup. build upon implementation slight variation attention model using grus instead lstms. grus encoding premise hypothesis separate parameters hypothesis initialized last state premise gru. settings methodology ﬁxed -dimensional glove vectors train epochs using adam learning rate drop-out probability choose regularization coefﬁcient experiments performed machines ×xeon .ghz cpus ram. dataset preprocessing. snli dataset apply minimal preprocessing skipping sentence pairs missing labels using provided tokenization. results training sentence pairs development sentence pairs test sentence pairs. report timing measurements table visualizations produced attention weights figure figure attention weights several examples also used hypotheses considered mimes complete silence. riding animal. dogs swim lake. attention mechanisms result correct classiﬁcations seen fusedmax prefers contiguous support segments even weights tied. sequence-to-sequence experiments. defaults unidirectional lstm dimensions word vectors lstm hidden representations drop-out probability global attention input-feeding following default train models epochs stochastic gradient updates weights initialized uniformly gradients normalized norm norm exceeds value. test scores visualizations model snapshot epoch highest validation accuracy. experiments section performed machines equiped xeon cpus nvidia tesla gpus. datasets. employ training test datasets multiple sources. table timing results training textual entailment snli using implementation experimental setup implementation fusedmax oscarmax fast sparsemax three sparse attention mechanisms slightly faster softmax. translation tasks ﬁrst second conferences machine translation available http//www.statmt.org/wmt/translation-task.html http//www.statmt.org/wmt/translation-task.html. training validation test sizes approximately romanian k/k/k german .m/k/k finnish .m/k/k latvian .m/k/k turkish k/k/k. preprocessing scripts moses tokenization needed sgml parsing. limit source target vocabulary sizes lower-cased tokens prune sentences longer tokens training time tokens test time. perform recasing. report bleu scores table showcase enhanced interpretability induced proposed attention mechanisms figure timing measurements found table table timing results french-to-english translation using opennmt-py simplicity attention mechanisms except softmax implemented thus incurring memory copies directions. even without special optimization sparsemax fusedmax oscarmax practical taking within training time softmax model gpu. figure attention alignment examples french-to-english translation following conventions figure denotes hyphen separated spaces. oscarmax induces multiple clusters denote using different bullets fusedmax often selects meaningful grammatical segments consacré well determiner-noun constructions. experimental setup data. exact experimental setup preprocessing machine translation described appendix preprocessed gigaword sentence summarization dataset made available authors https//github.com/harvardnlp/ sent-summary. since unlike perform tuning duc- report results dataset well. observe simple sequence-to-sequence model able keep summaries short without explicit constraints informed training data statistics; therefore section also report results without output truncation bytes also provide precision recall scores rouge-l table finally provide attention weights plots studied attention mechanisms number validation examples figure figure summarization attention examples. prior fusedmax captures well intuition aligning long input spans single expressive words supported rouge scores. figure summarization attention examples. here fusedmax recovers longer arguably better summary identifying separate important part input sentence.", "year": 2017}