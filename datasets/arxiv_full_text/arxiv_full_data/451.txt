{"title": "Enhancing Sentence Relation Modeling with Auxiliary Character-level  Embedding", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "Neural network based approaches for sentence relation modeling automatically generate hidden matching features from raw sentence pairs. However, the quality of matching feature representation may not be satisfied due to complex semantic relations such as entailment or contradiction. To address this challenge, we propose a new deep neural network architecture that jointly leverage pre-trained word embedding and auxiliary character embedding to learn sentence meanings. The two kinds of word sequence representations as inputs into multi-layer bidirectional LSTM to learn enhanced sentence representation. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Experimental results demonstrate that our approach consistently outperforms the existing methods on standard evaluation datasets.", "text": "word embedding serve lookup table word representations. level tasks language modeling tagging name entity recognition semantic role labeling high level tasks machine translation information retrieval semantic analysis deep word representation learning demonstrated importance tasks. tasks performance improvement learning either word level representations sentence level representations. hand researchers found character-level convolutional networks useful extracting information signals task language modeling text classiﬁcation. work focus deep neural network based sentence relation modeling tasks. explore treating sentence kind signal character level applying temporal convolution neural network highway multilayer perceptron multi-layer bidirectional lstm learn sentence representations. propose deep neural network architecture jointly leverage pre-trained word embedding character embedding represent meaning sentences. speciﬁcally approach ﬁrst generates kinds word sequence representations. kind sequence representations composition pre-trained word vectors. kind sequence representation comprise word vectors generating character-level convolutional network. inject sequence representations bidirectional lstm means forward directional lstm accept pre-trained word embedding output backward directional lstm accept auxneural network based approaches sentence relation modeling automatically generate hidden matching features sentence pairs. however quality matching feature representation satisﬁed complex semantic relations entailment contradiction. address challenge propose deep neural network architecture jointly leverage pre-trained word embedding auxiliary character embedding learn sentence meanings. kinds word sequence representations inputs multi-layer bidirectional lstm learn enhanced sentence representation. that construct matching features followed another temporal learn high-level hidden matching feature representations. experimental results demonstrate approach consistently outperforms existing methods standard evaluation datasets. traditional approaches sentence relation modeling tasks paraphrase identiﬁcation question answering recognized textual entailment semantic textual similarity prediction usually build supervised model using variety hand crafted features. hundreds features generated different linguistic levels exploited boost classiﬁcation. success deep learning much interest applying deep neural network based techniques improve prediction performances iliary character embedding output. ﬁnal sentence representation concatenation direction. that construct matching features followed another temporal learn high-level hidden matching feature representations. figure shows neural network architecture general sentence relation modeling. model shows trained small size datasets combining pre-trained word embeddings auxiliary character-level embedding improve sentence representation. word embeddings help capturing general word semantic meanings whereas char-level embedding help modeling task speciﬁc word meanings. note auxiliary character-level embedding based sentence representation require knowledge words even syntactic structure language. enhanced sentence representation generated multi-layer bidirectional lstm encapsulate character word levels informations. furthermore enhance matching features generated computing similarity measures sentence pairs. quantitative evaluations standard dataset demonstrate effectiveness advantages method. besides pre-trained word vectors also interested generating word vectors characters. achieve that leverage deep convolutional neural network. model accepts sequence encoded characters input. encoding done prescribing alphabet size input language quantize character using one-hot encoding. then sequence characters transformed sequence sized vectors ﬁxed length character exceeding length ignored characters alphabet quantized all-zero vectors. alphabet used model consists characters including english letters digits. below introduce character-level temporal convolution neural network. temporal convolution temporal convolution applies one-dimensional convolution input sequence. onedimensional convolution operation vector weights vector inputs viewed sequence vector ﬁlter convolution. concretely think input token single feature value associated i-th character token. idea behind one-dimensional convolution take product vector m-gram token obtain another sequence figure neural network architecture deep matching feature learning. m-blstm multilayer bidirectional lstm. orange color represents sequence representations concatenating pre-trained word vectors. purple color represents sequence representation concatenating word vectors generating character-level convolutional network hmlp. single value ddimensional vector rd×n. exist types convolution operations. called time delay neural networks introduced tdnn weights rd×m form matrix. convolved corresponding architecture sequence length represented highway convolutional neural network layers build another highway multilayer perceptron layer enhance character-level word embeddings. conventional applies afﬁne transformation followed nonlinearity obtain features nonlinearity called transform gate called carry gate. similar memory cells lstm networks highway layers allow adaptively carrying dimensions input directly input training deep networks. kinds word sequence representations. kind sequence representations composition pre-trained word vectors. kind sequence representation comprise word vectors generating character-level convolutional network. inject sequence representations bidirectional lstm learn sentence representation. speciﬁcally forward directional lstm accept pre-trained word embedding output backward directional lstm accept character embedding output. ﬁnal sentence representation concatenation direction. lstm recurrent neural networks capable modeling sequences varying lengths recursive application transition function hidden state. example time step takes input vector hidden state vector applies afﬁne transformation followed element-wise nonlinearity hyperbolic tangent function produce next hidden state vector major issue rnns using transition functions difﬁcult learn longrange dependencies training step components gradient vector grow decay exponentially schmidhuber addresses problem learning long range dependencies introducing memory cell able preserve state long periods time. concretely time step lstm unit deﬁned collection vectors input gate forget gate output gate memory cell hidden state refer memory dimensionality lstm. step lstm takes input produces following transition equations model description shortcoming conventional rnns able make previous context. text entailment decision made whole sentence pair digested. therefore exploring future context would better sequence meaning representation. bidirectional rnns architecture proposed solution making prediction based future words. dconv means one-dimensional convolution. join mean concatenate representation. intuition behind onedimensional convolution preserves common information sentence pairs. reshape feature planes recall multi-layer bidirectional lstm generates sentence representation matrix rn×d concatenating sentence hidden matrix rn×d reversed sentence hidden matrix rn×d. conduct element-wise merge form feature plane rn×d. therefore ﬁnal input temporal convolution layer tensor rf×n×d number matching feature plane number layers memory dimensionality lstm. note tensor convolutional layer input viewed image feature plane channel. computer vision image processing communities spatial convolution often used input image composed several input planes. experiment section compare convolution convolution. order facilitate temporal convolution need reshape tensor. topology matching feature planes viewed channels images image processing. scenario feature planes hold matching information. temporal convolutional neural network learn hidden matching features. mechanism temporal character-level temporal cnn. however kernels totally different. it’s quite important design good topology learn hidden features heterogeneous feature planes. several experiments found topological graphs deployed architecture. figure figure show graphs. topology stack temporal convolution kernel width tanh activation feature plane. that deploy another temporal convolution time step model maintains hid− states left-to-right propagation right-to-left propagation hidden state bidirectional lstm concatenation forward backward hidden states. following equations illustrate main deep rnns created stacking multiple hidden layer other output sequence layer forming input sequence next. assuming hidden layer function used layers stack hidden vectors iteratively computed multilayer bidirectional rnns implemented replacing hidden vector forward backward vectors ensuring every hidden layer receives input forward backward layers level below. furthermore apply lstm memory cell hidden layers construct multilayer bidirectional lstm. finally concatenate sequence hidden rn×d reversed sequence hidden matrix rn×d form sentence represenmatrix tation. refer number layers memory dimensionality lstm. next section matrixs generate matching feature planes linear algebra operations. inspired apply elementwise merge ﬁrst sentence matrix rn×d second sentence matrix rn×d. similar previous method deﬁne simple matching feature planes equations element-wise multiplication. measure interpreted elementwise comparison signs input representations. measure interpreted distance input representations. objective functions task semantic relatedness prediction tries measure degree semantic relatedness sentence pair assigning relatedness score ranging formally given sentence pair wish predict real-valued similarity score range integer. sequence ordinal scale similarity higher scores indicate greater degrees similarity. predict similarity score predicting probability learned hidden representation belongs ordinal scale. done projecting input representation onto hyperplanes corresponds class. distance input hyperplane reﬂects probability input located corresponding scale. referring textual entailment recognition task want maximize likelihood correct class. equivalent minimizing negative log-likelihood speciﬁcally label given inputs predicted softmax classiﬁer takes hidden state tanh activation operation kernel width topology however ﬁrst stack temporal convolution tanh activation kernel width deploy another temporal convolution tanh activation operation kernel width experiment results demonstrate topology slightly better topology conclusion reasonable. feature planes heterogeneous. conducting convolution tanh activation transformation makes sense compare values across different feature planes. selected related sentence relation modeling tasks semantic relatedness task measures degree semantic relatedness sentence pair assigning relatedness score ranging textual entailment task determines whether truth text entails truth another text called hypothesis. standard sick dataset evaluation. consists english sentence pairs annotated relatedness meaning entailment. ﬁrst initialize word representations using publicly available -dimensional glove word vectors lstm memory dimension number layers hand charcnn model threshold activation function temporal convolution pooling pairs charcnn input frame size equals alphabet size output frame size maximum sentence length kernel width temporal convolution step hidden units highwaymlp training done stochastic gradient descent shufﬂed mini-batches adagrad update rule learning rate mini-batch size model parameters regularized per-minibatch regularization strength note word embeddings ﬁxed training. proposed tree-structure based lstm pearson correlation score system reach compared approach method didn’t dependency parsing used predict tasks contains multiple languages. hope point implemented method results good method. results reported paper. based experiments believe method sensitive initializations thus achieve good performance different settings. however method pretty stable beneﬁt joint tasks training. tree lstm sequence lstm experiment compare tree lstm sequential lstm. limitation sequence lstm architectures allow strictly sequential information propagation. however tree lstms allow richer network topologies lstm unit able incorporate information multiple child units. standard lstm units tree-lstm unit contains input output gates memory cell hidden state difference standard lstm unit tree lstm units gating vectors memory cell updates dependent states possibly many child units. additionally instead single forget gate tree lstm unit contains forget gate child allows tree lstm unit selectively incorporate information results discussions table show pearson correlation accuracy comparison results semantic relatedness text entailment tasks. combining charcnn multi-layer bidirectional lstm yields better performance compared traditional machine learning methods maxent approach served many handcraft features. note method doesn’t need extra handcrafted feature extraction procedure. also method doesn’t leverage external linguistic resources wordnet parsing best results importantly task prediction results close state-of-the-art results. proved approaches successfully simultaneously predict heterogeneous tasks. note semantic relatedness task latest research recurrent neural networks regular cnns ﬁxedsize window slides time extract local features sentence; pool features vector usually taking maximum value dimension supervised learning. convolutional unit combined max-pooling compositional operator local selection mechanism recursive autoencoder however semantically related words ﬁlter can’t captured effectively shallow architecture. built deep convolutional models local features high-level layers. however deep convolutional models result worse performance hand take advantage parsing dependency tree sentence structure information used dependencytree recursive neural network text descriptions quiz answers. node tree represented vector; information propagated recursively along tree elaborate semantic composition. major drawback rnns long propagation path information near leaf nodes. gradient vanish propagated deep path long dependency buries illuminating information complicated neural architecture leading difﬁculty training. address issue proposed tree-structured long short-term memory networks. motivates investigate multi-layer bidirectional lstm directly models sentence meanings without parsing task. paper propose deep neural network architecture jointly leverage pre-trained word embedding character embedding learn sentence meanings. approach ﬁrst generates kinds word sequence representations inputs bidirectional lstm learn sentence representation. that construct matching features followed another temporal learn high-level hidden matching feature representations. model shows combining pre-trained word embeddings auxiliary character-level embedding improve sentence representation. enhanced sentence representation generated multi-layer bidirectional lstm encapsulate character word levels informations. furthermore enhance matching features generated computing similarity measures sentence pairs. experimental results benchmark datasets demonstrate framework achieved state-of-the-art references yoshua bengio patrice simard paolo fransconi. learning long-term dependencies gradient descent difﬁcult. ieee transactions neural networks johannes bjerva johan goot malvina nissim. meaning factory formal semantics recognizing textual entailment determining semantic similarity. proceedings semeval international workshop semantic evaluation. ronan collobert jason weston l´eon bottou michael karlen koray kavukcuoglu pavel kuksa. natural language processing scratch. journal machine learning research alex graves navdeep jaitly abdel rahman mohamed. hybrid speech ieee recognition deep bidirectional lstm. workshop automatic speech recognition understanding pages iyyer jordan boyd-graber leonardo claudino richard socher daum´e iii. neural network factoid empirical question answering paragraphs. methods natural language processing. sergio jimenez george duenas julia baquero alexander gelbukh. unal-nlp combining soft cardinality features semantic textual similarity relatedness entailproceedings semeval internament. tional workshop semantic evaluation. kalchbrenner phil blunsom. recurrent continuous proceedings translation models. conference empirical methods natural language processing pages seattle washington usa. association computational linguistics. kalchbrenner edward grefenstette phil blunsom. convolutional neural network modelling sentences. proceedings annual meeting association computational linguistics. xiang zhang junbo zhao yann lecun. character-level convolutional networks text classiﬁcation. advances neural information processing systems pages yoon kim. convolutional neural networks sentence classiﬁcation. proceedings conference empirical methods natural language processing pages doha qatar. association computational linguistics. jiang zhao tian tian lan. ecnu stone birds ensemble heterogenous measures semantic relatedness textual entailment. proceedings semeval international workshop semantic evaluation. alice julia hockenmaier. illinois-lh denotational distributional approach semantics. proceedings semeval international workshop semantic evaluation. tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases advances neural compositionality. information processing systems pages thomas proisl stefan evert. robust semantic similarity multiple levels using maximum weight matching. proceedings semeval international workshop semantic evaluation. richard socher eric huang jeffrey pennin christopher manning andrew dynamic pooling unfolding recursive autoencoders paraphrase detection. advances neural information processing systems pages richard socher jeffrey pennington eric huang andrew christopher manning. semi-supervised recursive autoencoders predicting sentiment distributions. proceedings conference empirical methods natural language processing pages association computational linguistics. richard socher andrej karpathy quoc christopher manning andrew grounded compositional semantics ﬁnding describing images sentences. transactions association computational linguistics. sheng richard socher christopher manning. improved semantic representations tree-structured long arxiv preprint short-term memory networks. arxiv.. wenpeng hinrich schutze. multigrancnn architecture general matching text chunks multiple levels proceedings annual granularity. meeting association computational linguistics pages", "year": 2016}