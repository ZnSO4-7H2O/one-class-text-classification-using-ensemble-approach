{"title": "Reinforcement Learning with Parameterized Actions", "tag": ["cs.AI", "cs.LG"], "abstract": "We introduce a model-free algorithm for learning in Markov decision processes with parameterized actions-discrete actions with continuous parameters. At each step the agent must select both which action to use and which parameters to use with that action. We introduce the Q-PAMDP algorithm for learning in these domains, show that it converges to a local optimum, and compare it to direct policy search in the goal-scoring and Platform domains.", "text": "direct policy search method. show appropriate update rules q-pamdp converges local optimum. methods compared empirically goal platform domains. found q-pamdp out-performed direct policy search ﬁxed parameter sarsa. markov decision process tuple states actions probability transitioning state state taking action probability receiving reward taking action state discount factor wish policy selects action state maximize expected discounted rewards expected return obtained taking action state following policy thereafter. using value function control requires model would prefer without needing model. approach problem learning allows directly select action maximizes learn optimal policy using method q-learning domains continuous state space represent using parametric function approximation parameters learn algorithms gradient descent sarsa problems continuous action space selecting optimal action respect nontrivial requires ﬁnding global maximum function continuous space. avoid problem using policy search algorithm class policies parameterized parameters given transforms problem direct optimization objective function several policy search approaches introduce model-free algorithm learning markov decision processes parameterized actions—discrete actions continuous parameters. step agent must select action parameters action. introduce q-pamdp algorithm learning domains show converges local optimum compare direct policy search goalscoring platform domains. reinforcement learning agents typically either discrete continuous action space discrete action space agent decides distinct action perform ﬁnite action set. continuous action space actions expressed single real-valued vector. continuous action space lose ability consider differences kind actions must expressible single vector. discrete actions lose ability ﬁnely tune action selection based current state. parameterized action discrete action parameterized real-valued vector. modeling actions introduces structure action space treating different kinds continuous actions distinct. step agent must choose action parameters execute with. example consider soccer playing robot kick pass run. associate continuous parameter vector actions kick ball given target position given force pass speciﬁc position given velocity. actions parameterized way. parameterized action markov decision processes model situations distinct actions require parameters adjust action different situations multiple mutually incompatible continuous actions. focus learn action-selection policy given pre-deﬁned parameterized actions. introduce q-pamdp algorithm alternates learning actionselection parameter-selection policies compare copyright association advancement artiﬁcial intelligence rights reserved. apply two-tiered approach action selection ﬁrst selecting parameterized action selecting parameters action. discrete-action policy denoted select parameters action deﬁne action-parameter policy action policy given parameterized tasks parameterized task problem deﬁned task parameter vector given beginning episode. parameters ﬁxed throughout episode goal learn task dependent policy. kober developed algorithms adjust motor primitives different task parameters. apply learn table-tennis darts different starting positions targets. silva introduced idea parameterized skill task dependent parameterized policy. sample tasks learn associated parameters determine mapping task policy parameters. deisenroth applied model-based method learn task dependent parameterized policy. used learn task dependent policies ball-hitting task solving block manipulation problem. parameterized tasks used parameterized actions. example learn parameterized task kicking ball position could used parameterized action kick-to. words select complete action sample discrete action sample parameter action policy deﬁned paω. action-parameter rameters denoted policy action determined parameters parameters given denoted second approach alternate updating parameter-policy learning action-value function discrete actions. pamdp ﬁxed parameter-policy exists corresponding discrete action algorithm describes method alternating updating algorithm uses input methods p-update q-learn positive integer parameter determines number updates iteration. p-update policy search method optimizes respect objective function q-learn algorithm q-learning function approximation. consider main cases q-pamdp algorithm q-pamdp q-pamdp. q-pamdp performs single update relearns convergence. step update once update convergence optimize respect next section show local optimum respect found local optimum respect show q-pamdp converges local global optimum mild assumptions. assume iterating p-update converges respect given objective function p-update step design choice selected appropriate convergence property. q-pamdp equivalent sequence summary locally optimize step local optimum conditions previous theorem assuming p-update local optimization method gradient based policy search. similar argument shows sequence converges global optimum respect q-pamdp converges global optimum problem step must re-learn updated value show updates bounded continuous function required updates also bounded. intuitively supposing small update results small change weights specifying discrete action choose. assumption continuous strong satisﬁed pamdps. necessary operation q-pamdp satisﬁed need completely re-learn update show selecting appropriate shrink differences desired. theorem continuous respect updates form ﬁrst consider simpliﬁed robot soccer problem single striker attempts score goal keeper. episode starts player random position along bottom bound ﬁeld. player starts ball possession keeper positioned ball goal. game takes place environment player keeper position velocity orientation ball position velocity resulting continuous state variables. episode ends keeper possesses ball player scores goal ball leaves ﬁeld. reward action non-terminal state terminal goal state terminal non-goal state distance ball goal. player parameterized actions kick-to kicks ball towards position shoot-goal shoots ball towards position along goal line. noise added action. player possession ball moves towards keeper ﬁxed policy moves towards ball player shoots goal keeper moves intercept ball. score goal player must shoot around keeper. means positions must shoot left past keeper others right past keeper. however point shoot keeper optimal policy discontinuous. split action parameterized actions shoot-goal-left shoot-goal-right. allows simple action selection policy instead complex continuous action policy. policy would difﬁculty represent purely continuous action space simpler parameterized action setting. represent action-value function discrete action using linear function approximation fourier basis features state variables must selective basis functions use. basis functions non-zero elements exclude velocity state variables. soft-max discrete action policy represent action-parameter policy normal distribution around weighted features matrix weights gives features state ﬁxed covariance matrix. specialized features action. shoot-goal actions using simple linear basis projection keeper onto goal line. kick-to linear features /||b want p-update optimize logical choice would gradient update. next theorem shows gradient equal gradient useful apply existing gradient-based policy search methods compute gradient respect proof follows fact global optimum respect gradient zero. theorem requires differentiable theorem differentiable respect differentiable respect gradient given proof. compute gradient chain rule summarize continuous p-update converges global local optimum q-pamdp converge global local optimum respectively q-learn step bounded update rate p-update step bounded. such p-update policy gradient update step q-pamdp theorem converge local optimum theorem q-learn step require ﬁxed number updates. policy gradient step gradient respect q-pamdp step performs full optimization full optimization step would optimize update update q-pamdp disadvantage requiring global convergence properties p-update method. theorem step q-pamdp bounded direct policy search approach episodic natural actor critic algorithm computing gradient respect q-pamdp approach gradientdescent sarsa algorithm q-learning enac algorithm policy search. step perform enac update based episodes reﬁt using gradient descent sarsa episodes. return directly correlated goal scoring probability graphs close indentical. easier interpret plot goal scoring probability ﬁgure direct enac outperformed q-pamdp q-pamdp. likely difﬁculty optimizing action selection parameters directly rather q-learning. methods goal probability greatly increased initial policy rarely scores goal q-pamdp q-pamdp increase probability goal roughly direct enac converged local maxima finally include performance sarsa action parameters ﬁxed initial achieves roughly scoring probability. q-pamdp q-pamdp strongly outperform ﬁxed parameter sarsa enac not. figure depicts single episode using converged q-pamdp policy— player draws keeper strikes goal open. next consider platform domain agent starts platform must reach goal avoiding enemies. agent reaches goal platform touches enemy falls platforms episode ends. domain depicted ﬁgure reward step change value step divided total length platforms gaps. agent primitive actions jump continue ﬁxed period agent lands respectively. different kinds jumps high jump enemies long jump gaps platforms. domain therefore three parameterized actions leap. agent takes actions ground enemies move agent platform. state space consists four variables representing agent position agent speed enemy position enemy speed respectively. learning previous domain linear function approximation fourier basis. apply softmax discrete action policy based gaussian parameter policy based scaled parameter features figure shows performance enac q-pamdp q-pamdp sarsa ﬁxed parameters. q-pamdp q-pamdp outperformed ﬁxed parameter sarsa method reaching average total distance respectively. suggest qpamdp outperforms q-pamdp nature platform domain. q-pamdp best suited domains smooth changes action-value function respect changes parameter-policy. platform domain initial policy unable make ﬁrst jump without modiﬁcation. policy reach second platform need drastically change actionvalue function account platform. therefore qparameterized wait action research also takes planning perspective considers time dependent domain. additionally size parameter space parameterized actions actions. hoey considered mixed discrete-continuous actions work bayesian affect control theory. approach problem form pomcp monte carlo sampling algorithm using domain speciﬁc adjustments compute continuous action components note discrete continuous components action space reﬂect different control aspects discrete control provides what continuous control describes research symbolic dynamic programming algorithms zamani considered domains discrete parameterized actions. actions different parameter space. symbolic dynamic programming form planning relational ﬁrstorder mdps logical relationships deﬁning dynamics reward function. algorithms represent value function extended algebraic decision diagram limited mdps predeﬁned logical relations. hierarchical action subtasks. subtask states actions subtasks. hierarchical mdps well-suited representing parameterized actions could consider selecting parameters discrete action subtask. maxq method value function decomposition hierarchical mdps possiblity maxq learning actionvalues parameterized action problem. pamdp formalism models reinforcement learning domains parameterized actions. parameterized actions give adaptibility continuous domains distinct kinds actions. also allow simple representation discontinuous policies without complex parameterizations. presented three approaches modelfree learning pamdps direct optimization variants q-pamdp algorithm. shown qpamdp appropriate p-update method converges local global optimum. q-pamdp global optimization step converges local optimum. examined performance approaches goal scoring domain platformer domain. robot soccer goal domain models situation striker must out-maneuver keeper score goal. these q-pamdp q-pamdp outperformed enac ﬁxed parameter sarsa. q-pamdp q-pamdp performed similarly well terms goal scoring learning policies score goals roughly time. platform domain found q-pamdp qpamdp outperformed enac ﬁxed sarsa. pamdp poorly suited domain small change parameters occurs failing making jump actually making results large change action-value function. better ﬁxed sarsa baseline much better direct optimization using enac reached figure shows successfully completed episode platform domain. hauskrecht introduced algorithm solving factored mdps hybrid discrete-continuous action space. however formalism action space mixed discrete continuous components whereas domain distinct actions different number continuous components action. furthermore assume domain compact factored representation consider planning. rachelson encountered parameterized actions form action wait given period time research time dependent continuous time mdps developed xmdps tmdps parameterized action space developed bellman operator domain later paper mentions timdppoly algorithm work parameterized actions although speciﬁcally refers zamani sanner fang symbolic dynamic programming proceedings continuous state action mdps. twenty-sixth aaai conference artiﬁcial intelligence.", "year": 2015}