{"title": "Parameter-less hierarchical BOA", "tag": ["cs.NE", "cs.AI", "G.1.6; I.2.6; I.2.8"], "abstract": "The parameter-less hierarchical Bayesian optimization algorithm (hBOA) enables the use of hBOA without the need for tuning parameters for solving each problem instance. There are three crucial parameters in hBOA: (1) the selection pressure, (2) the window size for restricted tournaments, and (3) the population size. Although both the selection pressure and the window size influence hBOA performance, performance should remain low-order polynomial with standard choices of these two parameters. However, there is no standard population size that would work for all problems of interest and the population size must thus be eliminated in a different way. To eliminate the population size, the parameter-less hBOA adopts the population-sizing technique of the parameter-less genetic algorithm. Based on the existing theory, the parameter-less hBOA should be able to solve nearly decomposable and hierarchical problems in quadratic or subquadratic number of function evaluations without the need for setting any parameters whatsoever. A number of experiments are presented to verify scalability of the parameter-less hBOA.", "text": "abstract. parameter-less hierarchical bayesian optimization algorithm enables hboa without need tuning parameters solving problem instance. three crucial parameters hboa selection pressure window size restricted tournaments population size. although selection pressure window size inﬂuence hboa performance performance remain low-order polynomial standard choices parameters. however standard population size would work problems interest population size must thus eliminated diﬀerent way. eliminate population size parameter-less hboa adopts population-sizing technique parameter-less genetic algorithm. based existing theory parameter-less hboa able solve nearly decomposable hierarchical problems quadratic subquadratic number function evaluations without need setting parameters whatsoever. number experiments presented verify scalability parameter-less hboa. hierarchical bayesian optimization algorithm designed argued hboa solve diﬃcult nearly decomposable hierarchical problems without need setting parameters. result solve black-box optimization problem suﬃcient plug problem hboa press start button wait hboa ﬁgures optimum argued hboa parameters except population size default values without aﬀecting good scalability hboa however choosing adequate population size argued crucial experiments user assumed population size optimally obtain best performance retaining reliable convergence. dream fully parameter-less optimizer entire class nearly decomposable hierarchical problems remained parameter away reality. genetic algorithm hboa. parameter-less hboa simulates collection populations diﬀerent sizes starting small base population size. next population twice large previous one. enable parallel simulation number populations number populations must simulated known advance population forced proceed speed smaller population speed considered respect number function evaluations. ensures although upper bound number populations simulated parallel size overall computational overhead still reasonable compared case optimal population size. fact theory exists shows parameter-less population sizing scheme parameter-less increase number function evaluations convergence logarithmic factor result hboa expected perform within logarithmic factor case optimal population size. veriﬁed number experiments nearly decomposable hierarchical problems. paper starts discussing probabilistic model-building genetic algorithms hierarchical section discusses parameter-less genetic algorithm serves primary source inspiration designing parameter-less hboa. section describes parameterless hboa. section describes experiments performed discusses empirical results. finally section summarizes concludes paper. probabilistic model-building genetic algorithms replace traditional variation operators genetic evolutionary algorithms two-step procedure. ﬁrst step probabilistic model built promising solutions selection. next probabilistic model sampled generate solutions. replacing variation operators inspired genetics machine learning techniques allow automatic discovery problem regularities populations promising solutions pmbgas provide quick accurate reliable solution broad classes diﬃcult problems many intractable using optimizers overview pmbgas please references pmbgas also known estimation distribution algorithms iterated density-estimation algorithms remainder section describes hierarchical bayesian optimization algorithm advanced powerful pmbgas. hierarchical bayesian optimization algorithm evolves population candidate solutions given problem. ﬁrst population candidate solutions usually generated random. population updated number iterations using basic operators selection variation. selection operator selects better solutions expense worse ones current population yielding population promising candidates. variation operator starts learning probabilistic model selected solutions. hboa uses bayesian networks local structures model promising solutions. variation operator proceeds sampling probabilistic model generate solutions incorporated original population using restricted tournament replacement ensures useful diversity population maintained long periods time. terminated good enough solution found population improved long time number generations exceeded given upper bound. figure shows pseudocode hboa. detailed description hboa parameter-less genetic algorithm eliminates need setting parameters—such population size selection pressure crossover rate mutation rate—in genetic algorithms. crossover rate selection pressure ensure consistent growth building blocks based schema theorem. mutation rate eliminated similar manner. population size eliminated simulating collection populations diﬀerent sizes. context hboa eliminating population size important part parameter-less selection pressure inﬂuences performance hboa constant factor crossover mutation rates hboa. parameter-less assumes selection recombination primary search operators. choice selection pressure crossover rate consider following facts. selection must strong enough ensure consistent growth superior building blocks optimum must strong otherwise diversity population might lost prematurely. hand crossover rate must large enough ensure suﬃcient exploration search space blind crossover operators used application crossover break important building block thus crossover must applied frequently. additionally tradeoﬀ selection crossover. greater selection pressures allow greater crossover rates. analogically smaller selection rates allow smaller crossover rates. tradeoﬀ formalized using simpliﬁcation schema theorem claims expected number copies partial solution schema selection crossover given characterizes selection strength factor number best solutions grow represents disruption schema crossover. ignoring mutation making conservative assumption crossover always disrupts schema easy show setting ensures growth schemata contained best solution. original parameter-less consider mutation. incorporate mutation probability disrupting schema mutation would incorporated bit-ﬂip mutation binary strings solution ﬂipped ﬁxed probability bounding case could assume schema consideration spans across entire solution. case probability disrupting schema mutation computed total number bits solution. interacting variables traditional variation operators fail even parameter-less going suﬀer excessive disruption ineﬀective mixing building blocks negative eﬀects blind variation cannot eliminated tweaking parameters modifying operators themselves. eliminate population size parameter-less simulates collection populations diﬀerent size important population size exists population size greater equal collection. otherwise problems require population size greater equal could solved. consequently collection must contain inﬁnitely many populations size cannot upper bounded. parameter-less arranges collection populations sequence. size ﬁrst population sequence small constant called base population size. size second population twice size ﬁrst population. general next population twice size previous population population size thus grows exponentially starting base population size. stant. original parameter-less considered means smaller population allowed proceed twice speed next larger population speed measured respect ﬁtness evaluations. shown inﬁnite collection populations simulated tractably without increasing number function evaluations convergence logarithmic factor respect optimal population size based convergence theory large small populations number generations assumed upper-bounded constant depend population size. since small populations process generations faster larger populations seems reasonable terminate simulation population point computational resources eﬃciently. population converges consists many copies single solution. case expected improvement take place anymore search become ineﬃcient. clearly criterion going much eﬀect niching used. larger population greater average ﬁtness. since larger populations converge generally fast smaller populations situation indicates smaller population stuck local optimum thus terminated. important note logarithmic overhead computed consider termination criterion parameter-less thus perform well even without terminating populations collection. parameter-less hboa incorporates population-sizing technique parameter-less hboa. since hboa ensures growth mixing important building blocks learning sampling probabilistic model promising solutions reason restrict crossover rate oﬀspring created sampling probabilistic model promising solutions. additionally selection pressure favors best candidate solutions used without changing scalability hboa constant factor. generation executed executing generations pi−. populations proceed speed respect number evaluations. population initialized ﬁrst iteration executed. pseudocode used simulate collection populations described shown figure implementation slightly diﬀerent based k-ary counter described ﬁrst parameter-less study termination criteria parameter-less used parameter-less hboa. however since hboa uses niching population collection expected converge long time. additionally terminate population executes number generations equal number bits input string. according experience enabling population generations improve performance further whereas decreasing limit number generations might endanger convergence exponentially scaled hierarchical problems. similarly parameterless parameter-less hboa populations also indeﬁnitely without increasing worst-case overhead compared case optimal population size predicted theory. parameterless hboa applied artiﬁcial hierarchical nearly decomposable problems spin glasses nearest neighbor interactions periodic boundary conditions. artiﬁcial problems problem size varied examine scalability parameter-less hboa. performance parameter-less hboa compared hboa optimal population size. spin glasses systems diﬀerent size tested random instances examined problem size ensure results would provide insight hboa performance wide range spin glass instances. improve hboa performance spin glasses deterministic local search ﬂips candidate solution solution cannot improved anyused improve candidate solution evaluated. local searcher favors best change iteration experiments base population size used. problem instance problem size parameter-less hboa ﬁrst optimum independent runs total number evaluations every recorded. average number function evaluations displayed. results compared results hboa minimum population size ensures independent runs converge optimum. minimum population size determined using bisection width resulting interval lower bound. deceptive function order input string ﬁrst partitioned independent groups bits each. partitioning unknown algorithm change run. -bit deceptive function applied group bits contributions deceptive functions added together form ﬁtness. -bit deceptive function deﬁned follows number input string bits. -bit deceptive function fully deceptive means variation operators break interactions bits group statistics lower order lead algorithm away optimum. crossover operators well model umda fail solving problem faster exponential number evaluations random search. since deceptive functions bound broad class decomposable problems performance parameter-less hboa class problems indicate performance expected decomposable problems. figure shows number evaluations hierarchical optimal population size determined bisection method parameter-less hboa order- deceptive functions bits. results indicate number function evaluations decomposable problems increases near constant factor eliminating parameters hboa thus aﬀect scalability hboa decomposable problems qualitatively. order- hierarchical trap levels total number bits candidate solution assumed integer power candidate solution evaluated multiple levels overall value hierarchical trap computed contributions levels. level basis function similar deceptive function order used contributions thus equally good superior combinations bits group. contributions groups added together form overall contribution ﬁrst level. group mapped single symbol next level using following interpretation function symbols second level also partitioned independent groups bits group contributes ﬁtness level using basis function ﬁrst level however groups contain symbol contribute ﬁtness all. overall contribution second level multiplied added contribution ﬁrst level. group mapped third level using interpretation function used ﬁrst level second one. principle used evaluate higher level except level contains symbols contribution level always multiplied overall contribution level magnitude. diﬀerence evaluating level symbols whereas global optimum hierarchical traps string however blocks seem equally good level. furthermore evolutionary algorithm biased solutions many neighborhood solutions many inferior. since hierarchical traps bound broad class hierarchical problems performance parameter-less hboa class problems indicate performance expected hierarchical problems. figure shows number evaluations hierarchical optimal population size determined bisection method parameter-less hboa hierarchical traps bits. results indicate number function evaluations hierarchically decomposable problems increases constant factor eliminating parameters hboa aﬀect scalability hboa class problems qualitatively. spin-glass system consists regular grid containing nodes. edges grid connect nearest neighbors. additionally edges ﬁrst last element dimension added introduce periodic boundary conditions. task ground state spin glass speciﬁed coupling constants ground state conﬁguration spins minimizes energy system given equation spin glass conﬁguration represented string bits corresponds spin represents spin represents spin created random spin glasses sizes generating coupling constants equal probabilities. spin glasses coupling constants restricted called spin glasses. figure shows number evaluations hierarchical optimal population size determined bisection method parameter-less hboa spin glasses. good news number evaluations convergence parameter-less hboa still grows low-order polynomial respect number decision variables. news that unlike single-level hierarchical traps case parameterless population sizing inﬂuence scalability hboa. speciﬁcally order polynomial approximates number evaluations increases clearly linear factor number evaluations increases disagrees existing theory claims factor logarithmic hypothesis happens case dynamics hboa local search changes increasing population size. using hybrid method enables searchers—hboa local searcher— cooperate solve problem faster. hboa allows local searcher exit enormous number local optima make local search intractable. hand local searcher decreases population sizes hboa signiﬁcantly making algorithm focus regions local optima. performance hybrid depends well division labor local global searcher done. increasing population size hboa local search inﬂuence division labor hybrid. since parameter-less hboa simulates number populations diﬀerent sizes believe division labor aﬀected opportunities given hboa expense local searcher. currently verifying hypothesis. paper described implemented tested parameter-less hierarchical boa. parameter-less hboa enables practitioner simply plug problem hboa without requiring practitioner ﬁrst estimate adequate population size problem-speciﬁc parameters. despite parameter-less scheme low-order polynomial time complexity hboa broad classes optimization problems retained. parameter-less hboa thus true black-box optimization algorithm applied hierarchical nearly decomposable problems without setting parameters whatsoever. interesting topic future work develop techniques improve hboa performance automatic tuning parameters maximum order interactions probabilistic model window size rtr.", "year": 2004}