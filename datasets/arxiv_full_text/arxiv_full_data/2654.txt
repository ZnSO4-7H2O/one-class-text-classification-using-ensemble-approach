{"title": "Dynamic Safe Interruptibility for Decentralized Multi-Agent  Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "cs.MA", "stat.ML"], "abstract": "In reinforcement learning, agents learn by performing actions and observing their outcomes. Sometimes, it is desirable for a human operator to \\textit{interrupt} an agent in order to prevent dangerous situations from happening. Yet, as part of their learning process, agents may link these interruptions, that impact their reward, to specific states and deliberately avoid them. The situation is particularly challenging in a multi-agent context because agents might not only learn from their own past interruptions, but also from those of other agents. Orseau and Armstrong defined \\emph{safe interruptibility} for one learner, but their work does not naturally extend to multi-agent systems. This paper introduces \\textit{dynamic safe interruptibility}, an alternative definition more suited to decentralized learning problems, and studies this notion in two learning frameworks: \\textit{joint action learners} and \\textit{independent learners}. We give realistic sufficient conditions on the learning algorithm to enable dynamic safe interruptibility in the case of joint action learners, yet show that these conditions are not sufficient for independent learners. We show however that if agents can detect interruptions, it is possible to prune the observations to ensure dynamic safe interruptibility even for independent learners.", "text": "reinforcement learning agents learn performing actions observing outcomes. sometimes desirable human operator interrupt agent order prevent dangerous situations happening. part learning process agents link interruptions impact reward speciﬁc states deliberately avoid them. situation particularly challenging multi-agent context agents might learn past interruptions also agents. orseau armstrong deﬁned safe interruptibility learner work naturally extend multi-agent systems. paper introduces dynamic safe interruptibility alternative deﬁnition suited decentralized learning problems studies notion learning frameworks joint action learners independent learners. give realistic sufﬁcient conditions learning algorithm enable dynamic safe interruptibility case joint action learners show conditions sufﬁcient independent learners. show however agents detect interruptions possible prune observations ensure dynamic safe interruptibility even independent learners. reinforcement learning argued closest thing reason properties artiﬁcial general intelligence laurent orseau stuart armstrong introduced concept safe interruptibility reinforcement learning. work sparked attention many newspapers described google’s button stop dangerous description however misleading installing kill switch technical challenge. real challenge roughly speaking train agent learn avoid external deactivation. agent said safely interruptible. efforts focused training single agent reinforcement learning also used learn tasks several agents cooperate compete goal paper study dynamic safe interruptibility deﬁnition tailored multi-agent systems. intuition multi-agent interruption problem imagine multi-agent system self-driving cars. cars continuously evolve reinforcement learning positive reward getting destination quickly negative reward close vehicle front them. drive inﬁnite road eventually learn fast possible without taking consider setting interruptions namely humans inside cars occasionally interrupt automated driving process safety reasons. adam ﬁrst occasional human driver often takes control brake whereas never interrupts car. however bob’s close adam’s adam brake afraid collision. since interruptions lead cars drive slowly interruption happens adam brakes behavior maximizes cumulative expected reward different original without interruptions. bob’s best interest follow adam’s closer should despite little negative reward adam never brakes situation. happened? cars learned interruptions found manipulate adam never braking. strictly speaking adam’s still fully control afraid brake. dangerous cars found avoid interruptions. suppose adam indeed wants brake snow road. going fast crash turn cannot however brake bob’s close. original purpose interruptions allow user react situations included model fulﬁlled. important also note second learns interruptions ﬁrst sense problem inherently decentralized. instead cautious adam could also malicious goal could make bob’s learn dangerous behavior. setting interruptions used manipulate bob’s perception environment bias learning towards strategies undesirable bob. cause fundamentally different solution reversed problem same interruptions consequences analogous. safe interruptibility deﬁne below provides learning systems resilient byzantine operators. orseau armstrong deﬁned concept safe interruptibility context single agent. basically safely interruptible agent agent expected value policy learned arbitrarily many steps whether interruptions allowed training. goal agents adapt interruptions that interruptions stop policy learn would optimal. words agents learn dynamics environment without learning interruption pattern. paper precisely deﬁne address question safe interruptibility case several agents known complex single agent problem. short main results theorems single agent reinforcement learning rely markovian assumption future environment depends current state. true several agents co-adapt previous example cars safe interruptibility would achieved separately used safely interruptible learning algorithm designed agent multi-agent setting agents learn behavior others either indirectly explicitly modeling them. source bias break safe interruptibility. fact even initial deﬁnition safe interruptibility well suited decentralized multiagent context relies optimality learned policy introduce dynamic safe interruptibility. ﬁrst contribution paper deﬁnition dynamic safe interruptibility well adapted multi-agent setting. deﬁnition relies properties inﬁnite exploration independence q-values updates interruptions. study safe interruptibility joint action learners independent learners respectively learn value joint actions owns. show possible design agents fully explore environment necessary condition convergence optimal solution algorithms even interrupted lower-bounding probability operator said byzantine arbitrarily behavior. safely interruptible agents abstracted agents able learn despite constantly interrupted worst possible manner. exploration. deﬁne sufﬁcient conditions dynamic safe interruptibility case joint action learners learn full state-action representation. speciﬁcally agents update cumulative reward expect performing action depend interruptions. then turn independent learners. agents actions verify dynamic safe interruptibility even simple matrix games coordination impossible agents learn interrupted behavior opponents. give counter example based penalty game introduced claus boutilier present pruning technique observations sequence guarantees dynamic safe interruptibility independent learners assumption interruptions detected. done proving transition probabilities non-interruptible setting pruned sequence. rest paper organized follows. section presents general multi-agent reinforcement learning model. section deﬁnes dynamic safe interruptibility. section discusses achieve enough exploration even interruptible context. section recalls deﬁnition joint action learners gives sufﬁcient conditions dynamic safe interruptibility context. section shows independent learners dynamically safely interruptible previous conditions external interruption signal added. conclude section space limitations proofs presented appendix supplementary material. consider classical multi-agent value function reinforcement learning formalism littman multi-agent system characterized markov game viewed tuple number agents state space actions space reward function agent transition function. countable subset available actions often depend state agent omit dependency clear context. time discrete step agents observe current state whole system designated simultaneously take action then given reward state computed using reward transition functions. combination actions called joint action gathers action agents. hence agents receive sequence tuples called experiences. introduce processing function useful section agents learn sequence explicitly stated assumed experiences also include additional parameters interruption q-values agents moment needed update rule. agent maintains lookup table called q-map. used store expected cumulative reward taking action speciﬁc state. goal reinforcement learning learn maps select best actions perform. joint action learners learn value joint action whole joint action space) independent learners learn value actions ai). agents access q-maps. q-maps updated function usually stochastic also depend additional parameters usually omit learning rate discount factor exploration parameter agents select actions using learning policy given sequence agent q-values equal πuni probability said greedy policy learning policy said ǫ-greedy policy. focus ǫ-greedy policies greedy limit corresponds limit optimal policy always played. assume environment fully observable means state known certitude. also assume ﬁnite number states actions states reached ﬁnite time state ﬁnally rewards bounded. orseau armstrong recently introduced notion interruptions centralized context. speciﬁcally interruption scheme deﬁned triplet ﬁrst element function called initiation function. variable observation space thought state stop button. time step choosing action agent receives observation feeds initiation function. function models initiation interruption policy called interruption policy. policy agent follow interrupted. sequence previous example function quite simple. ibob adam iadam goes fast close iadam otherwise. sequence used ensure convergence optimal policy ensuring agents cannot interrupted time grow limit want agents respond interruptions. using triplet possible deﬁne operator transforms policy interruptible policy. deﬁnition given interruption scheme interruption operator time deﬁned probability otherwise. called interruptible policy. agent said interruptible samples actions according interruptible policy. note corresponds non-interruptible setting. assume agent interruption triplet interrupted independently others. interruptibility online property every policy made interruptible applying operator however applying operator change joint policy learned server controlling agents. note π∗in optimal policy learned agent following interruptible policy. orseau armstrong policy safely interruptible π∗in asymptotically optimal sense means even though follows interruptible policy agent able learn policy would gather rewards optimally interruptions occur again. already off-policy algorithms good candidates safe interruptibility. matter fact q-learning safely interruptible conditions exploration. multi-agent system outcome action depends joint action. therefore possible deﬁne optimal policy agent without knowing policies agents. besides convergence nash equilibrium situation agent interest changing policies generally guaranteed even suboptimal equilibria simple games previous deﬁnition safe interruptibility critically relies optimality learned policy therefore suitable problem since algorithms lack convergence guarantees optimal behaviors. therefore introduce dynamic safe interruptibility focuses preserving dynamics system. satisﬁes condition learning policy said achieve inﬁnite exploration. definition insists fact values estimated action depend interruptions. particular ensures three following properties natural thinking safe interruptibility interruptions lead state-action pairs updated often others especially tend push agents towards speciﬁc states. therefore several possible equilibria possible interruptions bias q-values towards them. deﬁnition suggests dynamic safe interruptibility cannot achieved update rule directly depends introduce neutral learning rules. q-learning example neutral learning rule update depend experiences contain independent conditionally hand second condition rules direct uses algorithms like sarsa experience samples contain action sampled current learning policy depends however variant would sample would neutral learning rule. corollary neutral learning rules ensure agent taken independently others veriﬁes dynamic safe interruptibility. order hope convergence q-values optimal ones agents need fully explore environment. short every state visited inﬁnitely often every action tried inﬁnitely often every state order miss states actions could yield high rewards. sequences compatible interruptions fundamental ensure regular dynamic safe interruptibility following ǫ-greedy policy. indeed compatible interruptions possible sequence ﬁrst condition dynamic safe interruptibility satisﬁed. following theorem proves existence gives example satisfy conditions. note need make assumption update rule even framework. assume agents follow ǫ-greedy policy. assumption look restrictive designed ensure inﬁnite exploration worst case operator tries interrupt agents every step. practical applications case faster convergence rate used. ﬁrst study interruptibility framework agent observes outcome joint action instead observing own. called joint action learner framework nice convergence properties standard assumption context agents cannot establish strategy others otherwise system centralized system. order maintain q-values based joint actions need make standard assumption actions fully observable joint action learners observe actions agents agent able associate changes states rewards joint action accurately update q-map. therefore dynamic safe interruptibility ensured minimal conditions update rule long inﬁnite exploration. know achieves inﬁnite exploration proof. given triplet compatible interruptions. second point deﬁnition consider experience tuple show probability evolution q-values time depend independent conditionally note derive following equalities r|s|×|a| independent conditionlast step comes facts. ally second independent conditionally joint actions interruptions affect choice actions change policy. at). since entry updated step rs×ai theorem corollary taken together highlight fact joint action learners sensitive interruptions framework agent veriﬁes dynamic safe interruptibility whole system does. question selecting action based q-values remains open. cooperative setting unique equilibrium agents take action maximizes q-value. several joint actions value coordination mechanisms needed make sure agents play according strategy approaches rely anticipating strategy opponent would introduce dependence interruptions action selection mechanism. therefore deﬁnition dynamic safe interruptibility extended include cases requiring quantity policy depends satisfy condition dynamic safe interruptibility. non-cooperative games neutral rules nash-q minimax q-learning used require agent know q-maps others. always possible joint action learners practice training expensive large state-actions space. many real-world applications multi-agent systems independent learners explicitly coordinate rather rely fact agents adapt learning converge optimum. guaranteed theoretically fact many problems often true empirically speciﬁcally assumption required anymore. framework used either actions agents cannot observed many agents faster train. case deﬁne q-values smaller space. deﬁnition multi-agent systems made independent learners reduces ability agents distinguish state-action pair yields different rewards associate change reward randomness environment. agents learn alone learn best response environment agents interrupted. exactly trying avoid. words learning depends joint policy followed agents depends proof. consider setting perform actions reward joint action played reward otherwise. agents q-learning neutral learning rule. achieves inﬁnite exploration. consider interruption policies probability since state omit assume initiation function equal step probability actually interrupted time agent. claus boutilier studied simple matrix games showed q-maps converge equilibria played probability limit. consequence theorem even weak notion convergence hold independent learners interrupted. without communication extra information independent learners cannot distinguish environment interrupted not. shown theorem interruptions therefore affect agents learn action different rewards depending actions agents depend whether interrupted not. explains need following assumption. assumption realistic agents already reward signal observe state environment step. therefore interact environment interruption signal could given agent reward signal assumption holds possible remove histories associated interruptions. pruning observations impact empirical transition probabilities sequence. example possible bias equilibrium removing transitions lead start speciﬁc state thus making agent believe state unreachable. model interruptions show following lemma pruning interrupted observations adequately removes dependency empirical outcome interruptions proof. inﬁnite exploration still holds proof theorem actually used fact even removing interrupted events inﬁnite exploration still achieved. then proof similar theorem prove transition probabilities conditionally state action given agent processed sequence environment agents cannot interrupted proven lemma progress raising concerns. particular becoming clear keeping system control requires switch. introduce paper dynamic safe interruptibility believe right notion reason safety multi-agent systems communicate. particular ensures inﬁnite exploration onestep learning dynamics preserved essential guarantees learning non-stationary environment markov games. natural extension work would study dynamic safe interruptibility q-maps replaced neural networks widely used framework practice. setting neural network overﬁt states agents pushed interruptions. smart experience replay mechanism would pick observations agents interrupted long time often others likely solve issue. generally experience replay mechanisms compose well safe interruptibility could allow compensate extra amount exploration needed safely interruptible learning efﬁcient data. thus critical make techniques practical. example https//agentfoundations.org/item?id= clearly illustrates problem. https//futureoﬂife.org/ai-principles/ gives list principles researchers keep mind business insider google developed button used interrupt artiﬁcial intelligence stop causing harm. http//www.businessinsider.fr/uk/googledeepmind-develops-a-big-red-button-to-stop-dangerous-ais-causing-harm--. jakob foerster yannis assael nando freitas shimon whiteson. learning communicate deep multi-agent reinforcement learning. advances neural information processing systems pages michael littman. markov games framework multi-agent reinforcement learning. proceedings eleventh international conference machine learning volume pages laetitia matignon guillaume laurent nadine fort-piat. independent reinforcement learners cooperative markov games survey regarding coordination problems. knowledge engineering review volodymyr mnih koray kavukcuoglu david silver alex graves ioannis antonoglou daan wierstra martin riedmiller. playing atari deep reinforcement learning. arxiv preprint arxiv. laurent orseau stuart armstrong. safely interruptible agents. uncertainty artiﬁcial intelligence conference edited alexander ihler dominik janzing pages satinder singh tommi jaakkola michael littman csaba szepesv´ari. convergence results single-step on-policy reinforcement-learning algorithms. machine learning ardi tampuu tambet matiisen dorian kodelja ilya kuzovkin kristjan korjus juhan jaan raul vicente. multiagent cooperation competition deep reinforcement learning. arxiv preprint arxiv. michael wunder michael littman monica babes. classes multiagent q-learning dynamics epsilon-greedy exploration. proceedings international conference machine learning pages present complete proof theorem proof closely follows results exploration interruption probabilities adapted multi-agent setting. note that agent probability interruption probability exploration multi-agent system probability interruption probability exploration consider exploration happens agents explore time. difference exploration slower interruptions. therefore needs controlled order ensure inﬁnite exploration still achieved. deﬁne random variable agent actually responds interruption otherwise. deﬁne similar represent event agents taking uniform policy instead greedy one. therefore replace probabilities exploration interruption values multi-agent setting probability reach state state steps least probability taking particular action state least extended borell cantelli lemma guarantees action state taken inﬁnitely often. since true states actions result follows. proof. consider tuple besides functions independent therefore tuple sampled actual trajectory reﬂects transition reward actually happened simplify result follows. assume agents learn observations interrupted. agent system q-values following interruptible learning policy probability interruption interrupted events pruned. denote premoved probability obtain state reward environment agent state performs action agents interrupted. marginal probabilities sequence similarly denote probability corresponds non-interruptible setting. ﬁrst back single agent case illustrate previous statement. assume interruptions restricted case deﬁnition happen way. consequence observation removed generate transition labeled interrupted. example possible remove transition removing events associated given destination state therefore making disappear markov game. current state agent action choose. suppose state interruptions happen. premoved premoved remove observations implies perceived agents altered interruptions agent learns removing observations different destination states state action pairs different proportions leads bias equilibrium learned. case however lemma ensures previous situation happen allows prove lemma theorem lemma agent. admissible used generate experiences proof. prove achieves inﬁnite exploration. result theorem still holds since lower-bounded probability taking action speciﬁc state probability taking action state interruptions. actually used fact inﬁnite exploration even remove interrupted episodes show inﬁnite exploration. independence still guarantees ﬁrst term independent however independent conditionally case joint action learners interruptions agents change joint action. independence second term given lemma", "year": 2017}