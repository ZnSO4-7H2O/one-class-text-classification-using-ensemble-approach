{"title": "Data-Driven Sparse Structure Selection for Deep Neural Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Deep convolutional neural networks have liberated its extraordinary power on various tasks. However, it is still very challenging to deploy state-of-the-art models into real-world applications due to their high computational complexity. How can we design a compact and effective network without massive experiments and expert knowledge? In this paper, we propose a simple and effective framework to learn and prune deep models in an end-to-end manner. In our framework, a new type of parameter -- scaling factor is first introduced to scale the outputs of specific structures, such as neurons, groups or residual blocks. Then we add sparsity regularizations on these factors, and solve this optimization problem by a modified stochastic Accelerated Proximal Gradient (APG) method. By forcing some of the factors to zero, we can safely remove the corresponding structures, thus prune the unimportant parts of a CNN. Comparing with other structure selection methods that may need thousands of trials or iterative fine-tuning, our method is trained fully end-to-end in one training pass without bells and whistles. We evaluate our method, Sparse Structure Selection with several state-of-the-art CNNs, and demonstrate very promising results with adaptive depth and width selection.", "text": "rect pruning low-rank decomposition quantization another stream researches trained small efﬁcient networks directly knowledge distillation novel architecture designs sparse learning spare learning prior works pursued sparsity weights. however non-structure sparsity produce random connectivities hardly utilize current off-theshelf hardwares gpus accelerate model inference wall clock time. address problem recently methods proposed apply group sparsity retain hardware friendly structure. paper take another view jointly learn prune cnn. first introduce type parameter scaling factors scale outputs speciﬁc structures cnns. scaling factors endow ﬂexibility parameters. then sparsity regularizations scaling factors push zero training. finally safely remove structures correspond zero scaling factors pruned model. comparing direct pruning methods method data driven fully end-to-end. words network select unique conﬁguration based difﬁculty needs task. moreover model selection accomplished jointly normal training cnns. require extra ﬁne-tuning multi-stage optimizations introduces minor cost training. propose uniﬁed framework model training pruning cnns. particularly formulate joint sparse regularized optimization problem introducing scaling factors corresponding sparse regularizations certain structures cnns. utilize modiﬁed stochastic accelerated proximal gradient method jointly optimize weights cnns scaling factors sparsity regularizations. compared previous methods utilize heuristic ways force sparsity methods endeep convolutional neural networks liberated extraordinary power various tasks. however still challenging deploy state-of-the-art models realworld applications high computational complexity. design compact effective network without massive experiments expert knowledge? paper propose simple effective framework learn prune deep models end-to-end manner. framework type parameter scaling factor ﬁrst introduced scale outputs speciﬁc structures neurons groups residual blocks. sparsity regularizations factors solve optimization problem modiﬁed stochastic accelerated proximal gradient method. forcing factors zero safely remove corresponding structures thus prune unimportant parts cnn. comparing structure selection methods need thousands trials iterative ﬁne-tuning method trained fully end-to-end training pass without bells whistles. evaluate method sparse structure selection several state-of-the-art cnns demonstrate promising results adaptive depth width selection. deep learning methods especially convolutional neural networks achieved remarkable performances many ﬁelds computer vision natural language processing speech recognition. however extraordinary performances expense high computational storage demand. although power modern gpus skyrocketed last years high costs still prohibitive cnns deploy latency critical applications self-driving cars augmented reality etc. cnns skip connections main stream modern network design since mitigate gradient vanishing/exploding issue ultra deep networks help skip connections among work resnet variants attracted attention simple design principle state-ofthe-art performances. recently veit interpreted resnet exponential ensemble many shallow networks. minor impact performance removing single residual block. however deleting residual blocks impair accuracy signiﬁcantly. therefore accelerating state-of-the-art network architecture still challenging problem. paper propose data-driven method learn architecture kind network. scaling pruning residual blocks training method produce compact resnet faster inference speed even better performance. notations consider weights convolutional layer layers -dimensional tensor rnl×ml×hl×wl number output channels represents number input channels height width -dimensional kernel. denote weights k-th neuron layer scaling factors represented -dimensional vector number structures consider prune. refers i-th value denote soft-threshold operator sign+. sparse structure selection given training consisting sample-label pairs yi}≤i≤n layers represented function {wl}≤l≤l represents collection weights cnn. learned solving optimization problem form prior sparse based model structure learning work tried learn number neurons cnn. achieve goal added group sparsity regularization zero during training. another concurrent work adopted similar method multiple different structures. ideas straightforward implementations nontrivial. first optimization difﬁcult since test proposed method several state-of-theart networks resnet resnext prune neurons residual blocks groups respectively. adaptively adjust depth width accordingly. show promising acceleration performances cifar large scale ilsvrc image classiﬁcation datasets. network pruning pioneered early development neural network. optimal brain damage optimal brain surgeon unimportant connections removed based hessian matrix derived loss function. recently brought back idea pruning weights whose absolute value smaller given threshold. approach requires iteratively pruning ﬁne-tuning time-consuming. tackle problem proposed dynamic network surgery prune parameters training. however nature irregular sparse weights make yield effective compression faster inference terms wall clock time. tackle issue several works pruned neurons directly evaluating neuron importance speciﬁc criteria. methods focus removing neurons whose removal affect ﬁnal prediction least. hand diversity neurons kept also important factor consider recently formulate pruning optimization problem. ﬁrst select representative neurons minimize reconstitution error recover accuracy pruned networks. neuron level pruning achieve practical acceleration moderate accuracy loss still hard implement end-to-end manner without iteratively pruning retraining. recently used similar technique prune neurons. sparsify scaling parameters batch normalization select channels. discussed later work seen special case framework. model structure learning deep learning models attracted increasing attention recently. several methods explored learn architectures without handcrafted design stream explore design space reinforcement learning genetic algorithms another stream utilize sparse learning. added group sparsity regularizations weights neurons sparsiﬁed training stage. lately proposed general approach applied group sparsity multiple structures networks including ﬁlter shapes channels layers skip connections. figure network architecture method. represents residual function. gray block group neuron mean inactive pruned since corresponding scaling factors iteration proxηrs since however formulation friendly deep learning since additional pass updating need obtain extra forward-backward computation computational expensive deep neural networks. thus following derivation reformulate momentum based method deﬁne formulation similar modiﬁed nesterov accelerated gradient except update furthermore simpliﬁed update replacing following modiﬁcation widely used practical deep learning frameworks parameters several constraints weights simultaneously including weight decay group sparsity. improper optimization technique result slow convergence inferior results. consequently successful attempt directly apply methods large scale applications complicated modern network architectures. paper address structure learning problem simple effective way. different directly pushing weights group zero enforce output group zero. achieve goal introduce type parameter scaling factor scale outputs speciﬁc structures sparsity constraint training. goal obtain sparse namely safely remove corresponding structure since outputs contribution subsequent computation. fig. illustrates framework. update stochastic gradient descent momentum variants. adopt accelerated proximal gradient method solve better illustration shorten reformulate optimization weighted residual networks though sharing similarities motivations behind works different. work focuses train ultra deep resnet better results help scaling factors. particularly increase depth method aims decrease depth resnet scaling factors sparse regularizations sparsify output residual blocks. represents transformation parameters cardinality aggregated. practice grouped convolution ease implementation aggregated transformations. framework refer number group formulate weighted training several basic cardinalities chosen sparse form ﬁnal transformations. then inactive groups zero scaling factors safely removed shown note neuron pruning also seen special case group pruning group contains neuron. furthermore combine block pruning group pruning learn ﬂexible network structures. section evaluate effectiveness method three standard datasets including cifar- cifar- imagenet lsvrc neuron pruning adopt classical plain network validate method. blocks groups state-of-the-art networks resnet resnext respectively. optimization adopt modiﬁed update weights scaling factors respectively. weight decay momentum weights initialized scaling factors initialized experiments conducted mxnet code made publicly available paper accepted. framework scaling factors three different micro-structures including neurons groups blocks yield ﬂexible structure selection. introduce three cases following. note networks scaling factors prevent inﬂuence bias parameters. introduce scaling factors output channels prune neurons. training removing ﬁlters zero scaling factor result compact network. recent work proposed adopted similar idea network slimming. absorbed scaling parameters parameters batch normalization solve optimization subgradient descent. training scaling parameters whose absolute value lower threshold value comparing method general effective. firstly introducing scaling factor universal reusing parameters. hand networks batch normalization layers alexnet hand ﬁne-tune pre-trained models object detection semantic segmentation tasks parameters batch normalization usually ﬁxed small batch size. secondly optimization heuristic need iterative pruning retraining. contrast optimization stable end-to-end manner. seen special case method. structure skip connection cnns allows skip computation speciﬁc layers without cutting information network. stacking residual blocks resnet easily exploit advantage deep networks. formally residual block identity mapping formulated following formula shown optimization sparse residual block scaling factor pruned entirely learn much shallower resnet. prior work also adds scaling factors residual resnet start cifar dataset evaluate method. cifar- dataset consists training testing images classes. cifar- similar cifar- except classes. suggested input image randomly cropped zeropadded image ﬂipping. models experiments trained mini-batch size single gpu. start learning rate train models epochs. learning rate divided -th-th epoch. connected layer classiﬁcation. scale factors every batch normalization layers. fig. shows results method. parameters ﬂoating-point operations second reported. method save parameters computational cost minor lost performance. resnet learn number residual blocks resnet- resnet- baseline networks. resnet- consists residual blocks. block convolutional layers resnet- blocks bottleneck structure block. fig. summarizes results. easy achieves better performance baseline model similar parameters flops. resnet- yields speedup performance loss cifar- cifar. optimization found blocks early stages pruned ﬁrst. discovery coincides common design network spend budget later stage since diverse complicated pattern emerge receptive ﬁeld increases. additionally evaluate performance proposed adding group sparsity weights residual block. however found optimization network could converge. guess reason number parameters group deﬁned much larger original paper normal heuristic thresholding adopted unable solve optimization problem. resnext also test method resnext choose resnext- resnext- base networks. networks bottleneck structures groups residual blocks. resnext- focus groups pruning since residual blocks resnext- sparsity groups blocks. fig. shows experiment results. groups pruning block pruning show good trade-off parameters performance especially resnext-. combination groups blocks pruning extremely effective cifar-. saves flops achieves higher accuracy. resnext- groups ﬁrst second block pruned ﬁrst. similarly resnext- groups shallow residual blocks pruned mostly. demonstrate effectiveness method large-scale cnns conduct experiments imagenet lsvrc classiﬁcation task resnet- resnext- data augmentation based publicly available implementation fb.resnet mini-batch size gpus resnet- gpus resnext-. optimization initialization similar cifar experiments. train models epochs. learning rate initial value divided epoch. results imagenet dataset summarized table experiments pruning results pruning convolutional layers promising. computational cost terms flops equally distributed layer. number flops conv layers billion total whole network thus consider sparse penalty adjusted computational cost different layers. similar idea adopted introduce flops regularization pruning criteria. prune conv layers experiments. following sparse penalty conv prune conv conv. results found table results imagenet dataset. top- validation errors reported. number parameters flops inference different models also shown. here means million/billion respectively. table table shows detailed structure pruned vgg. pruned model save flops parameter saving negligible. fully-connected layers large amount parameters pruned fully-connected layers fair comparison methods. resnet- resnet- experiment three different settings explore performance method block pruning. simplicity denote trained models resnet- resnet- resnet- depending depths. structures shown table pruned models come accuracy loss certain extent. comparing original resnet- resnet- provides flops reduction top- accuracy loss resnet- saves flops top- loss. fig. shows top- validation errors models resnets function number parameters flops. results reveal pruned models perform original hand-crafted resnets whilst requiring less parameters computational cost. example comparing resnet- resnet- resnet- yield better performances less flops. resnext- resnext- sparsity constraint residual blocks groups results several pruned models. table summarizes performance models. learned resnext- yields top- error ilsvrc validation set. gets similar results original resnet half parameters less flops. resnext- three residual blocks conv stage pruned entirely. pruning result somewhat contradict common design cnns worth studied depth future. compare pruning methods including ﬁlter pruning channel pruning thinet table shows pruning results imagenet lsvrc dataset. best knowledge works reported resnet pruning results flops. comparing ﬁlter pruning results resnet- performs best least flops. channel pruning resnet- yields similar top- error pruned resnet- provided saves flops. also show comparison vgg. method including channel pruning thinet achieve signiﬁcant improvement pruning result competitive state-of-the-art. paper proposed data-driven method sparse structure selection adaptively learn structure cnns. framework training pruning cnns formulated joint sparse regularized optimization problem. pushing scaling factors introduced scale outputs speciﬁc structures zero method remove structures corresponding zero scaling factors. solve challenging optimization problem adapt deep learning models modiﬁed accelerated proximal gradient method. experiments demonstrate promising pruning results resnet resnext. adaptively adjust depth width cnns based budgets hand difﬁculties task. believe pruning results inspire design compact cnns. future work plan apply method applications object detection. also interesting investigate advanced sparse regularizers non-convex relaxations adjust penalty based complexity different structures adaptively.", "year": 2017}