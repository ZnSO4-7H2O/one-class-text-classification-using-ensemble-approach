{"title": "Efficient Approximations for the Marginal Likelihood of Incomplete Data  Given a Bayesian Network", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We discuss Bayesian methods for learning Bayesian networks when data sets are incomplete. In particular, we examine asymptotic approximations for the marginal likelihood of incomplete data given a Bayesian network. We consider the Laplace approximation and the less accurate but more efficient BIC/MDL approximation. We also consider approximations proposed by Draper (1993) and Cheeseman and Stutz (1995). These approximations are as efficient as BIC/MDL, but their accuracy has not been studied in any depth. We compare the accuracy of these approximations under the assumption that the Laplace approximation is the most accurate. In experiments using synthetic data generated from discrete naive-Bayes models having a hidden root node, we find that the CS measure is the most accurate.", "text": "discuss bayesian methods learning bayesian networks data sets incomplete. particular examine asymptotic approximations marginal likelihood incomplete data given bayesian network. consider laplace approximation less accurate eﬃcient bic/mdl approximation. also consider approximations proposed draper cheeseman stutz approximations eﬃcient bic/mdl accuracy studied depth. compare accuracy approximations assumption laplace approximation accurate. experiments using synthetic data generated discrete naive-bayes models hidden root node bic/mdl measure least accurate bias favor simple models draper measures accurate. step bayesian approach learning graphical models computation marginal likelihood data given model. given complete data set—that data sample contains observations every variable model marginal likelihood computed exactly efﬁciently certain assumptions contrast observations missing including situations variables hidden never observed exact determination marginal likelihood typically intractable. consequently approximate techniques computing marginal likelihood used. techniques include monte carlo approaches gibbs sampling importance sampling sequential updating methods asymptotic approximations paper examine asymptotic approximations comparing accuracy eﬃciency. consider laplace approximation bayesian information criterion equivalent risannen’s minimum-description-length measure. addition consider approximations described draper cheeseman stutz theoretical empirical studies shown laplace approximation accurate bic/mdl furthermore well known laplace approximation signiﬁcantly less eﬃcient bic/mdl draper cheeseman-stutz measures. knowledge however theoretical formal empirical studies compare accuracy laplace approximation draper cheeseman stutz. describe experimental comparison approximations learning directed graphical models discrete variables variable hidden. consequently compute posterior distribution using equation computation especially simple parameter sets mutually independent—an assumption call parameter independence—and prior distribution parameter dirichlet distribution making problem diﬃcult suppose also uncertain structure encodes true distribution. given prior distribution possible network-structure hypotheses compute corresponding posterior distribution using bayes rule bayesian approach example statisticians call model averaging. computation known marginal likelihood given simply marginal likelihood remainder paper assume priors network structure uniform relative posterior probability marginal likelihood same. prior knowledge restrict possible network structures manageable number select model equation approximate true expectation approximation example model practice selects model using selection. search procedure produces candidate network structures applying scoring function found structure retaining structure highest score. reasonable scoring function marginal likelihood dawid notes following interesting interpretation marginal likelihood scoring function. suppose model predict probability sample given previously observed samples assign utility prebayesian approach learning bayesian networks data follows. given domain variables suppose know true joint distribution encoded bayesian-network structure denote hypothesis encoding possible. also suppose uncertain parameters bayesian network determine true joint distribution. given prior distribution parameters random sample true joint distribution apply bayes’ rule infer posterior distribution consider case variables discrete. denote variables corresponding parents denote possible state possible state respectively. also denote number possible states respectively. assuming logical constraints true joint probabilities imposed network structure parameters correspond true probabilities associated bayesian-network structure. particular parameters θijk possible values θijk true probability given transpose column vector negative hessian evaluated ˜θs. substituting equation equation integrating taking logarithm result obtain laplace approximation dimension model given region ˜θs. bayesian network discrete variables dimension typically number kass shown that certain regularity conditions errors approximation bounded number samples eﬃcient less accurate approximation obtained retaining terms equation increase increases linearly increases also large approximated maximum likelihood value vector value maximum. thus obtain given regularity conditions similar laplace approximation accurate large error bounds approximation increase increases. thus select models select model whose posterior probability maximum diction using proper scoring rule. utility ﬁrst prediction make prediction based solely prior distribution utility second prediction compute prediction training network structure using ﬁrst sample utility prediction make prediction training network structure ﬁrst samples summing utilities obtain bayes’ rule equal marginal likelihood independent order process samples thus network structure highest marginal likelihood precisely model best sequential predictor data according scoring rule. random sample incomplete exact computation marginal likelihood intractable real-world problems thus approximations required. paper consider asymptotic approximations. well-known asymptotic approximation laplace gaussian approximation idea behind laplace approximation large amounts data that often approximated multivariate gaussian distribution. consequently variables observed sample term sample requires trivial computation either zero one. otherwise bayesian network inference algorithm evaluate term. computation called step algorithm. assignment called step algorithm. dempster showed that certain regularity conditions iteration steps converge local maximum. algorithm assumes parameter independence typically used whenever expected suﬃcient statistics computed eﬃciently imaginary data consistent expected suﬃcient statistics computed using step local value discrete variables approximation given logarithm right-hand-side equation nijk replaced call scoring function marginal likelihood expected data mled. mled desirable properties. because computes marginal likelihood punishes model complexity laplace draper bic/mdl measures. complete data computation measure eﬃcient. approximation interesting several respects. first depend prior. consequently approximation without assessing prior. second approximation quite intuitive. namely contains term measuring well model parameters value predicts data term punishes complexity model third approximation exactly additive inverse minimum description length scoring function described rissanen measure asymptotically correct conditions bic/mdl. ﬁnite data sets however draper mentions found approximation better bic/mdl. shall refer equation draper scoring function. compute laplace approximation must compute negative hessian evaluated ˜θs. meng rubin describe numerical technique computing second derivatives. raftery shows approximate hessian using likelihood-ratio tests available many statistical packages. thiesson demonstrates that discrete variables second derivatives computed using bayesian-network inference. computing approximations must determine ˆθs. technique ﬁnding maximum gradient ascent follow derivatives likelihood local maximum. buntine russell thiesson discuss compute derivatives likelihood bayesian network discrete variables. eﬃcient technique identifying local value algorithm applied bayesian networks discrete variables algorithm works follows. first assign values somehow next compute expected suﬃcient approximations using laplace approximation gold standard. verify assumption exact computations marginal likelihood possible models consider. thus results experiments must interpreted caution. particular rule possibility mled approximations better laplace approximation. evaluated accuracy mled draper bic/mdl approximations relative laplace approximation using synthetic models containing single hidden variable. reasons discussed section limited synthetic networks naive-bayes models discrete variables naive-bayes model variables encodes assertion mutually independent given network structure model contains single root node leaf nodes parent. generated variety naive-bayes models varying number states number observed variables determined parameters model sampling uniform distribution sampled data model make root node hidden variable. namely sampled data model using usual monte-carlo approach ﬁrst sampled state according sampled state according discarded samples retaining samples single experiment ﬁrst generated model given subsequently data sets given sample size next approximated marginal likelihood data given series test models identical synthesized model except allowed number states hidden variable vary. finally compared different approximations marginal likelihood context model averaging model selection. compare approximations model averaging simply compared plots marginal likelihood versus states hidden variable test model directly. compare approximations model selection compared number states hidden variable selected using given approximation number states selected using laplace approximation. dimension model given data region around ˆθs—that number parameters increases diﬀerence increase. also discussed either case mled asymptotically correct. simple modiﬁcation mled addresses problems equation ﬁrst proposed cheeseman stutz scoring function autoclass algorithm data clustering. shall refer equation cheeseman-stutz scoring function. note mled scoring functions easily extended directed gaussian-mixture models described lauritzen wermuth undirected gaussian-mixture models. accuracy approximations examine following sections must balanced computation costs. evaluation mled draper bic/mdl dominated determination time complexity task number iterations cost inference equation evaluation laplace approximation dominated computation determinant negative hessian time complexity computation typically laplace approximation least eﬃcient having complexity mentioned laplace approximation known accurate bic/mdl draper approximations. contrast knowledge theoretical work done comparing laplace approximation mled approximations. nonetheless experiments assume laplace approximation accurate approximations measure accuracy figure approximate marginal likelihood data given test model function number hidden states test model. sample data sets generated naive-bayes models observed variables hidden states. step. then retained copies parameters largest iterations. next retained copies parameters largest iterations. continued procedure four times parameters remained. guarantee convergence algorithm performed iterations following initialization phase. check algorithm converged measured relative change between successive iterations. using convergence criterion similar autoclass’ default said algorithm converged relative change fell algorithm converged runs. assigned dirichlet priors parameter used almost uniform prior αijk produced local maxima interior parameter space. conclusions sensitive range tested report results value bic/mdl measures map. given choice αijk diﬀerences values insigniﬁcant. used method thiesson evaluate negative hessian compute scoring function assumed dimensions equal. although proof assumption experiments geiger suggest assumption valid. first evaluated approximations model averaging comparing plots approximate marginal likelihood versus number states hidden variable test model. conducted three sets comparisons diﬀerent values results almost diﬀerent data sets given experiment consequently show results data figure approximate marginal likelihood data given test model function number hidden states test model. sample data sets generated naive-bayes models observed variables hidden states. ﬁrst experiments ﬁxed varied particular generated -sample data sets four naive-bayes models observed variables respectively model hidden variable four states. figure shows approximate marginal likelihood data given test models hidden variables eight states. second experiments ﬁxed varied particular generated -sample data sets four naive-bayes models hidden states respectively model observed variables. figure shows approximate marginal likelihood data test models values straddle value generative model. third experiments ﬁxed varied particular naivebayes model generated data sets sample sizes respectively. figure shows approximate marginal likelihood data test models hidden variables eight states. trends marginal-likelihood curves function surprising. approximation curves become peaked value increases increases decreases. ﬁrst result says learning improves amount data increases. second result reﬂection fact larger numbers observed variables provide evidence identity hidden variable. third result says becomes diﬃcult learn number hidden states increases. comparing curves note diﬀerences shape curves important. height curves important marginal likelihoods normalized process model averaging. overall draper scoring functions appear equally good approximations better bic/mdl. mled scoring functions almost identical except small values approximation better. next evaluated approximations model selection. experiment particular computed size model selected given approximation—that number states hidden variable test model havfigure approximate marginal likelihood data given test model function number hidden states test model. data sets size generated naive-bayes models observed variables hidden states. largest approximate marginal likelihood. subtracted number size model selected gold-standard laplace approximation yielding quantity called results shown table overall draper mled scoring functions accurate bic/mdl consistently selects models simple. mled results almost same except small values slightly better. mled measures tend select models complex whereas draper measure tends select models simple large values draper measure better approximation large values source diﬀerence seen graphs figure algorithms graphs show peaks broad peak around sharp peak around number hidden states generative model. laplace draper curves tend downward right sharply enough ﬁrst peak dominates. contrast curve remains fairly right second peak dominates. existence second peak true number hidden states quite interesting pursue paper. discussed accuracy results must balanced computational costs various approximations. time complexities given section overly pessimistic naive-bayes models probabilistic inferences cached reused. naive-bayes models evaluation mled draper bic/mdl measures time complexity evaluation laplace approximation given appreciate constants costs times experiment shown table ﬁndings valid naive-bayes models hidden root node. results important because apply directly autoclass algorithm growing popularity. also likely results extend models discrete variables data sets variable unobserved observed markov blanket. conditions bayesian inference required scoring functions reduces naive-bayes computation. nonetheless detailed experiments warranted address models general analysis scoring functions hidden-variable models made important assumption. namely assumed that true model contains hidden variable better learn searching models hidden variables without hidden variables. assumption trivially correct. given naive-bayes model variables joint distribution variables encoded bayesian network withhidden variables. thus attempt learn model containing hidden variables model accurate learned searching naive-bayes models hidden root node. tested assumption follows. first generated naive-bayes model model generated data size discarding observations variable second learned single naive-bayes model containing hidden root node using experimental technique described previous section. particular varied number hidden states naivebayes model selected largest marginal likelihood. third learned single model containing hidden variables using approach deevaluated learned models comparing marginal likelihoods. speciﬁcally computed used laplace approximation compute ﬁrst term exact expression marginal likelihood compute second term. repeating experiment times obtained indicating hidden-variable model better predicted data. additional experiments found increased increased size models. evaluated laplace mled draper bic/mdl approximations marginal likelihood naive-bayes models hidden root node assumption laplace approximation accurate scoring functions. experiments indicate bic/mdl measure least accurate bias favor simple models draper measures accurate bias favor simple complex models respectively cases. thank geiger chris meek useful discussions asymptotic approximations koos rommelse help system implementation anonymous reviewers suggestions.", "year": 2013}