{"title": "Clustering on Multi-Layer Graphs via Subspace Analysis on Grassmann  Manifolds", "tag": ["cs.LG", "cs.CV", "cs.SI", "stat.ML"], "abstract": "Relationships between entities in datasets are often of multiple nature, like geographical distance, social relationships, or common interests among people in a social network, for example. This information can naturally be modeled by a set of weighted and undirected graphs that form a global multilayer graph, where the common vertex set represents the entities and the edges on different layers capture the similarities of the entities in term of the different modalities. In this paper, we address the problem of analyzing multi-layer graphs and propose methods for clustering the vertices by efficiently merging the information provided by the multiple modalities. To this end, we propose to combine the characteristics of individual graph layers using tools from subspace analysis on a Grassmann manifold. The resulting combination can then be viewed as a low dimensional representation of the original data which preserves the most important information from diverse relationships between entities. We use this information in new clustering methods and test our algorithm on several synthetic and real world datasets where we demonstrate superior or competitive performances compared to baseline and state-of-the-art techniques. Our generic framework further extends to numerous analysis and learning problems that involve different types of information on graphs.", "text": "information single layer taken isolation. thus expect proper combination information contained different layers leads improved understanding structure data relationships entities dataset. paper consider m-layer graph individual graph layers represents common vertex represents edge i-th individual graph associated edge weights example three-layer graph shown fig. three graph layers share vertices different edges clearly different graph layers capture different types relationships vertices objective method properly combines information different layers. ﬁrst adopt subspace representation information provided individual graph layers inspired spectral clustering algorithms propose novel method combining multiple subspace representations representative subspace. speciﬁcally model graph layer subspace grassmann manifold. problem combining multiple graph layers transformed problem efﬁciently merging different subspaces grassmann manifold. study distances subspaces develop framework merge subspaces overall distance representative subspace individual subspaces minimized. show framework well justiﬁed results statistical learning theory proposed method dimensionality reduction algorithm original data; abstract—relationships entities datasets often multiple nature like geographical distance social relationships common interests among people social network example. information naturally modeled weighted undirected graphs form global multilayer graph common vertex represents entities edges different layers capture similarities entities term different modalities. paper address problem analyzing multi-layer graphs propose methods clustering vertices efﬁciently merging information provided multiple modalities. propose combine characteristics individual graph layers using tools subspace analysis grassmann manifold. resulting combination viewed dimensional representation original data preserves important information diverse relationships entities. information clustering methods test algorithm several synthetic real world datasets demonstrate superior competitive performances compared baseline state-of-the-art techniques. generic framework extends numerous analysis learning problems involve different types information graphs. pairwise relationships among sets entities; used various analysis tasks classiﬁcation clustering. traditionally graph captures single form relationships entities data analyzed light one-layer graph. however numerous emerging applications rely different forms information characterize relationships entities. diverse examples include human interactions social network similarities images videos multimedia applications. multimodal nature relationships naturally represented weighted undirected graphs share common vertices different edge weights depending type information graph. represented multi-layer multi-view graph gathers sources information unique representation. assuming graph layers informative likely provide complementary information thus offer richer transforming information original graph meaningful subspace representation. another example principal component analysis interpretation graphs described links graph structure subspace spanned eigenvectors graph laplacian matrix. works inspired consider subspace representation section iii. past decades subspace-based methods widely used classiﬁcation clustering problems notably image processing computer vision. authors discovered human faces characterized low-dimensional subspaces. authors proposed so-called eigenfaces recognition. inspired works researchers particularly interested data data points pattern represented subspace. growing interests ﬁeld increasingly large number works tools grassmann manifold theory provides natural tool subspace analysis. authors given detailed overview basics grassmann manifold theory developed optimization techniques grassmann manifold. author presented statistical analysis grassmann manifold. works study distances grassmann manifold. authors proposed learning frameworks based distance analysis positive semideﬁnite kernels deﬁned grassmann manifold. recent representative works include studies authors proposed optimal subspace representation optimization grassmann manifold analysis authors presented statistical methods stiefel grassmann manifolds applications vision. similarly work proposed novel discriminant analysis framework based graph embedding matching authors presented subspace indexing model grassmann manifold classiﬁcation. however none works considers datasets represented multilayer graphs. time multi-view data attracted large learning research communities. amount interest data form multi-layer graph representations generally refer data analyzed different viewpoints. setting challenge combine efﬁciently information multiple graphs learning purposes. existing techniques roughly grouped following categories. first straightforward form convex combination information individual graphs. example authors developed method learn optimal convex combination laplacian kernels different graphs. authors proposed markov mixture model corresponds convex combination normalized adjacency matrices individual graphs supervised unsupervised learning. authors presented several averaging techniques combining information individual graphs clustering. second following intuitive approaches ﬁrst category many existing works ﬁnding uniﬁed various learning problems solved using relationships classiﬁcation clustering. speciﬁcally focus paper clustering problem want uniﬁed clustering vertices utilizing representative subspace better clustering achieved graph layers independently. address problem ﬁrst apply generic framework subspace analysis grassmann manifold compute meaningful summarization information contained individual graph layers. implement spectral clustering algorithm based representative subspace. experiments synthetic real world datasets demonstrate advantages approach compared baseline algorithms like summation individual graphs well stateof-the-art techniques co-regularization finally believe framework beneﬁcial clustering also many data processing tasks based multilayer graphs multi-view data general. paper organized follows. ﬁrst review related work summarize contribution paper section section describe subspace representation inspired spectral clustering captures characteristics single graph. section review main ingredients grassmann manifold theory propose framework combining information multiple graph layers. propose novel algorithm clustering multi-layer graphs section compare performance clustering methods multiple graphs section finally conclude section vii. section review related work literature. first describe brieﬂy graph-based clustering algorithms particular focus methods subspace interpretations. second summarize previous works built upon subspace analysis grassmann manifold theory. finally report recent progresses ﬁeld analysis multi-layer graphs multi-view data. clustering graphs studied extensively numerous applications different domains. works given comprehensive overviews advancements ﬁeld last decades. algorithms based spectral techniques graphs particular interest typical examples spectral clustering modularity maximization spectral method speciﬁcally approaches propose embed vertices original graph dimensional space usually called spectral embedding consists eigenvectors special matrix special properties matrices clustering dimensional spaces usually becomes trivial. therefore corresponding clustering approaches interpreted representation multiple graphs using sophisticated methods. instances authors developed several joint matrix factorization approaches combine different views data uniﬁed optimization framework authors proposed uniﬁed spectral embedding original data integrating information different views. similarly clustering algorithms based canonical correlation analysis ﬁrst project data different views uniﬁed dimensional subspace apply simple algorithms like single linkage kmeans achieve ﬁnal clustering third unlike previous methods uniﬁed representation applying learning techniques another strategy literature integrate information individual graphs directly optimization problems learning purposes. examples include co-em clustering algorithm proposed clustering approaches proposed based frameworks co-training co-regularization fourth particularly analysis multiple graphs regularization frameworks graphs also applied. authors presented regularization framework edge weights multiple graphs compute improved similarity graph vertices authors proposed graph regularization frameworks vertex graph spectral domain combine individual graph layers. finally representative approaches include works authors deﬁned additional graph representations incorporate information original individual graphs works authors proposed ensemble clustering approaches integrating clustering results individual views. perspective proposed approach belongs second category mentioned above ﬁrst representative subspace information provided multi-layer graph implement clustering step learning tasks. believe type approaches intuitive easily understandable still ﬂexible generic enough applied different types data. summarize main differences related work contributions proposed paper following. first research work grassmann manifold theory mainly focused subspace analysis. subspace usually comes directly data linked graph-based learning problems. paper makes explicit link subspaces graphs presents fundamental intuitive approaching learning problems multi-layer graphs help subspace analysis grassmann manifold. second show link projection distance grassmann manifold empirical estimate hilbert-schmidt independence criterion therefore together results able offer uniﬁed view concepts three different perspectives namely projection distance grassmann manifold kullback-leibler divergence hsic helps understand better concept distance measure subspace analysis. finally using novel layer merging framework provide simple competitive solution problem clustering multilayer graphs. also discuss inﬂuence relationships individual graph layers performance proposed clustering algorithm. believe helpful towards design efﬁcient adaptive learning algorithms. section describe subspace representation information provided single graph. subspace representation inspired spectral clustering studies spectral properties graph information partitioning vertex graph several distinct subsets. consider weighted undirected graph {vi}n represents vertex represents edge associated edge weights respectively. without loss generality assume graph connected. adjacency matrix graph symmetric matrix whose entry represents edge weight edge vertex otherwise. degree vertex deﬁned weights edges incident graph degree matrix deﬁned diagonal matrix containing degrees vertex along diagonal. normalized graph laplacian matrix deﬁned graph laplacian broad interests studies spectral graph theory among several variants normalized graph laplacian deﬁned since spectrum always property favorable comparing different graph layers following sections. consider problem clustering vertices {vi}n distinct subsets vertices subset similar i.e. connected edges large weights. problem efﬁciently solved spectral clustering algorithms. speciﬁcally focus algorithm proposed solves following trace minimization problem s.t. number vertices graph target number clusters denotes matrix transpose operator. shown version rayleigh-ritz theorem solution problem contains ﬁrst eigenvectors columns. clustering vertices achieved applying k-means algorithm normalized vectors matrix shown behavior spectral clustering explained theoretically analogies several wellknown mathematical problems normalized graphcut problem random walk process graphs provide illustrative example spectral clustering algorithm. consider single graph fig. vertices belong three distinct clusters sake simplicity edge weights dimensional matrix solves problem contains orthonormal eigenvectors graph laplacian columns shown fig. matrix usually called spectral embedding vertices viewed coordinates corresponding vertex k-dimensional space. importantly properties graph laplacian matrix embedding preserves connectivity vertices original graph. words vertices strongly connected graph mapped vectors close k-dimensional space. result simple k-means algorithm applied normalized vectors achieve ﬁnal clustering vertices. inspired spectral clustering theory deﬁne meaningful subspace representation original vertices graph k-dimensional spectral embedding driven matrix built ﬁrst eigenvectors graph laplacian coordinates corresponding vertex dimensional subspace representation contains information connectivity vertices original graph. information used ﬁnding clusters vertices shown above also useful analysis tasks graphs. adopting subspace representation summarizes graph information multiple graph layers naturally represented multiple subspaces task multi-layer graph analysis transformed problem effective combination multiple subspaces. focus next section. fig. illustration spectral clustering. graph three clusters vertices; spectral embedding vertices computed graph laplacian matrix. vertices cluster mapped coordinates close described subspace representation graph layer multi-layer graph. discuss problem effectively combining multiple graph layers merging multiple subspaces. theory grassmann manifold provides natural framework problem. section ﬁrst review main ingredients grassmann manifold theory move onto generic framework merging subspaces. deﬁnition grassmann manifold k-dimensional linear subspaces unique subspace mapped unique point manifold. example fig. shows -dimensional subspaces mapped points advantage using tools grassmann manifold theory thus twofold provides natural representation problem subspaces representing individual graph layers considered different points grassmann manifold; analysis grassmann manifold permits efﬁcient tools study distances points manifold namely distances different subspaces. distances play important role problem merging information multiple graph layers. follows focus deﬁnition particular distance measure subspaces used framework later mathematically speaking point represented orthonormal matrix rn×k whose columns span corresponding k-dimensional subspace thus denoted span. example subspaces shown fig. denoted span span orthonormal matrices distance points manifold subspaces span span deﬁned based principal angles {θi}k subspaces principal angles measure subspaces geometrically close fundamental measures used deﬁne various distances grassmann manifold necessity normalization discussed omit discussion here. however normalization change nature spectral embedding hence affect derivation later. summarize grassmann manifold provides natural intuitive representation subspace-based analysis associated tools namely principal angles permit deﬁne meaningful distance measure captures geometric relationships subspaces. originally deﬁned distance measure subspaces projection distance naturally generalized analysis multiple subspaces show next section. equipped subspace representation individual graphs distance measure compare different subspaces ready present generic framework merging information multiple graph layers. given multi-layer graph individual layers {gi}m ﬁrst compute graph laplacian matrix represent spectral embedding matrix rn×k ﬁrst eigenvectors number vertices target number clusters. recall matrices {ui}m deﬁnes k-dimensional subspace denoted span. goal merge multiple subspaces meaningful efﬁcient way. philosophy representative subspace span close individual subspaces span time representation preserves vertex connectivity graph layer. notational convenience rest paper simply refer representations corresponding subspaces unless indicated speciﬁcally. squared projection distance subspaces deﬁned naturally generalized analysis multiple subspaces. speciﬁcally deﬁne squared projection distance target representative subspace individual subspaces {ui}m squared projection distances individual subspace given orthonormal matrices representing subspaces comparison. reason choosing projection distance two-fold projection distance deﬁned -norm vector sines principal angles. since uses principal angles therefore unbiased deﬁnition. favorable assume prior knowledge distribution data principal angles considered carry meaningful information; projection distance interpreted using one-to-one mapping preserves distinctness span rn×n. note squared projection distance rewritten third equality comes deﬁnition principal angles ﬁfth equality uses fact orthonormal matrices. seen projection distance related frobenius norm difference mappings subspaces span span rn×n. mapping preserves distinctness natural take projection distance proper distance measure subspaces. moreover third equality provides explicit computing projection distance subspaces matrix representations minimization distance measure enforces representative subspace close individual subspaces {ui}m terms projection distance grassmann manifold. time want preserve vertex connectivity graph layer. achieved minimizing laplacian quadratic form evaluated columns also indicated objective function spectral clustering. therefore ﬁnally propose merge multiple subspaces solving statistical learning points view. ﬁrst justiﬁcation work hamm authors shown kullback-leibler divergence well-known similarity measure probability distributions information theory closely related squared projection distance. speciﬁcally work suggests that certain conditions consider linear subspace ﬂattened limit factor analyzer distribution stands normal distribution mean rn×k full-rank matrix ambient noise level identity matrix dimension subspaces symmetrized divergence corresponding distributions rewritten form squared projection distance ignore constant factor shows that take probabilistic view subspace representations {ui}m projection distance subspaces considered consistent divergence. second justiﬁcation recently proposed hilbert-schmidt independence criterion measures statistical dependence random variables. given rn×n centered gram matrices kernel functions deﬁned random variables empirical estimate hsic given larger dhsic stronger statistical dependence case using idea spectral embedding consider rows individual subspace representations particular sets sample points drawn probability distributions governed information vertex connectivity respectively. words sets rows seen realizations random variables therefore deﬁne gram matrices linear kernels therefore rayleigh-ritz theorem solution problem given ﬁrst eigenvectors modiﬁed laplacian lmod computed using efﬁcient algorithms eigenvalue problems problem representative subspace multiple subspaces {ui}m representation preserves structural information contained individual graph layers encouraged ﬁrst term objective function also keeps minimum distance multiple subspaces enforced second term. notice minimization ﬁrst term corresponds simple averaging information different graph layers usually leads suboptimal clustering performance shall experimental section. similarly imposing small projection distance individual subspaces {ui}m necessarily guarantee good solution merging subspaces. fact given kdimensional subspace inﬁnitely many choices matrix representation considered meaningful summarizations information provided multiple graph layers. however additional constraint minimizing trace quadratic term uliu graphs vertex connectivity individual graphs tends preserved case smaller projection distance individual subspaces representative graph layers. therefore information-theoretic statistical learning points view smaller projection distance subspace representations similar information respective graphs represent. result representative subspace considered subspace representation summarizes information individual graph layers time captures intrinsic relationships vertices graph. imagine relationships crucial importance multi-layer graph analysis. summary concept treating individual graphs subspaces points grassmann manifold permits study desired merging framework unique principled way. able representative subspace multi-layer graph interest viewed dimensionality reduction approach original data. ﬁnally remark proposed merging framework easily extended take account relative importance individual graph layer respect speciﬁc learning purpose. instance prior knowledge importance information individual graphs available adapt value regularization parameter different layers representative subspace closer informative subspace representations. section introduced novel framework merging subspace representations individual layers multilayer graph leads representative subspace captures intrinsic relationships vertices graph. representative subspace provides dimensional form used several applications involving multi-layer graph analysis. particular study application namely problem clustering vertices multi-layer graph. analyze behavior proposed clustering algorithm respect properties individual graph layers already seen section success spectral clustering algorithm relies transformation information contained graph structure spectral embedding computed graph laplacian matrix embedding matrix treated coordinates corresponding vertex dimensional subspace. problem clustering multi-layer graph setting slightly different since ﬁnding uniﬁed clustering vertices takes account information contained individual layers multi-layer graph. however merging framework proposed previous section naturally applied context. fact leads natural solution clustering problem multi-layer graphs. details similarly spectral embedding matrix spectral cluttering algorithm subspace representation individual graph merging framework provides representative subspace contains information multiple graph layers. using representation follow steps spectral clustering achieve ﬁnal clustering vertices k-means algorithm. proposed clustering algorithm summarized algorithm clear algorithm direct generalization algorithm case multi-layer graphs. main ingredient clustering algorithm merging framework proposed section information individual graph layers summarized prior actual clustering process implemented. provides example illustrates generic merging framework applied speciﬁc learning tasks multi-layer graphs. analyze behavior proposed clustering algorithm different conditions. speciﬁcally ﬁrst outline link subspace distance clustering quality compare clustering performances scenarios relationships individual subspaces {ui}m seen section rows subspace representations {ui}m viewed realizations random variables {xi}m governed graph information. time spectral clustering directly utilizes purpose clustering. therefore {xi}m considered random variables control cluster assignment vertices. fact shown matrix closely related matrix contains cluster indicator vectors columns. since projection distance understood negative statistical dependence random variables minimization projection distance equivalent maximization dependence random variable representative subspace ones individual subspaces {ui}m discuss relationships individual subspaces possibly affect performance clustering algorithm sc-ml. intuitively since second term objective function represents distance representative subspace individual subspaces {ui}m tends drive solution towards subspaces close grassmann manifold. show clearly consider examples. ﬁrst example illustrated fig. -layer graph individual layers sharing vertices. sake simplicity edge weights one. addition three groundtruth clusters indicated colors vertices. table shows performances algorithm individual layers well algorithm multi-layer graph terms normalized mutual information respect groundtruth clusters. table shows projection distances various pairs subspaces. clear layers produce better clustering quality distance corresponding subspaces smaller. however vertex connectivity layer consistent groundtruth clusters corresponding subspace away ones case solution found sc-ml enforced close consistent subspaces hence provides satisfactory clustering results consider second example illustrated fig. example layers relatively quality information respect groundtruth clustering vertices. table corresponding subspaces close grassmann manifold. informative layer however represents subspace quite away ones time table clustering results better ﬁrst layer less informative layers. quality information different layers considered computing representative subspace sc-ml enforces solution closer layers relatively lower quality results unsatisfactory clustering performance case. analysis implies proposed clustering algorithm works well following assumptions majority individual subspaces relatively informative namely helpful recovering groundtruth clustering reasonably close grassmann manifold namely provide complementary contradictory information. assumptions made present work. shall next section assumptions seem appropriate realistic real world datasets. case assume preprocessing step cleans datasets least provides information reliability information different graph layers. section evaluate performance scml algorithm presented section several synthetic real world datasets. ﬁrst describe datasets evaluation explain various clustering algorithms adopt performance comparisons. ﬁnally present results terms three evaluation criteria well discussions. ﬁrst dataset synthetic dataset three point clouds forming english letters point cloud generated ﬁve-component gaussian mixture model different values mean variance gaussian distributions component represents class points speciﬁc color. -nearest neighbor graph constructed point cloud assigning weight edges connecting vertices reciprocal euclidean distance them. gives -layer graph vertices graph layer point cloud forming particular letter. goal dataset recover clusters vertices using three graph layers constructed three point clouds. second dataset contains data collected lausanne data collection campaign nokia research center lausanne. dataset contains mobile phone data users living working lake l´eman region switzerland recorded one-year period. considering users vertices graph construct three graphs measuring proximities users terms locations bluetooth scanning activities phone communication. speciﬁcally locations bluetooth scans measure many times users sufﬁciently close geographically many times users’ devices detected bluetooth devices respectively within minute time windows. aggregating results oneyear period leads weighted adjacency matrices represent physical proximities users measured different modalities. addition adjacency matrix phone communication generated assigning edge weights depending number calls pair users. three adjacency matrices form -layer graph vertices goal recover eight groundtruth clusters constructed users’ email afﬁliations. subset cora bibliographic dataset. dataset contains research papers three different ﬁelds namely natural language processing data mining robotics. considering papers vertices graph construct ﬁrst graphs measuring similarities among title abstract papers. clearly title abstract represent paper vector non-trivial words using term frequency-inverse document frequency weighting scheme compute cosine similarities every pair vectors edge weights graphs. moreover third graph reﬂects citation relationships among papers namely assign edge unit weight papers cited cited visualize graphs three datasets plot adjacency matrices graphs shown fig. synthetic cora dataset respectively orderings vertices made consistent groundtruth clusters. plot global view matrix every non-zero entry matrix represented blue shown ﬁgures clearly clusters synthetic cora datasets clusters dataset clear. reason that dataset email afﬁliations used create groundtruth clusters provides approximative information. explain brieﬂy clustering algorithms comparative performance analysis along implementation details. adopt three baseline algorithms well state-of-the-art technique namely co-regularization approach introduced shall interesting connection approach proposed algorithm. first describe implementation details proposed sc-ml algorithm co-regularization approach sc-ml spectral clustering multi-layer graphs presented section implementation scml pretty straightforward parameter choose regularization parameter experiments choose value multiple empirical trials report best clustering performance. speciﬁcally choose synthetic dataset real world datasets. discuss choice parameter later section. number vertices number eigenvectors used deﬁnition spectral kernels represents m-th eigenvector laplacian graph make comparable spectral clustering choose target number clusters experiments. alternating optimization scheme sc-cor. stopping criteria optimization process chosen optimization stops changes objective function smaller similarly choose value regularization parameter sccor multiple empirical trials report best clustering performance. parameter ﬁxed optimization steps graph layers. evaluate performance different clustering algorithms three different criteria namely purity normalized mutual information rand index results summarized table synthetic cora dataset respectively. scenario best results highlighted bold fonts. first expected clustering performances synthetic cora datasets higher dataset indicates latter indeed challenging approximative groundtruth information. second clear sc-ml sc-cor generally outperform baseline approaches three datasets. speciﬁcally although sc-sum scksum indeed improve clustering quality compared clustering individual graph layers provide limited improvement potential drawback summation methods considered similar building simple average graph representing different layers information. therefore depending data characteristics speciﬁc datasets might smooth particular information provided individual layers thus penalize clustering performance. comparison scml sc-cor always achieve signiﬁcant improvements clustering quality compared clustering using individual graph layers. take closer look comparisons sc-ml sc-cor. although latter developed viewpoint subspace analysis grassmann manifold actually interpreted process individual subspace representations updated based distance analysis framework. sense sc-cor uses distance measure similarities subspaces. merging solution however leads different optimization problem based slightly different merging philosophy. speciﬁcally enforces information contained individual subspace representations consistent other. alternating optimization scheme optimizes step subspace representation ﬁxing others. interpreted process subspace step becomes closer subspaces term projection distance grassmann manifold. upon convergence initial subspaces brought closer ﬁnal subspace representation informative graph layer considered combines information graph layers efﬁciently. illustrations sc-cor sc-ml shown fig. respectively. therefore hand results approaches demonstrate beneﬁt using distance analysis grassmann manifold merging information multi-layer graphs. indeed approaches since distances solutions individual subspaces minimized without sacriﬁcing much information individual graph layers resulting combinations considered good summarizations multiple graph layers. hand however sc-ml differs sccor mainly following aspects. first alternating optimization scheme sc-cor focuses optimizing subspace representation step requires sensible initialization guarantee algorithm ends good local minimum optimization problem; also guarantee subspace representations converge point grassmann manifold contrast sc-ml directly ﬁnds single representation unique optimization representative subspace respect graph layers jointly need alternating optimization steps careful initializations. possible reasons explain sc-ml performs better sc-cor experiments table iii. second worth noting that computational point view optimization process involved sc-ml much simpler sc-cor. speciﬁcally iterative nature sc-cor requires solving eigenvalue problem fig. illustrations graph layer merging. co-regularization iterative update individual subspace representations. upper index represents number iterative steps individual subspace representation. ﬁnal update subspace representation informative graph shown star) considered good combination; proposed merging framework representative subspace found step. times number individual graphs number iterations needed algorithm converge respectively. contrast since sc-ml aims ﬁnding globally representative subspace without modifying individual ones needs solve eigenvalue problem once. finally discuss inﬂuence choice regularization parameter performance sc-ml. fig. compare performances sc-ml sccor terms different values parameter corresponding implementations. experiments sc-ml achieves best performances chosen outperforms sc-cor large range synthetic datasets. cora dataset algorithms achieve performance different values sc-ml permits larger range parameter selection. furthermore worth noting optimal values sc-ml similar ranges across different datasets thanks adoption normalized graph laplacian matrix whose spectral norm upper bounded summary shows performance sc-ml reasonably stable respect parameter selection. paper provide framework analyzing information provided multi-layer graphs clustering vertices graphs rich datasets. generic approach based transformation information contained individual graph layers subspaces grassmann manifold. estimation representative subspace essentially considered problem ﬁnding good authors also proposed centroid-based co-regularization approach introduces consensus representation. however representation still computed alternating optimization scheme needs sensible initialization keeps iterative nature. summarization multiple subspaces using distance analysis grassmann manifold. proposed approach applied various learning tasks multiple subspace representations involved. appropriate realistic assumptions show framework applied clustering problem multi-layer graphs provides efﬁcient solution competitive state-ofthe-art techniques. finally mention following research directions interesting open problems. first subspace representation inspired spectral clustering valid representation graph information. suggested works eigenvectors modularity matrix graph also used dimensional subspace representation information contained graph. therefore interesting problem appropriate subspace representation data available either graphs general forms. second believe better clustering performance achieved prior information data available particular consistency information different graph layers. problems however left future studies. kirby sirovich application karhunen-loeve procedure characterization human faces ieee transactions pattern analysis machine intelligence vol. turaga veeraraghavan chellappa statistical analysis stiefel grassmann manifolds applications computer vision ieee conference computer vision pattern recognition harandi sanderson shirazi lovell graph embedding discriminant analysis grassmannian manifolds improved image matching ieee conference computer vision pattern recognition huang kamangar heterogeneous image feature integration multi-modal spectral clustering ieee conference computer vision pattern recognition dong frossard vandergheynst nefedov regularization framework mobile social network analysis ieee international conference acoustics speech signal processing bruno marchand-maillet multiview clustering late fusion approach using latent models sigir conference research development information retrieval greene cunningham matrix factorization approach integrating multiple data views european conference machine learning knowledge discovery databases kiukkonen blom dousse gatica-perez laurila towards rich mobile phone datasets lausanne data collection campaign international conference pervasive services", "year": 2013}