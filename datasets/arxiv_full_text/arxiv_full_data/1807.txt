{"title": "Word Embeddings via Tensor Factorization", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "Most popular word embedding techniques involve implicit or explicit factorization of a word co-occurrence based matrix into low rank factors. In this paper, we aim to generalize this trend by using numerical methods to factor higher-order word co-occurrence based arrays, or \\textit{tensors}. We present four word embeddings using tensor factorization and analyze their advantages and disadvantages. One of our main contributions is a novel joint symmetric tensor factorization technique related to the idea of coupled tensor factorization. We show that embeddings based on tensor factorization can be used to discern the various meanings of polysemous words without being explicitly trained to do so, and motivate the intuition behind why this works in a way that doesn't with existing methods. We also modify an existing word embedding evaluation metric known as Outlier Detection [Camacho-Collados and Navigli, 2016] to evaluate the quality of the order-$N$ relations that a word embedding captures, and show that tensor-based methods outperform existing matrix-based methods at this task. Experimentally, we show that all of our word embeddings either outperform or are competitive with state-of-the-art baselines commonly used today on a variety of recent datasets. Suggested applications of tensor factorization-based word embeddings are given, and all source code and pre-trained vectors are publicly available online.", "text": "demonstrates additive compositionality wordvec vectors vectors produced embedding compute vectors certain phrases rather vectors words. later paper show embeddings naturally give rise form multiplicative compositionality explored literature. almost recent word embeddings rely distributional hypothesis states word’s meaning inferred words tend surround utilize distributional hypothesis many embeddings given low-rank factor matrix derived co-occurrences large unsupervised corpus approaches rely matrix factorization utilize pairwise co-occurrence information corpus. extend approach creating word embeddings given factors tensors containing higher order co-occurrence data. related work common word embeddings related co-occurrence based matrix factorization include glove wordvec lexvec nnse contrast work studies word embeddings given factorization tensors. overview tensor factorization methods given work uses factorization symmetric nonnegative tensors studied past general factorization tensors applied factorization nonnegative tensors recently factorization symmetric tensors used create generic word embedding idea explored extensively. work studies idea much greater detail fully demonstrating viability tensor factorization technique training word embeddings. many state-of-the-art word embedding techniques involve factorization co-occurrence based matrix. extend approach studying word embedding techniques involve factorization co-occurrence based tensors present word embedding techniques based tensor factorization show outperform common methods several semantic tasks given data. train embeddings present joint tensor factorization problem approach solving furthermore modify performance metrics outlier detection task measure quality higher-order relationships word embedding captures. tensor-based methods signiﬁcantly outperform existing methods task using metric. finally demonstrate vectors embeddings composed multiplicatively create different vector representations meaning polysemous word. show property stems higher order information vectors contain thus unique tensor based embeddings. word embeddings used improve performance many tasks including language modelling machine translation sentiment analysis broad applicability word embeddings implies improvements quality likely widespread beneﬁts ﬁeld. word embedding problem learn mapping encodes meaningful semantic and/or syntactic information. instance many word embeddings since words semantically similar. complex relationships similarity also encoded word embeddings. example answer analogy queries form using simple arithmetic many state-of-the-art embeddings answer sleep chair given word whose vector representation closest embeddings encode information nonlinear plicative tensor-based methods typically composition used create vectors represent phrases sentences. work instead shows pairs word vectors composed multiplicatively create different vector representations various meanings single polysemous word. notation throughout paper write scalars lowercase italics vectors lowercase bold letters matrices uppercase bold letters tensors euler script notation standard literature. since negative values little grounded interpretation given indexed vocabulary construct ppmi matrix many existing word embedding techniques involve factorizing ppmi matrix paper study -way ppmi tensors mijk natural higherorder generalization ppmi matrix. leave study creating word embeddings n-dimensional ppmi tensors future work. elementwise vector multiplication later section multiplicative compositionality formulation gives rise meaningful interpretation elementwise product between vectors word embeddings. symmetric decomposition. paper consider symmetric decomposition nonnegative tensors since n-way ppmi nonnegative invariant permutation ppmi tensor nonnegative supersymmetric i.e. mijk mσσσ permutation known optimal rank-k decomposition exists symmetric nonnegative tensors ppmi tensor however ﬁnding decomposition hard general must consider approximate methods. computing symmetric decomposition size third order ppmi tensor presents number computational challenges. practice vary resulting tensor whose naive representation requires least bytes ﬂoats. even sparse representation tensor takes large fraction memory standard algorithms successive rank- approximation alternating least-squares infeasible uses. thus paper consider stochastic online formulation similar problem speciﬁc instance coupled tensor decomposition studied past problem goal factorize multiple tensors using least factor matrix common. similar formulation problem found studies blind source separation using algebraic geometric aspects jointly factorizing numerous supersymmetric tensors contrast work outline generic rank properties decomposition rather attacking problem numerically. also formulation rank ﬁxed approximate solution must found. exploring connection theoretical aspects joint decomposition quality word embeddings would interesting avenue future work. shifted considers factorization positive shifted matrices consider factorization positive shifted tensors mijk constant shift empirically found different levels shifting resulted different qualities word embeddings best shift found cp-s shift whereas nonzero shift jcp-s resulted worse embedding across board. discuss evaluation report results given factorization ppmi tensors shifted best value found speciﬁc embedding. computational notes considering going dimensions three perhaps necessary discuss computational issues problem size increase. however noted creation pre-trained embeddings seen pre-processing step many future tasks training completed once used forever therewithout take training time account. despite this found training embeddings considerably slower training order- equivalents sgns. explicitly trained cbow vectors seconds whereas training cp-s jcp-s took seconds respectively. section present quantitative evaluation comparing embeddings informationless embedding strong baselines. baselines random comparing model meaningful information encoded wordvec comparison commonly used embedding method well comparison technique related ppmi matrix factorization optimize decomposition online fashion using small random subsets nonzero tensor entries update decomposition time minibatch setting optimize decomposition based current minibatch previous decomposition time update ﬁrst deﬁne decomposition loss minimize loss respect using adam time take co-occurrence triples ﬁxed number sentences corpus. continue training depleted entire corpus. accurately model also include certain proportion elements zero ppmi similar empirically found proportion negative samples training leave discovery optimal negative sample proportion future work. word embedding proposals cp-s. ﬁrst embedding propose based symmetic decomposition ppmi tensor discussed mathematical preliminaries section. optimal setting word embedding jcp-s. potential problem cp-s trained third order information. rectify issue propose novel joint tensor factorization problem call joint symmetric rank-r decomposition. problem input ﬁxed rank list supersymmetric tensors different orders whose axis lengths equal tensor factorized rank-r symmetric decomposition using single factor matrix purpose evaluate extent embedding captures order relationships words. results quantitative experiments tensor methods outperform baselines validates approach. simple supervised tasks. points primary application word embeddings transfer learning tasks. argue evaluate embedding’s ability transfer information relevant task must measure embedding’s accessibility information actual downstream tasks. must cite performance simple supervised tasks training size increases commonly done transfer learning evaluation algorithm using word embedding performs well small amount training data information encoded embedding easily accessible. embeddings follows supervised analogy recovery. consider task solving queries form using simple neural network suggested analogy dataset google analogy testbed word similarity. standardize evaluation methodology evaluate embeddings using word similarity common mturk datasets overview word similarity evaluation quantitative results outlier detection results. results shown table ﬁrst thing note cp-s outperforms methods across outlier detection metric. since wikisem dataset semantically focused performance nnse comparison technique relies explicit ppmi matrix factorization fair comparison trained model corpus million sentences gathered wikipedia. removed stopwords words appearing fewer times reduce noise uninformative words. wordvec nnse baselines trained using recommended hyperparameters original publications optimizers using using default settings. hyperparameters always consistent across evaluations. dataset size results shown considered proof concept rather objective comparison state-of-the-art pre-trained embeddings. natural computational challenges arising working tensors leave creation full-scale production ready embedding based tensor factorization future work. quantitative tasks outlier detection. outlier detection task determine word list words unrelated chosen related. compute compactness score compactness {w}. explicitly computed mean similarity word pairs {w}. predicted outlier argmaxw∈lc related words form compact cluster high mean similarity. wikisem dataset includes sets words group gathered based semantic similarity. thus performance task correlated amount semantic information encoded word embedding. performance dataset shown well-correlated performance common task sentiment analysis metrics associated task accuracy outlier position percentage accuracy fraction cases true outlier correctly highest compactness score. measures close true outlier highest compactness score rewarding embeddings predicting outlier place rather sorting words compactness score -way outlier detection. tensor-based embeddings encode higher order relationships words introduce compute based groups input nnse matrix words co-occurrence patterns. experiments co-occurrence information number times appears within window words stated paper matrix entries weighted ppmi. baselines perform competitively decomposition based models considered methods clearly excel. since tensor-based methods trained directly third order information perform much better scores reﬂect amount third order information word embedding. validation order embeddings would naturally perform order embeddings task requires third order information. still superiority tensor-based embeddings demonstrates quality semantic information encode. supervised analogy results. results shown figure supervised semantic analogy task cp-s vastly outperforms baselines levels training data signifying amount semantic information encoded embedding technique. also training data presented tensor methods ones attain nonzero performance even limited data setting cp-s’s vectors results nearly accuracy. phenomenon also observed syntactic analogy tasks embeddings consistently outperform others training data presented. observations demonstrate accessibility information encoded word embeddings. thus conclude relational information encoded tensor-based embeddings easily accessible cbow nnse. thus methods would likely better suited transfer learning actual tasks particularly data-sparse settings. sentiment analysis results. results shown figure task jcp-s dominant method across levels training data difference obvious training data limited. indicates speciﬁc task information encoded tensorbased methods readily available baselines. thus evident exploiting second third order co-occurrence data leads higher quality semantic information encoded embedding. point clear jcp-s vastly outperforms cp-s task superiority strong baselines demonstrates quality information encoded jcp-s. discrepancy also illustrative fact single best word embedding different embeddings encode different types information thus used shine rather every task. beddings clearly outperform random embedding task. even outperform cbow datasets. worth including results word similarity task common evaluating embedding quality literature. however many intrinsic problems evaluating word embeddings using word similarity discuss further. even though explicitly trained tensor-based embeddings capture polysemy information naturally multiplicative compositionality. demonstrate property qualitatively provide proper motivation leaving automated utilization future work. tensor-based embeddings found create vector represents word context another word taking elementwise product vw∗vw. call meaning vector polysemous word example consider word star denote lead performer celestial body. create vector star lead performer sense taking elementwise product vstar vactor. produces vector lies near vectors words related lead performers related star’s senses. example even though galaxy likely appear context word star celestial body sense vstar vactor vgalaxy ppmi whereas vstar vactor vdrama ppmi high. thus vstar vactor represents meaning star lead performer sense. table present nearest neighbors multiplicative additive composed vectors variety polysemous words. words corresponding nearest neighbors composed vectors tensor methods semantically related intended sense multiplicative additive composition. contrast cbow additive composition yields vectors whose nearest neighbors semantically related intended sense. thus embeddings produce complementary sets polysemous word representations qualitatively valid whereas cbow guarantees meaningful additive compositionality. leave automated usage property future work. conclusion contributions follows novel tensor factorization based word embeddings. presented cp-s jcp-s word embedding techniques based symmetric decomposition. experimentally demonstrated embeddings outperform existing matrix-based techniques number downstream semantic tasks trained data. introduced utilized joint symmetric rank-r decomposition train jcp-s. problem multiple supersymmetric tensors must decomposed using single rank-r factor matrix. technique allows utilization second third order co-occurrence information word embedding training. embedding evaluation metric measure amount third order information. produce analogue outlier detection call metric evaluates degree third order information captured given word embedding. demonstrated showing tensor based techniques naturally encode third information perform better compared existing second order models. word vector multiplicative compositionality polysemous word representation. showed word vectors meaningfully composed multiplicatively create meaning vector different sense polysemous word. property consequence higher order information used train embeddings empirically shown unique tensorbased embeddings. tensor factorization appears highly applicable effective tool learning word embeddings many areas potential future work. leveraging higher order data training word embeddings useful encoding types information semantic relationships compared models trained using pairwise data. indicates techniques prove useful training word embeddings used downstream tasks.", "year": 2017}