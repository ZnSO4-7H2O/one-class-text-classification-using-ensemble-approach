{"title": "ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search", "tag": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "Learning to walk over a graph towards a target node for a given input query and a source node is an important problem in applications such as knowledge graph reasoning. It can be formulated as a reinforcement learning (RL) problem that has a known state transition model, but with partial observability and sparse reward. To overcome these challenges, we develop a graph walking agent called ReinforceWalk, which consists of a deep recurrent neural network (RNN) and a Monte Carlo Tree Search (MCTS). To address partial observability, the RNN encodes the history of observations and map it into the Q-value, the policy and the state value. In order to effectively train the agent from sparse reward, we combine MCTS with the RNN policy to generate trajectories with more positive rewards. From these trajectories, we update the network in an off-policy manner using Q-learning and improves the RNN policy. Our proposed RL algorithm repeatedly applies this policy improvement step to learn the entire model. At testing stage, the MCTS is also combined with the RNN to predict the target node with higher accuracy. Experiment results on several graph-walking benchmarks show that we are able to learn better policies from less number of rollouts compared to other baseline methods, which are mainly based on policy gradient method.", "text": "learning walk graph towards target node given input query source node important problem applications knowledge graph reasoning. formulated reinforcement learning problem known state transition model partial observability sparse reward. overcome challenges develop graph walking agent called reinforcewalk consists deep recurrent neural network monte carlo tree search address partial observability encodes history observations q-value policy state value. order effectively train agent sparse reward combine mcts policy generate trajectories positive rewards. trajectories update network off-policy manner using q-learning improves policy. proposed algorithm repeatedly applies policy improvement step learn entire model. testing stage mcts also combined predict target node higher accuracy. experiment results several graph-walking benchmarks show able learn better policies less number rollouts compared baseline methods mainly based policy gradient method. consider problem learning walk graph order target node given pair source node query problem appears example knowledge graph completion want missing target entity knowledge graph given head entity relation query problem could understood using graph source node query inputs predict target node want construct function predict however functional form generally unknown learned training dataset consists collection samples form reason problem could solved conventional search algorithms a∗-search seeks paths given source target nodes. instead search agent needs learn search policy given training dataset unseen pair query source node could walk graph correct target node. however since training sample form intermediate supervision correct search path. instead receives delayed evaluative feedbacks agent correctly predicts target node agent receive positive reward. therefore agent trained reinforcement learning instead supervised learning. major challenges problem problem partially observable usually requires entire history observations make correct decision reward sparse appears search path. interestingly formulation problem another useful property environment transition model known deterministic. important knowledge exploited develop effective learning prediction algorithm problem. motivated observations develop deep neural network architecture graph-walking agent named reinforcewalk. reinforcewalk agent consists deep structured recurrent neural network monte carlo tree search addresses partial observability encoding history observations maps q-function policy value function i.e. jointly models q-network policy network value network. integrate mcts training testing stages order exploit available model-free model-based information. important contribution work combine mcts develop reinforcement learning algorithm effectively learns policy sparse rewards. exploiting knowledge environment transition model mcts able generate trajectories signiﬁcantly positive rewards using policy network alone. trajectories update improve policy off-policy manner using q-learning. sharp contrast many existing methods solving graph walk problem policy gradient method. policy gradient method requires large amount rollouts obtain trajectory positive reward especially early stage learning. consequence approach able learn better policies less rollouts demonstrated experiments several benchmarks including synthetic task real knowledge base completion task. addition reinforcewalk agent also combines learned networks mcts testing stage predict target node accurately. rest paper organized below. section gives formal statement problem concrete example. section develops reinforcewalk agent including model architecture training testing algorithms. experimental results presented section finally discuss related works section conclude paper section graph walk problem consider graph consists nodes {ni} edges {eij} connect nodes. denote input query. objective problem given pair query source node ﬁnding desired target node example knowledge base completion tasks given knowledge graph nodes different entities edges relations connected nodes objective task predict tail entity given head entity target relation example figure given head entity obama query citizenship figure state transition denote state. activated circles edges denote observed information time step. double circle denotes current node denote edges nodes connected current node. state consists entire history observations including input query problem also understood predicting using inputs i.e. want construct function predict speciﬁcally want develop agent walk sequence nodes graph source node towards target node model prediction function however problem could solved conventional search algorithms a∗-search actual functional form unknown. instead learned training examples form objective that training done agent knows walk graph reach correct target node unseen pair important property problem partially observable. generally requires entire history observations make correct decision. example example figure access current node hawaii alone sufﬁcient know best action move usa. instead requires agent track entire history including input query citizenship reach decision. next section describe train agent walk graph. training sample presented agent speciﬁes input desired target node provides agent intermediate supervision correctly take action path target node. instead evaluative feedbacks provided action sequence leads correct target node agent receives positive reward. therefore problem solved reinforcement learning instead supervised learning. deﬁne markov decision process graph walk problem states actions reward function state transition probability. denote state time partial observability deﬁne collection observed information time augmentation previous state newly observed information nsens nns}. newly observed information include action taken previous time step current node time denote edges nodes connected respectively. figure illustrates transition states observations. time agent takes action based either choosing edges moving next node terminating walking graph outputting denotes prediction target node therefore feasible actions time {stop} entire action space ∪tat. note feasible action timevarying. furthermore figure also shows action taken next node associated ent+ nnt+ also determined. means next state also determined. therefore recursion deﬁnes deterministic state transition known agent. exploit important knowledge state transition training prediction algorithms sections .–.. finally discussed earlier reward zero otherwise. reward sparse appears path. section introduce deep neural network architecture reinforcewalk agent. consists deep recurrent neural network monte carlo tree search jointly models q-network policy network value network encoding entire history observations three outputs. q-network models long-term reward taking action state policy network deﬁnes probability taking action current state value network models long-term reward state variable denotes collection model parameters. mcts uses knowledge transition model form search tree. describe architecture defer discussion mcts next subsections. figure deep structured architecture reinforcewalk. gated recurrent units encode vector deep feedforward neural networks encode along quantities high-level embedding vectors hat} form hidden representa{hst tion state finally mapped q-value policy state value different output units. note vectors hatt gru-rnn compute qt+. therefore parts together become deep structured model jointly. parts shown figures respectively. since state collection observations time partial observability gated recurrent units encode vector encoded high-level representation vector deep neural network another encode edge entn associated vector hnt. max-pooling operator applied coordinate vectors different obtain ﬁxed-length representation vector hat. vectors together form representation state mapped different output units according softmax vector input arguments. pairwise inner product operation allows handle time-varying feasible action choose sigmoid units output q-function reward value considered problem always appears path. chosen forms applications. note computed applying different nonlinearities allow automatically update policy off-policy manner updating using q-learning finally gru-rnn encodes given training algorithm collect model parameters learned. discuss learn model parameters training data set. standard approach policy gradient method updates using stochastic gradient ascent learning rate samples following policy gradient expectation respect current policy environment transition probability policy gradient method uses current policy roll multiple trajectories compute gradient according expectation replaced empirical averaging trajectories. however policy gradient method generally sample efﬁciency especially reward signal sparse problem reward appears trajectory. furthermore policy gradient usually requires large amount monte carlo rollouts order obtain trajectory positive reward especially early stage learning. overcome challenges fact state transition model known combine policy network mcts generate trajectories positive rewards using trajectories improve policy proposed reinforcement learning algorithm repeatedly applies policy improvement step. speciﬁcally mcts simulation roll trajectory according variant policy network deﬁned section quantities stored mcts edge mcts simulation using value network running multiple mcts simulations generate trajectories. since mcts combines model-based knowledge model-free information mcts trajectories could viewed generated improved policy able learn trajectories improve however since trajectories generated policy different off-policy data could policy gradient method update on-policy nature. reason instead update q-network trajectories using q-learning applies off-policy learning ∇θqθ× note learn value network terminal state. evaluate value terminal states mcts. gradient gradient calculated back propagation deep recurrent neural network. testing stage learned policy value networks mcts generate mcts search tree training stage. generate prediction target node given need examine leaf states mcts search tree. note mcts leaf state associated candidate node graph. among them need choose likely true target node. however different mcts leaf states correspond node number times mcts edge visited total number mcts simulations. summation leaf states correspond node score gives average value candidate node pick predicted target node evaluate approach synthetic three glass puzzle knowledge graph competition task examine effectiveness approach. appendix also provide additional experiment text generation. three glass puzzle ﬁrst study proposed reinforcewalk algorithm synthetic three glass puzzle dataset. problem studied math puzzles graph theory speciﬁcally three milk containers denoted containers respectively respective capacities liters. none containers markings measure amount remaining liquid except total capacity. time step take three feasible actions container empty liquid pour liquid another container. objective problem given target volume liters take sequence actions three containers least liters. using graph walk formulation section node represents amounts liquid remaining containers respectively edge represents three feasible actions could taken. furthermore desired volume input query. task succeeds containers reaches desired volume training examples form denotes starting volumes three containers. figure gives concrete example action sequence shown reach desired volume generate unique three glass puzzle synthetic dataset samples used training rest samples used testing. sample agent starts initial state takes action move next state agent reaches maximum number actions agent takes action stop. ﬁnal state contains target volume reward zero otherwise. policy gradient advantage actor-critic baselines. task success rate evaluation metric. knowledge graph link prediction knowledge graph link prediction task goal target relation given initial entity query relation. following setup xiong nell- dataset evaluate knowledge graph completion task. dataset collected iteration nell system. nell- dataset contains triples unique entities unique relations. triple append dataset connect tail entity head entity reverse relation study relation task xiong independently. relation task remove triples knowledge graph. split removed triples training testing samples. previous proposed algorithms transe transr deeppath baselines. detailed hyperparameters described appendix a... evaluation model needs rank candidate entities. mean average precision scores query relation evaluation metric. ﬁrst evaluate performance reinforcewalk algorithm tasks compare baseline methods. figure show learning curves test accuracy reinforcewalk methods figure show learning curves test knowledge base completion task. result reinforcewalk learns better policies faster baseline methods. comprehensive comparison report ﬁnal test scores reinforcewalk knowledge base task table compare rl-based methods policy gradient advantage actor-critic analyze reinforcewalk algorithm using different experiment results understand contributions different components reinforcewalk. first figures show training success rate three glass puzzle task knowledge base completion task. compared policy gradient method advantage actor-critic methods reinforcewalk mcts able generate trajectories positive rewards continues improve training progresses. conﬁrms motivation using mcts generate higher-quality trajectories alleviate sparse reward steps maximum steps reach target node. reinforcewalk mcts algorithm able target node efﬁciently compared dfs. finally table present several examples knowledge graph traversal paths. examples three glass puzzle found appendix second understand importance mcts testing compare test accuracy across different algorithms different beam search sizes different mcts rollouts testing. number mcts simulations training reinforcewalk ﬁxed repeat experiments three times different random seeds report experiments results table observe reinforcewalk mcts achieves best test accuracy overall. addition larger beam search sizes mcts rollouts test accuracy improves substantially. furthermore replacing mcts reinforcewalk beam search test time degrades performance greatly therefore mcts also important reinforcewalk test time. mentioned earlier conventional graph traverse algorithms breadth-first search depthfirst search cannot applied graph walking problem know ground truth target node testing time. however understand quickly reinforcewalk mcts could correct target node compare following cheating setup dfs. speciﬁcally apply test three glass puzzle task disclosing target node them. table report average traversal recently deep reinforcement learning achieved great success solving many artiﬁcial intelligence related problems achieving human-level performance playing atari games deep neural networks reinforcement learning allows agent automatically learn policies data end-to-end manner. work also moves along direction modeling policy value functions using deep neural networks learned end-to-end manner observations. important nature graph walking problem partial observability. using recurrent neural networks encode history observations partially observable problems done hausknecht stone wierstra different works developed recurrent neural network architecture handle time-varying feasible action set. pairwise inner products model q-network also appeared playing text games consider partial observability problem. full body literatures combines model-based model-free silver weber .among them relevant ones silver combine mcts policy value networks achieve superhuman performance computer different work policy network value network alphago trained separately without help mcts used help mcts trained. alphago zero uses policy iteration method combines policy network value network mcts training; time step mcts search executed outputs probabilities playing move improved policy original policy network. probabilities moves mcts table examples paths found reinforcewalk nell- dataset. reinforcewalk learn traverse multiple paths reach target node learn explore paths reach dead used improve policy network. perspective work shares similar spirit also mcts policy network iteratively improve other. however method improves policy network mcts probabilities move method improves policy trajectories generated mcts. note mcts probabilities move silver constructed visit counts edges connected mcts root node meaning uses information near root node improve policy network. different silver improve policy network learning trajectories generated mcts. therefore information entire mcts search tree used. knowledge base reasoning task early work focused learning vector representations entities relations. recent approaches demonstrated limitations prior approaches using vector space models alone vector-space models suffer cascading error dealing compositional relationships hence recent work proposed various approaches injecting multistep paths random walk sequences triples training improving performance knowledge base reasoning tasks. neural explore multi-step relations using controller attention external memory. compared rl-based approaches traversal paths might hard interpret external memory models computationally expensive access entire graph memory recent work deeppath minerva rl-based approaches explore paths knowledge graphs. deeppath requires target entity information state representation uses gathered paths path ranking algorithm minerva uses policy gradient method explore paths training test. proposed model exploits state transition information integrating mcts algorithm. empirically proposed algorithm outperforms deeppath minerva knowledge base completion benchmark. developed agent called reinforcewalk learns walk graph towards desired target node given pair input query source node. speciﬁcally developed deep structured recurrent neural architecture model q-network policy network value network combined together monte carlo tree search search target node. architecture effectively addresses partial observability problem exploits knowledge environment transition model. furthermore propose reinforcement learning algorithm effectively learns sparse reward alternates mcts trajectory generation step policy improvement step. experiment results several graph-walking benchmarks demonstrate method could learn better policies less number rollouts compared baseline methods mainly based policy gradient method. references bordes antoine usunier nicolas garcia-duran alberto weston jason yakhnenko oksana. translating embeddings modeling multi-relational data. advances neural information processing systems carlson andrew betteridge justin kisiel bryan settles burr hruschka estevam mitchell toward architecture never-ending language learning. proceedings twenty-fourth aaai conference artiﬁcial intelligence rajarshi dhuliawala shehzaad zaheer manzil vilnis luke durugkar ishan krishnamurthy akshay smola alexander mccallum andrew. walk arrive answer reasoning paths knowledge bases using reinforcement learning. corr abs/. gardner matt talukdar partha pratim krishnamurthy jayant mitchell tom. incorporating vector space similarity random walk inference knowledge bases. emnlp mitchell cohen william random walk inference learning large scale knowledge base. proceedings conference empirical methods natural language processing mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg human-level control deep reinforcement learning. nature papineni kishore roukos salim ward todd wei-jing. bleu method automatic evaluation machine translation. proceedings annual meeting association computational linguistics shen yelong huang po-sen chang ming-wei jianfeng. modeling large-scale structured relationships shared memory knowledge base completion. proceedings workshop representation learning silver david huang maddison chris guez arthur sifre laurent driessche george schrittwieser julian antonoglou ioannis panneershelvam veda lanctot marc mastering game deep neural networks tree search. nature silver david schrittwieser julian simonyan karen antonoglou ioannis huang guez arthur hubert thomas baker lucas matthew bolton adrian mastering game without human knowledge. nature silver david hasselt hado hessel matteo schaul guez arthur harley dulac-arnold gabriel reichert david rabinowitz neil barreto andre degris thomas. predictron end-to-end learning planning. proceedings international conference machine learning volume proceedings machine learning research pmlr sutton richard mcallester david singh satinder mansour yishay. policy gradient methods reinforcement learning function approximation. advances neural information processing systems wang jane kurth-nelson tirumala dhruva soyer hubert leibo joel munos r´emi blundell charles kumaran dharshan botvinick matthew. learning reinforcement learn. corr abs/. weber theophane racani`ere s´ebastien reichert david buesing lars guez arthur rezende danilo jimenez badia adri`a puigdom`enech vinyals oriol heess nicolas yujia pascanu razvan battaglia peter silver david wierstra daan. imaginationaugmented agents deep reinforcement learning. corr abs/. xiong wenhan hoang thien wang william yang. deeppath reinforcement learning method knowledge graph reasoning. proceedings conference empirical methods natural language processing zhang xingxing lapata mirella. chinese poetry generation recurrent neural networks. proceedings conference empirical methods natural language processing mcts implementation maintain lookup table record values visited stateaction pair. state graph walk problem contains information along traversal path node current step assign index candidate action indicating ia-th action node thus state encoded path string build dictionary using path string record values backup stage values updated state-action pair along traversal path mcts experiments three glass puzzle randomly draw four integers represent capacity desired volume respectively. restrict puzzle avoid data duplication. clean puzzles solution them. finally keep unique puzzles experimental dataset puzzles used training used test model’s generalization capability unseen test set. current status container deﬁne puzzle status step one-shot representation encode value given smaller experiment dimension initial query obtained query embedding lookup table indicates x-th column. query embedding dimension three glass puzzle actions total container capacity empty container empty pour container container stop action terminate game. maximum length action sequence indicates stop action taken last step. stop action taken system evaluates action sequence assigns reward ﬁnal status success otherwise functions modeled different dnns architecture fully-connected layers hidden dimensions relu activation function. fully-connected layers hidden dimensions ﬁrst hidden layer relu activation function output layer linear activation function. modeled hidden size hyperparameters puct adam optimization algorithm learning rate training mini-batch size nell- knowledge dataset contains unique entities relations. entity embedding dimension relation embedding dimension maximum length graph walking path indicates stop action taken last step. stop action taken system evaluates action sequence assigns reward agent reaches target node otherwise initial query concatenation entity embedding vector relation embedding vector. functions modeled different dnns architecture fully-connected layers hidden dimensions relu activation function. fully-connected layers hidden dimensions ﬁrst hidden layer tanh activation function output layer linear activation function. modeled hidden size hyperparameters puct rollout mcts paths training prediction. adam optimization algorithm model training learning rate mini-batch size evaluate proposed approach text generation task. formulate possible generation space graph formulate text generation task graph walk problem. speciﬁc node represents word vocabulary edge represents generating next word current word chinese poem corpus experiments. corpus consists chinese quatrains containing four lines twenty characters total. split dataset parts percent training percent testing. setup experiments conditional generation task. given ﬁrst lines chinese quatrain goal generate following lines. bleu- score evaluation metric measure similarity generated texts ground-truth texts. build vocabulary keeping frequent words word assigned pre-trained dimensional chinese word embeddings using chakin. word embeddings ﬁxed experiments. lstm language model baseline model three lstm layer hidden units layer. implement self-critic sequence training reinforcement learning algorithm another baseline. scst model initialized pretrained lstm language model uses bleu- score reward. proposed reinforcewalk model also initialized pre-trained language model. bleu score reward reinforcewalk model. reinforcewalk model performs rollout training instance sampled current policy rollouts sampled montecarole tree search algorithm. hyperparameters experiments. value network three fully-connected layers hidden units tanh activation function. functions identity functions. modeled layer lstm hidden size table report evaluation results three approaches. scst baselines beam search size prediction uses mcts rollout prediction. proposed model shows signiﬁcant improvement baseline scst models.", "year": 2018}