{"title": "Revisiting Kernelized Locality-Sensitive Hashing for Improved  Large-Scale Image Retrieval", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We present a simple but powerful reinterpretation of kernelized locality-sensitive hashing (KLSH), a general and popular method developed in the vision community for performing approximate nearest-neighbor searches in an arbitrary reproducing kernel Hilbert space (RKHS). Our new perspective is based on viewing the steps of the KLSH algorithm in an appropriately projected space, and has several key theoretical and practical benefits. First, it eliminates the problematic conceptual difficulties that are present in the existing motivation of KLSH. Second, it yields the first formal retrieval performance bounds for KLSH. Third, our analysis reveals two techniques for boosting the empirical performance of KLSH. We evaluate these extensions on several large-scale benchmark image retrieval data sets, and show that our analysis leads to improved recall performance of at least 12%, and sometimes much higher, over the standard KLSH method.", "text": "present simple powerful reinterpretation kernelized locality-sensitive hashing general popular method developed vision community performing approximate nearest-neighbor searches arbitrary reproducing kernel hilbert space perspective based viewing steps klsh algorithm appropriately projected space several theoretical practical beneﬁts. first eliminates problematic conceptual difﬁculties present existing motivation klsh. second yields ﬁrst formal retrieval performance bounds klsh. third analysis reveals techniques boosting empirical performance klsh. evaluate extensions several large-scale benchmark image retrieval data sets show analysis leads improved recall performance least sometimes much higher standard klsh method. similarity search search) large databases plays critical role number important vision applications including content-based image video retrieval. usually data represented high-dimensional feature space number objects database scale billions modern applications. such fast indexing search vital component many large-scale retrieval systems. theoretical practical breakthrough similarity search problem development locality-sensitive hashing relies gaussian random projections euclidean distance provably retrieve approximate nearest neighbors time grows sublinearly number database items. vision community long employed core methods large-scale retrieval unfortunately cases image comparison criteria based functions simple euclidean distance corresponding image feature vectors makes inapplicable several settings. foundational work done extend kernels satisfying particular conditions hashing shift-invariant kernels based random fourier features generally kulis grauman proposed technique called kernelized approximate nearest neighbor searches arbitrary kernels thus extending situations kernel function evaluations possible. main idea behind klsh approximate necessary gaussian random projections kernel space using appropriate random combination items database based application central limit theorem. since publication klsh used extensively computer vision community related hashing methods built klsh foundations however klsh still suffers important drawbacks. first kulis grauman show central limit theorem ensures approximate random projections constructed become true gaussian random projections number sampled database items gets larger bounds explicitly given clarify tradeoff accuracy runtime. even worse approach klsh uses–that applying random projection vector–is conceptually inappropriate inﬁnite-dimensional kernel space since discuss later canonical gaussian distribution even exists. paper present simple powerful reinterpretation klsh describe section perspective gracefully resolves inﬁnite gaussian issue provides ﬁrst explicit performance bounds clearly demonstrate tradeoffs runtime retrieval accuracy. crucially tradeoff also reveals potential techniques boost empirical performance vanilla klsh. particular show modify klsh obtain improvements recall performance least sometimes much higher benchmarks examined section limited existing theoretical analysis klsh based nystr¨om approximation bounds however analysis examines average error original kernel function values approximations made klsh provide bounds retrieval performance. moreover discuss section subtle difference klsh nystr¨om method rendering aforementioned analysis problematic. further demonstrate section klsh bears advantages nystr¨om method number database items selected approximate kernel functions relatively small. conﬂicting views comparison klsh applying kernel data. example work concluded klsh clear performance edge kpca+lsh results contradicted empirical analysis demonstrated kpca projection step shows signiﬁcant improvement klsh. section seemingly disparate methods equivalent performance observed practice largely choice parameters. although gives error analysis projection step using cauchy-schwarz inequality explicit performance bounds proved. thus fails show interesting tradeoffs retrieval bounds derive section recently work kernel approximation-based visual search methods. asymmetric sparse kernel approximations approximate nearest neighbor search asymmetric similarity score computed randomly selected landmarks. shown excellent empirical performance kernelized random subspace hashing attempts randomly generate orthogonal bases m-dimensional subspace kernel space. classical hashing schemes employed projection subspace. approaches viewed variants nystr¨om method; note authors able provide preservation bound inter-vector angles showed better angle preservation klsh. main contribution summarized threefold. first provide interpretation klsh provides ﬁrmer theoretical footing also resolves issues revolving around comparisons klsh projection kernel pca. second able derive ﬁrst formal retrieval bounds klsh demonstrating tradeoff similar classic bias-variance tradeoff statistics. lastly importantly analysis reveals potential techniques boosting performance standard klsh. successfully validate techassume database samples given query user-deﬁned kernel function feature implicit reproducing kernel hilbert space interested ﬁnding similar item database query respect i.e. argmaxiκ. general technique constructing applying hash functions data similar objects likely hashed together hash functions binary hash functions employed results projection data b-dimensional binary space. note several possible schemes including non-binary hashes focus mainly binary hashing paper. advantage binary hashing nearest neighbor queries hamming space implemented quickly; tree-based data structures used approximate nearest neighbors hamming space time sub-linear number data points even exact nearest neighbor computation performed extremely quickly hamming space. here consider normalized kernel functions un-normalized kernels results applied normalization κ/pκ given valid hash families query time retrieving -nearest neighbors bounded hamming distance linear kernel normalized histograms charikar showed hash family constructed rounding output product random hyperplane random vector sampled standard multivariate gaussian distribution directly extended kernels known explicit representations dimension however case many commonly-used kernels vision applications. order deal arbitrary kernels klsh attempts mimic technique drawing approximate gaussian random vectors rkhs central-limit theorem advantage approach resulting hash function computation accomplished solely using kernel function evaluations. considering realizations random variable known mean covariance operator classical ensures random vector converges standard gaussian random vector thereest centered kernel matrix formed ˆxm} ¯k−/ vector ones entries corresponding samples. note constant scaling terms dropped without changing hash function evaluation. validity klsh relies heavily central limit theorem. crucial question addressed existence case kernel function based inﬁnite-dimensional embedding unfortunately canonical gaussian distribution rkhs given following lemma. lemma gaussian distribution covariance operator hilbert space exists appropriate base diagonal form non-negative eigenvalues eigenvalues ﬁnite. implied lemma convergence standard gaussian inﬁnite-dimensional hilbert space grounded eigenvalues covariance operator inﬁnity. such motivation klsh problematic best worst could render klsh inappropriate many retrieval settings speciﬁcally designed. time klsh shown solid empirical performance kernels associated inﬁnite-dimensional explain discrepancy empirical performance lack solid theoretical motivation? resolve issues next section. following provide simple powerful reinterpretation klsh allow circumvent aforementioned issues inﬁnite-dimensional particular show klsh viewed precisely kpca+lsh except gaussian vectors drawn drawn kpca projected space. klsh explicit embedding. take deeper look hash function utilizing eigen-decomposition covariance write expected approximation error. resulting viewed inner product between k-dimensional vectors. speciﬁcally ﬁrst vector simply projection onto subspace spanned principal components. k-dimensional vector plug deﬁnition interpretation truncation viewed computing inner product k-dimensional vectors ﬁrst vector data point projecting kpca second vector gaussian random vector. words interpreted ﬁrst computing kpca using draw random vectors projected space. since klsh uses sample data points data estimate covariance mean automatically obtain truncation estimated covariance -dimensional -dimensional centering operation). therefore klsh performs projecting -dimensional space principal component analysis thus klsh conceptually equivalent applying projection difference klsh uses central limit theorem approximately draw gaussian vectors projected space whereas standard kpca+lsh draws random vectors directly. note central limit theorem known converge rate practice obtains good approximations such results algorithms within small random variations other. summary able explain empirical performance also avoid technical issues non-existence standard gaussian distributions inﬁnite-dimensional hilbert spaces. perspective lead performance bound klsh section also sheds light simple techniques could potentially improve retrieval performance. comparison nystr¨om method. arguably popular kernel matrix approximation method nystr¨om method uses low-rank approximation original kernel matrix order reduce complexity computing inverses. although klsh bears similarities nystr¨om method pointed brieﬂy clarify differences approaches. rank-k approximation original kernel matrix kernel matrix entry ˆk†r pseudoinverse ˆkr. best rank-k approximation kernel matrix formed selected anchor points. write best rank-r approximation diagonal matrix whose diagonal entries leading eigenvalues matrix column corresponding eigenvector derive although look similar format representations turn yield different hashing performance. even though nystr¨om method aims approximate whole kernel matrix point centering operation used klsh essential give strong performance especially relatively small. empirically explore issue section section present main theoretical results consist performance bound klsh method analogous results standard lsh. perhaps importantly analysis suggests simple techniques improve empirical performance klsh method. present theoretical analysis klsh kpca+lsh perspective. make assumption klsh draws truly random gaussian vectors projected subspace simplicity presentation. reﬁned analysis would also include error included approximating gaussian random vectors central limit theorem. analysis possible incorporate berry-esseen theorem however discussed earlier practice clt-based random vectors provide sufﬁcient approximations gaussian vectors in-depth empirical comparison). ﬁrst formulate setting assumptions main results. suppose i.i.d. random samples drawn probability measure given query similarity function often referred kernel function want argmaxi∈sκ. standard machine learning literature kernel positive semi-deﬁnite associated feature maps original data points high dimensional even inﬁnite dimensional feature space kernel function gives inner product feature space. feature together induce distribution covariance operator denoted finally v··· corresponding eigenvalues eigenvectors respectively. assume explicit formula feature project onto k-dimensional subspace spanned ﬁrst principal components empirically sample points estimate note projection kernel function becomes theorem consider n-sample database query point success probability least query time dominated namely performance kpca+lsh saturates large enough usually thousands. low-rank extension. hand ﬁxed trade-off decreasing increasing increase increases whereas decrease larger result ﬁxed best choice actually original klsh algorithm could smaller. light this introduce low-rank extension standard klsh instead performing -dimensional subspace klsh actually achieve better results projecting smaller r-dimensional subspace obtained principal components. speciﬁcally replace equation best rank-r approximation authors recommend number principal components number hash bits applying kpca+lsh least kernel. show section fact optimal choice rank dependent dataset kernel number hash bits. moreover performance klsh quite sensitive choice extension monotone transformation. another relevant factor decaying property eigenvalues covariance operator induced rkhs affects also constant corresponds estimation error. unlike kernel methods classiﬁcation detection unique property regarding kernel functions retrieval applying monotone increasing transformation original kernel function change ranking provided original kernel. scale parameter ranking nearest neighbors stays matter value choose long however changing scaling impacts eigenvalues covariance operator. particular often slows decay eliminate decay entirely context bound means term increase slowly. moreover also reduces estimation error eigen-space. however value large also need large order keep term small. thus scaling must carefully tuned balance trade-off. perform experiments three benchmark datasets commonly used large-scale image search comparisons mirflickr consists million randomly sampled flickr images represented using -dimensional edge histogram descriptors; siftm million -dimensional local sift descriptors; gistm comprised million -dimensional gist descriptors. query size three datasets mirflickr gistm descriptors whose values zero removed database. throughout comparisons parameters klsh follows generate -bit hash code form random projection matrix equivalent performing kernel sample size choice number bits largely suppress performance variation randomness conclusions made consistent among different choices number bits. i-th entry perform exhaustive nearest neighbors search evaluate quality retrieval using recallr measure proportion query vectors nearest neighbor ranked ﬁrst positions. measure indicates fraction queries nearest neighbor retrieved correctly short list veriﬁed original space. note here represents database items. focus measure since gives direct measure nearest neighbor search performance. note deliberately comparing hashing schemes semi-supervised hashing methods optimization-based methods; goal demonstrate analysis used improve results klsh. existing work klsh variants considered comparisons klsh techniques given space restrictions focus comparisons here. figure shows effect rank smallest rank performed better least comparable vanilla klsh. conﬁrms empirical results shown kpca+lsh smaller number principal components beats klsh retrieval performance. however entire story. clearly performance tradeoff discussed section initially retrieval performance improves increasing number principal components used corresponds decreasing however figure siftm intersection kernel. recallr improvement full-rank nystr¨om method rank recallr results klsh rank vanilla klsh full-rank nystr¨om method change decaying properties exponential transformation scale parameter point performance drops corresponding increase mirflickr gistm difference among ranks dramatic showing sensitivity choice rank. addition best-performing rank dependent kernel chosen also critically dependent dataset examined. nonetheless still obtain least absolute improvement recall siftm data least affected trade-off. here performance different kernels quite similar divergent different datasets. instance optimal rank mirflickr data much smaller siftm gistm. however observe relationship rank number bits used making recommendations questionable. comparison nystr¨om method. figure shows effect rank nystr¨om method performance monotonically increasing rank. moreover shows unacceptable retrieval performance even rank contrasting obvious tradeoff choice ranks klsh results corroborate earlier observation klsh indeed different nystr¨om method. regarding performance comparison figure nystr¨om method performs worse low-rank version klsh standardfullrank klsh large margin. here show effect transformation introduced note free choose possible transformation long chosen transformation increasingly monotonic; choice simply illustration. figure changing scale parameter affects decay eigenvalues. particular increasing scaling slows decay continue decrease decay gets larger. figure demonstrates power transformation recallr steadily increases slow decaying speed decaying gradual. large drop performance signiﬁcantly. choice usefulness also largely depends kernel function dataset consideration effect kernel intersection kernel. hand effective gistm dataset siftm. note here comparing original kernel ﬁxed rank favors original kernel. thus room improvement choosing larger rank. table summaries total absolute improvement combining techniques together. retrieval improvement least sometimes much higher among benchmarks.this validates merit analysis section regarding interesting trade-offs shown performance bound demonstrates power simple techniques. introduced interpretation kernelized locality-sensitive hashing technique. perspective makes possible circumvent conceptual issues original algorithm provides ﬁrmer theoretical ground viewing klsh applying appropriately projected data. view klsh enables prove ﬁrst formal retrieval performance bounds suggests simple techniques boosting retrieval performance. successfully validated results empirically large-scale datasets showed choice rank monotone transformation vital achieve better performance.", "year": 2014}