{"title": "Modular Continual Learning in a Unified Visual Environment", "tag": ["cs.LG", "cs.AI", "q-bio.NC", "stat.ML"], "abstract": "A core aspect of human intelligence is the ability to learn new tasks quickly and switch between them flexibly. Here, we describe a modular continual reinforcement learning paradigm inspired by these abilities. We first introduce a visual interaction environment that allows many types of tasks to be unified in a single framework. We then describe a reward map prediction scheme that learns new tasks robustly in the very large state and action spaces required by such an environment. We investigate how properties of module architecture influence efficiency of task learning, showing that a module motif incorporating specific design principles (e.g. early bottlenecks, low-order polynomial nonlinearities, and symmetry) significantly outperforms more standard neural network motifs, needing fewer training examples and fewer neurons to achieve high levels of performance. Finally, we present a meta-controller architecture for task switching based on a dynamic neural voting scheme, which allows new modules to use information learned from previously-seen tasks to substantially improve their own learning efficiency.", "text": "core aspect human intelligence ability learn tasks quickly switch ﬂexibly. here describe modular continual reinforcement learning paradigm inspired abilities. ﬁrst introduce visual interaction environment allows many types tasks uniﬁed single framework. describe reward prediction scheme learns tasks robustly large state action spaces required environment. investigate properties module architecture inﬂuence efﬁciency task learning showing module motif incorporating speciﬁc design principles signiﬁcantly outperforms standard neural network motifs needing fewer training examples fewer neurons achieve high levels performance. finally present meta-controller architecture task switching based dynamic neural voting scheme allows modules information learned previouslyseen tasks substantially improve learning efﬁciency. course everyday functioning people constantly faced real-world environments required shift unpredictably multiple sometimes unfamiliar tasks nonetheless able ﬂexibly adapt existing decision schemas build ones response challenges humans support ﬂexible learning task switching largely unknown neuroscientiﬁcally algorithmically investigate solving problem neural module approach simple task-specialized decision modules dynamically allocated largely-ﬁxed underlying sensory system sensory system computes general-purpose visual representation decision modules read. sensory backbone large complex learned comparatively slowly signiﬁcant amounts training data task modules deploy information base representation must contrast lightweight quick learned easy switch between. case visually-driven tasks results neuroscience computer vision suggest role ﬁxed general purpose visual representation played ventral visual stream modeled deep convolutional neural network however algorithmic basis efﬁciently learn dynamically deploy visual decision modules remains obvious. figure modular continual learning touchstream environment touchstream environment touchscreen-like continual learning agents spectrum visual reasoning tasks posed large uniﬁed action space. timestep environment emits visual image reward agent recieves input emits action action represents touch location two-dimensional screen e.g. screen height width. environment’s policy program computing function agent’s action history. agent’s goal learn choose optimal actions maximize amount reward recieves time. agent consists several component neural networks including ﬁxed visual backbone learned neural modules meta-controller mediates deployment learned modules task solving. modules remap algorithm learn estimate reward function action conditional agent’s recent history. using sampling policy reward agent chooses optimal action maximize aggregate reward. standard supervised learning often assumed output space problem prespeciﬁed manner happens task hand e.g. classiﬁcation task discrete output ﬁxed number classes might determined ahead time continuous estimation problem one-dimensional real-valued target might chosen instead. convenient simpliﬁcation supervised learning single-task reinforcement learning contexts interested learning deployment decision structures rich environment deﬁning tasks many different natural output types simpliﬁcation becomes cumbersome. beyond limitation build uniﬁed environment many different tasks naturally embodied. speciﬁcally model agent interacting two-dimensional touchscreenlike call touchstream tasks encoded using single common intuitive albeit large output space. choice frees hand-design programmatically choose different output domain spaces forces confront core challenge naive agent quickly emergently learn implicit interfaces required solve different tasks. introduce reward prediction networks algorithm continual reinforcement learning able discover implicit task-speciﬁc interfaces large action spaces like touchstream environment. address major algorithmic challenges associated learning remap modules. first module architectural motifs allow efﬁcient task interface learning? compare several candidate architectures show incorporating certain intuitive design principles signiﬁcantly outperform standard neural network motifs needing fewer training examples fewer neurons achieve high levels performance. second system architectures effective switching tasks? present meta-controller architecture based dynamic neural voting scheme allowing modules information learned previously-seen tasks substantially improve learning efﬁciency. formalize touchstream environment. introduce remap algorithm. describe evaluate comparative performance multiple remap module architectures variety touchstream tasks. describe dynamic neural voting meta-controller evaluate ability efﬁciently transfer knowledge remap modules task switches. figure exemplar touchstream tasks. illustration several task paradigms explored work using touchstream environment. depicts observation bottom shows ground truth reward maps binary stimulus-response task. stereotyped match-to-sample task. match-to-sample task using ms-coco dataset. object localization. modern deep convolutional neural networks signiﬁcant impact computer vision artiﬁcial intelligence well computational neuroscience vision recent growing literature convnet-based neural modules used solving compositional visual reasoning tasks work apply idea modules solving visual learning challenges continual learning context. existing works rely choosing menu pre-speciﬁed module primitives using different module types solve subproblems involving speciﬁc input-output datatypes without addressing modules’ forms discovered ﬁrst place. paper show single generic module architecture capable automatically learning solve wide variety different tasks uniﬁed action/state space simple controller scheme able switch modules. results also closely connected literature lifelong learning part literature concerned learning solve tasks without catastrophically forgetting solve ones modules obviates problem instead shifts hard question newly-allocated modules learned effectively. continual learning literature also directly addresses knowlege transfer newly allocated structures largely addresses transfer learning lead higher performance rather addressing improve learning speed. aside reward performance focus issues speed learning task switching motivated remarkably efﬁcient adaptability humans task contexts. existing work continual learning also largely address speciﬁc architecture types learn tasks efﬁciently independent transfer. focusing ﬁrst identifying architectures achieve high performance quickly individual tasks transfer-learning investigation naturally focuses efﬁciently identify re-use components architectures works also make explicit priori assumptions structure tasks encoded models rather address general question emergence solutions embodied case meta-reinforcement learning approaches wang duan well schema learning ideas e.g. arbib mcclelland typically seek address issue continual learning complex meta-learner extract correlations tasks long timescale. context burden environment learning placed individual modules meta-controller thus comparatively light-weight compared typical meta-reinforcement approaches. unlike case meta-learning mostly limited small state action spaces. recent work general reinforcement learning dulac-arnold addressed issue large action spaces sought address multitask transfer learning large action spaces. agents real-world environment exposed many different implicit tasks arising without predeﬁned decision structures must learn appropriate decision interfaces situation. interested modeling agents on-the-ﬂy learning task environment mimic unconstrained nature real world. here describe touchstream environment attempts simpliﬁed two-dimensional domain. problem setup consists components environment agent interacting extended temporal sequence timestep environment emits image height width scalar reward conversely agent accepts images rewards input chooses action response. action space available agent consists two-dimensional pixel grid height width input image. environment equipped policy time step computes image reward function history agent actions at−} images xt−} rewards rt−}. work agent neural network composed visual backbone ﬁxed weights together meta-controller module whose parameters learned interaction environment. agent’s goal learn enact policy maximizes reward obtained time. unlike episodic reinforcement learning context touchstream environment continuous throughout course learning agent never signaled reset initial internal state. however unlike traditional continuous learning context e.g. sutton barto touchstream implicitly deﬁne many different tasks associated characteristic reward schedule. agent experiences continual stream tasks implicit association reward schedule state reset must discovered agent. framing action space agent possible pixel locations state space arbitrary image wide range possible tasks uniﬁed single framework cost requiring agents’ action space congruent input state space thus quite large. presents core efﬁciency challenges agent given task must able quickly recognize interface task transfer knowledge across tasks smart way. goals complicated fact large size agent’s state action spaces. although work modern large-scale computer vision-style datasets tasks work e.g. imagenet ms-coco also inspired visual psychology neuroscience pioneered techniques controlled visual tasks embodied real reinforcement learning paradigms especially useful three classes task paradigms span range ways discrete continuous estimation tasks formulated including stimulus-response match-to-sample localization tasks stimulus-response tasks stimulus-response paradigm common approach physically embodying discrete categorization tasks example simple two-way discrimination task shown fig. agent rewarded touches left half screen shown image right half shown butterﬂy. tasks made difﬁcult increasing number image classes complexity reward boundary regions. experiments images classes imagenet dataset match-to-sample tasks match-to-sample paradigm another common approach assessing visual categorization abilities task shown fig. trials consist sequence image frames sample screen followed match screen agent expected remember object category seen sample frame select onscreen button match screen corresponding sample screen category. unlike tasks tasks require working memory localized spatial control. complex tasks involve sophisticated relationships sample match screen. fig. using ms-coco object detection challenge dataset sample screen shows isolated template image indicating ms-coco classes match screen shows randomly-drawn scene dataset containing least instance sample-image class. agent rewarded chosen action located inside boundary instance correct class. ms-coco task hybrid categorical continuous elements meaning phrased standard supervised learning problem categorical readout continous readout would required. localization fig. shows two-step continuous localization task agent supposed mark bounding object touching opposite corners successive timesteps reward proportionate intersection union value predicted bounding relative ground truth bounding area localization unlike area paradigms choice made timestep constrains agent’s optimal choice future timestep although tasks become arbitrarily complex along certain axes tasks presented require ﬁxed-length memory future prediction. task requires knowledge past timesteps perfect solution always exists within timesteps point. minimal required values different across various tasks work. however investigations below maximum required values across tasks i.e. thus agent required learn safe ignore information past irrelevant predict past certain point future. begin considering restricted case environment runs semantic task indefinitely showing different architectures learn solve individual tasks dramatically different levels efﬁciency expand considering case environment’s policy consists sequence tasks unpredictable transitions tasks exhibit meta-controller cope effectively expanded domain touchstream environment necessarily involves working large action state spaces. methods handling situation often focus reducing effective size action/state spaces either estimating pseudo-counts state-action pairs clustering actions take another approach using neural network directly approximate mapping action space reward space allowing learnable regularities state-action interaction implicitly reduce large spaces something manageable simple choice policies. introduce off-policy algorithm efﬁcient multitask reinforcement learning large action state spaces reward prediction remap. standard reinforcement learning situation agent seeks learn optimal policy deﬁning probability density actions given image state remap algorithm off-policy calculated simple ﬁxed function estimated reward. remap network neural network parameters whose inputs history previous timesteps agent’s actions activation encoding agent’s state space; explicitly approximates expected reward across action space number future timesteps. mathematically number previous timesteps considered; length future horizon considered; ψt−kbt history state space encodings produced ﬁxed backbone network ht−kbt− history previously chosen actions action space reward space. predicted reward maps constructed computing expected reward obtained subsample actions drawn randomly believes single best action expected reward maps remap algorithm formulates normalizing predictions across maps separate probability distributions sampling action distribution maximum variance. agent computes policy follows sampling procedure described equation uses complementary ideas exploit spatial temporal structure efﬁciently explore large action space. since rewards real physical tasks spatially correlated distribution-based sampler equation allows effective exploration potentially informative actions would single-point estimate apparent optimum further order reduce uncertainty remap algorithm explores timesteps greatest reward variance. varargmax function nonlinearly upweights timeframe highest variance exploit fact points time carry disproportianate relevance reward outcome somewhat analagously max-pooling operates convolutional networks. although standard action selection strategy used place empirically found policy effective efﬁciently exploring large action space. parameters remap network learned gradient descent loss reward prediction error argminθ compared true reward rt+j. reward prediction corresponding action chosen timestep participates loss calculation backpropagation error signals. minibatch maps rewards actions collected several consecutive inference passes performing parameter update. observe encode state space network append state buffer subsample potential action choices uniformly produce expected reward maps select action according policy execute action environment store action buffer receive reward calculate loss previous timesteps batch size perform parameter update throughout work take ﬁxed backbone state space encoder vgg- convnet pretrained imagenet resolution input network pixels action space default functional family used action selection scheme identity although tasks beneﬁting high action precision often optimal sample low-temperature boltzmann distribution e−x/t reward prediction errors calculated using cross-entropy loss main question seek address section speciﬁc neural network structure used remap modules? considerations modules easy learn requiring comparatively training examples discover optimal parameters easy learn from meaning agent quickly build module reusing components ones. intuitive example intuition-building example consider case simple binary stimulusresponse task fig. decision module perfect reward predictor task expressed analytically heaviside function components action relative center screen |ψt| matrix expressing class boundary positive must also positive predict positive reward; conversly negative must negative predict reward. neither conditions hold terms equal zero formula predicts reward. since vertical location action affect reward involved reward calculation task. equation three basic ideas embedded structure early visual bottleneck high-dimensional general purpose feature representation greatly reduced dimension prior combination action space multiplicative interaction action vector visual features symmetry e.g. ﬁrst term formula sign-antisymmetric partner second next sections show three principles generalized parameterized family networks visual bottleneck decision structure emerge naturally efﬁcienty learning given task interest. section deﬁne generic remap module lightweight encodes three generic design principles perfect formula uses small number learnable parameters. deﬁne concatenated square nonlinearity cres nonlinearity introduces multiplicative interactions arguments component symmetry crelu. deﬁnition. -early bottleneck-multiplicative-symmetric module remap module given figure decision interfaces emerge naturally course training. remap modules allow agent discover implicit interfaces task. observe learning generally ﬁrst captures emergence natural physical constructs learning task-speciﬁc decision rules. examples include onscreen buttons appearing match screen task speciﬁc semantic meaning button learned general discovery objects boundaries task-speciﬁc category rule applied. image best viewed color. structure builds three principles described above. stage represents early bottleneck visual encoding inputs bottlenecked size combined actions performs cres stages introducing multiplicative symmetric interactions visual features actions. this perfect module deﬁnition binary task becomes special case two-layer module. note visual features bottlenecked encoder; practice work fully connected convolutional features vgg- backbone. experiments follow compare module wide variety alternative control motifs early bottleneck multiplicative symmetric features ablated. multiplicative nonlinearity bottleneck ablations spectrum standard activation functions including relu tanh sigmoid crelu forms. late bottleneck architectures effectively standard multi-layer perceptrons action vectors concatenated directly output visual encoder passed subsequent stages. test distinct architectures. detailed information found supplement. compared architecture across variants visual localization tasks using ﬁxed visual encoding features layer vgg-. task variants ranged complexity simple challenging complex tasks variants localization either single main salient object placed complex background complex scenes ms-coco details tasks used experiments found supplement. module weights initialized using normal distribution optimized using adam algorithm parameters learning rates optimized per-task per-architecture basis cross-validated fashion. architecture task optimizations different initialization seeds obtain mean standard error initial condition variability. fully-ablated late-bottleneck modules measured performance modules three different sizes smallest version equivalent size module medium large versions much larger emergence decision structures feature remap modules able discover novo underlying output domain spaces variety qualitatively distinct tasks emergent decision structures highly interpretable reﬂect true interfaces environment implicitly deﬁnes. spatiotemporal patterns learning robust across tasks replicable across initial seedings thus might serve candidate model interface learning humans. general observe modules typically discover figure modules components efﬁcient visual learning system. validation reward obtained course training modules -way stimulus-response reward split four quadrants -way randomly moving match templates -way randomly moving class templates shown time -way four randomly positioned images shown time. lines indicate mean reward different weight initializations. clarity seven total tested architectures displayed ta-n-auc metric area learning curve normalized highest performing module within task averaged across tasks. error values standard deviations mean ta-n-auc. example case discrete categorization task involves quick discovery onscreen buttons corresponding discrete action choices buttons mapped semantic meaning. case ms-coco task observe initial discovery high salience object boundaries followed category-speciﬁc reﬁnement. important note visual backbone trained categorization task quite distinct localization task ms-coco mts. thus module learn different decision structure well class boundaries ms-coco scratch training. efﬁciency module efﬁciency learning measured computing taskaveraged normalized area learning curve modules tested across task variants. fig. shows characteristic learning curves several tasks summarized table fig. results architectures tasks shown supplement figure module efﬁcient across tasks moreover architecture always achieves highest ﬁnal reward level task. increasing ablations structure lead increasingly poor performance terms learning efﬁciency ﬁnal performance. ablating low-order polynomial interaction largest negative effect performance followed importance symmetric structure large fully-ablated models performed signiﬁcantly worse smaller module single ablations better module neither symmetry multiplicative interactions small fully-ablated modules number parameters least efﬁcient oftentimes achieved much lower ﬁnal reward. summary main conceptual features special-case architecture solves binary task individually helpful combine usefully parameterized efﬁciently learned variety visual tasks. properties critical achieving effective task learning compared standard structures. second experiment focusing localization tasks tested module using convolutional features ﬁxed vgg- feature encoder reasoning localization tasks could beneﬁt ﬁner spatial feature resolution. using visual features explicit spatial information figure convolutional bottlenecks allow resolution localization detection complex scenes. mean intersection union obtained localization task. reward obtained ms-coco match-to-sample variant. require visual systems accomodate ﬁner spatial resolution understanding scene precise action placement tasks. convolutional variant module uses skip connections conv layers vgg- input whereas standard uses layer input. substantially improves task performance learning efﬁciency tasks knowledge results ms-coco ﬁrst demonstrated reinforcement learning achieve instance-level object segmentations. reward curves fig. show little difference late bottleneck modules size. models consistently achieve ems-like variants especially convolutional features. context baseline trained using supervised methods directly regress bounding boxes using features results we’ve considered case touchstream consists task. however agents real environments often faced switch tasks many encountering ﬁrst time. ideally agents would repurpose knowledge previously learned tasks relevant task. formally consider environment policies consisting sequences tasks last indeterminate period time. consider also modules module corresponds task-speciﬁc policy task begins agent allocate module added modules learning follows allocation weights modules held ﬁxed parameters module trained. however output system merely output module instead dynamically allocated mixture pathways computation graphs modules. mixture determined meta-controller meta-controller neural network learns dynamic distribution modules used building composite execution graph. intuitively composite graph composed small number relevant pathways match parts existing modules solve task potentially combination module components need learned. dynamic neural voting deﬁne meta-controller assigns weights layer module weight associated layer module weights probabilistic layer basis interpreted probability controller selecting e.g. assignment layer weights composite execution graph deﬁned meta-controller generated computing activations components layer weighted probabilities values passed next layer process repeats. mathematically composite layer stage expressed figure dynamic neural voting controller. dynamic neural voting solves tasks computing composite execution graph previously learned newly allocated modules. shown agent existing modules newly allocated module learned layer controller takes input activations three modules outputs voting results probabilistic weights used scale activations within corresponding components. voting done either per-layer basis per-unit basis clarity layer voting method depicted. weighted three scaled outputs used input next stage computation graph. task solved combination existing module components weighted highly module effectively unused e.g. assigned weights. however task quite different previously solved tasks module play larger role execution graph learns solve task. question probabilistic weights come from? core procedure dynamic neural voting process controller network learns boltzmann distribution module activations maximize reward prediction accuracy. process performed module layer module weightings given layer conditioned results voting previous layer. pppi learnable weight matrix controller. voting procedure operates online fashion controller continously learning meta-policy agent taking actions. deﬁned meta-controller constitutes fully-differentiable neural network learned gradient descent online. useful reﬁnement mechanism involves voting across units speciﬁcally meta-controller assigns probabilistic weights contrast layer-voting scheme dynamically generated execution graph computed meta controller becomes composite neurons activations empirically initialization schemes learnable controller parameters important consideration design specialized transformations also contribute slightly overall efﬁciency. details these please refer supplement. dynamic neural voting mechanism achieves meta-control neural network optimized online gradient descent modules solving tasks rather genetic algorithm operates longer timescale work fernando moreover contrast work rusu voting mechanism eliminates need fully-connected adaptation layers modules thus substantially reducing number parameters required transfer. no-switch switches ﬁrst experiments tested dynamic neural voting mechanism would respond no-switch switches i.e. ones although switch given module allocated environment policy’s task actually change cases performance almost instantly approaches pre-switch levels moreover weightings controller applies module words system recognizes module needed acts accordingly concentrating weights existing module. results show that formally assume agent cued task switches occurs theory could implement completely autonomous monitoring policy agent simply runs allocation procedure performance anomoly occurs system determines module unneeded could simply reallocate module later task switch. future work plan implement policy explicitly. figure dyanmic neural voting quickly corrects no-switch switches. although module allocated task transition task identitcal original task controller quickly learns reuse module components. postswitching learning curve module binary stimulus-response task trained task. clarity layer voting method compared baseline module trained scratch. bottom fraction original module reused course post-switch learning calculated averaging voting weights layer original module. real switches next tested dynamic voting controller handled switches environment policy substantially changed switching cue. using module large fully-ablated module described dynamic neural voting controller evaluated switching experiments using multiple variants tasks. speciﬁcally switches cover variety distinct switching types including addition classes dataset replacing current class entirely non-overlapping class addition visual variability previously less variable task addition visual interface elements e.g. buttons transformation interface elements e.g. screen rotation transitions different task paradigms e.g. tasks vice-versa module trained scratch task switching dynamic neural voting. post-switching learning curves module -way quadrant task learning -way task -way task match screen class templates. layer voting method single-unit voting method compared baseline module trained second task scratch. across twelve task switches evaluate relative gain baseline using voting methods module large-sized fully-ablated late bottleneck mlp. transfer gain metrics compared module types voting mechanisms. colors second task switch module transferred initial task using dynamic voting controller. dynamic voting controller allows rapid positive transfer module types across task switches general single-unit voting method somewhat better transfer mechanism layer voting method module large fully-ablated module shown inefﬁcient single-task performance beneﬁt dynamic neural voting modules switchable quantify fast switching gains realized transfer gain gain ∆max t∆max argmax time maximum t∆max amount reward difference switch occurs ∆max reward difference time. qualitatively high score transfer gain metric indicates large amount relative reward improvement achieved short amount time large fully-ablated modules positive transfer gain scores signiﬁcantly higher metric i.e. signiﬁcantly switchable large fully-ablated module hypothesize module able achieve high task performance signiﬁcantly fewer units larger fully-ablated module making former easier dynamic neural voting controller operate work introduce touchstream environment continual reinforcement learning framework uniﬁes wide variety spatial decision-making tasks within single context. describe general algorithm learning light-weight neural modules discover implicit task interfaces within large-action/state-space environment. show particular module architecture able remain compact retaining high task performance thus especially suitable ﬂexible task learning switching. also describe simple general dynamic task-switching architecture shows substantial ability transfer knowledge modules tasks learned. crucial future direction expand insights current work complete continual-learning agent. need show approach scales handle dozens hundreds task switches sequence. also need address issues agent determines build module consolidate modules appropriate also critical extend approach handle visual tasks longer horizons navigation game play extended strategic planning likely require recurrent memory stores part feature encoder. application point view particularly interested using techniques like described produce agents autonomously discover operate interfaces present many important real-world two-dimensional problem domains smartphones internet also expect many spatially-informed techniques enable remap/ems modules perform well touchstream environment also transfer naturally three-dimensional context autonomous robotics applications compelling. references jacob andreas marcus rohrbach trevor darrell klein. deep compositional question answering neural module networks. corr abs/. http//arxiv. org/abs/.. djork-arné clevert thomas unterthiner sepp hochreiter. fast accurate deep network learning exponential linear units corr abs/. http//arxiv. org/abs/.. michael cole jeremy reynolds jonathan power grega repovs alan anticevic todd braver. multi-task connectivity reveals ﬂexible hubs adaptive task control. nat. neurosci. coline devin abhishek gupta trevor darrell pieter abbeel sergey levine. learning modular neural network policies multi-task multi-robot transfer. corr abs/. http//arxiv.org/abs/.. duan john schulman chen peter bartlett ilya sutskever pieter abbeel. fast reinforcement learning slow reinforcement learning. corr abs/. http//arxiv.org/abs/.. gabriel dulac-arnold richard evans peter sunehag coppin. reinforcement learning large discrete action spaces. corr abs/. http//arxiv.org/abs/ chrisantha fernando dylan banarse charles blundell yori zwols david andrei rusu alexander pritzel daan wierstra. pathnet evolution channels gradient descent super neural networks. corr abs/. http//arxiv.org/abs/.. david gaffan susan harrison. inferotemporal-frontal disconnection fornix transection visuomotor conditional learning monkeys. behavioural brain research issn https//doi.org/./--. http //www.sciencedirect.com/science/article/pii/. alexa horner christopher heath martha hvoslef-eide brianne kent simon nilsson johan alsiö charlotte oomen andrew holmes lisa saksida touchscreen operant platform testing learning memory rats mice. nature protocols james kirkpatrick razvan pascanu neil rabinowitz joel veness guillaume desjardins andrei rusu kieran milan john quan tiago ramalho agnieszka grabska-barwinska demis hassabis claudia clopath dharshan kumaran raia hadsell. overcoming catastrophic forgetting neural networks. corr abs/. http//arxiv.org/abs/ tsung-yi michael maire serge belongie lubomir bourdev ross girshick james hays pietro perona deva ramanan piotr dollár lawrence zitnick. microsoft coco common objects context. corr abs/. http//arxiv.org/abs/ james mcclelland. incorporating rapid neocortical learning schema-consistent information complementary learning systems theory. journal experimental psychology general elisabeth murray mortimer mishkin. object recognition location memory monkeys excitotoxic lesions amygdala hippocampus. journal neuroscience issn http//www.jneurosci.org/content///. razavian hossein azizpour josephine sullivan stefan carlsson. features offthe-shelf astounding baseline recognition. computer vision pattern recognition workshops ieee conference ieee andrei rusu neil rabinowitz guillaume desjardins hubert soyer james kirkpatrick koray kavukcuoglu razvan pascanu raia hadsell. progressive neural networks. corr abs/. http//arxiv.org/abs/.. wenling shang kihyuk sohn diogo almeida honglak lee. understanding improving convolutional neural networks concatenated rectiﬁed linear units. corr abs/. http//arxiv.org/abs/.. anthony wagner daniel schacter michael rotte wilma koutstaal anat maril anders dale bruce rosen randy buckner. building memories remembering forgetting verbal experiences predicted brain activity. science jane wang kurth-nelson dhruva tirumala hubert soyer joel leibo rémi munos charles blundell dharshan kumaran matt botvinick. learning reinforcement learn. corr abs/. http//arxiv.org/abs/.. yamins hong cadieu solomon seibert dicarlo. performance-optimized hierarchical models predict neural responses higher visual cortex. proceedings national academy sciences stimulus-response experiment details image categories used drawn image-net ilsvr classiﬁcation challenge dataset deng four unique object classes taken dataset boston terrier monarch butterﬂy race panda bear. class unique training instances unique validation instances. match-to-sample experiment details sample screen images drawn image-net class stimulus-response tasks. face-centered unobstructed class instance also drawn image-net classiﬁcation challenge used match screen template image class. class template images match screen held ﬁxed pixels. variants task keep pixel buffer edges screen match images twelve pixel buffer adjascent edges match images themselves. variants without vertical motion match images vertically centered screen. localization experiment details localization task uses synthetic images containing single main salient object placed complex background yamins total unique classes dataset. contrast single-class localization datasets designed large face-centered centrally-focused object instance trivial policy always poke image corners could learned synthetic image offers larger variance instance scale position rotation agent forced learning non-trivial policies requiring larger precision action selection. ms-coco experiment details task uses entire ms-coco detection challenge dataset every timestep sample screen chosen ms-coco classes. constructed large unobstructed face centered representations class. match screen sample random scene ms-coco containing number objects containing least single instance sample class. agent rewarded action located inside instance correct class. modules sample actions low-temperature boltzmann policy empirically found result precise reward prediction. table aggregates number units layer ablated modules used conducting single-task task-switching experiments. fully-connected modules’ layer sizes shown here. details convolutional bottleneck module please refer figure exhaustive module performance study module ablation control modules measured area curve task variants. shown normalized highest performing module task. results averaged vertical task axis report salient subset ablations. \"convolutional bottleneck\" extension module shown paper skip connections link conv representation visual backbone. here \"scenelevel\" representation stored remap memory buffer tiled spatially match present convolution dimensions concatenated onto channel dimension. series convolutions plays role shallow visual bottleneck activations vectorized concatenated input cres layers standard module. results paper shown bottleneck consisting single tanh cres convolutions units each. downstream layers units well. motivation convolutional bottleneck lower-level features useful complex spatial tasks localization object detection hence result precise policy. tiling entire scene-level representation along convolution layer’s channel dimension form multiplicative template-matching possible objects must memorized inside present scene. investigated distinct ablations module across twelve task variants outlined symmetry ablations replace cres activation relu multiplicative ablations denoted specifying nonlinearity used place cres crelu shang additionally includes partial symmetry ablation visual bottleneck symmetric ablates relu symm module partial symm symm symm/partial mult mult/symm relu mult/symm tanh mult/symm mult/symm mult/symm crelu none relu none relu none relu none tanh none tanh none tanh none none none none none none none crelu none crelu none crelu learning trajectories seven additional tasks provided figure modules capable convergence task acheived values given task calculated point time majority models converge. figure additional single-task performance ablation learning curves. seven learning curves shown task variants seen main text body. shown ablations main text. note efﬁcient modules effectively produce minimal representation interaction action space observation agent’s optimal action space shifts remainder task context remains ﬁxed controller allow rapid targeted remapping since formulate modules remap networks input feature basis achieve remappings form fully-connected transformation vector action histories embed action space using small number learnable parameters. pseudo identity-preserving transformation practice initialize parameters transformation pseudo identity-preseving meaning representation learned level original module destroyed prior transfer. done initializing identity matrix i|aτ| added break symmetry. initialized vector ones size |aτ|. maps reﬂects agent’s uncertainty environment’s reward policy. task context remains stationary environment transitions reward schedule longer aligns module’s policy controller could transition e.g. containing mechanism allowing targeted transformation hence also complication arises remap since task-module learns optimal action space internally basis rather therefore transformations distribution must also re-encode mapping work investigate shallow adapter neural network lives existing module maps ﬁrst second layers deﬁned similar transformation above denotes elementwise multiplication learnable matrix embedding hidden state r|l|×|r learnable matrix embedding pseudo identity-preserving transformation similar transformation action space modify reward-map transformation pseudo identity-preserving well. done modifying original maps concatenated beginning transformation input vector targeted transformations several hyperparameters. conducted grid search optimize cross-validated fashion test task switches designed solved targeted transformations conducted independently dynamic voting controller independently transformation. optimal hyperparameters found experiments ﬁxed integrated dynamic voting controller optimized afterwards. action transformation hyperparameters conducted three tests using stimulus-response paradigm class reversal horizontal rotation reward boundaries switch original task work single non-activated linear transformation optimal state-space embedding using units initialized idendity-preserving transformation weights learning rate transformation found optimal reward transformation hyperparameters conducted tests using stimulusresponse paradigm squeezing task switch original task work optimal activations cres relu units hidden layer. weight initialization scheme found optimal initial bias optimal learning rate transformation found study conducted determine relative beneﬁt targeted transformations determined primary contribution dynamic neural controller fact voting mechanism cued task transition controller freezes learnable parameters task-module deploys unitialized task-module. controller initializes action reward transformation networks described module. transformations also voted inside dynamic neural controller every timestep. figure action reward transformation switch examples. three task switching experiments performed optimize hyperparameters targeted transformations augment dynamic neural voting controller. switches also retested fully-integrated meta-controller shown original switching result ﬁgure. binary stimulus-response class reversals left class becomes right class vice-versa. rotations binary stimulus-response reward boundaries. squeezing binary stimulus-response reward boundaries reward given task bottom half screen regardless class shown. figure illustration switching performance metrics. quantify switching performance dynamic neural controller task-modules metrics relative gain transfer gain relative measures overall gain relative scratch transfer gain measures speed transfer. curve shown module single-unit voting method evaluated switch -way task randomly moving class templates -way task four randomly moving templates.", "year": 2017}