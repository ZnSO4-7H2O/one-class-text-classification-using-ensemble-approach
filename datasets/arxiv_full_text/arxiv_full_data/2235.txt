{"title": "Learnable Explicit Density for Continuous Latent Space and Variational  Inference", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this paper, we study two aspects of the variational autoencoder (VAE): the prior distribution over the latent variables and its corresponding posterior. First, we decompose the learning of VAEs into layerwise density estimation, and argue that having a flexible prior is beneficial to both sample generation and inference. Second, we analyze the family of inverse autoregressive flows (inverse AF) and show that with further improvement, inverse AF could be used as universal approximation to any complicated posterior. Our analysis results in a unified approach to parameterizing a VAE, without the need to restrict ourselves to use factorial Gaussians in the latent real space.", "text": "class conditional distribution functions latent variable inﬁnitely many classes. models theoretically enough ﬂexibility capture highly complex distributions image manifolds practice found overshadowed tractable density models autoregressive models generative adversarial networks terms sample generation quality. believed relative poor performance sample quality lies fact introduction latent representation requires approximate inference model distribution biased simplifying posterior densities i.e. training achieved maximizing variational lower bound marginal likelihood discuss aspects training bound. first maximizing respect amounts minimizing kl||p); variational distribution thus viewed approximate true posterior simplifying problematic marginal likelihood interest optimized extent able approximate true posterior using variational distribution. motivates direct improvement variational inference second training part latent space explored. marginalizing input vector indicates true data distribution. marginal approximate posterior fails prior priorcontractive term requires would risk sampling untrained regions latent space. direct nonparametric treatment sampling regions prior would take prior integral intractable data distribution partially specipaper study aspects variational autoencoder prior distribution latent variables corresponding posterior. first decompose learning vaes layerwise density estimation argue ﬂexible prior beneﬁcial sample generation inference. second analyze family inverse autoregressive ﬂows show improvement inverse could used universal approximation complicated posterior. analysis results uniﬁed approach parameterizing without need restrict factorial gaussians latent real space. deep gaussian latent models also known variational autoencoders fall within paradigm maximum likelihood estimate often applied computer vision problems. however training usually leads overestimation entropy data distribution undesirable property natural images usually assumed within lower dimensional manifold additional entropy often leads marginal likelihood probability mass spread data space support training data causes blurriness samples. observations motivate design ﬂexible complex families model densities. mila universit´e montr´eal canada imagia inc. canada montr´eal canada cifar fellow canada. correspondence chin-wei huang <cw.huanggmail.com>. close prior. achieved limits following conditions words given perfect approximate posterior perfect marginal likelihood marginal converge prior i.e. equality direct result rearrangement terms. implies maximizing variational lower bound brings limit conditions marginal approximate posterior match prior given enough ﬂexibility assumed form densities. maximize respect holding ﬁxed like coordinate ascent samples drawn doubly stochastic process thought projected data distribution want model using prior distribution another advantage learnable prior visualized cartoon plot figure approximate posterior update prior becomes closer marginal approximate posterior concentrates probability mass true posterior becomes closer approximate posterior words region high posterior density covered approximate posterior reduced effectively means proposal variational distribution could improved better prior simpliﬁes true posterior. figure effect prior posterior. matching prior marginal approximate posterior makes true posterior easier model since pushes true posterior closer approximate posterior. limited training data. even take empirical distribution would mixture model components number training data points would impractical given scale modern machine learning tasks. workaround problem take random subset introduce learnable pseudo-data size prior shown promising recent work done tomczak welling another approach directly regularize autoencoder matching aggregated posterior prior makhzani paper make main contributions. first analyze effect making prior learnable. show training variational lower bound limit conditions matches marginal approximate posterior prior desirable generative model point view. decompose lower bound show updating prior alone brings prior closer marginal approximate posterior suggesting prior trainable beneﬁcial sample generation inference. second contribution prove using family inverse universally approximate posterior. theoretically justiﬁes inverse improve variational inference. uniﬁed aspects propose invertible functionals dinh kingma parameterize explicit densities prior approximate posterior. claim maximizing variational lower bound explicitly matches marginal prior decomposing lower bound suggest using learnable prior improve sampling i.e. prior matches marginal instead. kingma powerful family invertible functions called inverse autoregressive flows introduced improve variational inference. thus practical fundamental importance understand beneﬁts using inverse improve them. section show normalizing ﬂows base distribution autoregressive assumptions universal approximators density given enough capacity neural network used parameterize non-linear dependencies. lemma existence solution nonlinear independent component analysis problem. given random vector i=...m always exists mapping components random vector statistically independent. proof. hyvarjnen pajunen full proof. point transformation used proof falls within family autoregressive functions i=...m conditional distribution random variable warped independent distribution cdfs speciﬁcally kind gram-schmidt process-like construction. proposition inverse autoregressive transformation universal approximator density. random vector open assume positive continuous probability density distribution. exists sequence mappings parametrized autoregressive neural networks sequence converges distribution proof. consider mapping deﬁned proof lemma autoregressive jacobian upper triangular matrix whose diagonal entries equal conditional densities positive assumption. determinant jacobian equal product diagonal entries positive. inverse function theorem locally invertible. also injective globally invertible denotes inverse. autoregressive function universal approximation theorem know exists sequence mappings parametrized autoregressive neural networks converge uniformly real-valued bounded continuous function latter uniform convergence implies since converge pointwise continuity converges pointwise bounded dominated convergence theorem gives converges latter statement valid bounded continuous function converge distribution. note usually parameterized invertible function expense ﬂexibility tractable jacobian. special designs function afﬁne transformation could made improve ﬂow; otherwise would need compose multiple layers transformations richer distribution family. proof shows that careful designs approximate posteriors vaes could asymptotic consistency. proposed method suggested sections propose oneto-one correspondence deﬁne learnable explicit density model inference sample generation. first inspired found updating prior alone reminiscent mle. think data points projected onto latent space monte carlo sampling data distribution epd] space unimodal prior tends overestimate entropy powerful family real non-volume preserving transformations applied real variables. thus natural incorporate real vaes jointly train explicit density model prior. deﬁne prior change variable formula compute density projected data distribution inversely transform samples base variable tractable density deﬁne posterior likewise rezende mohamed objective thus modiﬁed permutation invariant latent variables implemented random masks. latent variables preserve spatial correlation convolutional network used choose checkerboard style mask ||p) zero forcing since samples region target density heavily penalized. begins sharper shape pays high penalty expansion move another mode. thus easy distribution stuck local minima true posterior multimodal learning prior mode seeking problem since forward zero avoiding. mnist. also tested proposed method binarized mnist report estimated negative likelihood evaluation metric. compare effects adding invertible transformation layers either prior posterior table models having ﬂexible prior easily outperform models ﬂexible posterior. likelihood model ﬂexible prior improved using expressive posterior real made introduce autoregressive dependencies paper ﬁrst reinterpret training variational lower bound layer-wise density estimation. treating monte carlo samples approximate posterior distributions projected data distribution suggests using ﬂexible prior avoid overestimate entropy. leave experiments larger datasets sample generation future work. second showed parameterizing inverse using neural networks allows universally approximate posterior theoretically justiﬁes inverse proof also implies using afﬁne coupling autoregressively warp distribution limited. thus possible consider designs ﬂexible invertible functions improve approximate posterior. figure fitting gaussian mixture distribution. indicates marginalization data clockwise left projection data distribution onto prior space base distribution space density maps base distribution transformed prior marginal model distribution table effect increasing prior complexity. lpost number made layers used posterior. hidden layers nodes used layer transformation. lprior number layers used prior. hidden layer nodes used layer transformation. multi-layer perceptron hidden layers nodes used dimension latent variable rectiﬁer used non-linear activation. residual convnet layers residual strided convolution feature maps using ﬁlter size stochastic layer hidden layer nodes used. dimension latent variable exponential linear units non-linearity. second posterior distribution construct inverse parallelizable combined made pixelcnn fact inverse thought generalization real jacobian masked operation used real upper triangular. kingma diederik salimans welling max. improving variational inference inverse autoregressive flow. advances neural information processing systems larochelle hugo murray iain. neural autoregresproceedings sive distribution estimator. international conference artiﬁcial intelligence statistics volume jmlr w&cp rezende danilo jimenez mohamed shakir wierstra daan. stochastic backpropagation approximate inproceedings ference deep generative models. international conference international conference machine learning volume icml’. jmlr.org buntine wray jakulin aleks. applying discrete data analysis. proceedings conference uncertainty artiﬁcial intelligence arlington virginia united states auai press. isbn ---. dinh laurent krueger david bengio yoshua. nice non-linear independent components estimation. international conference learning representation iain larochelle hugo. made masked autoencoder distribution estimation. proceedings international conference international conference machine learning icml’ goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. advances neural information processing systems curran associates inc.", "year": 2017}