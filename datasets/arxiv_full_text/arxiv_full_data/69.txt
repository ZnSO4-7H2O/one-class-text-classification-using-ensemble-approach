{"title": "Neural Enquirer: Learning to Query Tables with Natural Language", "tag": ["cs.AI", "cs.CL", "cs.LG", "cs.NE"], "abstract": "We proposed Neural Enquirer as a neural network architecture to execute a natural language (NL) query on a knowledge-base (KB) for answers. Basically, Neural Enquirer finds the distributed representation of a query and then executes it on knowledge-base tables to obtain the answer as one of the values in the tables. Unlike similar efforts in end-to-end training of semantic parsers, Neural Enquirer is fully \"neuralized\": it not only gives distributional representation of the query and the knowledge-base, but also realizes the execution of compositional queries as a series of differentiable operations, with intermediate results (consisting of annotations of the tables at different levels) saved on multiple layers of memory. Neural Enquirer can be trained with gradient descent, with which not only the parameters of the controlling components and semantic parsing component, but also the embeddings of the tables and query words can be learned from scratch. The training can be done in an end-to-end fashion, but it can take stronger guidance, e.g., the step-by-step supervision for complicated queries, and benefit from it. Neural Enquirer is one step towards building neural network systems which seek to understand language by executing it on real-world. Our experiments show that Neural Enquirer can learn to execute fairly complicated NL queries on tables with rich structures.", "text": "proposed neural enquirer neural network architecture execute natural language query knowledge-base answers. basically neural enquirer ﬁnds distributed representation query executes knowledgebase tables obtain answer values tables. unlike similar eﬀorts end-to-end training semantic parsers neural enquirer fully neuralized gives distributional representation query knowledge-base also realizes execution compositional queries series diﬀerentiable operations intermediate results saved multiple layers memory. neural enquirer trained gradient descent parameters controlling components semantic parsing component also embeddings tables query words learned scratch. training done end-to-end fashion take stronger guidance e.g. step-by-step supervision complicated queries beneﬁt neural enquirer step towards building neural network systems seek understand language executing real-world. experiments show neural enquirer learn execute fairly complicated queries tables rich structures. models natural language dialogue question answering ubiquitous need querying knowledge-base traditional pipeline query semantic parser obtain executable representations typically logical forms apply representation knowledge-base answer. semantic parsing query execution part quite messy complicated queries like which city hosted longest game game beijing? figure need carefully devised systems hand-crafted features rules derive correct logical form partially overcome diﬃculty eﬀort backpropagate result query execution revise semantic representation query actually falls thread work learning grounding drawback hand neural network-based models previously successful mostly tasks direct strong supervision natural language processing related domain examples including machine translation syntactic parsing. recent work learning execute simple python code lstm pioneers direction learning parse structured objects executing purely neural later work neural turing machine introduces modeling ﬂexibility equipping lstm external memory various means interacting work inspired above-mentioned threads research aims design neural network system learn understand query execute knowledge-base table examples queries answers. proposed neural enquirer encodes queries distributed representations executes compositional queries series diﬀerentiable operations. trained using query-answer pairs distributed representations queries optimized together query execution logic end-to-end fashion. demonstrates using synthetic question-answering task proposed neural enquirer capable learning execute compositional natural language queries complex structures. table outputs ranked list query answers. execution done ﬁrst using encoders encode query table distributed representations sent cascaded pipeline executors derive answer. figure gives illustrative example various types components involved query encoder encodes query distributed representation carries semantic information original query. encoded query embedding sent various executors compute execution result. table encoder encodes entries table distributed vectors. table encoder outputs embedding vector table entry retains twodimensional structure table. executor executes query table outputs annotations encode intermediate execution results stored memory layer accessed subsequent executor. basic assumption complex compositional queries answered multiple steps computation executor models speciﬁc type operation conditioned query. figure illustrates operation executor supposed perform answering diﬀerent classical semantic parsing approaches require predeﬁned possible logical operations neural enquirer capable learning logic executors end-to-end training using query-answer pairs. stacking several executors neural enquirer able answer complex queries involving multiple steps computation. given query composed sequence words query encoder parses dq-dimensional vectorial representation encode−−−−→ rdq. implementation neural enquirer employ bidirectional mission. speciﬁcally summarizes sequence word embeddings vector representation embedding worth noting query encoder representation rather general class symbol sequences agnostic actual representation queries neural enquirer capable learning execution logic expressed input query end-to-end training making generic model query execution. table encoder table encoder converts knowledge-base table distributional representation input neural enquirer. suppose table rows columns column comes ﬁeld name value table entry word vocabulary table encoder ﬁrst ﬁnds composite embedding entries table. speciﬁcally entry m-th n-th column value table encoder computes -dimensional embedding vector fusing embedding entry value embedding corresponding ﬁeld name follows table encoder functions diﬀerently classical knowledge embedding models embeddings entities relations learned unsupervised fashion minimizing certain reconstruction errors. embeddings neural enquirer optimized supervised learning towards end-to-end tasks. additionally shown experiments embeddings function indices necessarily encode exact semantic meaning corresponding words. neural enquirer executes input query table layers execution. layer executor captures certain type operation relevant input query returns intermediate execution results referred annotations saved external memory layer. query executed step-by-step sequence stacked executors. cascaded architecture enables neural enquirer answer complex compositional queries. illustrative example given figure executor annotated operation assumed perform. demonstrate section neural enquirer capable learning operation logic executor end-to-end training. design executor inspired neural turing machines data fetched external memory using read head subsequently processed controller whose outputs ﬂushed back memories. executor functions similarly reading data table using reader calling annotator calculate intermediate computational results annotations stored executor’s memory. assume annotations able handle operations require row-wise local information table annotations model superlative operations aggregating table-wise global execution results. therefore combination table annotations enables neural enquirer capture variety real-world query operations. attend subset entries pertain execution modeled reader. related content-based addressing neural turing machines attention mechanism neural machine translation models executor- annotator computes table annotations based fetched read vector executor-+). process repeated intermediate layers executor last layer ﬁnally generate answer. table annotations capturing global execution state table annotation summarized annotations global pooling operation. implementation neural enquirer employ pooling real-world often modeled schema involving various tables table stores speciﬁc type factual information. present neural enquirer-m adapted simultaneously operating multiple tables. challenge scenario multiplicity tables requires modeling interaction them. example neural enquirer-m needs serve join queries whose answer derived joining ﬁelds diﬀerent tables. details modeling experiments neural enquirer-m given appendix neural enquirer trained end-to-end fashion question answering tasks. training representations queries table entries well execution logic captured weights executors learned. speciﬁcally given end-to-end training executor discovers operation logic training data purely data-driven manner could diﬃcult complicated queries requiring four sequential operations. coerce executor ﬁgure logic particular operation relative ﬁeld. example executor- figure biasing weight host city ﬁeld towards value host city ﬁeld fetched sent computing annotations force executor learn whose host city beijing. setting referred step-by-step training. formally done introducing additional supervision signal section evaluate neural enquirer synthetic tasks queries varying compositional depth. ﬁrst brieﬂy describe synthetic task benchmark experimental setup discuss results diﬀerent settings. present synthetic task evaluate performance neural enquirer large amount examples various levels complexity generated evaluate single table multiple tables cases model. starting artiﬁcial tasks eases process developing novel deep models gained increasing popularity recent advances research modeling symbolic computation using dnns olympic games ﬁelds whose values drawn vocabulary size country city names numbers. figure gives example table row. next generate query using predeﬁned templates associated gold-standard answer task consists four types natural language queries summarized table annotated sql-like logical forms easy interpretation. generate queries various levels compositionality mimic real world scenario. complexity queries ranges simple select queries complicated nest ones involving multiple steps computation. queries ﬂexible enough involve complex matching phrases logical constituents makes query understanding execution nontrivial ﬁeld described diﬀerent phrases diﬀerent ﬁelds referred pattern simple constituents grounded complex logical operations experiments procedure generate benchmark datasets consisting diﬀerent types queries. make artiﬁcial task harder enforce queries testing appear training set. neural enquirer executors. number layers dnn) dnn) respectively. dimensionality word/entity embeddings row/table annotations hidden layers hidden states query encoder beginning input queries ﬁxed size. neural enquirer trained standard back-propagation. objective functions optimized using mini-batch size adaptive learning rates model converges fast within epochs. compare model sempre state-of-the-art semantic parser. evaluate performance neural enquirer sempre terms accuracy deﬁned fraction correctly answered queries. table summarizes results sempre neural enquirer endto-end step-by-step settings. show individual performance type query overall accuracy. evaluate sempre mixtured-k long training time even smaller mixtured-k give discussion eﬃciency issues appendix tion mixtured-k model outperforms sempre types queries marginal gain simple queries signiﬁcant improvement complex ones size training grows neural enquirer achieves near accuracy ﬁrst three types queries registering decent overall accuracy results suggest model eﬀective answering compositional natural language queries especially complex semantic structures compared state-of-the-art system. answer probability last executor outputs entry table. statistics obtained model trained mixtured-k. sampled queries dataset model answers correctly visualized corresponding values illustrated figure respectively. executor actually learns execution logic correct answers end-to-end training corresponds assumption. model executes query three steps last three executors performs speciﬁc type operation. executor- takes value participants ﬁeld input computes intermediate annotations executor- focuses medals ﬁeld. finally last executor outputs high probability duration ﬁeld row. attention weights executor- executor- appear meaningless requires three steps execution model learns defer meaningful execution last three executors. guess conﬁdently executing executor- performs conditional ﬁltering operation executor- performs ﬁrst part argmax last executor ﬁnishes execution assigning high probability duration ﬁeld maximum value medals. compared relatively simple complicated whose logical form involves nest sub-query requires steps execution. weights visualized ﬁgure last three executors function similarly case answering execution logic ﬁrst executors obscure. posit end-to-end training supervision signal propagated layer decayed along long path ﬁrst executors causes vanishing gradient problem. also investigate case model fails deliver correct answer complicated queries. figure gives query table together visualized weights. similar requires steps execution. besides messing weights ﬁrst executors last executor executor- predicts wrong entry query answer instead highlighted correct entry. alleviate vanishing gradient problem training complex queries described section next experiments trained neural enquirer model using step-by-step training encourage executor attend speciﬁc ﬁeld known priori relevant execution logic. results shown columns table stronger supervision signal model signiﬁcantly outperforms results end-to-end setting achieves near accuracy types queries shows proposed neural enquirer capable leveraging additional supervision signal given intermediate layers training setting answering complex compositional queries perfect accuracy. revisit query setting weights visualization figure contrast result setting attention weights ﬁrst executors obscure weights every executor perfectly skewed towards actual ﬁeld pertain layer execution quite interestingly attention weights executor- executor- exactly result setting weights executor- executor- signiﬁcantly diﬀerent suggesting neural enquirer learned diﬀerent execution logic setting. major challenges applying neural network models applications deal out-of-vocabulary words particularly severe hard cover existing tail entities time entities appear user-issued queries back-end everyday. quite interestingly simple variation neural enquirer able handle unseen entities almost without loss accuracy. basically divide words vocabulary entity words operation words. embeddings entity words function index facilitate matching entities queries tables layer-by-layer execution need updated initialized; operation words i.e. non-entity words carry semantic meanings relevant execution optimized training. therefore randomly initializing embedding matrix update embeddings operation words training keeping entity words unchanged. test model’s performance words modify queries testing portion mixtured dataset replace entity words ones unseen training set. results obtained using training summarized columns table shows neural enquirer training setting yields performance comparable non-oov setting indicating operation words entity words play diﬀerent roles query execution. interesting question investigate setting neural enquirer distinguishes diﬀerent types entity words queries since embeddings randomly initialized ﬁxed thereafter. example query many people watched game macau? macau entity. help understand model knows macau city give weights visualization figure interestingly model ﬁrst checks host city ﬁeld executor- host country executor- suggests model learns scan possible ﬁelds entity belong important direction semantic parsing research scale large knowledge source ability generalize large knowledge source. train model tables whose ﬁeld sets either subset entire test model tables ﬁelds queries whose ﬁelds span multiple subsets figure illustrates setting. note testing queries exhibit ﬁeld combinations unseen training data mimic diﬃculty system often encounter scaling large knowledge source usually poses great challenge model’s generalization ability. train test model dataset ﬁrst three types relatively simple queries sizes training/testing splits equal numbers diﬀerent query types. table lists results. neural enquirer still maintains reasonable performance even compositionality testing queries previously unseen showing model’s generalization ability tackling unseen query patterns composition familiar ones hence potential scale larger unseen knowledge sources. work falls research area semantic parsing problem parse natural language queries logical forms executable kbs. classical approaches semantic parsing broadly divided categories. ﬁrst line research resorts power grammatical formalisms parse queries generate corresponding logical forms requires curated/learned lexicons deﬁning correspondence phrases symbolic constituents model tuned annotated logical forms capable recovering complex semantics data often constrained speciﬁc domain scalability issues brought crisp grammars lack annotated training data. another line research takes semi-supervised learning approach adopts results query execution supervision signal parsers designed towards learning paradigm take diﬀerent types forms ranging generic chart parsers speciﬁcally engineered task-oriented ones semantic parsers category often scale open domain knowledge sources lack ability understanding compositional queries intractable search space incurred ﬂexibility parsing algorithms. work follows line research using query answers indirect supervision facilitate end-to-end training using tasks performs semantic parsing distributional spaces logical forms neuralized executable distributed representation. work also related recent advances modeling symbolic computation using deep neural networks. pioneered development neural turing machines line research studies problem using diﬀerentiable neural networks perform hard symbolic execution. independent line research similar ﬂavor zaremba designed lstm-rnn execute simple python programs parameters learned comparing neural network output correct answer. work related lines work like heavily external memory ﬂexible processing like neural enquirer learns execute sequence complicated structure model tuned executing them. highlight diﬀerence previous work deep architecture multiple layer external memory neural network operations highly customized querying tables. perhaps related work date recently published neural programmer proposed neelakantan studies task executing queries tables deep neural networks. neural programmer uses neural network model select operations query processing. query planning phase modeled softly using neural networks symbolic operations predeﬁned users. contrast neural enquirer fully distributional models query planning operations neural networks jointly optimized endto-end training. neural enquirer model learns symbolic operations using data-driven approach demonstrates fully neural end-to-end diﬀerentiable system capable modeling executing compositional arithmetic logic operations upto certain level complexity. paper propose neural enquirer fully neural end-to-end diﬀerentiable network learns execute queries tables. present results synthetic tasks demonstrate ability neural enquirer answer fairly complicated compositional queries across multiple tables. future plan advance work following directions. first apply neural enquirer natural language questions natural language answers input query output supervision noisier less informative. second going scale real world task deal large vocabulary novel predicates. third going work", "year": 2015}