{"title": "Temporal Autoencoding Restricted Boltzmann Machine", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Much work has been done refining and characterizing the receptive fields learned by deep learning algorithms. A lot of this work has focused on the development of Gabor-like filters learned when enforcing sparsity constraints on a natural image dataset. Little work however has investigated how these filters might expand to the temporal domain, namely through training on natural movies. Here we investigate exactly this problem in established temporal deep learning algorithms as well as a new learning paradigm suggested here, the Temporal Autoencoding Restricted Boltzmann Machine (TARBM).", "text": "much work done reﬁning characterizing receptive ﬁelds learned deep learning algorithms. work focused development gabor-like ﬁlters learned enforcing sparsity constraints natural image dataset. little work however investigated ﬁlters might expand temporal domain namely training natural movies. investigate exactly problem established temporal deep learning algorithms well learning paradigm suggested here temporal autoencoding restricted boltzmann machine early days machine learning feature extraction usually approached task-speciﬁc way. complexity high dimensionality involved unsupervised fashion seen major barrier expert features thought yield best results classiﬁcation representation tasks recently however number advances brought ﬁeld unsupervised feature extraction back center stage machine learning. increases computational power allowing algorithms trained large datasets together techniques train deep architectures yielded insightful results unsupervised feature learning even uncurated sets natural images examples algorithms denoising autoencoders restricted boltzmann machines unsupervised feature learning structure data deﬁnes features learnt given model. computational neuroscience link ensemble natural stimuli organism exposed shape tuning functions sensory systems subject great interest speciﬁcally ﬁeld vision neuroscience number principles proposed explain shape tuning functions primary visual cortex based properties natural images example redundancy minimization predictive coding recent years shown simple unsupervised learning algorithms sparse coding daes rbms also used learn structure natural stimuli independently labels supervision types structure learnt related back cortical receptive ﬁelds found mammalian brain research vision focused ﬁnding optimal ﬁlters representing decoding sets static natural images seek understand optimal ﬁlters extend temporal domain. build existing work ﬁeld develop temporal autoencoding restricted boltzmann machine show able learn high level structure natural movie dataset account transformation features time. restricted boltzmann machines autoencoders recent years become prominent methods unsupervised feature learning applications wide variety machine learning ﬁelds. models well known discussed length many papers introduce brieﬂy here. models two-layer neural networks connected layers intralayer connectivity. models consist visible hidden layer visible layer represents input model whilst hidden layer’s learn meaningful representation data dimensionality. represent visible layer activation variables hidden activations vector variables {vi} {hj}. autoencoders deterministic model weight matrices representing data visible-to-hidden hidden-to-visible layers respectively trained perform optimal reconstruction visible layer often minimizing mean-squared error reconstruction task. usually evaluated follows given activation pattern visible layer evaluate activation hidden layer sigmw bh). activations propagated back visible layer sigm weights trained minimize distance measure original reconstructed visible layers. example using squared euclidian distance cost function restricted boltzmann machines hand stochastic model assumes symmetric connectivity visible hidden layers seeks model structure given dataset. generally viewed energy-based models energy given conﬁguration activations {vi} {hj} given rbms usually trained contrastive divergence central idea stabilize transient induced presentation data visible layer therefore representing hidden layer optimally. practice achieved learning weights difference transient equilibrium correlations visible hidden layers. sample correlations ﬁrst presentation taken proxy transient correlations successive gibbs samples taken proxy equilibrium correlation. weight update deﬁned number auxiliary strategies used improve training process rbms mini-batch training free energy minimization parzen windows early stopping sparsity constraints. addition rbms stacked form called deep belief network additional models output previous form abstract/high level representation. date number based models proposed capture sequential structure time series data. models temporal restricted boltzmann machine conditional restricted boltzmann machine introduced below. temporal restricted boltzmann machine temporal extension standard whereby feed forward connections included previous time steps hidden layers visible-to-hidden layers visible-to-visible layers. learning conducted manner normal using contrastive divergence shown model used learn non-linear system evolutions dynamics ball bouncing restricted version model discussed seen figure contains temporal connections hidden layers. denote hidden layers visible layersthe energy model given conditional restricted boltzmann machine described contains temporal connections hidden layer includes connections visible layer previous time steps current hidden visible layers. model architecture seen figure again learning architecture requires small change energy function achieved contrastive divergence. crbm likely successfull temporal models date shown model generate data complex dynamical systems human motion capture data video textures present model tarbm extension temporal denoising autoencoder approach used pretrain temporal weights. show approach provides marked advantage contrastive divergence training alone model able outperform trbm crbm classical temporal sequence task yielding deeper insight temporal representation natural image sequence data. much motivation work gain insight typical evolution learned hidden layer features present natural movie stimuli. crbm possible unable explicitly model evolution hidden features without resorting deep network architecture. address using layerwise approach much vein used stacking rbms form deep belief network time. stack given number rbms side side time train temporal connections hidden layers minimize reconstruction error process similar autoencoder training simple autoregressive model used account dynamics hidden layer allowing train dynamic prior temporal evolution stimulus. model network energy-based function interactions hidden layers different time lags. energy model given equation case trbm essentially m-th order autoregressive model trained standard contrastive divergence. individual visible-to-hidden weights initialized contrastive divergence sparsity constraint static samples dataset. that ensure weights representing hidden-to-hidden connections encode dynamic structure ensemble initialize pre-training fashion denoising autoencoder. treating visible layer activation time temporal delay corrupted version true visible activation time view model learn reconstruct visible layer time transforming corrupted input model case denoising autoencoder. pretraining described algorithm regard weights representation static patterns contained data representing transformation undergone patterns time data sequences. allows separate representation form motion case natural image sequences desirable property frequently studied natural movies ﬁrst assess tarbm’s ability learn multi-dimensional temporal sequences applying dimensional motion capture data described comparing performance trbm graham taylor’s example crbm implementation three models implemented using theano temporal dependancy frames trained using minibatches samples epochs training time models approximately equal. training performed ﬁrst samples dataset models presented snippets data included training required generate next frame sequence. results single trial prediction dimensions dataset seen figure mean squared error model predictions repetitions task seen table tarbm outperforms trbm model task also somewhat better crbm gain performance trbm tarbm model structuraly identical would suggest approach autoencoding temporal dependancies gives model meaningful temporal representation achievable contrastive divergence alone. second experiment model natural movie dataset investigate types ﬁlters learned. take hollywood dataset introduced consisting number snippets various hollywood ﬁlms compare crbm implementation referenced tarbm model. dataset pixel patches extracted sequences frames long. contrast normalised whitened provide training approximately samples. models hidden units temporal dependancy frames trained initially epochs static frames data initialise weights convergence full temporal sequences. visualisation temporal receptive ﬁelds learnt crbm involves displaying weight matrix temporal weights hidden unit projection visible layer shows temporal dependance hidden unit past visible layer activations plotted time running left right. visualisation process tarbm somewhat complicated hidden unit also dependant number hidden units delay time model cannot visualised direct projection weights visible layer. understand units depend past forward projection method temporal delays whereby hidden unit delay time chosen starting point. relative weights unit likely units active time given unit active active units choose active units time given activations unit propogated units propogated process repeated full delay network mapped out. active hidden units projection onto patch hidden layer deﬁned weight matrix plotted trace displays likely evolution hidden layer delay period model hidden unit. subset temporal ﬁlters learned models seen figure tarbm left crbm right. tarbm crbm learn gabor like ﬁlters time dependance past markedly different. hidden units crbm fail capture structured dependance delay times greater makes crbms temporal ﬁlters difﬁcult interperet respect structure image. layerwise training temporal weights tarbm along forced reliance ﬁlters learned delay input give tarbm longer temporal dependance also allow weights learned easily interpereted transformation learned ﬁlters. figure shows forward projection method visualising tarbm selected hidden units. means delay step three likely ﬁlters active next point time shown. model able learn multiple trainformations time hidden unit receptive ﬁelds. transformations often represent simple operations rotation translation static features seperating modeling form motion. shown using autoencoder initialise temporal weights trbm forming call tarbm signiﬁcant performance increase achieved modelling generating sequential motion capture dataset. also show tarbm able learn high level structure natural movies account transformation features time. additionally evolution learned temporal ﬁlters easily interpretable help better understand model represents trained data. figure temporal features subset hidden units tarbm crbm tarbm plot active units described text group images represents temporal ﬁlter hidden unit lowest patch representing time patches representing delay steps model. temporal ﬁlters units highest temporal variation receptive ﬁelds models shown. units displayed rows columns ﬁlters temporal axis going bottom. figure temporal filters hidden units tarbm training hollywood dataset image shows schematic three images patch image represents activation single hidden unit time delay tarbm. second shows likely units activated given activation unit rows forming tree structure dependancy. ease interpritation units multiple descendants repeated column easily read bottom. presented model could minimal effort adapted deep architecture allowing represent higher order features temporal manner. propose learning higher order temporal features might prove useful control tasks image stabilization object tracking. addition hope study relation presented encoding strategy strategies employed mammalian visual cortex another interesting avenue research apply current model classiﬁcation generative tasks. work chris h¨ausler alex susemihl supported research training group would like thank prof. martin nawrot freie universit¨at berlin enthusiastic support invaluable feedback.", "year": 2012}