{"title": "Distilling the Knowledge in a Neural Network", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.", "text": "simple improve performance almost machine learning algorithm train many different models data average predictions unfortunately making predictions using whole ensemble models cumbersome computationally expensive allow deployment large number users especially individual models large neural nets. caruana collaborators shown possible compress knowledge ensemble single model much easier deploy develop approach using different compression technique. achieve surprising results mnist show signiﬁcantly improve acoustic model heavily used commercial system distilling knowledge ensemble models single model. also introduce type ensemble composed full models many specialist models learn distinguish ﬁne-grained classes full models confuse. unlike mixture experts specialist models trained rapidly parallel. many insects larval form optimized extracting energy nutrients environment completely different adult form optimized different requirements traveling reproduction. large-scale machine learning typically similar models training stage deployment stage despite different requirements tasks like speech object recognition training must extract structure large highly redundant datasets need operate real time huge amount computation. deployment large number users however much stringent requirements latency computational resources. analogy insects suggests willing train cumbersome models makes easier extract structure data. cumbersome model could ensemble separately trained models single large model trained strong regularizer dropout cumbersome model trained different kind training call distillation transfer knowledge cumbersome model small model suitable deployment. version strategy already pioneered rich caruana collaborators important paper demonstrate convincingly knowledge acquired large ensemble models transferred single small model. conceptual block prevented investigation promising approach tend identify knowledge trained model learned parameter values makes hard change form model keep knowledge. abstract view knowledge frees particular instantiation learned mapping input vectors output vectors. cumbersome models learn discriminate large number classes normal training objective maximize average probability correct answer side-effect learning trained model assigns probabilities incorrect answers even probabilities small much larger others. relative probabilities incorrect answers tell cumbersome model tends generalize. image example small chance mistaken garbage truck mistake still many times probable mistaking carrot. generally accepted objective function used training reﬂect true objective user closely possible. despite this models usually trained optimize performance training data real objective generalize well data. would clearly better train models generalize well requires information correct generalize information normally available. distilling knowledge large model small however train small model generalize large model. cumbersome model generalizes well because example average large ensemble different models small model trained generalize typically much better test data small model trained normal training used train ensemble. obvious transfer generalization ability cumbersome model small model class probabilities produced cumbersome model soft targets training small model. transfer stage could training separate transfer set. cumbersome model large ensemble simpler models arithmetic geometric mean individual predictive distributions soft targets. soft targets high entropy provide much information training case hard targets much less variance gradient training cases small model often trained much less data original cumbersome model using much higher learning rate. tasks like mnist cumbersome model almost always produces correct answer high conﬁdence much information learned function resides ratios small probabilities soft targets. example version given probability whereas another version around. valuable information deﬁnes rich similarity structure data little inﬂuence cross-entropy cost function transfer stage probabilities close zero. caruana collaborators circumvent problem using logits rather probabilities produced softmax targets learning small model minimize squared difference logits produced cumbersome model logits produced small model. general solution called distillation raise temperature ﬁnal softmax cumbersome model produces suitably soft targets. high temperature training small model match soft targets. show later matching logits cumbersome model actually special case distillation. transfer used train small model could consist entirely unlabeled data could original training set. found using original training works well especially small term objective function encourages small model predict true targets well matching soft targets provided cumbersome model. typically small model cannot exactly match soft targets erring direction correct answer turns helpful. simplest form distillation knowledge transferred distilled model training transfer using soft target distribution case transfer produced using cumbersome model high temperature softmax. high temperature used training distilled model trained uses temperature correct labels known transfer method signiﬁcantly improved also training distilled model produce correct labels. correct labels modify soft targets found better simply weighted average different objective functions. ﬁrst objective function cross entropy soft targets cross entropy computed using high temperature softmax distilled model used generating soft targets cumbersome model. second objective function cross entropy correct labels. computed using exactly logits softmax distilled model temperature found best results generally obtained using condiderably lower weight second objective function. since magnitudes gradients produced soft targets scale important multiply using hard soft targets. ensures relative contributions hard soft targets remain roughly unchanged temperature used distillation changed experimenting meta-parameters. case transfer contributes cross-entropy gradient dc/dzi respect logit distilled model. cumbersome model logits produce soft target probabilities transfer training done temperature gradient given high temperature limit distillation equivalent minimizing provided logits zero-meaned separately transfer case. lower temperatures distillation pays much less attention matching logits much negative average. potentially advantageous logits almost completely unconstrained cost function used training cumbersome model could noisy. hand negative logits convey useful information knowledge acquired cumbersome model. effects dominates empirical question. show distilled model much small capture knowledege cumbersome model intermediate temperatures work best strongly suggests ignoring large negative logits helpful. well distillation works trained single large neural hidden layers rectiﬁed linear hidden units training cases. strongly regularized using dropout weight-constraints described dropout viewed training exponentially large ensemble models share weights. addition input images jittered pixels direction. achieved test errors whereas smaller hidden layers rectiﬁed linear hidden units regularization achieved errors. smaller regularized solely adding additional task matching soft targets produced large temperature achieved test errors. shows soft targets transfer great deal knowledge distilled model including knowledge generalize learned translated training data even though transfer contain translations. distilled units hidden layers temperatures gave fairly similar results. radically reduced units layer temperatures range worked signiﬁcantly better higher lower temperatures. tried omitting examples digit transfer set. perspective distilled model mythical digit never seen. despite this distilled model makes test errors threes test set. errors caused fact learned bias class much low. bias increased distilled model makes errors right bias distilled model gets test correct despite never seen training. transfer contains training distilled model makes test errors biases reduced optimize test performance falls test errors. section investigate effects ensembling deep neural network acoustic models used automatic speech recognition show distillation strategy propose paper achieves desired effect distilling ensemble models single model works signiﬁcantly better model size learned directly training data. state-of-the-art systems currently dnns temporal context features derived waveform probability distribution discrete states hidden markov model speciﬁcally produces probability distribution clusters tri-phone states time decoder ﬁnds path states best compromise using high probability states producing transcription probable language model. although possible train decoder taken account marginalizing possible paths common train perform frame-by-frame classiﬁcation minimizing cross entropy predictions made labels given forced alignment ground truth sequence states observation parameters acoustic model maps acoustic observations time probability correct state determined forced alignment correct sequence words. model trained distributed stochastic gradient descent approach. architecture hidden layers containing rectiﬁed linear units ﬁnal softmax layer labels input frames mel-scaled ﬁlterbank coefﬁcients advance frame predict state frame. total number parameters slightly outdated version acoustic model used android voice search considered strong baseline. train acoustic model hours spoken english data yields training examples. system achieves frame accuracy word error rate development set. trained separate models predict using exactly architecture training procedure baseline. models randomly initialized different initial parameter values creates sufﬁcient diversity trained models allow averaged predictions ensemble signiﬁcantly outperform individual models. explored adding diversity models varying sets data model sees found signiﬁcantly change results opted simpler approach. distillation tried temperatures used relative weight cross-entropy hard targets bold font indicates best value used table table shows that indeed distillation approach able extract useful information training simply using hard labels train single model. improvement frame classiﬁcation accuracy achieved using ensemble models transferred distilled model similar improvement observed preliminary experiments mnist. ensemble gives smaller improvement ultimate objective mismatch objective function again improvement achieved ensemble transferred distilled model. recently become aware related work learning small acoustic model matching class probabilities already trained larger model however distillation temperature using large unlabeled dataset best distilled model reduces error rate small model error rates large small models trained hard labels. training ensemble models simple take advantage parallel computation usual objection ensemble requires much computation test time dealt using distillation. however another important objection ensembles individual models large neural networks dataset large amount computation required training time excessive even though easy parallelize. section give example dataset show learning specialist models focus different confusable subset classes reduce total amount computation required learn ensemble. main problem specialists focus making ﬁne-grained distinctions overﬁt easily describe overﬁtting prevented using soft targets. internal google dataset million labeled images labels. work google’s baseline model deep convolutional neural network trained months using asynchronous stochastic gradient descent large number cores. training used types parallelism first many replicas neural running different sets cores processing different mini-batches training set. replica computes average gradient current mini-batch sends gradient sharded parameter server sends back values parameters. values reﬂect gradients received parameter server since last time sent parameters replica. second replica spread multiple cores putting different subsets neurons core. ensemble training third type parallelism wrapped party; easter; bridal shower; baby shower; easter bunny; bridge; cable-stayed bridge; suspension bridge; viaduct; chimney; toyota corolla opel signum; opel astra; mazda familia; number classes large makes sense cumbersome model ensemble contains generalist model trained data many specialist models trained data highly enriched examples confusable subset classes softmax type specialist made much smaller combining classes care single dustbin class. reduce overﬁtting share work learning lower level feature detectors specialist model initialized weights generalist model. weights slightly modiﬁed training specialist half examples coming special subset half sampled random remainder training set. training correct biased training incrementing logit dustbin class proportion specialist class oversampled. order derive groupings object categories specialists decided focus categories full network often confuses. even though could computed confusion matrix used clusters opted simpler approach require true labels construct clusters. particular apply clustering algorithm covariance matrix predictions generalist model classes often predicted together used targets specialist models applied on-line version k-means algorithm columns covariance matrix obtained reasonable clusters tried several clustering algorithms produced similar results. investigating happens specialist models distilled wanted well ensembles containing specialists performed. addition specialist models always generalist model deal classes specialists decide specialists use. given input image top-one classiﬁcation steps step take specialist models whose special subset confusable classes non-empty intersection call active specialists full probability distribution classes minimizes denotes divergence denote probability distribution specialist model generalist full model. distribution distribution specialist classes plus single dustbin class computing divergence full distribution probabilities full distribution assigns classes dustbin. general closed form solution though models produce single probability class solution either arithmetic geometric mean depending whether kl). parameterize tmax gradient descent optimize logits w.r.t. note optimization must carried image. starting trained baseline full network specialists train extremely fast also specialists trained completely independently. table shows absolute test accuracy baseline system baseline system combined specialist models. specialist models relative improvement test accuracy overall. also report conditional test accuracy accuracy considering examples belonging specialist classes restricting predictions subset classes. specialist experiments trained specialist models classes sets classes specialists disjoint often multiple specialists covering particular image class. table shows number test examples change number examples correct position using specialist relative percentage improvement accuracy dataset broken number specialists covering class. encouraged general trend accuracy improvements larger specialists covering particular class since training independent specialist models easy parallelize. main claims using soft targets instead hard targets helpful information carried soft targets could possibly encoded single hard target. section demonstrate large effect using less data parameters baseline speech model described earlier. table shows data training baseline model hard targets leads severe overﬁtting whereas model trained soft targets able recover almost information full training even remarkable note early stopping system soft targets simply converged shows soft targets effective communicating regularities discovered model trained data another model. specialists used experiments dataset collapsed non-specialist classes single dustbin class. allow specialists full softmax classes much better prevent overﬁtting using early stopping. specialist trained data highly enriched special classes. means effective size training much smaller strong tendency overﬁt special classes. problem cannot solved making specialist smaller lose helpful transfer effects modeling non-specialist classes. experiment using speech data strongly suggests specialist initialized weights generalist make retain nearly knowledge non-special classes training soft targets non-special classes addition training hard targets. soft targets provided generalist. currently exploring approach. specialists trained subsets data resemblance mixtures experts gating network compute probability assigning example expert. time experts learning deal examples assigned them gating network learning choose experts assign example based relative discriminative performance experts example. using discriminative performance experts determine learned assignments much better simply clustering input vectors assigning expert cluster makes training hard parallelize first weighted training expert keeps changing depends experts second gating network needs compare performance different experts example know revise assignment probabilities. difﬁculties meant mixtures experts rarely used regime might beneﬁcial tasks huge datasets contain distinctly different subsets. much easier parallelize training multiple specialists. ﬁrst train generalist model confusion matrix deﬁne subsets specialists trained subsets deﬁned specialists trained entirely independently. test time predictions generalist model decide specialists relevant specialists need run. shown distilling works well transferring knowledge ensemble large highly regularized model smaller distilled model. mnist distillation works remarkably well even transfer used train distilled model lacks examples classes. deep acoustic model version used android voice search shown nearly improvement achieved training ensemble deep neural nets distilled single neural size easier deploy. really neural networks infeasible even train full ensemble shown performance single really trained long time significantly improved learning large number specialist nets learns discriminate classes highly confusable cluster. shown distill knowledge specialists back single large net.", "year": 2015}