{"title": "A DIRT-T Approach to Unsupervised Domain Adaptation", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on the digit, traffic sign, and Wi-Fi recognition domain adaptation benchmarks.", "text": "domain adaptation refers problem leveraging labeled data source domain learn accurate model target domain labels scarce unavailable. recent approach ﬁnding common representation domains domain adversarial training attempts induce feature extractor matches source target feature distributions feature space. however domain adversarial training faces critical limitations feature extraction function high-capacity feature distribution matching weak constraint non-conservative domain adaptation training model well source domain hurts performance target domain. paper address issues lens cluster assumption i.e. decision boundaries cross high-density data regions. propose novel related models virtual adversarial domain adaptation model combines domain adversarial training penalty term punishes violation cluster assumption; decision-boundary iterative reﬁnement training teacher model takes vada model initialization employs natural gradient steps minimize cluster assumption violation. extensive empirical results demonstrate combination models signiﬁcantly improve state-of-the-art performance digit trafﬁc sign wi-fi recognition domain adaptation benchmarks. development deep neural networks enabled impressive performance wide variety machine learning tasks. however advancements often rely existence large amount labeled training data. many cases direct access vast quantities labeled data task interest either costly otherwise absent labels readily available related training sets notable example scenario occurs source domain consists richly-annotated synthetic semi-synthetic data target domain consists unannotated real-world data however source data distribution often dissimilar target data distribution resulting signiﬁcant covariate shift detrimental performance source-trained model applied target domain solving covariate shift problem nature instance domain adaptation paper consider challenging setting domain adaptation provided fully-labeled source samples completely-unlabeled target samples existence classiﬁer hypothesis space generalization error source target domains guaranteed. borrowing approximately terminology ben-david refer setting unsupervised non-conservative domain adaptation. note tackle unsupervised domain adaptation ganin lempitsky proposed constrain classiﬁer rely domain-invariant features. achieved training classiﬁer perform well source domain minimizing divergence features extracted source versus target domains. achieve divergence minimization ganin lempitsky employ domain adversarial training. highlight issues approach feature function high-capacity source-target supports disjoint domain-invariance constraint potentially weak good generalization source domain hurts target performance non-conservative setting. saito addressed issues replacing domain adversarial training asymmetric tri-training relies assumption target samples labeled sourcetrained classiﬁer high conﬁdence correctly labeled source classiﬁer. paper consider orthogonal assumption cluster assumption input distribution contains separated data clusters data samples cluster share class label. assumption introduces additional bias seek decision boundaries high-density regions. based intuition propose novel models virtual adversarial domain adaptation model incorporates additional virtual adversarial training conditional entropy loss push decision boundaries away empirical data decision-boundary iterative reﬁnement training teacher model uses natural gradients reﬁne output vada model focusing purely target domain. demonstrate conservative domain adaptation classiﬁer trained perform well source domain vada used constrain hypothesis space penalizing violations cluster assumption thereby improving domain adversarial training. non-conservative domain adaptation account mismatch source target optimal classiﬁers dirt-t allows transition joint classiﬁer better target domain classiﬁer. interestingly demonstrate advantage natural gradients dirt-t reﬁnement steps. report results domain adaptation digits classiﬁcation trafﬁc sign classiﬁcation general object classiﬁcation wi-fi activity recognition show that nearly experiments vada improves upon previous methods dirt-t improves upon vada setting state-of-the-art performances across wide range domain adaptation benchmarks. adapting mnist svhn challenging task out-perform given extensive literature domain adaptation highlight several works relevant paper. shimodaira mansour proposed correct covariate shift re-weighting source samples discrepancy target distribution reweighted source distribution minimized. procedure problematic however source target distributions contain sufﬁcient overlap. huang long ganin lempitsky proposed instead project distributions feature space encourage distribution matching feature space. ganin lempitsky particular encouraged feature matching domain adversarial training corresponds approximately jensen-shannon divergence minimization better perform nonconservative domain adaptation saito proposed modify tri-training domain adaptation leveraging assumption highly-conﬁdent predictions correct predictions several aforementioned methods based ben-david theoretical analysis domain adaptation states following intuitively dh∆h measures extent small changes hypothesis source domain lead large changes target domain. evident dh∆h relates intimately complexity hypothesis space divergence source target domains. inﬁnite-capacity models domains disjoint supports dh∆h maximal. critical component paper cluster assumption states decision boundaries cross high-density regions assumption extensively studied leveraged semi-supervised learning leading proposals conditional entropy minimization pseudo-labeling recently cluster assumption many successful deep semi-supervised learning algorithms semi-supervised generative adversarial networks virtual adversarial training self/temporal-ensembling given success cluster assumption semi-supervised learning natural consider application domain adaptation. indeed ben-david urner formalized cluster assumption lens probabilistic lipschitzness proposed nearest-neighbors model domain adaptation. work extends line research showing cluster assumption applied deep neural networks solve complex high-dimensional domain adaptation problems. independently work french demonstrated application selfensembling domain adaptation. however work additionally considers application cluster assumption non-conservative domain adaptation. describing model ﬁrst highlight domain adversarial training sufﬁcient domain adaptation feature extraction function high-capacity. consider classiﬁer parameterized maps inputs -simplex number classes. suppose classiﬁer decomposed composite embedding function embedding classiﬁer source domain joint distribution input one-hot label marginal input distribution. analogously deﬁned target domain. loss functions weighting factor. minimization encourages learning feature extractor jensen-shannon divergence small. ganin lempitsky suggest successful adaptation tends occur source generalization error feature divergence small. easy however construct situations suggestion fails. particular inﬁnitecapacity source-target supports disjoint employ arbitrary transformations target domain match source feature distribution practice minimization requires solving mini-max optimization problem. discuss verify empirically that sufﬁciently deep layers jointly achieving small source generalization error feature divergence imply high accuracy target task given limitations domain adversarial training wish identify additional constraints place model achieve better reliable domain adaptation. paper apply cluster assumption domain adaptation. cluster assumption states input distribution contains clusters points cluster come class. assumption extensively studied applied successfully wide range classiﬁcation tasks cluster assumption holds optimal decision boundaries occur away data-dense regions space following grandvalet bengio achieve behavior minimization conditional entropy respect target distribution intuitively minimizing conditional entropy forces classiﬁer conﬁdent unlabeled target data thus driving classiﬁer’s decision boundaries away target data practice conditional entropy must empirically estimated using available data. however grandvalet bengio note approximation breaks classiﬁer locally-lipschitz. without locally-lipschitz constraint classiﬁer allowed abruptly change prediction vicinity training data points results unreliable empirical estimate conditional entropy allows placement classiﬁer decision boundaries close training samples even empirical conditional entropy minimized. prevent this propose explicitly incorporate locally-lipschitz constraint virtual adversarial training objective function additional term enforces classiﬁer consistency within norm-ball neighborhood sample note virtual adversarial training applied respect either target source distributions. combine conditional entropy minimization objective domain adversarial training yield basic combination domain adversarial training semi-supervised training objectives. refer virtual adversarial domain adaptation model. empirically observed hyperparameters easy choose work well across multiple tasks denote degree target-side cluster assumption violated. modulating enables vada trade-off hypotheses target-side cluster assumption violation hypotheses source-side generalization error. setting allows rejection hypotheses high target-side cluster assumption violation. rejecting hypotheses hypothesis space vada reduces dh∆h yields tighter bound target generalization error. verify empirically vada achieves signiﬁcant improvements existing models multiple domain adaptation benchmarks figure dirt-t uses vada initialization. removing source training signal dirtminimizes cluster assumption violation target domain series natural gradient steps. generalization error functions source target domains. means that given hypothesis class optimal classiﬁer source domain coincide optimal classiﬁer target domain. assume optimality results violation cluster assumption. words suppose source-optimal classiﬁer drawn hypothesis space necessarily violates cluster assumption target domain. insofar vada trained source domain hypothesize better hypothesis achievable introducing secondary training phase solely minimizes target-side cluster assumption violation. assumption natural solution initialize vada model minimize cluster assumption violation target domain. particular ﬁrst vada learn initial classiﬁer next incrementally push classiﬁer’s decision boundaries away data-dense regions minimizing target-side cluster assumption violation loss denote procedure decision-boundary iterative reﬁnement training deﬁnes neighborhood parameter space. notion neighborhood sensitive parameterization model; depending parameterization seemingly small step result vastly different classiﬁer. contradicts intention incrementally locally pushing decision boundaries local conditional entropy minimum requires decision boundaries hθ+∆θ stay close therefore important deﬁne neighborhood parameterization-invariant. following pascanu bengio instead select using following objective optimization step solves gradient step minimizes conditional entropy subject constraint kullback-leibler divergence hθ+∆θ small corresponding lagrangian suggests instead minimize sequence optimization problems practice optimization problems solved approximately ﬁnite number stochastic gradient descent steps. denote number steps taken reﬁnement interval similar tarvainen valpola adam optimizer polyak averaging interpret hθn− teacher student model trained stay close teacher model seeking reduce cluster assumption violation. result denote model decision-boundary iterative reﬁnement training teacher weakly-supervised learning. sequence optimization problems natural interpretation exposes connection weakly-supervised learning. optimization problem teacher model hθn− pseudo-labels target samples noisy labels. rather naively training student model noisy labels additional training signal allows student model place decision boundaries data. clustering assumption holds initial noisy labels sufﬁciently similar true labels conditional entropy minimization improve placement decision boundaries domain adaptation. alternative interpretation dirt-t recursive extension vada pseudo-labeling target distribution constructs source domain sequence optimization problems seen sequence non-conservative domain adaptation problems hθn− true conditional label distribution target domain. since dh∆h strictly zero sequence optimization problems domain adversarial training longer necessary. furthermore minimization improve student classiﬁer smaller time source domain updated. principle method applied domain adaptation tasks long deﬁne reasonable notion neighborhood virtual adversarial training comparison saito french focus visual domain adaptation evaluate mnist mnist-m street view house numbers synthetic digits synthetic trafﬁc signs german trafﬁc signs recognition benchmark cifar- stl-. non-visual domain adaptation evaluate wi-fi activity recognition. architecture small digits trafﬁc sign wi-fi domain adaptation experiments larger domain adaptation cifar- stl-. architectures available appendix fair comparison additionally report performance source-only baseline models demonstrate signiﬁcant improvements attributable proposed method. replacing gradient reversal. contrast ganin lempitsky proposed implement domain adversarial training gradient reversal follow goodfellow instead optimize alternating updates discriminator encoder instance normalization. explored application instance normalization image preprocessing step. procedure makes classiﬁer invariant channel-wide shifts rescaling pixel intensities. discussion instance normalization domain adaptation provided appendix show figure effect applying instance normalization input image. figure effect applying instance normalization input image. clockwise direction mnist-m gtsrb svhn cifar-. quadrant original image bottom instance-normalized image. hyperparameters. task tuned four hyperparameters randomly selecting labeled target samples training using validation set. observed extensive hyperparameter-tuning necessary achieve state-of-the-art performance. experiments instance-normalized inputs restrict hyperparameter search whereas mnist consists black-and-white handwritten digits svhn consists crops colored street house numbers. mnist signiﬁcantly lower intrinsic dimensionality svhn adaptation mnist svhn especially challenging input pre-processed instance normalization. instance normalization applied achieve strong state-ofthe-art performance equally impressive margin-of-improvement source-only interestingly reducing reﬁnement interval taking noisier natural gradient steps occasionally able achieve accuracies high however high-variance associated this omit reporting conﬁguration table problem transferring synthetic images real images. digits dataset consist images generated windows fonts varying text positioning orientation background stroke color amount blur. signs gtsrb. setting provides additional demonstration adapting synthetic images real images. unlike digits svhn signs gtsrb contains classes instead cifar. stl- cifar- -class image datasets. datasets contain nine overlapping classes. following procedure french removed non-overlapping classes reduce -class classiﬁcation problem. achieve state-of-the-art performance adaptation directions. cifar achieve margin-of-improvement performance accuracy note stl- contains small training difﬁcult estimate conditional entropy thus making dirt-t unreliable cifar stl. wi-fi activity recognition. evaluate performance models non-visual domain adaptation task applied vada dirt-t wi-fi activity recognition dataset wi-fi activity recognition dataset classiﬁcation task takes wifi channel state information data stream input predict motion activity within indoor area output domain adaptation necessary training testing data collected different rooms denote rooms table shows vada signiﬁcantly improves classiﬁcation accuracy compared source-only dann respectively. however dirt-t lead improvements dataset. perform experiments appendix suggests vada already achieves strong clustering target domain dataset therefore dirt-t expected yield performance improvement. table additional comparison margin improvement computed taking reported performance model subtracting reported source-only performance respective papers. w.i.n.i. indicates with instance-normalized input. overall. achieve state-of-the-art results across tasks. fairer comparison π-model table provides improvement margin respective source-only performance reported paper. four tasks achieve substantial margin improvement compared previous models. remaining three tasks improvement margin source-only model worth noting dirt-t consistently improves upon vada. since dirt-t operates incrementally pushing decision boundaries away target domain data relies heavily cluster assumption. dirt-t’s empirical success therefore demonstrates effectiveness leveraging cluster assumption unsupervised domain adaptation deep neural networks. study relative contribution virtual adversarial training vada dirt-t objectives respectively) perform extensive ablation analysis table removal virtual adversarial training component denoted no-vat subscript. results show vadano-vat sufﬁcient out-performing dann task. ability dirt-tno-vat improve upon vadano-vat demonstrates effectiveness conditional entropy minimization. ultimately seven tasks virtual adversarial training conditional entropy minimization essential achieving best performance. empirical importance incorporating virtual adversarial training shows locally-lipschitz constraint beneﬁcial pushing classiﬁer decision boundaries away data. considering natural whether deﬁning neighborhood respect classiﬁer truly necessary. figure demonstrate svhn mnist cifar removal kl-term negatively impacts model. since mnist data manifold low-dimensional contains easily identiﬁable clusters applying naive gradient descent also boost test accuracy initial training. however without constraint classiﬁer sometimes deviate signiﬁcantly neighborhood previous classiﬁer resulting spikes kl-term correspond sharp drops target test accuracy. cifar data manifold much complex contains less obvious clusters naive gradient descent causes immediate decline target test accuracy. analyze behavior vada dirt-t showing t-sne embeddings last hidden layer model trained adapt mnist svhn. figure source-only training shows strong clustering mnist samples performs poorly svhn vada offers signiﬁcant improvement exhibits signs clustering svhn. dirt-t begins vada initialization enhances clustering resulting best performance mnist svhn. domain adversarial training layer ablation table comparison model behavior domain adversarial training applied various layers. denote last layer neural network ablatively domain adversarial training last eight layers. lower bound jensen-shannon divergence computed training logistic regression model predict domain origin given layer embeddings. table applied domain adversarial training various layers domain adversarial neural network trained adapt mnist svhn. exception layers experienced training instability general observation layer gets deeper additional capacity corresponding embedding function allows better matching source target distributions without hurting source generalization accuracy. demonstrates combination divergence high source accuracy imply better adaptation target domain. interestingly classiﬁer regularized locally-lipschitz vada combination divergence high source accuracy appears correlate strongly better adaptation. paper presented novel models domain adaptation inspired cluster assumption. ﬁrst model vada performs domain adversarial training added term penalizes violations cluster assumption. second model dirt-t extension vada recursively reﬁnes vada classiﬁer untethering model source training signal applying approximate natural gradients minimize cluster assumption violation. experiments demonstrate effectiveness cluster assumption vada achieves strong performance across several domain adaptation benchmarks dirt-t improves vada performance. proposed models open several possibilities future work. possibility apply dirt-t weakly supervised learning; another improve natural gradient approximation k-fac given strong performance models also recommend downstream domain adaptation applications. gratefully acknowledge funding adobe toyota research institute future life institute intel. also thank daniel levy shengjia zhao jiaming song insightful discussions anonymous reviewers helpful comments suggestions. shai ben-david john blitzer koby crammer alex kulesza fernando pereira jennifer wortman vaughan. theory learning different domains. machine learning shai ben-david tyler teresa d´avid p´al. impossibility theorems domain adaptation. proceedings thirteenth international conference artiﬁcial intelligence statistics konstantinos bousmalis nathan silberman david dohan dumitru erhan dilip krishnan. unsupervised pixel-level domain adaptation generative adversarial networks. arxiv preprint arxiv. konstantinos bousmalis george trigeorgis nathan silberman dilip krishnan dumitru erhan. domain separation networks. advances neural information processing systems william fedus mihaela rosca balaji lakshminarayanan andrew shakir mohamed goodfellow. many paths equilibrium gans need decrease adivergence every step. arxiv preprint arxiv. goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems jiayuan huang arthur gretton karsten borgwardt bernhard sch¨olkopf alex smola. correcting sample selection bias unlabeled data. advances neural information processing systems dong-hyun lee. pseudo-label simple efﬁcient semi-supervised learning method deep neural networks. workshop challenges representation learning icml volume mingsheng long jianmin wang michael jordan. learning transferable features deep adaptation networks. international conference machine learning scott reed honglak dragomir anguelov christian szegedy dumitru erhan andrew rabinovich. training deep neural networks noisy labels bootstrapping. arxiv preprint arxiv. ozan sener hyun song ashutosh saxena silvio savarese. learning transferrable representations unsupervised domain adaptation. advances neural information processing systems david vazquez antonio lopez javier marin daniel ponsa david geronimo. virtual real world adaptation pedestrian detection. ieee transactions pattern analysis machine intelligence siamak youseﬁ hirokazu narui sankalp dayal stefano ermon shahrokh valaee. survey behavior recognition using channel state information. ieee communications magazine table small large architectures. leaky relu parameter convolutional dense layers classiﬁer pre-activation batch-normalized. images resized note additive gaussian noise addition motivated initial experiments observed domain adversarial training appears contract feature space. observed extensive hyperparameter-tuning necessary achieve state-of-the-art performance. demonstrate this restrict hyperparameter search task experiments instance-normalized inputs. ﬁxed note decision turn often determined priori based prior belief regarding extent covariate shift. absence prior belief reliable choice table hyperparameters task without instance-normalized input. exception mnist svhn without instance-normalized input. speciﬁc case dh∆h sufﬁciently large conditional entropy minimization quickly ﬁnds degenerate solution target domain. counter this remove conditional entropy minimization vada. apply target-side conditional entropy minimization virtual adversarial training dirt-t. compensate lower dirt-t phase allow larger natural gradient steps. target domain mnist/mnist-m task sufﬁciently simple allocate iterations optimization problem cases reﬁnement interval apply adam optimizer polyak averaging vada trained iterations dirt-t takes vada initialization trained iterations number iterations chosen hyperparameter. note goodfellow gradient ln)) tends smaller norm initial training since latter rescales gradient /d). following observation replace gradient reversal procedure alternating minimization choice using gradient reversal versus alternating minimization reﬂects difference choice approximating mini-max using saturating versus non-saturating optimization initial experiments observed replacement gradient reversal alternating minimization stabilizes domain adversarial training. however encourage practitioners either optimization strategy applying vada. standard deviation across spatial dimensions. notable property instance normalization invariant channel-wide scaling shifting input elements. formally consider visual data application instance normalization input layer makes classiﬁer invariant channel-wide shifts scaling pixel intensities. visual tasks sensitivity channel-wide pixel intensity changes critical success classiﬁer. such instance normalization input help reduce dh∆h without hurting globally optimal classiﬁer. interestingly figure shows input instance normalization equivalent gray-scaling since color partially preserved. test effect instance normalization report results without instance-normalized inputs. denote source target distributions respectively source covariate distribution deﬁne random variable support supp analogously deﬁned target domain. subsets deﬁne probabilities support consider embedding function embedding space embedding classiﬁer -simplex. denote classiﬁer composite embedding function embedding classiﬁer. simplicity restrict analysis simple case i.e. furthermore assume exists subset impose similar condition joint distribution denote generalization error classiﬁer hypothesis spaces embedding function embedding classiﬁer. intuitively domain adversarial training operates hypothesis good source generalization error conjunction source-target feature matching implies good target generalization error. classiﬁers satisﬁes feature-matching constraint achieving source general∗ ization error worse optimal source-domain hard classiﬁer. sufﬁces show includes hypotheses perform poorly target domain. empty constructing element set. choose partitioning ﬁrst show provide intuition \u0001pt) potentially large consider hypothetical source target domains worst-case partitioning subject probability mass constraint simply consequently justiﬁcation domain adversarial training ¯h-divergence term smaller h∆h-divergence thus yielding tighter upper bound theorem however shall ¯h-divergence term fact maximal. choose partitionings analysis assumes inﬁnite capacity embedding functions ability solve optimization problems exactly. empirical success domain adversarial training suggests ﬁnite-capacity convolutional neural networks combined stochastic gradient-based optimization provides necessary regularization domain adversarial training work. theoretical characterization domain adversarial training case ﬁnite-capacity convolutional neural networks gradient-based learning remains challenging important open research problem. evaluate performance models non-visual domain adaptation task applied vada dirt-t wi-fi activity recognition dataset wi-fi activity recognition dataset classiﬁcation task takes wi-fi channel state information data stream input predict motion activity within indoor area output dataset collected data stream samples associated seven activities denoted fall walk pick down stand however joint distribution data stream motion activity changes depending room data collected. since data collected multiple rooms selected rooms constructed unsupervised domain adaptation task using room source domain room target domain. compare performance dann vada dirt-t wi-fi domain adaptation task table using hyperparameters table shows vada signiﬁcantly improves classiﬁcation accuracy compared source-only dann. however dirt-t lead improvements dataset. believe attributable vada successfully pushing decision boundary away data-dense regions target domain. result application dirt-t would lead better decision boundaries. validate hypothesis visualize t-sne embeddings vada dirt-t figure show vada already capable yielding strong clustering target domain. verify decision boundary indeed change signiﬁcantly additionally provide confusion matrix vada dirt-t predictions target domain", "year": 2018}