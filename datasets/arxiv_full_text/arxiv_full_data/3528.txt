{"title": "Moonshine: Distilling with Cheap Convolutions", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Model distillation compresses a trained machine learning model, such as a neural network, into a smaller alternative such that it could be easily deployed in a resource limited setting. Unfortunately, this requires engineering two architectures: a student architecture smaller than the first teacher architecture but trained to emulate it. In this paper, we present a distillation strategy that produces a student architecture that is a simple transformation of the teacher architecture. Recent model distillation methods allow us to preserve most of the performance of the trained model after replacing convolutional blocks with a cheap alternative. In addition, distillation by attention transfer provides student network performance that is better than training that student architecture directly on data.", "text": "model distillation compresses trained machine learning model neural network smaller alternative could easily deployed resource limited setting. unfortunately requires engineering architectures student architecture smaller ﬁrst teacher architecture trained emulate paper present distillation strategy produces student architecture simple transformation teacher architecture. recent model distillation methods allow preserve performance trained model replacing convolutional blocks cheap alternative. addition distillation attention transfer provides student network performance better training student architecture directly data. despite advances deep learning variety tasks deployment deep learning embedded devices e.g. mobile phones digital cameras vehicle navigation systems relatively slow resource constraints devices operate. memory-intensive neural networks don’t devices networks expensive? dominant run-time memory cost neural networks number parameters need stored. networks substantially fewer parameters without commensurate loss performance? possible take large pre-trained teacher network outputs training smaller student network distillation process. student network powerful trained solely training data closer performance larger teacher. lower-parameter student network typically architecture shallow thinner mean ﬁlters less channels teacher. possible arbitrarily approximate network another limit neural network performance least part training algorithm rather representational power. paper take alternative approach designing student networks. instead making networks thinner shallow take standard convolutional block networks possess replace cheaper convolution block keeping original architecture. example resnet standard block pair sequential convolutions. show comparable number parameters student networks retain architecture teacher cheaper convolutional blocks outperform student networks original blocks smaller architectures. easy implement deep learning framework; replacing convolutional blocks simple substitution existing architecture. furthermore optimisation scheme used teacher network repeated student making another round hyperparameter optimisation unnecessary. cheap convolutional blocks suggest described section well overview methods employ distillation. section train number student networks task image classiﬁcation cifar- cifar- datasets demonstrate cheap convolutions perform better traditional student networks given parameter cost. level parameter reduction competitive much complicated methods literature methods complementary though possible train resulting architectures directly demonstrably less effective distilling larger teacher model. parameters deep networks great deal redundancy; shown many predicted subset parameters however challenge remains good ways exploit redundancy without losing model accuracy. observation along desire efﬁciency improvements driven development smaller less computationally-intensive convolutions. prominent examples depthwise separable convolution applies separate convolutional kernel channel followed pointwise convolution channels; depthwise separable convolutions used several architectures explicitly adapted mobile devices howard however separating spatial channel-wise elements simplify convolution. authors propose breaking general convolution pointwise convolutions along different axes. authors wang start separable convolutions topological subdivisioning treat sections tensors separately bottleneck spatial dimensions. methods demonstrate models several times smaller original model maintaining accuracy. separable convolution expensive part pointwise convolution proposed operation could also grouped sets channels. however maintain connections channels helpful operation mixing channels together simply squared reduction achieved applying bottleneck channels spatial convolution paper examine potency separable bottleneck structure. work discussed thus section involves learning compressed network scratch. clear alternatives retraining reducing number parameters interested learning smaller network student distillation conjunction large pre-trained teacher network. small student complex function large deep teacher network theoretically approximated network single hidden layer enough units difﬁculty practice learning function. knowledge distillation proposes information logits learnt network train smaller student network. early experiments however modern deep architectures prove harder compress. example deep convolutional network cannot trivially replaced feedforward architecture methods proposed deal this. first romero authors linear activations intermediate points produce extra loss function. second attention transfer authors choose instead match activations taking mean channels. context paper found attention transfer effective experiments described section given large deep network performs well given task interested compressing network uses fewer parameters. ﬂexible widely applicable reduce number parameters model replace convolutional layers cheaper alternative. replacement invariably impairs performance reduced network trained directly data. fortunately able demonstrate modern distillation methods enable cheaper model performance closer original large fully-convolutional network. paper utilise compare different distillation methods learning smaller student network large pre-trained teacher network knowledge distillation attention transfer knowledge distillation denote cross entropy probability vectors assume dataset elements element denoted element corresponding one-hot class label denote one-hot vector corresponding given trained teacher network teacher outputs corresponding logits denoted likewise student network outputs logits student. perform knowledge distillation train student network minimise following loss function softmax function temperature parameter parameter controlling ratio terms. ﬁrst term standard cross entropy loss penalising student network incorrect classiﬁcations. second term minimised student network produces outputs similar teacher network. idea outputs teacher network contain additional beneﬁcial information beyond class prediction. attention transfer consider choice layers teacher network corresponding layers student network. chosen layer teacher network collect spatial activations channel vector collect likewise student network correspondingly collect hyperparameter. zagoruyko komodakis recommended using number channels layer words loss targeted difference spatial average squared activations spatial normalised overall activation norm. examine loss further. ﬁrst term standard cross entropy loss. second term however ensures spatial distribution student teacher activations similar selected layers network explanation networks paying attention things layers. large fully-connected layers longer commonplace convolutions make almost parameters modern networks. therefore desirable make smaller. here present several convolutional blocks introduced place standard block network substantially reduce parameter cost. first consider standard dimensional convolutional layer contains nout ﬁlters size nout number channels layer output number channels input kernel size convolution. modern neural networks almost always case nout. max. parameter cost layer ninnoutk bounded typical residual network block contains convolutions. refer standard block outlined table alternative full convolutions parameters scale approximately break convolution groups shown figure restricting convolutions channels within group groups obtain substantial reduction number parameters grouped computation example nout cost changes standard layer groups parameter convolutions hence reducing parameter cost factor provide cross-group mixing following grouped convolution pointwise convolution parameter cost nout change channel size occurs across pointwise convolution). refer substitution operator illustrate figure original resnet paper authors introduce bottleneck block parameterised denoted table input ﬁrst channels decreased factor pointwise convolution full convolution carried out. finally another pointwise convolution brings representation back desired nout. reduce parameter cost block even replacing full convolution grouped one; bottleneck grouped pointwise block referred illustrated figure substitute blocks compared table computational costs given. practice varying bottleneck size number groups network parameter numbers vary orders magnitude; enumerated examples given tables figure grouped convolutions operate passing independent ﬁlters tensor separated groups channel dimension. consider grouped convolution input output channels ﬁlters needs operate channels. reduces parameter cost convolution factor standard figure grouped pointwise block substitutes full convolutions standard block grouped convolution followed pointwise convolution. reduce parameters further pointwise bottleneck used grouped pointwise convolution using grouped convolutions bottlenecks common methods parameter reduction designing network architecture. easy implement deep learning framework. sparsity inducing methods approximate layers also provide advantages complementary approaches here. structured reductions grouped convolutions bottlenecks advantageous sparsity methods sparsity structure need stored. claim paper structured parameter reductions sufﬁcient achieve model compression results line state using effective model distillation. table convolutional blocks used paper standard block grouped pointwise block bottleneck block bottleneck grouped pointwise block blocks pre-activations conv refers convolution. gconv grouped convolution convx pointwise convolution. bn+relu refers batchnorm layer followed relu activation. assume input output block channels channel size change particular convolution unless written explicitly applicable number groups grouped convolution bottleneck contraction. give parameter cost convolutions block terms parameters. batch-norm parameter cost also given markedly smaller. section train evaluate number student networks distilled large teacher network. distil knowledge distillation attention transfer. also train networks without form distillation observe whether distillation process necessary obtain good performance. demonstrate high performance comes distillation cannot achieved directly training student networks using data. comparison also study student networks smaller architectures teacher. enables test block transformations propose simply matter distilling networks smaller numbers parameters. compare smaller student architectures student architectures implementing cheap substitute convolutional blocks different convolutional blocks summarised table student networks described detail section table summary wide resnet structures used experiments; matching zagoruyko komodakis bulk parameters {conv conv conv} consist blocks channel width controlled explore effect substituting blocks cheaper alternatives. classes refers number object classes perhaps unsurprisingly cifar- cifar-. experiments utilise competitive wide residual network architecture brieﬂy summarised table bulk network lies {conv conv conv} groups network depth determines number convolutional blocks groups network width denoted affects channel size ﬁlters blocks. note employ attention transfer student teacher outputs groups {conv conv conv} used second term equation teacher network wrn-- standard blocks. kernels used non-pointwise convolutions student teacher networks unless stated otherwise. student networks wrn-- using bottleneck block channel contraction wrn-- using grouped pointwise block group sizes number channels block. allows explore spectrum full convolutions fully separable convolutions wrn-- bottlenecked grouped pointwise block groups sizes number channels bottleneck. notation represents fully separable convolutions easily denote divisions thereof. also used observe effect extreme compression. zeros random crop taken. image left-right ﬂipped probability half. training conducted epochs using standard momentum ﬁxed initial learning rate learning rate reduced factor start epochs knowledge distillation used temperature attention transfer figure compares parameter cost student network test error cifar- obtained attention transfer. plot ideal network would bottom-left corner fascinating almost every network architecture teacher cheap convolutional blocks performs better given parameter budget reduced architecture networks standard blocks outperforms despite considerably fewer parameters several networks blocks signiﬁcantly outperform less parameters. figure test error parameters student networks learnt attention transfer cifar-. note x-axis logarithmically scaled. points curve correspond networks convolutional blocks reduced architectures. networks wrn-- architecture teacher cheap convolutional blocks blocks described detail table notice student networks cheap blocks outperform smaller architectures standard convolutions given parameter budget. encouraging signiﬁcant compression possible small losses; several networks perform almost well teacher considerably less parameters error close teacher ﬁfth parameters. less tenth parameters teacher cost increase error. similar change error compression rates exceed found contemporary papers cohen welling even simply switching convolutions smaller dilated equivalents allows half parameters similar performance. important lesson learnt regarding grouped pointwise convolutions. often used depthwise-separable form however networks half quarter number groups perform substantially better modest increase parameters. parameters compared error lower. make number groups smaller performance gets close teacher network structure getting closer closer original convolutions teacher number groups easy parameter tune trade performance smaller network. grouped pointwise convolutions also work well conjunction bottleneck size although large bottlenecks error increases rather signiﬁcantly seen despite this still comparable performance half parameters. observe similar trends cifar- figure figure test error parameters student networks learnt attention transfer cifar-. points curve correspond networks convolutional blocks reduced architectures. networks wrn-- architecture teacher cheap convolutional blocks also observe attention transfer teacher network substantially better knowledge distillation training network structure directly data. consider table shows attention transfer errors figure alongside networks trained knowledge distillation distillation cifar-. cases student network trained attention transfer better student network trained distillation process appears necessary. performances particularly impressive blocks error higher teacher despite network half many parameters. also noticeable knowledge distillation gives similar even worse performance student network trained itself. conclusion mirrored cifar- training romero note performance issues knowledge distillation occur networks depth layers. zagoruyko komodakis also observe experiments training imagenet would converge using knowledge distillation. training large deep model prohibitively time consuming design model compression strategy order deploy many problems also difﬁcult achieve desired performance smaller model. demonstrated model compression strategy fast apply doesn’t require additional engineering. furthermore optimisation algorithm larger model sufﬁcient train cheaper student model. cheap convolutions used paper chosen ease implementation. future work could investigate complicated approximate operations described moczulski could make difference convolutions ﬁnal layers network. could also make custom blocks generated large scale black optimisation equally many methods rank approximations could applicable hope work encourages others consider cheapening convolutions compression strategy. acknowledgements. project received funding european union’s horizon research innovation programme grant agreement work supported swiss state secretariat education‚ research innovation contract number opinions expressed arguments employed herein necessarily reﬂect ofﬁcial views funding bodies. table student network test error cifar-. network wide resnet depth-width given ﬁrst column block type second. standard convolutional block. standard block dilated kernels. grouped pointwise block groups. bottleneck block contraction bottleneck block contraction grouped convolution groups. refers channel width block refers channel width bottleneck applicable. total parameter cost network given third column. errors reported learning distillation knowledge distillation teacher attention transfer teacher teacher used training given ﬁrst row. table shows attention transfer possible number parameters network retain high performance similar number parameters students cheap convolutional blocks outperform expensive convolutions smaller architectures. table student network test error cifar-. network wide resnet depth-width given ﬁrst column block type second. blocks described detail section total parameter cost network given third column. errors reported learning distillation knowledge distillation teacher attention transfer teacher teacher used training given ﬁrst row. taco cohen welling. group equivariant convolutional networks. maria florina balcan kilian weinberger editors proceedings international conference machine learning volume proceedings machine learning research pages york york pmlr. http//proceedings.mlr.press/v/cohenc.html. misha denil babak shakibi laurent dinh marc’aurelio ranzato nando freitas. predicting parameters deep learning. advances neural information processing systems timur garipov dmitry podoprikhin alexander novikov dmitry vetrov. ultimate tensorization compressing convolutional layers alike. corr abs/. http//arxiv.org/abs/.. song huizi william dally. deep compression compressing deep neural networks pruning trained quantization huffman coding. corr abs/. http//arxiv.org/abs/.. kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition andrew howard menglong chen dmitry kalenichenko weijun wang tobias weyand marco andreetto hartwig adam. mobilenets efﬁcient convolutional neural networks mobile vision applications. corr abs/. http//arxiv.org/abs/.. forrest iandola matthew moskewicz khalid ashraf song william dally kurt keutzer. squeezenet alexnet-level accuracy fewer parameters model size. corr abs/. http//arxiv.org/abs/ sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning pages jonghoon aysegul dundar eugenio culurciello. flattened convolutional neural networks feedforward acceleration. international conference learning representations adam paszke gross soumith chintala gregory chanan. pytorch tensors dynamic neural networks python strong acceleration. https//github. com/pytorch/pytorch. accessed october adriana romero nicolas ballas samira ebrahimi kahou antoine chassang carlo gatta yoshua bengio. fitnets hints thin deep nets. corr abs/. http//arxiv.org/abs/.. tara sainath brian kingsbury vikas sindhwani ebru arisoy bhuvana ramabhadran. low-rank matrix factorization deep neural network training high-dimensional output targets. ieee international conference acoustics speech signal processing gregor urban krzysztof geras samira ebrahimi kahou ozlem aslan shengjie wang rich caruana abdelrahman mohamed matthai philipose matt richardson. deep convolutional nets really need deep convolutional? international conference learning representations saining ross girshick piotr dollár zhuowen kaiming aggregated residual transformations deep neural networks. proceedings ieee conference computer vision pattern recognition zichao yang marcin moczulski misha denil nando freitas alex smola song ziyu wang. deep fried convnets. proceedings ieee international conference computer vision sergey zagoruyko nikos komodakis. paying attention attention improving performance convolutional neural networks attention transfer. international conference learning representations barret zoph vijay vasudevan jonathon shlens quoc learning transferable architectures scalable image recognition. corr abs/. http //arxiv.org/abs/..", "year": 2017}