{"title": "Frustratingly Short Attention Spans in Neural Language Modeling", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Neural language models predict the next token using a latent representation of the immediate token history. Recently, various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed. For predicting the next token, these models query information from a memory of the recent history which can facilitate learning mid- and long-range dependencies. However, conventional attention mechanisms used in memory-augmented neural language models produce a single output vector per time step. This vector is used both for predicting the next token as well as for the key and value of a differentiable memory of a token history. In this paper, we propose a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. This model outperforms existing memory-augmented neural language models on two corpora. Yet, we found that our method mainly utilizes a memory of the five most recent output representations. This led to the unexpected main finding that a much simpler model based only on the concatenation of recent output representations from previous time steps is on par with more sophisticated memory-augmented neural language models.", "text": "michał daniluk rockt¨aschel johannes welbl sebastian riedel department computer science university college london michal.daniluk.ucl.ac.uk {t.rocktaschelj.welbls.riedel}cs.ucl.ac.uk neural language models predict next token using latent representation immediate token history. recently various methods augmenting neural language models attention mechanism differentiable memory proposed. predicting next token models query information memory recent history facilitate learning midlong-range dependencies. however conventional attention mechanisms used memoryaugmented neural language models produce single output vector time step. vector used predicting next token well value differentiable memory token history. paper propose neural language model key-value attention mechanism outputs separate representations value differentiable memory well encoding next-word distribution. model outperforms existing memoryaugmented neural language models corpora. found method mainly utilizes memory recent output representations. unexpected main ﬁnding much simpler model based concatenation recent output representations previous time steps sophisticated memory-augmented neural language models. core language models ability infer next word given context. requires representing context-speciﬁc dependencies sequence across different time scales. hand classical n-gram language models capture relevant dependencies words short time distances explicitly suffer data sparsity. neural language models hand maintain update dense vector representation sequence time dependencies captured implicitly recent extension neural sequence models attention mechanisms capture long-range connections directly. however argue applying attention mechanism directly neural language models requires output vectors fulﬁll several purposes time need encode distribution predicting next token serve compute attention vector well encode relevant content inform future predictions. hypothesize overloaded output representations makes training model difﬁcult propose modiﬁcation attention mechanism separates functions explicitly inspired miller reed freitas gulcehre speciﬁcally every time step neural language model outputs three vectors. ﬁrst used encode next-word distribution second serves third value attention mechanism. term model key-value-predict attention show outperforms existing memory-augmented neural language models children’s book test corpus wikipedia articles. however observed model pays attention mainly previous memories. thus also experimented much simpler model uses concatenation output vectors previous time steps predicting next token. simple model sophisticated memory-augmented neural language models. thus main ﬁnding modeling short attention spans properly works well provides notable improvements neural language model attention. conversely seems notoriously hard train neural language models leverage long-range dependencies. paper investigate various memory-augmented neural language models compare previous architectures. contributions threefold propose key-value attention mechanism uses speciﬁc output representations querying sliding-window memory previous token representations demonstrate architecture outperforms previous memory-augmented neural language models mainly utilizes memory previous representations ﬁnally based observation experiment much simpler effective model uses concatenation three previous output representations predict next word. following discuss methods extending neural language models differentiable memory. ﬁrst present standard attention mechanism language modeling subsequently introduce methods separating usage output vectors attention mechanism using dedicated value separating value memory value representation encodes next-word distribution finally describe simple method concatenates previous output representations predicting next token augmenting neural language model attention straight-forward. simply take previous output vectors memory rk×l output dimension long short-term memory unit memory could principle contain previous output representations practical reasons keep sliding window previous outputs. output representation time step vector ones. attention weights computed comparison current previous lstm outputs. subsequently context vector calculated previous output vectors weighted respective attention value. formulated rk×k trainable projection matrices trainable vector. ﬁnal representation encodes next-word distribution computed non-linear combination attention-weighted representation previous outputs ﬁnal output vector inspired miller reed freitas gulcehre introduce key-value attention model separates output vectors keys used calculating attention distribution value part used encoding next-word distribution context representation. model depicted figure formally rewrite equations follows essence equation compares time step previous keys calculate attention distribution used equation obtain weighted context representation values associated keys. even key-value separation potential problem representation still used encoding probability distribution next word retrieval memory attention later. thus experimented another extension model separate value predict representation latter used encoding next-word distribution equations replaced precisely output vector divided three equal parts value predict. implementation simply split output vector hidden dimension key-value-predict attention model needs multiplicative three. consequently dimensions hidden dimension neural language models often work best combination traditional n-gram models since former excel generalization latter ensure memorization. addition initial experiments memory-augmented neural language models found usually previous output representations utilized. line observations tran hence experiment much simpler architecture depicted figure instead attention mechanism output representations previous time steps directly used calculate next-word probabilities. speciﬁcally every time step split lstm output vectors trainable projection matrix. model related higher-order rnns difference incorporate output vectors previous steps hidden state predicting next word. furthermore note time step ﬁrst part output vector contribute predicting next word contribute predicting second word thereafter output second part vectors previous time-steps used score next word call resulting model n-gram rnn. early attempts using memory neural networks undertaken taylor steinbuch piske performing nearest-neighbor operations input vectors ﬁtting parametric models retrieved sets. dedicated external memory neural architectures recently witnessed increased interest. weston introduced memory networks explicitly segregate memory storage computation neural network sukhbaatar trained model end-to-end attention-based memory addressing mechanism. neural turing machines graves external differentiable memory read-write functions controller recurrent neural network shown promising results simple sequence tasks copying sorting. models make external memory whereas model directly uses short sequence history tokens dynamically populate addressable memory. sequence modeling rnns lstms maintain internal memory state process input sequence. attending previous state outputs encoder improved performances wide range tasks including machine translation recognizing textual entailment sentence summarization image captioning speech recognition recently cheng proposed architecture modiﬁes standard lstm replacing memory cell memory network another proposal conditioning previous output representations higher-order recurrent neural networks soltani jiang found useful include information multiple preceding states computing next state. previous work centers around preceding state vectors whereas investigate attention mechanisms outputs i.e. vectors used predicting next word. furthermore instead pooling attention vectors calculate context representation previous memories. yang introduced reference-aware neural language model every position latent variable determines source target token generated e.g. copying entries table referencing entities mentioned earlier. another class models include memory sequence modeling recurrent memory networks here memory block accesses recent input words selectively attend relevant word representations global vocabulary. rmns global memory input word vector look-up tables attention mechanism consequently large number trainable parameters. instead proposed models need much fewer parameters producing vectors attended future seen memory dynamically populated language model. finally functional separation look-up keys memory content found useful memory networks neural programmer-interpreters dynamic neural turing machines fast associative memory apply extend principle neural language models. evaluate models different corpora language modeling. ﬁrst subset wikipedia corpus. consists english wikipedia articles belonging following categories people cities countries universities novels. chose categories expect articles categories often contain references previously mentioned entities. subsequently split corpus train development test part resulting corpora words words respectively. numbers dedicated numerical symbol restrict vocabulary frequent words encompassing training vocabulary. words replaced symbol. average length sentences tokens. addition wikipedia corpus also experiments children’s book test corpus designed cloze-style question-answering paper test well language models exploit wider linguistic context. adam initial learning rate mini-batch size optimization. furthermore apply gradient clipping gradient norm bias lstm’s forget gate initialized parameters initialized uniformly range backpropagation time used train network steps unrolling. reset hidden states articles wikipedia corpus stories respectively. take best conﬁguration based performance validation evaluate test set. ﬁrst experiments explore well proposed models tran al.’s recurrentmemory model make histories varying lengths. perplexity results different attention window sizes wikipedia corpus summarized figure average attention models speciﬁc positions history illustrated figure observed although models attend tokens past often recurrent-memory model attending longer history signiﬁcantly improve perplexity attentive model. much simpler n-gram model achieves comparable results seems work best history previous three output vectors result choose -gram model following n-gram experiments. model humans kneser-ney kneser-ney cache lstm memory network reader ensemble reader greedy ensemble qann hops glove reader single model reader mode reader ensemble epireader ensemble fofe hornn gated hornn lstm attention key-value key-value-predict -gram next experiments compared proposed models variety state-of-the-art models wikipedia corpora. results shown figure respectively. note models presented achieve state-of-the-art language models tailored towards cloze-sytle question answering. thus merely corpus comparing different neural language model architectures. reimplemented recurrent-memory model tran temporal matrix gating composition function figure attention weights key-value-predict model randomly sampled wikipedia article average attention weight distribution whole wikipedia test attention key-value key-value-predict models rightmost positions represent recent history. ensure comparable number parameters vanilla lstm model adjusted hidden size models roughly total number model parameters. attention window size n-gram model according best validation perplexity wikipedia corpus. discuss results detail. attention using neural language model attention mechanism dynamically populated memory observed points lower perplexity vanilla lstm wikipedia notable differences predicting verbs prepositions cbt. indicates incorporating mechanisms querying previous output vectors useful neural language modeling. key-value decomposing output vector key-value paired memory improves perplexity points compared baseline lstm points compared model. again small improvements. key-value-predict separating output vector value next-word prediction part lowest perplexity gain points baseline lstm points compared points compared splitting output value. accuracy increase percentage points verbs prepositions. stated earlier performance key-value-predict model improve signiﬁcantly increasing attention window size. leads conclusion none attentive models investigated paper utilize large memory previous token representations. moreover none presented methods differ signiﬁcantly predicting common nouns named entities cbt. n-gram main ﬁnding simple modiﬁcation using output vectors previous time steps next-word prediction leads perplexities better complicated neural language models attention. speciﬁcally -gram achieves slightly worse perplexities key-value-predict architecture. paper observed using attention mechanism neural language modeling separate output vectors value predict part outperform simpler attention mechanisms wikipedia corpus children book test however found attentive neural language models mainly utilize memory recent history fail exploit long-range dependencies. fact much simpler n-gram model uses concatenation output representations previous three time steps sophisticated memory-augmented neural language models. training neural language models take long-range dependencies account seems notoriously hard needs investigation. thus future work want investigate ways encourage attending longer history instance forcing model ignore local context allow attention output representations behind local history. work supported microsoft research engineering physical sciences research council scholarship programmes allen distinguished investigator award marie curie career integration award. ciprian chelba tomas mikolov mike schuster thorsten brants phillipp koehn tony robinson. billion word benchmark measuring progress statistical language modeling. arxiv preprint arxiv. shihao vishwanathan nadathur satish michael anderson pradeep dubey. blackout speeding recurrent neural network language models large vocabularies. iclr alexander miller adam fisch jesse dodge amir-hossein karimi antoine bordes jason weston. key-value memory networks directly reading documents. arxiv preprint arxiv. williams niranjani prasad david mrva tony robinson. scaling recurrent neural network language models. ieee international conference acoustics speech signal processing ieee kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation visual attention. icml", "year": 2017}