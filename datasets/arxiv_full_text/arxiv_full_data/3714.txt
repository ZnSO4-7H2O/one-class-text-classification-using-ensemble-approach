{"title": "Physics Informed Deep Learning (Part I): Data-driven Solutions of  Nonlinear Partial Differential Equations", "tag": ["cs.AI", "cs.LG", "cs.NA", "math.DS", "stat.ML"], "abstract": "We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information. In this first part, we demonstrate how these networks can be used to infer solutions to partial differential equations, and obtain physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters.", "text": "introduce physics informed neural networks neural networks trained solve supervised learning tasks respecting given physics described general nonlinear partial diﬀerential equations. part treatise present developments context solving main classes problems data-driven solution data-driven discovery partial diﬀerential equations. depending nature arrangement available data devise distinct classes algorithms namely continuous time discrete time models. resulting neural networks form class data-eﬃcient universal function approximators naturally encode underlying physical laws prior information. ﬁrst part demonstrate networks used infer solutions partial diﬀerential equations obtain physics-informed surrogate models fully diﬀerentiable respect input coordinates free parameters. explosive growth available data computing resources recent advances machine learning data analytics yielded transformative results across diverse scientiﬁc disciplines including image recognition natural language processing cognitive science genomics however often course analyzing complex physical biological engineering systems cost data acquisition prohibitive inevitably faced challenge drawing conclusions making decisions partial information. small data regime vast majority state-of-the machine learning techniques lacking robustness fail provide guarantees convergence. ﬁrst sight task training deep learning algorithm accurately identify nonlinear potentially high-dimensional input output data pairs seems best naive. coming rescue many cases pertaining modeling physical biological systems exist vast amount prior knowledge currently utilized modern machine learning practice. principled physical laws govern time-dependent dynamics system empirical validated rules domain expertise prior information regularization agent constrains space admissible solutions manageable size return encoding structured information learning algorithm results amplifying information content data algorithm sees enabling quickly steer towards right solution generalize well even training examples available. ﬁrst glimpses promise exploiting structured prior information construct data-eﬃcient physics-informed learning machines already showcased recent studies there authors employed gaussian process regression devise functional representations tailored given linear operator able accurately infer solutions provide uncertainty estimates several prototype problems mathematical physics. extensions nonlinear problems proposed subsequent studies raissi context inference systems identiﬁcation. despite ﬂexibility mathematical elegance gaussian processes encoding prior information treatment nonlinear problems introduces important limitations. first authors locally linearize nonlinear terms time thus limiting applicability proposed methods discrete-time domains compromising accuracy predictions strongly nonlinear regimes. secondly bayesian nature gaussian process regression requires certain prior assumptions limit representation capacity model give rise robustness/brittleness issues especially nonlinear problems work take diﬀerent approach employing deep neural networks leverage well known capability universal function approximators setting directly tackle nonlinear problems without need committing prior assumptions linearization local time-stepping. exploit recent developments automatic diﬀerentiation useful perhaps underused techniques scientiﬁc computing diﬀerentiate neural networks respect input coordinates model parameters obtain physics informed neural networks. neural networks constrained respect symmetry invariance conservation principles originating physical laws govern observed data modeled general time-dependent nonlinear partial diﬀerential equations. simple powerful construction allows tackle wide range problems computational science introduces potentially disruptive technology leading development data-eﬃcient physics-informed learning machines classes numerical solvers partial diﬀerential equations well data-driven approaches model inversion systems identiﬁcation. general work foundations paradigm modeling computation enriches deep learning longstanding developments mathematical physics. developments presented context main problem classes data-driven solution data-driven discovery partial diﬀerential equations. consider parametrized nonlinear partial diﬀerential equations general form operator parametrized setup encapsulates wide range problems mathematical physics including conservation laws diﬀusion processes advection-diﬀusion-reaction systems kinetic equations. motivating example dimensional burgers’ equation corresponds case partial diﬀerentiation either time space. given noisy measurements system interested solution distinct problems. ﬁrst problem predictive inference ﬁltering smoothing data driven solutions partial diﬀerential equations states given ﬁxed model parameters said unknown hidden state system? second problem learning system identiﬁcation data-driven discovery partial diﬀerential equations stating parameters best describe observed data? denotes latent solution nonlinear diﬀerential operator subset follows forth distinct classes algorithms namely continuous discrete time models highlight properties performance lens diﬀerent benchmark problems. code data-sets accompanying manuscript available https//github.com/maziarraissi/pinns. proceed approximating deep neural network. assumption along equation result physics informed neural network network derived applying chain rule diﬀerentiating compositions functions using automatic diﬀerentiation example consider burgers’ equation. equation arises various areas applied mathematics including ﬂuid mechanics nonlinear acoustics dynamics traﬃc fundamental partial diﬀerential equation derived navier-stokes equations velocity ﬁeld dropping pressure gradient term. small values viscosity parameters burgers’ equation lead shock formation notoriously hard resolve classical numerical methods. space dimension burger’s equation along dirichlet boundary conditions reads proceed approximating deep neural network. highlight simplicity implementing idea included python code snippet using tensorﬂow currently popular well documented open source libraries machine learning computations. simply deﬁned ui}nu here denote initial boundary training data f}nf specify collocations points loss corresponds initial boundary data enforces structure imposed equation ﬁnite collocation points. benchmarks considered work total number training data relatively small chose optimize loss functions using l-bfgs; quasi-newton fullbatch gradient-based optimization algorithm larger data-sets computationally eﬃcient mini-batch setting readily employed using stochastic gradient descent modern variants despite fact theoretical guarantee procedure converges global minimum empirical evidence indicates that given partial diﬀerential equation well-posed solution unique method capable achieving good prediction accuracy given suﬃciently expressive neural network architecture suﬃcient number collocation points general observation deeply relates resulting optimization landscape induced mean square error loss equation deﬁnes open question research sync recent theoretical developments deep learning here test robustness proposed methodology using series systematic sensitivity studies accompany numerical results presented following. figure summarizes results data-driven solution burgers equation. speciﬁcally given randomly distributed initial boundary data learn latent solution training parameters -layer deep neural network using mean squared error loss hidden layer contained neurons hyperbolic tangent activation function. general neural network given suﬃcient approximation capacity order accommodate anticipated complexity however example choice aims highlight robustness proposed method respect well known issue over-ﬁtting. speciﬁcally term equation acts regularization mechanism penalizes solutions satisfy equation therefore property physics informed neural networks eﬀectively trained using small data sets; setting often encountered study physical systems cost data acquisition prohibitive. panel figure shows predicted spatio-temporal solution along locations initial boundary training data. must underline that unlike classical numerical method solving partial diﬀerential equations prediction obtained without sort discretization spatio-temporal domain. exact solution problem analytically available resulting prediction error orders magnitude lower reported previous work data-driven solution partial diﬀerential equation using gaussian processes detailed assessment predicted solution presented bottom panel ﬁgure particular present comparison exact predicted solutions diﬀerent time instants using handful initial boundary data physics informed neural network accurately capture intricate nonlinear behavior burgers’ equation leads development sharp internal layer around latter notoriously hard accurately resolve classical numerical methods requires laborious spatio-temporal discretization equation analyze performance method performed following systematic studies quantify predictive accuracy diﬀerent number training collocation points well diﬀerent neural network architectures. table report resulting relative error diﬀerent number initial boundary training data diﬀerent number collocation points keeping -layer network architecture ﬁxed. general trend shows increased prediction accuracy total number training data increased given suﬃcient number collocation points observation highlights strength physics informed neural networks encoding structure underlying physfigure burgers’ equation predicted solution along initial boundary training data. addition using collocation points generated using latin hypercube sampling strategy. bottom comparison predicted exact solutions corresponding three temporal snapshots depicted white vertical relative diﬀerent number hidden layers diﬀerent number neurons layer total number training collocation points kept ﬁxed respectively. expected observe number layers neurons increased diﬀerent number initial boundary training data diﬀerent number collocation points here network architecture ﬁxed layers neurons hidden layer. example aims highlight ability method handle periodic boundary conditions complex-valued solutions well diﬀerent types nonlinearities governing partial diﬀerential equations. one-dimensional nonlinear schr¨odinger equation classical ﬁeld equation used study quantum mechanical systems including nonlinear wave propagation optical ﬁbers and/or waveguides bose-einstein condensates plasma waves. optics nonlinear term arises intensity dependent index refraction given material. similarly nonlinear term bose-einstein condensates result mean-ﬁeld interactions interacting n-body system. nonlinear schr¨odinger equation along here corresponds collocation points boundary represents collocation points consequently corresponds loss initial data enforces periodic boundary conditions penalizes schr¨odinger equation satisﬁed collocation points. integrated equation ﬁnal time using chebfun package spectral fourier discretization modes fourth-order explicit runge-kutta temporal integrator time-step data-driven setting observe measurements latent function time particular training consists total data points randomly parsed full high-resolution data-set well randomly sampled collocation points enforcing periodic boundaries. moreover assumed randomly sampled collocation points used enforce equation inside solution domain. randomly sampled point locations generated using space ﬁlling latin hypercube sampling strategy goal infer entire spatio-temporal solution schr¨odinger equation chose jointly represent latent function using -layer deep neural network neurons layer hyperbolic tangent activation function. figure summarizes results experiment. speciﬁcally panel ﬁgure shows magnitude predicted spatio-temporal solution sented bottom panel figure particular present comparison exact predicted solutions diﬀerent time instants using handful initial data physics informed neural network accurately capture intricate nonlinear behavior schr¨odinger equation. potential limitation continuous time neural network models considered stems need large number collocation points order enforce physics informed constraints entire spatiotemporal domain. although poses signiﬁcant issues problems spatial dimensions introduce severe bottleneck higher dimensional problems total number collocation points needed boundary training data. addition using collocation points generated using latin hypercube sampling strategy. bottom comparison predicted exact solutions corresponding three temporal snapshots depicted dashed vertical globally enforce physics informed constrain increase exponentially. next section forth diﬀerent approach circumvents need collocation points introducing structured neural network representation leveraging classical runge-kutta time-stepping schemes highlight features discrete time representation revisit problem data-driven solution burgers’ equation. case nonlinear operator equation given here {xni uni}nn corresponds data time runge-kutta scheme allows infer latent solution sequential fashion. starting initial data {xni uni}nn time data domain boundaries aforementioned loss function train networks predict solution time tn+. runge-kutta time-stepping scheme would prediction initial data next step proceed train predict etc. step time. classical numerical analysis steps usually conﬁned small stability constraints explicit schemes computational complexity constrains implicit formulations constraints become severe total number runge-kutta stages increased problems practical interest needs take thousands millions steps solution resolved desired ﬁnal time. sharp contrast classical methods employ implicit runge-kutta schemes arbitrarily large number stages eﬀectively extra cost. enables take large time steps retaining stability high predictive accuracy therefore allowing resolve entire spatio-temporal solution single step. result applying process burgers’ equation presented ﬁgure illustration purposes start initial data employ physics informed neural network induced implicit runge-kutta scheme stages predict solution time single step. theoretical error estimates scheme predict figure burgers equation solution along location initial training snapshot ﬁnal prediction snapshot bottom initial training data ﬁnal prediction snapshots depicted white vertical lines orders magnitude lower reported entirely attributed neural network’s capacity approximate well degree squared errors loss allows interpolation training data. network architecture used consists layers neurons hidden layer. detailed systematic study quantify eﬀect diﬀerent network architectures presented table keeping number runge-kutta stages ﬁxed time-step size varied number hidden layers number neurons layer parameters controlling performance discrete time algorithm total number runge-kutta stages time-step size table summarize results extensive systematic study network architecture hidden layers neurons layer vary number runge-kutta stages time-step size speciﬁcally cases numbers stages fail yield accurate results time-step size large. instance case corresponding classical trapezoidal rule case corresponding th-order gauss-legendre method cannot retain predictive accuracy time-steps larger thus mandating solution strategy multiple time-steps small size. hand ability push number runge-kutta stages even higher allows take large time steps eﬀectively resolve solution single step without sacriﬁcing accuracy predictions. moreover numerical stability sacriﬁced either implicit runge-kutta family time-stepping schemes remain a-stable regardless order thus making ideal stiﬀ problems properties unprecedented algorithm implementation simplicity illustrate highlights discrete time approach. example aims highlight ability proposed discrete time models handle diﬀerent types nonlinearity governing partial differential equation. consider allen-cahn equation along periodic boundary conditions allen-cahn equation well-known equation area reactiondiﬀusion systems. describes process phase separation multicomponent alloy systems including order-disorder transitions. allencahn equation nonlinear operator equation given corresponds data time generated training test data-set simulating allen-cahn equation using conventional spectral methods. speciﬁcally starting initial condition assuming periodic boundary conditions example assume initial data points randomly sub-sampled exact solution time goal predict solution time using single time-step size employ discrete time physics informed neural network hidden layers neurons layer output layer predicts quantities interest corresponding rungekutta stages un+ci solution ﬁnal time un+. figure summarizes predictions network trained using loss function equation evidently despite complex dynamics leading solution sharp internal layers able obtain accurate prediction solution using small number scattered measurements figure allen-cahn equation solution along location initial training snapshot ﬁnal prediction snapshot bottom initial training data ﬁnal prediction snapshots depicted white vertical lines physical laws govern given data-set described partial diﬀerential equations. work design data-driven algorithms inferring solutions general nonlinear partial diﬀerential equations constructing computationally eﬃcient physics-informed surrogate models. resulting methods showcase series promising results diverse collection problems computational science open path endowing deep learning powerful capacity mathematical physics model world around deep learning technology continuing grow rapidly terms methodological algorithmic developments believe timely contribution beneﬁt practitioners across wide range scientiﬁc domains. speciﬁc applications readily enjoy beneﬁts include limited data-driven forecasting physical processes model predictive control multi-physics/multi-scale modeling simulation. must note however proposed methods viewed replacements classical numerical methods solving partial diﬀerential equations methods matured last years many cases meet robustness computational eﬃciency standards required practice. message here advocated section classical methods rungekutta time-stepping schemes coexist harmony deep neural networks oﬀer invaluable intuition constructing structured predictive algorithms. moreover implementation simplicity latter greatly favors rapid development testing ideas potentially opening path data-driven scientiﬁc computing. highlighted second part paper physics informed neural networks test data-driven discovery partial diﬀerential equations. finally terms future work pressing question involves addressing problem quantifying uncertainty associated neural network predictions. although important element naturally addressed previous work employing gaussian processes captured proposed methodology present form requires investigation. work received support darpa equips grant muri/aro grant wnf--- afosr grant fa---. data codes used manuscript publicly available github https//github.com/maziarraissi/pinns.", "year": 2017}