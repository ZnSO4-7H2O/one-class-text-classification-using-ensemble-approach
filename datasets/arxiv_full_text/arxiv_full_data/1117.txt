{"title": "Latent Constraints: Learning to Generate Conditionally from  Unconditional Generative Models", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Deep generative neural networks have proven effective at both conditional and unconditional modeling of complex data distributions. Conditional generation enables interactive control, but creating new controls often requires expensive retraining. In this paper, we develop a method to condition generation without retraining the model. By post-hoc learning latent constraints, value functions that identify regions in latent space that generate outputs with desired attributes, we can conditionally sample from these regions with gradient-based optimization or amortized actor functions. Combining attribute constraints with a universal \"realism\" constraint, which enforces similarity to the data distribution, we generate realistic conditional images from an unconditional variational autoencoder. Further, using gradient-based optimization, we demonstrate identity-preserving transformations that make the minimal adjustment in latent space to modify the attributes of an image. Finally, with discrete sequences of musical notes, we demonstrate zero-shot conditional generation, learning latent constraints in the absence of labeled data or a differentiable reward function. Code with dedicated cloud instance has been made publicly available (https://goo.gl/STGMGx).", "text": "deep generative neural networks proven effective conditional unconditional modeling complex data distributions. conditional generation enables interactive control creating controls often requires expensive retraining. paper develop method condition generation without retraining model. post-hoc learning latent constraints value functions identify regions latent space generate outputs desired attributes conditionally sample regions gradient-based optimization amortized actor functions. combining attribute constraints universal realism constraint enforces similarity data distribution generate realistic conditional images unconditional variational autoencoder. further using gradient-based optimization demonstrate identity-preserving transformations make minimal adjustment latent space modify attributes image. finally discrete sequences musical notes demonstrate zero-shot conditional generation learning latent constraints absence labeled data differentiable reward function. code dedicated cloud instance made publicly available generative modeling complicated data images audio long-standing challenge machine learning. unconditional sampling interesting technical problem arguably limited practical interest right needs non-speciﬁc image simply pull something random unfathomably vast media databases web. naive approach work conditional sampling since attributes speciﬁed becomes exponentially less likely satisfactory example pulled database. might also want modify attributes object preserving core identity. crucial tasks creative applications typical user desires ﬁne-grained controls enforce user-speciﬁed constraints training time either training curated subset data conditioning variables. approaches effective enough labeled data available require expensive model retraining constraints leverage commonalities tasks. deep latent-variable models generative adversarial networks variational autoencoders learn unconditionally generate realistic varied outputs sampling semantically structured latent space. might hope leverage structure creating conditional controls sampling transformations here show constraints enforced post-hoc pre-trained unsupervised generative models. approach removes need retrain model constraints allowing users easily deﬁne custom behavior. separate problem creating unsupervised model learns reconstruct data latent embeddings leveraging latent structure exposed embedding space source prior knowledge upon impose behavioral constraints. figure diagram latent constraints vae. critic dattr predict regions latent space generate outputs desired attributes another critic drealism predict regions high mass marginal posterior training data. begin pretraining standard emphasis achieving good reconstructions. train actor-critic pair constraint-satisfaction labels train discriminate encodings actual data versus latent vectors sampled prior transformed prior samples similar conditional operate concatenation binary attribute vector allowing learn conditional mappings latent space. optimizer separate attribute discriminator dattr trained latent vector optimized reduce cost dattr drealism. sample intersection regions either gradient-based optimization amortized generator shift latent samples either prior sampling) data transformation). show possible generate conditionally unconditional model learning critic function latent space generating high-value samples either gradient-based optimization amortized actor function even nondifferentiable decoder focusing vaes address tradeoff reconstruction quality sample quality enforcing universal realism constraint requires samples latent space indistinguishable encoded data start reconstruct inputs well able apply identitypreserving transformations making minimal adjustment latent space needed satisfy desired constraints. example adjust person’s expression hair result still clearly identiﬁable person contrasts pure gan-based transformation approaches often fail preserve identity. zero-shot conditional generation. using samples generate exemplars learn actor-critic pair satisﬁes user-speciﬁed rule-based constraints absence labeled data. decoder-based deep generative models vaes gans generate samples approximate population distribution passing samples simple tractable distribution deep neural network. gans trained fool auxiliary classiﬁer tries learn distinguish real synthetic samples. vaes data figure typical vaes pixel-wise data likelihood σxi) produce coherent samples expense visual conceptual blurriness reconstructions actually change attributes original data. decreasing maximizes elbo increases ﬁdelity reconstructions cost sample realism using actor shift prior samples satisfy realism constraint achieve realistic samples without sacriﬁcing sharpness samples mapped closest point latent space satisﬁes realism constraint attributes original data. encoder distribution approximation posterior tractable likelihood function depends parameters output decoder function maximize evidence lower bound lelbo. likelihood often chosen product simple distributions gans vaes complementary strengths weaknesses. gans suffer modecollapse problem generator assigns mass small subset support population distribution—that generate realistic samples many realistic samples cannot generate. particularly problematic want gans manipulate data rather generate data; even variants include kind inference machinery determine best matches tend produce reconstructions reminiscent input preserve identity. hand vaes often exhibit tradeoff between sharp reconstructions sensible-looking samples depending hyperparameters trained vaes tend either produce blurry reconstructions plausible novel samples bizarre samples sharp reconstructions. argued holes problem; decoder trained samples marginal posterior high divergence presupposed marginal particular decoder reconstruct arbitrary values high accuracy typical posterior highly concentrated. show experimentally supplemental figure underestimates posterior variance marginal posterior also highly conz ppdz produce results typical reconstructions ep]. tune maximize elbo figure contour maps critic value functions marginal posterior constraint. look latent dimensions lowest average posterior standard deviation training taking variance space proxy inﬂuence generated images. latent dimensions held ﬁxed original values left sample held-out right). gray marks correspond points latent space generated images right. cross-section left taken prior sample shows contours point towards realistic looking digits. cross-section right sample validation resides within local maximum critic would hope. conditional gans conditional vaes generate samples conditioned attribute information available must trained knowledge attribute labels whole training clear adapt attributes without retraining scratch. furthermore cgans cvaes suffer problems mode-collapse blurriness unconditional cousins. take different approach conditional generation identity-preserving transformation. begin training unconditional hyperparameters chosen ensure good reconstruction train realism critic predict whether given maps high-quality sample. also train critics predict whether given maps sample manifests various attributes interest. generate samples realistic exhibit desired attributes option optimize random vectors satisfy realism attribute critics. alternately amortize cost training actor network random vectors subregion latent space satisﬁes constraints encoded critics. encouraging transformed vectors remain close possible started alleviate mode-collapse problem common gans. shown figure train critic differentiate samples critic loss simply cross-entropy labels found realism critic little trouble generalizing unseen data; able recognize samples realistic sampling prior sufﬁcient train models lower divergence divergence large chances sampling point high probability becomes vanishingly small. leads poor sample quality makes difﬁcult learn tight approximation solely sampling instead inner-loop gradient-based optimization gopt gradientdescent) move prior samples points deemed like clarity introduce shorthand log) figure conditional generation cgan actor-critic pair acting latent space starts different prior sample maps point latent space satisﬁes attribute constraints realism constraint. attribute constraints changed time produce smooth transition possible left right. bottom cgan regularized training prefer small shifts latent space compared images generated unregularized model images generated regularized model much less diverse across columns suggesting regularization indeed enforce degree identity preservation. regularized model produces images somewhat diverse across rows suggesting regularization ﬁghts mode collapse column complete list attributes given supplemental table since inner-loop optimization slow training amortize generation using neural network function approximator. many examples amortization tricks including encoder generator fast neural style transfer traditional parameters function updated maximize value ascribes shifted latent points. challenges using situation prone mode-collapse. however advantage applying latent space regularize closest point latent space satisﬁes thus encouraging diverse solutions. introduce regularization encourage nearby solutions allowing term ldist exploration mean square error term. utilizes fraction latent dimensions scale distance penalty dimension utilization indicated squared reciprocal scale encoder distribution averaged training dataset figure identity-preserving transformations optimization. separate critics trained attributes realism constraint. starting latent points corresponding data reconstructions perform gradient ascent latent space weighted combination critic values stopping threshold value passed critics. images remain semantically close original pixel-wise likelihood training encourages identity-preserving reconstructions dynamics gradient ascent naturally limited ﬁnding solutions close latent space. panels black attributes original image procedure returns original point latent space. want generate samples realistic also want control attributes exhibit. given binary attribute labels dataset accomplish using cgan latent space amounts replacing conditional versions concatenating input. actor critic attribute information must points latent space could samples attributes procedure computationally inexpensive relative training generative model scratch. experiments relatively large cgan actor-critic pair training uses fewer flops/iteration unconditional vae. also trained much smaller cgan actor-critic pair uses fewer flops/iteration achieves slightly worse results larger cgan figure demonstrates quality conditional samples cgan actor-critic pair effect distance penalty constrains generation closer prior sample maintaining similarity samples different attributes. regularized cgan actor less freedom ignore modes pushing many random vectors area latent space since penalized moving samples far. increased diversity across rows regularized cgan evidence regularization ﬁght mode-collapse however without distance penalty samples appear realistic prominent attributes. supported table separately trained attribute classiﬁcation model quantitatively evaluate samples. actor table accuracy separate model trained classify attributes images evaluated test data generated images. condition evaluate generated images labels test data. comparison results similar task using invertible cgans generation provided. however since full list salient attributes given paper emphasize directly comparable experiments slightly different attribute labels. also measure distance latent space prior samples shifted weighted actors trained latent distance penalty λdist slightly worse accuracy latent points much closer prior samples produce greater diversity images interestingly actor trained without distance penalty achieves higher classiﬁcation accuracy test itself possibly generating images exaggerated distinctive features real data. small model cgan fewer parameters generates images comperable quality. smaller capacity model ﬁnds local solutions slightly less attribute accuracy visually similar prior sample without explicit regularization term. although used base generative model approach could also used generate high-quality conditional samples pretrained classical autoencoders. show supplemental figure obtain reasonably good conditional samples learning decoder using training encourages much latent space possible turn encourages decoder latent space reasonable-looking images. prior also imposes natural scale latent variables. produce good reconstructions held-out data transform attributes output gradient-based optimization. simply need train critic dattr predict attribute labels data embeddings cross-entropy loss train. then starting data point perform gradient descent realism constraint attribute constraint jointly ldreal λattrldattr. note helpful maintain realism constraint keep image distorting unrealistically. using procedure also conditionally generate samples starting figure demonstrates transformations applied samples held-out evaluation dataset. note since reconstructions close original images transformed images also maintain much structure. contrasts supplemental figure distance-penalty-free cgan actor produces transformations share attributes original shift identity. could preserve identity introducing distance penalty much easier correct weighting realism cost attribute cost distance penalty optimization combination require retraining network. figure transformations prior sample melody model. -bar pianoroll time horizontal direction pitch vertical direction. prior sample notes falling outside major scale shown red. transformation gp=cmajd= sampled notes fall within scale without signiﬁcant change note density. transformation original gp=cmajd= sampled notes within scale density increases beyond synthesized audio samples heard https//goo.gl/ouult. assumed access labeled data train attribute classiﬁers. remove need provide labeled examples leveraging structure learned pre-trained model using generate exemplars scored user-supplied reward function. constrain reward function bounded problem becomes similar previous settings actor critic working together. aims best approximate true value latent state ex∼p aims shift samples prior highvalue states. critic loss cross-entropy actor loss equation distance penalty promote diversity outputs. note reward function decoder need necessarily differentiable critic learns value function approximate reward actor uses training. highlight this demonstrate output recurrent model constrained satisfy hardcoded rule-based constraints. ﬁrst train lstm melodic fragments. melody represented sequence categorical variables. order examine ability constrain pitch classes note density outputs deﬁne reward functions encourages notes pitches another encourages melodies least notes figure gives example controlling pitch class note density generated outputs quantitatively supported results table training actor goes several phases exploration exploitation oscillating expanding modes high reward contracting nearest locations modes eventually settling high value states require small movements latent space conditional gans vaes introduce conditioning variables training time. sohn allow variables affect distribution latent space still require tractable distribution. perarnau cgans table average rewards constraint satisfaction rates unconditional conditional generation. samples prior receive rewards average near zero satisfaction rates pitch class note density constraints. after applying actor optimized major scale pitch class constraint fully satisﬁed time minor effect density. average value close also indicates constraint satisﬁed typically notes. applying actor function optimized major scale high density causes constraints satisﬁed high rates slightly larger shift latent space. adjust images cgans cannot usually reconstruct arbitrary inputs accurately must resort image-space processing techniques transfer effects original input. white propose adding attribute vectors samples simple effective heuristic perform transformations relies heavily linearity latent space. recent work focused applying expressive prior constraints vaes prior maximizes elbo interpret realism constraint trying implicit distribution indistinguishable like adversarial autoencoder makhzani realism constraint relies discriminative model instead trying force equal simple weakly constrain classiﬁer clean results. like work recently proposed adversarially regularized autoencoder uses adversarial training generate latent codes latent space discovered autoencoder; work focuses unconditional generation. g´omez-bombarelli train classiﬁers latent space predict latent variables molecules various properties iterative gradient-based optimization latent space molecules desired properties. molecule data procedure generates invalid molecules rarely enough simply reject samples detected using off-the-shelf software. contrast probability generating realistic images pretrained astronomically small simple criterion detecting valid images exists. jaques also classiﬁer constrain generation; deep q-network auxiliary loss training lstm. closest section nguyen generate high quality conditional images optimizing sample latent space generative network create image maximizes class activations pretrained imagenet classiﬁer. work differs learn amortized generator/discriminator directly latent space achieve diversity regularizing natural scale latent space rather modiﬁed langevin sampling algorithm. possibility would plug different architectures including powerful autoregressive decoders adversarial decoder costs make assumptions speciﬁc independent likelihoods. considered constraints based implicit density estimation could also estimate constrained distribution directly explicit autoregressive model another variational autoencoder. efﬁcacy autoregressive priors vaes promising approach conditional samples could obtained ancestral sampling transformations using gradient ascent increase likelihood model. active semisupervised learning approaches could reduce sample complexity learning constraints. real-time constraint learning would also enable applications; might fruitful extend reward approximation section incorporate user preferences interactive machine learning end-user innoproceedings aaai symposium series designing user experience vation. machine learning systems http//research.gold.ac.uk/// bernardozbyszynskifiebrinkgrierson_uxml_.pdf. chen diederik kingma salimans duan prafulla dhariwal john schulman ilya sutskever pieter abbeel. variational lossy autoencoder. proceedings international conference learning representations http//arxiv.org/abs/ paul christiano leike brown miljan martic shane legg dario amodei. deep reinforcement learning human preferences. arxiv preprint https//arxiv. org/abs/.. vincent dumoulin ishmael belghazi poole olivier mastropietro alex lamb martin arjovsky aaron courville. adversarially learned inference. proceedings international conference learning representations https//arxiv.org/ abs/.. g´omez-bombarelli duvenaud hern´andez-lobato s´anchez-lengeling sheberla aguilera-iparraguirre hirzel adams aspuru-guzik. automatic chemical design using data-driven continuous representation molecules. arxiv e-prints october goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair advances neural aaron courville yoshua bengio. generative adversarial nets. information processing systems http//papers.nips.cc/paper/ -generative-adversarial-nets.pdf. ishaan gulrajani faruk ahmed martin arjovsky vincent dumoulin aaron courville. improved training wasserstein gans. arxiv preprint http//arxiv.org/ abs/.. natasha jaques shixiang dzmitry bahdanau miguel hernndez-lobato richard turner douglas eck. sequence tutor conservative ﬁne-tuning sequence generation models kl-control. proceedings international conference learning representations https//arxiv.org/abs/.. junbo zhao yoon kelly zhang alexander rush yann lecun. adversarially regularized autoencoders generating discrete structures. arxiv preprint http//arxiv.org/abs/.. diederik kingma jimmy adam method stochastic optimization. proceedings international conference learning representations http// arxiv.org/abs/.. diederik kingma salimans rafal jozefowicz chen ilya sutskever welling. improving variational inference inverse autoregressive flow. advances neural information processing systems http//arxiv.org/abs/.. chuan michael wand. precomputed real-time texture synthesis markovian generative adversarial networks. european conference computer vision springer ziwei ping xiaogang wang xiaoou tang. deep learning face attributes wild. proceedings international conference computer vision https //arxiv.org/abs/.. alireza makhzani jonathon shlens navdeep jaitly goodfellow. adversarial autoencoders. proceedings international conference learning representations http//arxiv.org/abs/.. nguyen alexey dosovitskiy jason yosinski thomas brox jeff clune. synthesizing advances preferred inputs neurons neural networks deep generator networks. neural information processing systems https//arxiv.org/abs/ nguyen jason yosinski yoshua bengio alexey dosovitskiy jeff clune. plug play generative networks conditional iterative generation images latent space. arxiv preprint arxiv. inworkshop adversarial training vertible conditional gans image editing. nips http//arxiv.org/abs/.http//www.cvc.uab. es/lamp/wp-content/uploads/projects/pdfs/presentationnips.pdf. alec radford luke metz soumith chintala. unsupervised representation learning deep convolutional generative adversarial networks. corr abs/. http// arxiv.org/abs/.. tation using deep conditional generative models. tion processing systems -learning-structured-output-representation-using-deep-conditional-generative-models. pdf. casper kaae sønderby tapani raiko lars maaløe søren kaae sønderby winther. ladder variational autoencoders. advances neural information processing systems dmitry ulyanov vadim lebedev andrea vedaldi victor lempitsky. texture networks feed-forward synthesis textures stylized images. proceedings international conference machine learning http//arxiv.org/abs/. images mnist digits dataset large-scale celebfaces attributes dataset mnist images pixels greyscale scaled attributes number class label digit. celeba images center-cropped pixels downsampled pixels scaled many attribute labels strongly correlated changes images narrow original attributes visually salient blond hair black hair brown hair bald eyeglasses facial hair smiling gender age. melodies scraped collect million publicly available midi ﬁles. extracted -bar melodies sliding window single stride non-percussion instrument time signature keeping note highest pitch multiple overlap. produced million unique melodies. represent melody sequence categorical variables taking discrete states sixteenth note note-on pitches hold state rest state. encoders decoders classiﬁers trained adam optimizer learning rate train dreal dattr follow training procedure gulrajani applying gradient penalty training step ratio adam optimizer learning rate necessary converge improves stability optimization. apply tricks training batch normalization minibatch discrimination one-sided label smoothing samples easier discriminate samples train sampling rate times less actors inner-loop optimization gopt iterations adam used learning rate encoder series linear layers outputs followed relu additional linear layer used produce outputs. half outputs used softplus half used parameterize -dimension multivariate gaussian distribution diagonal covariance matrix decoder series linear layers outputs followed relu additional linear layer used produce outputs. outputs passed sigmoid generate output image. encoder series convolutional layers followed relu. convolution kernels size output channels respectively. convolutional layers stride ﬁnal relu linear layer used produce outputs. half outputs used softplus half used parameterize -dimension multivariate gaussian distribution diagonal covariance matrix decoder passes linear layer series transposed convolutional layers last followed relu. deconvolution kernels size output channels respectively. encoder made single-layer bidirectional lstm units cell. ﬁnal output direction concatenated passed linear layer produce outputs. half outputs used softplus half used parameterize -dimension multivariate gaussian distribution diagonal covariance matrix since musical sequences often structure level hierarchical decoder model long melodies. first goes linear layer initialize state -layer lstm units layer outputs embeddings size each bar. embeddings passed linear layer produce initial states another -layer lstm units layer. bar-level lstm autoregressively produces individual sixteenth note events passing output linear layer softmax create distribution classes. categorical distribution used compute cross-entropy loss training samples inference time. addition generating initial state start embedding current concatenated previous output input time step. network series linear layers outputs followed relu additional linear layer used produce outputs. half outputs used sigmoid half used gates. transformed computed gates∗ aids training network predict shifts conditioning attribute labels compute labels passed linear layer producing outputs concatenated model input. figure optimization samples drawn prior satisfy realism constraint attribute constraints optimization takes steps images shown steps. trained inner-loop optimization gopt described section figure identity-distorting transformations cgan actor-critic. without penalty encourage small moves latent space actor maps latent vectors original data points generated images correct attributes different identity. panels black attributes original image procedure returns image reconstruction. figure architectures feed-forward mnist convolutional celeba hierarchical lstm melody vaes. convolutions stride lstm cells shown color share weights linear layers levels omitted. figure latent constraints applied vanilla autoencoder latent prior. samples similar quality vaes less diversity high-frequency visual artifacts. full attribute labels given supplementary table figure smaller decoder standard deviations lead lower-variance posteriors encoder averaged training dimension. x-axis sorted lowest highest variance. tighter posteriors correspond utilization latent dimension scale distance regularization square inverse per-dimension basis.", "year": 2017}