{"title": "Network Morphism", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "We present in this paper a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as \\emph{network morphism} in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement for this network morphism is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.", "text": "present paper systematic study morph well-trained neural network network function completely preserved. deﬁne network morphism research. morphing parent network child network expected inherit knowledge parent network also potential continue growing powerful much shortened training time. ﬁrst requirement network morphism ability handle diverse morphing types networks including changes depth width kernel size even subnet. meet requirement ﬁrst introduce network morphism equations develop novel morphing algorithms morphing types classic convolutional neural networks. second requirement network morphism ability deal nonlinearity network. propose family parametric-activation functions facilitate morphing continuous non-linear activation neurons. experimental results benchmark datasets typical neural networks demonstrate effectiveness proposed network morphism scheme. deep convolutional neural networks achieved state-of-the-art results diverse computer vision tasks image classiﬁcation object detection semantic segmentation however training network time-consuming. usually takes weeks even months train effective deep network alone exploration diverse network settings. much desired well-trained networks directly adopted related applications minimum retraining. accomplish ideal goal need systematically study morph well-trained neural network network function completely preserved. call operations network morphism. upon completion morphism child network shall inherit entire knowledge parent network also capable growing powerful much shortened training time process continues fundamentally different existing work related network knowledge transferring either tries mimic parent network’s outputs pre-trains facilitate convergence and/or adapt datasets possible total change network function mathematically morphism structure-preserving mathematical structure another context neural networks network morphism refers parameter-transferring parent network child network preserves function outputs. although network morphism generally impose constraints architecture child network limit investigation network morphism expanding mode intuitively means child network deeper and/or wider parent network. fig. illustrates concept network morphism variety morphing types demonstrated including depth morphing width morphing kernel size morphing subnet morphing. work derive network morphism equations successful morphing operation follow based novel network morphism algorithms developed morphing types. proposed algorithms work classic multi-layer perceptron models convolutional neural networks. since proposed network morphism required output unchanged complex morphing decomposed basic morphing steps thus solved easily. depth morphing important morphing type since current top-notch neural networks going deeper deeper heuristic approach embed identity mapping layer parent network referred idmorph. idmorph explored recent work potentially problematic sparsity identity layer might fail sometimes overcome issues associated idmorph introduce several practices morphism operation follow propose deconvolution-based algorithm network depth morphing. algorithm able asymptotically parameters non-zero elements. worst case non-zero occupying rate proposed algorithm still higher idmorph order magnitude. another challenge proposed network morphism face dealing non-linearity neural network. even simple idmorph method fails case works idempotent functions. work idempotent function deﬁned satisfy condition passes relu function fails commonly used activation functions sigmoid tanh. deal non-linearity introduce concept parametric-activation function family deﬁned adjoint function family arbitrary non-linear activation function. reduce non-linear operation linear parameter learned. therefore network morphism continuous non-linear activation neurons solved. best knowledge ﬁrst work network morphism except recent work introduces idmorph. conduct extensive experiments show effectiveness proposed network morphism learning scheme widely used benchmark datasets classic convolutional neural networks. effectiveness basic morphing operations also veriﬁed. furthermore show proposed network morphism able internally regularize network typically leads improved performance. finally also successfully morph well-known layered better training time comperforming model paring training scratch. brieﬂy introduce recent work related network morphism identify differences work. mimic learning. series work trying mimic teacher network student network developed usually need learning scratch. example tried train lighter network mimicking ensemble network. extended idea used shallower wider network mimic deep wide network. authors adopted deeper narrower network mimic deep wide network. proposed network morphism scheme different algorithms since instead mimicking goal make child network directly inherit intact knowledge parent network. allows network morphism achieve performance. networks called parent child instead teacher student. another major difference child network learned scratch. pre-training transfer learning. pre-training strategy proposed facilitate convergence deep neural networks transfer learning introduced overcome overﬁtting problem training large neural networks relatively small datasets. re-initialize last layers parent network layers remaining difference pretraining continues train child network dataset transfer learning continues one. however strategies totally alter parameters last layers well network function. netnet. netnet recent work proposed although targets problem several major differences network morphism netnet. first solution netnet still restricted idmorph approach netmorph ﬁrst make possible embed non-identity layers. second netnet’s operations work idempotent activation functions netmorph ﬁrst handle arbitrary non-linear activation functions. third netnet’s discussion limited width depth changes netmorph studies variety morphing types including depth width kernel size subnet changes. fourth netnet needs separately consider depth width changes netmorph able simultaneously conduct depth width kernel size morphing single operation. shall ﬁrst discuss depth morphing linear case actually also involves width kernel size morphing. shall describe deal non-linearities neural networks. finally shall present stand-alone versions width morphing kernel size morphing followed subnet morphing. next consider case deep convolutional neural network dcnn build-up blocks convolutional layers rather fully connected layers. thus call hidden layers blobs weight matrices ﬁlters. dcnn blob tensor shape represent number channels height width ﬁlters tensors shapes convolutional kernel sizes. start simplest case classic neural network. ﬁrst drop non-linear activation functions consider neural network connected fully connected layers. rcl− rcl+ rcl+×cl− feature dimensions bl+. network morphism shall insert hidden layer child network satisﬁes rcl×cl− rcl+×cl. obvious network morphism classic neural networks equivalent matrix decomposition problem although equation primarily derived depth morphing also involves network width kernel sizes thus called network morphism equation short remaining paper. since solutions equation might unique shall make morphism operation follow desired practices that parameters contain many nonzero elements possible parameters need consistent scale. practices widely adopted existing work since random initialization instead zero ﬁlling non-convex optimization problems preferred scale initializations critical convergence good performance deep neural networks next introduce algorithms based deconvolution solve network morphism equation i.e. general network morphism practical network morphism. former ﬁlls parameters nonzero elements certain condition latter depend condition asymptotically parameters non-zero elements. algorithm proposed solve equation certain condition. shown algorithm initializes convolution kernels child network random noises. iteratively solve ﬁxing other. iteration solved deconvolution. hence overall loss always decreasing expected converge. however guaranteed loss algorithm always converge easy check correctness condition multi-channel convolution written multiplication matrices. condition claims unknowns constraints hence undetermined linear system. since random matrices rarely inconsistent solutions undetermined linear system always exist. next propose variant algorithm solve equation sacriﬁce non-sparse practice. algorithm reduces zero-converging condition parameter number either less instead since focus network morphism expanding mode assume condition self-justiﬁed namely either expands expands thus claim algorithm solves network morphism equation described algorithm case expands starting iteratively call algorithm shrink size loss converges iteration shall terminate able guarantee loss case expands algorithm similar. sacriﬁce non-sparse practice algorithm illustrated fig. worst case might able parameters non-zero elements still asymptotically. ﬁgure compares non-zero element occupations idmorph netmorph. assume best case netmorph able occupy elements non-zeros order worst case order non-zero elements. generally netmorph lies best case worst case. idmorph order non-zeros elements. thus nonzero occupying rate netmorph higher idmorph least order magnitude. practice shall also thus netmorph asymptotically parameters non-zero elements. proposed network morphism also required deal non-linearities neural network. general trivial replace layer bl+) layers bl−)) represents non-linear activation function. idempotent activation function satisfying idmorph scheme netnet represents identity mapping. bl−) bl+) bl+). however although idmorph works relu activation function cannot applied commonly used activation functions sigmoid tanh since idempotent condition satisﬁed. handle arbitrary continuous non-linear activation functions propose deﬁne concept pactivation function family. family p-activation functions activation function deﬁned continuous function family maps linear identity transform p-activation function family might uniquely deﬁned. deﬁne canonical form p-activation function family follows {ϕa}|a∈ ϕid}|a∈ parameter control shape morphing activation function. figure non-zero element occupations different algorithms idmorph netmorph worst case netmorph best case represent channel size kernel size. ﬁgure shows convolutional ﬁlter shape ﬂattened seen ﬁlter idmorph sparse. figure network morphism non-linear. activations indicated green safely added; activation yellow needs linear beginning able grow non-linear learned. ϕid. concept p-activation function family extends prelu deﬁnition prelu coincides canonical form p-activation function family relu non-linear activation unit. idea leveraging p-activation function family network morphism shown fig. shown safe non-linear activations indicated green boxes need make sure yellow equivalent linear activation initially. linear activation shall grow non-linear value learned. formally need replace layer bl+) layers bl−)). morphing shall successful long network morphing equation satisﬁed mentioned network morphism equation involves network depth width kernel size morphing. therefore conduct width kernel size morphing introducing extra depth morphing algorithm modern networks going deeper deeper. challenging manually design tens even hundreds layers. elegant strategy ﬁrst design subnet template construct network subnets. typical examples mlpconv layer network network inception layer googlenet shown fig. study problem subnet morphing section network morphism minimal number layers parent network subnet child network. commonly used subnet stacked sequential subnet shown fig. exmaple inception layer googlenet four-way stacking sequential subnets. ﬁrst describe morphing operation sequential subnet based stacked version obtained. sequential subnet morphing morph single layer multiple sequential layers illustrated fig. similar equation derive network morphism equation sequential subnets single layer layers zero-padded version effective kernel p=··· kl+p kernel size layer similar algorithm subnet morphing equation solved iteratively optimizing parameters layer parameters layers ﬁxed. also develop practical version algorithm solve equation similar algorithm algorithm details omitted here. obvious either arbitrarily. following non-sparse practice less parameters random noises. zeros random noises ˜fl+ clustered together. break unwanted behavior perform random permutation change bl+. kernel size morphing propose heuristic effective solution. suppose convolutional layer kernel size want expand ˜kl. ﬁlters layer padded zeros side operation shall also apply blobs. shown fig. resulting blobs shape also values. figure subnet morphing. subnet examples mlpconv layer inception layer googlenet. sequential subnet morphing single layer layers. workﬂow stacked sequential subnet morphing. simplest case then path sequential subnet morphing conducted. fig. illustrate n-way stacked sequential subnet morphing second path morphed layers. section conduct experiments three datasets show effectiveness proposed network morphism scheme different morphing operations classic convolutional neural networks idempotent activations non-idempotent activations ﬁrst experiment conducted mnist dataset mnist standard dataset handwritten digit recognition training images testing images. section instead using state-of-the-art dcnn solutions adopt simple softmax regression model parent network evaluate effectiveness network morphism classic networks. grayscale digit images ﬂattened dimension feature vector input. parent model achieved accuracy considered baseline. then morphed model multiple layer perception model adding prelu ptanh hidden layer number hidden neurons fig. shows performance curves netmorph netnet morphing. that prelu activation netmorph works much better netnet. netmorph continues improve performance netnet improves also show curve netmorph non-idempotent activation ptanh fig. curve netnet unavailable since cannot handle non-idempotent activations. extensive experiments conducted cifar dataset verify network morphism scheme convolutional neural networks. cifar image recognition database composed color images. contains training images testing images object categories. baseline network adopted caffe cifar_quick model accuracy following uniﬁed notation cifar_ddd represent network architecture three subnets digit number convolutional dedescribed tailed architecture subnet following notation cifar_quick model layers fully connected layers denoted cifar_ architecture described additionally also indicate grouping layers subnet x<times> represent layers subnets. hence cifar_quick also denoted ]x]. note notation fully connected layers ignored. fig. shows comparison results netmorph netnet morphing sequence cifar_→→→→. detailed network architectures networks shown table table layers morphed adding convolutional layer channel size four times larger. good practice adopted design current state-of-the-art network algorithm leveraged morphing. fig. superiority netmorph netnet. netmorph improves performance netnet relatively inferior performance netnet caused idmorph netnet involving many zero elements embedded layer non-zero elements also consistent scale existing parameters. also veriﬁed comparing histograms embedded ﬁlters methods. parameter scale netmorph normal distribution relatively large standard derivation netnet shows peaks around fig. illustrates performance netmorph subnet morphing. architecture morphed cifar_ cifar_. seen netmorph achieves additional performance improvement fig. illustrates morphing cifar_ cifar_ performance improved around raw. network goes deeper becomes larger. interpret phenomena internal regularization ability netmorph. netmorph parameters learned multiple phases rather once. deep neural networks usually involve large amount parameters overﬁt training data occur easily. netmorph parameters learned placed good position parameter space. need explore relatively small region rather whole parameter space. thus netmorph learning process shall result regularized network achieve better performance. fig. shows curve kernel size morphing expands kernel size second layer subnet results performance higher parent network. double number channels ﬁrst layer subnet width morphing netmorph works arbitrary continuous non-linear activation functions netnet piece-wise linear ones. also conducted width morphing directly parent network tanh neurons results accuracy improvement. also conduct experiments imagenet dataset object categories. models trained million training images tested validation images. top- accuracies -view -view reported. proposed experiments based actually trained multi-scales caffe implementation favors single-scale fair comparison ﬁrst de-multiscale model continuing train imagenet dataset images resized process caused performance drop. coincides table model paper adopt de-multiscaled version parent network morph. morphing operation adopt convolutional layer ﬁrst three subsets each. detailed network architecture shown table continue train child network morphing ﬁnal model denoted vgg. results shown table that outperforms parent network also outperforms multi-scale version vgg. since -layer network also list table comparison. seen also outperforms large margin. note different architectures shown table therefore proposed netmorph scheme help improve performance also effective network architecture explorer. further able layers better performing model shall expected. compare training time cost netmorph learning scheme training scratch. trained around months single time include pre-training time -layered network. deeper network training time shall increase. layered network whole morphing training process ﬁnished within days resulting around speedup. paper introduced systematic study network morphism. proposed scheme able morph well-trained parent network child network network function completely preserved. child network potential grow powerful short time. introduced diverse morphing operations developed novel morphing algorithms based morphism equations derived. non-linearity neural network carefully addressed proposed algorithms enable morphing continuous non-linear activation neurons. extensive experiments carried demonstrate effectiveness proposed network morphism scheme. bucilu cristian caruana rich niculescu-mizil alexandru. model compression. proceedings sigkdd international conference knowledge discovery data mining girshick ross donahue jeff darrell trevor malik jagannath. rich feature hierarchies accurate object detection semantic segmentation. ieee conference computer vision pattern recognition ieee glorot xavier bengio yoshua. understanding difﬁculty training deep feedforward neural networks. international conference artiﬁcial intelligence statistics yangqing shelhamer evan donahue jeff karayev sergey long jonathan girshick ross guadarrama sergio darrell trevor. caffe convolutional architecture fast feature embedding. proceedings international conference multimedia krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems oquab maxime bottou leon laptev ivan sivic josef. learning transferring mid-level image representations using convolutional neural networks. ieee conference computer vision pattern recognition ieee shaoqing kaiming girshick ross jian. faster r-cnn towards real-time object detection region proposal networks. advances neural information processing systems russakovsky olga deng krause jonathan satheesh sanjeev sean huang zhiheng karpathy andrej khosla aditya bernstein michael imagenet large scale visual recognition challenge. international journal computer vision szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. arxiv preprint going deeper convolutions. arxiv.", "year": 2016}