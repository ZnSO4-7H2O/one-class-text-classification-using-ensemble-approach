{"title": "An Analysis of Active Learning With Uniform Feature Noise", "tag": ["stat.ML", "cs.AI", "cs.LG", "math.ST", "stat.TH"], "abstract": "In active learning, the user sequentially chooses values for feature $X$ and an oracle returns the corresponding label $Y$. In this paper, we consider the effect of feature noise in active learning, which could arise either because $X$ itself is being measured, or it is corrupted in transmission to the oracle, or the oracle returns the label of a noisy version of the query point. In statistics, feature noise is known as \"errors in variables\" and has been studied extensively in non-active settings. However, the effect of feature noise in active learning has not been studied before. We consider the well-known Berkson errors-in-variables model with additive uniform noise of width $\\sigma$.  Our simple but revealing setting is that of one-dimensional binary classification setting where the goal is to learn a threshold (point where the probability of a $+$ label crosses half). We deal with regression functions that are antisymmetric in a region of size $\\sigma$ around the threshold and also satisfy Tsybakov's margin condition around the threshold. We prove minimax lower and upper bounds which demonstrate that when $\\sigma$ is smaller than the minimiax active/passive noiseless error derived in \\cite{CN07}, then noise has no effect on the rates and one achieves the same noiseless rates. For larger $\\sigma$, the \\textit{unflattening} of the regression function on convolution with uniform noise, along with its local antisymmetry around the threshold, together yield a behaviour where noise \\textit{appears} to be beneficial. Our key result is that active learning can buy significant improvement over a passive strategy even in the presence of feature noise.", "text": "active learning user sequentially chooses values feature oracle returns corresponding label paper consider eﬀect feature noise active learning could arise either measured corrupted transmission oracle oracle returns label noisy version query point. statistics feature noise known errors variables studied extensively non-active settings. however eﬀect feature noise active learning studied before. consider well-known berkson errors-in-variables model additive uniform noise width simple revealing setting one-dimensional binary classiﬁcation setting goal learn threshold deal regression functions antisymmetric region size around threshold also satisfy tsybakov’s margin condition around threshold. prove minimax lower upper bounds demonstrate smaller minimiax active/passive noiseless error derived castro nowak noise eﬀect rates achieves noiseless rates. larger unﬂattening regression function convolution uniform noise along local antisymmetry around threshold together yield behaviour noise appears beneﬁcial. result active learning signiﬁcant improvement passive strategy even presence feature noise. active learning machine learning paradigm algorithm interacts labelproviding oracle feedback driven loop past training data used guide design subsequent queries. typically oracle queried exact feature value oracle returns label corresponding precisely feature value. however many scenarios feature value queried noisy helps analyze would happen setting. situations include noisy sensor measurements features corrupted transmission data source storage access limited noisy oracle. errors-in-variables model well studied statistical literature eﬀect profound. density estimation gaussian error causes minimax rate become logarithmic sample size instead polynomial results passive regression refer fuller carroll passive classiﬁcation loustau marteau however classiﬁcation studied berkson model introduced below. also deconvolution estimators require noise fourier transform bounded away zero ruling uniform noise. finally best knowledge feature noise studied active learning setting. paper focus berkson error model since intuitively makes sense active learning captures idea request label feature oracle returns label corrupted version generated i.e. noise occurs label request oracle output. uniform noise since yields insightful behavior also addressed literature. conjecture qualitatively similar results hold symmetric error models. setup threshold classiﬁcation. {+−} denote classiﬁcation rule. assuming loss risk classiﬁcation rule known bayes optimal classiﬁer best measurable classiﬁer minimizes risk minf following form passive sampling assume given batch unif corresponding labels sampled independently {wj}j=i {yj}j=i. case stratseeing yi}n active sampling allowed sequentially choose possibly random function past queries labels randomness case strategy sequence functions independent queries labels. adding noise point means convolution) crosses half diﬀer point crosses half. however antisymmetry assumption boundary assumption together imply thresholds same. getting seems substantially diﬃcult. remark. paper focus learning threshold relevant threshold maybe intrinsic interest also interest prediction example future queries could made diﬀerent noise model obtained noise-free. similar results derived /-risk. zero noise. assumptions vacuously true class matches class considered castro nowak rates i.e. precisely passive active minimax point error rates castro small noise. noise small expect risk change noise long noise smaller noiseless error. words long passive learning passive learners noise smaller noiseless error rate similarly really able notice tiny noise minimax rate remains active learning active long noise smaller noiseless error rate learners really able notice tiny noise minimax rate remains rates small aforementioned active rates large noise assumption noise large curious behaviour rates. error rates seem smaller/better larger noise active passive learning furthermore noisy rates also better noiseless rate might seem violate information processing inequality intuition noise shouldn’t help estimation. moreover noiseless active learner able simulate noisy situation adding noise querying resulting point better rates violating lower bounds castro nowak however make following crucial subtle observation. claimed rates ﬁxed function class assumption function class changes fact requires antisymmetry regression function hold larger region larger functions actually getting smaller larger even though functions given contradiction results castro nowak fundamental information theoretic ideas also intuitive explanation assumption helps large noise. later ﬁgure convolution noise seems stretch/unﬂatten function around threshold. speciﬁcally larger regression function quite around threshold convolution noise makes less linear fact behaves linearly large region width nearly true regardless whether assumption holds however hold convolved threshold point convolved function crosses half need original threshold dropping assumption hurt want convolved threshold given estimate problem ﬁguring much threshold shifted quite non-trivial. hence large noise ensures behaviour less linear around threshold assumption ensures threshold doesn’t shift intuitively large noise help technically contradiction becasue function class getting progressively simpler controlled growth around threshold. main takeaway settings active learning yields gain passive sampling. describe upper lower bounds lead theorem case handled detail intuitionb proofs appendix. ﬁgures tsybakov’s margin condition holds plot linear regression function blue curves show linear growth around remains linear. middle bottom ﬁgure ﬂatter regression function respectively plotted separately clarity. harder curve ﬂatter around making harder pinpoint threshold. however plots noise actually helps smoothing making linear. however note eﬀect assumption cannot understated plots threshold noise cross half point. eﬀect noise seen following section. derive lower bounds follow approach ibargimov hasminskii tsybakov exempliﬁed lower bounds active learning problems without feature noise castro nowak standard methodology reduce problem classiﬁcation class hypothesis testing. similar castro nowak class models. associated probability measure deﬁned common probability space. semi-distance. also assume denotes kullback-leibler divergence. then following bound independent model follows independence future past passive learning. holds iterated expectation. used active learning needed passive learning. follows approximation passive sampling present modiﬁed histogram estimator widehist noise level larger noiseless minimax rate assume simplicity sampled points equally spaced mimic uniform distribution lying epochs/rounds. subroutine uses optimal passive learning algorithm like widehist. round actpass runs widehist progressively smaller domains restricted budget hence activizes widehist achieves optimal active rate process. algorithm inspired similar idea ramdas singh proof theorem proofs simply generalizations again present concise arguments settings algorithm actually detect noise i.e. noise level larger noiseless minimax rate cases algorithm remains unchanged. ﬁrst phase large possibly smaller widehist achieve noiseless rates within epoch. second phase widehist achieve noisy rates shrunk enough become larger epochs. paper propose simple berkson error model one-dimensional threshold classiﬁcation inspired setup model analysed castro nowak analyse active learning additive uniform feature noise. best knowledge ﬁrst attempt jointly tackling feature noise label noise active learning. simple setting already yields interesting behaviour depending additive feature noise level label noise underlying regression function. passive active learning whenever noise level smaller minimax noiseless rate learner cannot notice noise continue achieve noiseless rate. noise gets larger rates depend noise level. importantly achieve better rates passive learning scenarios propose unique algorithms/estimators achieve tight rates. idea activizing passive algorithms like algorithm actpass seems especially powerful could carry forward settings beyond paper ramdas singh immediate future work direct extension paper concerns main weakness paper possibility getting assumption hurdle fair comparision noiseless setting. would like re-emphasize ﬁrst glance rates misleading counterintuitive appears larger noise could possibly help estimation presence denominator larger however point class functions constant depends fact gets smaller sense larger assumption becomes stringent. observation non-constant function class along fact convolution uniform noise seems unﬂatten regression function shown ﬁgures together cause rates seemingly improve larger noise levels. analysing case without seems quite challenging task since noiseless convolved thresholds diﬀerent attempt formulate kernel-based estimators additional assumptions presently tight bounds leave future work.", "year": 2015}