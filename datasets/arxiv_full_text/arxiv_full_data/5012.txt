{"title": "Leveraging Distributional Semantics for Multi-Label Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "We present a novel and scalable label embedding framework for large-scale multi-label learning a.k.a ExMLDS (Extreme Multi-Label Learning using Distributional Semantics). Our approach draws inspiration from ideas rooted in distributional semantics, specifically the Skip Gram Negative Sampling (SGNS) approach, widely used to learn word embeddings for natural language processing tasks. Learning such embeddings can be reduced to a certain matrix factorization. Our approach is novel in that it highlights interesting connections between label embedding methods used for multi-label learning and paragraph/document embedding methods commonly used for learning representations of text data. The framework can also be easily extended to incorporate auxiliary information such as label-label correlations; this is crucial especially when there are a lot of missing labels in the training data. We demonstrate the effectiveness of our approach through an extensive set of experiments on a variety of benchmark datasets, and show that the proposed learning methods perform favorably compared to several baselines and state-of-the-art methods for large-scale multi-label learning. To facilitate end-to-end learning, we develop a joint learning algorithm that can learn the embeddings as well as a regression model that predicts these embeddings given input features, via efficient gradient-based methods.", "text": "present novel scalable label embedding framework large-scale multi-label learning a.k.a exmlds approach draws inspiration ideas rooted distributional semantics speciﬁcally skip gram negative sampling approach widely used learn word embeddings natural language processing tasks. learning embeddings reduced certain matrix factorization. approach novel highlights interesting connections label embedding methods used multi-label learning paragraph/document embedding methods commonly used learning representations text data. framework also easily extended incorporate auxiliary information label-label correlations; crucial especially missing labels training data. demonstrate effectiveness approach extensive experiments variety benchmark datasets show proposed learning methods perform favorably compared several baselines state-of-the-art methods large-scale multi-label learning. facilitate end-toend learning develop joint learning algorithm learn embeddings well regression model predicts embeddings given input features efﬁcient gradient based methods. modern data generated various domains increasingly \"multi-label\" nature; images documents often identiﬁed multiple tags online advertisers often associate multiple search keywords multi-label learning problem learning assign multiple labels instances received great deal attention last years; especially context learning millions labels popularly known extreme multi-label learning jain prabhu varma predicting labels tail becomes signiﬁcantly hard trainreasons sheer scale data. rendered data impracticable. state-of-the-art approaches extreme learning fall broadly classes multi-label embedding based methods e.g. leml wsabie sleec pd-sparse ﬁrst class approaches generally scalable work embedding highdimensional label vectors lower-dimensional space learning regressor space. cases methods rely assumption binary label matrix rank consequently label vectors embedded lower-dimensional space. time prediction decompression matrix used retrieve original label vector low-dimensional embeddings. corroborated recent empirical evidence standard structural assumptions low-rank label matrix fail perform poorly tail. second class methods learning move away rigid multi-label structural demonstrated work well especially tail labels. embedding based approach sleec leverages word vector embedding technique found resounding success natural language processing tasks. unlike embedding based methods sleec ability learn non-linear embeddings aiming preserve local structures example neighborhoods. show learning rich wordvec style embedding instances achieve competitive multi-label prediction accuracies often improve performance state-of-the-art embedding approach sleec cope missing labels incorporating auxiliary information form labellabel co-occurrences state-of-the-art methods not. furthermore learning algorithm admits signiﬁcantly faster implementation compared embedding based approaches. distinguishing aspect work draws inspiration distributional widely used learning non-linear representations text data natural language processing tasks understand word document semantics classifying documents etc. leverage interesting connection problem learning distributional semantics text data analysis multi-label learning problem. best knowledge novel application. unlike existing multi-label learning methods method also leverage label co-occurrence information learning embeddings; especially appealing large fraction labels missing label matrix. show improvement training time compared state-of-art label embedding methods extreme multilabel learning competitive terms label prediction accuracies; demonstrate scalability prediction performance several state-of-the-art moderateto-large scale multi-label benchmark datasets. outline paper follows. begin setting notation background describing problem formulation section section present training algorithms based learning word embeddings understanding word document semantics. propose objectives progressively incorporate auxiliary information viz. label correlations. present comprehensive experimental evaluation section conclude. standard multi-label learning algorithm given training instances associated label vectors real-world multi-label learning data sets usually observe irrelevant labels; indicates label relevant instance indicates label missing irrelevant. }n×l denote matrix label vectors. addition access labellabel co-occurrence information denoted zl×l goal multilabel learning learn vector-valued function scores labels. embedding-based approaches typically model composite function where example assuming linear transformations obtains formulation proposed functions learnt using training instances label vectors both. recently non-linear embedding methods shown help improve multi-label prediction accuracies signiﬁcantly. work follow framework linear transformation non-linear particular based k-nearest neighbors embedded feature space. denotes k−nearest neighbor training instances embedded space. algorithm predicting labels instance identical sleec presented convenience algorithm note that speeding predictions algorithm relies clustering training instances cluster instances different linear embedding denoted learnt. input test point nearest neighbors desired labels partition closest nearest neighbors embedded instances deﬁned return scoring labels according work focus learning algorithms functions inspired successes natural language processing context learning distributional semantics particular techniques inferring word-vector embeddings learning function using training label vectors label-label correlations rl×l. word embeddings desired natural language processing order understand semantic relationships between words classifying text documents etc. given text corpus consisting collection documents goal embed word space words appearing similar contexts closer space not. particular wordvec embedding approach learn embedding instances using label vectors sleec also uses nearest neighbors space label vectors order learn embeddings. however show experiments wordvec based embeddings richer help improve prediction performance signiﬁcantly especially missing labels. subsequent section discuss algorithms learning embeddings training phase multi-label learning. literature multiple algorithms work skip gram negative sampling technique reasons shown competitive natural language processing tasks importantly presents unique advantage terms scalability address shortly discussing technique. skip gram negative sampling. sgns goal learn embedding word vocabulary. words considered contexts occur; context typically deﬁned ﬁxed size window words around occurrence word. goal learn words similar contexts closer embedded space. denote word context word then likelihood observing pair data modeled sigmoid inner product similarity note denotes k-nearest neighborhood instance space label vectors instance embedding. learning label embeddings learn function regressing onto sleec. solving using standard wordvec implementations computationally expensive requires training multiple-layer neural networks. fortunately learning signiﬁcantly sped using observation promote dissimilar words apart negative sampling used wherein randomly sampled negative examples used. overall objective favors zw′′ maximize likelihood observing likelihood randomly sampled negative instances. typically negative examples sampled observed example resulting sgns objective given derive analogous embedding technique multi-label learning. simple model treat instance \"word\"; deﬁne \"context\" k-nearest neighbors given instance space formed training label vectors cosine similarity metric. arrive objective identical learning embeddings instances respectively refer algorithm based fast ppmi matrix factorization learning label vector embeddings exmlds. also optimize objective using neural network model refer wordvec method learning embeddings algorithm exmlds. various practical natural language processing applications superior performance obtained using joint models learning embeddings text documents well individual words corpus example pv-dbow objective learning embeddings maximize similarity alternately consider neighborhood ddimensional feature space however perform clustering space speed therefore label vectors likely preserve discriminative information within clusters. objective efﬁciently utilizes label-label correlations help improve embedding importantly cope missing labels. complete training procedure using sppmi factorization presented algorithm note arguments given show proposed combined objective solved sppmi factorization joint matrix given step algorithm embedded documents words compose documents. negative sampling also included objective minimize similarity document embeddings embeddings high frequency words. multi-label learning want learn embeddings labels well instances jointly. here think labels individual words whereas label vectors paragraphs documents. alluded beginning section many real world problems also auxiliary label correlation information label-label co-occurrence. easily incorporate information joint modeling approach outlined above. propose following objective incorporates information label vectors well label correlations matrix here denote embeddings instances denote embeddings labels. denotes k-nearest neighborhood instance space label vectors. denotes k-nearest neighborhood label space labels. here deﬁnes instance-instance correlation i.e. label-label correlation matrix. clearly identical tries embed labels vector space correlated labels closer; tries embed instances vector space correlated instances closer; ﬁnally {z¯z} tries embed labels instances common space labels occurring instance closer embedded instance. overall combined objective o{z¯z} promotes learning common embedding space correlated labels correlated instances observed labels given instance occur closely. hyper-parameters weight contributions type correlation. negative examples sampled observed label negative examples sampled observed instance context labels negative examples sampled observed instance context instances. hence proposed test time given data point could algorithm labels. alternately propose algorithm also incorporates similarity label embeddings along prediction especially training labels learn from. practice prediction approach useful. note corresponds corresponds refer algorithm based combined learning objective exmlds. conduct experiments commonly used benchmark datasets extreme multi-label classiﬁcation repository provided authors datasets pre-processed prescribed train-test splits. statistics datasets used experiments shown table standard practically relevant precision evaluation metric prediction performance. preck denotes number correct labels predictions. code baselines linux machine cores ram. implemented prediction algorithms matlab. learning algorithms implemented parlty python partly matlab. source code made available later. evaluate three models exmlds i.e. algorithm based fast ppmi matrix factorization learning label embeddings described section exmlds based optimizing objective described section using neural network exmlds i.e. algorithm based combined learning objective leml embedding based method. method also facilitates incorporating label information code given authors leml uses item features. refer latter method uses label correlations leml-imc. hyperparameters. embedding dimensionality preserve number nearest neighbors learning embeddings well prediction time number data partitions used sleec method exmldsand exmlds. small datasets negative sample size number iterations neural network training tuned based separate validation set. large datasets negative sample size number iterations tuned validation set. exmlds parameters identical exmlds. baselines either report results respective publications used best hyperparameters reported authors experiments needed. performance evaluation. performance compared methods reported table performances proposed methods exmlds exmlds found similar experiments optimize objective include results exmlds table. proposed methods achieve competitive prediction performance among state-of-the-art embedding tree-based approaches. particular note medialmill delicious-k datasets method achieves best performance. training time. objective trained using neural network described training neural network model give input k-nearest neighbor instance pairs training instance neighborhood computed space label vectors google wordvec code training. parallelize training cores linux machine speed-up. recall call method exmlds. compare training time method exmlds uses fast matrix factorization approach learning embeddings. algorithm involves single opposed iterative used sleec therefore signiﬁcantly faster. present training time measurements table anticipated observe exmlds uses neural networks slower exmlds also among smaller datasets exmlds trains faster compared sleecon bibtex dataset. large dataset delicious-k exmlds trains faster sleec. coping missing labels. many real-world scenarios data plagued lots missing labels. desirable property multi-label learning methods cope missing labels yield good prediction performance training labels. dearth training labels auxiliary information label correlations come handy. described section method exmlds learn additional information. benchmark datasets however come auxiliary information. simulate setting hide non-zero entries training label matrix reveal training labels learning algorithms. proxy label correlations matrix simply label-label co-occurrence table comparing prediction performance different methods. note although sleec performs slightly better model much faster shown results table also note performance model table signiﬁcant fraction labels missing considerably better sleec training data i.e. denotes full training matrix. give higher weight training algorithm prediction algorithm takes missing labels account. compare performance exmldswith sleec leml leml-imcin table note sleec leml methods incorporate auxiliary information leml-imc does. particular spectral embedding based features i.e. take singular vectors corresponding non-zero singular values label features. observed three datasets exmlds performs signiﬁcantly better huge margins. particular lift leml-imc signiﬁcant even though methods information. serves demonstrate strength approach. implement joint learning modiﬁed existing public code state embedding based extreme classiﬁcation approach annexml replacing dssm training objective wordvec objective keeping cosine similarity partitioning algorithm approximate nearest prediction algorithm same. efﬁcient training rare label keep coefﬁcient ratio negative positive samples training. used hyper-parameters i.e.embedding size number learner cluster number nearest neighbor number embedding partitioning iteration gamma label normalization true number threads obtain state result i.e. similar dismec ppdsparse annexml large datasets table details results. proposed novel objective learning label embeddings multi-label classiﬁcation leverages wordvec embedding technique; furthermore proposed formulation optimized efﬁciently sppmi matrix factorization. comprehensive experiments showed proposed method competitive compared state-ofthe-art multi-label learning methods terms prediction accuracies. also extended sgns objective joint learning embeddings regressor obtain state results. proposed novel objective incorporates side information particularly effective handling missing labels.", "year": 2017}