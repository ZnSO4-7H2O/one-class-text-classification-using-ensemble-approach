{"title": "Neural Combinatorial Optimization with Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent network using a policy gradient method. We compare learning the network parameters on a set of training graphs against learning them on individual test graphs. Despite the computational expense, without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied to the KnapSack, another NP-hard problem, the same method obtains optimal solutions for instances with up to 200 items.", "text": "paper presents framework tackle combinatorial optimization problems using neural networks reinforcement learning. focus traveling salesman problem train recurrent neural network that given city coordinates predicts distribution different city permutations. using negative tour length reward signal optimize parameters recurrent neural network using policy gradient method. compare learning network parameters training graphs learning individual test graphs. despite computational expense without much engineering heuristic designing neural combinatorial optimization achieves close optimal results euclidean graphs nodes. applied knapsack another np-hard problem method obtains optimal solutions instances items. combinatorial optimization fundamental problem computer science. canonical example traveling salesman problem given graph needs search space permutations optimal sequence nodes minimal total edge weights variants myriad applications planning manufacturing genetics etc. overview). finding optimal solution np-hard even two-dimensional euclidean case nodes points edge weights euclidean distances pairs points. practice solvers rely handcrafted heuristics guide search procedures competitive tours efﬁciently. even though heuristics work well problem statement changes slightly need revised. contrast machine learning methods potential applicable across many optimization tasks automatically discovering heuristics based training data thus requiring less handengineering solvers optimized task only. successful machine learning techniques fall family supervised learning mapping training inputs outputs learned supervised learning applicable combinatorial optimization problems access optimal labels. however compare quality solutions using veriﬁer provide reward feedbacks learning algorithm. hence follow reinforcement learning paradigm tackle combinatorial optimization. empirically demonstrate that even using optimal solutions labeled data optimize supervised mapping generalization rather poor compared agent explores different tours observes corresponding rewards. propose neural combinatorial optimization framework tackle combinatorial optimization problems using reinforcement learning neural networks. consider approaches based policy gradients ﬁrst approach called pretraining uses training optimize recurrent neural network parameterizes stochastic policy solutions using expected reward objective. test time policy ﬁxed performs inference greedy decoding sampling. second approach called active search involves pretraining. starts random policy iteratively optimizes parameters single test instance using expected reward objective keeping track best solution sampled search. combining pretraining active search works best practice. euclidean graphs nodes neural combinatorial optimization signiﬁcantly outperforms supervised learning approach obtains close optimal results allowed computation time. illustrate ﬂexibility testing method knapsack problem optimal results instances items. results give insights neural networks used general tool tackling combinatorial optimization problems especially difﬁcult design heuristics for. traveling salesman problem well studied combinatorial optimization problem many exact approximate algorithms proposed euclidean non-euclidean graphs. christoﬁdes proposes heuristic algorithm involves computing minimum-spanning tree minimum-weight perfect matching. algorithm polynomial running time returns solutions guaranteed within factor optimality metric instance tsp. best known exact dynamic programming algorithm complexity making infeasible scale large instances points. nevertheless state solvers thanks carefully handcrafted heuristics describe navigate space feasible solutions efﬁcient manner solve symmetric instances thousands nodes. concorde widely accepted best exact solvers makes cutting plane algorithms iteratively solving linear programming relaxations conjunction branch-andbound approach prunes parts search space provably contain optimal solution. similarly lin-kernighan-helsgaun heuristic inspired linkernighan heuristic state approximate search heuristic symmetric shown solve instances hundreds nodes optimality. generic solvers google’s vehicle routing problem solver tackles superset typically rely combination local search algorithms metaheuristics. local search algorithms apply speciﬁed local move operators candidate solutions based hand-engineered heuristics -opt navigate solution solution search space. metaheuristic applied propose uphill moves escape local optima. popular choice metaheuristic variants guided local search moves local minimum penalizing particular solution features considers occur good solution. difﬁculty applying existing search heuristics newly encountered problems even instances similar problem well-known challenge stems free lunch theorem search algorithms performance averaged problems must appropriately rely prior problems selecting search algorithm guarantee performance. challenge fostered interest raising level generality optimization systems operate underlying motivation behind hyper-heuristics deﬁned search method learning mechanism selecting generating heuristics solve computation search problems. hyper-heuristics easier problem speciﬁc methods partially abstracting away knowledge intensive process selecting heuristics given combinatorial problem shown successfully combine human-deﬁned heuristics superior ways across many tasks survey). however hyper-heuristics operate search space heuristics rather search space solutions therefore still initially relying human created heuristics. application neural networks combinatorial optimization distinguished history majority research focuses traveling salesman problem earliest proposals hopﬁeld networks tsp. authors modify network’s energy function make equivalent objective lagrange multipliers penalize violations problem’s constraints. limitation approach sensitive hyperparameters parameter initialization analyzed overcoming limitation central subsequent work ﬁeld especially parallel development hopﬁeld networks work using deformable template models solve tsp. perhaps prominent invention elastic nets means solve application self organizing addressing limitations deformable template models central following work area even though neural networks many appealing properties still limited research work. carefully benchmarked yielded satisfying results compared algorithmic methods perhaps negative results research direction largely overlooked since turn century. motivated recent advancements sequence-to-sequence learning neural networks subject study optimization various domains including discrete ones particular revisited introduction pointer networks recurrent network non-parametric softmaxes trained supervised manner predict sequence visited cities. despite architecural improvements models trained using supervised signals given approximate solver. focus euclidean paper. given input graph represented sequence cities dimensional space {xi}n concerned ﬁnding permutation points termed tour visits city minimum total length. deﬁne length tour deﬁned permutation denotes norm. learn parameters stochastic policy given input points assigns high probabilities short tours probabilities long tours. neural network architecture uses chain rule factorize probability tour inspired previous work makes factorization based chain rule address sequence sequence problems like machine translation. vanilla sequence sequence model address output vocabulary however major issues approach networks trained fashion cannot generalize inputs cities. needs access groundtruth output permutations optimize parameters conditional log-likelihood. address isssues paper. generalization beyond pre-speciﬁed graph size follow approach makes non-parameteric softmax modules resembling attention mechanism approach named pointer network allows model effectively point speciﬁc position input sequence rather predicting index value ﬁxed-size vocabulary. employ pointer network architecture depicted figure policy model parameterize encoder network reads input sequence city time transforms sequence latent memory states {enci}n enci input encoder network time step d-dimensional embedding point obtained linear transformation shared across input steps. decoder network also maintains latent memory states deci step uses pointing mechanism produce distribution {deci}n next city visit tour. next city selected passed input next decoder step. input ﬁrst decoder step d-dimensional vector treated trainable parameter neural network. attention function formally deﬁned appendix takes input query vector deci reference vectors {enc enck} enci predicts distribution references. probability distribution represents degree model pointing reference upon seeing query vinyals also suggest including additional computation steps named glimpses aggregate contributions different parts input sequence much like discuss approach details appendix experiments utilizing glimpse pointing mechanism yields performance gains insigniﬁcant cost latency. vinyals proposes training pointer network using supervised loss function comprising conditional log-likelihood factors cross entropy objective network’s output probabilities targets provided solver. learning examples undesirable np-hard problems performance model tied quality supervised labels getting high-quality labeled data expensive infeasible problem statements cares ﬁnding competitive solution replicating results another algorithm. contrast believe reinforcement learning provides appropriate paradigm training neural networks combinatorial optimization especially problems relatively simple reward mechanisms could even used test time. hence propose model-free policy-based reinforcement learning optimize parameters pointer network denoted training objective expected tour length which given input graph deﬁned training graphs drawn distribution total training objective involves sampling distribution graphs i.e. es∼s resort policy gradient methods stochastic gradient descent optimize parameters. gradient formulated using well-known reinforce algorithm simple popular choice baseline exponential moving average rewards obtained network time account fact policy improves training. choice baseline proved sufﬁcient improve upon christoﬁdes algorithm suffers able differentiate different input graphs. particular optimal tour difﬁcult graph still discouraged shared across instances batch. using parametric baseline estimate expected tour length π∼pθ typically improves learning. therefore introduce auxiliary network called critic parameterized learn expected tour length found current policy given input sequence critic trained stochastic gradient descent mean squared error objective predictions actual tour lengths sampled recent policy. additional objective formulated critic’s architecture tsp. explain critic maps input sequence baseline prediction critic comprises three neural network modules lstm encoder lstm process block -layer relu neural network decoder. encoder architecture pointer network’s encoder encodes input sequence sequence latent memory states hidden state process block similarly performs steps computation hidden state processing step updates hidden state glimpsing memory states described appendix feeds output glimpse function input next processing step. process block obtained hidden state decoded baseline prediction fully connected layers respectively unit. training algorithm described algorithm closely related asynchronous advantage actor-critic proposed difference sampled tour lengths critic’s predictions unbiased estimate advantage function. perform updates asynchronously across multiple workers worker also handles mini-batch graphs better gradient estimates. evaluating tour length inexpensive agent easily simulate search procedure inference time considering multiple candidate solutions graph selecting best. inference process resembles solvers search large feasible solutions. paper consider search strategies detailed below refer sampling active search. sampling. ﬁrst approach simply sample multiple candidate tours stochastic policy select shortest one. contrast heuristic solvers enforce model sample different tours process. however control diversity sampled tours temperature hyperparameter sampling non-parametric softmax sampling process yields signiﬁcant improvements greedy decoding always selects index largest probability. also considered perturbing pointing mechanism random noise greedily decoding obtained modiﬁed policy similarly proves less effective sampling experiments. active search. rather sampling ﬁxed model ignoring reward information obtained sampled solutions reﬁne parameters stochastic policy inference minimize π∼pθ single test input approach proves especially competitive starting trained model. remarkably also produces satisfying solutions starting untrained model. refer approaches pretraining-active search active search model actively updates parameters searching candidate solutions single test instance. active search applies policy gradients similarly algorithm draws monte carlo samples candidate solutions single test input. resorts exponential moving average baseline rather critic need differentiate inputs. active search training algorithm presented algorithm note training require supervision still requires training data hence generalization depends training data distribution. contrast active search distribution independent. finally since encode cities sequence randomly shufﬂe input sequence feeding pointer network. increases stochasticity sampling procedure leads large improvements active search. conduct experiments investigate behavior proposed neural combinatorial optimization methods. consider three benchmark tasks euclidean generate test graphs. points drawn uniformly random unit square across experiments mini-batches sequences lstm cells hidden units embed coordinates point -dimensional space. train models adam optimizer initial learning rate decay every steps factor initialize parameters uniformly random within clip norm gradients attention glimpse. searching mini-batches either consist replications test sequence permutations. baseline decay active search. model training code tensorﬂow made availabe soon. table summarizes conﬁgurations different search strategies used experiments. variations method experimental procedure results follows. supervised learning. addition described baselines implement train pointer network supervised learning similarly supervised data consists million optimal tours supervised learning results good reported suspect learning optimal tours harder supervised pointer networks subtle features model cannot ﬁgure looking given supervised targets. thus refer results report results suboptimal compared approaches. pretraining. experiments generate training mini-batches inputs update model parameters actor critic algorithm validation randomly generated instances hyper-parameters tuning. critic consists encoder network architecture policy network followed processing steps fully connected layers. clipping logits tanh activation function described appendix helps exploration yields marginal performance gains. simplest search strategy using pretrained model greedy decoding i.e. selecting city largest probability decoding step. also experiment decoding greedily pretrained models inference time. graph tour found individual model collected shortest tour chosen. refer approaches pretraining-greedy pretraining-greedy. pretraining-sampling. test instance sample candidate solutions pretrained model keep track shortest tour. grid search temperature hyperparameter found respective temperatures yield best results tsp. refer tuned temperature hyperparameter since sampling require parameter udpates entirely parallelizable larger batch size speed purposes. pretraining-active search. test instance initialize model parameters pretrained model active search training steps batch size sampling total candidate solutions. learning rate hundredth active search. allow model train much longer account fact starts scratch. test graph active search training steps tsp/tsp training steps tsp. compare methods different baselines increasing performance complexity christoﬁdes vehicle routing solver or-tools optimality. christoﬁdes solutions obtained polynomial time guaranteed within ratio optimality. or-tools improves christoﬁdes’ solutions simple local search operators including -opt version lin-kernighan heuristic stopping reaches local minimum. order escape poor local optima ortools’ local search also conjunction different metaheuristics simulated annealing tabu search guided local search or-tools’ vehicle routing solver tackle superset operates higher level generality solvers highly speciﬁc tsp. state-of-the common choice general routing problems provides reasonable baseline simplicity basic local search operators sophistication strongest solvers. optimal solutions obtained concorde lk-h’s local search concorde provably solves instances optimality empirically lk-h also achieves optimal solutions test sets. report average tour lengths approaches table notably results demonstrate training signiﬁcantly improves supervised learning methods comfortably surpass christoﬁdes’ heuristic including pretraining-greedy also rely search. table compares running times greedy methods aforementioned baselines methods running single nvidia tesla concorde lk-h running intel xeon .ghz ortool intel haswell cpu. greedy approaches time-efﬁcient percents worse optimality. searching inference time proves crucial closer optimality comes expense longer running times. fortunately search pretraining-sampling pretrainingactive search stopped early small performance tradeoff terms ﬁnal objective. seen table show performances corresponding running times function many solutions consider. also many pretraining methods outperform or-tools’ local search including pretraining-greedy runs similarly fast. table appendix presents performance metaheuristics consider solutions corresponding running times. table average tour lengths pretraining-sampling pretraining-active search sample solutions. corresponding running times single tesla parantheses. present detailed comparison methods figure sort ratios optimality different learning conﬁgurations. pretraining-sampling pretrainingactive search competitive neural combinatorial optimization methods recover optimal solution signiﬁcant number test cases. small solution spaces pretraining-sampling ﬁnetuned softmax temperature outperforms pretraining-active search latter sometimes orienting search towards suboptimal regions solution space furthermore pretraining-sampling beneﬁts fully parallelizable runs faster pretraining-active search. however larger solution spaces rl-pretraining active search proves superior controlling number sampled solutions running time. interestingly active search starts untrained model also produces competitive tours requires considerable amount time finally show randomly picked example tours found methods figure appendix a..make section discuss apply neural combinatorial optimization problems tsp. neural combinatorial optimization model architecture tied given combinatorial optimization problem. examples useful networks include pointer network output permutation truncated permutation subset input classical seqseq model kinds structured outputs. combinatorial problems require assign labels elements input graph coloring also possible combine pointer module softmax module simultaneously point assign decoding time. given model encodes instance given combinatorial optimization task repeatedly branches subtrees conadditionally also needs ensure feasibility obtained solutions. certain combinatorial problems straightforward know exactly branches lead feasible solutions decoding time. simply manually assign zero probability decoding similarly enforce model point city twice pointing mechanism however many combinatorial problems coming feasible solution challenge itself. consider example travelling salesman problem time windows travelling salesman additional constraint visiting city speciﬁc time window. might branches considered early tour lead solution respects time windows. cases knowing exactly branches feasible requires searching subtrees time-consuming process much easier directly searching optimal solution unless using problem-speciﬁc heuristics. rather explicitly constraining model sample feasible solutions also model learn respect problem’s constraints. simple approach veriﬁed experimentally future work consists augmenting objective function term penalizes solutions violating problem’s constraints similarly penalty methods constrained optimization. guarantee model consistently samples feasible solutions inference time necessarily problematic simply ignore infeasible solutions resample model also conceivable combine approaches assigning zero probabilities branches easily identiﬁable infeasible still penalizing infeasible solutions entirely constructed. example ﬂexibility neural combinatorial optimization consider knapsack problem another intensively studied problem computer science. given items ...n weight value maximum weight capacity knapsack problem consists maximizing values items present knapsack weights less equal knapsack capacity taking real values problem np-hard simple strong heuristic take items ordered weight-to-value ratios weight capacity. apply pointer network encode knapsack instance sequence vectors decoding time pointer network points items include knapsack stops total weight items collected exceeds weight capacity. generate three datasets knap knap knap thousand instances items’ weights values drawn uniformly random without loss generality capacities knap knap knap. present performances pretraining-greedy active search table compare simple baselines ﬁrst baseline greedy weight-to-value ratio heuristic; second baseline random search sample many feasible solutions seen active search. pretraining-greedy yields solutions that average less optimal active search solves instances optimality. paper presents neural combinatorial optimization framework tackle combinatorial optimization reinforcement learning neural networks. focus traveling salesman problem present results variation framework. experiments demonstrate neural combinatorial optimization achieves close optimal results euclidean graphs nodes. authors would like thank vincent furnon oriol vinyals barret zoph lukasz kaiser mustafa ispir google brain team insightful comments discussion. mart´ın abadi paul barham jianmin chen zhifeng chen andy davis jeffrey dean matthieu devin sanjay ghemawat geoffrey irving michael isard tensorﬂow system largescale machine learning. arxiv preprint arxiv. volodymyr mnih adri puigdomnech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. arxiv preprint arxiv. manfred padberg giovanni rinaldi. branch-and-cut algorithm resolution largescale symmetric traveling salesman problems. society industrial applied mathematics chen yutian hoffman matthew colmenarejo sergio gomez denil misha lillicrap timothy freitas nando. learning learn global optimization black functions. arxiv preprint arxiv. glimpse function essentially computes linear combination reference vectors weighted attention probabilities. also applied multiple times reference finally ultimate vector passed attention function produce probabilities pointing mechanism. observed empirically glimpsing parameters made model less likely learn barely improved results.", "year": 2016}