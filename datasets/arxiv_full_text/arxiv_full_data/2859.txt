{"title": "DAGs with NO TEARS: Smooth Optimization for Structure Learning", "tag": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "abstract": "Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint and are not well-suited to general purpose optimization packages for their solution. In this paper, we introduce a fundamentally different strategy: We formulate the structure learning problem as a smooth, constrained optimization problem over real matrices that avoids this combinatorial constraint entirely. This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting nonconvex, constrained program involves smooth functions whose gradients are easy to compute and only involve elementary matrix operations. By using existing black-box optimization routines, our method uses global search to find an optimal DAG and can be implemented in about 50 lines of Python and outperforms existing methods without imposing any structural constraints.", "text": "estimating structure directed acyclic graphs challenging problem since search space dags combinatorial scales superexponentially number nodes. existing approaches rely various local heuristics enforcing acyclicity constraint well-suited general purpose optimization packages solution. paper introduce fundamentally diﬀerent strategy formulate structure learning problem smooth constrained optimization problem real matrices avoids combinatorial constraint entirely. achieved novel characterization acyclicity smooth also exact. resulting nonconvex constrained program involves smooth functions whose gradients easy compute involve elementary matrix operations. using existing black-box optimization routines method uses global search optimal implemented lines python outperforms existing methods without imposing structural constraints. learning directed acyclic graphs data np-hard problem owing mainly combinatorial acyclicity constraint diﬃcult enforce eﬃciently. time dags popular models practice applications biology genetics machine learning causal inference reason development methods learning dags remains central challenge machine learning statistics. acyclic graphs matrix exponential. given real matrix rd×d interpret directed graph letting }d×d binary matrix zero otherwise; adjacency matrix directed graph. }d×d denote subset superexponentially moreover unconstrained algorithms gradient descent obviously cannot enforce constraint since acyclicity constraint nonconvex projection operator well-deﬁned rendering projection methods inadequate. compensate this specialized optimization schemes developed order enforce constraint. methods include cutting planes dynamic programming search order search greedy approaches coordinate descent also substantial progress learning dags various structural statistical assumptions approaches avoid optimizing directly include constraint-based methods hybrid methods bayesian methods long smooth smooth equality constrained program host optimization schemes available. longer need specialized algorithms tailored acyclicity constraint freely apply black-box constrained optimization solver optimize decoupling problem signiﬁcant practical implications since fast solvers developed without need specialized expertise graphical models learning. perhaps interestingly approach simple implemented lines python code. result simplicity eﬀortlessness implementation call resulting method notears non-combinatorial optimization trace exponential augmented lagrangian structure learning. instructive compare existing methods learning dags methods machine learning literature. focus popular models undirected graphical models deep neural networks. undirected graphical models also known markov networks written convex program hence solved using black-box convex optimizers recently extremely eﬃcient algorithms developed problem using coordinate descent newton methods deep neural networks often learned using various descendents stochastic gradient descent although recent work proposed techniques admm gauss-newton keys success models—and many models machine learning—was closed-form tractable program existing techniques extensive optimization literature could applied. cases application principled optimization techniques signiﬁcant breakthroughs. undirected graphical models major technical tool convex optimization deep networks major technical tool sgd. unfortunately general problem learning beneﬁtted arguably intractable form program host heuristic methods often borrowing tools optimization literature always resorting clever heuristics accelerate algorithms. section discuss pros cons existing methods. methods suﬀer problems highlighted below aware methods simultaneously avoid them. broadly speaking camps approximate algorithms exact algorithms latter guaranteed return globally optimal solution. exact algorithms form intriguing class methods based around np-hard combinatorial optimization problem methods remain computationally intractable general. example recent state-of-the-art work scale problems dozen nodes older methods based dynamic programming methods also scale roughly dozen nodes. contrast state-of-the-art approximate methods scale thousands nodes arguably popular approaches optimizing involve local search wherein edges parent sets added sequentially node time. eﬃcient long node parents number possible parents grows local search rapidly becomes intractable. furthermore strategies typically rely severe structural assumptions bounded in-degree bounded treewidth edge constraints. since real-world networks well-known exhibit scale-free small-world topologies highly connected nodes kinds structural assumptions diﬃcult satisfy impossible check. note promising work towards relaxing assumption discrete data contrast method uses global search wherein entire matrix updated step. literature learning tends split methods operate discrete data methods operate continuous data. viewed lens reasons clear since discrete continuous data considered special cases general score-based learning framework. nonetheless much methods cited already work speciﬁc assumptions data common categorical gaussian since agnostic form data loss function signiﬁcant interest ﬁnding general methods tied speciﬁc model assumptions. finally higher level signiﬁcant drawback existing methods conceptual complexity straightforward implement require deep knowledge concepts graphical modeling literature accelerating involves many clever tricks. contrast method propose paper conceptually simple requires background graphical models implemented lines code using existing black-box solvers. dag-ness mean quantiﬁcation severe violations acyclicity become gets example might proportional number cycles useful practice diagnostics. function exists rd×d rd×d case simple equality constraint equivalent acyclicity constraint course particularly useful derivatives computable. assuming derivatives exist computed hope apply existing machinery constrained optimization lagrange multipliers penalty barrier methods etc. consequently learning problem becomes equivalent solving numerical optimization problem respect matrix discrete space cannot take gradients continuous optimization restrictive assumption must relaxed. second step relax function originally deﬁne binary matrices real matrices unfortunately condition strong although automatically satisﬁed generally true otherwise furthermore projection nontrivial. alternatively instead require however impractical numerical reasons entries easily exceed machine precision even small values makes function gradient evaluations highly unstable. therefore remains characterization holds possible also numerical stability. worth pointing matrix exponential well-deﬁned square matrices. addition everywhere convergence characterization added bonus number edges increases along number nodes number possible cycles grows rapidly trace characterization unfortunately characterization fails replace arbitrary weighted matrix however replace nonnegative weighted matrix argument prove proposition shows still characterize acyclicity. thus extend matrices positive note proof proposition shows power series simply counts number cycles matrix exponential simply re-weights counts. replacing amounts thus larger means either counting weighted cycles weight edge cycles cycles heavily weighted moreover notice since term series nonnegative. gives another theorem establishes existence smooth algebraic characterization acyclicity also computable. implies following equality constrained program equivalent still inherit diﬃculties associated nonconvex optimization. compared however major upshot simplicity tractability solved stationarity using standard techniques smooth constrained optimization. classical algorithm constrained problems projected gradient descent projects iterates produced gradient descent back feasible step. unfortunately since feasible nonconvex eﬃcient projection operator available. therefore need look general methods solve ecp. simple option penalty method solves parameter penalizes violations constraint unfortunately penalty methods known ill-conditioned since needs large order enforce approximate feasibility penalty parameter unlike penalty method nice property augmented lagrangian method approximates well solution constrained problem solution unconstrained problem without increasing penalty parameter inﬁnity making objective reasonably conditioned throughout. detailed outline augmented lagrangian scheme listed algorithm note eﬀectively implemented lines python requires minimal change diﬀerent choice loss since returned dag. however since designed quantify dag-ness \u0001-feasible solution often close enough dag. number algorithms selecting acyclic subgraphs describe easy post-processing procedure deﬁne smallest threshold element-wise indicator function. since removing edges preserves acyclicity know case desired graph sparsity {ω}’s. combining algorithm solving post-processing procedure described previous paragraph obtain complete algorithm returning data call algorithm notears. compared method fast greedy search algorithm lingam since accuracy lingam signiﬁcantly lower either notears report results here. consistent previous work score-based learning chosen since state-of-the-art algorithm scales large problems. dataset notears compare performance reconstructing used highly optimized java implementation comments order estimates graph output parameter estimates; instead returning returns cpdag contains undirected edges; single tuning parameter controls strength regularization. thus evaluations treated favourably treating undirected edges true positives long true graph directed edge place undirected edge. tuning parameters used values suggested authors code. denote estimate returned bfgs. based estimate returned tuned notears threshold selected graph number edges bfgs fairly compares method graphs roughly complexity. denote estimate resulting adjacency matrix completeness also chose oracle threshold level minimized true graph. useful illustrating lower performance metrics compared bfgs four common graph metrics false discovery rate true positive rate false positive rate structural hamming distance recall total number edge additions deletions reversals needed convert estimated true dag. since consider directed graphs distinction true positives reversed edges needed former estimated correct direction whereas latter figure results simulation experiments. lower better except higher better. columns correspond four data generating mechanisms; rows correspond four graph ensembles used error bars represent conﬁdence intervals given metric simulations. not. likewise false positive edge undirected skeleton true graph. addition positive estimated edges true true edges false non-edges ground truth graph. finally extra edges skeleton missing edges skeleton. four metrics given results experiments shown figures consistent previous work greedy methods competitive number edges small rapidly deterioriates even modest numbers edges latter regime notears shows signiﬁcant improvements. consistent across four metrics diﬀerence even larger number nodes gets larger. also notice algorithm performs uniformly better cases gaussian gumbel noise without leveraging speciﬁc knowledge noise type. also observe small continuous data signiﬁcant advantage fgs. note therefore large numerator happens large. furthermore many cases much diﬀerence estimated best selected knowing ground truth graph. note slow convergence black-box solvers used unable obtain results logistic models. also likely explanation drop performance logistic data well. future work plan study convergence notears logistic loss closely. also compared notears real dataset provided sachs dataset consists continuous measurements expression levels proteins phospholipids human immune system cells dataset common benchmark graphical models since comes known consensus network gold standard network based experimental annotations widely accepted biological community. experiments estimated total edges compared notears small-scale experiment interpreted inital validation. although enforce explicit sparsity regularization output notears sparse matrix. illustrated figure implicit sparsity consequence trace exponential moreover implicit regularization yields results competitive employs explicit regularization. future work would interesting explicit sparse regularization method output improved further. worth emphasizing equality constrained program nonconvex program. thus although overcome diﬃculties combinatorial optimization formulation still inherits diﬃculties associated nonconvex optimization. particular black-box solvers best stationary points exception exact methods however existing methods suﬀer drawback well. main advantage notears smooth global search opposed combinatorial local search. discussed section drawbacks existing approaches strong dependence topology true graph. since notears based continuous optimization edges updated time instead entire weight matrix updated once. result accuracy notears strongly aﬀected changes underlying network topology. observations illustrated convincingly experiments wherein shows good performance sparse graphs substanital drop-oﬀ number edges nodes increases. contrast notears robust modiﬁcations. although unique approach note methods algorithm output cpdag instead return parameter estimates. notears course estimates genuine simultaneously returning parameter estimates edge weights proposed characterization acyclicity trace exponential function applied problem score-based learning dags. compared existing methods able implement notears using black-box solvers without specialized optimization routines. moreover simulations notears shows dramatic improvements graphs realistic topologies scale-free networks. although notears demonstrated competitive state-of-the-art methods main contribution work simple elegant reformulation learning terms smooth optimization problem. hope formulation able inspire faster robust methods solving problem much like optimization literature done countless methods machine learning literature.", "year": 2018}