{"title": "Hierarchical Memory Networks", "tag": ["stat.ML", "cs.CL", "cs.LG", "cs.NE"], "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.", "text": "memory networks neural networks explicit memory component read written network. memory often addressed soft using softmax function making end-to-end training backpropagation possible. however computationally scalable applications require network read extremely large memories. hand well known hard attention mechanisms based reinforcement learning challenging train successfully. paper explore form hierarchical memory network considered hybrid hard soft attention memory networks. memory organized hierarchical structure reading done less computation soft attention memory also easier train hard attention memory. speciﬁcally propose incorporate maximum inner product search training inference procedures hierarchical memory network. explore various state-of-the approximate mips techniques report results simplequestions challenging large scale factoid question answering task. recently traditional machine learning approaches challenging tasks image captioning object detection machine translation consisted complex pipelines algorithms separately tuned better performance. recent success neural networks deep learning research become possible train single model end-to-end using backpropagation. end-to-end systems often outperform traditional approaches since entire model directly optimized respect ﬁnal task hand. however simple encode-decode style neural networks often underperform knowledge-based reasoning tasks like question-answering dialog systems. indeed cases nearly impossible regular neural networks store necessary knowledge parameters. neural networks memory deal knowledge bases external memory component used explicitly store knowledge. memory accessed reader writer functions made differentiable entire architecture trained end-to-end using backpropagation. memory-based architectures also considered generalizations rnns lstms memory analogous recurrent hidden states. however much richer structure handle long-term dependencies vector stored copied time step time step thus stay long time exists several variants neural networks memory component memory networks neural turing machines dynamic memory networks share major components memory input module reader writer output module. memory memory array cells capable storing vector. memory often initialized external data ﬁlling cells pre-trained vector representations data. input module input module compute representation input used modules. writer writer takes input representation updates memory based writer simple ﬁlling slots memory input vectors sequential memory bounded instead sequential writing writer decide write rewrite cells reader given input current state memory reader retrieves content memory used output module. often requires comparing input’s representation function recurrent state memory cells using scoring function product. output module given content retrieved reader output module generates prediction often takes form conditional distribution multiple labels output. rest paper name memory network describe model form components. would like highlight components except memory learnable. depending application components also ﬁxed. paper focus situation network write reads memory. paper focus application memory networks large-scale tasks. speciﬁcally focus large scale factoid question answering. problem given large facts natural language question goal system answer question retrieving supporting fact question answer derived. application memory networks task studied however depended keyword based heuristics ﬁlter facts smaller manageable training. however heuristics invariably dataset dependent interested general solution used facts structure. design soft attention retrieval mechanisms convex combination cells retrieved design hard attention retrieval mechanisms cells memory retrieved. soft attention achieved using softmax memory makes reader differentiable hence learning done using gradient descent. hard attention achieved using methods like reinforce provides noisy gradient estimate discrete stochastic decisions made model. soft attention hard attention limitations. size memory grows soft attention using softmax weighting scalable. computationally expensive since complexity linear size memory. also initialization gradients dispersed much reduce effectiveness gradient descent. problems alleviated hard attention mechanism training method choice reinforce. however reinforce brittle high variance existing variance reduction techniques complex. thus rarely used memory networks paper propose memory selection mechanism based maximum inner product search scalable easy train. considered hybrid soft hard attention mechanisms. idea structure memory hierarchical easy perform mips hence name hierarchical memory network hmns scalable training inference time. main contributions paper follows since exact mips computationally expensive full soft attention model propose train memory networks using approximate mips techniques scalable memory access. section describe proposed hierarchical memory network paper hmns differ regular memory networks components memory reader. memory instead array cells memory structure hmns leverages hierarchical memory structure. memory cells organized groups groups organized higher level groups. choice memory structure tightly coupled choice reader essential fast memory access. consider three classes approaches memory’s structure hashing-based approaches tree-based approaches clustering-based approaches. explained detail next section. reader reader different readers memory networks. flat memorybased readers either soft attention entire memory hard attention retrieves single cell. mechanisms might work small memories hmns interested achieving scalability towards large memories. instead readers soft attention selected subset memory. selecting memory subsets guided maximum inner product search algorithm exploit hierarchical structure organized memory retrieve relevant facts sub-linear time. mips-based reader explained detail next section. hmns reader thus trained create mips queries retrieve sufﬁcient facts. standard applications mips focused settings query vector database vectors precomputed ﬁxed memory readers hmns learning mips updating input representation result mips retrieval contains correct fact. section describe memory reader uses maximum inner product search learning inference. begin formal deﬁnition k-mips. given points query vector goal argmax returns indices top-k maximum values. case hmns corresponds memory corresponds vector computed input module. simple inefﬁcient solution k-mips involves linear search cells memory performing product memory cells. return exact result k-mips costly perform deal large-scale memory. however many practical applications often sufﬁcient approximate result k-mips trading speed-up cost accuracy. exist several approximate k-mips solutions literature approximate k-mips solutions form hierarchical structure memory visit subset memory cells maximum inner product given query. hashing-based approaches hash cells multiple bins given query search k-mips cell vectors bins close associated query. tree-based approaches create search trees cells leaves tree. given query path tree followed mips performed leaf chosen path. clustering-based approaches cluster cells multiple clusters given query perform mips centroids clusters. refer readers extensive comparison various state-of-the-art approaches approximate k-mips. proposal exploit rich approximate k-mips literature achieve scalable training inference hmns. instead ﬁltering memory heuristics propose organize memory based approximate k-mips algorithms train reader learn perform mips. speciﬁcally consider following softmax memory reader perform every reading step retrieve relevant candidates advantage using softmax naturally focuses cells would normally receive strongest gradients learning. full softmax gradients otherwise dispersed across cells given large number cells despite many contributing small gradient. experiments show results slower training. problematic situation learning softmax initial stages training k-mips reader including correct fact candidate. avoid issue always include correct candidate top-k candidates retrieved k-mips algorithm effectively performing fully supervised form learning. training reader updated backpropagation output module subset memory cells. additionally log-likelihood correct fact computed using k-softmax also maximized. second supervision helps reader learn modify query maximum inner product query respect memory yield correct supporting fact candidate set. described exact k-mips-based learning framework still requires linear look-up memory cells would prohibitive large-scale memories. scenarios replace exact k-mips training procedure approximate k-mips. achieved deploying suitable memory hierarchical structure. approximate k-mipsbased reader used inference stage well. course approximate k-mips algorithms might return exact mips candidates likely hurt performance beneﬁt achieving scalability. memory representation ﬁxed paper updating memory along query representation improve likelihood choosing correct fact. however updating memory reduce precision approximate k-mips algorithms since assume vectors memory static. designing efﬁcient dynamic k-mips improve performance hmns even further challenge hope address future work. clustering-based approximate k-mips proposed shown outperform various state-of-the-art data dependent data independent approximate k-mips approaches inference tasks. show experiments section clustering-based mips also performs better used training hmns. hence focus presentation clustering-based approach propose changes found helpful learning hmns. following approximate k-mips algorithms converts mips maximum cosine similarity search problem data vectors norm mcss equivalent mips. however often restrictive additional constraint. instead appends additional dimensions query data vectors convert mips mcss. terminology would correspond adding dimensions memory cells input representations. algorithm introduces hyper-parameters ﬁrst step scale vectors memory factor maxi ||xi|| apply mappings memory cells input vector respectively. mappings simply concatenate components vectors make norms data points roughly mappings deﬁned follows hierarchical version approximate speedup cosine similarity search. memory clustered every read operation requires dot-products number cluster centroids. since approximation error-prone. using approximation learning process introduces bias gradients affect overall performance hmn. alleviate bias propose three simple strategies. instead using top-k candidates single read query also top-k candidates retrieved every read query mini-batch. serves purposes. first efﬁcient matrix multiplications leveraging gpus since k-softmax minibatch elements. second also helps decrease bias introduced approximation error. every read access instead using clusters maximum product read query also sample clusters rest based probability distribution log-proportional product cluster centroids. also decreases bias. memory networks introduced applied comprehension-based question answering large scale question answering dialogue systems considered supervised memory networks correct supporting fact given training stage introduced semi-supervised memory networks learn supporting fact itself. introduced dynamic memory networks considered memory network types memory regular large memory episodic memory. another related class model neural turing machine uses softmax-based soft attention. later extended hard attention using reinforcement learning. alleviate problem scalability soft attention initial keyword based ﬁltering stage reduces number facts considered. work generalizes ﬁltering using mips ﬁltering. desirable mips applied modality data even overlap words question words facts. softmax arises various situations relevant work scaling methods large vocabulary neural language modeling. neural language modeling ﬁnal layer softmax distribution next word exist several approaches achieve scalability. proposes hierarchical softmax based prior clustering words binary generally n-ary tree serves ﬁxed structure learning process model. complexity training reduced clustering tree structure resembles clusteringbased mips techniques explore paper. however approaches differ fundamental level. hierarchical softmax deﬁnes probability leaf node product probabilities computed intermediate softmaxes leaf node. contrast approximate mips search imposes constraining structure probabilistic model better thought efﬁciently searching winners amounts large ordinary softmax. methods noice constrastive estimation negative sampling avoid expensive normalization constant sampling negative samples marginal distribution. contrast approach approximates softmax explicitly including negative samples candidates likely would large softmax value. introduces importance sampling approach considers words mini-batch candidate set. general might also include mips candidates highest softmax values. work know proposing mips learning. proposes hashingbased mips sort hidden layer activations reduce computation every layer. however small scale application considered data-independent methods like hashing likely suffer dimensionality increases. section report experiments factoid question answering using hierarchical memory networks. speciﬁcally simplequestions dataset experiments achieve state-of-the-art results dataset. rather propose analyze various approaches make memory networks scalable explore achieved tradeoffs speed accuracy. simplequestions large scale factoid question answering dataset. simplequestions consists natural language questions paired corresponding fact freebase. fact triple answer question always object. dataset divided training validation test sets. unlike additionally considered keyword-based heuristics ﬁltering facts question simplequestions keyword-based heuristics. allows direct comparison full softmax approach reasonable amount time. moreover would like highlight dataset keyword-based ﬁltering efﬁcient heuristic since questions appropriate source entity matching word. nevertheless goal design general purpose architecture without strong assumptions nature data. model vocabulary words natural language questions. |vq| matrix dimensional embedding word question vocabulary. matrix initialized random values learned training. given question represent bag-of-words representation summing vector representation word question. {wi}p then relevant fact memory call k-mips-based reader module query. uses equation compute output reader rout. reader trained minimizing negative likelihood correct fact. index correct fact ﬁxing memory embeddings transe embeddings learning question embeddings. model simpler reported esay analyze effect various memory reading strategies. trained model adam optimizer ﬁxed learning rate used mini-batches size used dimensional embeddings transe entities yielding dimensional embeddings facts concatenating embeddings subject relation object. also experimented summing entities triple instead concatenating found difﬁcult model differentiate facts way. learnable parameters model question word embeddings. entity distribution simplequestions extremely sparse hence following also artiﬁcial questions facts natural language questions. unlike additional tasks like paraphrase detection model mainly study effect reader. stopped training models validation accuracy consistently decreased epochs. section compare performance full soft attention reader exact k-mips attention readers. goal verify k-mips attention fact valid useful attention mechanism fares compared full soft attention. k-mips attention tried would like emphasize that training time along candidates particular question also k-candidates question mini-batch. exact size softmax layer would higer training. table report test performance memory networks using soft attention reader k-mips attention reader. also report average softmax size training. table clear k-mips attention readers improve performance network compared soft attention reader. fact smaller value better performance. result suggests better k-mips layer instead softmax layer whenever possible. interesting convergence model slowed change softmax computation experiment conﬁrms usefulness k-mips attention. however exact k-mips complexity full softmax. hence scale training need efﬁcient forms k-mips attention focus next experiment. mentioned previously designing faster algorithms k-mips active area research. compared several state-of-the-art data-dependent data-independent methods faster approximate k-mips found clustering-based mips performs signiﬁcantly better approaches. however focus comparison performance inference stage. hmns k-mips must used training stage inference stages. verify trend seen learning stage well compared three different approaches clustering explained detail section wta-hash winner takes hashing hashing-based k-mips algorithm also converts mips mcss augmenting additional dimensions vectors. method used hash functions hash function different random permutations vector. preﬁx constituted ﬁrst elements permuted vector used construct hash vector. pca-tree pca-tree state-of-the-art tree-based method converts mips vector augmentation. uses principal components data construct balanced binary tree data residing leaves. fair comparison varied hyper-parameters algorithm average speedup approximately same. table shows performance three methods compared full softmax. table clear clustering-based method performs signiﬁcantly better methods. however performances lower compared performance full softmax. next experiment analyze various strategies proposed section reduce approximation bias clustering-based k-mips top-k strategy picks vectors clusters candidates. sample-k strategy samples clusters without replacement based probability distribution based product query cluster centroids. combined top-k strategy ignore clusters selected top-k strategy sampling. rand-block strategy divides memory several blocks uniformly samples random block candidate. experimented clusters clusters. comparing various training strategies made sure effective speedup approximately same. memory access facts query models approximately hence yielding speedup. results given table observe best approach combine top-k sample-k strategies rand-block beneﬁcial. interestingly worst performances correspond cases sample-k strategy ignored. paper proposed hierarchical memory network exploits k-mips attentionbased reader. unlike soft attention readers k-mips attention reader easily scalable larger memories. achieved organizing memory hierarchical way. experiments simplequestions dataset demonstrate exact k-mips attention better soft attention. however existing state-of-the-art approximate k-mips techniques provide speedup cost accuracy. future research investigate designing efﬁcient dynamic k-mips algorithms memory dynamically updated training. reduce approximation bias hence improve overall performance. anshumali shrivastava ping improved asymmetric locality sensitive hashing maximum inner product search proceedings conference uncertainty artiﬁcial intelligence jesse dodge andreea gane xiang zhang antoine bordes sumit chopra alexander miller arthur szlam jason weston. evaluating prerequisite qualities learning end-to-end dialog systems. corr abs/.", "year": 2016}