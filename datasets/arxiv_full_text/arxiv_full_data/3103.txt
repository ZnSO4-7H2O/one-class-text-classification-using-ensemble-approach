{"title": "Improved Performance of Unsupervised Method by Renovated K-Means", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Clustering is a separation of data into groups of similar objects. Every group called cluster consists of objects that are similar to one another and dissimilar to objects of other groups. In this paper, the K-Means algorithm is implemented by three distance functions and to identify the optimal distance function for clustering methods. The proposed K-Means algorithm is compared with K-Means, Static Weighted K-Means (SWK-Means) and Dynamic Weighted K-Means (DWK-Means) algorithm by using Davis Bouldin index, Execution Time and Iteration count methods. Experimental results show that the proposed K-Means algorithm performed better on Iris and Wine dataset when compared with other three clustering methods.", "text": "abstract clustering separation data groups similar objects. every group called cluster consists objects similar another dissimilar objects groups. paper k-means algorithm implemented three distance functions identify optimal distance function clustering methods. proposed k-means algorithm compared k-means static weighted k-means dynamic weighted k-means algorithm using davis bouldin index execution time iteration count methods. experimental results show proposed k-means algorithm performed better iris wine dataset compared three clustering methods. clustering clustering technique group together items similar characteristics. clustering considered important problem. like every problem kind deals finding structure collection unlabeled data. loose definition clustering could process organizing objects groups whose members similar way. cluster therefore collection objects similar dissimilar objects belonging clusters. show simple graphical example. dealing different types attributes. discovering clusters arbitrary shape. ability deal noise outliers. insensitivity order input records. high dimensionality forming clusters distance matrices. hierarchical techniques optimization techniques mixture model important types. discuss first types here. discuss mixture models separate note includes classification regression well clustering. data divided. similarity criterion distance objects belong cluster close according given distance called distance-based clustering. another kind clustering conceptual clustering. objects belong cluster defines concept common objects. search possible labeling optimum value criterion clearly computationally prohibitive. practice therefore algorithm typically multiple times different starting states best configuration obtained runs issued output clustering. data discover groups similar access patterns. rest paper organized follows section describes three different distance functions execution time method clustering algorithms viz. k-means weighted k-means proposed k-means clustering algorithms studied implemented. section presents experimental analysis conducted various data sets data repository section concludes paper. k-means algorithm iterative procedure clustering requires initial classification data. computes center cluster computes partitions assigning every object cluster whose center closest object. cycle repeated given number iterations assignment changed iteration. algorithm based approach random cluster base selected original dataset element update nearest element base average attributes. k-means possibly commonly-used clustering algorithm. effective relatively smaller data sets. k-means finds locally optimal solution minimizing distance measure data nearest cluster center. basic k-means algorithm commonly measured intra-cluster inter-cluster criterion. typical intra-cluster criterion squared-error criterion. commonly used good measure within-cluster variation across partitions. process iterates following steps representing nested grouping patterns similarity levels groupings change. dendogram broken different levels yield different clustering data. hierarchical clustering algorithms variants single-link complete-link minimum-variance algorithms. single-link complete link algorithms popular. algorithms differ characterize similarity pair clusters. single-link method distance clusters minimum distances pairs patterns drawn clusters complete-link algorithm distance clusters minimum pair wise distances patterns clusters. either case clusters merged form larger cluster based minimum distance criteria. complete-link algorithm produces tightly bound compact clusters. single-link algorithm contrast suffers chaining effect. tendency produce clusters straggly elongated. clusters obtained complete link algorithm compact obtained single-link algorithm. single partition data instead clustering structure dendogram produced hierarchical technique. partitional methods advantages applications involving large data sets construction dendogram computationally prohibitive. problem accompanying partitional algorithm choice number desired output clusters. partitional technique usually produce clusters optimizing criterion function defined either locally globally combinatorial explained following section. static weighted kmeans weight fixed constant dynamic weight calculated equation weighted k-means clustering algorithm explained algorithm proposed method first determines initial cluster centroids using equation given following algorithm proposed k-means algorithm improved selecting initial centroids manually instead selecting centroids randomly. selects objects initially represents cluster mean centroids. remaining objects object assigned cluster similar based distance object cluster mean. computes mean cluster. process iterates criterion function converges. paper proposed k-means algorithm implemented instead traditional k-means explained algorithm k-means algorithm works follows. first iteratively selects objects initially represents cluster mean center. remaining objects object assigned cluster similar based distance object cluster mean. computes mean cluster. process iterates criterion function converges. typically euclidean distance used. weighted k-means clustering algorithm weighted k-means algorithm clustering algorithms based k-means algorithm calculating weights. algorithm normal k-means algorithm adding weights. weighted k-means attempts decompose objects disjoint clusters taking consideration fact numerical attributes objects often come independent identical normal distribution. weighted kmeans algorithms iterative hill-climbing find optimal solution thus usually gives converge local minimum. below. first iteratively selects objects initially represents cluster mean center. selecting centroids calculate weights using weighted k-means algorithm. remaining objects object assigned cluster similar based distance objects centroids weights corresponding object. computes mean cluster. process iterates criterion function converge. typically euclidean distance used clustering process. determine similarity dissimilarity pair objects. useful denote distance instances valid distance measure symmetric obtains minimum value case identical vectors. distance functions classified types. flower. data contains samples four attributes. dataset collected location given link. http//archive.ics.uci.edu/ml/ machinelearning-databases /iris/ iris.data cluster validity common goal find clustering results compact clusters well separated. clustering validity concept used evaluate quality clustering results. clustering validity index also used find optimal number clusters measure compactness separation clusters. chosen cluster validity measure shown able detect correct number clusters several experiments. davis bouldin validity combination functions. first calculates compactness data cluster second computes separateness data different clusters. ratio within-cluster scatter betweencluster separation. dispersion cluster dvij denotes dissimilarity clusters cluster similarity matrix â€¦..c} defined functions k-means studied compared dataset ecoli. k-means clustering methods executed varying cluster centroids data set. clearly shows euclidean distance function obtained minimum index values different clusters values. hence euclidean distance function better clustering algorithms distance functions. four algorithms studied section implemented software matlab methods executed compared ecoli iris yeast wine dataset. davis bouldin index used determine performance clustering algorithms. results obtained various clustering algorithms listed table below. samples attributes. dataset collected location given link. http//archive.ics.uci.edu/ml/machine-learning-databases /ecoli/ecoli.data yeast yeast dataset contains samples attributes. origin wines. contains samples attributes dataset collected location given link. http//archive.ics.uci.edu/ml/machine-learningdatabases/ yeast/wine.data performance distance functions k-means clustering method executed three different distance functions manhattan euclidean chebyshev iris dataset used select centroid value obtained results listed table given below. clustering algorithms executed iris dataset varying cluster centroids proposed kmeans algorithm obtained minimum execution time clustering centroids. swk-means clustering algorithm produce minimum execution time various values algorithms produce minimum execution time centroids values. hence proposed k-means clustering executed minimum execution time performed better algorithms. four clustering algorithms compared performance using iteration count method wine dataset. iteration count defined number times clustering algorithm executed convergence criteria met. cluster centres increased time number iterations clustering algorithms obtained listed table algorithms executed four different data called ecoli iris yeast wine constant cluster centroid whose value proposed k-means algorithm obtained minimum index values ecoli wine data also obtained next minimum index algorithm. hence proposed k-means clustering algorithm obtained good clustering results. different clustering algorithms compared performances using time required cluster dataset. execution time varying selecting number initial cluster centroids. execution time increased number cluster centroid increased. obtained results depicted following table partitional clustering studied applying k-means algorithm three different distance functions find optimal distance function clustering process. demerits k-means algorithm random selection initial centroids desired clusters. overcome proposed k-means initial cluster centroid selection process finding initial centroids avoid selecting centroids randomly produces distinct better results. four clustering algorithms executed four different dataset proposed k-means method performs well obtains minimum index value. execution time iteration count compared four different clustering algorithms different cluster values. proposed kmeans achieved less execution time minimum iteration swkcount k-means static weighted k-means dynamic weighted k-means clustering methods. therefore proposed k-means clustering method applied application area various cluster validity measures used improve cluster performance future work.", "year": 2013}