{"title": "Neural Component Analysis for Fault Detection", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Principal component analysis (PCA) is largely adopted for chemical process monitoring and numerous PCA-based systems have been developed to solve various fault detection and diagnosis problems. Since PCA-based methods assume that the monitored process is linear, nonlinear PCA models, such as autoencoder models and kernel principal component analysis (KPCA), has been proposed and applied to nonlinear process monitoring. However, KPCA-based methods need to perform eigen-decomposition (ED) on the kernel Gram matrix whose dimensions depend on the number of training data. Moreover, prefixed kernel parameters cannot be most effective for different faults which may need different parameters to maximize their respective detection performances. Autoencoder models lack the consideration of orthogonal constraints which is crucial for PCA-based algorithms. To address these problems, this paper proposes a novel nonlinear method, called neural component analysis (NCA), which intends to train a feedforward neural work with orthogonal constraints such as those used in PCA. NCA can adaptively learn its parameters through backpropagation and the dimensionality of the nonlinear features has no relationship with the number of training samples. Extensive experimental results on the Tennessee Eastman (TE) benchmark process show the superiority of NCA in terms of missed detection rate (MDR) and false alarm rate (FAR). The source code of NCA can be found in https://github.com/haitaozhao/Neural-Component-Analysis.git.", "text": "lower dimensional feature space performing fault detection fault diagnosis feature space principal component analysis widely used linear techniques fault detection. orthogonal linear projection separates data information subspaces signiﬁcant subspace contains variation training data residual subspace includes noises outliers training data. pca-based methods inherently nonlinear processes lead unreliable inefﬁcient fault detection since linear transformation hard tackle nonlinear relationship different process variables deal problem various nonlinear extensions proposed fault detection. extensions divided categories. ﬁrst category kernel approaches. kernel mostly used kernel approaches fault detection kpca implicitly maps data input space high dimensional nonlinear feature sapce linear applied. kpca need perform eigen decomposition kernel gram matrix whose size square number data points. many data points calculation becomes hard perform moreover kpca need determine kernel associated parameters advance. second category based linear approximation nonlinear process. linear approximation several local linear models constructed integrated bayesian inference linear approximation simple easy realize able handle strong nonlinearities process. third category neural-network-based models robust autoencoder autoassociative neural network models train feedforward neural network perform identity encoding inputs outputs network same. network contains internal bottleneck layer feature extraction. autoassociative neural network mapping input layer bottleneck layer considered encoding de-mapping bottleneck layer output layer considered decoding. although encoding deal nonlinearities present data consideration orthogonal constraints used pca. recent years trend neural-network-based techniques known deep learning become popular artiﬁcial intelligence machine learning deep-learning-based models widely used unsupervised training learn representation original data. although models often derived viewed extensions laboratory advanced control optimization chemical processes ministry education school information science engineering east china university science technology largely adopted chemical process monitoring numerous pcabased systems developed solve various fault detection diagnosis problems. since pca-based methods assume monitored process linear nonlinear models autoencoder models kernel principal component analysis proposed applied nonlinear process monitoring. however kpca-based methods need perform eigen-decomposition kernel gram matrix whose dimensions depend number training data. moreover preﬁxed kernel parameters cannot effective different faults need different parameters maximize respective detection performances. autoencoder models lack consideration orthogonal constraints crucial pcabased algorithms. address problems paper proposes novel nonlinear method called neural component analysis intends train feedforward neural work orthogonal constraints used pca. adaptively learn parameters backpropagation dimensionality nonlinear features relationship number training samples. extensive experimental results tennessee eastman benchmark process show superiority terms missed detection rate false alarm rate source code found https//github.com/haitaozhao/neural-component-analysis.git. note practitioner online monitoring chemical process considered critical hard task real industrial applications. paper innovative method called neural component analysis proposed fault detection. uniﬁed model including nonlinear encoder linear decoder. simple intuitive format superior performance computational efﬁciency fault detection makes suitable process monitoring real industrial applications. moreover experimental results presented reproduced effortlessly. monitoring process conditions crucial normal operation last decades data-driven multivariate statistical process monitoring widely applied fault diagnosis industrial process operations production results data-based nature mspm relatively convenient apply real processes large scale comparing methods based theoretical modelling rigorous derivation process systems task mspm challenging mainly curse dimensionality problem data rich information poor problem. many methods proposed transform original high dimensional process data feature usually referred latent code latent feature latent representation. here element-wise activation function sigmoid function hyperbolic tangent function. matrix parameter matrix bias vector. lack consideration orthogonal constraints used pca. orthogonal constraints quite important since largely reduce correlations extracted features. figure shows simple plots features vector obtained orthogonal projections non-orthogonal projections respectively. figure easy largely correlated. means extracted features contain redundant information distort reconstruction original vector motivated analysis paper proposes novel uniﬁed model called neural component analysis fault detection. ﬁrstly utilizes nonlinear neural network encoder extract features. linear orthogonal transformation adopted decode features original data space. finaly uniﬁed model trained minimizing reconstruction error original data decoded data. training used unsupervised learning method extract features process data. paper hotelling statistic squared prediction error statistic used fault detection. merits proposed method demonstrated theoretical analysis case studies tennessee eastman benchmark process. artiﬁcial neural network adopted unsupervised feature extraction autoencoder model tries learn representation original data speciﬁcally purpose dimensionality reduction. recently research works deep learning autoencoder concept widely accepted generative models data samples assume simplest structure autoencoder model feedforward neural network consists input layer inputs hidden layer units output layer number nodes input layer purpose structure reconstruct inputs. therefore autoencoder model belongs unsupervised learning. equation shows difference autoencoder. firstly uniﬁed model nonlinear encoding linear decoding. linear decoding shown later computation transformation matrix quite simple gradient-descent-based optimization needed. secondly orthogonormal constraints ip×p added nca. means decoding latent features orthogonal reconstruction largely reduce correlation different variables. matrix corresponding score matrix features want obtain analysis fault detection diagnosis. however difﬁcult compute optimal simultaneously since rn×p rn×p linear transformation matrices. baldi hornik showed that covariance matrix associated data invertible unique local global minimum equation corresponding orthogonal projection onto subspace spanned ﬁrst principal eigenvectors covariance matrix precisely optimal solutions obtain un×p un×p eigenvectors corresponding ﬁrst largest eigenvalues covariance matrix u··· deﬁned loading vectors principal components score matrix corresponding loading matrix although orthogonal constraints transformation matrices solutions equation composed orthogonormal bases. orthogonal decomposition transform original data space orthogonal subspaces principal subspace contains variation original data residual subspace includes noises outliers. statistic statistic often adopted indicators fault detection corresponding principal subspace residual subspace respectively. orthogonal decomposition minimizes correlations between subspaces makes linear reconstruction original data least distortion orthogonal property widely used process monitoring many fault detection methods considered extensions nonlinear autoencoder model trained extract latent features. however lacking orthogonal property signiﬁcant information original data information noises outliers largely combined model. autoencoder model turns overﬁt original data learns capture much information possible rather reducing correlations original data extracting signiﬁcant information. problem nonlinear autoencoder models used widely pca. section propose novel uniﬁed model called neural component analysis fault detection. ﬁrstly utilizes nonlinear neural network encoder extract features. linear orthogonal transformation adopted decode features original data space. considered combination nonlinear linear models. variable; performed obtain neural network nonlinear feature extraction. finally hotelling squared prediction error statistics used fault detection. nonlinear latent features covariance matrix associated prior information available distribution compute conﬁdence limit statistics approximately kernel density estimation unknown density statistics g··· kernel density estimator statistic new) xnew following condition checked obtained backpropagation algorithm widely adopted training feedforward neural networks. solution equation determine orthogonal matrix rotates original data matrix linear algebra statistics procrustes analysis standard technique geometric transformations matrices. orthogonal procrustes problem viewed matrix approximation problem tries optimal rotation reﬂection transformation matrix respect other. theorem shows solve reduced rank procrustes rotation problem. theorem reduced rank procrustes rotation. mn×n nn×p matrices. consider constrained minimization problem novel fault detection method based developed section. implementation procedures given follows. firstly modeling stage process data collected normal process conditions scaled tennessee eastman process widely used process monitoring community source publicly available data comparing different algorithms. simulated mainly based practical industrial process kinetics operation units altered speciﬁc reasons. data generated nonlinear strong coupling dynamic major units chemical reactor condenser recycle compressor vapor/liquid separator stripper. sheet implemented control structure shown figure matlab codes downloaded http//depts.washington.edu/control/larry/te/download.html. besides normal data simulator also generate different types faults order test process monitoring algorithms. selected monitoring variables experiments. training data contained normal data. twenty-one different faults generated data fault chosen testing fault happened data data. fault modes listed table paper compare proposed method kpca autoencoder. kpca widely used gaussian kernel select kernel parameter mean standard deviations different variables order intuitively visualize features different methods extract features method plot figure figure blue indicates normal data indicates fault data fault found normal data fault data largely overlapped figure case kpca training stage. found converges fast. thanks gpu-accelerated computing total training time seconds. perform experiment computer intel core .ghz nvidia geforce comparison kpca performed computer training time seconds. subsection investigate performance proposed nca. performance compared kpca autoencoder. source codes methods found https//github.com/haitaozhao/neuralcomponent-analysis.git. according cumulative percentage variance rule reduced dimensionality determined energy eigenspetrum retained. order give fair comparison value used kpca autoencoder nca. missed detection rate refers rate abnormal events falsely identiﬁed normal events monitoring process applied fault detection situation. testing faults recorded together table smaller values indicate better performances. false alarm rate refers normal process monitoring results kpca autoencoder shown parentheses. small fars also indicate better performances. table best achieved performance fault highlighted bold. study consider fault cases mdr< far≤ mdr≥ detection performance would even worse random guess whose moreover adopt threshold value commonly used fault detection. outperforms kpca autoencoder cases lower mdrs. autoencoder gives best performance cases kpca gives best performance cases gives best performance cases. autoencoder kpca provide better results autoencoder cannot signiﬁcant information fault detection. however figure overlapping normal samples fault samples much less figure nonlinear feature extraction based orthogonal constraints directly gives features two-dimensional space include signiﬁcant information fault detection. figure shows difference loading matrix pca. figure shows result easy contains orthogonormal columns i.e. moreover otherwise correlations columns plotted figure obviously nonlinear feature extract optimum solution utterly different loading matrix pca. fault cases indicates nonlinear extensions generally obtain better fault detection results linear pca. although kpca include orthogonal constraints feature extraction performances much better kpca. reason adaptively learns parameters neural network kpca uses preﬁxed kernel associated parameter. obtains different parameters different variables nonlinear combination backpropagation strategy much suitable nonlinear feature extraction following fault detection tasks. figure illustrate detailed fault detection results fault ﬁgures blue points indicate ﬁrst normal samples points represent following fault samples. black dash lines control limits according threshold blue points control limits lead false alarm points control limits cause missed detection. according table methods successfully detect fault fars statistic statistic respectively. fars kpca statistic statistic. autoencoder achieves false alarm statistic statistic however proposed fars zero statistics. figure shows plots results four methods fault method subplot results statistic subplot results statistic. small overlay plots clearly show results ﬁrst normal samples subplot. based table figure fault found lowest false alarm. figure illustrates results fault experiment neither kpca autoencoder detect fault. using statistic. means kpca autoencoder suitable detection figure shows detection results four methods fault statistic greater reason high linear decomposition cannot obtain appropriate residual subspace determine right control limit. kpca detect fault statistic autoencoder statistic much higher kpca. outperforms nonlinear methods missed detection statistic cause high autoencoder statistic autoencoder overﬁts training data becomes prone accept noises outliers normal data construct signiﬁcant subspace residual subspace. condition features residual subspace contain enough information detect fault samples. thanks orthogonal constraints kpca successfully detect fault data. although single method gives optimal performance fault cases consisting diverse numbers fault conditions. outperforms kpca autoencoder regard number best performances emerges clear winner. since considers nonlinear feature extraction orthogonal constraints feature extraction becomes effective fault detection. although iterative method need time training paper propose nonlinear feature extraction method nerual component analysis fault detection. uniﬁed model includes nonlinear encoder part linear decoder part. orthogonal constraints adopted alleviate overﬁtting problem occurred autoencoder improve performances fault detection. takes advantages backpropogation technique eigenvalue-based techniques. convergence iteration scheme fast. idea behind general potentially extended detection diagnosis problems process monitoring. compare linear nonlinear fault detection methods kpca autoencoder. based case studies clear outperforms kpca autoencoder. considered alternative prevalent data driven fault detection techniques. future works contributed design regularization terms optimization problem equation reconstruction-based feature extraction methods kpca autoencoder mainly focus global euclidean structure process data overlook latent local correlation process data. future design constraints ensure local information considered nonlinear feature extraction. ding haghani zhang comparison study basic data-driven fault diagnosis process monitoring methods benchmark tennessee eastman process journal process control vol. askarian escudero graells zarghami jalali-farahani mostouﬁ fault diagnosis chemical processes incomplete observations comparative study computers chemical engineering vol. wang multiscale neighborhood normalizationbased multiple dynamic monitoring method batch processes frequent operations ieee transactions automation science engineering vol. rato blue pinaton reis translation-invariant multiscale energy-based monitoring batch processes semiconductor manufacturing ieee transactions automation science engineering vol. april mansouri nounou nounou karim kernel pca-based glrt nonlinear fault detection chemical processes journal loss prevention process industries vol. supplement zhang improved training algorithm back propagation neural network self-adaptive learning rate international conference computational intelligence natural computing vol. june", "year": 2017}