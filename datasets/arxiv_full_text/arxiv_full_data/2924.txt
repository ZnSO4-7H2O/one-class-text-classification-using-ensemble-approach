{"title": "Measuring Catastrophic Forgetting in Neural Networks", "tag": ["cs.AI", "cs.CV", "cs.LG"], "abstract": "Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than re-training the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem has yet to be solved.", "text": "figure catastrophic forgetting impairs incremental learning neural networks. network incrementally trained ideally performance would match model trained ofﬂine data upfront paper develop methods benchmarks measuring catastrophic forgetting. experiments show even methods designed prevent catastrophic forgetting perform signiﬁcantly worse ofﬂine model. incremental learning many real-world applications allows model adapt deployed. catastrophically forgetting previously learned training data. fixing problem critical making agents incrementally improve deployment. non-embedded personalized systems catastrophic forgetting often overcome simply storing training examples re-training either entire network scratch possibly last layers. cases retraining uses previously learned examples examples randomly shufﬂing independent identically distributed retraining slow especially dataset millions billions instances. catastrophic forgetting ﬁrst recognized mlps years since then multiple attempts mitigate phenomenon however methods vary considerably train evaluate models focus small datasets e.g. deep neural networks used many state-of-the-art systems machine perception. network trained speciﬁc task e.g. bird classiﬁcation cannot easily trained tasks e.g. incrementally learning recognize additional bird species learning entirely different task ﬂower recognition. tasks added typical deep neural networks prone catastrophically forgetting previous tasks. networks capable assimilating information incrementally much like humans form memories time efﬁcient retraining model scratch time task needs learned. multiple attempts develop schemes mitigate catastrophic forgetting methods directly compared tests used evaluate vary considerably methods evaluated small-scale problems paper introduce metrics benchmarks directly comparing different mechanisms designed mitigate catastrophic forgetting neural networks regularization ensembling rehearsal dual-memory sparse-coding. experiments real-world images sounds show mechanism critical optimal performance vary based incremental training paradigm type data used demonstrate catastrophic forgetting problem solved. basic architecture training algorithms behind deep neural networks years interest never greater industry artiﬁcial intelligence research community. owing larger datasets increases computational power innovations activation functions dnns achieved near-human super-human abilities number problems including image classiﬁcation speech-to-text face identiﬁcation algorithms power recent advances semantic segmentation visual question answering reinforcement learning systems become capable standard multi-layer perceptron architecture typical training algorithms cannot handle incrementally learning tasks categories withmnist. clear methods scale larger datasets containing hundreds categories. paper remedy problem providing comprehensive empirical review methods mitigate catastrophic forgetting across variety metrics. catastrophic forgetting occurs unsupervised frameworks focus supervised classiﬁcation. major contributions demonstrate despite popular claims regularization ensembling rehearsal dual-memory models sparsecoding. unlike previous work directly compare distinct approaches. paper study catastrophic forgetting mlp-based neural networks incrementally trained classiﬁcation tasks. setup labeled training dataset organized study sessions i.e. study session consists labeled training data discrete label. variable across sessions. model permitted learn sessions sequentially order. time network learn study session however models auxiliary memory store previously observed sessions memory must reported. assume sessions e.g. sessions contain data single category. sessions model evaluated test data. paper’s focus catastrophic forgetting focus less representation learning obtain feature vectors using embeddings pre-trained networks. note papers sessions called ‘tasks.’ refer ﬁrst study session model’s ‘base knowledge.’ catastrophic forgetting neural networks occurs stability-plasticity dilemma model requires sufﬁcient plasticity acquire tasks large weight changes cause forgetting disrupting previously learned representations. keeping network’s weights stable prevents previously learned tasks forgotten much stability prevents model learning tasks. prior research tried solve problem using broad approaches. ﬁrst keep representations separate done using distributed models regularization ensembling. second prevent forgetting prior knowledge simply training tasks well tasks thereby preventing tasks forgotten. besides requiring costly relearning previous examples additional storage scheme still effective simply combining data data completely re-training model scratch. solution inefﬁcient prevents development deployable systems capable learning tasks course lifetime. french exhaustively reviewed mechanisms preventing catastrophic forgetting explored goodfellow compared different activation functions learning algorithms affected catastrophic forgetting methods explicitly designed mitigate catastrophic forgetting. authors concluded learning algorithms larger impact focus paper. sequentially trained network separate tasks using three different scenarios identical tasks different forms input similar tasks dissimilar tasks. adopt similar paradigm experiments involve much larger number tasks. also focus methods explicitly designed mitigate catastrophic forgetting. soltoggio stanley risi reviewed neural networks adapt plasticity time called evolved plastic artiﬁcial neural networks. review covered wide-range brain-inspired algorithms also identiﬁed ﬁeld lacks appropriate benchmarks. however conduct experiments establish benchmarks measuring catastrophic forgetting. remedy literature establishing large-scale benchmarks evaluating catastrophic forgetting neural networks compare methods distinct mechanisms mitigating exhaustive identiﬁed main approaches pursued mitigating catastrophic forgetting mlp-like architectures describe next subsections. describe models selected greater detail experimental setup section. regularization methods constraints network’s weight updates session learned without interfering prior memories. hinton plaut implemented network ‘fast’ ‘slow’ training weights. fast weights high plasticity easily affected changes network ‘slow’ weights high stability harder adapt. kind dual-weight architecture similar idea dual-network models proven sufﬁciently powerful learn large number tasks. elastic weight consolidation adds constraint loss function directs plasticity away weights ensemble methods attempt mitigate catastrophic forgetting either explicitly implicitly training multiple classiﬁers together combining generate ﬁnal prediction. explicit methods learn++ tradaboost prevents forgetting entirely sub-network trained session however memory usage scale number sessions highly non-desirable. moreover prevents portions network reused session. methods alleviate memory usage problem accuracy weighted ensembles life-long machine learning methods automatically decide whether sub-network removed added ensemble. pathnet considered implicit ensemble method uses genetic algorithm optimal path ﬁxed-size neural network study session. weights path frozen; sessions learned knowledge lost. contrast explicit ensembles base network’s size ﬁxed possible learned representations re-used allows smaller deployable models. authors showed pathnet learned subsequent tasks quickly well earlier tasks retained. selected pathnet evaluate ensembling mechanism show well retains pre-trained information. rehearsal methods mitigate catastrophic forgetting mixing data earlier sessions current session learned cost requires storing past data resource efﬁcient. pseudorehearsal methods network generate pseudopatterns combined session currently learned. pseudopatterns allow network stabilize older memories without requirement storing previously observed training data points. draelos used approach incrementally train autoencoder session contained images speciﬁc category. autoencoder learned particular session passed session’s data encoder stored output statistics. replay used statistics decoder network generate appropriate pseudopatterns class. geppnet model proposed gepperth karaoguz reserves training data replay class trained. model used self-organizing hidden-layer topologically reorganize data input layer model explore value rehearsal. ries distinct neural networks. newly formed memories stored brain region known hippocampus. memories slowly transferred/consolidated pre-frontal cortex sleep. several algorithms based ideas created. early work used fast slow training networks separate pattern-processing areas passed pseudopatterns back forth consolidate recent remote memories general dual-memory models incorporate rehearsal rehearsal-based models dualmemory models. another model proposed gepperth karaoguz denote geppnet+stm stores inputs yield highly uncertain prediction short-term memory buffer. model seeks consolidate memories entire network separate sleep phase. showed geppnet+stm could incrementally learn mnist classes without forgetting previously trained ones. geppnet geppnet+stm evaluate dual-memory approach. catastrophic forgetting occurs internal representations interfere previously learned ones sparse representations reduce chance interference; however sparsity impair generalization ability learn tasks models implicitly sparsity calm alcove. learn data calm searches among competing nodes nodes committed another representation alcove shallow neural network uses sparse distance-based representation allows weights assigned older tasks largely unchanged network presented data sparse distributed memory convolution-correlation model uses sparsity reduce overlap internal representations charm todam also convolution-correlation models internal codings ensure input representations remain orthogonal another fixed expansion layer model creates sparse representations ﬁxing network’s weights specifying neuron triggering conditions uses excitatory inhibitory ﬁxed weights sparsify input gates weight updates throughnetwork. enables network retain prior learned mappings reduce representational overlap. evaluate sparsity mechanism. explore well methods mitigate catastrophic forgetting scale hard datasets involving ﬁne-grained image audio classiﬁcation. datasets chosen because contain different data modalities large number classes small number samples class. datasets meaningful practical mnist. mental materials supplemental materials provides stopping criteria model deﬁned creators involved training ﬁxed period time using test accuracy stop training early. standard multi-layer perceptron baseline standard mlp. architecture chosen optimizing performance using entire training audioset i.e. trained ofﬂine. ofﬂine achieves accuracy cub- test audioset test set. hyperparameter search number units hidden layer number hidden layers weight decay parameter model also trained incrementally measure severity catastrophic forgetting. combined loss function network’s parameters loss session hyperparameter indicates important task compared task fisher information matrix trainable parameters important previously trained tasks. fisher matrix used constrain weights important previously learned tasks original value; plasticity directed trainable parameters contribute least performing previously trained tasks. size hidden-layer chosen match baseline mlp’s capacity. pathnet pathnet ﬁxed size neural network uses genetic algorithm optimal path network. path trainable learning particular session authors described model evolutionary dropout network. pathnet creates independent output layer task order preserve previously trained tasks cannot used without modiﬁcations incremental class learning. since entire portions network sequentially frozen tasks learned risk pathnet losing ability learn maximum capacity reached. pathnet’s capacity chosen match capacity baseline. geppnet geppnet geppnet+stm biologicallyinspired approaches rehearsal mitigate forgetting. models training initial task starts initializing som-layer used project probability density input higher two-dimensional lattice. som-layer features passed linear regression classiﬁcation layer make prediction. training somlayer initialized ﬁrst session ﬁxed-period time somclassiﬁcation-layers trained jointly. som-layer updated training example determined model novel image classiﬁcation dataset containing different bird species version. high-resolution image turned -dimensional vector resnet- deep convolutional neural network pre-trained imagenet extracting image features last hidden layer pre-trained dcnns common practice computer vision. report mean-per-class accuracy cub- standard. audioset audioset hierarchically organized audio classiﬁcation dataset built youtube videos. million human-labeled second sound bytes drawn classes. used pre-extracted frame-wise features audioset concatenated order. features extracted using variant resnet- audio data pre-trained early version youtube-m dataset used classes audioset none super sub-classes other. classes restrictions based audioset ontology quality estimation audio sample multiple labels chose training testing samples labeled classes. evaluated models correspond mechanisms described previous section pathnet geppnet geppnet+stm fel. choose number parameters across models established baseline architecture performed well cub- audioset trained ofﬂine. goal determine mechanism work best various incremental learning paradigms. provide fair comparison number parameters model chosen close possible number parameters baseline mlp. optimized model’s hyperparameters work well benchmarks given suppletotal number sessions αnewi test accuracy session immediately learned αbasei test accuracy ﬁrst session sessions learned αall test accuracy test data classes seen point αideal ofﬂine accuracy base assume ideal performance. ωbase ωnew normalized area curve metrics. ωbase measures model’s retention ﬁrst session learning later study sessions. ωnew measures model’s ability immediately recall tasks. normalizing ωbase ωall αideal results easier compare datasets. unless model exceeds αideal results enables comparison datasets. ωall computes well model retains prior knowledge acquires information. data permutation experiment experiment evaluates model’s ability retain multiple representations dataset representation learned sequentially. representations created randomly permuting elements input feature vectors random permutation changing sessions. identically permuted test used along session. paradigm provides overlapping tasks session contains information categories session equal complexity. paradigm identical used goodfellow kirkpatrick results given table nearly every case ωall greater mnist cub- audioset demonstrating need alternative incremental learning benchmarks. extent pathnet geppnet geppnet+stm retain prior knowledge without forgetting; however geppnet geppnet+stm fail learn sessions. pathnet seem retain base knowledge still learning information; however pathnet performs better audioset mnist performs better cub- incremental class learning incremental class learning experiment model’s ability sequentially learn classes tested. ﬁrst session learned contains training data half classes dataset mnist cub- audioset. base learned subsequent session contained training data single class. measure meanper-class accuracy base class learned assess model’s long-term memory. also calculate accuracy class trained ensure prediction probabilities generate conﬁdence measure). geppnet trained initial session ﬁxed period time incrementally learns subsequent sessions. geppnet performs updates somlayer classiﬁcation-layer training example considered novel. geppnet+stm detects novelty instead uses ﬁxed-size short-term memory buffer store training example replays sleep phase. sleep phase repeats ﬁxed number training iterations. since replay queue ﬁxed-size geppnet+stm model train efﬁciently geppnet. geppnet stores previous training data replays along previous data portion incremental learning step. geppnet+stm also stores previous training data; however training example replayed model uncertain prediction. addition geppnet+stm capable making real-time predictions determining desired memory short-term memory long-term storage fixed expansion layer uses sparsity mitigate catastrophic forgetting hidden-layer second hiddenlayer higher capacity ﬁrst fullyconnected layer weights sparse remain ﬁxed training. fel-layer unit connected subset units ﬁrst hidden layer connections split excitatory inhibitory weights. subset fel-layer units allowed non-zero output ﬁnal classiﬁcation layer causes units ﬁrst hidden layer updated. data permutation experiment elements every feature vector randomly permuted permutation held constant within session varying across sessions. model evaluated ability recall data learned prior study sessions. session contains number examples. data permutation incremental class learning experiments model also evaluated mnist. goal examine whether results mnist generalize real-world datasets. results including detailed plots found supplementary materials. pathnet incapable learning classes incrementally creates separate output layer additional session. accessing output layer prediction time requires priori information session model needs access. means pathnet requires testing label make appropriate prediction would result misleading high test accuracy. reason omitted pathnet experiment. results summarized table fig. contains plots mean-per-class test accuracy classes seen far. models able retain base knowledge learn classes geppnet geppnet+stm clear winner geppnet. much like data permutation experiment cub- audioset results noticeably lower mnist results. geppnet+stm well retaining base struggled learn classes cub- audioset. could model trains sleep efﬁciency reasons. additionally short-term memory buffer emptied training study session model evaluated. type model could work better real-time environment. learned classes well suffered forgetting base set. beneﬁt larger model capacity would require memory/processing power. learning tasks multi-modal data using single network could efﬁcient building separate neural network modality experiment evaluated model’s ability perform image audio classiﬁcation cub- audioset respectively. experiment incrementally learned sessions session contains audioset cub-. compare learning audioset ﬁrst cub- learning audioset resnet features obtained cub- higher dimensionality audioset features zero-padded audioset input match dimensionality cub-. experiment done training dataset completion followed training dataset completion modalities trained evaluate ﬁrst modality trained order measure well model able retain learned ﬁrst task. table shows summary results multi-modal experiment corresponding modality trained ﬁrst i.e. cub- cub- learned ﬁrst followed audioset. additional results supplementary materials. although several models perform well experiments model capable preserving ﬁrst modality also learning second modality cases explore discussion. multi-modal experiment goal multi-modal experiment determine network learn retain multiple dissimilar tasks inputs different dimensionality feature distributions different number classes. system like could useful also enabling learning information. table summarizes results experiments averaging ωall datasets. method excels incremental learning perform better others. pathnet performed best overall data permutation experiments exception cub-. however pathnet requires told session test instance from whereas models information. give unfair advantage. pathnet works locking optimal path given session. permuting data reduce feature overlap model requires trainable weights build discriminative model causing pathnet saturate quickly. pathnet reaches saturation point trainable parameters output layer. second best performing method permutation experiments redirects plasticity instead freezing trainable weights. geppnet variants performed best incremental class learning. models make slow gradual changes network inspired memory consolidation sleep. models som-layer ﬁxed number trainable parameters models. classes corresponds hidden layer neurons class respectively. experiments mnist gepperth karaoguz used hidden-layer neurons class performance improve model capacity signiﬁcantly increased would demand memory computation. choice separating non-redundant data pathnet work well working data different entirely dissimilar representations. explore this used fast correlation based filter proposed show features mnist audioset redundant cub- performance pathnet data permutation multi-modal experiments consistent hypothesis. table shows memory constraints usage model. kept number trainable parameters roughly across models hidden layers require additional memory resources. pathnet generates output layer session. geppnet variants store training data rehearse incremental learning stage. creators stored validation data previous sessions used minimize forgetting learning session. done experiments fairly compare models access validation data current session. table shows total time train model data permutation incremental class learning experiments using cub-. variants geppnet orders magnitude slower train model sample time. pathnet also slow data permutation task optimal path large dcnn needs found permutation. ﬁxed-size models noticeably faster; however effective mitigating catastrophic forgetting paper developed metrics evaluating catastrophic forgetting. identiﬁed families mechanisms mitigating catastrophic forgetting dnns. found performance mnist signiﬁcantly better larger datasets used. using metrics experimental results show combination rehearsal/pseudo-rehearsal dualmemory systems optimal learning classes incrementally regularization ensembling best separating multiple dissimilar sessions common framework. although rehearsal system performed reasonably well required retaining training data replay. type system scalable real-world lifelong learning system; however indicate models pseudorehearsal could viable option realtime incremental learning systems. future work lifelong learning frameworks involve combinations mechanisms. models perform better others different scenarios work shows catastrophic forgetting solved single method. model capable assimilating information simultaneously efﬁciently preserving old. urge community larger datasets future work. table summary optimal performer incremental class learning data permutation multi-modal experiments well memory/computational efﬁciency model. sessions allowed store data prior sessions limited real-world application. opinion methods mitigating catastrophic forgetting amount total memory constrained. summary statistics take account important factor deploying method learns incrementally. reason chose keep number trainable parameters ﬁxed across models. alternative would tune number trainable parameters model experiment data permutation incremental class learning experiments well although cases base performance increased changes conclusions model/mechanism yielded superior performance. interesting thing observe sparsity model sometimes improve signiﬁcantly; however cost increase models memory footprint. reinforces claim model uses sparsity mechanism mitigate catastrophic forgetting ideal deployed environment. urge future incremental learning algorithm creators take memory footprint account especially comparing models. training paradigms reinforcement learning unsupervised learning etc. reinforcement learning agent learns initial study-session represents base knowledge. would track performance base-knowledge model learns additional games ensure model learning games well. main difference performance metrics would normalized maximum performance studysession model learn single study session. unsupervised learning could follow experiments performed metrics would same would train models using different loss function", "year": 2017}