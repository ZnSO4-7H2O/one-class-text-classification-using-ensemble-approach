{"title": "Knowledge Completion for Generics using Guided Tensor Factorization", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Given a knowledge base (KB) rich in facts about common nouns or generics, such as \"all trees produce oxygen\" or \"some animals live in forests\", we consider the problem of deriving additional such facts at a high precision. While this problem has received much attention for named entity KBs such as Freebase, little emphasis has been placed on generics despite their importance for capturing general knowledge. Different from named entity KBs, generics KBs involve implicit or explicit quantification, have more complex underlying regularities, are substantially more incomplete, and violate the commonly used locally closed world assumption (LCWA). Consequently, existing completion methods struggle with this new task. We observe that external information, such as relation schemas and entity taxonomies, if used correctly, can be surprisingly powerful in addressing the challenges associated with generics. Using this insight, we propose a simple yet effective knowledge guided tensor factorization approach that achieves state-of-the-art results on two generics KBs for science, doubling their size at 74\\%-86\\% precision. Further, to address the paucity of facts about rare entities such as oriole (a bird), we present a novel taxonomy guided submodular active learning method to collect additional annotations that are over five times more effective in inferring further new facts than multiple active learning baselines.", "text": "given knowledge base rich facts common nouns generics trees produce oxygen some animals live forests consider problem deriving additional facts high precision. problem received much attention named entity freebase little emphasis placed generics despite importance capturing general knowledge. diﬀerent named entity generics involve implicit explicit quantiﬁcation complex underlying regularities substantially incomplete violate commonly used locally closed world assumption consequently existing completion methods struggle task. observe external information relation schemas entity taxonomies used correctly surprisingly powerful addressing challenges associated generics. using insight propose simple eﬀective knowledge guided tensor factorization approach achieves state-of-the-art results generics science doubling size precision. further address paucity facts rare entities oriole present novel taxonomy guided submodular active learning method collect additional annotations times eﬀective inferring facts multiple active learning baselines. consider problem completing partial knowledge base containing facts generics common nouns represented third-order tensor triples since generics represent classes similar individuals truth value generics triple depends quantiﬁcation semantics associates natural setting especially relevant noisy facts derived automatically text associate categorical quantiﬁcation {all some none} associate some. instance butterﬂies pollinate ﬂower some animals live forest. treat quantiﬁcation categorical label triple given triples task infer many triples possible high precision. tensor factorization graph based methods found eﬀective expanding knowledge bases focused named entity freebase involving relations clear semantics bornin isacityin disambiguated entities barack obama hawaii. observed many reliable horn clauses ﬁrst-order logic rules rare part generic representing collection entities often similarities diﬀerences w.r.t. various relations. example true many cats caribou little tangible similarity animals. hand generics often come additional rich background knowledge complementing information present itself taxonomic hierarchy entities corresponding entity types relation schema. insight that used appropriately taxonomic schema information surprisingly eﬀective making tensor factorization methods vastly eﬀective generics deriving high precision facts. intuitively many properties generics tend shared siblings taxonomy siblings named entities diﬀer widely propose three ways using information empirically demonstrate eﬀectiveness generics. first observe simply imposing schema consistency derived facts boost performance state-of-the-art methods holographic embeddings nearly facts precision facts starting generics size. low-dimensional embedding methods transe rescal cntf also obtained facts precision. graph-based completion methods scale densely connected tensors. second boost performance transferring knowledge taxonomic hierarchy using quantiﬁcation semantics generics show simply expanding starting tensor applying tensor factorization results statistically signiﬁcant higher precision facts yield. finally propose novel limited-budget taxonomy guided active learning method address challenge signiﬁcant incompleteness generics quantifying uncertainty siblings reliable facts generics much harder derive using state-of-the-art information extraction methods facts named entities. large number many-to-many relations makes challenging cover true facts using textual information books corpora. this turn makes generics vastly incomplete little information certain entities caribou oriole. predict facts previously unseen rarely seen entity ﬁrst identify annotate small number active queries feed factorization framework infer facts speciﬁcally deﬁne correlation based measure uncertainty triple involving based frequently corresponding triple true ˜e’s siblings taxonomic hierarchy propose submodular objective function optimally balances diversity coverage demonstrate annotating balanced subset makes tensor factorization derive substantially interesting facts compared several active learning baselines. smaller animals tensor generated high-precision facts hours. typically produces better results unable ﬁnish training classiﬁer relation day. hand hole trained couple minutes even larger science tensor. traversal techniques complete highly eﬀective named entity class solutions doesn’t scale well setting diﬀerent connectivity characteristics generics tensors compared named entity ones several embedding-based methods highly successful completion. compare recent work incorporating entity type relation schema tensor factorization focused factual databases diﬀerent characteristics generics tensors. nimishakavi entity type information matrix context non-negative rescal schema induction medical research documents. byproduct complete missing entries tensor schema-compatible manner. show proposal performs better generics tensors method cntf. chang trescal system also incorporates types rescal manner similar cntf. schema-aware discriminative training embeddings ﬂexible ratio negative samples schema consistent schema inconsistent triples. combined ideas however improve upon vanilla hole standard dataset. also consider imposing hierarchical types freebase entities diﬀerent meanings diﬀerent types issue typically apply generics kbs. incorporating given ﬁrst order logic rules explored simpler case matrix factorization consider inferring facts entity given ‘description’ entity. convolutional neural networks encode description deriving embedding description context would correspond knowing factual triples restricted version active learning setting. krishnamurthy singh consider active learning decomposition tensors. start empty tensor look informative slices columns completely achieve optimal sample complexity. incoherence assumption column space framework builds upon however apply generics consider knowledge expressed terms triples abbreviated triple refer style facts commonly used information extraction. source target entity generic noun e.g. animals habitats food items. examples relations include foundin etc. generics triple addition list triples assume access background information form entity types corresponding relation schema well taxonomic hierarchy. denote possible entity types. relation relation schema imposes type constraint entities appear source target. speciﬁcally schema collection example all) some) none). predicting labels thus multi-class classiﬁcation problem. given labeled triples goal tensor factorization learn low-dimensional embedding entity relation function best captures given labels. given triple learned predict probability label often contains positive triples i.e. label some. common step discriminative training thus negative sampling i.e. generating additional triples label none. deduced eqns. model capture asymmetry relations. addition circular correlation performed using fast fourier transform making hole highly eﬃcient. occur relation incorporate knowledge training test times. test time simply translates relabeling schema inconsistent predicted triples none. incorporating knowledge training time done constraint random negative samples method generates complement given typically positive triples resulting negative samples mimic true distribution labels. worth noting locally closed world assumption plays important role determining ratio. however idea used literature without considering nature dataset resulting seemingly contradicting empirical results optimal ratio challenging come complex horn ﬁrst order logic rules generics entity represents class individuals behave identically. however derive simple highly eﬀective rules based categorical quantiﬁcation labels leveraging fact entities robust. speciﬁcally given initial triples applicable rules derive additional triples perform tensor factorization revisit triples using predicted address incomplete nature generics consider rare entities facts entities present taxonomy facts goal tensor factorization generate high quality facts entities. instance consider task inferring facts oriole know bird. assume restricted budget number facts query oriole using would like predict many high-quality facts given ﬁxed query budget optimal queries generate human annotation rare entity task? view active learning problem propose two-step algorithm. first taxonomy guided uncertainty sampling propose list potentially query. next describe submodular objective function corresponding human annotation append result original perform tensor factorization predict additional facts notational simplicity without loss generality throughout section consider case appears source entity triple; ideas apply equally appears target entity triple. discuss active learning speciﬁcally uncertainty sampling method propose list triples query. uncertainty sampling considers uncertainty possible triple deﬁned minus conditional probability fact given facts already know question model conditional probability. simple baseline consider random queries i.e. selected randomly list relations entities tensor respectively. expected agree well. algorithm example oriole siblings birds exist tensor e.g. hummingbird ﬁnch hummingbird woodpecker etc. hence infer oriole. agreement hence added query list. since queried samples eventually tensor factorization would like cover would like diverse i.e. prioritize relations entities varied. similar. denote objective relations entities respectively. decompose terms correspond coverage diversity redundancy resp. corresponding non-negative weights. next propose functional forms terms. note function captures described properties used instead long objective remains submodular. deﬁne diversity measure relation ratio number entities appear source target number entities. entity deﬁne notion diversity manner shown below. note diversity measure intrinsic characteristic entity relationship dictated independent represent diversity measure relation entity respectively. denote sources targets appear relation tensor factorization yields embedding relation given facts participated therefore learned embeddings best options capturing similarities. denote learned embedding entity deﬁne theorem follows proving objective eqn. submodular. since addition preserves submodularity weights non-negative suﬃces show term submodular. proof appendix compare query proposal subset dataset setup assess quality guided completion method consider knowledge bases generics science tensor containing facts animals occupations activities locations etc. animals sub-tensor focuses facts animals. data include all) style triples objective function eqn. rather multi-class eqn. dalvi pipeline consisting open extractions aggregation clean crowd-sourcing generate domain-targeted facts elementary level science. facts come relevant wordnet based taxonomy entity types relation schema. table summarizes statistics datasets experimentation ensure every entity mentioned least times corresponding tensor. order limit error propagation two-level abstraction taxonomy. guided completion ﬁrst compare method existing completion techniques animals tensor demonstrate eﬀectiveness carries scalably larger science tensor well. examine alternatives generating negative samples given triple replace entity entity type resulting perturbed triple treated negative sample present also considered weighted consider extensions three state-of-the-art embedding-based completion methods hole transe rescal. mentioned earlier leading graph-based methods scale well. vanilla transe rescal resulted poor performance; report numbers extensions. speciﬁcally consider baselines hole transe+schema cntf extends rescal incorporates schema. figure shows resulting precision-yield curves predictions made method animals dataset containing facts. transe+itrs gave precision around omitted plot. make observations. first deriving facts generics tensors high precision challenging speciﬁcally none baseline methods producing triples precision further incorporating entity taxonomy address tensor sparsity results yield statistically signiﬁcantly higher precision next evaluate proposal entire science dataset facts. since graphbased methods scale well much smaller animals dataset methods performed substantially worse there focus scalability prediction quality method. found hole+itrs+iet scales well high dimension doubling number facts adding facts precision. although science tensor times larger animals tensor method took longer active learning entities assess quality active learning mechanism consider predicting facts entity animals tensor. illustration choose science vocabulary ensuring present wordnet taxonomy. setup follows. ﬁrst query generation mechanism propose ordered list facts annotate. next perform subset selection identify subset promising queries. annotated table assess quality ways many true facts complementary view focusing overall number facts inferred increases. highlight observations. first surprisingly randomly choosing triples annotate ineﬀective. second choosing schema consistent triples results true triples facts help tensor factorization little resulting additional triples proposed sibling guided querying mechanism results nearly facts true along true facts inferred sibling agreement also combined submodular subset selection balancing diversity coverage ultimately results facts including reindeer fruit wolf chase reindeer reindeer provide fur. plot figure demonstrates qualitative trends remain work explores completion class problems namely completing generics essential step including general world knowledge intelligent machines. diﬀerences generics much studied named entity make existing techniques either scale well produce facts undesirably precision box. demonstrate incorporating entity taxonomy relation schema appropriately highly eﬀective generics kbs. further address scarcity facts certain entities devise novel active learning approach using sibling guided uncertainty estimation along submodular subset selection. proposed techniques substantially outperform various baselines setting authors would like thank peter clark fruitful discussions valuable feedback crowdsourcing annotations; matt gardner constructive comments assessing graph-based completion methods datasets; udai saini partha talukdar evaluating cntf approach datasets.", "year": 2016}