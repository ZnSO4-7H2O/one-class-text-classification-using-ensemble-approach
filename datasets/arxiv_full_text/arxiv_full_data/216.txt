{"title": "Hyperparameter Transfer Learning through Surrogate Alignment for  Efficient Deep Neural Network Training", "tag": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "abstract": "Recently, several optimization methods have been successfully applied to the hyperparameter optimization of deep neural networks (DNNs). The methods work by modeling the joint distribution of hyperparameter values and corresponding error. Those methods become less practical when applied to modern DNNs whose training may take a few days and thus one cannot collect sufficient observations to accurately model the distribution. To address this challenging issue, we propose a method that learns to transfer optimal hyperparameter values for a small source dataset to hyperparameter values with comparable performance on a dataset of interest. As opposed to existing transfer learning methods, our proposed method does not use hand-designed features. Instead, it uses surrogates to model the hyperparameter-error distributions of the two datasets and trains a neural network to learn the transfer function. Extensive experiments on three CV benchmark datasets clearly demonstrate the efficiency of our method.", "text": "abstract. recently several optimization methods successfully applied hyperparameter optimization deep neural networks methods work modeling joint distribution hyperparameter values corresponding error. methods become less practical applied modern dnns whose training take days thus cannot collect sufﬁcient observations accurately model distribution. address challenging issue propose method learns transfer optimal hyperparameter values small source dataset hyperparameter values comparable performance dataset interest. opposed existing transfer learning methods proposed method hand-designed features. instead uses surrogates model hyperparameter-error distributions datasets trains neural network learn transfer function. extensive experiments three benchmark datasets clearly demonstrate efﬁciency method. george santayana deep neural networks shown powerful methods thus attracted great attention computer vision community. however adoption application somewhat hampered high complexity particular many choices hyperparameter values must take care applying dataset problem. finding appropriate values hyperparameters although essential achieving good performance usually time consuming difﬁcult. further complex state-of-the-art deep learning models take several days training making traditional hyperparameter tunning approaches grid-search impractical. aroused great interest developing efﬁcient systematic hyperparameter optimization approaches even methods need hundred hyperparameter evaluations near-optimal hyperparameter values thus remain impractical models requiring long training time. promising approach address challenging highly complex hyperparameter optimization problems transfer knowledge well-tuned hyperparameters evaluated source dataset hyperparameter optimization evaluation dataset. however optimal hyperparameter values different datasets vary greatly terms scale location. makes knowledge transfer hyperparameter optimization difﬁcult problem research works explored. feurer address issue extracting task dataset features metafeatures initialize hyperparameter optimization methods. bardenet describe approach based surrogate ranking techniques collaborative tunning constructs common performance surface model. similar fashion yogatama mann propose deviations per-dataset mean common representation model’s performance datasets. proposed methods hand-crafted features knowledge transfer hyperparameter optimizations different datasets. however deep learning models evidence hand-designed features typically inferior learned features. mind explore direction propose hyperparameter transfer learning method following unique features first utilizes surrogates efﬁciently model distribution validation second employs small neural network parameterize knowledge transfer function maps hyperparameter values source dataset similarly performing hyperparameters values target dataset. unique features enable proposed method efﬁciently optimize hyperparameters dataset interest using much times less evaluations demonstrated extensive experiments three image classiﬁcation datasets. name method hyperparameter transfer using surrogates short. previous approaches knowledge transfer hyperparameter optimization methods depend hand-crafted features capture dataset speciﬁc properties. following brieﬂy describe methods compare proposed method. yogatama mann describe bayesian optimization method maps validation errors hyperparameter values multiple datasets common space scaling errors per-dataset mean standard deviation. approach based assumption different datasets produce similar validation errors aside location scale error. argue relationship validation errors different datasets much complex cannot captured solely per-dataset deviations. contrast proposed method provides transfer function maps validation errors different datasets thus used hyperparameter optimization method. feurer dataset metafeatures compute dataset distance metric used transfer high-performing hyperparameter values close datasets. proposed method complex relies many metafeatures. metafeature carries assumption datasets properties hold methods employ hand-crafted features transfer knowledge gained hyperparameter optimization source dataset target dataset. method learns function able hyperparameter values source dataset hyperparameter values offering comparable performance target dataset. best knowledge ﬁrst method directly learn parameters transfer function maps hyperparameters across different datasets. problem setup consider deep learning algorithm conﬁgurable hyperparameters collecconstrained domain hyperparameter optimization aims best hyperparameter conﬁguration minimize validation error algorithm trained hyperparameter values. training evaluation using hyperparameter considered evaluation expensive function maps hyperparameter validation error. search minimizing function respect focus learning transfer hyperparameter sets perform well applied source dataset hyperparameter sets perform well algorithm applied target dataset method description deﬁne pairs hyperparameter conﬁgurations corresponding validation errors {)}n algorithm source dataset similarly deﬁne pairs validation error algorithm evaluated another dataset goal best hyperparameter sets obtain lowest validation errors well. however hyperparameter sets yielded lowest validation errors necessarily give lowest validation errors domain shift datasets. address problem propose hyperparameter transfer learning algorithm automatically adapts hyperparameter values dataset another. concretely ﬁrst deﬁne hyperparameter transfer function maps hyperparameter conﬁguration another validation error target dataset high positive correlation validation error source dataset. parameterize function small neural network learnable parameters estimated minimizing following objective function problem approach insufﬁciency training data optimizing neural network since small number elements populating pairs expensive. circumvent problem substituting equation surrogate models using ﬁtting populate evaluating latin hypercube samples another practical problem difﬁculty backpropagation correlation surrogate functions. this construct arrays samples sort array according value sample evaluated array evaluated consequence values sorted arrays high positive correlation substitute correlation function. objective function simpliﬁed function aims reduce mean squared distance hyperparameter values rank arrays. accordingly neural network training contains samples evaluated inputs samples ranked values desired outputs. algorithm hyperparameter transfer using surrogates inputs {set pairs hyperparameters evaluation source dataset {number latin hypercube samples initialize ωt}; {transfer step-size}; tmax {number evaluations allowed target dataset dt}; {function maps s.t. high positive correlation approximate evaluation hyperparameter target dataset surrogate model training containing samples evaluated inputs samples ranked desired outputs. learn function training neural network convergence neural network sample hyperparameter sets lowest validation errors {)}r rameter sets validation errors update surrogate model continue training neural network using updated surrogate model. repeat cycle exhaust available budget hyperparameter evaluations formal algorithm description given algorithm give implementation details discussions method variations analysis supplementary. fig. validation error curves optimizing hyperparameters network cifar- svhn. left compare optimization progress cifar- using source dataset using mnist source dataset using svhn source dataset ﬁnally using top-performing svhn hyperparameter values without using mapping function adapt cifar- i.e. linear mapping. right compare optimization progress svhn using source dataset using mnist source dataset mapping top-performing mnist hyperparameters without adaptation. designed experiments demonstrate efﬁciency method compared baseline method knowledge transfer. baseline choose recently proposed hord method since shown obtain stateof-the-art performance hyperparameter optimization tasks. compare baseline method since cannot compare method designed transfer hyperparameters subset dataset. also cannot compare work deals transfer general purpose datasets small size further made method’s code public time constraints leave reimplementation subsequent comparison method future work. experiments optimize hyperparameters convolutional neural network stochastic gradient descent algorithm train network optimize learning rate momentum. also optimize number nodes fully-connected layers dropout rate. apply network three computer vision benchmark datasets mnist svhn cifar-. give details hyperparameters ranges datasets details supplementary. ﬁrst experiment apply network cifar- dataset optimize hyperparameters. compare optimization progress cifar- using source dataset optimization progress using mnist source dataset using svhn source dataset. since mnist contains grayscale images digits transferring hyperparameter conﬁgurations task classifying color images everyday objects found cifar- requires complex mapping function. further simple networks typically achieve validation error around relatively easy task classifying mnist digits applied cifar- dataset achieve error around similarly svhn contains color images house numbers simple typically achieves error around means hyperparameter mapping function also learn deal different scale validation error. nonetheless successfully maps best found hyperparameters svhn cifar- reaches validation error using times less evaluations baseline method. progress slightly less using mnist source dataset since task much easier svhn cifar-. next demonstrate ability hyperparameter mapping function adapt source hyperparameters appropriate target hyperparameters. this perform optimization sort hyperparameters according validation error source dataset then order directly evaluate target dataset. svhn source dataset denote optimization svhn-linear. expected sort optimization starts error fails make progress since able properly adapt hyperparameters complex dataset cifar-. second experiment apply network svhn dataset optimize hyperparameters. compare optimization progress withknowledge transfer optimization progress knowledge transfer optimization mnist. similar experiment method achieves validation errors using much three times less evaluations baseline method. demonstrate effectiveness transfer function hyperparameter values achieve much lower validation error optimization directly source hyperparameter values. presented method transferring hyperparameter conﬁgurations datasets. proposed method efﬁciently learns top-performing hyperparameter conﬁgurations source dataset hyperparameter conﬁgurations comparable relative performance target dataset. resulting transfer function used transfer top-performing conﬁgurations also used initialize hyperparameter optimization method signiﬁcantly speed convergence. future plan evaluate method transferring hyperparameter conﬁgurations broader datasets.", "year": 2016}