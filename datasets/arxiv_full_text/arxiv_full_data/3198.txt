{"title": "Modeling sequential data using higher-order relational features and  predictive training", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Bi-linear feature learning models, like the gated autoencoder, were proposed as a way to model relationships between frames in a video. By minimizing reconstruction error of one frame, given the previous frame, these models learn \"mapping units\" that encode the transformations inherent in a sequence, and thereby learn to encode motion. In this work we extend bi-linear models by introducing \"higher-order mapping units\" that allow us to encode transformations between frames and transformations between transformations.  We show that this makes it possible to encode temporal structure that is more complex and longer-range than the structure captured within standard bi-linear models. We also show that a natural way to train the model is by replacing the commonly used reconstruction objective with a prediction objective which forces the model to correctly predict the evolution of the input multiple steps into the future. Learning can be achieved by back-propagating the multi-step prediction through time. We test the model on various temporal prediction tasks, and show that higher-order mappings and predictive training both yield a significant improvement over bi-linear models in terms of prediction accuracy.", "text": "bi-linear feature learning models like gated autoencoder proposed model relationships frames video. minimizing reconstruction error frame given previous frame models learn mapping units encode transformations inherent sequence thereby learn encode motion. work extend bi-linear models introducing higher order mapping units allow encode transformations frames transformations transformations. show makes possible encode temporal structure complex longer-range structure captured within standard bi-linear models. also show natural train model replacing commonly used reconstruction objective prediction objective forces model correctly predict evolution input multiple steps future. learning achieved back-propagating multi-step prediction time. test model various temporal prediction tasks show higher-order mappings predictive training yield signiﬁcant improvement bi-linear models terms prediction accuracy. image sequences. contrast existing work modeling relations propose training scheme call predictive training transformation extracted frames video model tries predict next frame assuming constancy transformations time. model learns relational features well higherorder relational features representing relations transformations themselves. bottomlayer bilinear model infers representation motion seed frames well representation motion later frames. layer bilinear model learns represent relation inferred lower-level transformations. thought learning second-order derivative temporal evolution high-dimensional input time series. show effective train models ﬁrst pre-train layers individually using pairs frames bottom layer pairs inferred transformations next layer subsequently ﬁne-tune parameters using complete sequences back-propagating multi-step lookahead cost time. model whole thought model dynamical system second order partial difference equation. principle model could stacked take account differences arbitrary order demonstrate two-layer model surprisingly effective modeling variety complicated image sequences. recently shown useful recurrent neural networks contrast work multiplicative interactions gate connections hidden states observation thought blending separate hidden state transition. natural application sequences discrete symbols model consequently demonstrated text. work role multiplicative interactions explicitly yield encodings transformations frames video apply model primarily video data. model also bears similarity model mocap data using generatively trained three-way restricted boltzmann machine second layer hidden units used model abstract features time series. contrast work higher-order units three-way units used expressly model higher-order transformations furthermore show predictive ﬁnetuning using backprop time allows train model discriminatively yields much better performance generative training itself. video given frame multitude potential next frames therefore common bi-linear models like gated boltzmann machine gated autoencoder similar models whose hidden variables represent transformation pool many shown weighted products ﬁlter responses able identify transformation. reason weighted large angle ﬁlters similar angle frames. hidden units represent observed transformation form phase-differences invariant subspaces transformation class hidden units encode transformation images rather content images commonly referred mapping units. shall focus autoencoder variant models purposes paper models gbm. training. shown minimizing reconstruction error image pairs turn corresponding pair phase-shifted ﬁlters. together ﬁlters span invariant subspaces transformation class inherent training pairs model trained. result component tuned phase-delta learning independent absolute phase image quite natural extension concept relational features motivated looking relational models performing kind ﬁrst-order taylor approximation input sequence hidden representation models partial ﬁrst-order derivatives inputs respect time. based view propose approach exploits correlations subsequent sequence elements model dynamical system approximates sequence. different address longrange correlations assuming memory units explicitly keep state instead assume structure temporal evolution input stream focus capturing structure. intuitive example consider video known sinusoidal signal unkown frequency phase motion direction. complete video speciﬁed exactly completely ﬁrst three seed images. therefore given three images would principle able predict rest video inﬁnitum. ﬁrst-order partial derivative multidimensional discrete-time dynamical system describes correspondences state vectors subsequent time steps. relational feature learning applied subsequent elements sequence viewed learn derivatives suggesting model higher-order partial derivatives higher-order relational features. model second-order derivatives cascading relational features pyramid depicted given sequence inputs ﬁrst-order relational features describe transformations subsequent inputs second-order relational features describe correspondences ﬁrstorder relational features modeling analog partial second-order derivatives inputs respect time. section presents experiments layers relational features support view. figure first-order relational features describe correspondences multiple entities e.g. frames video. second-order relational features describe correspondences ﬁrst-order relational features. implement higher-order gated autoencoder using following modular approach. second-order hgae constructed using modules relates inputs another relates mappings ﬁrst gae. ﬁrst-layer instance models correspondences between input pairs using ﬁlter matrices using ﬁrst-layer mappings overlapping input pairs inferred pair ﬁrst-layer mappings used input second instance. second models correspondences mappings ﬁrst-layer using ﬁlter matrices perspective orthogonal transformations subspace rotations. stated summing ﬁlter-response products yield transformation detectors invariant initial phase transformation also partially invariant content images. relative rotation angle projections angle relation viewed angular acceleration. contrast standard two-frame model model reconstruction error directly applicable however natural train model training data forms sequence discuss next. given ﬁrst frames sequence compute prediction third frame follows. first mappings inferred used compute prediction applying inferred transformation frame applying transformation amounts computing prediction good prediction assumption frame-to-frame transformations approximately same words transformations assumed approximately constant time. instead minimizing reconstruction error equation type supervised training objective contrast standard objective also guide mapping representation invariant image content encoding content general help predicting assumption constancy transformations violated higher layer model transformations change time. require farther look-ahead predictive training discuss following. two-layer hgae amounts assumption second-order relational structure sequence changes slowly time assumption compute prediction steps first prediction made ﬁrst-order relational features describing correspondence prediction process simply consists iteratively computing predictions next lower level’s activations beginning top. compute top-level activations themselves needs number seed frames corresponding depth model. frames sufﬁcient infer transformations case three frames required case two-layer model. evaluate whether predictive training yields better representations transformations training reconstruction objective classiﬁcation experiment videos showing artiﬁcially transformed natural images performed. patches cropped berkeley segmentation data data sets videos featuring constant velocity shifts rotations generated. elements shift vectors constshift data sampled uniformly interval rotation angles sampled uniformly interval labels constshift data generated dividing shift vectors shown figure constrot angles divided equally-sized bins. data sets partitioned training containing validation containing test containing sequences. numbers ﬁlters mapping units chosen using grid search. setting best performance validation ﬁlters mapping units training objectives data sets. models trained epochs using stochastic gradient descent learning rate momentum experiment mappings ﬁrst inputs used input logistic regression classiﬁer. experiment performed multiple times data sets mean classiﬁcation accuracies reported table trials trained step predictive training achieved higher accuracy trained reconstruction objective. suggests predictive training able generate explicit representation transformations plagued less image content discussed section figure prediction made steps top-to-bottom. second-order relational feature inferred sequence assumed slowly changing used make prediction ﬁrst-order relational feature describes correspondences prediction used transform prediction experiments observed starting singlestep prediction training iteratively increasing number prediction steps training considerably stabilizes dynamics model helps prevent explosions magnitude predictions. tested compared models videos varying degrees complexity synthetic constant synthetic accelerated transformations complex realworld transformations. predictive training hgae worked layerwise pre-training. used gradient descent learning rate momentum without pretraining parameters converge useful conﬁguration. ﬁrst-layer trained reconstruct pairs subsequent sequence elements pairs mappings computed three subsequent inputs using pretrained ﬁrst-layer gae. mapping number ﬁlters mapping units respectively after pretraining hgae trained gradient descent using learning rate momentum ﬁrst epochs single-step prediction epochs two-step prediction. figure discretization shift vectors plane divided four quadrants magnitude threshold chosen distribution samples shown bins uniform. denotes maximum magnitude respective data set. training ﬁrstsecond-layer mappings inferred ﬁrst three frames test sequences. classiﬁcation accuracies using logistic regression second-layer mappings hgae descriptor using individual ﬁrst-layer mappings using concatenation ﬁrst-layer mappings reported table data sets accelerated shifts rotations natural image patches generated. patches cropped berkeley segmentation data artiﬁcially transformed initial velocity constant acceleration. scalar angular accelerations sampled uniformly interval degrees. initial angular velocites sampled interval. labels classiﬁcation angular accelerations divided equally sized bins. accelerated shifts data elements velocity acceleration vectors sampled interval second-layer mappings achieved signiﬁcantly higher accuracy data sets predictive training. accrot data concatenation ﬁrst-layer mappings performed better second-layer mappings before ﬁnetuning angular acceleration data based one-parameter transformation thus simpler shift acceleration data based two-parameter transformation. predictive ﬁnetuning also helped improve intermediate representation observed increase accuracy concatenation ﬁrst-layer mappings. table classiﬁcation accuracies using different layer mappings hgae -step ﬁnetuning accelerated rotations accelerated shifts data set. denotes concatenation ﬁrst-layer mappings. figure examples seven prediction steps hgae model accrot data shown bottom groundtruth predictions model pretraining one- twothree-step predictive training. figure examples predictions using hgae model accshift data shown bottom groundtruth predictions model pretraining onetwo-step predictive training. results shows second layer hgae build much better representation second-order relational structure data single-layer model. show predictive training improves capability models crucial twolayer model work well. experiment test capability models predict previously unseen sequences multiple steps future. allows assess degree modeling second order derivatives makes possible capture temporal evolution without resorting explicit representation hidden state. training test sequences generated seeding models three seed frames. figure shows ﬁlter pairs learned hgae different data sets predictive training. figures show predictions hgae model data sets introduced section different stages training. seen ﬁgures accuracy predictions increases signiﬁcantly multi-step training. norbvideos data introduced contains videos objects norb dataset objects divided classes instances. frames video norbvideos data show incrementally changed viewpoints objects. trained sequence learning models data using author’s original split videos objects instances training instance objects test set. yields training examples test examples. frame size videos frames long. hgae model trained multi-step prediction task learning rate momentum models used features mapping units testperformance model seemed stop improving features hgae able make additional parameters. figure shows predictions made models. hgae manages generate predictions correctly reﬂect structure data. contrast model much better extrapolating observed transformations. note seed frames test data. large input dimensionality number training samples ﬁlters shown figure seem overﬁtting training data many others localized gabor-like features. also trained hgae bouncing balls data whether hgae captures highly non-linear dynamics data set. number features number mappings figure shows predictions test data. predictions show second-order model able correctly capture reﬂections boundaries balls makes consistent predictions cases around frames. major long-standing problem sequence modeling deal long range correlations. proposed deep learning help address problem ﬁnding representations capture better abstract semantic content inputs work propose learning representations explicit goal enable prediction temporal evolution input stream multiple time steps ahead. thus seek hidden representation captures exactly aspects input data allow make predictions future. memisevic roland hinton geoffrey. unsupervised proceedings learning image transformations. ieee conference computer vision pattern recognition sutskever ilya martens james hinton geoffrey. generating text recurrent neural networks. proceedings international conference machine learning taylor graham hinton geoffrey. factored conditional restricted boltzmann machines modeling moproceedings international tion style. conference machine learning montreal june omnipress. applying observation time later. difference genuine analogy making task target image unrelated source image pair whereas target source related. would interesting apply model word representations language general domain both sequentially structured data analogical relationships data-points play crucial role lecun yann huang bottou leon. learning methods generic object recognition invariance computer vision pattern pose lighting. recognition cvpr proceedings ieee computer society conference volume ii–. ieee martin fowlkes malik database human segmented natural images application evaluating segmentation algorithms measuring ecological statistics. proc. int’l conf. computer vision volume july", "year": 2014}