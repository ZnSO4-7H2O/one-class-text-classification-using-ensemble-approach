{"title": "Learning how to Active Learn: A Deep Reinforcement Learning Approach", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Active learning aims to select a small subset of data for annotation such that a classifier learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic. Importantly, our method allows the selection policy learned using simulation on one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning.", "text": "active learning aims select small subset data annotation classiﬁer learned data highly accurate. usually done using heuristic selection methods however effectiveness methods limited moreover performance heuristics varies datasets. address shortcomings introduce novel formulation reframing active learning reinforcement learning problem explicitly learning data selection policy policy takes role active learning heuristic. importantly method allows selection policy learned using simulation language transferred languages. demonstrate method using cross-lingual named entity recognition observing uniform improvements traditional active learning. natural language processing tasks obtaining sufﬁcient annotated text training accurate models critical bottleneck. thus active learning applied tasks minimise expense annotating data active learning aims reduce cost identifying subset unlabelled data annotation selected maximise accuracy supervised model trained data many successful applications e.g. tomanek used active learning algorithm conll corpus score reduction annotation cost prior work active learning algorithms designed english based doubt active learning extremely important languages particularly lowresource languages annotation typically difﬁcult obtain annotation budgets modest settings natural application active learning however little work end. potential reason active learning algorithms require substantial ‘seed set’ data learning basic classiﬁer used active data selection. however given dearth data low-resource setting assumption make standard approaches infeasible. paper propose short policy based active learning novel approach learning dynamic active learning strategy data. allows strategy applied data settings cross-lingual applications. algorithm ﬁxed heuristic instead learns actively select data formalised reinforcement learning problem. intelligent agent must decide whether select data annotation streaming setting decision policy learned using deep q-network policy informed observations including sentences’ content information supervised model’s classiﬁcations conﬁdence. accordingly rich dynamic policy learned annotating data based past sequence annotation decisions. learning another language transfer target language. easy learn policy high resource language plentiful data english. cross-lingual word embeddings learn compatible data representations languages learned policy easily ported language. work different prior work active learning nlp. previous active learning algorithms developed tasks based language applied language itself. another main difference many active learning algorithms ﬁxed data selection heuristic uncertainty sampling however algorithm implicitly uncertainty information kind observations agent. remainder paper organised follows. section brieﬂy review related work. section present active learning algorithms cross multiple languages. experimental results presented section conclude work section supervised learning methods often require training data active learning technique selects subset data annotate training best classiﬁer. existing active learning algorithms generally considered three categories uncertainty sampling selects data current classiﬁer uncertain; query committee selects data committee disagree most; expected error reduction selects data contribute largest model loss reduction current classiﬁer labelled. applications active learning include text classiﬁcation relation classiﬁcation structured prediction qian used uncertainty sampling jointly perform english chinese. stratos collins zhang deployed uncertainty-based algorithms languages minimal supervision. general-purpose framework decision making based representation learning. recently notable examples include deep qlearning deep visuomotor policies attention recurrent networks model predictive control embeddings important works include massively parallel frameworks dueling architecture expert move prediction game produced policies matching monte carlo tree search programs squarely beaten professional player combined search also studied tasks. example recently studied information extraction problem designed framework decide acquire external evidence framework reinforcement learning method. however fairly little work using learn active learning strategies language processing tasks especially cross-lingual settings. recent deep learning work also looked transfer learning recent work deep learning also considered transferring policies reusing policy parameters between environments using either regularization novel neural network architectures though work looked transfer active learning strategies between languages shared feature space state. show active learning formalised decision process show allows active learning selection policy learned data using deep reinforcement learning. later introduce method transferring policy languages. active learning simple technique labelling data involves ﬁrst selecting instances unlabelled dataset annotated human oracle repeated many times termination criterion satisﬁed e.g. annotation budget exhausted. often selection function based predictions trained model labelled dataset stage algorithm datapoints selected based model’s predictive uncertainty divergence predictions ensemble idea methods instances model likely make errors labelling inclusion training model becomes robust types errors unseen data. steps active learning viewed decision process means formalising active learning algorithm sequence decisions stages active learning correspond state system. accordingly state corresponds selected data labelling labels step active learning algorithm corresponds selection action wherein heuristic selects next items pool. process terminates budget exhausted. effectively active learning heuristic operating decision policy form function taking input current state comprising labelled data model trained candidate unlabelled data point e.g. model uncertainty. raises opportunity consider general policy functions based state data point inputs resulting labelling decision accordingly mechanism learning functions data. elaborate components process namely formulation decision process architecture policy function means learning decision policy automatically data. stream-based learning simplicity make streaming assumption whereby unlabelled data arrive stream instance arrives agent must decide action take namely whether instance manually annotated. process illustrated figure illustrates space decision sequences small corpus. part process separate model trained labelled data updated accordingly labelled dataset expanded annotations arfigure example illustrating sequential active learning markov decision process. data arrives sequentially time active learning policy must decide whether labelled based state includes predictive model parameterised unlabelled data instance process continues termination e.g. annotation budget exhausted. solid green path shows maximum scoring decision sequence. form markov decision process allows learning policy dynamically select instances informative. illustrated figure time agent observes current state includes sentence learned model agent selects binary action denoting whether label according policy corresponding sentence labelled added labelled data model updated include training point. process repeats terminating either dataset exhausted ﬁxed annotation budget reached. termination reward computed based accuracy ﬁnal model represent framework tuple space possible states actions reward function transition function. state state time comprises candidate instance considered annotation labelled dataset constructed steps represent state using continuous vector using concatenation vector representation outputs model trained labelled data. outputs predictive marginal distributions model instance representation model’s conﬁdence. elaborate component. content representation input agent content sentence encode using convolutional neural network arrive ﬁxed sized vector representation following involves embedding words sentence produce matrix xi··· xin} series wide convolutional ﬁlters applied using multiple ﬁlters different gram sizes. ﬁlter uses linear transformation rectiﬁed linear unit activation function. finally ﬁlter outputs merged using max-pooling operation yield hidden state used represent sentence. representation marginals prediction outputs training model central active learning heuristics accordingly include approach. order generalise existing techniques elect predictive marginals directly rather using statistics thereof e.g. entropy. generality allows different nuanced concepts learned including patterns probabilities span several adjacent positions sentence another convolutional neural network process predictive marginals shown figure convolutional layer contains ﬁlters relu activation based window width height equal number classes stride token. wide convolution padding input matrix either size vectors zeros. feature maps subsampled mean pooling network easily able capture average unconﬁdence sequential prediction last component score indicates conﬁdence model prediction. deﬁned based probable label sequence model e.g. using viterbi algorithm probability sequence used represent conﬁdence action turn action denotes whether human oracle must annotate current sentence. agent selects either annotate case agent proceeds consider next instance xi+. action chosen oracle requested annotate sentence newly annotated sentence added training data updated accordingly. special ‘terminate’ option applies data remains annotation budget exhausted concludes active learning reward training signal learning policy takes form scalar ‘reward’ provides feedback quality actions made agent. obvious reward wait game conclude measure held-out performance model trained labelled data. however reward delayed difﬁcult related individual actions long game. compensate this reward shaping whereby small intermediate rewards assigned speeds learning process step intermediate reward deﬁned change held-out performance i.e. denotes predictive accuracy trained model action take place include additional training instance. accordingly considering aggregate reward game intermediate terms cancel total reward measures performance improvement whole game. note value positive negative indicating beneﬁcial detrimental effect performance. budget ﬁxed budget total number instances annotated corresponds terminal state mdp. predeﬁned number chosen according time cost constraints. game ﬁnished data exhausted budget reached ﬁnal result being dataset thus created upon ﬁnal model trained. reinforcement learning remaining question components used learn good policy. different policies make different data selections thus result models different performance. adopt reinforcement learning approach learn policy resulting highly accurate model. represented problem episode sequence transitions episode active learning produces ﬁnite sequence states actions rewards. deep q-learning approach formalises policy using function determines utility taking state according policy qlearning agent iteratively updates using rewards obtained episode updates based recursive bellman equation optimal following deep q-learning make deep neural network compute expected q-value order update parameters. implement q-function using single hidden layer neural network taking input state representation outputting scalar values corresponding values network uses rectiﬁed linear unit activation function hidden layer. parameters learnt using stochastic gradient descent based regression objective match q-values predicted expected q-values bellman equation maxa following experience replay memory store transition used episode sample mini-batch transitions memory minimize loss function esars maxa θi−) target q-value based current parameters expectation minibatch. learning updates made every training step based stochastic gradient descent minimise w.r.t. parameters algorithm learning summarised algorithm train policy running multiple active learning episodes training data episode simulated active learning run. episode shufﬂe data hide known labels revealed requested run. disjoint held-out used compute reward i.e. model accuracy ﬁxed episodes. episode model reset initialisation condition main changes different data ordering evolving policy function. extensive training dataset policy application makes sense employed different data setting e.g. domain task language different. paper consider cross-lingual application task train policy source language transfer learned policy different target language. cross-lingual word embeddings provide common shared representation facilitate application policy languages. illustrate policy transfer algorithm algorithm algorithm broadly similar algorithm differences. firstly algorithm makes pass data rather several passes beﬁts application low-resource language oracle labelling costly. secondly algorithm also assumes initial policy tuned episode based held-out performance policy adapt test scenario. moreover algorithm extended traditional batch setting evaluating batch data instances selectinag best instances labelling policy. could applied either transfer step initial policy training both. side learning loop. former implies supervision ideal low-resource setting latter places limitations communication annotator well necessity real-time processing unlikely ﬁeld linguistics setting. data andcommunication-impoverished setting denoted cold-start allow chance request labels target data held-out data allow policy updates. agent needs select batch unlabelled target instances annotations cannot resulting annotations feedback reﬁne selection. this difﬁcult cold-start setting bootstrap process initial model agent make informative decisions absence feedback. procedure outlined algorithm using cross-lingual word embeddings transfer policy model target language. model trained source language policy learned different source language. policy learning uses small change step model initialised using consequently learned policy exploit knowledge cross-lingual initialisation ﬁgure aspects need corrected using target annotated data. overall allows estimates conﬁdence values produced model thus providing agent sufﬁcient information data selection. conduct experiments validate proposed active learning method cross-lingual setting whereby active learning policy trained source language transferred target language. allow repeated active learning simulations source language annotated corpora plentiful learn policy target languages permit single episode mimic language without existing resources. conﬁguration outline parameter settings experimental runs. learning active learning policy episodes budget sentences using alg. content representations three convolutional ﬁlters size using ﬁlters size predictive marginals convolutional ﬁlters width using ﬁlters. size last hidden layer discount factor used adam algorithm mini-batches size training neural network. report performance apply learned policy target training ﬁnal trained model report score. figure performance active learning methods bilingual multilingual settings three target languages whereby active learning policy trained languages excluding target respectively. word embeddings shelf trained multilingual embeddings using dimensional embedding ﬁxing training policy model. model standard linear chain ﬁrst sets experiments cold-start case basic classiﬁer multilingual embeddings before dimensional hidden layer. proposed method referred shorthand policy based active learning. subscripts used denote bilingual multilingual cold-start experimental conﬁgurations. comparative baselines following methods uncertainty sampling total token entropy measure takes instance maximising token entropy. whole training data pool select single instance labelling active learning step. method shown achieve best result among model-independent active learning methods conll data. three target languages. uncertainty sampling ineffective particularly towards start consequence dependence high quality model. content information allows palb make stronger start despite poor initial model. also shown figure results multilingual policy learning palm outperform approaches including palb. illustrates additional training several languages gives rise better policy using source language. superior performance particularly marked early stages runs spanish dutch indicate approach better able learn exploit sentence content information. evaluate cold-start setting figure recall setting policy model updates heldout data used annotations arrive batch. model however initialised tagger trained different language explains performance methods starts around rather even challenging evaluation setting algorithm palc outperforms baseline methods showing deep learning allows better exploitation pretrained classiﬁer alongside sentence content. lastly report results approaches table based training full labelled sentences selected different methods. clear methods outperform baselines among multilingual training palm outperforms bilingual setting palb. surprisingly palc gives overtable results active learning using different methods approach constructs training sentences. three target languages shown columns reporting score relative cost reduction match stated performance random strategy. best results despite using static policy model target application underscoring importance model pretraining. table also reports cost reduction versus random sampling showing methods reduce annotation burden paper proposed active learning algorithm capable learning active learning strategies data. formalise active learning markov decision framework whereby active learning corresponds sequence binary annotation decisions applied stream data. based this design active learning algorithm policy based deep reinforcement learning. show learned active learning policies transferred languages empirically show provides consistent sizeable improvements baseline methods including traditional uncertainty sampling. holds true even difﬁcult cold-start setting evaluation data available ability react annotations. work sponsored defense advanced research projects agency information innovation ofﬁce resource languages emergent incidents program issued darpa/io contract hr--c-. views expressed authors reﬂect ofﬁcial policy position department defense u.s. government. trevor cohn supported australian research council future fellowship. references waleed ammar george mulcaire yulia tsvetkov guillaume lample chris dyer noah smith. massively multilingual word embeddings. arxiv preprint arxiv. jimmy volodymyr mnih koray kavukcuoglu. multiple object recognition visual attention. proceedings international conference learning representations meng fang trevor cohn. model transfer tagging low-resource languages using bilingual dictionary. proceedings annual meeting association computational linguistics garrette jason baldridge. learning part-of-speech tagger hours annotation. proceedings conference north american chapter association computational linguistics human language technologies pages john lafferty andrew mccallum fernando pereira. conditional random ﬁelds probabilistic models segmenting labeling sequence data. proceedings eighteenth international conference machine learning pages david lewis william gale. sequenprotial algorithm training text classiﬁers. ceedings international sigir conference research development information retrieval. pages chris maddison huang ilya sutskever david silver. move evaluation using deep convolutional neural networks. proceedings international conference learning representations andrew kachites mccallumzy kamal nigamy. employing pool-based active learnproceedings text classiﬁcation. international conference machine learning pages volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature arun nair praveen srinivasan blackwell cagdas alcicek rory fearon alessandro maria vedavyas panneershelvam mustafa suleyman charles beattie stig petersen shane legg volodymyr mnih koray kavukcuoglu david silver. massively parallel methods deep proceedings icml reinforcement learning. workshop deep learning. karthik narasimhan adam yala regina barzilay. improving information extraction acquiring external evidence reinforcement learning. proceedings conference empirical methods natural language processing emilio parisotto jimmy ruslan salakhutdinov. actor-mimic deep multitask proceedings transfer reinforcement learning. international conference learning representations longhua qian haotian yanan guodong zhou qiaoming zhu. bilingual active learning relation classiﬁcation pseudo parproceedings annual allel corpora. meeting association computational linguistics pages nicholas andrew mccallum. toward optimal active learning monte carlo estiproceedings mation error reduction. international conference machine learning pages andrei rusu neil rabinowitz guillaume desjardins hubert soyer james kirkpatrick koray kavukcuoglu razvan pascanu raia hadsell. progressive neural networks. arxiv preprint arxiv. burr settles mark craven. analysis active learning strategies sequence labeling proceedings conference emtasks. pirical methods natural language processing pages shen zhang jian guodong zhou chew-lim tan. multi-criteria-based active learning named entity recognition. proceedings annual meeting association computational linguistics david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature cynthia thompson mary elaine califf raymond mooney. active learning natural language parsing information extraction. proceedings international conference machine learning pages katrin tomanek joachim wermter hahn. approach text corpus construction cuts annotation costs maintains reusabilproceedings annotated data. joint conference empirical methods natural language processing computational natural language learning pages ziyu wang schaul matteo hessel hado hasselt marc lanctot nando freitas. dueling network architectures deep reinforcement learning. proceedings international conference machine learning manuel watter jost springenberg joschka boedecker martin riedmiller. embed control locally linear latent dynamics model control images. advances neural information processing systems pages boliang zhang xiaoman tianlu wang ashish vaswani heng kevin knight daniel marcu. name tagging low-resource incident languages based expectation-driven learning. proceedings conference north american chapter association computational language technologies pages", "year": 2017}