{"title": "Extrinsic Methods for Coding and Dictionary Learning on Grassmann  Manifolds", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Sparsity-based representations have recently led to notable results in various visual recognition tasks. In a separate line of research, Riemannian manifolds have been shown useful for dealing with features and models that do not lie in Euclidean spaces. With the aim of building a bridge between the two realms, we address the problem of sparse coding and dictionary learning over the space of linear subspaces, which form Riemannian structures known as Grassmann manifolds. To this end, we propose to embed Grassmann manifolds into the space of symmetric matrices by an isometric mapping. This in turn enables us to extend two sparse coding schemes to Grassmann manifolds. Furthermore, we propose closed-form solutions for learning a Grassmann dictionary, atom by atom. Lastly, to handle non-linearity in data, we extend the proposed Grassmann sparse coding and dictionary learning algorithms through embedding into Hilbert spaces.  Experiments on several classification tasks (gender recognition, gesture classification, scene analysis, face recognition, action recognition and dynamic texture classification) show that the proposed approaches achieve considerable improvements in discrimination accuracy, in comparison to state-of-the-art methods such as kernelized Affine Hull Method and graph-embedding Grassmann discriminant analysis.", "text": "abstract sparsity-based representations recently notable results various visual recognition tasks. separate line research riemannian manifolds shown useful dealing features models euclidean spaces. building bridge realms address problem sparse coding dictionary learning grassmann manifolds i.e. space linear subspaces. propose embed grassmann manifolds space symmetric matrices isometric mapping. turn enables extend sparse coding schemes grassmann manifolds. furthermore propose algorithm learning grassmann dictionary atom atom. lastly handle non-linearity data extend proposed grassmann sparse coding dictionary learning algorithms embedding higher dimensional hilbert spaces. experiments several classiﬁcation tasks show proposed approaches achieve considerable improvements discrimination accuracy comparison state-of-the-art methods kernelized afﬁne hull method graph-embedding grassmann discriminant analysis. keywords riemannian geometry grassmann manifolds sparse coding dictionary learning harandi hartley college engineering computer science australian national university nicta australia e-mail mehrtash.harandinicta.com.au e-mail richard.hartleynicta.com.au past decade sparsity become popular term neuroscience information theory signal processing related areas sparse representation compressive sensing possible represent natural signals like images using non-zero coefﬁcients suitable basis. computer vision sparse overcomplete image representations ﬁrst introduced modeling spatial receptive ﬁelds simple cells human visual system linear decomposition signal using atoms dictionary shown deliver notable results various visual inference tasks face recognition image classiﬁcation subspace clustering image restoration motion segmentation name few. signiﬁcant steps taken develop theory sparse coding dictionary learning euclidean spaces similar problems non-euclidean geometry received comparatively little attention paper introduces techniques sparsely represent p-dimensional linear subspaces using combination linear subspaces. linear subspaces considered core many inference algorithms computer vision machine learning. example reﬂectance functions produced lambertian objects lies linear subspace several state-of-the-art methods matching videos image sets model given data subspaces auto regressive moving average models typically employed model dynamics spatio-temporal processing also expressed linear subspaces applications linear subspaces computer vision include chromatic noise ﬁltering subspace clustering motion segmentation domain adaptation object tracking despite wide applications appealing properties subspaces special type riemannian manifold namely grassmann manifold makes analysis challenging. paper tackles provides efﬁcient solutions following fundamental problems grassmann manifolds coding. given subspace {di}n spaces {di}n main motivation develop methods analyzing video data image sets. inspired success sparse signal modeling related topics suggest natural signals like images efﬁciently approximated superposition atoms dictionary. generalize traditional notion coding operates vectors coding subspaces. coding dictionary subspaces seamlessly used categorizing video data. considering problem coding dictionary learning grassmann manifolds previous studies intrinsic general framework sparse coding riemannian manifolds. intrinsic formulation exploits tangent bundle manifold sparse coding. computational complexity logarithm grassmann manifolds performing intrinsic sparse coding might computationally demanding problems interested moreover learning dictionary based intrinsic formulation proposed requires computing gradient cost function includes terms based logarithm map. shown later involvement logarithm sparse coding grassmann manifold. given dictionary query signal grassmann manifold interested estimating query signal sparse combination atoms taking account geometry manifold dictionary learning grassmann manifold. given observations grassmann manifold interested determining dictionary describe observations sparsely taking account geometry. ﬁgure best seen color. contributions. light discussion paper introduce extrinsic methods coding dictionary learning grassmann manifolds. propose embed grassmann manifolds space symmetric matrices diffeomorphism preserves several properties grassmannian structure. show coding accomplished induced space devise algorithm updating grassmann dictionary atom atom. furthermore order accommodate non-linearity data propose kernelized versions coding dictionary learning algorithms. contributions therefore three-fold apply proposed grassmann dictionary learning methods several computer vision tasks data videos image sets. proposed algorithms outperform state-of-the-art methods wide range classiﬁcation tasks including gender recognition gait scene analysis face recognition image sets action recognition dynamic texture classiﬁcation. section overviews grassmann geometry provides groundwork techniques described following sections. since term manifold often used computer vision somewhat loose sense emphasize word used paper strict mathematical sense. throughout paper bold capital letters denote matrices bold lower-case letters denote column vectors notations used demonstrate elements position vector matrix respectively. denote norms respectively indicating transposition. grouping together points span subspace obtain grassmann manifold formally stiefel manifold admits right action orthogonal group matrix also element furthermore columns span subspace thought representatives element grassmann manifold thus orbits group action form elements grassman manifold. resulting orbits manifold according quotient manifold theorem details construction critical understanding rest paper. linear combinations. vectors ordered columns matrix said span write span. follows refer subspace hence point basis matrix choice basis unique effect develop later. riemannian metric manifold deﬁned formally smooth inner product tangent bundle. form riemannian metric however shall concerned geodesic distances grassmann manifold allows avoid many technical points give straight-forward deﬁnition. riemannian manifold points connected smooth curves. geodesic distance points deﬁned length shortest curve manifold connecting them. stiefel manifold embedded matrices seen euclidean space rd×p distances deﬁned frobenius norm. consequently length smooth curve deﬁned length curve rd×p. given points distance dgeod deﬁned length shortest path points members equivalence clases operators namely logarithm logx inverse exponential expx deﬁned riemannian manifolds switch manifold tangent space point fact logarithm inverse closed-form solutions grassmann manifolds. efﬁcient numerical approaches computing maps proposed paper however exponential logarithm maps used describing previous work authors. vector spaces coding mean general notion representing vector combination vectors belonging dictionary. typically else afﬁne combination reconstruct query combination dictionary atoms forcing coefﬁcients combination i.e. structure. quantity thought coding cost combining squared residual coding error reﬂected energy term along penalty term encourages structure sparsity. function could norm lasso problem form locality proposed problem dictionary learning determine given ﬁnite observations minimizing total coding cost observations namely good dictionary small residual coding error observations producing codes desired structure. example case sparse coding norm usually taken obtain common form dictionary learning literature. speciﬁcally sparse dictionary learning problem written full jointly minimizing total coding cost choices coefﬁcients dictionary common approach solving alternate sets variables {yi}m proposed example detailed treatment). minimizing sparse codes dictionary ﬁxed convex problem. similarly minimizing overall problem ﬁxed {yi}m generalizing coding problem general space distance metric encoding function assigning element every choice coefﬁcients dictionary. note special case this represents linear afﬁne combination euclidean distance metric. deﬁne coding need specify metric used encoding function although formulation apply wide range spaces shall concerned chieﬂy coding grassmann manifolds. seemingly straightforward method coding dictionary learning embedding manifolds euclidean spaces ﬁxed tangent space. embedding function case would logp default base point. natural choice base point notation reminds norm tangent space shall refer straightforward approach log-euclidean sparse coding following terminology used idea deployed action recognition manifold symmetric positive deﬁnite matrices since tangent space distances base point equal true geodesic distances log-euclidean solution take account true structure underlying riemannian manifold. moreover solution dependent upon particular point used base point. elegant intrinsic approach work tangent bundle manifold varying particular tangent space according point approximated. idea roots work extends various methods dimensionality reduction riemannian manifolds. sparse coding show working tangent space i.e. encoding cost written extra afﬁne constraint i.e. necessary avoid trivial solution used successfully applications dimensionality reduction subspace clustering coding name few. similar euclidean case problem solved iterative optimization {yi}m done solving update proposed gradient descent approach along geodesics. update time {yi}m difﬁculty arises. since logarithm closed-form expression grassmann manifolds analytic expression cannot sought case interest work i.e. grassmann manifolds. mind propose extrinsic approaches coding dictionary learning specialized grassmann manifolds. proposal different intrinsic method following points compared intrinsic approach extrinsic coding methods noticeably faster. especially attractive vision applications dimensionality grassmann manifolds high. similar intrinsic method proposed dictionary learning approach alternating method. however contrast intrinsic method updating rule dictionary atoms admits analytic form. proposed extrinsic methods kernelized. kernelization intrinsic method possible fact logarithm closedform analytic expression grassmann manifolds. kernelized coding enables model non-linearity data better shown experiments kernelized coding result higher recognition accuracies compared linear coding. abstract riemannian manifold gradient smooth real function point denoted gradf element satisfying gradf here denotes directional derivative direction interested reader referred details gradient function grassmann manifolds computed. work propose embed grassmann manifolds space symmetric matrices projection embedding projection embedding previously used subspace tracking clustering discriminant analysis classiﬁcation purposes idempotent symmetric matrices rank projection embedding given span. mapping diffeomorphism thought simply alternative form grassmann manifold. smooth compact submanifold dimension embedding manifold inherits riemannian metric frobenius norm sym. important fact mapping also isometry respect riemannian metric standard riemannian metric deﬁned section hence preserves length curves shortest path length points deﬁnes distance metric called geodesic metric. future shall denote representing action projection embedding. furthermore represents frobenius inner product thus trxy note computing necessary compute andy explicitly instead note trxy metric used context recast coding consequently dictionarylearning problem terms chordal distance. presenting proposed methods establish interesting link coding notion weighted mean metric space. weighted karcher mean. underlying concept coding using dictionary represent point space interest combination elements space. usual method coding given represented linear combination dictionary elements ﬁrst term represents coding error. coding manifold problem address linear combinations make sense. wish element represented terms dictionary elements suggested proposed method generalize case prefer method direct generalization euclidean case way. words afﬁne combination dictionary elements equal weighted mean. although linear combinations deﬁned points manifolds metric spaces weighted mean deﬁnition given points riemannian manifold weights point called weighted karcher mean points generally ﬁnding karcher mean manifold involves iterative procedure converge local minimum even simple manifold however replace geodesic metric different metric order simplify calculation. propose chordal metric grassman manifold deﬁned corresponding mean deﬁnition called weighted chordal mean points. contrast karcher mean weighted chordal mean grassman manifold simple closed-form. therefore problem convex efﬁciently solved using common packages like spams problem transposed vectorized sparse coding problem. speciﬁcally equivalent fig. conceptual diagram extrinsic sparse coding addressed work. hemisphere used represent point surface hemisphere intended grassmannian point represented symmetric idempotent rank matrix. grassmann dictionary four atoms sparsely describing query point shown blue circle using dictionary atoms. here combination atoms could step overcomplete dictionary possible arbitrarily close manifold. spirit similar sparse coding vector spaces. speciﬁcally unit norm vector dictionary {di} unit norm atoms result sparse coding might outside unit norm sphere unlike conventional sparse coding vector spaces −x∀x results antipodals points equivalent. special case solution proposed understood sparse coding higher dimensional quadratic space i.e. note quadratic space several studies favor locality coding process locality could lead sparsity necessarily vice versa follows describe coding scheme based neighborhood information. show codes local constraints obtained closed-form turn avoids convex optimization problems required sparse coding. however free lunch since algorithm requires parameter namely number nearest neighbors generate codes. query rd×nlc local basis obtained simply stacking nearest neighbors global dictionary rd×n dimensional vector. recasting problem depicted grassmann manifolds using mapping obtain fig. conceptual diagram extrinsic locality constrained coding grassmann manifolds. hemisphere used represent point surface hemisphere intended grassmannian point represented symmetric idempotent rank matrix. grassmann dictionary four atoms locality constrained coding describe query point here closest atoms query point contribute coding. enough neighbors possible arbitrarily close manifold. similar formulation albeit intrinsic purpose nonlinear embedding riemannian manifolds developed aside different purpose exploit additional codebook learning step dictionary learning based intrinsic formulation analytic solution. atoms dictionary labeled generated sparse codes training query data euclidean-based classiﬁers like support vector machines classiﬁcation. inspired sparse representation classiﬁer atoms sparse dictionary labeled generated codes query sample directly used classiﬁcation. class-speciﬁc sparse codes class label atom span discrete dirac function. efﬁcient utilizing class-speciﬁc sparse codes computing residual errors. case residual error query sample span class deﬁned alternatively similarity query sample class deﬁned even non-linear like preliminary experiments suggest leads higher classiﬁcation accuracies compared aforementioned alternatives. theorem formulation precisely form coding weighted chordal mean. involvement makes solving challenging. seeking efﬁcient ways solving interesting beyond scope work. coding error given normally close making efﬁcient compromise solution. aiming sparsity -norm regularization usually employed obtain common form depicted choice problem dictionary learning grassmann manifolds written non-convexity inspired solutions euclidean spaces propose solve alternating sets variables {yi}m speciﬁcally minimizing sparse codes dictionary ﬁxed convex problem. similarly minimizing overall problem ﬁxed {yi}m convex well. algorithm details pseudo-code learning dictionary grassmann manifolds. fig. shows examples ballet dance atoms learned proposed method. atom plot dominant eigendirection visually informative. note learned atoms capture ballerina movements. perform coding relaxed idempotent rank constraints mapping since matrix addition subtraction preserve constraints. however dictionary learning orthogonality constraint ensures dictionary atoms required structure. concluding section note dictionary learning follows verbatim. difference developed algorithm coding step done using algorithm section interested coding dictionary learning higher-dimensional grassmann manifolds. treatment helpful dealing non-linearity data since hope higher-dimensional manifolds diminish non-linearity. follows practice using higher dimensional spaces vector spaces make mapping reproducing kernel hilbert space real-valued kernel function goal perform coding dictionary learning efﬁciency want avoid explicitly working words would like obtain solutions using following text show achieved. connection trick used compute principal components matrix considerably less columns rows easily established picking largest singular values corresponding elements similar statement holds convexity therefore sparse codes obtained frobenius inner products elements dictionary i.e. {xi}qx frobenius inner product corresponding subspaces obtained matrix i-th j-th column entry therefore similar approach employed obtain sparse codes algorithm provides pseudo-code performing kernel sparse coding grassmann manifolds solution problem given leading eigenvectors generalized eigenvalue problem practice might want pick small number contributed dominantly describe hence reduce computational load dictionary learning. steps determining kernel dictionary grassmann manifolds shown algorithm sets experiments presented section. ﬁrst experiments evaluate performance proposed coding methods without dictionary learning. contrast proposed coding schemes previous stateof-the-art methods several popular closed-set classiﬁcation tasks. point training considered atom dictionary. since atoms dictionary labeled case residual error approach classiﬁcation used determine label query point. second experiments performance coding methods evaluated conjunction proposed dictionary learning algorithms described delving experiments discuss videos image-sets modeled linear subspaces hence points grassmann manifolds. deﬁne video ordered collection images time-stamp information image-set simply orderless collection images. section brieﬂy demonstrate videos image-sets modeled subspaces ﬁrst consider approach time-stamp information ignored followed approach dynamics image sequences taken account. appearance image-set video vectorized representation i-th observation represented linear subspace orthogonalization procedure like svd. speciﬁcally ﬁrst columns represent optimized subspace order seen point grassmann manifold modeling linear subspaces generally take account order images. property sounds restrictive many practical situations order frames important decision making. however possible capture information related order extended type image-sets obtained block hankel matrix formalism modeled normal distributions zero mean covariance matrices rd×d rn×n respectively. loosely speaking advantage arma model decouples appearance spatio-temporal data dynamics given video ﬁnite observability parameter arma model estimated described above. represent subspace spanned columns orthonormal basis computed gram-schmidt orthonormalization. result linear dynamic system described point grassmann manifold corresponding column space observability matrix. appearance modeling presented seen special case arma modeling part compare contrast performance proposed methods several state-of-the-art methods discriminant canonical correlation analysis kernelized afﬁne hull method grassmann discriminant analysis graph-embedding grassmann discriminant analysis intrinsic sparse coding evaluate performance tasks gender recognition gait hand gesture recognition scene analysis. iterative learning method maximizes measure discrimination between image sets distance sets expressed canonical correlations. kahm images considered points linear afﬁne feature space image sets characterized convex geometric region spanned feature points. considered extension kernel discriminant analysis grassmann manifolds transform grassmann manifold learned simultaneously maximize measure inter-class distances minimize intraclass distances. ggda considered extension local discriminant transform grassmann manifolds learned. achieved incorporating local similarities/dissimilarities within-class between-class similarity graphs. kglc. value parameter experiments determined cross validation. following three experiments grassmannian dictionary proposed kgsc kglc constituted available training data. classiﬁcation method described used determine label query sample. class speciﬁc residual error case kgsc kglc obtained kernelizing gait deﬁned manner walking used biometric measure recognize among things gender humans task gender recognition gait data used dataset-b casia gait database constitutes individuals casia dataset gait subject captured angles. every video represented gait energy image size shown effective recognition gender cropped samples images shown fig. used videos captured normal clothes created subspace order using corresponding geis. resulted points randomly selected individuals training used remaining individuals testing. overlap individuals training test sets. table shows comparison kernelized versions kahm ggda. four proposed methods consistently outperform previous state-of-the-art algorithms margin. highest accuracy attained kgsc followed kglc. expected kernel extensions perform better glc. however burden determining kernel parameters could sometimes overwhelming. hand-gesture recognition task used cambridge hand-gesture dataset consists image sequences gesture classes. class image sequences performed subjects captured illuminations arbitrary motions. classes deﬁned three primitive hand shapes three primitive motions. sequence recorded resolution front ﬁxed camera roughly isolated gestures space time. fig. examples. followed test protocol deﬁned resized sequences sequences normal illumination considered training remaining sequences used testing. report recognition rates four illumination sets. addition ggda kahm proposed methods also compared tensor canonical correlation analysis product manifolds tcca name implies extension canonical correlation analysis multiway data arrays tensors. canonical correlation analysis standard method measuring similarity subspaces method tensor characterized point product manifold classiﬁcation performed space. product manifold created applying modiﬁed high order singular value decomposition tensors interpreting factorized space grassmann manifold. grassmann-based methods represented video arma modeling. observability order arma model subspace dimension selected respectively. results presented table show proposed approaches obtain highest performance. kglc achieves best recognition accuracy four sets. kahm performs poorly task conjecture illumination differences training test sets. table hand-gesture recognition. recognition accuracy hand-gesture recognition task using kahm ggda tcca product manifold proposed approaches. scene analysis employed ucsd trafﬁc dataset contains video sequences highway trafﬁc varying patterns various weather conditions video recorded resolution pixels duration ranging frames. used normalized grayscale version dataset. normalization process video clip involves subtracting mean image normalizing pixel intensities unit variance. useful reduce impact illumination variations. dataset labeled three classes respect amount trafﬁc congestion sequence. total sequences heavy trafﬁc medium trafﬁc light trafﬁc fig. examples. addition ggda kahm proposed methods also compared linear dynamical system compressive sensing linear dynamical system results presented table show proposed approaches obtain best overall performance kglc achieving highest overall accuracy. worth mentioning performance kglc competes state-of-the-art algorithms dataset table average correct recognition rate ucsd video trafﬁc dataset dynamic spatio-temporal models using compressive-sensing ggda proposed approaches. contrast log-euclidean intrinsic solutions proposed approach performed experiments synthetic data. speciﬁcally considered multi-class classiﬁcation problems ﬁrst experiment involved relatively simple classiﬁcation problem matched properties logeuclidean approach second experiment considered realistic scenario. experiments randomly generated four classes samples class obey normal distribution speciﬁc tangent space achieved considering normal distributions speciﬁc tangent space followed mapping points back using exponential details exponential map). created four classiﬁcation problems increasing difﬁculty ﬁxing mean class increasing class variance. following discussion problems referred ‘easy’ ‘medium’ ‘hard’ ‘very hard’. given problem samples class considered dictionary atoms samples class generated query data. results multiclass recognition problem samples dictionary size generated samples mapped back manifold using exponential used logeuclidean intrinsic proposed sparse coding approaches. task data generation procedure repeated times; average recognition rates reported. table comparison proposed approach log-euclidean sparse coding intrinsic sparse coding methods synthetic data. ﬁrst experiment samples class obey normal distribution identity tangent space. second experiment reﬂects challenging scenario samples class obey normal distribution random tangent space instead identity tangent space. span class variance samples various classes intertwined turn leads decrease recognition accuracy. log-euclidean approach considered setups. ﬁrst setup center projection ﬁxed second setup center projection fr´echet mean data. even though experiment matches characteristics log-euclidean approach approaches obtain performance easy case. medium case log-euclidean approaches achieve higher accuracy followed isc. note ﬁxed log-euclidean method performs better adaptive setup experiment. second experiment relaxed location tangent space order simulate challenging scenario. speciﬁcally instead generating distributions identity tangent space tangent space selected randomly. shown table experiment log-euclidean approaches perform poorly compared isc. among setups log-euclidean approach adaptive performs better ﬁxed one. similar previous experiments approach consistently outperforms isc. analyze performance proposed dictionary learning techniques described three classiﬁcation tasks face recognition action recognition dynamic texture classiﬁcation. following experiments classiﬁer gaussian kernel used perform recognition. training testing data ﬁrst coded learned dictionary sparse codes classiﬁer. parameters classiﬁer determined cross validation. since intrinsic dictionary learning proposed analytic solution grassmann manifolds compare kernel extensions conjunction dictionary learning discriminant canonical correlation analysis kernelized afﬁne hull method grassmann discriminant analysis graph-embedding grassmann discriminant analysis following experiments. face recognition single still image extensively studied recognition based group still images relatively new. popular choice modeling image-sets representing linear subspaces task image-set face recognition used youtube celebrity dataset contains video clips subjects. fig. examples. face recognition dataset challenging since videos high compression ratio low-resolution. create image video used cascaded face locator extract face regions video followed resizing regions describing histogram local binary patterns image represented linear subspace order randomly chose dataset training remaining testing. process random splitting repeated times average classiﬁcation accuracy reported. results table show proposed coding methods outperform competitors. kgsc dictionary learning achieved highest accuracy percentage points better dictionary learning. similarly performance kglc dictionary learning observed higher dictionary learning. ballet dataset contains videos collected instructional ballet dataset consists complex motion patterns performed subjects actions include ‘left-to-right hand opening’ ‘right-to-left hand opening’ ‘standing hand opening’ ‘leg swinging’ ‘jumping’ ‘turning’ ‘hopping’ ‘standing still’. fig. shows examples. dataset challenging signiﬁcant intra-class variations terms speed spatial temporal scale clothing movement. extracted image sets grouping frames exhibited action image set. described image subspace order histogram oriented gradients frame descriptor available samples randomly split training testing sets process random splitting repeated times average classiﬁcation accuracy reported. table shows proposed coding approaches superior performance compared kahm ggda. example difference dynamic textures videos moving scenes exhibit certain stationary properties time domain videos pervasive various environments sequences rivers clouds swarms birds humans crowds. experiment used challenging dyntex++ dataset comprised classes contains sequences ﬁxed size split dataset training testing sets randomly assigning half videos class training using rest query data. random split repeated twenty times; average accuracy reported. generate grassmann points used histogram three orthogonal planes takes account dynamics within videos. video split subvideos length frame overlap. subvideo deaddition kahm ggda proposed approaches compared methods speciﬁcally designed dynamic texture classiﬁcation dynamic fractal spectrum distance learning pegasos seen concatenation components volumetric component encodes stochastic self-similarities dynamic textures volumes multi-slice dynamic component captures structures dynamic textures slices along various views volume. dl-pegasos uses three descriptors learns descriptors linearly combined best discriminate dynamic texture classes. table running time comparison proposed approach intrinsic sparse coding method synthetic data. times measured second quad-core machine matlab. algorithm solves coding. computing logarithm required. efﬁcient implementation logarithm requires matrix inversion size matrix multiplications size thin size computing thin using stable algorithm like golubgive reader better sense computational efﬁciency algorithm performed experiment. assuming complexity vector sparse coding algorithms similar measured time required compute projecting tangent space considered three cases using geometry randomly generated dictionary size case measured time required compute tangent projection query points. results given table show algorithm signiﬁcantly faster isc. coding locality linear coding performed induced space. also tackled problem dictionary learning grassmann manifolds devised closed-form solution updating dictionary atom atom using geometry induced space. finally proposed kernelized version sparse coding locality linear coding dictionary learning grassmann manifolds handle non-linearity data. experiments several classiﬁcation tasks show proposed approaches achieve notable improvements discrimination accuracy comparison state-of-the-art methods discriminant analysis canonical correlation analysis afﬁne hull method grassmann discriminant analysis graphembedding grassmann discriminant analysis intrinsic sparse coding method work grassmann dictionary learned reconstruction error minimized. necessarily optimum solution labeled data available. beneﬁt labeled data recently proposed consider discriminative penalty term along reconstruction error term optimization process currently pursuing line research seeking solutions discriminative dictionary learning grassmann manifolds. moreover formulation understood extrinsic solution problem coding dictionary learning grassmann manifolds. would interesting devise intrinsic solutions based geometry induced space i.e. symmetric matrices. nicta funded australian government represented department broadband communications digital economy well australian research council centre excellence program. work funded part discovery grant shen’s participation part supported future fellowship appendix give proofs following theorems. theorem symmetric matrix eigenvalue decomposition contains eigenvalues descending order. closest matrix diagu hand symmetric dimension p×p. eigenvalues corresponding unit eigenvectors. orthogonal unit vectors subspace spanned space spanned eigenvectors counting dimensions therefore chordal mean. points distance called chordal distance points. given several points element minimizesm chordal mean {xi}m theorem chordal mean points equal projm xi). argument theorem minimizingm maximizingm thus required closest point", "year": 2014}