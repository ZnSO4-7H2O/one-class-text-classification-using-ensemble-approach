{"title": "Dual Rectified Linear Units (DReLUs): A Replacement for Tanh Activation  Functions in Quasi-Recurrent Neural Networks", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "In this paper, we introduce a novel type of Rectified Linear Unit (ReLU), called a Dual Rectified Linear Unit (DReLU). A DReLU, which comes with an unbounded positive and negative image, can be used as a drop-in replacement for a tanh activation function in the recurrent step of Quasi-Recurrent Neural Networks (QRNNs) (Bradbury et al. (2017)). Similar to ReLUs, DReLUs are less prone to the vanishing gradient problem, they are noise robust, and they induce sparse activations.  We independently reproduce the QRNN experiments of Bradbury et al. (2017) and compare our DReLU-based QRNNs with the original tanh-based QRNNs and Long Short-Term Memory networks (LSTMs) on sentiment classification and word-level language modeling. Additionally, we evaluate on character-level language modeling, showing that we are able to stack up to eight QRNN layers with DReLUs, thus making it possible to improve the current state-of-the-art in character-level language modeling over shallow architectures based on LSTMs.", "text": "paper introduce novel type rectiﬁed linear unit called dual rectiﬁed linear unit drelu comes unbounded positive negative image used drop-in replacement tanh activation function recurrent step quasi-recurrent neural networks similar relus drelus less prone vanishing gradient problem noise robust induce sparse activations. independently reproduce qrnn experiments bradbury compare drelu-based qrnns original tanh-based qrnns long short-term memory networks sentiment classiﬁcation word-level language modeling. additionally evaluate character-level language modeling showing able stack eight qrnn layers drelus thus making possible improve current state-of-the-art character-level language modeling shallow architectures based lstms. rectiﬁed activation functions widely used modern neural networks. commonly known rectiﬁed linear units essentially neurons rectiﬁed activation function relus part many successful feed-forward neural network architectures resnets densenets reaching state-of-the-art results visual recognition tasks. compared traditional activation functions sigmoid tanh relus oﬀer number advantages simple fast execute mitigate vanishing gradient problem induce sparseness. result relus permit training neural networks layers however unboundedness rectiﬁed activation functions experienced success recurrent neural networks compared sigmoid tanh activation functions. recently introduced quasi-recurrent neural network hybrid recurrent/convolutional neural network partly solve issue avoiding hidden-to-hidden matrix multiplications. however using rectiﬁed activation function instead tanh activation function calculate hidden state still cumbersome. indeed given relus positive image strictly negative values cannot added hidden state thus making subtraction values impossible. paper present type neural network component called dual rectiﬁed linear unit rather unit single activation function drelus subtract output regular relus thus coming positive negative image. consequently similar relus drelus make possible avoid vanishing gradients enforce sparseness. moreover similar tanh units drelus negative image. however diﬀerent tanh units drelus exactly zero. indeed tanh units diﬃculties exactly zero often leading introduction noise. result state drelus exhibit properone particular issue relus image negative domain relu always zero. therefore gradient neuron backpropagation. introduction parametric relus multiply input values negative domain small value rather setting zero. however non-saturating behavior negative domain introduce noise image standard relus always exactly zero. recently exponential linear units showed signiﬁcant improvements relus. elus sacriﬁce simplicity relus natural combating bias shift time noise robust. exponential linear activation function deﬁned follows despite success relus convolutional neural networks relus experienced success recurrent neural networks application recurrent step possibly inﬁnite ties relus tanh units thus making possible drelus replace functionality neurons make tanh activation function time preventing occurrence vanishing gradients stacking multiple layers. finally also introduce variant based recently introduced exponential linear units called dual exponential linear units extensively evaluate delus drelus three diﬀerent natural language processing tasks comparing orginal qrnn architecture sentiment classiﬁcation word-level language modeling character-level language modeling. introduce neural network component called dual rectiﬁed linear unit newly proposed unit able successfully replace units tanh activation function quasirecurrent neural networks independently reproduce qrnns experiments bradbury sentiment classiﬁcation word-level language modeling. additionally introduce third evaluation task namely character-level language modeling. sustain reproducibility transparency full implementation three models available online demonstrate easily stack eight drelu-based qrnn layers without need skip connections obtaining state-of-the-art results character-level language modeling. remainder paper organized follows. section give overview related work. overview followed formal problem deﬁnition solution section section evaluate proposed solution means four diﬀerent benchmarks. finally conclusions section sequence bounded activation functions sigmoid tanh needed. managed train simple relu activation function carefully initializing hidden state weight matrix identity matrix. though good results obtained still outperformed long short-term memory networks general relus used feed-forward parts neural networks rather recurrent parts variable length text rnns still preferred type neural network modeling sequences words lstms particular cnns less commonly used need large receptive ﬁelds ﬁxed-input size capture long-range dependencies. recently number hybrid approaches emerged also leveraging cnns. kalchbrenner used ﬁxed-size perform character-level neural machine translation obtaining state-of-the-art results. bradbury introduced concept quasi-recurrent neural networks main components qrnns cnns lstm-based simpliﬁed recurrent step. simpliﬁed recurrent step allows modeling sequences variable length without using ﬁxed-size window combination cnns eﬀectuates signiﬁcant speed-ups. current methods modeling variable-length sequences still rely tanh activation function. activation function however causes gradients vanish backpropagation. relus less prone vanishing gradient problem unbounded positive image. hence less suited rnns. proposed concept drelu leverages strengths types units order train models variablelength sequences. section ﬁrst argue relus cause training issues recurrent neural networks simple rnns long short-term memory networks next explain quasi-recurrent neural networks proven viable alternative lstms. finally rnns neural networks contain cyclic connections. cyclic connections allow preserving history well suited modeling variable-length sequences. exist many types rnns notably simple rnns lstms. weight matrices bias vector. function activation function practice sigmoid tanh function. functions sigmoid tanh bounded functions. implies functions exists real number |gpxq| domain activation functions sigmoid tanh bounded consequently hidden states also bounded absolute value never larger unbounded activation function relu becomes clear hidden state grow exponentially large. indeed largest eigenvalue weight matrix larger norm vector continuously grow eventually explode holds true lstms current cell state hidden state dependent previous hidden state matrix multiplications. consequently replacing tanh activation function unbounded activation function would still problematic leading similar unbounded multiplication simple rnns. lstms cnns fast highly parallelizable lstms able naturally model long-range dependencies. however simple rnns lstms contain slow recurrent step executed sequence. indeed depends depends turn thus creating execution bottleneck. reduce computational eﬀort needed recurrent step hidden-to-hidden matrix multiplications lstm removed convolution size input introduced. additionally input forget gates connected yielding gates namely forget gate output gate formally qrnn fo-pooling deﬁned follows consequently recurrent step reduced simple gated/weighted thus eﬀectuating faster execution. opens possibilities unbounded activation functions rectiﬁed linear activation functions given hidden state cannot exponentially grow anymore. introduced previous section qrnns contain hidden-to-hidden matrix multiplications anymore thus avoiding hidden state explosions unbounded activation function used. however replacing tanh function candidate cell state relu would limit expressiveness cell state update indeed values gates bounded zero one. activation function candidate cell state relu cell state updated positive values. consequently means values added previous cell state subtracted. result lower values hidden state gates zero. tanh function issue given image bounded minus one. therefore cell state also updated negative values. able update cell state negative values time unbounded rectiﬁed linear activation function introduce novel concept dual rectiﬁed linear unit rather using single relu propose subtract relus. dual rectiﬁed linear activation function twodimensional activation function. analogy rectiﬁed linear activation function proposed function formally deﬁned follows important beneﬁt drelus tanh activation functions ability exactly zero. indeed sparseness allows eﬃcient training larger stacks neural networks makes relus noise-robust compared related activation functions leaky relus. diﬀerent sigmoid tanh activation functions drelus less prone vanishing gradients property also shared relus. relu drelu active magnitude gradient activation function neither ampliﬁed diminished. additionally rectiﬁed linear activation functions fast execute. elus shown perform better relus image classiﬁcation indeed nonzero image negative domain combats bias shift speeds learning. however elus contain complex calculations cannot exactly zero. analogy drelus deﬁne delus. dual exponential linear activation function expressed follows evaluate proposed drelus delus part quasi-recurrent neural network three diﬀerent sequencing tasks. first reproduce results sentiment classiﬁcation word-level language modeling comparing drelus tanh-based qrnns lstms. second evaluate drelus delus third task challenging nature namely character-level language modeling. latter consists benchmarks. support reproducibility exact implementation experiments found online including experiments original qrnn paper full implementation available. ﬁrst task consider sentiment classiﬁcation imdb movie reviews. binary classiﬁcation task documents tokens. goal compare drelus delus part qrnn original tanh-based qrnn model lstms. also investigate impact densely connecting layers. follow exact setup bradbury models initialized dimensional glove word embeddings four stacked recurrent layers. regularize dropout applied every layer regularization used. initialization strategy speciﬁed empirically found using glorot normal initialization yielded similar results reported bradbury imdb movie review dataset contains positive negative reviews equally divided training testing. exact validation speciﬁed split training distinct validation sets every experiment times diﬀerent validation set. bradbury report experiments densely connected layers also report results experiments without layers. case densely connecting layers means adding concatenative skip connections every layer except ﬁnal classiﬁcation layer. mean standard deviation runs diﬀerent experiments reported table general observe small accuracy diﬀerences densely connected lstms qrnns independent activation function used. however skip connections used tanh-based qrnns obtain worse accuracy results compared lstms drelu-based qrnns moreover absolute diﬀerence accuracy drelu-based qrnns dc-qrnns tanh-based qrnns dc-qrnns indeed skip connections typically used avoid vanishing gradients. relu-based activation functions less prone vanishing gradients tanh activation functions favoring drelus tanh activation functions qrnns. similar conclusions apply delus. table evaluation qrnns drelu delu activation functions compared lstms qrnns tanh activation functions task sentiment classiﬁcation. accuracy standard deviation reported. means densely connected. additionally compared qrnn lstm observed speed-up drelutanh-based qrnn lstm respectively. consequently drelu-based qrnns obtain similar results lstms double speed without deploying skip connections needed tanh-based qrnns. rather focusing obtaining state-of-the-art perplexity results section emphasis investigating relative performance relus drelus delus compared stacked lstms stacked qrnns tanh activation functions similar conditions. follow exact setup bradbury training qrnns drelus delus. two-layer stacked qrnn hidden units equally sized embeddings convolution size two. qrnn tanh activation function initialized weights uniformly interval r´.; qrnns rectiﬁed activation functions initialized weights randomly following normal distribution delus training neural network training procedure described bradbury results experiments shown table general observe qrnns drelus delus outperform qrnn tanh activation function applied candidate cell state ˜ct. interestingly qrnn using single relu activation functable activation statistics cell states qrnn three diﬀerent activation functions applied candidate cell states ˜ct. account near-zero results interval statistics calculated full test set. tion performs much worse. perplexity qrnn single dual relu activation function perplexity points. consequently relus inferior replacements tanh-based units drelus successfully replace even outperform them. comparing lstm following setup observe outperform medium-sized lstm hidden units. analyze activation pattern diﬀerent activation functions induce also calculated activation statistics cell states qrnn tanh relu drelu activation function. results depicted table cell states nearly zero using tanh activation function nearly zero using relu using drelu. non-zero cell states drelu tanh activation functions equally divided positive negative values. moreover qrnns using relus drelus equally active positive domain. consequently drelu activation functions show similar behavior tanh activation functions additional beneﬁt being able exactly zero induce sparse activations. indeed sparse layer outputs allow information disentangling variable-size intermediate representations eventually training large neural networks terms width depth. section showed drelus delus similar properties tanh activation functions suitable replacements. single relu however inferior replacement tanh activation function. additionally using drelus cell state becomes sparse important beneﬁt training large neural networks. ﬁnal task consider character-level language modeling. goal predict next character sequence characters. stack eight qrnn layers either using drelus delus compare results current state-of-the-art. language modeling experiments consider small large challenging dataset. small dataset penn treebank dataset consisting roughly characters large dataset enwik/hutter prize dataset contains characters extracted wikipedia. dataset challenging contains markup nonlatin characters. adopt train/validation/test split mikolov dataset m/m/m dataset split hutter prize dataset. neural network architecture consists embedding layer number qrnn layers ﬁnal classiﬁcation layer softmax activation function. embeddings size hidden states size number qrnn layers width convolution ﬁrst layer always layers convolution width two. weight matrices orthogonally initialized. trained neural network using adam learning rate constrained norm gradient following used batch size sequence length language modeling experiments penn treebank dataset. memory constraints used batch size experiments hutter prize dataset used sequence length instead regularized model using dropout output every layer dropout probability models hidden state size dropout probability models hidden state size applied batch normalization qrnn models evaluation metric used bits-per-character penn treebank experiments. table results several experiments dataset listed. ﬁrst part shows results number successful stateof-the-art character-level language modeling methods including vanilla two-layer lstm second part shows results qrnns trained several activation functions. general observe score tanhbased qrnns worse score dreludelu-based qrnns. drelus perform equally good delus. doubling number qrnn layers four eight parameter eﬃcient doubling hidden state size units units. language modeling performance however same. using eight qrnn layers hidden units obtain score outperforming two-layer hyperlstm similar number parameters state-ofthe-art result dataset. hutter prize experiments. compared dataset hutter prize dataset challenging dataset consisting characters well non-latin characters markup. results experiments depicted table apart number layers trained identical qrnn-based architectures. consider drelus given delus gave similar results benchmarks. model four layers hidden state units layer obtains score performs better hyperlstms using similar number parameters equally good hierarchical multiscale lstms doubling number layers eight reduce score outperforming complicated architectures bytenet recurrent highway networks note latter evaluated context language modeling. quasi-recurrent neural networks similar tanh units drelus positive negative activations. addition drelus several advantages tanh units decrease magnitude gradients active exactly zero making noise robust induce sparse output activations layer. evaluated drelus delus three diﬀerent tasks. evaluated sentiment classiﬁcation showed drelus delus need dense connections improve gradient backpropagation compared tanh activation functions stacking four layers qrnns. also demonstrated drelus delus improve perplexity task wordlevel language modeling compared tanh activation functions single relu inferior replacement. finally trained model eight stacked dreludelu-based qrnn layers obtained state-of-the-art results task character-level language modeling diﬀerent datasets tanh-based qrnns advanced architectures", "year": 2017}