{"title": "Why Regularized Auto-Encoders learn Sparse Representation?", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks-- \\textit{Internal Covariate Shift}-- the current solution has certain drawbacks. For instance, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate due to shifting parameter values (especially during initial training epochs). Another fundamental problem with BN is that it cannot be used with batch-size $ 1 $ during training. We address these drawbacks of BN by proposing a non-adaptive normalization technique for removing covariate shift, that we call \\textit{Normalization Propagation}. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with BN. We exploit the observation that the pre-activation before Rectified Linear Units follow Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers.", "text": "sparse distributed representation learning useful features deep learning algorithms efﬁcient mode data representation also importantly captures generation process real world data. number regularized autoencoders enforce sparsity explicitly learned representation others don’t little formal analysis encourages sparsity models general. objective formally study general problem regularized auto-encoders. provide sufﬁcient conditions regularization activation functions encourage sparsity. show multiple popular models activations satisfy conditions; thus conditions help explain sparsity learned representation. thus theoretical empirical analysis together shed light properties regularization/activation conductive sparsity unify number existing auto-encoder models activation functions analytical framework. sparse distributed representation constitutes fundamental reason behind success deep learning. hand efﬁcient representing data robust noise; fact main advantages sparse distributed representation context deep neural networks shown information disentangling manifold ﬂattening well better linear separability representational power hand reasons objective paper investigate number regularized auto-encoders exhibit similar behaviour especially terms learning sparse representations. especially interesting matter clear distinction learned encoder representation decoder output. contrast deep models clear distinction encoder decoder parts. idea learning sparse representations new. aforementioned biological connection natural follow-up pursued number researchers propose variants encouraged sparsity learned representation hand also work empirically analyzing/suggesting sparseness hidden representations learned pre-training unsupervised models however best knowledge prior work formally analyzing regularized learn sparse representation general. main challenge behind analysis non-convex objective functions. addition questions regarding efﬁcacy activation functions choice regularization objective often raised since multiple available choices both. also address questions regards paper. address questions parts. first prove sufﬁcient conditions regularizations encourage pre-activations hidden units. analyze properties activation functions coupled regularizations result sparse representation. multiple popular activations desirable properties. second show multiple popular objectives including de-noising auto-encoder contractive auto-encoder random variable function random variable average activation fraction unit mass distribution hidden unit activates. ﬁnite sample datasets becomes fraction data samples unit activates. also note dictates representational units participate data representation units activate single data sample. thus major difference dead units since sparsity general also achieved units dead. however latter scenario undesirable truly capture sdr. thus model study conditions encourage sparsity hidden units; also empirically show conditions capable achieving sdr. analysis linear decoding addresses case continuous real valued data distributions. show regularization activation function play important role achieving sparsity. order make following assumption assumption assume data drawn distribution identity matrix. denote reconstruction further residual auto-encoder training iteration training sample assume every dimension i.i.d. random variable following gaussian distribution mean standard deviation denote hidden unit training iteration respectively denotes notice assumption true remove encoding bias optimization expected preactivation becomes unconditionally iterations. consider activation function activation threshold δmin i.e. data sample pre-activation would de-activate unit δmin activate otherwise. unit exhibit sparse behaviour expected pre-activation always zero majority samples pre-activation below δmin. then order average zero indeed suggested form regularization; thus explaining existing encourage sparsity latent representation. based theoretical analysis also empirically study multiple popular models activation functions order analyze comparative behaviour terms sparsity learned representations. analysis thus shows various models activations lead sparsity. result uniﬁed under framework uncovering fundamental properties regularizations activation functions existing models possess. auto-encoders class single hidden layer neural networks trained unsupervised manner. consists encoder decoder. input ﬁrst mapped latent space hidden representation vector encoder activation rm×n weight matrix encoder bias. then maps hidden output back original space reconstructed counterpart decoder activation. objective basic auto-encoder minimize following respect parameters squared loss function. motivation behind objective capture predominant repeating patterns data. thus although auto-encoder optimization learns input back itself focus learning noise invariant representation data. objective convex ﬁxed hence generally solved alternately variable ﬁxing other. note penalty driving force objective forces latent variable sparse. minority threshold taken larger values average compared majority. however strategy limits degree sparsity unit achieve given data distribution following assumption weight lengths upper bounded preactivation value also become upper bounded. bounded weight length condition desired practice convergence achieved regularizations like weight decay max-norm thus order hidden units exhibit sparse behaviour encoding bias needs part optimization. established importance encoding bias make following deduction based assumption lemma assumption true encoding activation function ﬁrst derivative ∂jae/∂bej keep reducing every training iteration. theorem rm×n eters regularized auto-encoder learning rate. true general multiple iterations terms depend weight vectors also change every iteration depending value however generally interested direction weight vectors reconstruction instead scale. thus length weight vectors term ∂bej bounded ﬁxed term w.r.t. weight vectors depend bias data distribution. under circumstances increasing value conducive lower expected pre-activation strictly greater zero. hand changing value signiﬁcant effect expected pre-activation values especially weight length ﬁxed. case weight length ﬁxed changing value affect value weight length turn affect term ∂jae also affects expected pre-activation unit; effect largely unpredictable depending form ∂jae next section connect notions expected pre-activation sparsity activation functions certain properties extend arguments sparsity hidden units. finally relaxed cases weight lengths constrained ﬁxed length upper bound weight vectors’ length easily guaranteed using maxnorm regularization weight decay widely used tricks training deep networks prior case every weight vector simply constrained within ball ﬁxed constant) every gradient update. shown property regularization functions encourages lower pre-activations introduce classes regularization functions inherit property thus manifest predictions made above. corollary non-decreasing activation function monotonically increasing function along negative gradient updating jrae results exwt+ corollary non-decreasing convex activation function ﬁrst derivative along nega∃λ updating tive gradient jrae results var]) necessarily imply reduction hidden unit value thus sparsity. however regularizations become immediately useful consider nonproceeding would like mention although general notion sparsity entails majority units de-activated i.e. value less certain threshold practice representation truly sparse usually yields better performance extending argument theorem obtain theorem formally connects notions expected pre-activation expected sparsity hidden unit. speciﬁcally shows usage non-decreasing activation functions lead lower expected pre-activation thus higher probability de-activated hidden units theorem applies. result coupled property lima→−∞ implies average sparsity hidden units keeps increasing sufﬁcient number iterations activations. notice convexity desired regularizations corollary thus summary non-decreasing convex ensure ∂r/∂bej positive regularizations corollary turn encourages expected pre-activation suitable values ﬁnally leads higher sparsity lima→−∞ notice derive strict inequality theorem even though corollaries suggest non-decreasing convex activations imply relaxed case done reasons ensure sparsity monotonically increases iterations tmin tmax condition ∂r/∂bej unlikely activations non-zero ﬁrst/second derivatives term depends entire data distribution. popular choice activation functions relu maxout sigmoid tanh softplus. maxout tanh applicable framework satisfy negative saturation property. relu non-decreasing convex function; thus corollary apply. note relu second derivative. thus practice lead poor sparsity regularization corollary lack bias gradients regularization i.e. ∂r/∂bej side advantage relu enforces hard zeros learned representations. softplus non-decreasing convex function hence encourages sparsity suggested regularizations. contrast relu softplus positive bias gradients smoothness. hand note softplus produce hard zeros asymptotic left saturation sigmoid corollary applies unconditionally sigmoid corollary doesn’t apply general. hence sigmoid guaranteed lead sparsity used regularizations form speciﬁed corollary notice activation functions ﬁrst derivative conclusion maxout tanh satisfy negative saturation property hence guarantee sparsity others– relu softplus sigmoid– properties encourage sparsity learned representations suggested regularizations. point natural question whether existing learn sparse representation. complete loop show popular objectives regularization term similar proposed corollaries thus indeed learn sparse representation. de-noising auto-encoder aims minimizing reconstruction error every sample reconstructed vector using corresponding corrupted version corrupted version sampled conditional distribution original objective given denotes conditional distribution given since objective analytically intractable corruption process take second order taylor’s approximation objective around distribution mean order overcome difﬁculty theorem represent parameters squared loss linear decoding i.i.d. gaussian corruption zero mean variance point training data sampled distribution corresponding denotes corruption variance intended input dimension. authors mdae proposed algorithm primary goal speeding training deriving approximate form omits need iterate large number explicitly corrupted instances every training sample. remark represent parameters mdae linear decoding squared loss point training data sampled distribution then apart justifying sparsity equivalences also expose similarity mdae regularization follow form corollary note goal achieving invariance hidden original representation respectively mdae show mere factor weight length regularization case linear decoding. desired average activation thus requires additional parameter needs pre-determined. make follow paradigm thus tuning value would automatically enforce balance ﬁnal level average sparsity reconstruction error. thus objective becomes ﬁrst term regularization form stated corollary even though second term doesn’t exact suggested form straight forward term generates non-negative bias gradients non-decreasing convex activation functions note last term depends reconstruction error practically becomes small epochs training regularization terms take over. besides term usually ignored positivedeﬁnite. suggests capable learning sparse representation. denotes jacobian matrix objective aims minimizing sensitivity hidden representation slight changes input. remark represent parameters regularization coefﬁcient point training data sampled distribution then thus regularization also form identical form suggested corollary thus hidden representation learned also sparse. addition since ﬁrst order regularization term higher order suggests cae+h objective similar properties term sparsity. form speciﬁed corollary showed enforces sparsity. thus although expected regularization enforce sparsity intuitive standpoint results show indeed theoretical perspective. experimental protocols since neural network optimization non-convex training different optimization conditions lead drastically different outcomes. however things make training difﬁcult well designed optimization strategies without learn useful features. analysis based certain assumptions data distribution conditions weight matrices. thus order empirically verify analysis following experimental protocols make optimization well conditioned. experiments mini-batch stochastic gradient descent momentum optimization epochs batch size hidden units train mdae hyper-parameters experiments. regularization coefﬁcient values models except values represent variance gaussian noise added. models activation functions squared loss linear decoding. initialize bias zeros normalized initialization weights. further subtract mean divide standard deviation samples. learning rate small won’t move weights initialized region convergence would slow. hand large learning rate change weight direction drastically something don’t desire predictions hold. middle ground choose terminology interested analysing sparsity hidden units function regularization coefﬁcient experiments. recall notion sparsity denoted fraction data samples deactivate hidden unit instead fraction hidden units deactivate given data sample. choice made order treat hidden unit random variable. since cannot identify particular hidden unit across auto-encoders trained different values measuring level sparsity autoencoder units compute average activation fraction deﬁned follows indicator operator denotes hidden unit data sample δmin activation threshold. case relu δmin case sigmoid softplus δmin also denote total number data samples number hidden units respectively. notice sparsity hidden unit inversely related average activation fraction single unit. thus deﬁnition avg. activation fraction indicator average sparsity across hidden units. finally measuring avg. activation fraction training also keep track fraction dead units. dead units hidden units deactivate data samples thus unused network data reconstruction. notice achieving sparsity desired minimal hidden units dead alive units activate small fraction data samples. main predictions made based theorem sparsity hidden units remain unchanged respect bias gradient weight lengths ﬁxed pre-determined value expected pre-activation becomes completely independent notice prediction accounts change sparsity result change expected pre-activation corresponding unit. sparsity also increase expected pre-activation unit ﬁxed result change weight directions majority samples take pre-activation values activation threshold minority takes values overall expected value remains unchanged. change weight directions also affected since regularization functions speciﬁed corollary contain weight bias terms. however latter factor contributing change sparsity unpredictable terms changing values. hence desired sparsity largely affected bias gradient present better predictive power. hence analyse effect regularization coefﬁcient sparsity representations learned models using relu activation function weight lengths constrained one. notice relu zero bias gradient mdae also equivalent regularization derived plots shown ﬁgure effect bias gradient largely dominates behaviour hidden units terms sparsity. specifically predicted average activation fraction remains unchanged respect regularization coefﬁcient relu applied mdae absence bias gradient. also analyse effect regularization coefﬁcient sparsity representations learned models using relu activation functions weight lengths constrained. plots seen trend becomes unpredictable mdae discussed theorem without weight length constraint affects weight length turn affects ∂jae changes value expected pre-activation. however effect unpredictable thus undesired. hand constrained length case number dead units start rising average activation fraction reaches around however case unconstrained weight length relu avg. activation fraction shows constrained weight length achieves higher level sparsity giving rise dead units. summary bias gradient dominates behaviour hidden units terms sparsity. also experiments suggest predictive power better sparsity hidden weights constrained ﬁxed length. notice restrict usefulness representation leaned auto-encoders since interested ﬁlter shapes scale. surprising part experiments stable decreasing sparsity trend relu although regularization form given corollary fact relu practically generate bias gradients form regularization brings attention interesting possibility relu generating positive bias gradient ﬁrst order regularization term dae. recall marginalize ﬁrst order term taking expectation corrupted versions training sample. however mathematically equivalent objective obtained analytical marginalization optimize practice. optimizing explicit corruption batch-wise manner indeed non-zero ﬁrst order term vanish ﬁnite sampling thus explaining sparsity relu. test hypothesis optimizing explicit taylor’s expansion ﬁrst order term mnist cifar- using standard experimental protocols gaussian corrupted version activation fraction corruption variance edae shown ﬁgure conﬁrms ﬁrst order term contributes towards sparsity. general note lower order terms highly non-linear functions generally change slower compared higher order terms. conclusion explicit corruption advantages times compared marginalization captures effect lower higher order terms together. predicted theorem bias gradient strictly positive increasing value lead smaller expected pre-activation thus increasing sparsity. specially true weight lengths ﬁxed length. term depend weight length also affected however since effect hard predict sparsity always proportional un-constrained weight length. order verify intuitions ﬁrst analyse effect regularization coefﬁcient sparsity representations learned models using sigmoid activation function weight lengths constrained one. plots shown ﬁgure plots show stable increasing sparsity trend increasing regularization coefﬁcient predicted analysis. finally analyse effect regularization coefﬁcient sparsity representations learned models using sigmoid activation function weight lengths unconstrained. plots shown ﬁgure mentioned above unconstrained weight length leads unpredictable behaviour sparsity respect regularization coefﬁcient. seen mdae datasets summary weight lengths constrained ﬁxed value lead better predictive power terms sparsity. however either case empirical observations substantiate claim sparsity autoencoders dominated effect bias gradient regularization instead weight direction. explains existing regularized auto-encoders learn sparse representation effect regularization coefﬁcient sparsity. establish formal connection features learned regularized auto-encoders sparse representation. contribution multi-fold show regularizations positive encoding bias gradient encourage sparsity zero bias gradient affected regularization coefﬁcient; activation functions non-decreasing negative saturation zero encourage sparsity regularizations multiple existing activations property existing regularizations form suggested corollary brings uniﬁed framework also shows general forms regularizations encourage sparsity. empirical side bias gradient dominates effect sparsity hidden units; speciﬁcally sparsity general proportional regularization coefﬁcient bias gradient positive remains unaffected zero constraining weight vectors optimization ﬁxed length leads better sparsity behaviour predicted analysis. notice restrict usefulness representation leaned autoencoders since interested ﬁlter shapes scale. side without length constraint behaviour auto-encoders w.r.t. regularization coefﬁcient becomes unpredictable cases. explicit corruption advantages marginalizing captures ﬁrst second order effects. conclusion analysis combined together uniﬁes existing activation functions bringing under uniﬁed framework also uncovers general forms regularizations fundamental properties encourage sparsity hidden representation. analysis also yields insights provides novel tools analysing existing regularization/activation functions help predicting whether resulting learns sparse representations. rifai salah mesnil gr´egoire vincent pascal muller xavier bengio yoshua dauphin yann glorot xavier. higher order contractive auto-encoder. ecml/pkdd hinton geoffrey srivastava nitish krizhevsky alex sutskever ilya salakhutdinov ruslan. improving neural networks corr preventing co-adaptation feature detectors. abs/. notice terms containing ∂jae equation disappear terms already function expectation deal expected cost function. thus terms linear hence taking expectation results lemma ∂jae theorem represent parameters squared loss linear decoding i.i.d. gaussian corruption zero mean variance point training data sampled distribution corresponding sample then remark rm×n represent parameters marginalized de-noising auto-encoder point training activation function linear decoding squared loss data sampled distribution corresponding sample then", "year": 2015}