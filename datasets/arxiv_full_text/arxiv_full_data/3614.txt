{"title": "Adversarial Examples that Fool both Human and Computer Vision", "tag": ["cs.LG", "cs.CV", "q-bio.NC", "stat.ML"], "abstract": "Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we create the first adversarial examples designed to fool humans, by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by modifying models to more closely match the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.", "text": "figure cases adversarial examples fool humans brief exposure example depicted strong effect even long viewing duration. left show image cat. right show image adversarially perturbed look like dog. although easily overlooked note cat-speciﬁc features still identiﬁed. instance original boundary head wall still visible adversarial image despite head seeming lower. also long white whiskers remain visible. attack models attacker access naturally raises question whether humans susceptible adversarial examples. clearly humans prone many cognitive biases optical illusions generally resemble small perturbations natural images currently generated optimization machine learning loss function. thus current understanding ﬁeld class transferable adversarial examples effect human visual perception thorough empirical investigation performed. rigorous investigation question creates opportunity machine learning gain knowledge neuroscience neuroscience gain knowledge machine learning. neuroscience often provided existence proofs machine learning—before working object recognition algorithms hypothesized possible build human brain recognize objects. hassabis review inﬂuence neuroscience artiﬁcial intelligence. machine learning models vulnerable adversarial examples small changes images cause computer vision models make mistakes identifying school ostrich. however still open question whether humans prone similar mistakes. here create ﬁrst adversarial examples designed fool humans leveraging recent techniques transfer adversarial examples computer vision models known parameters architecture models unknown parameters architecture modifying models closely match initial processing human visual system. adversarial examples strongly transfer across computer vision models inﬂuence classiﬁcations made time-limited human observers. machine learning models easily fooled adversarial examples inputs optimized adversary produce incorrect model classiﬁcation computer vision adversarial example usually image formed making small perturbations example image dataset. many popular algorithms constructing adversarial examples rely access architecture parameters model perform gradient-based optimization input. without similar access brain methods seem applicable constructing adversarial examples humans. work done member google residency progoogle brain stanford university gram berkeley pennsylvania state university. correspondence gamaleldin elsayed <gamaleldingoogle.com> jascha sohl-dickstein <jaschasdgoogle.com>. figure adversarial examples optimized models viewpoints sometimes appear meaningful humans. observation clue machine-to-human transfer possible. canonical example adversarial image reproduced goodfellow adversarial attack moderate limited ability fool model geometric transformations fool models model used generate image. adversarial attack causing image labeled computer robust geometric transformations adopted athalye unlike attack image contains features seem semantically computer-like humans. adversarial patch causes images labeled toaster optimized cause misclassiﬁcation multiple viewpoints reproduced brown similar patch contains features appear toaster-like human. experiments similar effect adversarial images generated fool multiple models rather fool model multiple viewpoints. images presented correspond sequence adversarial attacks classify image dog. left right attack performed larger ensemble models class predictions test models included image. number models targeted attack increases resulting image appears dog-like humans. bottom attack magnitude varied attack models. image appears somewhat dog-like humans even knew conclusively human brain could resist certain class adversarial examples would provide existence proof similar mechanism machine learning security. knew conclusively brain fooled adversarial examples machine learning security research perhaps shift focus designing models robust adversarial examples designing systems secure despite including non-robust machine learning components. likewise adversarial examples developed computer vision affect brain phenomenon discovered context machine learning could lead better understanding brain function. work investigate inﬂuence adversarial examples strongly transfer across computer vision models human visual perception. leverage three ideas test whether adversarial examples cause observable effect human visual system. first recent black adversarial example construction techniques create adversarial examples target model without access model’s architecture parameters. second adapt machine learning models mimic initial visual processing humans making likely adversarial examples transfer model human observer. third evaluate classiﬁcation decisions human observers time-limited setting even subtle effects human perception detectable. words humans achieve near-perfect accuracy classiﬁcation task small changes performance correspond measurable changes accuracy. making image presentation sufﬁciently brief humans unable achieve perfect accuracy even clean images small changes performance lead measurable changes accuracy. additionally brief image presentation limits time brain utilize recurrent top-down processing pathways believed make processing brain closely resemble feedforward artiﬁcial neural network. adversarial examples transfer across computer vision models successfully inﬂuence perception human observers thus uncovering class illusions shared computer vision models human brain. inputs machine learning models attacker intentionally designed cause model make mistake. context visual object recognition adversarial examples images usually formed applying small perturbation naturally occurring image breaks predictions made machine learning classiﬁers. figure canonical example adding small perturbation image panda causes misclassiﬁed gibbon. perturbation small enough imperceptible perturbation noise—it relies carefully chosen structure based parameters neural network—but magniﬁed perceptible human observers cannot recognize meaningful structure. note adversarial examples also exist domains like malware detection focus image classiﬁcation tasks. adversarial examples designed cause mistake. deﬁned differ human judgment. adversarial examples deﬁned deviation human output would deﬁnition impossible make adversarial examples humans. tasks like predicting whether input numbers prime clear objectively correct answer would like model correct answer answer provided humans challenging deﬁne constitutes mistake visual object recognition since adding perturbation image likely longer corresponds photograph real physical scene philosophically difﬁcult deﬁne real object class image picture real object. work assume adversarial image misclassiﬁed output label differs human-provided label clean image used starting point adversarial examples. make small adversarial perturbations assume small perturbations insufﬁcient change true class. adversarial examples deﬁned imperceptible. case would impossible deﬁnition make adversarial examples humans changing human’s classiﬁcation would constitute change human perceives. moreover many domains possible make imperceptible changes another model trained different training even trained different algorithm transfer effect makes possible perform black attacks adversarial examples fool models attacker access kurakin found adversarial examples transfer digital physical world despite many transformations lighting camera effects modify appearance photographed physical world. showed transferability adversarial example greatly improved optimizing fool many machine learning models rather individual machine learning model adversarial example fools models used optimization process extremely likely fool arbitrary sixth model. observations give clues transfer humans possible adversarial examples transfer across viewpoints models. adversarial perturbations generated individual machine learning model single viewpoint typically appear meaningful humans. however recent studies adversarial examples transfer across multiple settings sometimes produced adversarial examples appear meaningful human observers. instance adversarially perturbed resemble computer transfering across geometric transformations develops features appear computer-like ‘adversarial toaster’ brown possesses features seem toaster-like observe similar effect experiments adversarial example forced transfer across ensemble models rather across geometric transformations development human-meaningful features consistent adversarial example coming closer fooling humans. however interpreted cautiously also consistent adversarial example simply coming closer real example adversarial target class primate visual system motivates possibility adversarial examples transfer computer vision models humans. cadieu yamins dicarlo observed activity deeper layers predictive activity recorded visual pathway primates. riesenhuber poggio developed model object recognition cortex closely resembles many aspects modern cnns. k¨ummerer showed cnns predictive human gaze ﬁxation. style transfer demonstrated intermediate layers capture notions artistic style meaningful humans. freeman simoncelli used representations cnn-like model develop psychophysical metamers indistinguishable humans viewed brieﬂy carefully controlled ﬁxation. geirhos rajalingham performed psychophysics experiments comparing pattern errors made humans made neural network classiﬁers. differences machine human vision occur early sensory apparatus. images typically presented cnns static rectangular pixel grid constant spatial resolution. primate hand eccentricity dependent spatial resolution. resolution high fovea central visual ﬁeld falls linearly increasing eccentricity high spatial frequency perturbation periphery image might occur part adversarial example would undetectable thus would impact human perception. differences include sensitivity temporal well spatial features well non-uniform color sensitivity modeling early visual system continues area active study describe section mitigate differences using biologically-inspired image input layer. beyond early visual processing major computational differences cnns human brain. cnns consider fully feedforward architectures visual cortex many times feedback feedforward connections well extensive recurrent dynamics possibly differences architecture humans found experimentally make classiﬁcation mistakes qualitatively different made deep networks additionally brain treat scene single static image actively explores saccades common psychophysics experiments mitigate differences processing limiting image presented time subject process described section section details machine learning vision pipeline. section describes psychophysics experiment evaluate impact adversarial images human subjects. experiment used images imagenet imagenet contains highly speciﬁc classes typical people able identify chesapeake retriever. thus combined classes form coarse classes conﬁdent would familiar experiment subjects grouped classes following groups pets group hazard group vegetables group constructed ensemble models trained imagenet model instance architectures inception inception inception resnet resnet resnet resnet better match initial processing human visual system prepend model input retinal layer incorporates transformations performed human eye. layer perform eccentricity dependent blurring image approximate input received visual cortex human subjects retinal lattice. details model described appendix eccentricity-dependent spatial resolution measurements essen anderson along known geometry viewer screen determine degree spatial blurring image location limit information also available human visual system. layer fully differentiable allowing gragiven image group wish generate targeted adversarial examples strongly transfer across models. means class pair generate adversarial perturbations models classify perturbed images similarly perturbed images classiﬁed different perturbation constructed image; however norm perturbations constrained formally given classiﬁer assigns probability coarse class given input image speciﬁed target class ytarget maximum perturbation want image xadv minimizes log) constraint ||xadv x||∞ appendix details computing coarse class probabilities classiﬁer’s parameters perform iterated gradient descent order generate xadv. pipeline image drawn source coarse class perturbed classiﬁed image target coarse class. attack method iterative targeted attack performed sign)) cost function described below ytarget label target class step size original clean image xadv ﬁnal adversarial image. given per-condition section optimization perturbation whose ∞-norm less scaled ∞-norm guarantee consistent norm perturbations. goal create adversarial examples transferred across many machine learning models assessing transferability humans. accomplish this created ensemble geometric mean several image classiﬁers performed iterative attack ensemble loss coarse class probabilities model pens probability ensemble. encourage high transfer rate retained adversarial examples successful models condition least models false condition figure experiment setup task. experiment setup recording apparatus. task structure timings. subject asked repeatedly identify classes brieﬂy presented image belongs image either adversarial belongs several control conditions. section details. total subjects normal corrected vision participated experiment. subjects gave informed consent participate study awarded reasonable compensation time effort. subjects ﬁxed chair away high refreshrate computer screen room dimmed light subjects asked classify images appeared screen classes pressing buttons response time using ﬁngers right hand. assignment classes buttons randomized experiment session. trial started ﬁxation cross displayed middle screen instructing subjects direct gaze ﬁxation cross ﬁxation period image size presented brieﬂy center screen period image followed sequence high contrast binary random masks displayed subjects asked classify object image pressing buttons starting image presentation time lasting mask turned off. wait start next trial whether subjects responded quickly slowly. realized exposure durations times reported above measured photodiode oscilloscope separate test experiment. subject’s response time recorded response time relative image presentation time case subject pressed button trial class corresponding ﬁrst choice considered. subject completed trials. image added adversarial perturbation δadv crafted cause model misclassiﬁcation opposite class group used perturbation size large enough noticeable humans computer screen still small respect image intensity scale flip similar adversarial perturbation ﬂipped vertically adding image. control condition test whether adversarial perturbations change human perceptions non-adversarial perturbations nearly identical statistics. false condition subject forced make mistake. include condition adversarial perturbations reduce accuracy human observers could perturbations degrade image quality. show adversarial perturbations actually control chosen class include condition neither options available subject correct accuracy always zero test whether adversarial perturbations inﬂuence wrong choices make. show random image imagenet class classes group adversarially perturb toward classes group. subject must choose classes. example might show airplane adversarially perturbed toward class subject session classifying images cats dogs. used slightly larger perturbation condition formly sampling condition type sessions randomly shufﬂing sequence identical trial counts condition sessions. number trials class group also constrained equal. similarly false condition number trials adversarially perturbed towards class class balanced session. reduce subjects using strategies based overall color brightness distinctions classes pre-ﬁltered dataset remove images showed obvious effect nature. signiﬁcantly pets group excluded images included large green lawns ﬁelds since almost cases photographs dogs. list images included experiment coarse class given appendix examples images condition figures supp. supp.. ﬁrst assess transferability constructed images test models included ensemble used generate adversarial examples. speciﬁcally test models adversarially trained inception model denoted inception resnet model. measure attack success models deﬁned fraction images misclassiﬁed model coming target classes. also report classiﬁcation accuracy table supp.. image condition accuracy models ensemble three groups pre-ﬁltered images ensure clean images correctly classiﬁed models ensemble. test models used accuracy image pets hazard vegetables groups high measured success targeted attacks. models attack success rate flip images images attack success model ensemble designed adversarial examples generation pipeline importantly attack success test models also generally high consistent previous work results demonstrate iterative fast gradient sign method large ensemble generate strong black-box adversarial attacks. figure adversarial images transfer humans. adding adversarial perturbations image able bias incorrect choices subjects make. plot shows probability choosing adversarially targeted class true image class choices subjects report estimated averaging responses subjects adversarial images cause mistakes either clean images images adversarial perturbation ﬂipped vertically applied. plot shows probability choosing true image class true class choices subjects report estimated averaging across subjects. accuracy signiﬁcantly less even clean images brief image presentation time. described section used false condition test whether adversarial perturbations inﬂuence incorrect classes subject chooses measured ability cause subject choose incorrect class rather correct class result could degrading image quality discarding information rather inﬂuencing speciﬁc decision subject. measured effectiveness changing perception subjects measuring rate subjects reported adversarially targeted class. adversarial perturbation completely ineffective would expect choice targeted class uncorrelated subject’s reported class. average rate subject chooses target class metric would false image perturbed class class group equal probability. figure shows probability choosing target class averaged across subjects three experiment groups. cases probability signiﬁcantly chance level demonstrates adversarial perturbations generated using cnns biased human perception towards targeted class. effect stronger hazard pets vegetables group. difference probability among class groups signiﬁcant also observed signiﬁcant difference mean response time class groups interestingly response time pattern across image groups inversely correlated perceptual bias pattern words subjects made quicker decisions hazard group pets group vegetables group. consistent subjects being conﬁdent decision adversarial perturbation successful biasing subjects perception. inverse correlation attack success response time observed within group well groups demonstrated able bias human perception target class true class image options subjects choose. show adversarial perturbations used cause subject choose incorrect class even though correct class available response. described section presented image flip adv. result simply imply signal noise ratio adversarial images lower clean images. already partially addressed objection false experiment results section additionally tested accuracy flip images. control case uses perturbations identical statistics vertical axis. however control breaks pixel-topixel correpsondence adversarial perturbation image. majority subjects lower accuracy condition flip condition averaging across trials effect signiﬁcant pets vegetables group less signiﬁcant hazard group results suggest direction adversarial image perturbation combination figure examples adversarial images. image time-limited humans frequently perceived similar spider image perceived snake. right accuracy adversarial image presented brieﬂy compared presented long time examples types manipulations performed adversarial attack. figures supp. supp. additional examples adversarial images. also figure supp. adversarial examples false condition. speciﬁc image perceptually relevant features human visual system uses classify objects. ﬁndings thus give evidence strong black adversarial attacks transfer cnns humans show remarkable similarities failure cases cnns human visual system. average response time cases longer cases relative conditions though result statistically signiﬁcant comparisons. trend remains predictive would seem contradict case presented false images interpretation false case transfer adversarial features humans accompanied conﬁdence whereas transfer accompanied less conﬁdence possibly competition adversarial true class features condition. adversarial examples designed fool human perception careful using subjective human perception understand work. caveat observed categories recurring modiﬁcation types illustrated figure disrupting object edges especially mid-frequency modulations perpendicular edge; enhancing edges increasing contrast creating texture boundaries; modifying texture; taking advantage dark regions image perceptual magnitude small perturbations larger. adversarial images paper transferred humans presented extremely short exposure times followed masking stimulus. conﬁguration little time single feedforward pass human visual pathway. upon longer consideration true class images remained obvious cases. likely explanation greater resistance humans show upon longer consideration additional time toprecurrent effects higher level cognitive mechanisms improves classiﬁcation accuracy robustness suggests classiﬁcation models sabour tang incorporate feedback recurrent dynamics prove robust adversarial examples human brain. another possibility brain-like models instead lead stronger adversarial examples transfer humans even longer consideration. development machine learning models generate fake images audio video appears realistic already source acute concern adversarial examples provide machine learning might plausibly used subtly manipulate humans. instance ensemble deep models might trained human ratings face trustworthiness. might possible generate adversarial perturbations enhance reduce human impressions trustworthiness perturbed images might used news reports political advertising. speculative risks involve possibility crafting sensory stimuli hack brain diverse ways larger effect example many animals observed susceptible supernormal stimuli. instance cuckoo chicks generate begging calls associated visual display causes birds species prefer feed cuckoo chick offspring adversarial examples seen form supernormal stimuli neural networks. worrying possibility supernormal stimuli designed inﬂuence human behavior emotions rather merely perceived class label image might also transfer machines humans. possible generate adversarial examples broader impact work would promising well worrying applications. instance perhaps image perturbations could designed improve saliency attentiveness performing tasks like trafﬁc control examination radiology images potentially tedious consequences inattention dire. user interface designers could image perturbations create naturally intuitive designs. study raises fundamental questions adversarial examples work models work brain works. adversarial attacks transfer cnns humans semantic representation similar human brain? instead transfer representation human brain similar inherent semantic representation naturally corresponds reality? interpretation issues clouded nature task studied visual object recognition difﬁcult deﬁne objectively correct answers. figure objectively objectively fools people thinking dog? future work could clarify interpretations studying human performance tasks objectively correct answers. example could study adversarial examples human machine question answering systems applied math questions. future research explores properties adversarial example cause transfer humans properties relate properties physical world great interest perhaps provide better understanding brain deep neural network models. grateful morcos bruno olshausen david sussillo hanlin tang santani teng daniel yamins useful discussions. also thank abolaﬁa simon kornblith katherine niru maheswaranathan catherine olsson david sussillo santani teng helpful feedback manuscript. thank google brain residents useful feedback work. also thank deanna chen leslie philips sally jesmonth phing turner melissa strader lily peng ricardo prada assistance experiment setup. biggio battista corona igino maiorca davide nelson blaine srndic nedim laskov pavel giacinto giorgio roli fabio. evasion attacks machine learning test time. machine learning knowledge discovery databases european conference ecml pkdd prague czech republic september proceedings part ./---- cadieu charles hong yamins daniel pinto nicolas ardila diego solomon ethan majaj najib dicarlo james deep neural networks rival representation primate cortex core visual object recognition. plos computational biology geirhos robert janssen david sch¨utt heiko rauber jonas bethge matthias wichmann felix comparing deep neural networks humans object recognition signal gets weaker. arxiv preprint arxiv. papernot nicolas huang sandy duan abbeel pieter clark jack. attacking machine learning adversarial examples https//blog.openai.com/ adversarial-example-research/. grosse kathrin papernot nicolas manoharan praveen backes michael mcdaniel patrick adesversarial examples malware detection. orics ---- https//doi.org/. /----_. jacob buckman aurko colin raffel goodfellow. thermometer encoding resist adversarial examples. international conference learning representations https//openreview. net/forum?id=ssu--cw. accepted poster. madry aleksander makelov aleksandar schmidt ludwig tsipras dimitris vladu adrian. towards deep learning models resistant adversarial attacks. arxiv preprint arxiv. szegedy christian zaremba wojciech sutskever ilya bruna joan erhan dumitru goodfellow fergus rob. intriguing properties neural networks. arxiv preprint arxiv. tang hanlin lotter bill schrimpf martin paredes caro josue ortega hardesty walter david kreiman gabriel. recurrent computations visual pattern completion. arxiv preprint arxiv. essen anderson information processing strategies pathways primate visual system. zornetzer davis mckenna introduction neural electronic networks diego academic press. papernot nicolas mcdaniel patrick somesh fredrikson matt celik berkay swami ananthram. limitations deep learning adversarial settings. corr abs/. papernot nicolas mcdaniel patrick goodfellow ian. transferability machine learning phenomena black-box attacks using adversarial samples. arxiv preprint arxiv. papernot nicolas mcdaniel patrick swami ananthram harang richard. crafting adversarial input sequences recurrent neural networks. military communications conference milcom ieee ieee papernot nicolas mcdaniel patrick somesh swami ananthram. distillation defense adversarial perturbations deep neural networks. security privacy ieee symposium ieee papernot nicolas mcdaniel patrick goodfellow somesh celik berkay swami ananthram. practical black-box attacks machine learning. proceedings asia conference computer communications security rajalingham rishi issa elias bashivan pouya kohitij schmidt kailyn dicarlo james largescale high-resolution comparison core visual object recognition behavior humans monkeys stateof-the-art deep artiﬁcial neural networks. biorxiv https//www.biorxiv. org/content/early////.. table supp.. adversarial examples transfer humans. number subjects reported correct class images condition lower mean accuracy compared mean accuracy image flip conditions. table supp.. accuracy models imagenet validation set. models trained imagenet retina layer pre-pended train data augmented rescaled images range model trained adversarial examples augmented data. first models models used adversarial training ensemble. last models models used test transferability adversarial examples. table supp.. accuracy ensemble used generate adversarial examples images different conditions. models trained imagenet retina layer appended train data augmented rescaled images range numbers triplet reﬂects accuracy images pets hazard vegetables groups respectively. table supp.. accuracy test models images different conditions. model trained clean adversarial images. numbers triplet accuracy pets hazard vegetables groups respectively. figure supp.. adversarial perturbation bias human perception conﬁdent probability choosing adversarially targeted class true image class choices subjects report estimated averaging responses subjects probability choosing targeted label computed binning trials within percentile reaction time ranges bias relative chance level signiﬁcant people reported decision quickly signiﬁcant reported decision slowly. finally target low-pass spatial frequency pixel used linearly interpolate pixel value corresponding pixel pass ﬁltered images described following algorithm additionally cropped xretinal width remove artifacts algorithm applying retinal blur image ximg input image image containing corresponding target lowpass frequency pixel computed norm spatial frequency position cutoff freqs list frequencies cutoffs low-pass ﬁltering cutoff freqs note per-pixel blurring performed using linear interpolation images low-pass ﬁltered fourier space transformation fast compute fully differentiable. calculate probability model assigns coarse class summed probabilities assigned individual classes within coarse class. starget individual labels target coarse class. sother individual labels target coarse class. |starget| |sother| since labels imagenet. coarse class variable ytarget target coarse class. compute probability model assigns models ensemble publicly available pretrained checkpoints others instances architectures speciﬁcally trained experiment imagenet retinal layer prepended. encourage invariance image intensity scaling augmented training batch another batch images rescaled range instead supplementary table supp. identiﬁes models used ensemble shows top- accuracies along holdout models used evaluation. .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ snake .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’ .jpeg’. cabbage", "year": 2018}