{"title": "Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM  Encoder-Decoder", "tag": ["cs.CL", "cs.AI", "cs.NE", "cs.SI"], "abstract": "We present Tweet2Vec, a novel method for generating general-purpose vector representation of tweets. The model learns tweet embeddings using character-level CNN-LSTM encoder-decoder. We trained our model on 3 million, randomly selected English-language tweets. The model was evaluated using two methods: tweet semantic similarity and tweet sentiment categorization, outperforming the previous state-of-the-art in both tasks. The evaluations demonstrate the power of the tweet embeddings generated by our model for various tweet categorization tasks. The vector representations generated by our model are generic, and hence can be applied to a variety of tasks. Though the model presented in this paper is trained on English-language tweets, the method presented can be used to learn tweet embeddings for different languages.", "text": "present tweetvec novel method generating generalpurpose vector representation tweets. model learns tweet embeddings using character-level cnn-lstm encoderdecoder. trained model million randomly selected english-language tweets. model evaluated using methods tweet semantic similarity tweet sentiment categorization outperforming previous state-ofthe-art tasks. evaluations demonstrate power tweet embeddings generated model various tweet categorization tasks. vector representations generated model generic hence applied variety tasks. though model presented paper trained english-language tweets method presented used learn tweet embeddings diﬀerent languages. recent years micro-blogging site twitter become major social media platform hundreds millions users. short noisy idiosyncratic nature tweets make standard information retrieval data mining methods ill-suited twitter. consequently ever growing body data mining literature focusing twitter. however works employ extensive feature engineering create ∗the ﬁrst authors contributed equally work. permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copyrights components work owned others author must honored. abstracting credit permitted. copy otherwise republish post servers redistribute lists requires prior speciﬁc permission and/or fee. request permissions permissionsacm.org. sigir july pisa italy copyright held owner/author. publication rights licensed acm. isbn ----//. http//dx.doi.org/./. paper present tweetvec method generating general-purpose vector representation tweets used classiﬁcation task. tweetvec removes need expansive feature engineering used train standard oﬀ-the-shelf classiﬁer tweetvec uses cnn-lstm encoderdecoder model operates character level learn generate vector representation tweets. method especially useful natural language processing tasks twitter particularly diﬃcult engineer features speech-act classiﬁcation stance detection several works generating embeddings words famously wordvec mikolov also number diﬀerent works encoder-decoder models based long short-term memory gated recurrent neural networks methods used mostly context machine translation. encoder maps sentence source language vector representation decoder conditions encoded vector translating target language. perhaps work related work mikolov extended wordvec model generate representations sentences however models function word level making ill-suited extremely noisy idiosyncratic nature tweets. character-level model hand better deal noise idiosyncrasies tweets. plan make model data used train publicly available used researchers work tweets. model operates character level generates vector representation tweets. encoder consists convolutional layers extract features characters lstm layer encode sequence features vector representation decoder consists lstm layers predict character time step output encoder. duced zhang model perform temporal convolutional temporal max-pooling operations computes one-dimensional convolution pooling functions respectively input output. given adapted model employs temporal convolution pooling operations tweets. character includes english alphabets numbers special characters unknown character. characters total given below gram character feature sliding window characters ﬁrst layer learns abstract textual features subsequent layers. convolution ﬁrst layer operates sliding windows character convolutions deeper layers deﬁned similar way. generally tweet feature layer generated ﬁlter applied across possible windows characters tweet produce feature map. output convolutional layer followed max-overtime pooling operation feature selects maximum value prominent feature current ﬁlter. apply ﬁlters layer. pooling size vary layer layer pooling operation shrinks size feature representation ﬁlters trivial features like unnecessary combination characters. window length number ﬁlters pooling size layer given table section brieﬂy describe lstm model given input sequence lstm computes hidden vector sequence output vector sequence time step output module controlled gates function previous hidden state input current time step forget gate input gate output gate gates collectively decide transitions current memory cell current hidden state lstm transition functions deﬁned follows tiplication. extent information memory cell discarded controlled controls extent information stored current memory cell output based memory cell lstm explicitly designed learning long-term dependencies therefore choose lstm convolution layer learn dependencies sequence extracted features. sequence-to-sequence generation tasks lstm deﬁnes distribution outputs sequentially predicts tokens using softmax function. cnn-lstm encoder-decoder model draws intuition sequence features extracted encoded vector representation using lstm embed meaning whole tweet. figure illustrates complete encoderdecoder model. input output model tweet represented matrix one-hot vector representation characters. procedure encoding decoding explained following section. extracts features character representation. one-dimensional convolution involves ﬁlter vector sliding sequence detecting features diﬀerent positions. successive higher-order window representations lstm since lstm extracts representation sequence input apply pooling convolution higher layers character-level model. encoding procedure summarized conv extracted feature matrix considered time-step lstm hidden representation time-step lstm operates conv along hidden vectors previous time-step produce embedding subsequent time-steps. vector output ﬁnal time-step encn used represent entire tweet. case size encn refers character time-step represents one-hot vector character time-step result softmax decoded tweet matrix eventually compared actual tweet synonymreplaced version tweet learning parameters model. trained cnn-lstm encoder-decoder model million randomly selected english-language tweets populated using data augmentation techniques useful controlling generalization error deep learning models. data augmentation context refers replicating tweet replacing words replicated tweets synonyms. synonyms obtained wordnet contains words grouped together basis meanings. involves selection replaceable words tweet number words replaced. probability number given geometric distribution paencoded representation actual tweet synonymreplaced version tweet augmented data. used training. also make sure tags replaced words completely diﬀerent actual words. regularization apply dropout mechanism penultimate layer. prevents co-adaptation hidden units randomly setting proportion hidden units zero decoder operates encoded representation layers lstms. initial time-step end-to-end output encoding procedure used original input ﬁrst lstm layer. last lstm decoder generates character sequentially combines previously generated hidden vectors size ﬁrst evaluation based semeval -task paraphrase semantic similarity twitter given pair tweets goal predict semantic equivalence binary yes/no judgement. dataset provided task contains tweet pairs training train logistic regression model features using dataset. cross-validation used tuning threshold classiﬁcation. contrast model methods used task largely based extensive feature engineering combination feature engineering semantic spaces. table shows performance model compared four models semeval competition also model trained using paragraphvec. model outperforms models without resorting extensive task-speciﬁc feature engineering. second evaluation based semeval -task twitter message polarity classiﬁcation given tweet task classify either positive negative neutral sentiment. size training test sets tweets tweets respectively last task ﬁrst extract vector representation tweets dataset using tweetvec train logistic regression classiﬁer using vector representations. even though three classes semeval task binary task. performance measured average f-score positive negative class. table shows performance model compared four models semeval competition paragraphvec. model outperforms models without resorting feature engineering. paper presented tweetvec novel method generating general-purpose vector representation tweets using character-level cnn-lstm encoder-decoder architecture. best knowledge ﬁrst attempt learning applying character-level tweet embeddings. character-level model deal noisy peculiar nature tweets better methods generate embeddings word level. model also robust synonyms help data augmentation technique using wordnet. vector representations generated model generic thus applied tasks diﬀerent nature. evaluated model using diﬀerent semeval tasks twitter semantic relatedness sentiment classiﬁcation. simple oﬀ-the-shelf logistic regression classiﬁers trained using vector representations generated model outperformed top-performing methods tasks without need extensive feature engineering. despite fact resource limitations tweetvec model trained relatively small also method outperformed paragraphvec extension wordvec handle sentences. small noteworthy illustration tweet embeddings best-suited deal noise idiosyncrasies tweets. future work plan extend method include augmentation data reordering words tweets make model robust word-order exploiting attention mechanism model improve alignment words tweets decoding could improve overall performance. dean. distributed representations words phrases compositionality. advances neural information processing systems pages rosenthal nakov kiritchenko mohammad", "year": 2016}