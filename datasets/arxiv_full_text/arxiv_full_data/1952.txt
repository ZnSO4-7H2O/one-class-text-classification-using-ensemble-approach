{"title": "Variational Gaussian Process Dynamical Systems", "tag": ["stat.ML", "cs.AI", "cs.CV", "math.PR", "60G15 (Primary), 62-09, 58E30", "G.3; G.1.2; I.2.6; I.5.4"], "abstract": "High dimensional time series are endemic in applications of machine learning such as robotics (sensor data), computational biology (gene expression data), vision (video sequences) and graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are required. In this paper we introduce the variational Gaussian process dynamical system. Our work builds on recent variational approximations for Gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space. The approach also allows for the appropriate dimensionality of the latent space to be automatically determined. We demonstrate the model on a human motion capture data set and a series of high resolution video sequences.", "text": "high dimensional time series endemic applications machine learning robotics computational biology vision graphics practical nonlinear probabilistic approaches data required. paper introduce variational gaussian process dynamical system. work builds recent variational approximations gaussian process latent variable models allow nonlinear dimensionality reduction simultaneously learning dynamical prior latent space. approach also allows appropriate dimensionality latent space automatically determined. demonstrate model human motion capture data series high resolution video sequences. nonlinear probabilistic modeling high dimensional time series data challenge machine learning community. standard approach simultaneously apply nonlinear dimensionality reduction data whilst governing latent space nonlinear temporal prior. difﬁculty approaches analytic marginalization latent space typically intractable. markov chain monte carlo approaches also problematic latent trajectories strongly correlated making efﬁcient sampling challenge. promising approach time series extend gaussian process latent variable model dynamical prior latent space seek maximum posteriori solution latent points extend models fully bayesian ﬁltering robotics setting. refer class dynamical models based gp-lvm gaussian process dynamical systems however approximation training models presents problems. firstly since latent variables marginalised parameters dynamical prior cannot optimized without risk overﬁtting. further dimensionality latent space cannot determined model adding dimensions always increases likelihood data. paper build recent developments variational approximations gaussian processes introduce variational gaussian process dynamical system latent variables approximately marginalized optimization rigorous lower bound marginal likelihood. well providing principled approach handling uncertainty latent space allows parameters latent dynamical process dimensionality latent space determined. approximation enables application model time series containing millions dimensions thousands time points. illustrate modeling human motion capture data high dimensional video sequences. model assume multivariate times series dataset tn}n data vector observed time especially interested cases high dimensional vector therefore assume exists dimensional manifold governs generation data. speciﬁcally temporal latent function governs intermediate hidden layer generating data feature data vector produced according latent mapping dimensional space dimension observation space inverse variance white gaussian noise. want make strong assumptions functional form latent functions instead would like infer fully bayesian non-parametric fashion using gaussian processes therefore assume multivariate gaussian process indexed time different multivariate gaussian process indexed write here individual components latent function taken independent sample paths drawn gaussian process covariance function similarly components independent draws gaussian process covariance function covariance functions parametrized parameters respectively play distinct roles model. precisely determines properties temporal latent function instance ornstein-uhlbeck covariance function yields gauss-markov process squared-exponential kernel gives rise smooth non-markovian processes. experiments focus squared exponential covariance function matern differentiable periodic covariance function used data exhibit strong periodicity. kernel functions take form covariance function determines properties latent mapping maps dimensional variable observed vector wish mapping non-linear smooth thus suitable choice squared exponential covariance function assumes different scale latent dimension. this variational bayesian formulation gp-lvm enables automatic relevance determination procedure i.e. allows bayesian training switch unnecessary dimensions driving values corresponding scales zero. matrix rn×d collectively denote observed data corresponds data point similarly matrix rn×d denote mapping latent variables i.e. associated observations usly rn×q store dimensional latent variables further refer columns matrices vectors given latent variables assume independence data features given time assume independence latent dimensions give bayesian inference using model poses huge computational challenge instance marginalization variables appear non-linearly inside kernel matrix troublesome. practical approaches considered marginalise seek solution next section describe efﬁcient variational approximations applied marginalize extending framework variational bayesian training difﬁculty bayesian approach propagating prior density nonlinear mapping. mapping gives expressive power model simultaneously renders associated marginal likelihood intractable. invoke variational bayesian methodology approximate integral. following standard procedure introduce variational distribution compute jensen’s lower bound logarithm denotes model’s parameters. however form lower bound problematic becauce appears non-linearly inside kernel matrix making integration difﬁcult. shown intractability removed applying data augmentation principle. precisely augment joint probability model including extra samples latent mapping known inducing points sample. inducing points evaluated pseudo-inputs rm×q. augmented joint probability density takes form posterior latent variables strong correlations taken full covariance matrix. optimization variational lower bound provides approximation true posterior augmented probability model difﬁcult term appearing replaced eventually cancels ﬁrst factor variational distribution marginalised analytically. given breaking logarithm obtain ﬁnal form lower bound terms involves data second involves prior. information regarding data point correlations captured term connection observations comes variational distribution. therefore ﬁrst term analytical solution derived maximized using gradient-based methods. however factorizing across data points yields variational parameters optimize. issue addressed next section. work described obtain less correlated variational parameters. speciﬁcally ﬁrst take derivatives variational bound w.r.t. zero stationary points n−dimensional vector. stationary conditions tell that since depends diagonal matrix reparametrize using n−dimensional diagonal matrix denoted then optimise parameters obtain original parameters using capture underlying commonality data. handle allowing different temporal latent function independent sequences latent variables corresponding sequence sets priori assumed independent since correspond separate sequences dropped conditioning time simplicity. factorisation leads block-diagonal structure time covariance matrix block corresponds sequenece. setting block observations generated corresponding according latent function governs mapping shared across sequences gaussian noise. supplementary material detailed derivation equations gradients. term variational parameters refer parameters although inducing points also variational algorithm models temporal evolution dynamical system. capable generating completely sequences reconstructing missing observations partially observed data. generating novel sequence given training data model requires time vector input computes density reconstruction partially observed data time stamp information additionally accompanied partially observed sequence rn∗×dp whole indices indicating present missing dimensions respectively reconstruct missing dimensions computing bayesian predictive distribution predictive densities also used estimators tasks like generative bayesian classiﬁcation. whilst time stamp information always provided next section drop dependence avoid notational clutter. predictions given test time points approximate predictive density need introduce underlying latent function values rn∗×d latent variables rn∗×q. write predictive density gaussian computed analytically since variational framework optimal setting also found gaussian term approximated gaussian variational distribution km∗k∗m. expectations taken w.r.t. calculated analytically denotes cross-covariance matrix training inducing inputs quantities calculated analytically finally since noisy version mean covariance computed β−in∗. analytically intractable. obtain approximation ﬁrstly need apply variational inference approximate gaussian distribution. requires optimisation variational lower bound accounts contribution partially observed data lower bound approximates true marginal likelihood exactly analogous form lower bound computed training data moreover variational optimisation requires deﬁnition variational distribution needs optimised fully correlated across optimisation approximation true posterior given marginal much faster less accurate method would decouple test training latent variables imposing factorisation used however current implementation. variational framework avoids typical cubic complexity gaussian processes allowing relatively large training sets further model scales linearly number dimensions speciﬁcally number dimensions matters performing calculations involving data matrix ﬁnal form lower bound matrix appears form precomputed. means that calculate substitute work instead matrix. practically speaking allows work data sets involving millions features. experiments model directly pixels quality video exploiting trick. consider different types high dimensional time series human motion capture data consisting different walks high resolution video sequences. experiments intended explore various properties model evaluate performance different tasks matlab source code repeating following experiments available on-line http//staffwww.dcs.shef.ac.uk/people/n.lawrence/vargplvm/. followed considering motion capture data walks runs taken subject motion capture database. treated motion independent sequence. data constructed preprocessed described results separate -dimensional frames split training sequences average length frames each. model jointly trained explained section walks runs i.e. algorithm learns common latent space motions. test time investigate ability model reconstruct test data previously unseen sequence given partial information test targets. tested providing dimensions correspond body subject providing correspond legs. compare results used approximations dynamical models nearest neighbour. also indirectly compare binary latent variable model used slightly different data preprocessing. assess performance using cumulative error joint scaled space deﬁned root mean square error angle space suggested model initialized nine latent dimensions. performed runs using matern covariance function dynamical prior using rbf. table variational gaussian process dynamical system considerably outperforms approaches. appropriate latent space dimensionality data automatically inferred models. model employed covariance govern dynamics retained four dimensions whereas model used matern kept three. latent dimensions completely switched parameters. best performance legs body reconstruction achieved vgpds model used matern covariance function respectively. table errors obtained motion capture dataset considering nearest neighbour angle space scaled space gplvm vgpds. body datasets preprocessed corresponding datasets corresponds error scaled space taylor error angle space. best error column bold. second experiments considered video sequences. sequences typically preprocessed modeling extract informative features reduce dimensionality problem. work directly pixel values demonstrate ability vgpds model data vast number features. also allows directly sample video learned model. firstly used model reconstruct partially observed frames test video sequences. ﬁrst video discussed gave partial information approximately pixels gave approximately pixels frame. mean squared error pixel measured compare k−nearest neighbour method datasets considered following ﬁrstly ‘missa’ dataset standard benchmark used image processing. -dimensional video showing woman talking frames. data challenging translations pixel space. also considered video dimensionality shows artiﬁcially created scene ocean waves well −dimensional video showing running frames. later approximately periodic nature containing several paces dog. ﬁrst videos used matern kernel respectively model dynamics interpolated reconstruct blocks frames chosen whole sequence. ‘dog’ dataset constructed compound kernel term employed capture divergence approximately periodic pattern. used model reconstruct last frames extrapolating beyond original video. seen table method outperformed cases. results also demonstrated visually ﬁgure reconstructed videos available supplementary material. figure demonstrate reconstruction achieved vgpds respectively challenging frame ‘missa’ video i.e. translation occurs. shows another example reconstruction achieved vgpds given partially observed image. depict reconstruction achieved frame ‘ocean’ dataset. finally demonstrate ability model automatically select latent dimensionality showing initial lengthscales kernel values obtained training ‘dog’ data set. second task used generative model create samples generate video sequence. effective ‘dog’ video training examples approximately periodic nature. model trained frames generated frames correspond next time points future. input given generation future frames time stamp vector results show smooth transition training test amongst test video frames. resulting video continuing sharp high quality. experiment demonstrates ability model reconstruct massively high dimensional images without blurring. frames result shown ﬁgure full video available supplementary material. introduced fully bayesian approach modeling dynamical systems probabilistic nonlinear dimensionality reduction. marginalizing latent space reconstructing data using gaussian processes results generic model capturing complex non-linear correlations even high dimensional method’s effectiveness demonstrated tasks; ﬁrstly modeling human motion capture data secondly reconstructing generating high dimensional video sequences. promising future direction follow would enhance formulation domain-speciﬁc knowledge encoded example sophisticated covariance functions data preprocessed. thus obtain application-oriented methods used tasks areas robotics computer vision ﬁnance. research partially supported university shefﬁeld moody endowment fund greek state scholarships foundation also thank colin litster life allowing video ﬁles datasets.", "year": 2011}