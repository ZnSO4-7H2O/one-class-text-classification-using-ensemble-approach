{"title": "An Analysis of Neural Language Modeling at Multiple Scales", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "Many of the leading approaches in language modeling introduce novel, complex and specialized architectures. We take existing state-of-the-art word level language models based on LSTMs and QRNNs and extend them to both larger vocabularies as well as character-level granularity. When properly tuned, LSTMs and QRNNs achieve state-of-the-art results on character-level (Penn Treebank, enwik8) and word-level (WikiText-103) datasets, respectively. Results are obtained in only 12 hours (WikiText-103) to 2 days (enwik8) using a single modern GPU.", "text": "constructed repeatedly selecting limited characters. introduces issues characterlevel however slower process word-level counterparts number tokens increases substantially this experience extreme issues vanishing gradients. tokens also less informative character-level word-level understand likely topics sentence cover discriminative words sufﬁcient. comparison character-level would require least half dozen. given distinction wordcharacter-level commonly trained vastly different architectures. vanilla long short term memory networks shown achieve state-of-the-art performance wordlevel hitherto considered insufﬁcient competitive character-level e.g. paper show given baseline model framework composed vanilla adaptive softmax capable modeling characterword-level tasks multiple scales data whilst achieving state-of-the-art results. further present additional analysis regarding comparison lstm qrnn architectures importance various hyperparameters model. conclude paper discussion concerning choice datasets model metrics. recent research shown well tuned lstm baseline outperform complex architectures task word-level language modeling model merity also aims well optimized components nvidia cudnn lstm highly parallel quasi-recurrent neural network allowing rapid convergence experimentation efﬁcient hardware usage. models shown achieve state-of-the-art results modest language modeling datasets application larger-scale word-level language modeling character-level language modeling successful. many leading approaches language modeling introduce novel complex specialized architectures. take existing state-of-the-art word level language models based lstms qrnns extend larger vocabularies well character-level granularity. properly tuned lstms qrnns achieve stateof-the-art results character-level word-level datasets respectively. results obtained hours days using single modern gpu. language modeling foundational tasks natural language processing. task involves predicting token sequence given preceding tokens. trained useful many applications including speech recognition machine translation natural language generation learning token embeddings general-purpose feature extractor downstream tasks language models operate various granularities tokens formed either words sub-words characters. underlying objective remains across sub-tasks unique beneﬁts challenges. practice word-level appear perform better downstream tasks compared character-level suffer increased computational cost large vocabulary sizes. even large vocabulary word-level still need replace infrequent words out-of-vocabulary tokens. character-level suffer problem potentially inﬁnite potential words require vast amounts time resources. this ensure model train matter days hours single modern achieve results competitive current state-of-the-art results. underlying architecture based upon model used merity model consists trainable embedding layer layers stacked recurrent neural network softmax classiﬁer. embedding softmax classiﬁer utilize tied weights decrease total parameter count well improve classiﬁcation accuracy rare words. experimental setup features various optimization regularization variants randomizedlength backpropagation time embedding dropout variational dropout activation regularization temporal activation regularization model framework shown achieve state-of-the-art results using either lstm qrnn based model nvidia quadro lstm works well terms task performance optimal ensuring high utilization. lstm sequential nature relying output previous timestep work begin current timestep. limits concurrency lstm size batch even result substantial cuda kernel overhead timestep must processed individually. qrnn attempts improve utilization recurrent neural networks ways uses convolutional layers processing input apply parallel across timesteps uses minimalist recurrent pooling function applies parallel across channels. convolutional layer rely output previous timestep input processing batched single matrix multiplication. recurrent pooling function sequential fast element-wise operation applied existing prepared inputs thus producing next overhead. investigations overhead applying dropout inputs signiﬁcant recurrent pooling function. composed approximately equal size lstm qrnn found qrnn based models overall faster epoch. word-level qrnns also found require fewer epochs converge achieved comparable state-of-the-art results word-level penn treebank wikitext- datasets. given sizes datasets intend process many millions tokens length potential speed beneﬁt qrnn interest especially results remain competitive lstm. truncated backpropagation time necessary long form continuous datasets. traditional relatively small bptt windows less word-level less character-level. granularities however gradient approximation made truncated bptt highly problematic. single sentence well longer truncated bptt window used charactereven wordlevel modeling preventing useful long term dependencies discovered. exacerbated long term dependencies split across paragraphs pages. longer sequence length used truncated bptt back long term dependencies explicitly formed potentially beneﬁting model’s accuracy. addition potential accuracy beneﬁt longer bptt windows improve utilization. models primary manner improve utilization increasing number examples batch shown highly effective tasks image classiﬁcation parallel nature qrnn qrnn process long sequences entirely parallel comparison sequential lstm. qrnn feature slow sequential hidden-to-hidden matrix multiplication timestep instead relying fast element-wise operator. large vocabulary sizes major issue large-scale word-level datasets result impractically slow models models impossible train lack memory address issues modiﬁed version adaptive softmax extended allow tied weights softmax approximation strategies exist adaptive softmax shown achieve results close full softmax whilst maintaining high efﬁciency. frequency reduce computation time. words split levels ﬁrst level second level forms clusters rare words. cluster representative token short-list determines overall probability assigned across words within cluster. zipf’s words require softmax short-list training reducing computation memory usage. clusters second level also constructed matrix multiplications required standard batch near optimal efﬁciency. original adaptive softmax implementation words within clusters second level feature reduced embedding size reducing embedding dimensionality minimizes total parameter count size softmax matrix multiplications. paper justiﬁes noting rare words unlikely need full embedding size ﬁdelity infrequently occur. weight tying cannot used memory optimization however. weight tying re-uses word vectors embedding targets softmax embedding dimensionality must equal words. discard adaptive softmax memory optimization order utilize tied weights. counter-intuitively weight tying actually reduces memory usage even adaptive softmax memory optimization weight tying allows halve memory usage re-using word vectors embedding layer. dataset originally composed wall street journal articles preprocessed dataset removes many features considered important capturing language modeling. oddly vocabulary words character-level dataset limited vocabulary used word level dataset. vastly simpliﬁes task character-level language modeling character transitions limited found within limited word level addition limited vocabulary character-level dataset lower case punctuation removed replaces numbers would considered important subtasks character-level language model cover. rare words encountered character word level datasets they’re replaced token <unk>. makes little sense character-level dataset doesn’t suffer vocabulary issues word level dataset would. <unk> token token characters sufﬁciently advanced model always output unk> upon seeing time angle brackets used. report results using bits character metric table model uses three-layered lstm bptt length embedding sizes hidden layers size regularize model using lstm dropout weight dropout weight decay. values tuned coarsely. full hyper-parameter values refer table train model using adam optimizer learning rate epochs reduce learning rate epochs model optimized convergence speed takes hours train nvidia volta gpu. lstm qrnn model beat current state-of-the-art though using parameters. note qrnn model uses layers achieve result lstm layers suggesting limited recurrent computation capacity qrnn issue. hutter prize wikipedia dataset also known enwik byte-level dataset consisting ﬁrst million bytes wikipedia dump. simplicity shall refer character-level dataset. within million bytes unique tokens. hutter prize launched focused around compressing enwik dataset efﬁciently possible. dump contains wide array content including english articles data hyperlinks special characters. data processed features many intricacies language modeling natural artiﬁcial would like models capture. experiments follow standard setup train validation test sets consist ﬁrst table statistics character-level penn treebank character-level enwik dataset wikitext-. vocabulary rate notes percentage tokens replaced token applicable character-level datasets. report results table model uses threelayered lstm bptt length embedding sizes hidden layers size explicit regularizer employ weight dropped lstm magnitude full hyper-parameter values refer table train model using adam optimizer default hyperparameter epochs reduce learning rate epochs dataset challenging character-level penn treebank multiple reasons. dataset times training tokens data processed maintaining complex character character transitions character-level penn treebank. potentially result this qrnn based model underperforms models comparable numbers parameters. limited recurrent computation capacity qrnn appears become major issue moving toward realistic character-level datasets. wikitext- wikitext- datasets introduced merity contain lightly preprocessed wikipedia articles retaining majority punctuation numbers. data contains approximately million words training million validation test sets. data contains larger training million words validation testing wikipedia articles relatively long focused single topic capturing utilizing long term dependencies models obtaining strong performance. underlying model used paper achieved state-ofthe-art language modeling word-level wikitext- datasets show that conjunction tied adaptive softmax achieve stateof-the-art perplexity wikitext- data using awd-qrnn framework; table qrnn lstm times slower reported table qrnn based model’s performance wordutilizing qrnn tied adaptive softmax able train state-of-the-art result nvidia volta hours. model used consisted -layered qrnn model embedding size nodes hidden layer. trained model using batch size bptt length using adam optimizer epochs reducing learning rate epoch avoid over-ﬁtting employ regularization strategies proposed including variational dropout random sequence lengths l-norm decay. values model hyperparameters tuned coarsely; full hyperparameter values refer table qrnns lstms operate sequential data vastly different ways. word-level language modeling qrnns allowed similar training generalization outcomes fraction lstm’s cost work however found qrnns less successful character-level tasks even substantial hyperparameter tuning. investigate this figure shows comparison wordcharacter-level tasks well lstm qrnn. experiment plot probability assigned correct token function token position model conditioned ground-truth labels token position. attempt similar situations wordcharacter-level datasets lstm qrnn models respond differently. character-level datasets model confusion highest beginning word stage little information confusion decreases characters word seen. closest analogy word-level datasets start sentence. proxy ﬁnding sentences token record model confusion point. similarly character-level dataset assumed confusion would highest early stage information gathered decreased tokens seen. surprisingly clearly behavior datasets quite different. word-level datasets gain clarity seeing additional information compared character-level datasets. also qrnn underperforming lstm penn treebank enwik character-level datasets. case word-level datasets. hypothesis character-level language modeling requires complex hidden-to-hidden transition. lstm full matrix multiplication timesteps able quickly adapt changing situations. qrnn hand suffers simpler hidden-tohidden transition function element-wise preventing full communication hidden units rnn. might also explain qrnns need deeper lstm achieve comparable results layer qrnn table qrnn perform complex interactions hidden state sending next qrnn layer. suggest state-of-the-art architectures characterword-level language modeling quite different readily transferable. given large number hyperparameters neural language models process tuning models datasets laborious expensive. section attempt determine relative importance various hyperparameters. this train models random hyperparameters regress validation perplexity using mse-based random forests random forest’s feature importance proxy hyperparameter importance. hyperparameters random place natural bounds prevent unrealistic models given awd-qrnn framework recommended hyperparameters. purpose dropout values bounded truncated bptt length bounded number layers bounded embedding hidden sizes bounded present results word-level task wikitext- data using awd-qrnn model figure models trained epochs table hyper-parameters wordcharacter-level language modeling experiments. training time noted epochs nvidia volta. dropout refers embedding hidden input output. figure understand models respond uncertainty different datasets visualize probability assigned correct token deﬁned starting point. character datasets token position refers characters within space delimited words. word level datasets token position refers words following word approximating start sentence. figure better understand character level datasets analyze average probability getting character different positions correct within word. words deﬁned characters spaces different length words different colors. include prediction ﬁnal space allows capturing uncertainty word bounded vocabulary ptbc model conﬁdent word’s ending model enwik. mikolov processed penn treebank data long central dataset experimenting language modeling poset fundamentally ﬂawed. noted earlier character-level dataset feature punctuation capitalization numbers important aspects would wish language model capture. limited vocabulary <unk> tokens also result substantial problems. figure compare level average surprise between models producing words given length trained penn treebank trained enwik. enwik noticeable drop predicting ﬁnal character compared penn treebank results. character transitions exist within penn treebank dataset contained within word vocabulary. models trained enwik dataset can’t equivalent conﬁdence words branch unpredictably unrestricted vocabulary. addition lack capitalization punctuation numerals removes many aspects language fundamental task character-level language modeling. potentially counter-intuitively recommend benchmark even though achieve state-of-theart results dataset. measure complexity computational cost training evaluating models number parameters model often reported compared. certain cases given model intended embedded hardware parameter counts appropriate metric. conversely parameter count unusually high require substantial resources parameter count still function actionable metric. general however model’s parameter count acts poor proxy complexity hardware requirements given model. model high parameter count runs quickly modest hardware would argue better model lower parameter count runs slowly requires resources. generally parameter counts discouraging proper modern hardware especially parameter counts historically motivated defunct hardware requirement. figure analyze relative importance hyperparameters deﬁning model using random forest approach word-level task smaller wikitext- data awdqrnn model. results show weight dropout hidden dropout embedding dropout impact performance number layers embedding hidden dimension sizes matters relatively less. similar results observed word level data set. adam initial learning rate reduced epochs results show weight dropout hidden dropout embedding dropout impact performance number layers sizes embedding hidden layers matter relatively less. experiments penn treebank data also yielded similar results. given results evident presence limited tuning resources educated choices made layer sizes dropout values ﬁnely tuned. figure plot joint inﬂuence heatmaps pairs parameters understand couplings bounds. experiment consider three pairs hyperparameters weight dropout hidden dropout weight dropout embedding dropout weight dropout embedding size plot perplexity obtained wikitext- experiment above form projected triangulated surface plot. results suggest strong coupling high-inﬂuence hyperparameters narrower bounds acceptable performance hidden dropout compared weight dropout. inﬂuence embedding size hyperparameter also evident heatmap; long embeddings small large inﬂuence hyperparameter performance drastic. finally plots also suggest educated guess dropout values lies range tuning weight dropout ﬁrst leaving rest hyperparameters estimates would good starting point ﬁne-tuning model performance further. figure heatmaps plotting joint inﬂuence three sets hyperparameters ﬁnal perplexity awd-qrnn language model trained wikitext- data set. results show ranges permissible values experiments strong coupling them. hidden dropout experiment results suggest narrower band acceptable values embedding dropout experiment suggests forgiving dependence long embedding dropout kept low. joint plot weight dropout embedding size suggests relative insensitivity embedding size long size small large. fast well tuned baselines important part research community. without baselines lose ability accurately measure progress time. extending existing state-of-the-art word level language model based lstms qrnns show well tuned baseline achieve state-of-the-art results character-level word-level datasets without relying complex specialized architectures. additionally perform empirical investigation learning network dynamics lstm qrnn cells across different language modeling tasks highlighting differences learned character word level models. finally present results shed light relative importance various hyperparameters neural language models. wikitext- data awd-qrnn model exhibited higher sensitivity hidden-to-hidden weight dropout input dropout terms relative insensitivity embedding hidden layer sizes. hope insight would useful practitioners intending tune similar models datasets. melis several state-of-the-art language model architectures re-evaluated using large-scale automatic black-box hyper-parameter tuning. results show standard lstm baseline properly regularized tuned outperform many recently proposed state-of-the-art models word-level language modeling tasks merity proposes weight-dropped lstm uses dropconnect hidden-to-hidden weights form recurrent regularization nt-asgd variant averaged stochastic gradient method. applying techniques standard lstm language modeling baseline achieve state-of-the-art results similarly melis recent results character-level datasets generally involve complex architectures however. mujika introduce fast-slow splits standard language model architecture fast changing cell slow changing cell. show slow rnn’s hidden state experiences less change fast allowing longer term dependency similar multiscale rnns. additional improvements model obtained dynamic evaluation mixture-of-softmaxes since goal evaluate underlying model employ strategies addition. gutmann hyv¨arinen noise-contrastive estimation estimation principle unnormalized statistical models. proceedings thirteenth international conference artiﬁcial intelligence statistics krueger maharaj kram´ar pezeshki ballas goyal bengio larochelle courville zoneout regularizing rnnss randomly preserving hidden activations. arxiv preprint arxiv. marcus marcinkiewicz macintyre bies ferguson katz schasberger penn treebank annotating predicate argument structure. proceedings workshop human language technology association computational linguistics melis dyer blunsom state evaluation neural language models. international conference learning representations https//openreview.net/forum? id=byjhutga-. merity keskar socher regularizing optimizing lstm language models. international conference learning representations https //openreview.net/forum?id=syygpptz. yang salakhutdinov cohen breaking softmax bottleneck high-rank laninternational conference learning guage model. representations https//arxiv.org/ abs/..", "year": 2018}