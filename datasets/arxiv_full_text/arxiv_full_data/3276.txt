{"title": "An Effective Semi-supervised Divisive Clustering Algorithm", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Nowadays, data are generated massively and rapidly from scientific fields as bioinformatics, neuroscience and astronomy to business and engineering fields. Cluster analysis, as one of the major data analysis tools, is therefore more significant than ever. We propose in this work an effective Semi-supervised Divisive Clustering algorithm (SDC). Data points are first organized by a minimal spanning tree. Next, this tree structure is transitioned to the in-tree structure, and then divided into sub-trees under the supervision of the labeled data, and in the end, all points in the sub-trees are directly associated with specific cluster centers. SDC is fully automatic, non-iterative, involving no free parameter, insensitive to noise, able to detect irregularly shaped cluster structures, applicable to the data sets of high dimensionality and different attributes. The power of SDC is demonstrated on several datasets.", "text": "laboratory neuroinformation ministry education china school life science technology university electronic science technology china chengdu china *corresponding author. email liyjuestc.edu.cn abstract nowadays data generated massively rapidly scientific fields bioinformatics neuroscience astronomy business engineering fields. cluster analysis major data analysis tools therefore significant ever. here propose effective semi-supervised divisive clustering algorithm data points first organized minimal spanning tree. next tree structure transitioned in-tree structure divided sub-trees supervision labeled data. points sub-trees directly associated specific cluster centers. fully automatic non-iterative involving free parameter insensitive noise able detect irregularly shaped cluster structures applicable data sets high dimensionality different attributes. power demonstrated several datasets. introduction cluster analysis fundamental computation area aiming classifying data points similarities. diverse experimental data ranging microarray gene expression data biology spectrum data astronomy require clustered signal meaningful correlation data. massive documents images internet also needed effectively organized promote efficiency search engines. clustering method k-means popular simplicity sensitive noise initialization thus limited lack reliability. hierarchical clustering simple intuitive thus widely used especially biology whereas needs large computation result variable similarity measures clusters. moreover cluster number methods needs prespecified determined threshold well-known algorithms either involve complex optimization postprocessing limited range applications distribution attribute data although affinity propagation much better performance k-means cluster number determined automatically good detecting nonspherical clusters recently effective clustering algorithms proposed together form pool clustering methods based in-tree structure involve free parameter. sdc. provided several labeled data points find underlying clustering structure four steps first connects data points minimal spanning tree which edge lengths minimum among possible connected graphs. then selects node ancestor node successively identifies offspring children nodes children’s children etc. nodes considered. parent node child node whereas tracing backward child node parent grandparent etc. common ancestor end. determining family relationship pair connected data points child node treated initial node points parent node i.e. undirected edge becomes directed consequently becomes directed tree precisely in-tree which along edge direction node path reach root node. third step structure divided several sub-trees removing undesired edges based simple divisive rules sub-tree must contain least labeled node; impure sub-trees divide. rules edges explored decreasing order lengths fig. five longest edges longest edges removed. consequently three sub-trees obtained still structure root node then exploration stops. finally searching along direction edges data points converge root nodes different sub-trees nodes connected root nodes assigned clusters. clusters containing labeled nodes category thus cluster number sure category number labeled nodes. whole process fully automatic. free parameter involved. cluster number needed specify advance. application constraints shape attribute dimensionality imposed test data. also degree insensitive noise especially outliers. although requires data points previously labeled advantage accompanied participation labeled data i.e. clustering result could reliable unsupervised clustering methods since result least consistent category labeled data. experiments figure provides synthetic data several clusters differing size shape density contaminated noise. nine points labeled different categories easy eyes spot clusters whereas previously hard computer virtue clustering algorithm help algorithm computer intelligent previously rather tedious scientists label mushrooms either poisonous edible one. however scientists could label small number remaining satisfactory prediction made algorithm actually close work experts. labeled data reliable prediction thanks in-tree structure obtained step exploration edge step though involving judgment whether time-consuming. suppose edge removed nodes first associated root nodes process step easy judge according divisive rules whether labeled node whether labeled nodes among offspring nodes root node. since process finding root nodes step extremely fast time cost step also negligible also applied cluster olivetti face dataset unlike mushroom dataset dataset much less instances features since data points -dimensional space distribution data points extremely sparse thus task clustering quite challenging however it’s troublesome algorithm provided several images labeled identical subjects different otherwise. figure shows case labeled images subject. consequently remaining images clustered exactly clusters wrong classifications better result recently reported though additional need labeled data. moreover like case clustering mushrooms labeled data result reliable clustering result since labeled data lead elaborate division. discussions inherits advantages structures semi-supervised learning strategy. structure long attracted people’s attention fascinating characteristics example minimal principle provides effective universal organize data points graph structure regardless distribution attribute data points need parameter. minimal principle also close conformity proximity principle gestalt perceptual organization sparse coding feature nervous system. structure great beauty order certainty efficiency power structure demonstrated recently proposed clustering algorithm moreover semi-supervised learning combined labeled unlabeled data attracted increasing interest shown superiority supervised unsupervised learning relies either labeled data unlabeled data natural transition structure effective semi-supervised cutting mechanism make simple fast effective reliable. presents interesting learning behavior computer materials first organized sparse form i.e. structure effectively transitioned sparse structure computer informed materials different structure starts evolve divide line known examples computer learns from reliable evolution consequently unknown materials explicitly associated known light which unknown also lighted words computer becomes knowledgeable. since proposed algorithm provides effective indirectly derive structure structure viewed member clustering family proposed references macqueen proceedings fifth berkeley symposium mathematical statistics probability neyman eds. vol. ward stat. assoc. eisen spellman brown botstein proc. natl. acad. sci. u.s.a. wunsch ieee trans. neural netw. malik ieee trans. pattern anal. mach. intell. fraley raftery stat. assoc. fukunaga hostetler ieee trans. inf. theory cheng ieee trans. pattern anal. mach. intell. frey dueck science rodriguez laio science yang preprint available http//arxiv.org/abs/. kruskal proc. math. soc. prim bell. syst. tech. gross yellen handbook graph theory jain pattern recognit. lett. http//archive.ics.uci.edu/ml/. samaria harter proceedings ieee workshop applications computer vision zahn ieee trans. comput. wertheimer source book gestalt psychology ellis olshausen nature chapelle schölkopf zien semi‐supervised learning fig. proposed algorithm works. flowchart sdc. illustration step structure constructed five nodes selected root node children nodes children’ children nodes node successively identified. child node points parent node. illustration divisive rules step sub-tree containing labeled nodes called pure otherwise impure one. synthetic dataset. labeled data denoted three triangles different colors represent different categories labeled data. respectively corresponds step fig. clustering synthetic data points. synthetic dataset. triangles denote place nine labeled data points locate. different colors triangles represent different categories labeled data. clustering result. points colors belong clusters. fig. clustering mushrooms. small portion mushroom mushroom featured characters. distance pair mushrooms measured number positions different elements. plot cluster number error rate versus number labeled mushrooms respectively. error rate ratio number falsely assigned instances unlabeled instances. plot time cost semi-supervised cutting versus number labeled data. points indicate means error bars indicate standard errors random tests. fig. clustering faces. four hundred grayscale faces subjects. shows varying images subject. treated -dimensional vector. distance pair faces measured euclidean distance. labeled faces. faces randomly selected subject. clustering assignments unlabeled images. fifteen images falsely assigned. plot cluster number error rate versus number labeled faces person respectively. points indicate means error bars indicate standard errors random tests.", "year": 2014}