{"title": "Deep Successor Reinforcement Learning", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms. There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map. The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards. The value function of a state can be computed as the inner product between the successor map and the reward weights. In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework. DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy. We show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains (MazeBase) and the Doom game engine.", "text": "learning robust value functions given observations rewards possible model-free model-based deep reinforcement learning algorithms. third alternative called successor representations decomposes value function components reward predictor successor map. successor represents expected future state occupancy given state reward predictor maps states scalar rewards. value function state computed inner product successor reward weights. paper present generalizes within end-to-end deep reinforcement learning framework. several appealing properties including increased sensitivity distal reward changes factorization reward world dynamics ability extract bottleneck states given successor maps trained random policy. show efﬁcacy approach diverse environments given pixel observations simple grid-world domains doom game engine. many learning problems involve inferring properties temporally extended sequences given objective function. instance reinforcement learning task policy maximizes expected future discounted rewards algorithms fall main classes model-free algorithms learn cached value functions directly sample trajectories model-based algorithms estimate transition reward functions values computed using tree-search dynamic programming. however third class based successor representation factors value function predictive representation reward function. speciﬁcally value function state expressed product vector expected discounted future state occupancies immediate reward successor states. representing value function using several appealing properties. combines computational efﬁciency comparable model-free algorithms ﬂexibility model-based algorithms. particular adapt quickly changes distal reward unlike model-free algorithms. paper also highlight feature less well-investigated ability extract bottleneck states successor representation random policy subgoals used within hierarchical framework. paper develop powerful function approximation algorithm architecture using deep neural network call deep successor reinforcement learning enables learning reward function sensory observations end-to-end training. figure model architecture consists feature branch takes images computes features successor branch computes msta possible action deep convolutional decoder produces input reconstruction linear regressor predict instantaneous rewards q-value function estimated taking inner-product reward weights consists sub-components reward feature learning component constructed deep neural network predicts intrinsic extrinsic rewards learn useful features observations; component constructed separate deep neural network estimates expected future feature occupancy conditioned current state averaged actions. value function estimated product factored representations. train sampling experience trajectories experience replay memory apply stochastic gradient descent optimize model parameters. avoid instability learning algorithm interleave training successor reward components. show efﬁcacy approach different domains learning solve goals grid-world domains using mazebase game engine learning navigate maze gather resource using doom game engine. show empirical convergence results several policy learning problems well sensitivity value estimator given distal reward changes. also demonstrate possibility extracting plausible subgoals hierarchical performing normalized-cuts used neuroscience model describing different cognitive phenomena. showed temporal context model model episodic memory fact estimating using temporal difference algorithm. introduced model based preplay rapid path planning region hippocampus. interpret attractor network low–dimensional space show network stimulated goal location generate path goal. suggested model tying problems navigation reward maximization brain. claimed brain’s spatial representations designed support reward maximization problem showed behavior place cells grid cells explained ﬁnding optimal spatial representation support based model proposed identifying reasonable subgoals spectral features work also discussed utilizing subgoal option discovery. also models similar applied rl-related domains. introduced model evaluating positions game model reminiscent predicts fate every position board instead overall game score. another reward-independent model universal option model proposed uses state occupancy function build general model options. proved option given reward function construct traditional option model. also work option discovery tabular setting recent work machado presented option discovery algorithm agent encouraged explore regions previously reach. however option discovery non-linear state approximations required still open problem. model also related literature value function approximation using deep neural networks. deep-q learning model variants successful learning q-value functions high-dimensional complex input states. background consider states actions reward function discount factor transition distribution given policy q-value function selecting action state deﬁned expected future discounted return large state spaces representing learning become intractable; hence appeal non-linear function approximation. represent state d-dimensional feature vector output deep neural network parameterized feature vector deﬁne feature-based expected future occupancy features denote msa. approximate another deep neural network parameterized also approximate immediate reward state linear function feature vector weight vector. since reward values sparse also train intrinsic reward predictor g˜θ. good intrinsic reward channel give dense feedback signal provide features preserve latent factors variations data putting pieces together q-value function approximated argmaxauα· parameter αprev denotes previously cached parameter value periodically essential stable q-learning function approximations learning weights reward approximation function following squared loss function parameter used obtaining shared feature representation reward prediction approximation. ideal good predictor immediate reward state good discriminator states. ﬁrst condition handled minimizing loss function however also need loss function help second condition. deep convolutional auto-encoder reconstruct images loss function. dense feedback signal interpreted intrinsic reward function. loss function stated optimizing respect parameters iteratively update given learn feature representation minimizing optimal iteration important ensure successor branch back-propagate gradients affect experience replay memory size store transitions apply stochastic gradient descent learning rate momentum discount factor exploration parameter annealed training progresses. algorithm highlights learning algorithm greater detail. learning policies given sparse delayed rewards signiﬁcant challenge current reinforcement learning algorithms. mainly inefﬁcient exploration schemes \u0001−greedy. existing methods like boltzmann exploration thomson sampling offer signiﬁcant improvements \u0001-greedy limited underlying models functioning level basic actions. hierarchical reinforcement learning algorithms options framework provide ﬂexible framework create temporal abstractions enable exploration different time-scales. agent learn options reach subgoals used intrinsic motivation. context hierarchical discuss framework subgoal extraction using structural aspects learned policy model. inspired previous work subgoal discovery probability sample random action otherwise choose argmaxauα execute obtain next state reward environment store transition randomly sample mini-batches perform gradient descent loss respect perform gradient descent respect anneal exploration variable state trajectories tabular learned generate plausible subgoal candidates. given random policy train convergence collect large number states {msa msnan}. following generate afﬁnity matrix given applying radial basis function pairwise entry diagonal matrix wij. second largest eigenvalue matrix gives approximation minimum normalized value partition states end-points plausible subgoal candidates provide path community state groups. given randomly sampled collect statistics many times particular state lies along cut. pick top-k states subgoals. experiments indicate possible extract useful subgoals dsr. section demonstrate properties approach mazebase grid-world environment doom game engine environments observations presented pixels agent. ﬁrst experiment show approach comparable goal-reaching tasks. next investigate effect modifying distal reward initial q-value. finally using normalized-cuts identify subgoals given successor representations environments. solving maze mazebase learn optimal policy maze shown figure using compare performance cost living moving water blocks reward value experiment discount rate learning rate anneal steps; furthermore training reward branch anneal number samples factor training episode. experiments prioritize reward training keeping database non-zero rewards sampling randomly replay buffer probability database. figure shows average trajectory rewards obtained episodes. plot suggests performs dqn. finding goal environment created rooms using vizdoom platform shown figure share network architecture case mazebase. agent spawned inside room explore three rooms. agent gets per-step penalty positive reward collecting item figure environments mazebase agent starts arbitrary location needs goal state. agent gets penalty per-step step water-block reaching goal state. model observes pixel images learning. doom using vizdoom engine agent starts room another room collect ammo sample screen-shots agent exploring maze. figure changing value distal reward train model learn optimal policy maze shown figure convergence change value distal reward update q-value optimal action origin order value function converge again model needs update linear weights given external rewards. decomposition value function immediate reward prediction allows rapidly adapt changes reward function. order probe this performed experiments measure adaptability value function distal reward changes. given grid-world figure train agent solve goal speciﬁed highlighted section without changing goal location change reward scalar value upon reaching goal hypothesis sr-based value decomposition value estimate converge change updating reward weights shown figure conﬁrm able quickly adapt value function updating extracting subgoals following section also extract subgoals collect running random policy mazebase vizdoom. learning update reconstruction branch immediate reward state zero shown figures subgoal extraction scheme able capture useful subgoals clusters environment reasonable segments. scheme periodically within hierarchical reinforcement learning framework exploration. inherent limitation approach random policy subgoal candidates often quite noisy. future work address limitation provide statistically robust ways extract plausible candidates. additionally subgoal extraction algorithm non-parametric handle ﬂexible number subgoals. figure subgoal extraction grid-world given random policy train convergence collect large number sample transitions corresponding successor representations described section apply normalized cut-based algorithm obtain partition environment well bottleneck states subgoals states separate different partitions environments normalized-cut algorithm. approach able reasonable subgoal candidates. partitions environment reﬂect latent structure environment. presented novel deep reinforcement learning framework learn goal-directed behavior given sensory observations. estimates value function taking inner product immediate reward predictions. factorization value function gives rise several appealing properties existing deep reinforcement learning methods—namely increased sensitivity value function distal reward changes possibility extracting subgoals random policy. future work plan combine hierarchical reinforcement learning. learning goaldirected behavior sparse rewards fundamental challenge existing reinforcement learning algorithms. enable efﬁcient exploration periodically extracting subgoals learning policies satisfy intrinsic goals subsequently learning hierarchical policy subgoals options framework major issues learning discriminative features. order scale approach expressive environments crucial combine various deep generative self-supervised models figure subgoal extraction doom subgoals extracted using normalized cut-based algorithm samples collected based random policy. subgoals mostly correspond rooms’ entrances common area rooms. random policy sometimes observe high variance subgoal quality. future work address robust statistical techniques obtain subgoals well non-parametric approaches obtaining ﬂexible number subgoals. approach. addition subgoals using extracting intrinsic motivation measures improvements predictive world model mutual information worth pursuing. matthew botvinick weinstein. model-based hierarchical reinforcement learning human action control. philosophical transactions royal society london biological sciences dane corneil wulfram gerstner. attractor network dynamics enable preplay rapid path planning maze–like environments. advances neural information processing systems pages eslami nicolas heess theophane weber yuval tassa koray kavukcuoglu geoffrey hinton. attend infer repeat fast scene understanding generative models. arxiv preprint arxiv. michał kempka marek wydmuch grzegorz runc jakub toczek wojciech ja´skowski. vizdoom doom-based research platform visual reinforcement learning. arxiv preprint arxiv. tejas kulkarni karthik narasimhan ardavan saeedi joshua tenenbaum. hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. arxiv preprint arxiv. shie mannor ishai menache amit hoze klein. dynamic abstraction reinforcement learning clustering. proceedings twenty-ﬁrst international conference machine learning page volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. arxiv preprint arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature shakir mohamed danilo jimenez rezende. variational information maximisation intrinsically motivated reinforcement learning. advances neural information processing systems pages arun nair praveen srinivasan blackwell cagdas alcicek rory fearon alessandro maria vedavyas panneershelvam mustafa suleyman charles beattie stig petersen massively parallel methods deep reinforcement learning. arxiv preprint arxiv. schaul daniel horgan karol gregor david silver. universal value function approximators. proceedings international conference machine learning pages nicol schraudolph peter dayan terrence sejnowski. temporal difference learning position evaluation game advances neural information processing systems pages david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature özgür ¸sim¸sek alicia wolfe andrew barto. identifying useful subgoals reinforcement learning local graph partitioning. proceedings international conference machine learning pages", "year": 2016}