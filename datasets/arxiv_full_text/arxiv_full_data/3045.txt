{"title": "Projection Based Weight Normalization for Deep Neural Networks", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "Optimizing deep neural networks (DNNs) often suffers from the ill-conditioned problem. We observe that the scaling-based weight space symmetry property in rectified nonlinear network will cause this negative effect. Therefore, we propose to constrain the incoming weights of each neuron to be unit-norm, which is formulated as an optimization problem over Oblique manifold. A simple yet efficient method referred to as projection based weight normalization (PBWN) is also developed to solve this problem. PBWN executes standard gradient updates, followed by projecting the updated weight back to Oblique manifold. This proposed method has the property of regularization and collaborates well with the commonly used batch normalization technique. We conduct comprehensive experiments on several widely-used image datasets including CIFAR-10, CIFAR-100, SVHN and ImageNet for supervised learning over the state-of-the-art convolutional neural networks, such as Inception, VGG and residual networks. The results show that our method is able to improve the performance of DNNs with different architectures consistently. We also apply our method to Ladder network for semi-supervised learning on permutation invariant MNIST dataset, and our method outperforms the state-of-the-art methods: we obtain test errors as 2.52%, 1.06%, and 0.91% with only 20, 50, and 100 labeled samples, respectively.", "text": "optimizing deep neural networks often suffers ill-conditioned problem. observe scaling-based weight space symmetry property rectiﬁed nonlinear network cause negative effect. therefore propose constrain incoming weights neuron unit-norm formulated optimization problem oblique manifold. simple efﬁcient method referred projection based weight normalization also developed solve problem. pbwn executes standard gradient updates followed projecting updated weight back oblique manifold. proposed method property regularization collaborates well commonly used batch normalization technique. conduct comprehensive experiments several widely-used image datasets including cifar- cifar- svhn imagenet supervised learning state-of-the-art convolutional neural networks inception residual networks. results show method able improve performance dnns different architectures consistently. also apply method ladder network semi-supervised learning permutation invariant mnist dataset method outperforms state-of-the-art methods obtain test errors labeled samples respectively. deep neural networks achieved great success across broad range domains computer vision speech processing natural language processing deep complex structure provides powerful representation capacity appealing advantages learning feature hierarchies also makes learning difﬁcult. literatures various heuristics optimization algorithms studied order improve efﬁciency training including weight initialization normalization internal activation sophistic optimization methods despite progress training deep neural networks ensuring satisfactory performance still considerably open problem non-convexity nature ill-conditioned problems. deep neural networks large number local minima fact usually suffer model identiﬁability problem. model called identiﬁable sufﬁciently large training rule setting model’s parameters neural networks often identiﬁable obtain equivalent models swapping weights other called weight space symmetry addition commonly used rectiﬁed nonlinear maxout network also construct equivalent models scaling incoming weight neuron factor scaling outgoing weight refer scaling-based weight space symmetry issues imply extremely large even uncountably inﬁnite amount local minima neural network. although still remains open question whether difﬁculty optimizing neural networks originates local minima observe scaling-based weight space symmetry cause hessian matrix ill-conditioned deemed prominent challenge optimization alleviate negative effect scaling-based weight space symmetry propose constrain incoming weights neuron unit-norm. simple strategy ensure weight matrix layer almost magnitude. besides keep norm back-propagation information linear transformations. training neural networks constraints formulated optimization problem oblique manifold address optimization problem propose projection based weight normalization method improve performance efﬁciency. method executes standard gradient updates followed projecting updated weight back oblique manifold. point proposed method property regularization weight decay viewed regularization term adaptive regularization factors. show method implicitly adjusts learning rate ensures unit-norm characteristic incoming weight neuron condition batch normalization employed networks. conduct comprehensive experiments several widely-used image datasets including cifar- cifar svhn imagenet supervised learning state-of-the-art convolutional neural networks inception residual network experimental results show method improve performance deep neural networks different architectures without revising experimental setups. also consider semi-supervised learning permutation invariant mnist dataset applying method ladder network method outperforms state-of-the-art results task achieve test errors labeled training samples respectively. code reproduce experimental results available https//github.com/huangleibuaa/normprojection. contributions below. propose projection based weight normalization method serves simple effective efﬁcient solution optimization oblique manifold dnns. analyze pbwn property regularization weight decay also collaborates well commonly used batch normalization technique. apply pbwn state-of-the-art cnns large scale datasets improve performance networks different architectures without revising experimental setups. besides additional computation cost introduced pbwn negligible. consider learning problem training data using feed-forward neural network l-layers refers input corresponding target. network parameterized weights biases layer composed linear transformation element-wise nonlinearity paper mainly focus rectiﬁer activation function property drop biases simplifying discussion description. given loss function measures mismatch desired output predicted output train neural network minimizing empirical loss follows formulation gradient information dominates tuning network parameters. weight updating rule layer iteration usually designed based stochastic gradient descent consider simple two-layer linear model neuron layer abuse rectiﬁed nonlinear layer simplifying discussion without loss generalization. layers deﬁne loss function assume magnitude. based scaling-based weight space symmetry consider another two-layer linear model parameterized parameterization still model output input models back-propagated gradient information fact based simple algebra derivation easy obtain phenomenon implies different magnitude gradient information inversely different terms magnitude. subsequently becomes larger likely hessian matrix ill-conditioned shown figure figure illustrative example scaling-based weight space symmetry cause ill-conditioned problem. error landscape magnitude; error landscape scaling factor respectively different magnitudes. formulation unit-norm constraint relieve negative effect scaling-based weight space symmetry paper propose constrain incoming weights neuron unit-norm. speciﬁcally reformulate optimization problem eqn. follows ddiag denotes operation extracts diagonal elements matrix sets off-diagonal elements drop index simplifying denotation. indeed constraint weight matrix rn×p layer deﬁnes embedded submanifold rn×p called oblique manifold riemannian optimization method provides good solution problem however also introduces extra non-ignorable computational cost. instance calculate riemannian gradient subtracting extra term ddiagw project weight tangent space back oblique manifold multiplying iteration. possible reduce computational cost without performance loss meanwhile guarantee solution satisfying unitnorm constraints? batch normalization popular technique stabilizes distribution activations layer thus accelerates convergence. works normalizing preactivation neuron zero-mean unit-variance mini-batch extra learnable scale bias parameters recommended restore representation power networks. speciﬁcally neuron batch normalization formulation follows norm projection operation eqn. viewed scaling therefore combined batch normalization norm projection also keep output training rectiﬁer network therefore ensure norm projection drop learned information weight matrix even thought execute norm projection outside gradient descent steps. another interesting point norm projection eventually affects backpropagation information combined batch normalization. batch normalization owns property means dominant term compared eqn. also observe fact experiments. therefore recommend simply using ordinary gradient solve problem much less computation cost follows here eqn. works projecting updated weight back oblique manifold thus call eration norm projection. indeed operation combining eqn. equivalent retractor operation note weight updating based ordinary gradient eqn. norm projection operation make updating along negative gradient direction subsequently disturbs gradient information. disturbance eventually harm learning shown figure observe using ordinary gradient nearly identical training loss curve using riemannian gradient. efﬁcient computation also execute norm projection operation eqn. interval rather iteration. empirically trick work well practice. pointed executing norm projection operation large method lose information learned weight matrix also suffer instability norm projection shown figure initial phase executing norm projection large interval results sudden increase loss. mainly change scale ﬁlter results predictions different input. fortunately remedy issue combing batch normalization discuss next subsection. summarize show projection based weight normalization framework algorithm extra norm projection executed interval. note proposed riemannian optimization oblique manifold described viewed speciﬁc instance framework conditions riemannian gradient steepest gradient descent interval figure illustrative experiment mnist using multi-layer perceptron structure layer sizes ---. train model stochastic gradient descent mini-batch size search learning rate report best performance method ‘normal’ indicates original network. ‘pbwn-riem’ ‘pbwn’ refers projection based weight normalization methods respectively apply norm projection iteration based riemannian ordinary gradient ‘pbwn-tt performs norm projection every iterations based ordinary gradient. summarize combined batch normalization rectiﬁer network norm projection operation enjoys following characteristics guaranteeing incoming weight unit-norm; keeping output operation training; implicitly adjusting learning rate factor characteristics make projection based weight normalization stable optimization process. projection based weight normalization strong connections weight decay weight decay simple effective technique regularize neural networks. update formulation weight decay constant weight decay factor. indeed weight decay considered solution loss function appended regularization term perspective treat weight decay soft constraint method hard constraint neuron’s incoming weight another perspective weight updating formulation method based eqn. similar weight updating form weight decay. particularly weight-speciﬁc decay rate also weight-speciﬁc learning rate. therefore solution optimization oblique manifold viewed regularization method adaptive regularization factors. eventually weight matrix free degree computational cost let’s consider standard linear layer rn×p mini-batch input data size iteration computational cost standard linear layer flops. experiments adopt random weight initialization default described unless specify weight initialization methods. refer original networks ‘normal’. projection based weight normalization methods evaluate three setups follows ‘pbwn-riem’ performing norm projection iteration based riemannian gradients; ‘pbwn’ performing norm projection iteration based ordinary gradients; ‘pbwnepoch’ performing norm projection epoch based ordinary gradients. also choose another related work named weight normalization baseline. ﬁrst evaluate method inception architecture equipped batch normalization inserted convolution layer. models trained mini-batch size considering memory constraints gpu. adopt momentum weight decay regarding learning rate annealing start learning rate divide epochs terminate training epochs empirically. results also obtained averaging random seeds. figure show training error respect epochs cifar- cifar- dataset respectively table lists test errors. figure observe model converge signiﬁcantly faster baselines. particularly ‘pbwn-riem’ ‘pbwn’ nearly identical training curves means need calculate reimannian gradient performing norm projection inception network test performance table demonstrates methods also achieve signiﬁcant improvements baselines mainly owing desirable regularization ability. following part evaluated method cifar datasets stateof-the-art cnns including inception residual network cifar- consists training images test images classes cifar- classes. input image consists pixels. dataset preprocessed described subtracting means dividing variance channel. follow simple data augmentation pixels padded side investigate performance vgg-e architecture global average pooling batch normalization inserted convolution layer. initialize model he-init models trained mini-batch size momentum weight decay here start learning rate divide epochs terminate training epochs empirically. averaged test errors training shown table easily conclusion inception architecture model signiﬁcantly boost test performance baselines. table comparison test errors residual network variational layers cifar- results averaged random seeds. ‘res-l’ indicates residual network layers ‘baseline*’ indicates results reported res- reported res- reported runs. experiment apply method famous residual network architecture follow exactly experimental protocol described adopt publicly available torch implementation residual network. table respectively show results different methods cifar- cifar- using residual network architecture varied depths methods consistently achieve better performance using different depths. especially depth increasing methods obtain performance gains. besides observe signiﬁcant difference among performance different norm projection methods using different gradient information updating intervals. indeed ‘pbwn-epoch’ works best cases. indicates effectiveness efﬁcient model executing norm projection interval meanwhile without performance degeneration. conducted tesla gpu. results reported table ‘pbwn-epoch’ costs almost time ‘normal’ architectures means introduce extra time cost practice analyzed previous sections. ‘pbwn’ also requires little extra time cost ‘pbwn-riem’ needs non-ignorable extra time. results show norm projection solution faithfully improve efﬁciency optimization unit-norm constraints meanwhile achieve satisfying performance. svhn dataset comprehensively study performance proposed method consider larger datasets svhn digit recognition. svhn consists color images house numbers collected google street view. includes train images test images. besides appended extra augmented images training set. experiment based wide residual network achieves state-of-the-art results dataset. wrn- does follow experimental setting provided input images divided ensure range; training used momentum dampening weight decay mini-batch size initial learning rate dropped epochs total epochs complete. dropout here apply method ‘pbwn-epoch’ wrn-- architecture namely execute norm projection epoch considering time cost large dataset. results shown table comparing several state-of-the-art methods literature. easily achieves best performance compared baselines method improves simply executing efﬁcient norm projection operation epoch. imagenet validate effectiveness method large-scale dataset employ imagenet consisting classes train models given ofﬁcial training images evaluated validation images. evaluate classiﬁcation performance based top- top- error. note part mainly focus whether proposed method able handle diverse large-scale datasets provide relative beneﬁt conventional architecture rather achieving state-of-the-art results. layers residual network pre-activation version perform classiﬁcation task. stochastic gradient descent applied mini-batch size momentum weight decay exponential decay initial learning rate training epochs. initial learning rate select best results shown table ‘pbwnepoch’ achieves lower test errors compared original residual network pre-activation residual networks. section applied proposed method semisupervised learning tasks ladder network permutation invariant mnist dataset. three semisupervised classiﬁcation tasks considered respectively labeled examples. labeled examples sampled randomly balanced number class. table comparison test errors semi-supervised setup permutation invariant mnist dataset. show test error given number samples={ ladder* indicates implementation ladder network catgan skip deep generative model auxiliary deep generative model virtual adversarial ladder ladder+amlp feature matching triple-gan ladder* ladder+pbwn adopt setup described layer sizes model ------; models trained adam optimization respectively mini-batch size models trained iterations initial learning rate followed iterations decaying linearly execute simple hyper-parameters search learning rate weight decay case experiments random seeds. table report results ladder based implementation ‘pbwn’ performs norm projection iteration. table method signiﬁcantly improves performance original ladder network achieves state-of-the-art results tasks labeled examples. especially labeled examples method achieves test error. conjecture appealing results method mainly stemming well regularization ability. exist number methods regularize neural networks bounding magnitude weights. commonly used method weight decay considered solution loss function appended regularization term squared l-norm weight vector. max-norm constrains norm incoming weights hidden unit bounded constant. viewed constrained optimization problem ball parameter space method addresses optimization problem oblique manifold. path normalization follows idea max-norm bounds product weights along path input output nodes also viewed regularizer weight decay weight normalization decouples length incoming weight vector directions. extra scaling parameter considered weight normalization viewed normalizing incoming weight. however solves problem re-parameterization guarantee whether conditioning hessian matrix proxy parameter improved; method performs normalization projection optimization original parameter space ensures improvement conditioning hessian matrix shown figure experimentally show method outperforms weight normalization terms effectiveness computation efﬁciency. large amount work introducing orthogonality weight matrix deep neural networks address gradient vanish explosion problem. solving problem orthogonality constraint usually limited hidden-to-hidden transformation recurrent neural networks work also consider orthogonal weight matrix feed forward neural networks solutions introduce expensive computation costs. normalizing activations deep neural networks also studied. batch normalization famous effective technique normalize activations. standardizes pre-activation neuron zero-mean unit-variance mini-batch. layer normalization computed statics zero-mean unit-variance hidden units layers targeting scenario size mini-batch limited. division normalization proposed uniﬁed view normalization includes batch layer normalization special cases. methods focus normalizing activations data dependent normalization method normalizing weights therefore data independent normalization. based fact method orthogonal methods provide analysis experimental results showing method improve performance batch normalization combining together. concurrent work propose optimize grassmann manifold aiming improve performance neural networks equipped batch normalization differences work work aspects traditional riemannian optimization method solve constraint optimization problem introduce non-trivial commutation cost; consider riemannian optimization method proposed general efﬁcient projection based weight normalization framework introduces negligible extra computation cost; requires gradient clipping technique make optimization stable also needs tailored revision momentum. contrary method general without requiring extra tailored revision also collaborate well techniques training neural networks.", "year": 2017}