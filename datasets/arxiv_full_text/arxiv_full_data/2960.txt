{"title": "Modeling Latent Variable Uncertainty for Loss-based Learning", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "We consider the problem of parameter estimation using weakly supervised datasets, where a training sample consists of the input and a partially specified annotation, which we refer to as the output. The missing information in the annotation is modeled using latent variables. Previous methods overburden a single distribution with two separate tasks: (i) modeling the uncertainty in the latent variables during training; and (ii) making accurate predictions for the output and the latent variables during testing. We propose a novel framework that separates the demands of the two tasks using two distributions: (i) a conditional distribution to model the uncertainty of the latent variables for a given input-output pair; and (ii) a delta distribution to predict the output and the latent variables for a given input. During learning, we encourage agreement between the two distributions by minimizing a loss-based dissimilarity coefficient. Our approach generalizes latent SVM in two important ways: (i) it models the uncertainty over latent variables instead of relying on a pointwise estimate; and (ii) it allows the use of loss functions that depend on latent variables, which greatly increases its applicability. We demonstrate the efficacy of our approach on two challenging problems---object detection and action detection---using publicly available datasets.", "text": "pawan kumar center visual computing ´ecole centrale paris france equipe galen inria saclay ˆile-de-france france universit´e paris-est ligm ´ecole ponts paristech france packer consider problem parameter estimation using weakly supervised datasets training sample consists input partially speciﬁed annotation refer output. missing information annotation modeled using latent variables. previous methods overburden single distribution separate tasks modeling uncertainty latent variables training; making accurate predictions output latent variables testing. propose novel framework separates demands tasks using distributions conditional distribution model uncertainty latent variables given input-output pair; delta distribution predict output latent variables given input. learning encourage agreement distributions minimizing loss-based dissimilarity coeﬃcient. approach generalizes latent important ways models uncertainty latent variables instead relying pointwise estimate; allows loss functions depend latent variables greatly increases applicability. demonstrate eﬃcacy approach challenging problems—object detection action detection—using publicly available datasets. example computer vision wish learn model detecting object ‘deer’ images location deer unknown therefore treated latent variable. computational medicine wish diagnose patient based observed symptoms well unknown factors family’s medical history represented using latent variables. typically employs single distribution three types variables observed variables input whose values known training testing; unobserved variables output whose values known training; unknown latent variables. setting natural framework would model uncertainty value latent variables learn marginalizing however approach unsuited applications require accurate prediction latent variables test time. example deer detection application would like infer whether image contains deer also exact location deer. alternately delta distribution provides pointwise estimate output latent variables however discarding uncertainty latent variables make approach prone error noise argument illustrates deﬁciency using single joint distribution output latent variables address separate tasks modeling uncertainty latent variables training; making accurate predictions testing. address deﬁciency proposing novel framework consists distributions conditional distribution model uncertainty latent variables given input-output pair; delta distribution predict output latent variables given input. order learn distributions training dataset build intuition agree other output predicted delta distribution match ground-truth output; latent variables predicted delta distribution high probability according conditional distribution. limited representational power model able achieve complete agreement order make distributions similar possible minimize regularized upper bound loss-based dissimilarity measure distributions. unlike previous loss-based learning frameworks lvms lsvm consider general loss function depends output also latent variables. loss function essential solving problems require accurate prediction latent variables restricting form loss function framework greatly enhances applicability loss-based learning latent variables. fact framework viewed strict generalization lsvm sense that loss function independent true value latent variables reduces lsvm. throughout paper assume latent variables helpful predicting correct output sample. example want distinguish between images deers elephants would expect background clutter similar appearance categories accurate object localization essential correct prediction. cases assumption hold. example images deers cars could distinguished detecting roads objects commonly found urban environments. however even cases able learn detect object providing fully supervised annotations small fraction training images would help guide learner towards correct object locations weakly supervised training images. elegant probabilistic interpretation maximizing likelihood ground-truth output marginalizes latent variables makes unsuited problems require accurate prediction latent variables. furthermore employ user-speciﬁed loss function captures user’s assessment quality solution. approach lsvm recently proposed generalization called maxmargin min-entropy models parameters lsvm learned minimizing regularized upper bound training loss. however loss function restricted independent true value latent variables. loss functions useful fact successfully employed practice cannot model several important problems including employed experiments—object detection action detection. contrast framework allows general loss function. section show that loss functions independent true value latent variable framework reduces lsvm. earlier work proposed iterative lsvm strategy using general loss function. section show ilsvm corresponds using delta functions model conditional distribution latent variables given input output. experiments show using non-delta conditional distribution signiﬁcantly outperforms ilsvm. denote parameters delta distribution predicts output latent variables given input parameters conditional distribution latent variables given input output denoted rao’s dissimilarity coeﬃcient. provide brief description dissimilarity measure used framework ﬁrst introduced given loss function diversity coeﬃcient distributions deﬁned expected loss samples drawn randomly distributions respectively h−βh−h note ﬁxed order ensure dissimilarity coeﬃcient symmetric however dissimilarity coeﬃcients necessarily symmetric hence general version shown equation showed formulation generalizes commonly used dissimilarity coeﬃcients mahalanobis distance gini-simpson index. refer reader details. wish address separate tasks accurately model distribution latent variables given input-output pair; accurately predict output latent variables given input instead addressing tasks single distribution previous works deﬁne separate distributions focused single task. mentioned earlier since true value latent variables unknown would like model uncertainty values. deﬁne separate conditional distribution parameterized partition function ensures distribution sums joint feature vector input output latent variables feature vector different joint feature vector used specify delta distribution again log-linear distribution used simplify description. framework valid general form distribution using conditional distribution also specify joint distribution follows given dataset loss function propose learn parameters minimizes corresponding dissimilarity coeﬃcient training samples. delving details give broad overview objective function. ﬁxed predicted output similar ground-truth output objective encourages probability corresponding latent variables pθ|si) similar latent variables high. predicted output dissimilar ground-truth output objective encourages diversity coeﬃcient corresponding distribution high. words correctly predicting sample conditional distribution peaky incorrectly predicted sample conditional distribution ﬂat. however learn parameters minimizing upper bound risk overﬁtting training data. order prevent this introduce regularization terms parameters. work norms though norms also employed. summarize parameters learned solving following optimization problem hyperparameters relative weights regularization upper bound dissimilarity coeﬃcient respectively. note upper bound derivation resulting optimization problem similar lsvm framework. fact problem shown strict generalization lsvm. observation follows fact that loss function independent latent variables i|si) hence optimization problem equivalent minimizing regularization fact even loss function depend predicted latent variable optimization problem still generalizes lsvm. follows fact that case lsvm problem equivalent using delta distributions model formal proofs omitted. provide mathematical description learning framework. however throughout section next reiterate intuition appropriate places. training objective dissimilarity coeﬃcient training samples. using deﬁnition dissimilarity coeﬃcient equation objective written terms expected loss minimizing objective encourages desirable properties predicted output similar ground-truth output predicted latent variable similar latent variables high probabilities importantly similarity outputs latent variables speciﬁed loss function hence learning parameters tuned according user’s domain knowledge regarding quality solution. ability learn loss-speciﬁc parameters absent traditional frameworks variants. objective smooth diﬀerentiable commonly used choices loss function highly non-smooth non-smoothness objective results diﬃcult optimization problem makes learner prone local minimum solutions. order overcome deﬁciency minimize upper bound objective similar lsvm formulation block coordinate descent. speciﬁcally starting initial estimate parameters alternately sets parameters optimizing problem parameters. process said terminate decrease objective falls hyperparameter problem user speciﬁed tolerance. following subsections provide details optimization parameters. problem requires computation expected loss deﬁned equation found time pair suﬃciently small operation computationally feasible. large latent variable space options. first choose joint feature vector conditional distribution decomposable manner facilitate eﬃcient computation suﬃcient statistics note still allows complex joint feature vector make predictions given test sample. second problem requires complex encode conditional distribution resort using several inference techniques compute approximate suﬃcient statistics. however note several important problems machine learning formulated using latent variables whose space sufﬁciently small allow exact computations expected loss including motif ﬁnding image classiﬁcation digit recognition problems used experiments namely object detection action detection. since term constant samples expected loss intuitive objective gives weight loss corresponding latent variables high probability less weight corresponding latent variables probability. formally ﬁxed optimization problem reduces following regularization term ||w|| convex. term diﬀerence functions pointwise maximum linear functions. since pointwise maximum convex functions convex observation follows. similar lsvm local minimum saddle point solution problem found using concave-convex procedure main steps cccp outlined algorithm iteratively estimates value latent variables using current estimate updates parameters solving convex optimization problem several eﬃcient algorithms problem example work -slack reformulation method proposed joachims also solve problem using selfpaced learning algorithm objective consider simple loss predicts correct output sample ﬁrst term objective dominates second. case parameter encouraged assign high probability predicted latent variables similar latent variables order minimize objective. ﬁrst term constant. thus parameter encouraged maximize diversity conditional distribution words correct prediction output learn peaky distribution incorrect prediction output learn distribution. formally ﬁxed optimization problem reduces following loss function measured using latent variables estimated ﬁrst step instead true latent variables. following observation shows ilsvm special case framework. observation ﬁrst step ilsvm minimizes objective restricted delta distributions. second step ilsvm solves lsvm problem similar described previous subsection optimizing observation regarding second step straightforward. ﬁrst step follows fact ilsvm minimizes second divergence coeﬃcient vanishes using delta conditional distributions ilsvm eﬀectively minimizes objective ﬁxed formal proof omitted. iteration takes time similar expected loss performed exactly suﬃciently small space latent variables appropriate choice joint feature vector large latent variable space complex joint feature vector would resort approximate inference. overall approach similar ﬂavor ilsvm algorithm iterates following steps convergence obtain value latent variables training samples using current estimate parameters; update parameters solving lsvm demonstrate eﬃcacy framework challenging machine learning applications object detection action detection. speciﬁcally show approach models uncertainty values latent variables training outperforms previous loss-based learning frameworks namely lsvm ilsvm estimate likely assignment latent variables. three methods used experiments share common hyperparameter vary take values addition framework introduces hyperparameters experiments however obtain better results carefully tuning hyperparameters. tolerance value methods problem formulation. application learn discriminative object models predict category location object present image. fully supervised setting would required specify tight bounding around object present training samples. collection annotations onerous expensive would like learn object models using image-level labels considerably easier obtain. formally sample input image. output number object categories. latent variable models tight bounding around object image. similar previous works joint feature vectors deﬁned using descriptor extracted using pixels bounding box. experiments consider non-overlapping putative bounding boxes pixels apart results maximum bounding boxes image dataset. allows compute exact expected loss exact subgradients learning. employ diﬀerent loss functions loss overlap loss deﬁned below. speciﬁcally loss best test loss lsvm ilsvm method respectively overlap loss best test loss respectively. improvement overlap loss statistically signiﬁcant according paired t-test improvement loss statistically signiﬁcant ratio area intersection area union bounding boxes loss functions encourage models predict right category also right location object. note similar experimental setup also used blaschko dataset. images diﬀerent mammals previously employed image classiﬁcation split images category approximately training testing. report results using folds. results. figure shows test loss lsvm ilsvm method using diﬀerent values. test loss computed using ground-truth labels bounding boxes test samples. recall that training ground-truth labels assumed known bounding boxes modeled latent variables. lsvm initially proposed loss functions depend value true latent variable adopted similar approach cccp algorithm lsvm solve object detection problem. brieﬂy iterate steps estimating value latent variables solving convex structured problem objective function could decreased user-speciﬁed tolerance. experiments approach provided similar results ilsvm method. figure average test loss folds object detection application diﬀerent values left loss; right overlap loss. framework outperforms lsvm ilsvm provides statistically signiﬁcant improvements loss problem formulation. application learn human action models predict action class location person present image. similar object detection fully supervised dataset would require annotating training image person bounding box. instead image-level labels indicate action performed person image. formally sample input image. output number action classes. latent variable models tight bounding around person image. joint feature vectors poselet descriptor bounding box. consider approximately putative bounding boxes image obtained automatically using standard person detector small search space latent variables avoids need approximate inference. again report results using loss overlap loss. results. figure shows test loss three methods computed using ground-truth labels bounding boxes. loss best test loss values lsvm ilsvm method respectively. overlap loss best test loss respectively. method signiﬁcantly outperforms lsvm ilsvm conﬁrmed paired t-test figure average test loss folds action detection application diﬀerent values left loss; right overlap loss. framework outperforms lsvm ilsvm provides statistically signiﬁcant improvements types loss proposed novel framework parameter estimation using weakly supervised datasets. framework consists distributions conditional distribution captures uncertainty latent variables delta distribution predicts output latent variable values. parameters distributions learned minimizing loss-based dissimilarity coeﬃcient distributions samples training dataset. empirically demonstrate beneﬁt approach previous loss-based learning frameworks using publicly available datasets challenging problems—object detection action detection. proposed optimization requires computation expected loss learning delta distribution lossdependent subgradient learning conditional distribution. special cases terms computed exactly. general would resort several existing approximate inference techniques design customized algorithms compute suﬃcient statistics. note that since conditional distribution used testing approximate estimate parameters able accurately model uncertainty latent variables would suﬃce practice. acknowledgments. work partially funded european research council european community’s seventh framework programme /erc grant agreement number inriastanford associate team splendid grant muri contract boeing company. thank michael stark proof-reading paper subhransu maji poselets data daniel selsam andrej karpathy helpful discussions.", "year": 2012}