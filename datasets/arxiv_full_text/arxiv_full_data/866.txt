{"title": "Stacked What-Where Auto-encoders", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We present a novel architecture, the \"stacked what-where auto-encoders\" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the \"what\" which are fed to the next layer, and its complementary variable \"where\" that are fed to the corresponding layer in the generative decoder.", "text": "junbo zhao michael mathieu ross goroshin yann lecun courant institute mathematical sciences york university broadway floor york {junbo.zhao mathieu goroshin yann}cs.nyu.edu present novel architecture stacked what-where auto-encoders integrates discriminative generative pathways provides uniﬁed approach supervised semi-supervised unsupervised learning withrelying sampling training. instantiation swwae uses convolutional encode input employs deconvolutional produce reconstruction. objective function includes reconstruction terms induce hidden states deconvnet similar convnet. pooling layer produces sets variables what next layer complementary variable where corresponding layer generative decoder. desirable property learning models ability trained supervised unsupervised semi-supervised mode single architecture single learning procedure. another desirable property ability exploit advantageous discriminative generative models. popular approach pre-train auto-encoders layer-wise fashion subsequently ﬁne-tune entire stack encoders supervised discriminative manner gregor lecun henaff kavukcuoglu ranzato ranzato lecun approach fails provide uniﬁed mechanism unsupervised supervised learning. another approach provides uniﬁed framework three training modalities deep boltzmann machine model larochelle bengio layer restricted boltzmann machine seen kind auto-encoder. deep rbms desirable properties however exhibit poor convergence mixing properties ultimately reliance sampling during training. main issue stacked auto-encoders asymmetry. mapping implemented feed-forward pathway often many-to-one example mapping images invariant features class labels. conversely mapping implemented feed-back pathway one-to-many e.g. mapping class labels image reconstructions. common deal view reconstruction mapping probabilistic. approach rbms dbms missing information required generate image category label dreamed sampling. sampling approach lead interesting visualizations impractical training large scale networks tends produce highly noisy gradients. mapping input output feed-forward pathway one-to-one mappings directions would well-deﬁned functions would need sampling reconstructing. internal representations possess good invariance properties desirable mapping layer next many-to-one. example convnet invariance achieved layers max-pooling subsampling. model attempts satisfy objectives learn factorized representation encodes invariance equivariance want leverage labeled unlabeled data learn representation uniﬁed framework. main idea approach propose simple whenever layer implements many-to-one mapping compute complementary variables enable reconstruction. schematic model depicted ﬁgure max-pooling layers convnets view position max-pooling switches complementary information necessary reconstruction. model proposed consists feed-forward convnet coupled feed-back deconvnet. stage architecture call what-where auto-encoder. encoder convolutional layer relu followed max-pooling layer. output max-pooling what variable next layer. complementary variables max-pooling switch positions seen where variables. what variables inform next layer content incomplete information position where variables inform corresponding feed-back decoder interesting features located. feed-back decoder reconstructs input unpooling what using where running result reconstructing convolutional layer. what-where convolutional auto-encoders stacked trained jointly without requiring alternate optimization reconstruction penalty layer constrains hidden states feed-back pathway close hidden states feed-forward pathway. system trained purely supervised manner bottom input feed-forward pathway given input layer feed-back pathway given desired output weights decoders updated minimize reconstruction costs. top-level cost used model reverts purely supervised backprop. hidden layer reconstruction costs used model seen supervised reconstruction regularization. unsupervised mode top-layer label output left unconstrained simply copied output feed-forward pathway. model becomes stacked convolutional auto-encoder. boltzmann machines underlying learning algorithm doesn’t change supervised unsupervised modes switch different learning modalities clamping unclamping certain variables. model particularly suitable faced large amount unlabeled data relatively small amount labeled data. fact sampling required gives model good scaling properties; essentially backprop particular architecture. idea what where deﬁned previously different ways. related method proposed known transforming auto-encoders capsule units introduced. work sets variables trained encapsulate invariance equivariance respectively providing parameters particular transformation states network. work carried unsupervised fashion doesn’t require true latent state still able encode similar representations within what where. switches information also made visualization work zeiler work generative pass merely uses feed-forward pass initialization step. similar deﬁnitions applied learn invariant features henaff kavukcuoglu ranzato ranzato lecun makhzani frey masci among them works merely shed light unsupervised feature learning therefore failed unify different learning modalities. another relevant hierarchical architecture proposed ranzato lecun however architecture trained layer-wise greedy manner performance competitive jointly trained models. terms joint loss minimization semi-supervised learning work linked weston ranzato szummer main advantage easiness extend convnet deconvnet thereby enabling utilization unlabeled data. paine analyzed regularization effect similar architectures layer-wise fashion. recent work rasmus proposed adopt deep auto-encoders support supervised learning completely different strategy employed harness lateral connection stage encoder-decoder pairs however. work decoders receive entire pre-pooled activation state encoder whereas decoders swwae receive where state corresponding encoder stages. further lack unpooling mechanism incorporated ladder networks restricted reconstruct layer within generative pathway looses ladder structure. contrast swwae doesn’t suffer necessity. pooling layers encoder split information what where components depicted ﬁgure what essentially where carries argmax i.e. switches maximally activation deﬁned local coordinate frame pooling region. what component upward encoder where lateral connections stage feed-back decoding pathway. decoder uses convolution unpooling operations approximately invert output encoder reproduce input shown ﬁgure unpooling layers where variables unpool feature maps placing what positions indicated preserved switches. negative log-likelihood loss classiﬁcation loss reconstructions; llrec denotes reconstruction loss input-level denotes middle reconstruction loss. notation represents input represent feature activations convnet respectively. similarly input activations deconvnet respectively. entire model architecture shown ﬁgure notice following represent weighted llrec denotes activation feature maps represent spatial location take normalized values stands pooling region. note hyperparameter always non-negative. parametrizes soft pooling larger closer soft-pooling approaches max-pooling small approximates mean-pooling. interpolation unpooling stage handle continuous value conveyed where. soft pooling unpooling embedded seamlessly swwae model virtue backpropogate contrast hard max-pooling differentiable w.r.t argmax switch locations. furthermore soft-pooling operators enable location information accurately represented thus enable features capture details input evidenced visualization experiments mentioned swwae provides uniﬁed framework learning three learning modalities within single architecture single learning algorithm i.e. stochastic gradient descent backprop. switching modalities achieved follows idea behind using reconstruction regularizer studied previously erhan although uses unsupervised pre-training setup. terms this swwae connected unsupervised pre-training sense paradigms attempt provide better generalization forcing model reconstruct. argument unsupervised learning acting regularizer supervised loss drives model unsupervised pre-training captures input distribution learning helpful learning however argue applying statement unsupervised pre-training setup appears unconvincing. argue using merely initialize model learning weak effect; i.e. gradients learning completely overwrite initial weights thus eliminating regularizing effect obtained learning argue joint training effective strategy i.e. swwae; approach tries model together jointly training. comparisons different regularizers shown appendix. moreover training jointly multiple losses helps avoid collapsing learning trivial representation. thing common issue auto-encoders learn little identity function; copying input perfect reconstruction. another sparse auto-encoders makhzani frey attain well known trivial solutions adding penalty hidden layers likely scale encoder weights scale decoders weight order reconstruct achieving small activations. argue direct avoid trivial solutions include supervised loss directly optimizes non-trivial useful criterion helps factorize data semantically relevant factors variation. reasons adding intermediate reconstruction terms listed follow. first prevents feature planes shufﬂed where conveyed encoder guaranteed match what decoder ith. otherwise unpooling what where shufﬂe orders hence cannot work properly. second particular training classiﬁcation loss intermediate terms disallow scenario upper layers become idle lower layers busy reconstructing case ﬁlters unemployed layers regularized. related classiﬁcation performance comparison intermediate terms shown appendix. third correspondence layer-wise auto-encoder training intermediate encoder/pool/unpool/decoder units swwae combined intermediate terms seen single-layer convolutional auto-encoder following notation describe architecture e.g. c-c-p-fc denotes convolution layer feature maps kernel size denotes pooling layer denotes fully-connection layer connects hidden units. relu omitted notation. address necessity where showing difference reconstructions using where versus using where. upsampling alternative unpooling without dreaming where respect what agnostic where hence gets copied positions. figure displays group reconstructed digits sampled mnist’s testing generated trained swwae using mnist training set. architecture c-c-xp pooling size experimented varies note hard max-pooling experiment architecture trained unsupervised mode. hand generations given unpooling obviously clearer cleaner ones upsampling experiment demonstrates where critical information demanded reconstructing; barely obtain well reconstructed images without preserving where. hand experiment also considered example using swwae generative purpose. section examine relationship what where using visualization approach proposed transforming auto-encoders number capsules trained learn representation consisting equivariant invariant components. analogously what where model’s representation correspond invariant equivariant components respectively. experiment recipe stated follow. train swwae using horizontally vertically translated mnsit digits training set; feed untranslated digits testing swwae obtain what where horizontally vertically translate digits feed swwae cache what where correspondingly; plot relationship what where obtained translated digits versus untranslated ones shown ﬁgure architecture c-c-p-c-p soft pooling/unpooling since experiment demands large pooling size hence plot generations ﬁgure make sure swwae works appropriately large pooling settings. draw conclusion ﬁgure what where behave much like invariance equivariance capsules hinton hand where learns highly localized representation. element where approximately linear response pixellevel translation either horizontal/vertical direction learns invariant another. figure scatter plots depicting feature response produced translating input. horizontal axis represents what where output feature plane untranslated digit image; vertical axis represents what where output feature plane image translated pixels either horizontal vertical direction. left right ﬁgures respectively ﬁrst what horizontally translated digits versus original digits; second where horizontally translated digits versus original digits; third what vertically translated digits versus original digits; fourth where vertically translated digits versus original digits. note circles used feature translation triangles where related plots denote dimensions where respectively. figure reconstructed mnist digits capsule emulation experiments. shows original input; second shows reconstruction original inputs; bottom rows display reconstruction horizontally translated digits positive negative direction respectively. figure validation-error v.s. range datasets swwae semi-supervised experiments. left mnist. right svhn. different curves denote different number labels used. start access effect swwae classiﬁcation performing semi-supervised supervised experiments mnist svhn. attempt demonstrate introducing paired deconvnet group reconstruction losses help generalization provide effective solution make unlabeled data. note classiﬁcation experiments hard version pooling performs better soft counterparts terms classiﬁcation. start constructing semi-supervised datasets datasets. mnist dataset consists images different classes size training samples test samples. follow previous work data preparation randomly select labeled samples training rest samples used without labels sizes labeled subset respectively ensure class number digits chosen labeled set. svhn dataset consists digits training digits testing extra training samples less difﬁcult. likewise construct labeled dataset svhn contains samples uniformly distributed classes chosen randomly non-extra training set. order attain reliable results experiment several rounds whereby datasets refreshed round average performances rounds ﬁnal evaluation. approach standalone regularization effect swwae datasets plotting validation error v.s. ﬁgure standalone mean well-known regularizer applied. evaluate swwae testing svhn chosen hypertable shows results. additionparameters indicated validation error. ally evaluate swwae svhn pure supervised manner availtesting error decreases yielded able labels) swwae versus vanilla convnet conﬁguration. architecture mnist svhn respectively c-p-c-p-c-p-fc c-p-c-c-p-c-p-fc. exploration mnist shown appendix. stl- contains larger pixel images relatively less labeled data training mapped predeﬁned folds images each. therefore stl- ratio amount unlabeled samples labeled ones fold. follow testing protocol stl- ﬁrst tune hyper-parameters fold validation error best performed model predict testing set. ﬁnal score reported averaging testing score folds. stl- access possibility combine batch normalization swwae. furthermore carry spatial batch normalization preserves mean standard deviation feature normalized independently based statistics. devise vgg-style deep c-p-c-p-c-c-p-c-c-c-cc-c-p-fc convolution layer followed spatial batch normalization layer applied convnet deconvnet pathways. results shown table table accuracy swwae cifar- cifar- comparison best published single-model results. results obtained common experimental setting adopt contrast normalization small translation horizontal mirroring data preprocessing. dataset cifar- cifar- sampled labeled million tiny images dataset datasets contain images small portions million images. contrast former classiﬁcation experiments experiment involves substantially abundant unlabeled data relation amount labeled data. carry swwae vgg-style network c-c-p-c-c-p-c-c-p-c-cp-fc-fc convolution bundled followed spatial batch normalization convnet deconvnet. compare results approaches perform experiments common experimental setting adopts contrast normalization small translation horizontal mirroring data preprocessing. results shown table overall system seen pairing convnet deconvnet yields good accuracy variety semi-supervised supervised tasks. envision architecture also useful video related tasks unlabeled samples abound. alexey dosovitskiy jost tobias springenberg martin riedmiller thomas brox. discriminative unsupervised feature learning convolutional neural networks. advances neural information processing systems dumitru erhan yoshua bengio aaron courville pierre-antoine manzagol pascal vincent samy bengio. unsupervised pre-training help deep learning? journal machine learning research mikael henaff kevin jarrett koray kavukcuoglu yann lecun. unsupervised learning sparse features scalable audio classiﬁcation. proceedings international symposium music information retrieval koray kavukcuoglu marc’aurelio ranzato yann lecun. fast inference sparse coding algorithms applications object recognition. technical report computational biological learning courant institute tech report cbll-tr---. koray kavukcuoglu marc’aurelio ranzato fergus yann lecun. learning invariant features proc. international conference computer vision pattern recognition koray kavukcuoglu pierre sermanet y-lan boureau karol gregor micha¨el mathieu yann lecun. learning convolutional feature hierachies visual recognition. advances neural information processing systems volume diederik kingma shakir mohamed danilo jimenez rezende welling. semi-supervised learning deep generative models. advances neural information processing systems jonathan masci ueli meier cires¸an j¨urgen schmidhuber. stacked convolutional auto-encoders hierarchical feature extraction. artiﬁcial neural networks machine learning–icann springer ranzato yann lecun. sparse locally shift invariant feature extractor applied document images. document analysis recognition icdar ninth international conference volume ieee ranzato huang boureau yann lecun. unsupervised learning invariant feature hierarchies applications object recognition. computer vision pattern recognition cvpr’. ieee conference ieee marc’aurelio ranzato martin szummer. semi-supervised learning compact document representations deep networks. proceedings international conference machine learning salah rifai pascal vincent xavier muller xavier glorot yoshua bengio. contractive auto-encoders explicit invariance feature extraction. proceedings international conference machine learning antonio torralba fergus william freeman. million tiny images large data nonparametric object scene recognition. pattern analysis machine intelligence ieee transactions tend exhibit experimental results mnist respects. first validation compare performance swwae regularization methods shown table note order make comparison realistic closer practical uses dropout fully-connected layers default comparisons. regularizers comparison include dropout convolution layers sparsity penalty hidden layers. besides also train swwae unsupervisedly separately train softmax classiﬁer afterwards using labeled samples; disjointly trained architecture denoted unsup-sfx. similarly using swwae unsupervised pre-training approach followed ﬁne-tuning entire convnet part driven labeled data denoted unsup-pretr. note difference unsup-pretr unsup-sfx lies convnet part frozen training softmax classiﬁer top. addition nolm written experiments swwae trained reconstruction loss input level i.e. λlrec chosen validation error. second report testing error rate obtained swwae chosen hyper-parameter swwae compare best published results table note experiments mnist testing labeled generated sampling entire mnist training set; experiments validation instead sample labeled data subset mnist training rest deemed validation set. swwae conﬁguration c-p-c-p-c-p-fc. table comparison regularization approaches disjoint training approaches mnist dataset. scores validation error rate dropout added fully-connected layers default. aside semi-supervised setting also explore swwae training full labeled training dataset swwae achieves better testing error rate versus obtained convnet conﬁguration. reason swwae working well ladder networks rasmus fact reconstructing mnist digits overly easy swwae. assume layer swwae pooling unpooling layer implemented pathway respectively. since mnist roughly binary dataset thus within unpooling stage decoding doesn’t necessarily demand information what reconstruction; i.e. could perfect reconstruction pinning positions indicated where. therefore believe reconstructing mnist dataset renders insufﬁcient regularization encoding pathway. however phe-", "year": 2015}