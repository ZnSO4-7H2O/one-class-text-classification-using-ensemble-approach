{"title": "Neural CRF Parsing", "tag": ["cs.CL", "cs.NE"], "abstract": "This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages.", "text": "paper describes parsing model combines exact dynamic programming parsing rich nonlinear featurization neural approaches. model structurally factors anchored rule productions instead linear potential functions based sparse features nonlinear potentials computed feedforward neural network. potentials still local anchored rules structured inference unchanged sparse case. computing gradients learning involves backpropagating error signal formed standard sufﬁcient statistics using dense features neural already exceeds strong baseline model combination sparse features system achieves section penn treebank generally outperforms best prior single parser results range languages. neural network-based approaches structured tasks strengths weaknesses compared conventional models conditional random ﬁelds strength neural approaches ability learn nonlinear interactions underlying features. case unstructured output spaces capability gains problems ranging syntax lexical semantics neural methods also powerful tools case structured output spaces. here past work often relied recurrent architectures propagate information structure realvalued hidden state result admit efﬁcient dynamic programming however natural marriage nonlinear induced features efﬁcient structured inference explored collobert case sequence modeling feedforward neural networks used score local decisions reconciled discrete structured modeling framework allowing inference dynamic programming. work present constituency parser based principles individual anchored rule productions scored based nonlinear features computed feedforward neural network. separate identicallyparameterized replicate network exists possible span split point. input takes vector representations words split point span boundaries; outputs scores anchored rules applied span split point. scores thought nonlinear potentials analogous linear potentials conventional crfs. crucially network replicates connected uniﬁed model computations factor along substructures standard crfs. prior work parsing using neural network models often sidestepped problem structured inference making sequential decisions reranking contrast framework permits exact inference since model’s structured interactions purely discrete involve continuous hidden state. therefore exploit neural net’s capacity learn nonlinear features without modifying figure neural model. right anchored rule tree independently scored function perform inference compute marginals viterbi tree. left show process scoring anchored rule neural features words embedded neural network hidden layer compute dense intermediate features whose conjunctions sparse rule indicator features scored according parameters core inference mechanism allowing tricks like coarse pruning make inference efﬁcient purely sparse model. model trained gradient descent exactly conventional gradient network parameters naturally computed backpropagating difference expected anchored rule counts network span split point. using dense learned features alone neural model obtains high performance outperforming parser hall sparse indicators used addition resulting model gets section penn treebank outperforming parser socher well berkeley parser matching discriminative parser carreras model also obtains best single parser results nine languages outperforming system hall figure shows neural model. model decomposes anchored rules scores potential function; standard potentials typically linear functions sparse indicator features whereas figure example anchored rule production rule anchoring extract either sparse surface features sequence word indicators embedded form vector representation anchoring’s lexical properties. approach nonlinear functions word embeddings. section describes notation anchored rules section talks scored. discuss speciﬁc choices featurization backbone grammar used structured inference anchored rules fundamental units parsing models consider anchored rules. shown figure deﬁne anchored rule tuple indicator rule’s identity indicates span split point rule. tree simply collection anchored rules subject constraint rules form tree. parsing models crfs decompose anchored rule productions place probability distribution trees conditioned sentence follows throughout work primarily consider potential functions linear functions sparse indicators nonlinear neural networks dense continuous features. although modeling choices possible points design space reﬂect common choices past work suggested nonlinear functions indicators linear functions dense features perform less well scoring function considers input sentence anchored rule question. figure shows scoring process schematically. module left neural linear function surface features combination long provides anchored rule scores structured inference component regardless pcfg estimated maximum likelihood independent anchoring words except preterminal productions; basic discriminative parser might learned parameter still disregard surface information. however surface features capture useful syntactic cues consider example figure proposed parent preceded word reﬂected followed period surface context characteristic object position. beginning ending personality typical properties well choice particular rule supported fact proposed child begins information captured sparse features describe below neural network taking lexical context input. sparse vector features expressing properties sparse vector surface features associated words sentence anchoring shown figure matrix weights. scoring particular anchored rule depicted figure note surface features rule indicators conjoined systematic way. role equally well played vector dense features learned neural neta conventional expression scoring function vector parameters single feature extractor jointly inspects surface rule. however feature representation conjoins rule surface properties sentence systematic equivalent formalism. figure sparse neural scoring functions parsing. surface feature vectors sparse neural models extracted anchored spans split points. sparse case multiply weight matrix sparse output vector score rule production. neural case ﬁrst embed transform one-layer neural network order produce intermediate feature representation combining embedding words allows standard pre-trained vectors easily tying embeddings across word positions substantially reduces number model parameters. however embedding features rather words also shown effective increases ability overﬁt. following hall grammars little annotation horizontal markovization experiments english experiments neural vertical markovization also beneﬁt making system much faster smaller state space dynamic programming. using parent annotation useful languages grammar reﬁnement consider. using rectiﬁed linear units nonlinearity objective everywhere differentiable. interaction parameters nonlinearity also makes objective nonconvex. however spite this still follow subgradients optimize objective standard practice. note outer products give matrices feature counts isomorphic second expression simpliﬁed terms expected feature counts. update standard backpropagation ﬁrst computing features take features described hall preterminal layer model considers preﬁxes sufﬁxes length current word neighboring words well words’ identities. nonterminal productions indicators words start split point anchored rule well span properties span length span shape neural model take productions words surrounding beginning span split point shown figure particular look words either direction around point interest meaning neural takes words input. word embeddings pre-trained word vectors bansal compare sources word vectors section contrary standard practice update vectors training; found provide accuracy beneﬁt slowed training considerably. grammar reﬁnements recurring issue discriminative constituency parsing granularity annotation base grammar using ﬁner-grained symbols rules gives model greater capacity also introduces parameters learning uses adadelta employed past work found adagrad performed equally well tuned regularization step size parameters adadelta worked better box. momentum term regularize weights all. used minibatch size trees although system particularly sensitive this. treebank trained either passes treebank minibatches whichever shorter. initialized output weight matrix zero. break symmetry lower level neural network parameters initialized entry independently sampled gaussian mean variance gaussian performed better uniform initialization variance important. baseline neural model score anchored rule productions. standard fashion compute either expected anchored rule counts viterbi tree maxt speed inference using coarse pruning pass. follow hall prune according x-bar grammar headoutward binarization ruling constituent whose marginal probability less pruning number spans split points considered greatly reduced; however still need compute neural network activations remaining span split point thousands given sentence. improve efﬁciency noting word appear position large number span/split point combinations cache contribution hidden layer caused word computing hidden layer simply requires adding vectors together applying nonlinearity instead costly matrix multiply. reason choose include rule identity input network requires computing even larger number network activations since cannot reuse across rules span split point. combined sparse neural model trains penn treebank hours single machine parallelized implementation. reference purely sparse model parentannotated grammar takes around hours machine. table shows results section english penn treebank computed using evalb. full test results comparisons systems shown table compare variants system along axes whether standard linear sparse features nonlinear dense features neural both whether word representations used. sparse neural neural table outperforms sparse even sparse heavily annotated grammar. surprising result features sparse carefully engineered capture range linguistic phenomena guarantee word vectors capture same. example tagging layer sparse model looks preﬁxes sufﬁxes words give model access morphology predicting tags unknown words typically regular inﬂection patterns. contrast neural model must rely geometry vector space exposing useful regularities. time strong performance combination systems indicates featurization approaches highperforming complementary strengths. unlabeled data much attention paid choice word vectors various tasks notably whether capture syntactic semantic phenomena primarily vectors bansal train skipgram model mikolov using contexts dependency links; similar approach also suggested levy goldberg table results sparse neural combined parsing models section penn treebank. systems broken whether local potentials come sparse features and/or neural network level vertical markovization kind word representations use. neural outperforms sparse even heavily annotated grammar used combined approach substantially better either individual model. contribution neural architecture cannot replaced brown clusters even word representations learned penn treebank surprisingly effective however embeddings trained relatively small corpus natural wonder whether lesssyntactic embeddings trained larger corpus might useful. case line table shows performance neural using wikipedia-trained word embeddings collobert perform better vectors bansal isolate contribution continuous word representations themselves also experimented vectors trained text training penn treebank using skip-gram model window size vectors somewhat lower performing still provide surprising noticeable gain stacked sparse features suggesting dense sparse representations complementary strengths. result also reinforces notion utility word vectors come primarily importing information out-of-vocabulary words table exploration implementation choices feedforward neural network sentences length section penn treebank. rectiﬁed linear units perform better tanh cubic units network hidden layer performs best embedding output feature vector gives worse performance. sparse model similar information true apples-to-apples comparison. brown clusters shown effective vehicles past incorporate brown clusters baseline model analogous embedding features used dense model surface features ﬁred brown cluster identities words. brown clusters trained data vectors bansal however table shows features provide beneﬁt baseline model suggests either difﬁcult learn reliable weights sparse features different regularities captured word embeddings. neural design space large wish analyze particular design choices made system examining performance several variants neural architecture used system. table shows development results potential alternate architectural choices discuss. table compares performance three nonlinearities. rectiﬁed linear units perform best followed tanh units followed cubic units. drawback tanh activation function easily saturated input unit away zero causing backpropagation derivatives unit essentially cease; known cause problems training requiring special purpose machinery deep networks depth given using rectiﬁed linear units bears asking whether implementation improving substantially linear features continuous input. embedding vector anchored span directly input basic linear shown figure table shows purely linear architecture performs surprisingly well still less effective network hidden layer. agrees results wang manning noted dense features typically beneﬁt nonlinear modeling. also compare two-layer neural network also performs worse one-layer architecture. densifying output features overall appears beneﬁcial dense representations surface features; natural question might whether technique applied sparse output feature vector apply approach srikumar manning multiply sparse output vector dense matrix giving following scoring function performance cube decreased substantially late learning; peaked around dropout useful alleviating type overﬁtting experiments dropout beneﬁcial overall. figure additional forms scoring function. linear version dense model equivalent continuous-valued input features. version dense model outputs also embedded according learned matrix experimented show results table unfortunately approach seem work well parsing. learning output representation empirically unstable also required careful initialization. tried gaussian initialization initializing model clustering rules either randomly according parent symbol. latter shown table gave substantially better performance. hypothesize blurring distinctions output classes harm model’s ability differentiate closely-related symbols required good parsing performance. using pretrained rule embeddings layer might also improve performance method. penn treebank table reports results section penn treebank focus comparison single parser systems opposed rerankers ensembles self-trained methods first compare table results nine treebanks spmrl shared tasks; values f-scores sentences lengths using version evalb distributed shared task. parser substantially outperforms strongest single parser results dataset berkeley-tags improved version berkeley parser designed shared task best reranked ensemble modiﬁed berkeley parsers constitutes best published numbers dataset table test results section penn treebank. compare several categories parsers literatures. outperform strong baselines berkeley parser stanford parser match performance sophisticated generative discriminative parsers. single parser shindo knowledge latter systems highest performing ptb-only single parser data condition; match performance though also word vectors computed unlabeled data. compare shiftreduce parser uses unlabeled data form brown clusters. method achieves performance close parser. also compare compositional vector grammar parser socher well lstm-based parser vinyals conditions parsers operating slightly different former reranker stanford parser latter trains much larger amounts data parsed product berkeley parsers regardless outperform parser well single parser results vinyals also examine performance parser speciﬁcally nine morphologically-rich languages used spmrl shared tasks train word vectors monolingual data distributed spmrl shared task using skip-gram approach wordvec window size backbone grammar found beneﬁcial overall. table shows system improves upon performance parser hall well single parser shared task robust improvements languages. work presented parser scores anchored rule productions using dense input features computed feedforward neural net. neural component modularized easily integrate preexisting learning inference framework based around dynamic programming discrete parse chart. combined neural sparse model gives strong performance english languages. work partially supported under darpa contract hr--c- facebook fellowship ﬁrst author google faculty research award second author. thanks david hall assistance epic parsing framework preliminary implementation neural architecture kush rastogi training word vectors spmrl data jurafsky helpful discussions anonymous reviewers insightful comments. mohit bansal kevin gimpel karen livescu. tailoring continuous word representations dependency parsing. proceedings association computational linguistics. yonatan belinkov regina barzilay amir globerson. exploring compositional architectures word vector representations prepositional phrase attachment. transactions association computational linguistics yoshua bengio r´ejean ducharme pascal vincent christian janvin. neural probabilistic lanjournal machine learning reguage model. search march. anders bj¨orkelund ozlem cetinoglu rich´ard farkas thomas mueller wolfgang seeker. ranking meets morphosyntax state-of-the-art results spmrl shared task. proceedings fourth workshop statistical parsing morphologically-rich languages. ozlem etino˘glu agnieszka fale´nska rich´ard farkas thomas mueller wolfgang seeker zsolt sz´ant´o. introducing ims-wrocław-szeged-cis entry spmrl shared task reranking morpho-syntax proceedings first meet unlabeled data. joint workshop statistical parsing morphologically rich languages syntactic analysis non-canonical languages. xavier carreras michael collins terry koo. dynamic programming perceptron efﬁcient feature-rich parsing. proceedings conference computational natural language learning. eugene charniak mark johnson. coarseto-fine n-best parsing maxent discriminative proceedings association reranking. computational linguistics. danqi chen christopher manning. fast accurate dependency parser using neural networks. proceedings empirical methods natural language processing. ronan collobert jason weston l´eon bottou michael karlen koray kavukcuoglu pavel kuksa. natural language processing journal machine learning research scratch. benoit crabb´e djam´e seddah. multilingual discriminative shift-reduce phrase structure parsing spmrl shared task. proceedings first joint workshop statistical parsing morphologically rich languages syntactic analysis non-canonical languages. john duchi elad hazan yoram singer. adaptive subgradient methods online learning stochastic optimization. journal machine learning research july. jenny rose finkel alex kleeman christopher manning. efﬁcient feature-based conditional random field parsing. proceedings association computational linguistics. james henderson. inducing history representations broad coverage statistical parsing. proceedings north american chapter association computational linguistics. sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. arxiv preprint arxiv.. ozan ˙irsoy claire cardie. opinion mining deep recurrent neural networks. proceedings conference empirical methods natural language processing. kalchbrenner edward grefenstette phil blunsom. convolutional neural network modelling sentences. proceedings association computational linguistics. alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems. phong willem zuidema. insideoutside recursive neural network model dependency parsing. proceedings conference empirical methods natural language processing. yuan zhang regina barzilay tommi jaakkola. low-rank tensors scoring dependency structures. proceedings association computational linguistics. tomas mikolov chen greg corrado jeffrey dean. efﬁcient estimation word repreproceedings sentations vector space. international conference learning representations. slav petrov klein. sparse multi-scale grammars discriminative latent variable parsing. proceedings conference empirical methods natural language processing. djam´e seddah reut tsarfaty sandra k¨ubler marie candito jinho choi rich´ard farkas jennifer foster iakes goenaga koldo gojenola galletebeitia yoav goldberg spence green nizar habash marco kuhlmann wolfgang maier joakim nivre adam przepi´orkowski ryan roth wolfgang seeker yannick versley veronika vincze marcin woli´nski alina wr´oblewska. overview spmrl shared task cross-framework evaluation parsing morphoproceedings logically rich languages. fourth workshop statistical parsing morphologically-rich languages. djam´e seddah sandra k¨ubler reut tsarfaty. introducing spmrl shared task proparsing morphologically-rich languages. ceedings first joint workshop statistical parsing morphologically rich languages syntactic analysis non-canonical languages. hiroyuki shindo yusuke miyao akinori fujino masaaki nagata. bayesian symbol-reﬁned tree substitution grammars syntactic parsing. proceedings association computational linguistics. richard socher john bauer christopher manning andrew parsing compositional vector grammars. proceedings association computational linguistics. vivek srikumar christopher manning. learning distributed representations structured output prediction. advances neural information processing systems. neural networks leverage corpus-wide information part-of-speech tagging. proceedings conference empirical methods natural language processing. joseph turian ratinov yoshua bengio. word representations simple general method semi-supervised learning. proceedings association computational linguistics. mengqiu wang christopher manning. effect non-linear deep architecture sequence labeling. proceedings international joint conference natural language processing. muhua zhang wenliang chen zhang jingbo zhu. fast accurate shiftreduce constituent parsing. proceedings association computational linguistics.", "year": 2015}