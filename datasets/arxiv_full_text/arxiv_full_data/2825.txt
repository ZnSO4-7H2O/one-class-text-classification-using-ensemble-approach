{"title": "Learning Multiple Levels of Representations with Kernel Machines", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We propose a connectionist-inspired kernel machine model with three key advantages over traditional kernel machines. First, it is capable of learning distributed and hierarchical representations. Second, its performance is highly robust to the choice of kernel function. Third, the solution space is not limited to the span of images of training data in reproducing kernel Hilbert space (RKHS). Together with the architecture, we propose a greedy learning algorithm that allows the proposed multilayer network to be trained layer-wise without backpropagation by optimizing the geometric properties of images in RKHS. With a single fixed generic kernel for each layer and two layers in total, our model compares favorably with state-of-the-art multiple kernel learning algorithms using significantly more kernels and popular deep architectures on widely used classification benchmarks.", "text": "despite fact capable universal function approximation enjoy solid mathematical foundation kernel machines like many general-purpose learning machines overshadowed multilayer neural networks challenging ﬁelds computer vision natural language processing etc. extensive works argued dominance neural networks ﬁelds widely accepted that learn highly complicated functions able represent high-level abstractions required complex tasks highly desirable mathematical perspective terms biological plausiblity learning machine learn multiple levels distributed representations architectural constraints kernel machines naturally possess learning capability. hence ﬁrst contribution bridge between neural networks kernel machines building multilayer network kernel machines layer consisting array kernel machines units. network whole learns hierarchical distributed representations function compositions. propose solution second issue ﬁrst argue choice kernel matters practice. well-established that assumption topological space together mild conditions easily inﬁnite number kernels prei= dense supremum norm continuous functions compact support whose domain further given machine learning task despite kernel capable implementing functions proper subspace ﬁxed representer theorem unless clear context noted otherwise always assume kernel discussed paper meets conditions kernel universal induces kernel machine capable universal approximation. propose connectionist-inspired kernel machine model three advantages traditional kernel machines. first capable learning distributed hierarchical representations. second performance highly robust choice kernel function. third solution space limited span images training data reproducing kernel hilbert space together architecture propose greedy learning algorithm allows proposed multilayer network trained layerwise without backpropagation optimizing geometric properties images rkhs. single ﬁxed generic kernel layer layers total model compares favorably state-of-the-art multiple kernel learning algorithms using signiﬁcantly kernels popular deep architectures widely used classiﬁcation benchmarks. address issues commonly considered inherent kernel machines i.e. learning machines arbitrary real kernel function. first kernel machines unable learn multiple levels distributed representations become source criticism since capability generally considered essential complicated artiﬁcial intelligence tasks. second performance kernel machine usually highly dependant choice kernel since governs quality accessible function space rkhs rules good heuristics exist topic task-dependent department electrical computer engineering university florida gainesville florida department mathematics university florida gainesville florida usa. correspondence shiyu duan <michaelshiyuuﬂ.edu> jose principe <principecnel.uﬂ.edu>. guarantees subspace includes optimal solution given regularized empirical risk minimization problem. practice however regularization function class necessary learning happen functions kernel machine effectively implement denoted strict subset longer arbitrary. normally words despite inﬁnite number kernels turned kernel machines capable universal approximation theory choice critical practice since governs goodness accessible subset hence quality regularized solution. arguably popular approach tackle limitation multiple kernel learning mitigates issue learning mixture kernels. terms function space resulting kernel machine effectively utilize enlarges accessible function space combining several rkhs’s parametric usually linear fashion. however since rkhs’s induced many generic kernels already large enough contain practically function fruitful efﬁcient learning machine explore freely single rkhs without limited span images training data. show possible optimize accessible function space kernel machine directly machine effectively utilize better subspace rkhs original sense subspace contains better potentially efﬁcient approximation true target function. purpose present layer-wise learning algorithm proposed model classiﬁcation setting arbitrary number classes. algorithm involves single feedforward phase essentially makes network learn kernel matrix layer time. show objective learning equivalent driving images given training data rkhs closer orthonormal class labels encoded directions vectors. incentive distribution images rkhs tightens margin-based generalization bounds linear classiﬁers learning rather robust choice kernel sufﬁces single generic kernel hyperparameters ﬁxed. proposed learning algorithm similar spirit pioneering work contribution two-fold. first provide geometric interpretations learning utilize insights training multilayer network without backpropagation. second perhaps importantly show even single completely ﬁxed generic kernel obtain practically arbitrary kernel matrix desire optimization need constrained make resulting kernel matrix positive semideﬁnite. contrasts existing works learning usually concerns kernel selection among multiple kernels formulated explicitly constrained optimization problem many settings training choice cost function restricted accommodate requirements. describe build connectionist model kernel machines. since describing multilayer network beneﬁtial establish nomenclature avoid confusions layer closest input called ﬁrst layer. shall subscripts distinguish components within layer superscripts numbering components denotes third different layers. example kernel machine second layer. note paper shall propose model well learning algorithm classiﬁcation arbitrary number classes. describe architecture multilayer kernel network mlkn layer array kernel machines. take ﬁrst layer exfunction form written ample arbitrary admissible given random sample. universality representer theorem know universal approximator practically function number kernel machines ﬁrst layer. equivalently capable learning practically representation original random sample given element representation would simf shorthand much possible. trained greedy learning algorithm shall propose following subsection mapping learned kernel machines ﬁrst layer learned representation later mapped rkhs mapping deﬁned images form orthonormal following property class images identical; otherwise images orthogonal. mapping successfully learned terms advantages classifying ﬁrst straightforward always linearly separable rkhs furthermore unsigned property nonzero constant classes margin respect class linear functions smaller appendix proof. according suggests among possible representations learned representation optimal kernel machine terms generalization. mlkn described two-layer mlkn ﬁrst layer array kernel machines trained representation learning second layer kernel machine trained classiﬁcation. note need multiple kernel machines second layer classiﬁcation classes. construct mlkn layers instead training second layer classiﬁer previously repeat construction training process ﬁrst layer build second layer also representation learner. speciﬁc build another array kernel machines input i.e. βjik entire two-layer model whole learns composed mapping universal approximator practically function mapping construction. similar ﬁrst layer second layer learn mapping representation later mapped rkhs induced kernel mapping deﬁned orthonormal rkhs. ﬁrst second layers properly trained kernel mai= trained classiﬁcation. three-layer mlkn ﬁrst layers hierarchical representation learner third layer classiﬁer works learned representation. figure illustration architecture. construction repeated obtain layers. remarks addressing mlkn different traditional kernel machine order. first mlkn learns hierarchical distributed representations. this ﬁrst note that construction mlkn capable learning multiple levels representations function compositions also figure illustration architecture three-layer mlkn. learning objective ﬁrst layers that random sample gone learned mappings images rkhs learned representation written form orthonormal labels encoded directions vectors. last layer kernel machine classiﬁer works representation internal representation distributed follows directly uniformity kernel machines within layer full connectivity layers. further appendix show like neural networks in-layer uniformity full inter-layer connectivity necessarily result kernel machines within layer learning identical solution functions. secondly mlkn layer images given random sample rkhs kernel mapping learned whereas traditional kernel machine completely determined kernel function. take previously described three-layer mlkn example consider kernel machine second layer naturally fully characterized noted ﬁxed learning images hand hence determined subject learning. thus except ﬁrst layer images rkhs layer optimized adjusting weights kernel machines preceding layer. view composed kernel mapping fact class kernel mappings even though introduced ﬁxed kernel layers respectively. equivalent conclude characterizes class kernel functions justiﬁes mlkn robust choice kernel another interpretation observation since subject learning span optimizing ﬁrst determines solution space utilize subspace worse layer enables span sense former contains approximation target function worse best latter. appendix justiﬁcation statement assumption observation effectively means contrast traditional kernel machines mlkn function kernel machine second layer implement longer limited span images training data corresponding rkhs. straightforward generalize construction converted multiple kernel learning algorithm spirit. simply kernels layer different. considering two-layer model example ﬁrst layer kernel machines form distinct kernels. arrangement kernel machine second layer denoted capable learning practically function universality. hence effectively fuse representations learned kernels practically arbitrary way. words combination base kernels considered arbitrary. note extra optimization problem solved kernel combination contrast methods. side effect lose model interpretability kernel learning result since longer explicit form combination base kernels. sense mlkn general algorithms mlkn need kernel involved able improve quality solution space mlkn also provides general framework introducing multiple kernels learning process. architectural constraints inherent multilayer neural networks resort backpropagation drive error information explicit output layer hidden layer make learning gradient descent possible. albeit conceptually simple efﬁcient backpropagation suffers vanishing gradient especially applied deep architectures. bottleneck successfully remedied neural networks piecewise linear activation functions rectiﬁed linear units greedy layer-wise pre-training ﬁnding solutions problem kernel-based learning machines popular research topic models usually shallow architecture friendly effective mathematical programming techniques make things worse mlkn gradient prone vanishing layers close input common kernel functions gaussian kernel highly nonlinear often exponential decay. solution propose supervised greedy learning algorithm mlkn consists single training phase layers trained time feedforward fashion. introducing algorithm describe couple concepts needed. ideal gram matrix given random sample belonging class exists positive deﬁnite kernel whose range contains induces ideal gram matrix deﬁned matrix always positive semideﬁnite since always factored simply taking matrix column eclass standard basis thus valid gram matrix induced kernel. since equals inner product corresponding image vectors kernel mapping deﬁned implies orthonormal directions vectors determined class labels class otherwise orthogonal. appendix detailed proof statement. notion ideal gram matrix established describe training objective using two-layer model example. recall kernel mapping second layer governed composed mapping characterizes kernel subject learning. naturally training objective gram matrix induced would valid maximizing alignment viable realize training objective. refer kernel that discrete case always induces positive semideﬁnite gram matrix kernel always induces positive deﬁnite gram matrix strictly kernel. gram matrix sample induced kakb. alignment viewed cosine angle vectors since easy check real matrices vector space deﬁnes inner product vector space. light cauchy-schwarz inequality suggests maximum alignment construction attained non-zero scalar multiplication elementwise. suggests able achieve perfect alignment range contain least non-zero value cannot furthermore always scale kernel preceding properties multiplying turn elements main diagonal gram matrix induced easily proved always assume without loss generality elements main diagonal identically assumption straightforward conclude perfect alignment attained consequently maximizing alignment realize desirable training objective train kernel machines ﬁrst layer information theoretic perspective alignment corresponds renyi’s quadratic mutual information means measuring divergence probability density functions images rkhs. note essentially contains true label information always measure alignment actual gram matrix without utilizing information second layer except function form kernel error information explicitly available training ﬁrst layer. conceptually justiﬁes fact backpropagation necessarily needed network. terms optimization representation-learning layer training cannot formulated convex optimization problem instead resort gradient descent. kernel machines ﬁrst layer trained kernel machine second layer trained classiﬁcation either radial basis function network iterative non-iterative method support vector manetwork layers training still begins ﬁrst layer proceeds pairwise feedforward fashion pair representationlearning layers training layer closer input identical described ﬁrst layer two-layer model kernel machines layer optimized φs+) last layer always trained classiﬁer. prove desirable property greedy training algorithm appendix namely prove certain assumptions representation-learning layer learn worse mapping preceding layers sense alignment maximization given kernels corresponding representation-learning layers denoted assume alignment layers calculated kernel layer alignment denoted suppose layer trained greedily gradient descent. layers fully trained gradient descent converged local global minimum according reasonable stopping criterion result guarantees deeper mlkn perform worse shallower counterparts terms maximizing empirical alignment despite training purely local layer. result somewhat justiﬁes beyond conceptual level training purely layer-wise effective learning approach mlkn arbitrary number layers. training algorithm summarized algorithm consider m-layer model example simplicity write denote parameters kernel machines layer technicality that since deﬁnition cost function something minimized negative alignment cost function learning algorithm. write negative alignment layer since function representation learned layer denoted |αi) kernel function subsequent layer denoted ideal gram matrix last layer |αm) universal approximator practically function even though mlkn. result holds true regardless whether kernels used mlmkl mkln same. apart difference architecture authors trained sets parameters mlmkl intervening fashion kernel machine parameters ﬁrst optimized quadratic programming kept ﬁxed mixing coefﬁcients trained using backpropagation together gradient descent. repeat process speciﬁed stopping criterion met. clear training approach likely suffers vanishing gradient. compare mlkn state-of-the-art algorithms including classic convex model kernels learned using extended level method proposed norm regularization kernel weights cutting plane algorithm second order taylor approximation adopted; generalized target kernel class hadamard product single gaussian kernel deﬁned dimension; inﬁnite kernel learning mkllevel embedded optimizer kernel weights; -layer multilayer kernel machine -layer inﬁnite -layer second method discussed section different version kernels iteratively added base kernels training. eleven binary classiﬁcation data sets widely used literature split evenly training test normalized zero mean unit variance prior training. runs identical settings random initializations repeated method. repetition training-test split selected randomly. attempts made building deep kernel machines. authors used consecutive kernel mappings construct composed kernel deﬁned kdeep φ)). mapping scheme generic kernels gaussian polynomial would fail provide nontrivial interpretation. specially engineered kernel proposed authors mimic processing data multilayer neural networks endowing kdeep many highly interesting interpretations. ﬁrst introducing idea building deep kernel machines work inspiring generalizable generic kernels. work inspired general proposed fusion idea deep kernel machines function form kernel machine second layer two-layer version proposed multilayer multiple kernel learning formalized apart focus mlmkl whereas work aims providing general framework constructions still different. mlmkl output ﬁrst layer under reaching second layer kernel machines. mixing coefﬁcients call optimization complicating training. extra linear functional mlmkl fact superﬂuous since kernel machine second layer mlkn already table average error rates runs standard deviations benchmarks. results overlapping conﬁdence intervals considered equivalent. best results marked bold. computing conﬁdence intervals limited sizes data sets pool random samples. mlkn results achieved using two-layer model number kernel machines ranging ﬁrst layer different data sets. second layer single kernel machine. kernel machines within layer gaussian kernel e−ax−y kernels layers differ kernel width train mlkn proposed greedy algorithm also backpropagation comparison. column named mlknbp corresponds results training layers simultaneously backpropagation crossentropy cost function. column named mlkngreedy contains results training proposed layer-wise learning algorithm. note subsection recommended train kernel machine second layer would yield optimal decision boundary theory. experiments hand trained second layer also gradient descent cross-entropy cost function fair comparison backpropagation. hyperparameters chosen cross-validation. algorithms compared data uses gaussian kernel whose width determined -fold cross-validation. algorithms base kernels contain gaussian kernels different widths features single feature polynomial kernels degree features single feature. lmklinf gaussian kernel added base kernels iteration. base kernel matrix normalized unit trace. lpmkl selected degree parameter chosen hyperparameters selected cross-validation. rather small size. nevertheless worth noting gaussian kernels used mlkn whereas algorithms except significantly kernels. corroborates earlier claim that effective approach search beyond span training data images within rkhs need multiple rkhs’s increase richness solution space. results greedy training mostly marginally better backpropagation expect performance widen training mlkn complicated tasks potentially layers theoretical guarantee greedy algorithm learning optimal representation generalization immune vanishing gradient. compared traditional kernel machines also many algorithms mlkn require computational resource construction depending number kernel machines within layer number layers network. mature methods speeding general kernel machines readily applied mitigitate issue degree also actively working solution tailored mlkn. variation empirically show deep architectures favorable complex tasks benchmarks show mlkn deep architecture itself competitive popular deep models including multilayer perceptron deep belief network stacked autoencoder data sets consist grayscale images. ﬁrst data known rectangles training images validation images test images. learning machine required tell rectangle contained image larger width length. location rectangle random. border rectangle pixel value pixels rest image value second data rectangles-image essentially rectangles except inside outside ractangle replaced image patch respectively. rectanglesimage training images validation images test images. third data convex consists images white regions black background. learning machine needs distinguish region convex. data training images validation images test images. sample images three data sets given figure actual training testing pixel values normalized detailed descriptions data sets experimental settings follows two-layer mlkn ﬁrst layer consisting kernel machines gaussian kernel second layer single kernel machine another gaussian kernel. model trained proposed greedy learning algorithm. kernel machine second layer instead trained gradient descent cross-entropy cost function fair comparison deep architectures. hyperparameters including kernel widths regularization coefﬁcients step size gradient descent selected using validation set. validation used ﬁnal training early-stopping based validation classiﬁcation error. layer sizes hyperparameters also selected using validation set. model selection size hidden layer ranges dbn- sizes three layers vary intervals respectively. means dimension representation spaces architectures signiﬁcantly larger mlkn- also note although number parameters kernel machine scales linearly size training mlkn- actually second smallest number parameters among models data sets dbn- sae- signiﬁcantly trainable parameters mlp- mlkn-. like training mlkn- validation also reserved early-stopping ﬁnal training. sae- dbn- pre-trained unsupervisedly supervised training phase following algorithms described detailed settings models reported table performance mlkn popular mature deep architectures even though hidden layer dimension number parameters mlkn used achieve results much smaller models. terms broadness function space induced deep architectures need distinguish since dense subsets bigger function space practical interest however since function space induced kernel machine inner product space much tractable theoretical analysis thanks many useful mathematical constructions orthogonality deﬁned inner product space. hence propose mlkn compete eliminate existing deep architecture hope make possible interesting theoretical discoveries unveil makes deep architectures powerful challenging machine learning problems. presented deep architecture based kernel machines. compared traditional kernel machines construction capable learning hierarchical distributed representations. moreover robust choice kernel. compared deep architectures model trained layer-wise fashion well-structured underlying function space making tractable theoretical analysis. reported favorable results multiple classiﬁcation benchmarks. larochelle erhan courville bergstra bengio empirical evaluation deep architectures problems many factors variation. proceedings international conference machine learning broomhead lowe radial basis functions multi-variable functional interpolation adaptive networks. technical report royal signals radar establishment malvern proofs proposition given random sample belonging arbitrary number classes kernel induces ideal gram matrix deﬁned section rkhs induced denoted images random sample kernel mapping following property orthogonal belong different classes; belong class. proof. this ﬁrst note class implies orthogonal whenever different classes. since class particular where course used norm induced inner product since unsigned restricted cauchy-schwarz φhφh equality holds nonzero constant using conclude equality holds result class implies class. proposition given random sample belonging arbitrary number classes assume unsigned kernel property gram matrix induced ideal gram matrix deﬁned section classes margin respect class linear functions smaller kernel mapping rkhs induced respectively random sample proof. first include deﬁnition margin completeness. using class real-valued functions classiﬁcation thresholding random sample labels taking values given margin example respect function given function class deﬁned minimum margins examples refered margin respect function class consists linear functions assuming inner product deﬁned space functions class linear margin function becomes min∈sw bias hyperplane. interpretation geometric smallest distance examples hyperplane. maximum margin functions denoted called margin respect consider kernel assumption. since unsigned inner product nonnegative φhφh angle hence angle vectors less equal since classes given random sample margin respect linear functions maximized orthogonal class. construction random sample. random sample since guaranteed induces ideal gram matrix corresponding margin larger remark condition relaxed nonzero constant since always scale kernel proposition given random sample kernels representation-learning layers deﬁned similarly deﬁned subsection assume alignment layers calculated kernel layer alignment denoted suppose layer trained greedily gradient descent. layers fully trained gradient descent converged local global minimum according reasonable stopping criterion proof. assume layers including layer fully trained layer-wise learning algorithm i.e. mappings compositon determined learning algorithm hence ﬁxed vectors ﬁrst show initialize initial state initialize identity restricted without loss generality assume amounts showing parameters kernel machines found kernel machines initialized certain parameters denote kernel machine ﬁrst layer strictly solving equation matrix parameters deﬁned gram matrix deﬁned matrix whose column data point straightforward parameters needed initializing ﬁrst layer desired state. moreover always solved analytically since strictly suggests invertible. numerically unstable invert resort iterative method initial parameters guaranteed identity restricted uniformly approximated this ﬁrst smallest subspace includes taking intersection subspaces include minimal subspace closed bounded construction hence also compact. further subspace banach space since consequently since identity restricted subspace bounded construction continuous hence declare minimal subspace. universality assumption representer theorem approximates identity uniformly optimal solution empirical risk deﬁned existence arbitrarily close approximation established practice optimize minimize convex optimization technique guaranteed optimal solution identity construction implemented resulting state optimization used initialize note case needs tolerate approximation error limitations iterative method used parameters. suppose ﬁrst layer trained greedy learning algorithm gradient descent successfully converged according reasonably given criterion. denote state ﬁrst layer training nature gradient namely always points direction maximum increase function remark note architecture learning algorithm described section dimension layer surely chosen freely alignment layer calculated using kernel next layer. however proposition implicitly assumed layers layer dimension. also assumed alignments layers calculated assumptions made need preceding proof. experiments hand never followed restrictions. besides always initialize layer randomly. however conclusion proposition always true practice. hence assumptions needed proof actually needed layer-wise learning algorithm possess desirable property described proposition lemma given random sample kernels deﬁned subsection approximating solution function given empirical risk minimization problem best approximation span worse proof. using shown proposition conclude initialized conclusion follows noting reasoning proposition easily generalized empirical risk apart alignment. also follows although lemma stated ﬁrst layer generalizable adjacent layers mlkn. proof. optimization nonconvex layer nonlinear straightforward. denote initial states initialization condition also proposition obvious probability equivalence true strictly smaller since optimization based gradiet descent sufﬁcient prove derivative cost function necessarily equal respect parameter derivative cost function respect corresponding parameter simplicity consider mean square error instead alignment cost function layer also valid cost function carry learning objective subsection proof alignment follows strategy derivation slightly cumbersome. without loss generality speciﬁc parameter kernel machine αsgk note proposition characterizes class kernels result potentially different gradient kernel machine within layer. surely kernels desirable property described here. easy enough check speciﬁc kernel. proposition two-layer mlmkl special case two-layer mlkn given sets functions mlmkl proof. without loss generality since kernels constructions universal assumption sufﬁcient prove following deﬁne linear functional compact subspace arbitrary. consider adjoint ∪ab∈rt inclusion automatic since composition continuous functions continuous prove inequality contradiction. suppose every exists preimages characterized identical. pick suppose valid choice. parameterizes circle centered around origin chosen parameterizes line suggests speciﬁes union lines cannot identical circle hence exist equal functions. hence ∪ab∈rt note proof taken account constraint kernel combination coefﬁcients positive mlmkl make mlmkl even smaller. equation gradient kernel machine iteration depends past state since initial states kernel machines different performance surface kernel machine multiple local minima probability kernel machines within layer converge local minimum strictly less proves proposition remark neural networks even uniformity within layers full connectivity layers derivatives parameters unit within layer necessarily identical across units suggested backpropagation particular chain rule. observation makes neural networks particularly interesting efﬁcient representation learning models since diversity solution functions possible even units layer identical construction. hand proposition shown mild conditions kernel machines layer likely distinct gradient information hence learn different solution functions enriching resulting internal representation layer. observation justiﬁes multiple kernel machines", "year": 2018}