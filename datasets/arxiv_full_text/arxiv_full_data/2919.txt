{"title": "Learning Spatiotemporal Features for Infrared Action Recognition with 3D  Convolutional Neural Networks", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "abstract": "Infrared (IR) imaging has the potential to enable more robust action recognition systems compared to visible spectrum cameras due to lower sensitivity to lighting conditions and appearance variability. While the action recognition task on videos collected from visible spectrum imaging has received much attention, action recognition in IR videos is significantly less explored. Our objective is to exploit imaging data in this modality for the action recognition task. In this work, we propose a novel two-stream 3D convolutional neural network (CNN) architecture by introducing the discriminative code layer and the corresponding discriminative code loss function. The proposed network processes IR image and the IR-based optical flow field sequences. We pretrain the 3D CNN model on the visible spectrum Sports-1M action dataset and finetune it on the Infrared Action Recognition (InfAR) dataset. To our best knowledge, this is the first application of the 3D CNN to action recognition in the IR domain. We conduct an elaborate analysis of different fusion schemes (weighted average, single and double-layer neural nets) applied to different 3D CNN outputs. Experimental results demonstrate that our approach can achieve state-of-the-art average precision (AP) performances on the InfAR dataset: (1) the proposed two-stream 3D CNN achieves the best reported 77.5% AP, and (2) our 3D CNN model applied to the optical flow fields achieves the best reported single stream 75.42% AP.", "text": "infrared imaging potential enable robust action recognition systems compared visible spectrum cameras lower sensitivity lighting conditions appearance variability. action recognition task videos collected visible spectrum imaging received much attention action recognition videos signiﬁcantly less explored. objective exploit imaging data modality action recognition task. work propose novel two-stream convolutional neural network architecture introducing discriminative code layer corresponding discriminative code loss function. proposed network processes image ir-based optical ﬁeld sequences. pretrain model visible spectrum sports-m action dataset ﬁnetune infrared action recognition dataset. best knowledge ﬁrst application action recognition domain. conduct elaborate analysis different fusion schemes applied different outputs. experimental results demonstrate approach achieve state-of-the-art average precision performances infar dataset proposed two-stream achieves best reported model applied optical ﬁelds achieves best reported single stream deep convolutional neural networks shown remarkable success various computer vision tasks static images object detection recognition segmentation encouraged success researchers proposed cnn-based algorithms action recognition visible spectrum videos promising approach using two-stream architecture developed consists spatial stream network learning salient appearance features video frames temporal stream network learning motion patterns. prediction computed averaging outputs networks. architecture showed improved performance traditional action recognition approaches improved dense trajectories features however pointed convolutions temporal network applied multiframe stacking optical ﬁelds generate representations; temporal network loses temporal information important action recognition ﬁrst convolution layer. address this introduced takes multiple frames inputs performs convolution pooling preserving temporal information. models process appearance motion information simultaneously hence able learn spatiotemporal features action recognition. action recognition infrared videos limited work uses deep cnns combine spatial temporal cues spatiotemporal feature learning applied two-stream cnns infrared action recognition. however stream processes infrared image sequence achieved worse performance several hand-crafted lowlevel features spatio-temporal interest point dense sift potential reasons this infrared infar dataset large enough learn spatiotemporal features leading severe overﬁtting; loses temporal information contained input video volume pointed properly model temporal action patterns. paper propose two-stream architecture learn spatio-temporal features infrared action recognition. two-stream contains separate recognition networks combined using late fusion. order reduce chance overﬁtting learn discriminative spatio-temporal features incorporate discriminative code loss introduced combine softmax classiﬁcation loss form objective function used network training. faster convergence training pretrained model parameters large-scale sports-m action dataset main contributions following develop two-stream architecture learn spatiotemporal features infrared videos. two-stream model learns representations capture spatial temporal information simultaneously. discriminative code layer last fully-connected layer combine discriminative code loss softmax classiﬁcation loss train cnn. discriminative code layer generates class-speciﬁc representations infrared videos. achieve state-of-the-art performances infar dataset. single layer trained using optical ﬁeld images achieve excellent infrared action recognition performance. many popular video feature extraction classiﬁcation approaches developed action recognition visible spectrum including low-level features scale-invariance feature transform optical ﬁelds improved dense trajectory feature high-level semantic concepts action parts recent years approaches based convolutional neural networks proposed action recognition two-steam architecture achieved impressive recognition performances. explored deeper two-stream network architectures reﬁned technical details yielding performance improvements. investigated effective fusion strategy space time steam networks. however state-of-the-art approaches focus action recognition visible light spectrum. compared visible spectrum imaging infrared imaging nice property work well even poor light conditions useful nighttime surveillance still limited works address action recognition infrared spectrum. trained classiﬁer trained visible light spectrum based bag-ofvisual-words video representation adapted infrared domain. recently applied two-stream architecture infrared videos. architecture employs motion-history-image stream network optical-ﬂow stream network extract image-level features. stream replaced infrared image stream action recognition performance becomes poor showing two-stream highly relies stream network. figure action recognition pipeline two-stream trained softmax classiﬁcation loss discriminative code loss outputs aggregated using late fusion. different introduce two-stream cnns model video appearance motion information simultaneously infrared video. best knowledge architectures explored action recognition infrared videos. addition integrated discriminative code loss objective function network training. makes learned representations discriminative good classiﬁcation tasks speciﬁcally action recognition. sources novelties enable action recognition system advances state-of-the performance action recognition videos evidenced experiments. pipeline action recognition videos presented figure pipeline inputs video clips obtained splitting frame sequences nonoverlapping segments consecutive frames. motivated success two-stream architectures operating visible spectrum videos derived optical ﬁelds convert consecutive pair frames optical ﬁeld. resize optical images height width feeding optical clips corresponding networks. infar dataset experiments used frame size creating input optical clips dimension number channels image temporal clip process corresponding optical clips using novel two-stream architecture. streams processed using model extends existing d-cnn architecture introduction additional discriminative code layer. discriminative code loss associated discriminative code layer combined softmax classiﬁcation loss train cnn. fuse probabilistic outputs softmax layers proposed two-stream network using weighted average single-layer neural network fusion two-layer fusion. compared architecture architecture better suited action recognition task models spatial temporal information jointly using convolution pooling operations. convolution architectures process multiple input frames different input channels transform representations; convolution transforms input volume representation preserving temporal information. best knowledge cnns applied task action recognition videos. two-stream architecture based model discriminative code layer presented figure follows architecture proposed network eight convolution layers combined using max-pooling layers. last pooling layer followed fully connected layers details sizes convolutional kernels numbers ﬁlters different convolutional layers sizes maxpooling fully connected layers provided section proposed architecture output layers softmax layer generates m-dimensional one-hot encoding activity categories discriminative code layer generates discriminative codes inputs signals. assuming discriminative code layer neurons training goal make generate dimensional p-hot encoding activity categories. p-hot encoding represents sample action category binary vector coordinates equal rest coordinates equal zero. intuitively group neurons activates sample corresponding category presented. order achieve this introduce discriminative code loss associated discriminative code layer activations. loss encourages groups output neurons activate simultaneously encoding category label. let’s assume architecture layers levels including convolution layers pooling layers fully connected layers layer including softmax layer discriminative code outputs. output layer denoted represents input. therefore network architecture concisely expressed represents network parameters convolution fully-connected layers linear operation non-linear activation function contains parameters linear transformation implemented discriminative code layer contains softmax layer parameters. predicted code overall loss function used network training linear combination softmax classiﬁcation loss discriminative code loss cost-balancing hyper parameter binary denotes label encoding indicates ideal activations neurons neuron assigned class neurons associated classes activated corresponding entries zero. note parameter learned cost component. network parameters )i=...n) trained back-propagation using mini-batch stochastic gradient descent method. compared parameter update equations multi-layer without discriminative cost loss gradient term i.e. changes introduced gradient terms nets. softmax layer directly provides probabilistic output propose method convert discriminative code layer outputs multinomial distribution action classes. given predicted code test sample nearest neighbors class training calculate average distances sample neighbor training samples class. then convert average distances sample-to-class similarity weights using gaussian kernel. finally obtain probability vector action categories normalization similarity weights. fuse probability outputs optical nets using simple weighted average approach. addition apply neural network based methods fuse predicted codes optical nets concatenate predicted codes nets obtained vector input single softmax layer neural network outputs probability estimates action classes concatenated predicted codes inputs two-layer neural network consisting convolution layer softmax output layer evaluate approach recently released infar video dataset collected using infrared cameras. dataset contains videos different action classes videos class. figure shows video examples dataset. first using dataset evaluate performance three widely used low-level descriptor features action class prediction task dense sift opponent sift motion features improved dense trajectories features then evaluate prediction performance semantic feature vector produced concept detectors figure video samples action classes infar action dataset action classes ‘ﬁght’ ‘handclap’ ‘handshake’ ‘hug’ ‘jog’ ‘jump’ ‘punch’ ‘push’ ‘skip’ ‘walk’ ‘wave’ ‘wave’. outputs concept score given low-level feature vector finally evaluate two-stream cnns discriminative code layer using different output fusion strategies. extract low-level image-based features dense sift opponent sift uniformly sample frames video. fisher vector encoding obtain video-level representations local lowlevel descriptors. video-level features computed spatio-temporal pooling frame-based features. addition low-level video representations extract features whose dimensions correspond measures evidence high-level concepts videos. extracting semantic concept features trained concept detectors utilizing videostory dataset detectors trained predict high-level concept features different fv-encoded features finally trained multiple linear multi-class classiﬁers predict actions different low-level features concept features. architectures flow identical follow assume convolutional layer kernel size ﬁlters stride max-pooling layer temporal size kernel spatial size kernel temporal stride spatial stride fully connected layer ﬁlters. softmax layer ﬁlters. discriminative code layer ﬁlters. main architecture follows –c–p –c–c– –c–c–p c–sm temporal length clip frames. parameter experiments. stream flow stream nets pretrained large-scale sports-m dataset ﬁnetuned infar dataset. learning rate training batch size weight decay coefﬁcient maximum iterations respectively. extract video-level features split video frame long clips without overlapping consecutive clips. video-level representations simply averaged representations extracted clip belonging video. classify actions ways employ softmax output layer produce conﬁdence scores action classes video clip average predict video-level class label employ k-nn classiﬁcation based video-level representation computed average predicted codes video clips belonging video follow standard setting randomly select samples category training rest testing. repeat experiments times report performance average ﬁnal performance paper. evaluation metrics used average precision average recognition precisions actions. evaluate static appearance features motion feature corresponding high-level semantic concept features extracted infrared videos. perform early fusion concatenating concept feature vectors obtained using concept detectors different low-level features. finally perform late fusion averaging posterior scores svms trained different features. table summarizes recognition performances approaches. high-level concept features achieved similar better performance compared corresponding low-level features. addition early fusion concept features provided similar results late fusion approach combined prediction scores classiﬁers. table recognition results d-cnns trained without discriminative code loss using different classiﬁcation methods. results ‘two-stream cnn-’ ‘two-stream cnn’ copied original paper stream d-cnn based architecture consists taking -frame clips inputs taking -frame sequences optical ﬁelds. trained ways using softmax classiﬁcation loss only; using softmax classiﬁcation loss discriminative code loss ﬁrst trained flow nets using softmax classiﬁcation only. called networks withdcl’ ‘flow without dcl’ respectively. maintain softmax layer networks discriminative code layer layer train stream networks referred net’ ‘flow net’. nets employ softmax k-nn classiﬁcation methods. performances different training classiﬁcation methods presented table shown table table without dcl’ still obtain marginally better results late fusion low-level features concept features. ‘flow without dcl’ obtain around improvement average precision compared partner stream demonstrate motion information important action recognition. ‘k-nn’ classiﬁcation method achieved better performance ‘softmax’ method increase inter-class distances compare results two-stream results reported ‘two-stream cnn-’ uses two-stream convolutional architecture consisting stream stream ours. ‘two-stream cnn-’ similar ‘two-stream cnn-’ stream replaced stream taking optical motion history images inputs. pointed model appearance motion simultaneously hence outperforms two-stream-cnn- trained images optical ﬁelds. two-stream-cnn- achieves good results motion history images. however result based optical ﬁeld only still comparable two-stream-cnn-. late fusion. given predicted code vector discriminative code layer compute average distances class based neighbor training samples class convert similarity vector using gaussian kernel function average distance class normalization factor. finally compute probability vector normalization similarity vector. experiment. parameter combine probability vectors different streams using simple weighted average rule. single-layer fusion. ﬁrst concatenate discriminative codes flow nets. then construct single softmax layer ‘shallow’ neural network concatenated discriminative codes inputs action classes outputs. two-layer fusion. train two-layer neural network consisting convolution layer softmax output layer concatenated discriminative codes flow nets inputs. convolution layer play role selecting good features input concatenated features. used ﬁlters convolutional layer. table presents results fusion strategies. simple weighted rule ‘late fusion’ ‘late fusion’ lead performance improvement single stream cnn. complex single-layer two-layer fusion methods outperform simple weighted average fusion rule likely limited size infar dataset since insufﬁcient training samples learn parameters convolution softmax layers figure visualization learned discriminative codes testing videos. axis indicates dimensions predicted codes position axis correspond test video. blue color indicates ‘low value’ color indicates ‘high value’. color color located bottom subﬁgure represents action class subset testing videos. simple average rule combine conﬁdence scores actions video method ‘late fusion’ scores generated method ‘early fusion concepts’ section achieve means high-level concept features provide complementary information features action recognition. figure visualizes learned discriminative codes testing videos using flow nets. discriminative predicted code matrix testing videos block-diagonal. axis indicates dimension predicted code position axis correspond test figure confusion matrices infrared action recognition. cnns net’ ‘flow net’ trained softmax loss discriminative code loss. weighted average rule used fuse net’s flow net’s prediction scores. figure effects parameter selection average precision performance infar dataset. effects parameter selection cost-balancing hyper-parameter effects parameter selection k-nn neighborhood size visually similar. ‘walk’ misclassiﬁed ’ﬁght’ possibly caused presence moving people background. fusion nets helps correcting typical misclassiﬁcations confusion between ‘handclap’ ‘wave’ classes. figure plot performance curves range parameter net. observe approach sensitive selection addition simple k-nn classiﬁcation scheme consistently outperforms ‘softmax’ classiﬁcation full range generated predicted codes using approach discriminative. figure show performances using different nets. approach sensitive increase inter-class distances discriminative code space. figure video examples classes high classiﬁcation precision infar dataset. pair images sampled different testing videos corresponding optical ﬁelds. stream trained softmax classiﬁcation loss discriminative code loss making extracted representations infrared videos become discriminative. nets initialized pretraining visible spectrum videos ﬁnetuned infrared videos. experiments show using even single stream achieve state-of-the-art performance infar dataset. goals future work extend current approach cross-spectral feature learning explore domain adaptation techniques effectively exploit high resource spectrum action recognition low-resource spectrum. work supported intelligence advanced research projects activity department interior national business center contract number dpc. u.s. government authorized reproduce distribute reprints government purposes notwithstanding copyright annotation thereon. disclaimer views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied iarpa doi/nbc u.s. government.", "year": 2017}