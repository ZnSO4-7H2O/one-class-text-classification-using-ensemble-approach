{"title": "Joint Causal Inference from Observational and Experimental Datasets", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We introduce Joint Causal Inference (JCI), a powerful formulation of causal discovery from multiple datasets that allows to jointly learn both the causal structure and targets of interventions from statistical independences in pooled data. Compared with existing constraint-based approaches for causal discovery from multiple data sets, JCI offers several advantages: it allows for several different types of interventions in a unified fashion, it can learn intervention targets, it systematically pools data across different datasets which improves the statistical power of independence tests, and most importantly, it improves on the accuracy and identifiability of the predicted causal relations. A technical complication that arises in JCI is the occurrence of faithfulness violations due to deterministic relations. We propose a simple but effective strategy for dealing with this type of faithfulness violations. We implement it in ACID, a determinism-tolerant extension of Ancestral Causal Inference (ACI) (Magliacane et al., 2016), a recently proposed logic-based causal discovery method that improves reliability of the output by exploiting redundant information in the data. We illustrate the benefits of JCI with ACID with an evaluation on a simulated dataset.", "text": "institute informatics university amsterdam amsterdam netherlands joris mooij institute informatics university amsterdam amsterdam netherlands introduce joint causal inference powerful formulation causal discovery multiple datasets allows jointly learn causal structure targets interventions statistical independences pooled data. compared existing constraint-based approaches causal discovery multiple data sets offers several advantages allows several different types interventions uniﬁed fashion learn intervention targets systematically pools data across different datasets improves statistical power independence tests importantly improves accuracy identiﬁability predicted causal relations. technical complication arises occurrence faithfulness violations deterministic relations. propose simple effective strategy dealing type faithfulness violations. implement acid determinism-tolerant extension ancestral causal inference recently proposed logic-based causal discovery method improves reliability output exploiting redundant information data. illustrate beneﬁts acid evaluation simulated dataset. keywords causal inference structure learning constraint-based causal discovery observational experimental data interventions discovering causal relations data foundation scientiﬁc method. traditionally causal relations either recovered experimental data variable interest perturbed purely observational data e.g. using seminal algorithms recent years several methods combining observational experimental data discover causal relations proposed showing combination improve greatly accuracy identiﬁability predicted causal relations. proposed methods score-based i.e. evaluate models using penalized likelihood score others constraint-based i.e. statistical independences express constraints possible models. work propose joint causal inference formulation causal discovery multiple datasets causal structure targets interventions jointly learnt independence test results pooled data. related approach already proposed scorebased methods eaton murphy extend constraint-based methods employed. goal combine idea joint inference observational experimental data eaton murphy advantages constraint-based methods score-based methods namely ability handle latent confounders selection bias naturally nonparametric approach especially case logic-based methods easy integration background knowledge. existing constraint-based methods multiple datasets typically learn causal structure dataset separately merge learnt structures merging process depends type interventions existing methods support interventions known targets. instead allows several different types interventions; learn intervention targets; systematically pools data across different datasets improves statistical power independence tests; improves identiﬁability accuracy predicted causal relations. hand poses challenges current constraint-based methods susceptibility violations causal faithfulness assumption. speciﬁcally induces faithfulness violations deterministic relations would typically result erroneous inferences standard constraint-based methods. propose simple effective strategy dealing type faithfulness violations. strategy applied constraint-based causal discovery method observational data handle partial inputs i.e. missing results certain independence test thus extending method handle combination observational experimental data. implement strategy acid determinism-tolerant extension recently proposed logic-based causal discovery method improves reliability output exploiting redundant information input. evaluation synthetic data show acid improves accuracy causal predictions respect simply merging separately learned causal graphs illustrating advantage joint causal discovery. section review useful concepts related work introduce notation rest paper. concepts described explained detail seminal books pearl spirtes following represent variables uppercase letters sets variables denoted boldface. throughout paper assume data generating process modeled causal directed acyclic graph contain latent variables. simplicity consider selection bias. directed edge causal represents direct causal relationship cause effect parent denote parents sequence directed edges directed path. directed path ancestor denote ancestors directed path denote sequence unique nodes pair xi+} connected edge called path. collider node path incoming arrow heads neighboring nodes path i.e. form xi+. node path called non-collider. variables extend deﬁnition parents union parents variable similarly deﬁne ancestors write exists least effect ancestor i.e. write disjoint sets random variables distributed according joint probability distribution denote conditional independence given conditional dependence often omit obvious context probability distribution referring call cardinality order conditional dependence relation. following assumptions constraint-based causal discovery thoroughly discussed literature connect conditional independences observational distribution d-separations underlying causal figure examples faithfulness violations left example show faithfulness violations functionally determined relation parent fully determines child right example binary variables function. conditioning fully determines even though ancestors creates series non-trivial faithfulness violations listed graph. graphs represent variables resulting structural equations without noise terms double circled node. assume causal markov causal faithfulness assumptions hold conditional independences observational distribution correspond one-to-one d-separations causal dag. setting favourable causal discovery thus assumptions usually made constraint-based approaches. although often reasonable causal faithfulness assumption violated cases notably common case deterministic relations among variables e.g. structural causal model equations noise term. faithfulness violations related determinism captured extension d-separation criterion d-separation criterion ﬁrst introduced geiger later extended spirtes causal markov assumption formulation d-separation presented geiger proven complete restricted setting determinism arises functionally determined relations deﬁned recursively variables fully determined parents precisely deﬁnition variable functionally determined given parents functionally determined show example faithfulness violations functionally determined relation figure following geiger represent variables resulting structural equations without noise terms double circled node. section book spirtes extend d-separation model also deterministic relations variables determined conditioning nonancestors. figure show example deﬁnition d-separation geiger fails capture faithfulness violation deterministic relation non-ancestors extended notion d-separation spirtes correctly captures although version d-separation spirtes retains completeness restricted case functionally determined relations proven complete general. nevertheless spirtes introduce several useful concepts handling general deterministic relations summarize ﬁndings here adapting notation. start assumption complete knowledge deterministic relations system. assumption complete deterministic relations among variables entry vn−} indicates variable deterministic function variables vn−} strict subset vn−}. example figure left {{x} figure right assumption restrictive seem ﬁrst practice easily reconstruct deterministic relations data using several standard methods. deﬁne function maps given variables variables determined deﬁnition given variables complete deterministic relations deﬁne detd variables determined according omit using obvious context referring note detd trivially includes itself. also variable constant value deﬁnition detd determined following spirtes extend d-separation deterministic relations. deﬁnition given three disjoint sets variables complete deterministic relations deﬁne d-separated w.r.t. denoted every path least following holds causal markov assumption formulation d-separation proven imply independence precisely markov respect complete deterministic relations hold case functionally determined relations version d-separation complete i.e. completely identiﬁes additional independences functionally determined relations setting reduces version d-separation geiger shown complete geiger given observational interventional datasets constraint-based methods handle multiple datasets learn causal structure dataset separately merge learned structures methods e.g. combine perform steps sequentially applying greedy procedure resolve potential conﬂict causal structures learnt ﬁrst step. others e.g. combine learning merging single procedure solving potential conﬂicts formulating optimization problem. recent approach etio combines aspects previous approaches learning merging single procedure using greedy algorithm resolving conﬂicts. merging causal structures learnt dataset separately several drawbacks respect method jointly datasets example score-based method eaton murphy first merging approaches require known targets cannot learn targets interventions since type information available considering multiple datasets jointly. moreover cannot take advantage certain interventional datasets e.g. case single data point interventional setting example happens popular genomics dataset important drawbacks loss statistical power separation smaller isolated datasets show examples section less identiﬁable relations respect joint causal inference method. related work special cases apply constraint-based methods mixtures observational experimental datasets problem systematically discussed formalized general framework yet. particular best knowledge existing work addresses learning intervention targets possibly jointly causal graph independence tests. approach described tian pearl learns causal structure combining standard constraint-based method observational data information extracted changes marginal probability variable. information extracted sequence datasets comparing pair datasets sequence assumption difference datasets mechanism change single known variable. quite restrictive assumption practice. approaches describe sufﬁcient although restrictive conditions pooling data change conditional distribution variables consideration. particular eberhardt describes naively pooling data different experimental settings discarding information experimental setting sample taken give wrong results. thus eberhardt proposes sufﬁcient condition allows pool data given independence test conditional distribution tested variables experimental conditions. lagani present approaches perform conditional independence tests separately dataset deﬁne pooled dependence disjunction single dependences; pool experimental conditions differ value intervened variable. approaches describe restrictive conditions pool datasets paper argue that done systematically e.g. show next section always pool available datasets. approaches like invariant causal prediction focus certain speciﬁc combinations independence tests performed jointly datasets. causal discovery method looks invariance across different experimental settings returning conservative subset ancestors given target variable main assumption conditional distribution given parents change different interventional settings assumption also referred invariance modularity since method searches patterns invariant across different settings safely pool together subset settings virtual experimental setting increase statistical power settings data. hand show examples section conservativeness estimates sometimes signiﬁcantly reduces causal information inferred. like makes invariance assumption allows combine different datasets. invariance assumption made causal structure invariant across experimental settings propose model jointly single causal graph observational experimental datasets {dr}r∈{...n}. assume unique underlying causal datasets deﬁned variables call system variables {xj}j∈x possibly hidden. dataset associated joint probability distribution prj∈x represents data points collected interventions possibly unknown intervention targets. context paper observational data simply datasets empty interventions. assume distribution markov faithful respect causal assumption precludes certain types interventions notably perfect interventions hand allows many types interventions e.g. soft interventions mechanism changes fat-hand interventions activity interventions etc. long induce dependences seen modiﬁcations underlying using terminology dawid call different distributions datasets regimes. related work different names used e.g. experimental conditions environments introduce types dummy variables data intervention variables {ii}i∈i deterministic functions regime intervention variables represent interventions performed dataset. absence information interventions performed datasets intervention variables indicator variables datasets. state main assumption jci. assumption assume causal relations system variables {xj}j∈x introduced dummy variables {ii}i∈i represented acyclic structural causal figure prototypical example setting four experimental datasets form pooled tabular form addition dummy variables causal representing causal structure system variables regime variable intervention variables intervention variable represents temperature experiment performed represents addition drug experiments. here system variable parents denote intervention parents exogenous parent distribution corresponding dataset given pj∈x denote corresponding causal causal markov assumption holds construction. show example figure model four datasets underlying causal structure. assumptions applicable value regime/intervention variables determined experimenter system variables measured. generally assumption system variables cannot cause regime/intervention variables. addition assume values experimenter chosen independently possible cause system variables. words assume confounders regime variable system variables intervention variables system variables. purposes causal discovery intended paper nothing else really distinguishes regime/intervention variables system variables considered random variables distribution regime/intervention variables reﬂects empirical distribution experimental design chosen experimenter. moreover distinction observational experimental datasets allowing several observational datasets possibly different contexts. table example experimental design matrix additional deterministic relations beyond ones allowed reduced version allowed deterministic relations right version joined iakt-inh single intervention variable idrug representing addition single drug deﬁnition deﬁne experimental design matrix matrix representing functional relations intervention variable corresponding probabilities regime variable assume intervention variables complete sense every effect regime variable mediated intervention variable. words assume direct effects system variables. general deterministic relations dummy variables arise. example consider example table left iakt-inh represents drug added regime number indicates another drug added regime even number. variables determine other. even though clear experimental design matrix visible causal inﬂuence diagram. paper focus special case allowing certain types deterministic relations. assume regime determines intervention variables {ii}i∈i. optionally allow additional deterministic relation namely intervention variables {ii}i∈i together determine regime assume deterministic relations. assumption deterministic relations hold joint distribution optionally {ii}i∈i deterministic relations hold joint distribution. practice often normalize system satisfy assumption. example experimental design matrix table left contains also deterministic relations allowed arise redundant intervention variables. table right shows joining intervention variables yield normalized system satisﬁes assumptions. correspond intervention variable function regime variable only. notion d-separation introduced geiger proved sounds complete causal markov assumption functionally determined relations. deterministic relations functionally determined relations notion d-separation equivalent spirtes here. thus statement follows. case show d-separations change removing deterministic relation regime variable determined intervention variables {ii}i∈i. denote complete deterministic relations according assume {ii}i∈i {ii}i∈i associated claim disjoint sets variables assume happen {ii}i∈i otherwise detd detd∗ d-separations identical deﬁnition. must exist path d-open w.r.t. d-closed w.r.t. means must contain non-collider. also otherwise path would closed w.r.t. since disjoint must contain least node adjacent must intervention variables {ii}i∈i. since intervention variables non-colliders assumptions {ii}i∈i must d-block hence arrived contradiction cannot d-open w.r.t. moreover cannot happen d-separations cannot additional d-separations w.r.t. compared d-separations w.r.t. therefore removing particular deterministic relation change d-separation statements hence completeness follows also case. conjecture general case arbitrary deterministic relations dummy variables e.g. table left d-separation deﬁned spirtes still complete leave proof future work. completeness d-separation important context motivates relaxation standard causal faithfulness assumption. setting standard assumption restrictive relax allow violations deterministic relations regime intervention variables. deﬁne relaxed version call d-faithfulness assumption follows assumption conjunction previous ones implies independences correspond one-to-one d-separations paves road constraint-based causal discovery. completeness d-separation suggests relaxation tight i.e. relax standard causal faithfulness assumption allow extra independences deterministic relations assumption more. consequence d-faithfulness intervention variables pairwise dependent excludes experimental design matrices e.g. matrix similar table practice often alleviate problem e.g. dropping data points. figure simple example improves identiﬁability consider datasets underlying soft intervention learn causal graphs dataset separately merge them e.g. described triantaﬁllou tsamardinos cannot learn causal direction dependent adds variables thus conditional independence tests allow distinguish direction here since w.l.o.g. represent details main text. assumptions deﬁne joint causal inference problem inferring causal distributions r=...n ﬁnite samples r=...n those. moreover call causal discovery method solve instance method. show section ideas behind previous approaches e.g. seen special cases jci. already mentioned formulating causal inference multiple datasets offers several advantages respect approaches causal graphs learnt separately dataset merged. advantages improved identiﬁability. section show examples where simplicity assume oracle inputs model regime variable figure show simple example improves identiﬁability. absence information intervention targets cannot identify causal direction variables learn structures separately combine them. case method able correctly reconstruct causal structure using additional conditional independence tests intervention variable speciﬁcally using background knowledge system variables cannot cause intervention variable i.e. latent confounders system variables infer confounders method supporting direct causal relations. note example since datasets simplicity represent datasets targets intervention known possible retrieve descendants checking variables change interventional dataset respect observational case. technique successfully applied retrieve list weighted ancestral relations could used background figure complex example improves identiﬁability background knowledge intervention targets e.g. know datasets intervened upon information extract extra background knowledge form ancestral relations. merging separately learnt causal structures extra knowledge still enough information recover causal structure instead method identify true causal structure precisely admg since represent details main text. although ancestral relations help simple cases previous example general case cannot reproduce results jci. example given figure knowing dataset intervention targets observing changes signiﬁcantly allows reconstruct targets causes enough reconstruct causal graph merging pags learnt dataset separately. instead method supports direct causal relations take advantage additional conditional independence tests regime variable infer complete proposition example figure method supports direct causal relations reconstruct correctly underlying causal graph precisely acyclic directed mixed graph oracle independence test results. proof readability provide proof appendix. local causal discovery simple algorithm searches variables satisfy pattern variable caused variable consideration. apply multiple observational experimental datasets soft interventions using since regime variable assumption caused variable. summarized figure examples overly conservative setting left example variable ﬁnds several sets satisfy e.g. intersection instead method supports direct causal relations correctly infer single parent right example naively adding intervention variables allow estimate ancestors variable would otherwise estimated correctly. extension approach used invariant causal prediction also considers regime variable model intervention variables given target variable directly intervened upon reformulate main idea behind search intersection sets absence confounders conservative estimate subset parents even causal faithfulness assumption violated. cannot exclude presence confounders setting requires causal faithfulness assumption provide estimate subset ancestors reformulation special case extends conservative estimate. principle could easily integrate conservative estimate method provide accurate estimates predictions leave future work. hand depending interventions available datasets overly conservative compared jci. besides restriction variable directly intervened upon dataset necessary cases provides overly conservative estimate ancestors. show examples figure speciﬁcally left example estimated ancestors variable hops away intervened variable empty method supports direct causal relations correct parent {x}. similarly shown right example naively adding intervention variables reduces applicability estimate ancestors variables directly intervened upon naive addition would allow learn intervention targets structure causal graph. work dealing faithfulness violations algorithm assumes causal sufﬁciency cannot handle background knowledge. logic-based constraint-based algorithms handle complex background knowledge causal insufﬁciency existing implementations cannot deal faithfulness violations deterministic relations. propose simple effective strategy dealing faithfulness violations deterministic relations. rephrase constraints constraint-based algorithm terms dseparations d-connections instead independence test results. testing time decide independence test result d-separation d-connection soundly derived provide d-separations d-connections input modiﬁed constraint-based algorithm. introducing rules derive sound d-separations d-connections input independence test results ﬁrst summarise basic properties conditional independence originally introduced dawid prove intermediate lemma. follow notation ordering recent publication lemma prove sound conversion d-separation statements d-separation statements theorem variables denote variables determined different variables disjoint det. causal markov d-faithfulness assumptions following holds ﬁrst equivalence follows causal markov d-faithfulness assumptions second based lemma third equivalence follows causal markov d-faithfulness assumptions last based deﬁnition d-separation reduces d-separation conditioning using result theorem introduce strategy dealing faithfulness violations deterministic relations. first rephrase constraint-based algorithm terms d-separations d-connections usually trivial change shown section convert problem possibly unfaithful independences problem possibly incomplete input. speciﬁcally derive subset sound d-separations d-connections independence test results follows corollary disjoint variables. variables determined causal markov d-faithfulness assumptions following holds simple strategy corollary applied constraint-based method providing deal partial inputs i.e. missing results certain independence tests. logic-based methods out-of-the-box partial inputs standard algorithms like would require possibly non-trivial extensions. anytime allows ignore dependences certain order order required available algorithm would also require possibly non-trivial extensions. implement strategy corollary ancestral causal inference determinism determinism-tolerant extension ancestral causal inference recently introduced logic-based method describing acid differs brieﬂy describe itself. rules shown sound assuming causal markov causal faithfulness assumptions. causal discovery reformulated optimization problem loss function optimized possible ancestral structures. given list weighted inputs e.g. conditional independences weighted conﬁdence loss function sums weights inputs violated candidate ancestral structure. addition provides method scoring causal predictions roughly approximates marginal probability. out-of-the-box able deal faithfulness violations deterministic relations thus cannot used jci. therefore propose ancestral causal inference determinism extends following strategy discussed section reformulate logical rules terms d-separation completely decoupling assumption relation dependences d-separations/connections e.g. causal faithfulness. rules call acid rules almost identical original rules except independences substituted d-separations dependences d-connections show following lemma disjoint variables changes interpretation implemented rules change code required. changes inputs sound d-separations d-connections derived corollary used inputs acid. similarly logic-based methods acid rules sound also partial inputs hand using partial inputs reduce completeness causal discovery. consider minor issue since focus prediction accuracy already known complete general case nevertheless shown obtain state-of-the-art accuracies improve identiﬁability accuracy predictions also background knowledge series logical rules describing causal structure regime intervention variables apply setting adding background knowledge provides simple means ruling several spurious candidate causal structures satisfy modeling assumptions. integration complex knowledge main advantages logic-based methods. refer combination acid rules acid-jci. intervention targets known also information background knowledge. example know inhibitor iakt−inh targets protein simply background knowledge iakt−inh akt. given ﬂexibility logic-based approaches also include complex cases. example mutually exclusive targets intervention background knowledge evaluate acid simulated data setting. simulator builds simulator used related work implements soft interventions unknown targets. combination number system variables interventions generate randomly linear acyclic models latent variables gaussian noise simulate soft interventions random targets. sample data points model randomly distributed experimental datasets observational dataset perform independence tests weight dependence statements using weighting schemes described magliacane setting evaluate causal discovery methods applied datasets unknown-target soft interventions. existing constraint-based methods apply setting assume perfect interventions known targets. moreover wish evaluate effect using respect merging separately learnt causal structures factoring effect different algorithms. therefore choose compare ancestral structure predicted acid-jci naive baseline based merged aci. baseline merge ancestral structures learnt dataset separately averaging scores causal predictions datasets. inputs algorithms provide weighted independence test results non-ancestral predictions using substantially improves accuracy. figure precision-recall curves synthetic data system variables interventions ancestral predictions non-ancestral predictions also setting substantially improves accuracy. figures report precision recall curves predicting ancestral relations non-ancestral relations different settings system variables interventions. ﬁgures acid-jci improves signiﬁcantly accuracy predictions respect baseline. expected interventional datasets available better accuracy acid-jci case figure datasets e.g. performance methods improves number system variables increases method gets smaller case figure hand variables e.g. figure acid-jci able predict correctly several ancestral relations identiﬁable otherwise. particular case predicted ancestral relations standard methods like merged explains curve ancestral relations figure starts recall. instead acid-jci able accurately predict relations illustrating joint causal inference framework leads statistical advantages also enlarges identiﬁable ancestral relations compared methods deal dataset separately. paper presented joint causal inference powerful formulation causal discovery multiple datasets unexploited constraint-based methods. current constraint-based methods cannot applied out-of-the-box faitfulness violations proposed simple strategy dealing type faithfulness violations. implemented strategy acid determinism-tolerant extension recently proposed causal discovery method applied acid showing beneﬁts evaluation simulated data. limitation assumption unique underlying causal precludes certain types interventions. several techniques extend formulation problem perfect interventions interventions induce independences. example given observational dataset could identify datasets perfect interventions noticing additional independences perform causal inference separately merge predictions similar methods presented hyttinen triantaﬁllou tsamardinos future work plan investigate techniques well techniques include fat-hand interventions induce dependences intervention targets. moreover plan investigate possible strategies extensions existing algorithms dealing faithfulness violations deterministic relations. finally although accurate ﬂexible logic-based methods limited number possible variables handle. introduces additional variables reducing scalability even more. plan investigate improvements execution times methods like acid. supported netherlands organization scientiﬁc research also supported dutch programme commit/ datasemantics project. also supported grant eu-fp grant agreement giorgos borboudakis ioannis tsamardinos. towards robust versatile causal discovery business applications. proceedings sigkdd international conference knowledge discovery data mining pages francisco claassen heskes. causal discovery multiple models different experiments. proceedings international conference neural information processing systems pages vancouver british columbia canada diego colombo marloes maathuis markus kalisch thomas richardson. learning highdimensional directed acyclic graphs latent selection variables. annals statistics gregory cooper. simple constraint-based algorithm efﬁciently mining observational databases causal relationships. data mining knowledge discovery gregory cooper changwon yoo. causal discovery mixture experimental observational data. proceedings fifteenth conference uncertainty artiﬁcial intelligence pages stockholm sweden daniel eaton kevin murphy. exact bayesian structure learning uncertain interventions. proceedings eleventh international conference artiﬁcial intelligence statistics juan puerto rico doris entner patrik hoyer peter spirtes. data-driven covariate selection nonparametric estimaproceedings sixteenth international conference artiﬁcial intelligence tion causal effects. statistics scottsdale alain hauser peter b¨uhlmann. characterization greedy learning interventional markov equivalence classes directed acyclic graphs. journal machine learning research antti hyttinen frederick eberhardt matti j¨arvisalo. constraint-based causal discovery conﬂict proceedings thirtieth conference uncertainty resolution answer programming. artiﬁcial intelligence pages quebec city quebec canada patrick kemmeren katrin sameith loes a.l. pasch joris benschop tineke lenstra thanasis margaritis eoghan o’duibhir apweiler sake wageningen cheuk sebastiaan heesch mehdi kashani giannis ampatziadis-michailidis mariel brok nathalie a.c.h. brabers anthony miles diane bouwmeester sander hooff harm bakel erik sluiters linda bakker berend snel philip lijnzaad leenen marian j.a. groot koerkamp frank c.p. holstege. large-scale genetic perturbations reveal regulatory networks abundance gene-speciﬁc repressors. cell vincenzo lagani ioannis tsamardinos soﬁa triantaﬁllou. learning mixture experimental data constraint–based approach. proceedings artiﬁcial intelligence theories applications hellenic conference lamia greece lemeire stijn meganck francesco cartella tingting liu. conservative independence-based causal structure learning absence adjacency faithfulness. international journal approximate reasoning proceedings sara magliacane claassen joris mooij. ancestral causal inference. advances neural information processing systems pages barcelona spain florian markowetz steffen grossmann rainer spang. probabilistic soft interventions conditional proceedings tenth international workshop artiﬁcial intelligence gaussian networks. statistics bridgetown barbados joris mooij heskes. cyclic causal discovery continuous equilibrium data. proceedings annual conference uncertainty artiﬁcial intelligence pages jonas peters peter b¨uhlmann nicolai meinshausen. causal inference using invariant prediction identiﬁcation conﬁdence intervals. journal royal statistical society series soﬁa triantaﬁllou ioannis tsamardinos. constraint-based causal discovery multiple interventions overlapping variable sets. journal machine learning research proposition example figure method supports direct causal relations reconstruct correctly underlying causal graph precisely acyclic directed mixed graph oracle independence test results. proof proof proceed steps ﬁrst reconstruct ancestral relations show subset relations actually direct causal relations ﬁnally show absence confounders. given simplicity ﬁrst step rules show detail lemma section background knowledge latent confounders system variables infer reasoning similar also infer rule lemma implies given background knowledge infer similarly infer since must imply since ancestral relations transitive moreover acyclicity ancestral relations statements imply completeness provide proofs acid rules slight modiﬁcations proofs rules. proofs already implicitly based d-separation concepts additional implicit steps based causal markov faithfulness assumptions conditional independence implies dseparation another d-separation implies conditional independence. reformulation make steps explicit decouple rules. lemma disjoint variables strengthened version rule note additional assumptions made redundant actually used proof. completeness give proof here. directed path paths blocked directed path must contain node hence contradiction exists path noncollider every collider ancestor exists collider ancestor collider closest ancestor note path d-connected given contradiction). subpath must d-connected given least outgoing edge follow edge along reaching either ﬁrst collider. collider reached follow directed path hence directed path i.e. assume must paths noncollider collider ancestor node closest also path collider ancestor noncollider path must blocked given would noncollider path would need order block however must also noncollider hence cannot therefore must collider path cannot ancestor show ancestor collider would ancestor contradiction. hence must outgoing arrow pointing towards encounter collider following directed edges contradiction collider hence would ancestor hence ancestor therefore", "year": 2016}