{"title": "Wembedder: Wikidata entity embedding web service", "tag": ["stat.ML", "cs.CL", "cs.LG", "I.2.4; H.3.5"], "abstract": "I present a web service for querying an embedding of entities in the Wikidata knowledge graph. The embedding is trained on the Wikidata dump using Gensim's Word2Vec implementation and a simple graph walk. A REST API is implemented. Together with the Wikidata API the web service exposes a multilingual resource for over 600'000 Wikidata items and properties.", "text": "sparql-based wikidata query service general functions ﬁnding related items generating ﬁxed-length features machine learning knowledge available. research combines machine learning wikidata e.g. mousselly-sergieh gurevych presented method aligning wikidata items framenet based wikidata labels aliases property suggestion running live wikidata editing interface helps editors recall appropriate properties items manual editing form recommender system. researchers investigated various methods process scholia https//tools.wmflabs.org/scholia/ service presents scholarly proﬁles based data wikidata extracted wdqs counting co-occurrence patterns sparql queries scholia list related items based query item. instance scholia lists related diseases based overlapping associated genes. countsparql-based methods scholia limited means show related items scholia user. several research groups provide word embedding services gpl-licensed webvectors uses flask gensim instances english russian http//rusvectores. org/ english norwegian http//vectors. nlpl.eu/explore/embeddings/. turku bionlp group provides flask-based word embedding service http //bionlp-www.utu.fi/wv_demo/ based english google news finnish corpora. service handling multilingual word embeddings also announced wembedder distinguished services using wikidata entities words embedding using live wikidata service provide multilingual labels entities. wembedder model setup wikimedia foundation provides wikidata dumps https//dumps.wikimedia.org/ wikidatawiki/entities/. setup initial model downloaded so-called truthy dumps available notation format. speciﬁc large compressed wikidata--truthy-beta.nt.bz. e.g. wmflabs.org/scholia/disease/q. abstract present service querying embedding entities wikidata knowledge graph. embedding trained wikidata dump using gensim’s wordvec implementation simple graph walk. rest implemented. together wikidata service exposes multilingual resource wikidata items properties. wordvec model spawned interest dense word representation low-dimensional space considerable number models beyond word level. recent avenue research domain uses knowledge graphs systems take advantage large knowledge graphs e.g. dbpedia freebase graph embedding. graph embedding simplest case would individual nodes network continuous low-dimensional space embedding knowledge graphs would typically handle typed links between knowledge items/nodes. wikidata https//www.wikidata.org/ relatively knowledge graph resource. wikimedia foundation also behind wikipedia thus wikidata regarded sister site wikipedia. wikipedia extensively used data text mining resource wikidata seen less machine learning contexts. several advantages wikidata. wikidata tied single language include labels hundreds languages item knowledge graph. such embedding works wikidata items principle multilingual another advantage wikidata item provide extra contextual data wikidata statements. search wikidata enabled string-based search engines wikidata well truthy dumps limited triples wikidata associated preﬁx. dump extracted triples subject object wikidata items i.e. leaving triples object value identiﬁer date name etc. generated contains lines http//www.wikidata.org/entity/ triple. preﬁxes http//www.wikidata.org/prop/direct/ stripped ﬁrst lines generated following content format similar magnus manske’s quickstatements format wordvec model kept gensim defaults. setup model ends vocabulary number includes properties wikidata items. gensim store model parameters ﬁles combined size megabytes. permanent version model parameters available zenodo ./zenodo.. service service python flask framework apache-licensed code available github repository https//github.com/fnielsen/wembedder. figure shows interface. version wembedder runs https //tools.wmflabs.org/wembedder/ i.e. cloud service provided wikimedia foundation. wembedder service relies wikidata https//www.wikidata.org/w/api.php wbsearchentities action searching items multiple languages implementation based search facility wikidata homepage. labels searching labels results generated ajax calls wikidata api. line regarded simple graph walk consisting single step wikidata item typed property next wikidata item. triple data regard sentence three words treated standard word embedding implementations. wordvec model gensim program initial model trained used cbow training algorithm embedding dimension window minimum count i.e. word must appear times included model. rest parameters rest implemented part wembedder returns json-formatted results e.g. /api/most-similar/q return similar entities query berners-lee also figure similarity computations implemented /api/similarity/q/q. earth venus compared. human interface wembedder uses rest ajax fashion returning html page empty result list javascript actual fetching results. evaluation embedding current version wembedder fairly simple compared state embeddings uses complex/holographic knowledge graph embedding multiple knowledge graphs pre-trained corpora-based resources building embedding expect wembedder perform state level comparison wordsim- dataset semantic relatedness evaluation shows poor performance pearson spearman correlations used evaluate wikidata graph embedding matching needed english wordsim- words wikidata items. straightforward usually semantic difference words items. often possible word english label wikidata item instance wordsim- word japanese must decide whether linked japanese language japanese people another item average items. attempted match words items left several unmatched word pairs possible analysis. correlations computed word pairs. skipgram trained model yielded even lower performance correlations pearson spearman correlations respectively. cbow model trained higher number iterations performed somewhat better correlations discussion future work wembedder—with -dimensional gensim model query—will usually able return results around second call considerably faster. means could used interactive related items search. sparql-based related items queries scholia usually takes several seconds. wikidata current state mostly encyclopedic source having little lexical information. state relational modeling conceptnet setup encyclopedic lexical knowledge graphs well corpus-based embeddings embeddings based wikidata could presumably perform better using link wikipedia different language versions wikipedia acting corpora. exist several works describing joint models words entities knowledge bases/graphs e.g. reference therein. work underway enable wikidata represent lexical information wikidata-based embedding beneﬁt data. web-interfaces vector semantic models webvectors toolkit. proceedings software demonstrations conference european chapter association computational linguistics specht empirical evaluation property recommender systems wikidata collaborative knowledge bases. proceedings international symposium open collaboration ˇrek sojka software framework topic modelling large corpora. challenges frameworks programme", "year": 2017}