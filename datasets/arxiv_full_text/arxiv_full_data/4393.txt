{"title": "Robust Kronecker-Decomposable Component Analysis for Low-Rank Modeling", "tag": ["stat.ML", "cs.CV"], "abstract": "Dictionary learning and component analysis are part of one of the most well-studied and active research fields, at the intersection of signal and image processing, computer vision, and statistical machine learning. In dictionary learning, the current methods of choice are arguably K-SVD and its variants, which learn a dictionary (i.e., a decomposition) for sparse coding via Singular Value Decomposition. In robust component analysis, leading methods derive from Principal Component Pursuit (PCP), which recovers a low-rank matrix from sparse corruptions of unknown magnitude and support. However, K-SVD is sensitive to the presence of noise and outliers in the training set. Additionally, PCP does not provide a dictionary that respects the structure of the data (e.g., images), and requires expensive SVD computations when solved by convex relaxation. In this paper, we introduce a new robust decomposition of images by combining ideas from sparse dictionary learning and PCP. We propose a novel Kronecker-decomposable component analysis which is robust to gross corruption, can be used for low-rank modeling, and leverages separability to solve significantly smaller problems. We design an efficient learning algorithm by drawing links with a restricted form of tensor factorization. The effectiveness of the proposed approach is demonstrated on real-world applications, namely background subtraction and image denoising, by performing a thorough comparison with the current state of the art.", "text": "taken factorized form obtain range different models depending choice regularization. imposing sparsity dictionary leads sparse sparsity code yields sparse dictionary learning. current methods choice sparse dictionary learning k-svd variants seek overcomplete dictionary sparse representation minimizing following constrained objective ||.||f frobenius norm ||.|| pseudonorm counting number non-zero elements. problem solved iterative manner alternates sparse coding data samples current dictionary process updating dictionary atoms better data using singular value decomposition used reconstruct images k-svd trained overlapping image patches allow overcompleteness problems form suffer high computational burden limits applicability small patches capture local information prevent scaling larger images. recent developments robust component analysis attributed advancements compressive sensing area emblematic model robust principal component analysis proposed rpca assumes observation matrix lowrank component sparse matrix collects gross errors outliers. finding minimal rank solution sparsest solution combinatorial np-hard. therefore rpca obtained solving principal component pursuit discrete rank -norm functions approximated convex envelopes follows dictionary learning component analysis part well-studied active research ﬁelds intersection signal image processing computer vision statistical machine learning. dictionary learning current methods choice arguably k-svd variants learn dictionary sparse coding singular value decomposition. robust component analysis leading methods derive principal component pursuit recovers lowrank matrix sparse corruptions unknown magnitude support. however k-svd sensitive presence noise outliers training set. additionally provide dictionary respects structure data requires expensive computations solved convex relaxation. paper introduce robust decomposition images combining ideas sparse dictionary learning pcp. propose novel kronecker-decomposable component analysis robust gross corruption used low-rank modeling leverages separability solve signiﬁcantly smaller problems. design efﬁcient learning algorithm drawing links restricted form tensor factorization. effectiveness proposed approach demonstrated real-world applications namely background subtraction image denoising performing thorough comparison current state art. sparse dictionary learning robust principal component analysis popular methods learning representations assume different structure answer different needs special cases structured matrix factorization. assuming data samples represented columns matrix seek decomposition meaningful components given structure generic form paper propose novel method separable dictionary learning based robust tensor factorization learns simultaneously dictionary sparse representations. extend previous approaches introducing additional level structure make model robust outliers. seek overcompleteness rather promote sparsity dictionary learn lowrank representation input tensor. regard method combines ideas sparse dictionary learning robust pca. propose non-convex parallelizable admm algorithm provide experimental evidence effectiveness. finally compare performance method several tensor matrix factorization algorithms computer vision benchmarks show model systematically matches outperforms state art. make code available online along supplementary material. notations throughout paper matrices denoted uppercase boldface letters e.g. denotes identity matrix compatible dimensions. column denoted tensors considered multidimensional equivalent matrices vectors denoted bold calligraphic letters e.g. order tensor number indices needed address elements. consequently element th-order tensor addressed indices i.e. ii...im xii...im sets real integer numbers denoted respectively. th-order real-valued tensor deﬁned tensor space ri×i×···×im mode-n product tensor ri×i×...×im matrix rj×in denoted deﬁned element-wise finally deﬁne tensor tucker rank vector ranks mode-n unfoldings tensor multi-rank vector ranks frontal slices. details tensors deﬁnitions tensor slices mode-n unfoldings found example. ||.||∗ nuclear norm ||.|| standard norm. like sparse dictionary learning robust special case nuclear-norm relaxation convex cost performing svds full-size matrix high. factorization-based formulations exploit fact rank-r matrix rm×n decomposed rm×r rn×r impose low-rankness. formulations non-convex algorithms stuck local optima saddle points. refer recent developments convergence non-convex matrix factorizations lowrank problems speciﬁcally matrix completion. analytical separable dictionaries proposed generalizations -dimensional transforms form e.g. tensor-product wavelets since fallen ﬂavour models drop orthogonality favour overcompleteness achieve geometric invariance however recent work compressed sensing literature shown regain interest separable dictionaries high-dimensional signals scalability hardrequirement. notably high-resolution spatial angular representations diffusion-mri signals hardi represent voxel signal yielding prohibitive sampling times. traditional compressed sensing techniques handle signal sparsity bounded number voxels still cannot meet needs medical imaging kronecker extensions orthogonal matching pursuit dual admm fista allow sparser signals. recent separable dictionary learning considers dictionary factorizes kronecker product smaller dictionaries matrix observations observations sparse representations bases learning problem recast +λg+κr+κr regularizers promote respectively sparsity representations mutual-coherence dictionary here constrained orthogonal columns i.e. pair shall product manifold product sphere manifolds. different approach taken separable dictionary learnt two-step strategy similar k-svd. matrix observation represented aribt. ﬁrst step sparse representations found omp. second step decomposition performed tensor residuals regularized alternating least problem jointly convex convex component individually. resort alternatingdirection method propose non-convex admm procedure operates frontal slices. minimizing a||f presents challenge product high-dimensional bases coupled loss non-smooth. using identity b||p ||a||p||b||p ||.||p denotes schatten-p norm remarking ||b||f||a||f ||a|| minimize simpler upper bound resulting sub-problems smaller therefore scalable. order obtain exact proximal steps encodings introduce split variable thus solve consider following sparse dictionary learning problem frobenius-norm regularization dictionary decompose observations rmn×rr representations ||ri|| ||d||f suppose observations obtained vectorizing two-dimensional data images i.e. rm×n. preferable keep observations matrix form preserves spatial structure images allows solve matrix equations efﬁciently instead high-dimensional linear systems. without loss generality choose rr×r recast problem matrices concatenated frontal slices -way tensors. figure illustrates decomposition. impose natural upper bound rank frontal slices mode- mode- ranks. efﬁcient algorithm recognize equation stein equation solved cubical time quadratic space solvers discrete-time sylvester equations hessenberg-schur method instead naive time space solution vectorizing equation linear system. practice solve slightly different problem ||ei||. introduces additional degree freedom corresponds rescaling coefﬁcients found modiﬁed problem numerically stable allow tuning relative importance ||r|| ||e||. obtain algorithm show experimentally algorithm successfully recovers components decomposition prove formulation encourages different notions low-rankness tensors ﬁnally discuss issues non-convexity convergence. generated synthetic data following model’s assumptions ﬁrst sampling random bases known ranks gaussian slices core forming ground truth modeled additive random sparse laplacian noise tensor whose entries probability equal probability otherwise. generated data leading noise density respectively measured reconstruction error density varying values model achieved near-exact recovery exact recovery density suitable values experimental evidence presented figure noise case. algorithm appears robust small changes suggests value lead optimal results simple criterion provides consistently good reconstruction derived robust noise case observe increase density increases error order seeing model perspective robust seeks low-rank representation dataset minimize rank low-rank tensor precisely show theorem simultaneously penalize tucker rank multi-rank proof. equivalence norms ﬁnite-dimensions +||a b||∗ k||a b||f. minimise λ||e|| α||r|| b||f. choosing penalize rank rankrank. given rank non-negative integer rank rank decreases necessarily. therefore minimize mode- mode- ranks additionally rank rank rank). exhibiting valid help choice paramen||x||. deﬁnition schatten-p norm norm singular values. recalling nuclear norm frobenius norm schatten- schatten- norms singular values hence result. convergence initialization problem nonconvex therefore global convergence priori guaranteed. recent work studies convergence admm non-convex possibly non-smooth objective functions linear constraints. here constraints linear. authors derive conditions global optimality speciﬁc non-convex matrix tensor factorizations extended formulations including model. however theoretical study global convergence algorithm scope paper left future work. instead provide experimental results discuss strategy implemented. propose simple initialization scheme wake initialize bases core performing observation uisivt initialize dual-variables constraint aribt take scaling coefﬁcient chosen practice similarly chose correspond averaging initial values individual slice corresponding constraint. convergence criterion corresponds primal-feasibility problem given errrec ||ri−ki|| maxi empirically obtained systematic convergence good solution linear convergence rate shown figure parameter tuned affects speed convergence. high values lead algorithm diverging values closer lead increased number iterations. used upper bounds usual admm. computational complexity time space complexity iteration algorithm minr since terms asymptotically negligible practice useful know computational requirements scale size dictionary. similarly initialization procedure cost o+)+mn)+mn) time needs quadratic space slice assuming standard algorithm used svd. several steps algorithm summations independent terms trivially distributed mapreduce way. proximal operators separable nature therefore parallelizable. consequently highly parallel distributed implementations possible computational complexity reduced adaptively adopting sparse linear algebra structures algorithms. image denoising background subtraction. baseline report performance matrix robust implemented inexact non-negative robust dictionary learning chose following methods include recent representatives various existing approaches low-rank modeling tensors singleton version higher-order robust optimizes tucker rank tensor nuclear norms unfoldings. authors consider similar model robust m-estimators loss functions either cauchy loss welsh loss support hard soft thresholding; tested soft-thresholding models non-convex tensor robust adapts tensors matrix nonconvex rpca finally tensor rpca algorithms work slightly different deﬁnitions tensor nuclear norm convex surrogate tensor multi-rank. model identiﬁed maximum parameters tune grid-search order keep parameter tuning tractable. criteria heuristics choosing parameters provided authors chose search space around value obtained them. cases tuning process explored wide range parameters maximize performance. range values investigated provided supplementary material. performance method signiﬁcantly worse other result reported clutter text made available supplementary material. case separable dictionary learning whose drastically different nature renders unsuitable robust low-rank modeling compared completeness. reason compare method k-svd background subtraction common task computer vision tackled robust low-rank modeling static mostly static background video sequence effectively represented low-rank tensor foreground forms sparse component outliers. experimental procedure compared algorithms benchmarks. ﬁrst excerpt highway dataset consists video sequence cars travelling highway; background completely static. kept gray-scale images re-sized pixels. second airport hall dataset chosen challenging benchmark since background fully static scene richer. used excerpt frames kept frames original size pixels. tion problem. since ground truth frames available excerpts report videos. value experiments. results provide original ground truth recovered frames figure hall experiment table presents scores algorithms ranked order mean performance benchmarks. matrix methods rank high benchmarks half tensor algorithms match outperform baseline. proposed model matches best performance highway dataset provides signiﬁcantly higher performance challenging hall benchmark. visual inspection results show kdrsdl method doesn’t fully capture immobile people background therefore achieves best trade-off foreground detection background-foreground contamination. many natural artiﬁcial images exhibit inherent low-rank structure suitably denoised low-rank modeling algorithms. section assess performance cohort datasets chosen popularity typical cases represent. noise level nearly every method provided good excellent recovery original images. therefore omit noise level hand methods notable exception kdrsdl trpca trpca failed provide acceptable reconstruction gross corruption case. thus present denoised images level compare performance three best performing methods table noise level. clear differences appeared noise level demonstrated quantitative metrics visual inspection figure overall performance markedly lower level methods started lose much details. visual inspection results conﬁrms higher reconstruction quality kdrsdl. invite reader look texture skin white reﬂection light subject’s skin pupil. latter particular close nature white pixel corruption salt pepper noise. methods kdrsdl provided best reconstruction quality algorithm removed noise aforementioned details distinguishable reconstruction. report quantitative metrics designed measure aspects image recovery. peak signal noise ratio used indicator element-wise reconstruction quality signals feature similarity index evaluates recovery structural information. quantitative metrics perfect replacements subjective visual assessment image quality; therefore present sample reconstructed images veriﬁcation. measure choice determining images compare visually fsim higher correlation human evaluation psnr. monochromatic face images face denoising experiment uses extented yale-b dataset different subject different lighting conditions. according face images subject various illuminations approximately -dimensional subspace therefore suitable low-rank modeling. used pre-cropped images ﬁrst subject kept full resolution. resulting collection images constitutes -way tensor images size mode corresponds respectively columns rows image illumination component. three expected low-rank spatial correlation within frontal slices correlation images subject different illuminations. present comparative quantitative performance methods tested figure provide visualizations reconstructed ﬁrst image noise level figure report metrics averaged images. constructions conﬁrms difference image recovered kdrsdl noise level comparable output competing algorithms noise level. color image denoising benchmark facade image rich details lighting makes interesting assess reconstruction. geometric nature building’s front wall strong correlation bands indicate data modeled low-rank -way tensor frontal slice color channel. noise level kdrsdl attained highest psnr among highest fsimc values. methods provided excellent reconstruction agreement high values metrics shown figure previous benchmark results omitted because space constraints noise level cauchy exhibited highest psnr trpca scored best fsimc metric. kdrsdl second highest psnr among highest fsimc. details results provided figure visually clear differences visible best seen details picture black iron ornaments light coming window. method best preserved dynamics lighting sharpness details provided reconstruction visually closest original. competing models tend oversmooth image make light dimmer; indicating substantial losses high-frequency dynamic information. kdrsdl appears also provide best color ﬁdelity. gross-corruption case kdrsdl best performer psnr fsimc. seen figure kdrsdl method trpca horpca-s provide reconstruction distinguishable details best. method propose combines aspects robust principal component analysis sparse dictionary learning. k-svd algorithm learns iteratively alternatively dictionary sparse representations. similarly robust method assumes data decompose additively sparse low-rank component able separate signals. introduce two-level structure dictionary allow scalable training robustness outliers. imposing structure exhibits links tensor factorizations allows better model spatial correlation images classical matrix methods. theoretical advantages translate directly experimental performance method exhibits desirable scalability properties matches outperforms current state lowrank modeling.", "year": 2017}