{"title": "Twin Learning for Similarity and Clustering: A Unified Kernel Approach", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Many similarity-based clustering methods work in two separate steps including similarity matrix computation and subsequent spectral clustering. However, similarity measurement is challenging because it is usually impacted by many factors, e.g., the choice of similarity metric, neighborhood size, scale of data, noise and outliers. Thus the learned similarity matrix is often not suitable, let alone optimal, for the subsequent clustering. In addition, nonlinear similarity often exists in many real world data which, however, has not been effectively considered by most existing methods. To tackle these two challenges, we propose a model to simultaneously learn cluster indicator matrix and similarity information in kernel spaces in a principled way. We show theoretical relationships to kernel k-means, k-means, and spectral clustering methods. Then, to address the practical issue of how to select the most suitable kernel for a particular clustering task, we further extend our model with a multiple kernel learning ability. With this joint model, we can automatically accomplish three subtasks of finding the best cluster indicator matrix, the most accurate similarity relations and the optimal combination of multiple kernels. By leveraging the interactions between these three subtasks in a joint framework, each subtask can be iteratively boosted by using the results of the others towards an overall optimal solution. Extensive experiments are performed to demonstrate the effectiveness of our method.", "text": "spectral clustering low-dimensional embedding similarity matrix data performing k-means clustering similarity every pair points input leverages manifold information clustering model. thus similarity-based clustering methods usually show better performance k-means algorithm. however performance kind methods largely determined similarity matrix variations similarity measurement metric neighborhood size data scale lead suboptimal performance. recently self-expression successfully utilized subspace recovery rank representation recommender systems represents data point terms points. solving optimization problem similarity information automatically learned data. approach reveal low-dimensional structure also robust noise data scale paper perform clustering built upon idea using samples data express itself. rather local structure learning approach extracts global structure data extended kernel spaces. unlike existing clustering algorithms work separate steps simultaneously learn similarity matrix cluster indicators imposing rank constraint laplacian matrix learned similarity matrix. leveraging intrinsic interactions between learning similarity cluster indicators proposed model seamlessly integrates joint framework result task used improve one. capture nonlinear structure information inherent many real world data sets directly develop method kernel space well known ability explore nonlinear relation. design efﬁcient algorithm optimal solution model show theoretical analysis connections kernel k-means k-means spectral clustering methods. many similarity-based clustering methods work separate steps including similarity matrix computation subsequent spectral clustering. however similarity measurement challenging usually impacted many factors e.g. choice similarity metric neighborhood size scale data noise outliers. thus learned similarity matrix often suitable alone optimal subsequent clustering. addition nonlinear similarity often exists many real world data which however effectively considered existing methods. tackle challenges propose model simultaneously learn cluster indicator matrix similarity information kernel spaces principled way. show theoretical relationships kernel k-means k-means spectral clustering methods. then address practical issue select suitable kernel particular clustering task extend model multiple kernel learning ability. joint model automatically accomplish three subtasks ﬁnding best cluster indicator matrix accurate similarity relations optimal combination multiple kernels. leveraging interactions between three subtasks joint framework subtask iteratively boosted using results others towards overall optimal solution. extensive experiments performed demonstrate effectiveness method. clustering fundamental topic data mining machine learning partitions data points different groups objects within group similar another different groups. various methods proposed past decades. well-known algorithms include k-means clustering spectral clustering hierarchical clustering thanks simplicity effectiveness kmeans algorithm widely used. however fails identify arbitrarily shaped clusters. kernel k-means developed capture nonlinear structure information hidden data sets. kernelbased learning methods requires specify kernel means assumes certain shape underlying copyright association advancement artiﬁcial intelligence rights reserved. data samples input space reproducing kernel hilbert space containing samples transformation kernel similarity data samples deﬁned predeﬁned kernel kxixj easy observe similarities computed exclusively using kernel function need know transformation known kernel trick greatly simpliﬁes computations kernel space kernels precomputed. becomes ideally expect number connected components exactly given data consists clusters however solution might satisfy desired property. therefore another constraint based following theorem theorem multiplicity eigenvalue laplacian matrix equal number connected components graph similarity matrix positive semi-deﬁnite thus i-th smallest eigenvalue rank easy enforce constraint optimization problem rank constraint known combinatorial complexity. mitigate difﬁculty incorporates rank constraint objective function regularizer. motivated consideration relax constraint reformulate model effective kernel often enormous inﬂuence performance kernel method. unfortunately suitable kernel speciﬁc task usually unknown advance. exhaustive search user-deﬁned pool kernels time-consuming impractical sizes pool data become large thus propose multiple kernel algorithm model. another beneﬁt applying multiple kernels fully utilize information different sources equipped heterogeneous features alleviate effort kernel construction integrating complementary information learn appropriate consensus kernel linear combination multiple input kernels. result joint model simultaneously learn similarity information cluster indicator matrix optimal combination multiple kernels. extensive empirical results real-world benchmark data sets show method consistently outperforms stateof-the-art methods. notations. paper matrices written upper case letters vectors represented boldface lowercase letters. i-th column element matrix denoted respectively. -norm vector deﬁned means transpose. denotes identity matrix denotes column vector elements one. trace operator. means elements range weight j-th sample. similar data points receive bigger weights weights smaller less similar points. thus also called similarity matrix represents global structure data. note similar spirit locally linear embedding assumes data points manifold data point expressed linear combination nearest neighbors. difference lies fact specify neighborhood automatically determined method. drawback assumes linear relations between samples. recover nonlinear relations data points extend kernel spaces deploying general kernelization framework deﬁne kernel mapping connection kernel k-means k-means introduce theorem states equivalence scsk kernel k-means k-means condition. theorem problem equivalent kernel k-means problem. proof. constraint rank makes solution block diagonal. rni×ni denote similarity matrix i-th component number data samples component. problem equivalent solving following problem easy deduce mean cluster kernel space. therefore exactly kernel kmeans. thus proposed algorithm solve kernel k-means problem corollary linear kernel adopted problem equivalent k-means problem. rn×c indicator matrix. elements i-th measure membership data point belonging clusters. finally model twin learning similarity clustering single kernel formulated algorithm algorithm scmk input kernel matrices {ki}r initialize random matrix repeat calculate update smallest eigenvectors update i-th column calculate update stopping criterion met. optimal solution obtained eigenvectors corresponding smallest eigenvalues. generally directly used clustering since exactly connected components. obtain ﬁnal clustering results k-means discretization procedures must performed proposed algorithm similarity matrix predeﬁned existing spectral clustering methods literature. also similarity matrix learned taking account clustering task hand opposed existing subspace clustering methods literature focus learning similarity matrix without considering effect clustering method graph learned partitioned connected components using optimal solution formed eigenvectors deﬁned corresponding smallest eigenvalues. therefore proposed algorithm learns similarity matrix cluster indicator matrix simultaneously coupled leads better result real applications existing spectral methods shown experiments since learns adaptive graph clustering. although model automatically learn similarity matrix cluster indicator matrix performance largely determined choice kernel. often impractical exhaustively search suitable kernel. moreover real world data sets often generated different sources along heterogeneous features. single kernel method able fully utilize information. multiple kernel learning capable integrating complementary information identifying suitable kernel given task. present learn appropriate consensus kernel convex combination several predeﬁned kernel matrices. functions ferent kernel spaces denoted {hi}r suppose total number different kernel correspondingly would difi=. augmented constructed concatenating kernel spaces using mapping wrφr]t difφ combined kernel ferent weights represented note convex combination positive semideﬁnite kernel matrices still positive semideﬁnite kernel matrix. thus combined kernel still satisﬁes mercer’s condition. propose joint similarity data sets altogether eight benchmark data sets used experiments. table summarizes statistics data sets. among them image ones three text corpora. image data sets consist four commonly used face databases jaffe) binary alpha digits data experiment setup assess effectiveness multiple kernel learning design kernels include seven gaussian kernels form expx max)) dmax maximal distance samples varies linear kernel four polynomial kernels furthermore kernels rescaled dividing element largest pair-wise squared distance. http//www-users.cs.umn.edu/ han/data/tmdata.tar.gz http//www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html http//vision.ucsd.edu/content/yale-face-database http//www.ece.ohio-state.edu/ aleix/ardatabase.html http//www.kasrl.org/jaffe.html http//www.cs.nyu.edu/ roweis/data.html https//github.com/sckangz/aaai spectral clustering like aasc kmeans spectral embedding obtain clustering results. reduce inﬂuence initialization follow strategy suggested repeat clustering times present results best objective values. number clusters true number classes clustering algorithms. clustering result table shows clustering results terms accuracy purity data sets. seen proposed scsk scmk produce promising results. especially method substantially improve performance jaffe data sets. difference best average results conﬁrms fact choice kernel huge inﬂuence performance single kernel methods. difference motivates development multiple kernel learning method. besides multiple kernel clustering approaches usually improve results single kernel clustering methods. http//imp.iis.sinica.edu.tw/ivclab/research/sean/mkfc/code http//imp.iis.sinica.edu.tw/ivclab/research/sean/aasc/code https//github.com/csliangdu/rmkkm table clustering results measured benchmark data sets. ’-a’ denotes average performance kernels. best results single kernel multiple kernel methods highlighted boldface. parameter selection models. e-}. figure shows clustering results scmk terms purity vary jaffe yale data sets. observe performance scmk stable respect large range values sensitive value paper ﬁrst propose clustering method simultaneously perform similarity learning cluster indicator matrix construction. method similarity learning cluster indicator learning integrated within framework; method easily extended kernel spaces capture nonlinear structure information. connections proposed method kernel k-means kmeans spectral clustering also established. avoid extensive search best kernel incorporate multiple kernel learning model. similarity learning cluster indicator construction kernel weight learning boosted using results two. extensive experiments conducted real-world benchmark data sets demonstrate superior performance method.", "year": 2017}