{"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative  Models", "tag": ["stat.ML", "cs.AI", "cs.LG", "stat.CO", "stat.ME"], "abstract": "We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.", "text": "marry ideas deep neural networks approximate bayesian inference derive generalised class deep directed generative models endowed algorithm scalable inference learning. algorithm introduces recognition model represent approximate posterior distribution uses optimisation variational lower bound. develop stochastic backpropagation rules gradient backpropagation stochastic variables derive algorithm allows joint optimisation parameters generative recognition models. demonstrate several real-world data sets using stochastic backpropagation variational inference obtain models able generate realistic samples data allow accurate imputations missing data provide useful tool high-dimensional data visualisation. immense eﬀort machine learning statistics develop accurate scalable probabilistic models data. models called upon whenever faced tasks requiring probabilistic reasoning prediction missing data imputation uncertainty estimation; simulation-based analyses common many scientiﬁc ﬁelds genetics robotics control require generating large number independent samples model. recent eﬀorts develop generative models focused directed models since samples easily obtained ancestral sampling generative process. directed models belief networks similar latent variable models easily sampled from cases eﬃcient inference algorithms remained elusive. eﬀorts combined demand accurate probabilistic inferences fast simulation lead seek generative models deep since hierarchical architectures allow capture complex structure data allow fast sampling fantasy data inferred model iii) computationally tractable scalable high-dimensional data. meet desiderata introducing class deep directed generative models gaussian latent variables layer. allow eﬃcient tractable inference introduce approximate representation posterior latent variables using recognition model acts stochastic encoder data. generative model derive objective function optimisation using variational principles; recognition model specify structure regularisation exploiting recent advances deep learning. using construction train entire model modiﬁed form gradient backpropagation allows optimisation parameters generative recognition models jointly. tional inference allows joint optimisation variational model parameters exploiting properties latent gaussian distributions gradient backpropagation figure graphical model dlgms corresponding computational graph. black arrows indicate forward pass sampling recognition generative models solid lines indicate propagation deterministic activations dotted lines indicate propagation samples. arrows indicate backward pass gradient computation solid lines indicate paths deterministic backpropagation used dashed arrows indicate stochastic backpropagation. deep latent gaussian models general class deep directed graphical models consist gaussian latent variables layer processing hierarchy. model consists layers latent variables. generate sample model begin top-most layer drawing gaussian distribution. activation lower layer formed non-linear transformation layer perturbed gaussian noise. descend hierarchy generate observations sampling observation likelihood using activation lowest layer process described graphically ﬁgure mutually independent gaussian variables. transformations represent multi-layer perceptrons matrices. visible layer data generated appropriate distribution whose parameters speciﬁed transformation ﬁrst latent layer. throughout paper refer parameters generative model i.e. parameters maps matrices construction allows make many deterministic stochastic layers needed. adopt weak gaussian prior itly deﬁned equation gaussian distributions mean covariance equation makes explicit generative model works applying complex nonlinear transformation spherical gaussian distribuysis general mappings allow non-linear factor analysis mappings form simple elementwise non-linearities probit function rectiﬁed linearity recover non-linear gaussian belief network describe relationship existing models section given speciﬁcation task develop method tractable inference. number approaches known widely used include mean-ﬁeld variational wake-sleep algorithm stochastic variational methods related control-variate estimators also follow stochastic variational approach shall develop alternative existing inference algorithms overcomes many limitations scalable eﬃcient. loss function assume integrable smooth. quantity diﬃcult compute directly since expectation unknown problems indirect dependency parameters expectation taken. theorems bonnet price respectively. equations true expectation integrable smooth function equation direct consequence location-scale transformation gaussian equation derived successive application product rule integrals; provide proofs identities appendix equations especially interesting since allow unbiased gradient estimates using small number samples assume mean covariance matrix depend parameter vector able write general rule gaussian gradient computation combining equations using chain rule gradient hessian function respectively. equation interpreted modiﬁed backpropagation rule gaussian distributions takes account gradients mean covariance reduces standard backpropagation rule constant. unfortunately rule requires knowledge hessian matrix algorithmic complexity inference dlgms later introduce unbiased though higher variance estimator requires quadratic complexity. using suitable co-ordinate transformations. also derive stochastic backpropagation rules distribution written smooth invertible transformation standard base distribug gradient evaluated provides lower-cost alternative price’s theorem transformations well known many distributions especially self-similarity property location-scale formulation gaussian student’s t-distribution stable distributions generalised extreme value distributions. stochastic backpropagation contexts. gaussian gradient identities described appear widely used. identities recognised opper archambeau variational inference gaussian process regression following work graves parameter learning large neural networks. concurrently paper kingma welling present alternative discussion stochastic backpropagation. approaches developed simultaneously provide complementary perspectives derivation stochastic backpropagation rules. perform inference dlgms must integrate eﬀect latent variables requires compute integrated marginal likelihood. general intractable integration instead optimise lower bound marginal likelihood. introduce approximate posterior distribution allow best possible inference speciﬁcation recognition model must ﬂexible enough provide accurate approximation posterior distribution motivating deep neural networks. regularise recognition model introducing additional noise speciﬁcally bit-ﬂip drop-out noise input layer small additional gaussian noise samples recognition model. rectiﬁed linear activation functions non-linearities deterministic layers neural network. found regularisation essential without recognition model unable provide accurate inferences unseen data points. optimise monte carlo methods expectations stochastic gradient descent optimisation. optimisation require eﬃcient estimators gradients terms equation respect parameters generative recognition models respectively. obtain gradients respect recognition parameters rules gaussian backpropagation developed section address complexity hessian general rule co-ordinate transformation gaussian write gradient respect factor matrix derived equation derivatives computed distribution speciﬁed directed acyclic graph node graph gaussian conditioned linear non-linear transformations parents. joint distribution case non-gaussian stochastic backpropagation still applied. recognition model whose design independent generative model. recognition model allows introduce form amortised inference variational methods share statistical strength allowing generalisation across posterior estimates latent variables using model. implication generalisation ability faster convergence training; gradients used descend free-energy surface respect generative recognition parameters single optimisation step. figure shows computation dlgms. algorithm proceeds ﬁrst performing forward pass consisting bottom-up phase top-down phase updates hidden activations recognition model parameters gaussian distributions backward pass gradients computed using appropriate backpropagation rule deterministic stochastic layers. take descent step using number approaches parameterising covariance matrix recognition model maintaining full covariance matrix equation would entail algorithmic complexity training sampling layer number latent variables layer. simplest approach diagonal covariance matrix diag k-dimensional vector. approach appealing since allows linear-time computation sampling allows axis-aligned posterior distributions. representation allows arbitrary rotations gaussian distribution along principal direction relatively additional parameters application matrix inversion lemma obtain covariance matrix terms product arbitrary vector computed without computing explicitly. also allows sample eﬃciently gaussian since gaussian random variable mean written since covariance parametrisation linear cost number latent variables also parameterise variational distribution layers jointly instead factorised assumption computational complexity producing sample generative model average number latent variables layer number layers computational complexity training sample training also matching auto-encoder. generative models number applications simulation prediction data visualisation missing data imputation forms probabilistic reasoning. describe testing methodology present results number tasks. figure analysis true approximate posterior mnist. within image show four views posterior zooming region centred estimate. comparison test likelihoods. visualise posterior distribution model gaussian latent variables ﬁgure true posterior distribution shown grey regions computed importance sampling large number particles aligned grid ﬁgure posterior distributions elliptical spherical shape thus reasonable assume well approximated gaussian. samples prior spread widely space samples fall region signiﬁcant posterior mass explaining ineﬃciency estimation methods rely samples prior. samples recognition model concentrated posterior mass indicating recognition model learnt correct posterior statistics lead eﬃcient learning. ﬁgure samples recognition model aligned axis capture posterior correlation. correlation captured using structured covariance model ﬁgure posteriors gaussian shape recognition places mass best location possible provide reasonable approximation. benchmark comparison performance terms test log-likelihood shown ﬁgure using architecture factor analysis wake-sleep algorithm approach using diagonal structured covariance approaches. experiment generative model consists latent variables feeding deterministic layer evaluate performance three layer latent gaussian model mnist data set. model consists deterministic layers hidden units stochastic layer latent variables. mini-batches observations trained model using stochastic backpropagation. samples model shown ﬁgure also compare test log-likelihood large number existing approaches table used binarised dataset uria quote log-likelihoods lower part table work. results show approach competitive best models currently available. generated digits also match true data well visually appear good best visualisations competing approaches. consisting deterministic layer hidden units stochastic layer latent variables. samples produced model shown ﬁgure cifar natural images data consists latent variable models often used visualisation high-dimensional data sets. project mnist data -dimensional latent space embedding visualisation data embedding mnist shown ﬁgure classes separate diﬀerent regions suggesting embeddings useful understanding structure high-dimensional data sets. figure sampled generated dlgms three data sets norb cifar frey faces. images left image shows samples training data right side shows generated samples. test imputation ability diﬀerent missingness types missing-atrandom consider pixels missing randomly missing-atrandom consider square region image missing. model produces good completions test cases. uncertainty identity image reﬂected errors completions resampling procedure demonstrates ability model capture diversity underlying data. integrate missing values procedure simulates markov chain show converges true marginal distribution missing given observed pixels. imputation procedure discussed appendix figure imputation results svhn. frey faces. rows mnist. col. shows true data. col. shows pixel locations missing grey. remaining columns show imputations iterations. linear gaussian belief networks related models include sigmoid belief networks deep auto-regressive networks auto-regressive bernoulli distributions layer instead gaussian distributions. gaussian process latent variable model deep gaussian processes form nonparametric analogue model employ gaussian process priors non-linear functions between layer. neural auto-regressive density estimator uses function approximation model conditional distributions within directed acyclic graph. nade amongst competitive generative models currently available several limitations inability allow deep representations diﬃculties extending locally-connected models preventing scaling easily high-dimensional data. alternative latent gaussian inference. alternative approaches inferring latent gaussian distributions meet desiderata scalable inference seek. laplace approximation concluded poor approximation general addition computationally expensive. inla restricted models hyperparameters whereas interest s-s. cannot applied latent variable models inability match moments joint distribution latent variables model parameters. furthermore reliable methods exist moment-matching means covariances formed non-linear transformations linearisation importance sampling either inaccurate slow. thus variational approach present remains general-purpose competitive approach inference. monte carlo variance reduction. control variate methods amongst general effective techniques variance reduction monte carlo methods used popular approach reinforce algorithm since simple implement applicable discrete continuous models though control variate methods becoming increasingly popular variational inference problems unfortunately estimators undesirable property variance scales linearly number independent random variables target function variance bounded constant k-dimensional latent variables variance reinforce scales whereas scales important family alternative estimators based quadrature series expansion methods methods low-variance price introducing biases estimation. recently combination series expansion control variate approaches proposed blei relation denoising auto-encoders. denoising auto-encoders introduce random corruption encoder network attempt minimize expected reconstruction error corruption noise additional regularisation terms. variational approach stochastic encoder setting. direct correspondence expression free energy reconstruction error regularization terms used denoising auto-encoders bengio thus denoising auto-encoders realisation variational inference latent variable models. diﬀerence form encoding ‘corruption’ regularisation terms used model derived directly using variational principle provide strict bound marginal likelihood known directed graphical model allows easy generation samples. daes also used generative models simulating markov chain behaviour markov chains problem speciﬁc lack consistent tools evaluate convergence. introduced general-purpose inference method models continuous latent variables. approach introduces recognition model seen stochastic encoding data allow eﬃcient tractable inference. derived lower bound marginal likelihood generative model speciﬁed structure regularisation recognition model exploiting recent advances deep learning. developing modiﬁed rules backpropagation stochastic layers derived eﬃcient inference algorithm allows joint optimisation parameters. show several real-world data sets model generates realistic samples provides accurate imputations missing data useful tool high-dimensional data visualisation.", "year": 2014}