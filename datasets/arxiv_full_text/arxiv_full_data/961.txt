{"title": "An exact mapping between the Variational Renormalization Group and Deep  Learning", "tag": ["stat.ML", "cond-mat.stat-mech", "cs.LG", "cs.NE"], "abstract": "Deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data. Recently, such techniques have yielded record-breaking results on a diverse set of difficult machine learning tasks in computer vision, speech recognition, and natural language processing. Despite the enormous success of deep learning, relatively little is understood theoretically about why these techniques are so successful at feature learning and compression. Here, we show that deep learning is intimately related to one of the most important and successful techniques in theoretical physics, the renormalization group (RG). RG is an iterative coarse-graining scheme that allows for the extraction of relevant features (i.e. operators) as a physical system is examined at different length scales. We construct an exact mapping from the variational renormalization group, first introduced by Kadanoff, and deep learning architectures based on Restricted Boltzmann Machines (RBMs). We illustrate these ideas using the nearest-neighbor Ising Model in one and two-dimensions. Our results suggests that deep learning algorithms may be employing a generalized RG-like scheme to learn relevant features from data.", "text": "deep learning broad techniques uses multiple layers representation automatically learn relevant features directly structured data. recently techniques yielded record-breaking results diverse diﬃcult machine learning tasks computer vision speech recognition natural language processing. despite enormous success deep learning relatively little understood theoretically techniques successful feature learning compression. here show deep learning intimately related important successful techniques theoretical physics renormalization group iterative coarse-graining scheme allows extraction relevant features physical system examined diﬀerent length scales. construct exact mapping variational renormalization group ﬁrst introduced kadanoﬀ deep learning architectures based restricted boltzmann machines illustrate ideas using nearest-neighbor ising model two-dimensions. results suggests deep learning algorithms employing generalized rg-like scheme learn relevant features data. central goal modern machine learning research learn extract important features directly data. among promising successful techniques accomplishing goal associated emerging sub-discipline deep learning. deep learning uses multiple layers representation learn descriptive features directly training data successfully utilized often achieving record breaking results diﬃcult machine learning tasks including object labeling speech recognition natural language processing work focus deep learning algorithms known deep neural networks dnns biologically-inspired graphical statistical models consist multiple layers neurons units layer receiving inputs units layer below them. despite enormous success still unclear advantages deep multi-layer architectures possess shallower architectures similar number parameters. particular still well understood theoretically dnns successful uncovering features structured data. possible explanation success architectures viewed iterative coarse-graining scheme high-level layer neural network learns increasingly abstract higherlevel features data initial layers thought low-level feature detecters higher layers combine low-level features abstract higher-level features providing useful times reduced representation data. successively applying feature extraction dnns learn deemphasize irrelevant features data simultaneously learning relevant ones. iterative coarse-graining procedure designed tackle diﬃcult physics problems involving many length scales. central goal extract relevant features physical system describing phenomena large length scales integrating short distance degrees freedom. sequence ﬂuctuations integrated starting microscopic scale moving iteratively ﬂuctuations larger scales. procedure certain features called relevant operators become increasingly important features dubbed irrelevant operators diminishing eﬀect physical properties system large scales. general impossible carry renormalization procedure exactly. therefore number approximate procedures developed theoretical physics community approximate method class variational real-space renormalization schemes introduced kadanoﬀ performing spin systems kadanoﬀ’s variational scheme introduces coarse-grained auxiliary hidden spins coupled physical spin systems unknown coupling parameters. parameter-dependent free energy calculated coarse-grained spin system coupled system integrating physical spins. coupling parameters chosen variational procedure minimizes diﬀerence free energies physical hidden spin systems. ensures coarse-grained system preserves long-distance information present physical system. carrying procedure results transformation maps physical spin system coarse-grained description terms hidden spins. hidden spins serve input next round renormalization. introduction layers hidden spins also central component dnns based restricted boltzmann machines rbms hidden spins coupled visible spins describing data interest. coupling parameters visible hidden layers chosen using variational procedure minimizes kullback-leibler divergence true probability distribution data variational distribution obtained marginalizing hidden spins. like variational rbms used state visible spins data sample description terms hidden spins. number hidden units less number visible units mapping thought compression. deep learning individual rbms stacked output serving input next. moreover variational procedure often performed iteratively layer layer. preceding paragraphs suggest intimate connection deep learning. indeed construct exact mapping variational scheme kadanoﬀ dnns based rbms mapping suggests dnns implement generalized rglike procedure extract relevant features structured data. paper organized follows. begin reviewing kadanoﬀ’s variational renormalization scheme context ising model. introduce rbms deep learning architectures stacked rbms. show procedure variational unsupervised training dnn. illustrate ideas using onetwo-dimensional nearestneighbor ising models. discussing implication mapping physics machine learning. statistical physics often considers ensemble binary spins {vi} take values index labels position spin lattice. thermal equilibrium probability spin conﬁguration given boltzmann distribution note throughout paper temperature equal without loss generality. typically hamiltonian depends couplings parameters {ks} parameterizes possible hamiltonians. example binary spins could couplings describing spin interactions various orders idea behind coarse-grained description spin system integrated short distance ﬂuctuations. introduce binary spins {hj}. spins serve coarse-grained degree freedom ﬂuctuations small scales averaged out. typically coarse-graining procedure increases characteristic length scale describing system lattice spacing. example block spin renormalization picture introduced kadanoﬀ represents state local block physical spins figure shows block-spin procedure twodimensional spin system square lattice represents block visible spins. result coarse-graining procedure lattice spacing doubled step renormalization procedure. general interactions {vi} induce interactions coarse-grained spins {hj}. particular coarse-grained system described coarse-grained hamiltonian form describe interactions hidden spins {hj}. physics literature renormalization transformation often represented mapping couplings ˜k}. course exact mapping depends details scheme used. variational scheme proposed kadanoﬀ coarse graining procedure implemented constructing function depends variational parameters encodes interactions physical coarsegrained degrees freedom. coupling auxiliary spins {hj} physical spins {vi} integrate visible spins arrive coarse-grained description physical system entirely terms {hj}. function naturally deﬁnes hamiltonian {hj} fig. block spin renormalization. block spin renormalization physical system coarse grained introducing block variables describe eﬀective behavior block spins. example ﬁgure four adjacent spins grouped blocks. system described terms block variables. scheme iterated create even block variables average even larger original spins. notice lattice spacing doubles iteration. thus ignored problem choosing variational parameters deﬁne transformation intuitively clear choose ensure long-distance physical observables system invariant coarse graining procedure. done choosing parameters minimize free energy diﬀerence physical coarse grained systems. notice based powerful class energy-based models called restricted boltzmann machines restrict discussion rbms acting binary data drawn probability distribution {vi} binary spins labeled index example black white images spin encodes whether given pixel distribution encodes statistical properties ensemble images model data distribution rbms introduce hidden spin variables {hj} couple visible units. interactions visible hidden units modeled using energy function form since objective purposes unsupervised learning parameters chosen minimize kullback-leibler divergence between true distribution data variational distribution general possible explicitly minimize dkl||pλ) minimization usually performed using approximate numerical methods contrastive divergence note number less hidden units restricted rbms stacked that trained hidden layer serves visible layer next rbm. particular conﬁguration visible spins conﬁguration hidden layer conditional probability distribution thus training treat activities hidden layer response visible data sample data learning second layer hidden spins variational couplings hidden visible spins encoded operators rbms analogous role played joint energy function fact show below objects related equation using deﬁnition easy show hamiltonian originally deﬁned hamiltonian coarse-grained degrees freedom performing also describes hidden spins rbm. equivalent statement marginal distribution describing hidden spins boltzmann form hamiltonian prove this divide sides results also provide natural interpretation variational entirely language probability theory. operator viewed variational approximation conditional probability hidden spins given visible spins. this notice going ﬁrst line second line used eqs. implies performed exactly variational hamiltonian identical true hamiltonian describing data hrbm exactly conditional probability. means variational distribution exactly reproduces true data distribution dkl||pλ general possible perform variational transformation exactly. instead constructs family variational approximations exact transform discussion makes clear variational distributions work level hamiltonians free energies. contrast machine learning literature variational approximations usually made minimizing divergence dkl||pλ thus fig. deep learning one-dimensional ising model. decimation based renormalization transformation ferromagnetic ising model. step half spins decimated doubling eﬀective lattice spacing. after successive decimations spins described using ising models coupling spins. couplings given layer related couplings previous layer square hyberbolic tangent function. decimation-based renormalization transformations also realized using deep architecture weights n-th hidden layer given visualizing renormalization group couplings ferromagnetic ising model. four successive decimations equivalently move four layers deep architecture couplings smaller. eventually couplings attracted stable ﬁxed point approaches employ distinct variational approximation schemes coarse graining. finally notice correspondence rely explicit form energy hence holds boltzmann machine. gain intuition mapping deep learning helpful consider simple examples detail. begin examining onedimensional nearest-neighbor ising model transformation carried exactly. numerically explore two-dimensional nearest-neighbor ising model using rbm-based deep learning architecture. one-dimensional ising model describes collection binary spins {vi} organized along one-dimensional lattice lattice spacing system described hamiltonian form ferromagnetic coupling energetically favors conﬁgurations neighboring spins align. perform transformation decimate every spin. doubles lattice spacing results eﬀective interaction between spins denote coupling after performing successive transformations transformation also naturally gives rise deep learning architecture shown figure spins given layer natural interpretation decimated spins performing transformation layer below. notice coupled spins bottom layers dnns fig. form eﬀective one-dimensional chain isomorphic original spin chain. thus marginalizing spins bottom layer identical decimating every spin original spin systems. implies hidden spins second layer also described transformed hamiltonian coupling neighboring spins. repeating argument spins coupled second third layers obtains deep learning architecture shown fig. implements decimation. advantage simple deep architecture presented easy interpret requires calculations construct. however important shortcoming contains information half visible spins namely spins couple hidden layer. fig. deep learning ising model deep neural network four layers size spins trained using samples drawn ising model slightly critical temperature visualization eﬀective receptive ﬁelds layer spins. pixel image depicts eﬀective receptive ﬁeld spins layer visualization eﬀective receptive ﬁelds spins middle layer calculated eﬀective receptive ﬁelds larger moves deep neural network. consistent expected successive application block renormalization. three representative samples drawn ising model reconstruction trained dnn. samples reconstructed dnns next applied deep learning techniques numerically coarse-grain two-dimensional nearest-neighbor ising model square lattice. model described hamiltonian form hiji indicates nearest neighbors ferromagnetic coupling favors conﬁgurations neighboring spins align. unlike one-dimensional ising model dimensional ising model phase transition occurs phase transition characteristic length scale system correlation length diverges. reason near critical point system productively coarse-grained using procedure similar kadanoﬀ’s block spin renormalization inspired mapping variational dnns applied standard deep learning techniques samples generated ising model critical temperature. samples generated periodic ising model using standard equilibrium monte carlo techniques served input rbm-based deep neural network four layers spins respectively furthermore imposed penalty weights layers trained network using contrastive divergence penalty serves sparsity promoting regularizer encourages weights zero prevents overﬁtting ﬁnite number samples. practice ensures visible hidden spins interact small subset spins rbm. implementing coarse-graining scheme similar block spin renormalization spin hidden layer couples local block spins layer below. iterative blocking consistent kadanoﬀ’s intuitive picture coarse-graining implemented near critical point. moreover size blocks coupling hidden unit layer approximately size characteristic size increasing layer surprisingly local block spin structure emerges training process suggesting self-organizing implement block spin renormalization. furthermore shown fig. reconstructions coarse grained qualitatively reproduce macroscopic features individual samples despite spins layer compression ratio deep learning successful paradigms unsupervised learning emerge last years. enormous success deep learning techniques variety practical machine learning tasks ranging voice recognition image classiﬁcation raises natural questions theoretical underpinnings. here demonstrated one-to-one mapping rbm-based deep neural networks variational renormalization group. illustrated mapping analytically constructing ising model numerically examining ising model. surprisingly found dnns self organize implement coarse-graining procedure reminiscent kadanoﬀ block renormalization. suggests deep learning implementing generalized rg-like scheme learn important features data. plays central role modern understanding statistical physics quantum ﬁeld theory. central ﬁnding long distance physics many disparate physical systems dominated long distance ﬁxed points. gives rise idea universality many microscopically dissimilar systems exhibit macroscopically similar properties long distances physicists developed elaborate technical machinery exploiting ﬁxed points universality identify salient long distance features physics systems. interesting what complex machinery imported deep learning. potential obstacle importing ideas physics deep learning framework commonly applied physical systems many symmetries. contrast deep learning often applied data limited structure. recently suggested modern techniques developed context quantum systems matrix product states tensor networks natural interpretation terms variational techniques exploit ideas entanglement entropy disentanglers create features minimum amount redundancy. open question whether ideas imported deep learning algorithms. mapping also suggests route applying real space renormalization techniques complicated physical systems. real space renormalization techniques variational often limited inability make good approximations. techniques deep learning represent possible route overcoming problems. details given materials methstacked rbms trained variant ods. code available code https//code.google.com/p/matrbm/. particular unsupervised learning phase performed. individual rbms trained contrastive divergence epochs momentum using mini-batches size total samples ising model additionally regularization implemented strength instead weight decay. regularization strength chosen ensure could all-to-all couplings layers dnn. reconstructions performed supplementary ﬁles matlab variable containing learned model. eﬀective receptive ﬁeld visualize spins visible layer coupled given spin hidden layers. denote eﬀective receptive ﬁeld matrix layer number spins layer visible layer corresponding column vector encodes receptive ﬁeld single spin hidden layer computed convoluting weight matrices encoding weights spins layers compute ﬁrst used recursion relationship thus eﬀective receptive ﬁeld spin measure much hidden spin inﬂuences spins visible layer. grateful charles fisher useful conversations. also grateful javad noorbakhsh alex lang comments manuscript. work partially supported simons foundation investigator award mathematical modeling living sys-", "year": 2014}