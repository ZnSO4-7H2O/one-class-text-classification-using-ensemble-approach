{"title": "A Theory of Local Learning, the Learning Channel, and the Optimality of  Backpropagation", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "In a physical neural system, where storage and processing are intimately intertwined, the rules for adjusting the synaptic weights can only depend on variables that are available locally, such as the activity of the pre- and post-synaptic neurons, resulting in local learning rules. A systematic framework for studying the space of local learning rules is obtained by first specifying the nature of the local variables, and then the functional form that ties them together into each learning rule. Such a framework enables also the systematic discovery of new learning rules and exploration of relationships between learning rules and group symmetries. We study polynomial local learning rules stratified by their degree and analyze their behavior and capabilities in both linear and non-linear units and networks. Stacking local learning rules in deep feedforward networks leads to deep local learning. While deep local learning can learn interesting representations, it cannot learn complex input-output functions, even when targets are available for the top layer. Learning complex input-output functions requires local deep learning where target information is communicated to the deep layers through a backward learning channel. The nature of the communicated information about the targets and the structure of the learning channel partition the space of learning algorithms. We estimate the learning channel capacity associated with several algorithms and show that backpropagation outperforms them by simultaneously maximizing the information rate and minimizing the computational cost, even in recurrent networks. The theory clarifies the concept of Hebbian learning, establishes the power and limitations of local learning rules, introduces the learning channel which enables a formal analysis of the optimality of backpropagation, and explains the sparsity of the space of learning rules discovered so far.", "text": "physical neural system storage processing intimately intertwined rules adjusting synaptic weights depend variables available locally activity prepost-synaptic neurons resulting local learning rules. systematic framework studying space local learning rules obtained ﬁrst specifying nature local variables functional form ties together learning rule. framework enables also systematic discovery learning rules exploration relationships learning rules group symmetries. study polynomial local learning rules stratiﬁed degree analyze behavior capabilities linear non-linear units networks. stacking local learning rules deep feedforward networks leads deep local learning. deep local learning learn interesting representations cannot learn complex input-output functions even targets available layer. learning complex input-output functions requires local deep learning target information communicated deep layers backward learning channel. nature communicated information targets structure learning channel partition space learning algorithms. learning algorithm capacity learning channel deﬁned number bits provided error gradient weight divided number required operations weight. estimate capacity associated several learning algorithms show backpropagation outperforms simultaneously maximizing information rate minimizing computational cost. result also shown true recurrent networks unfolding time. theory clariﬁes concept hebbian learning establishes power limitations local learning rules introduces learning channel enables formal analysis optimality backpropagation explains sparsity space learning rules discovered far. keywords machine learning; neural networks; deep learning; backpropagation; learning rules; hebbian learning; learning channel; recurrent networks; recursive networks; supervised learning; unsupervised learning. deep learning problem viewed problem learning connection weights large computational graphs particular weights deep connections away inputs outputs spite decades research algorithms proposed address task. among important ones somewhat opposition other backpropagation hebbian learning backpropagation dominant algorithm least terms successful applications ranged years computer vision high-energy physics spite many attempts better algorithm found least within standard supervised learning framework. contrast backpropagation well deﬁned algorithm–stochastic gradient descent–hebbian learning remained nebulous concept often associated notions biological unsupervised learning. less successful backpropagation applications periodically inspired development theories aimed capturing essence neural learning within general context goal work create precise framework organize study space learning rules properties address several questions particular hebbian learning? capabilities limitations hebbian learning? connections hebbian learning backpropagation? learning algorithms better backpropagation? questions addressed parts ﬁrst part focuses hebbian learning second part backpropagation. core many neural system models idea information stored synapses typically represented synaptic weight. synapses could conceivably complex require multiple variables describing states simplicity single synaptic weight framework although ideas readily extended complex cases. framework synapses faced task adjusting individual weights order store relevant information collectively organize order sustain neural activity leading appropriately adapted behavior level organism. daunting task thinks scale synapses remote sensory inputs motor outputs. sufﬁce rescaled factor synapse size violin tennis racket ought help control miles away. core deep learning problem. donald hebb credited among ﬁrst think problem attempt come plausible solution book organization behavior however hebb primarily psychologist ideas stated rather vague terms when axon cell near enough excite cell repeatedly persistently takes part ﬁring growth process metabolic change takes place cells efﬁciency cells ﬁring increased often paraphrased neurons together wire together. single equation found book. concept hebbian learning played important role development neuroscience machine learning lack crispness becomes obvious soon raises simple questions like backpropagation learning rule hebbian? oja’s learning rule hebbian? rule depends function output hebbian? learning rule depends input hebbian? forth. lack crispness simple semantic issue. helped ﬁeld early stages–in vague concepts like gene consciousness helped molecular biology neuroscience also prevented clear thinking address basic questions regarding instance behavior linear networks hebbian learning capabilities limitations hebbian learning shallow deep networks. time several attempts putting concept hebbian learning center biological learning hopﬁeld proposed hebbian learning store memories networks symmetrically connected threshold gates. resulting model elegant amenable interesting analyses oversimpliﬁes problem considering shallow networks units visible targets. fukushima proposed neocognitron architecture computer vision inspired earlier neurophysiological work hubel wiesel essentially form multi-layer convolutional neural network. importantly present work fukushima proposed learn parameters neocognitron architecture self-organized using kind hebbian mechanism. fukushima program remained source inspiration several decades result paper show program cannot succeed ﬁnding optimal weights feedforward architecture regardless speciﬁc form hebbian learning used. partly related nebulous nature hebbian learning observation entire machine learning ﬁeld able come learning rules like backpropagation rule hebb’s rule. familiar rules perceptron learning rule delta learning rule oja’s rule viewed special cases variations backpropagation hebb additional variations found also instance discussions learning rules general standpoint creates potentially unsatisfactory situation given important learning algorithms ﬁrst could derived newton leibniz second shrouded vagueness. furthermore raises broader question nature space learning rules. particular space seem sparse? rules remain discovered space? table common learning rules on-line expressions. represents activity postsynaptic neuron activity presynaptic neuron synaptic strength corresponding connection. represents back-propagated error postsynaptic neuron. perceptron delta learning rules originally deﬁned single unit case readily available output target. origin vagueness hebbian learning idea indiscriminately mixes fundamental distinct ideas learning ought depend local information associated prepost-synaptic neurons; learning ought depend correlation activities neurons yielding spectrum possibilities correlations computed used change synaptic weights. concept local learning rule mentioned exploited fundamental concept hebbian learning rule explicitly exposes general notion locality implicit somehow hidden vagueness hebbian concept. address issues ﬁrst observation physical implementation learning rule adjust synaptic weight include local variables. thus bring clarity computational models must ﬁrst deﬁne variables considered local given model. consider backpropagation learning rule ∆wij bioj postsynaptic backpropagated error presynaptic activity. backpropagated error considered local variable backpropagation local learning rule thus hebbian. backpropagated error considered local variable backpropagation hebbian sense local simple product local prepost-synaptic terms. presynaptic term invalidate hebbian character depending again interprets vague hebbian concept.] function systematic study local learning requires systematic analysis many cases terms functions also terms computing units transfer functions network topologies possible parameters simplicity ﬁrst consider single processing units input-output functions form input vector transfer function identity linear case logistic function hyperbolic tangent function corresponding threshold functions necessary bias included framework considering general assume self-connections general computing units subdivided three subsets corresponding input units output units hidden units. formalism includes feedforward recurrent networks ﬁrst part paper focus primarily feedforward networks. however issues feedback recurrent networks become important second part. instance consider cases output clamped value error signal component learning rule. latter case perceptron learning algorithm deep targets algorithm described below backpropagation special case. equation represents local learning rule assumes target locally available. targets clearly available local output layer. however generation local availability targets deep layers fundamental separate question addressed later sections. thus essential note concept locality orthogonal concept unsupervised learning. unsupervised learning rule non-local depends activities synaptic weights away network. likewise supervised learning rule local target assumed local variable. finally also assume learning rate local variable contained function simplicity assume value shared units although general models possible. short time move away vagueness term hebbian learning replace clear deﬁnition situation variables considered local; functional form used combine local variables local learning rule. goal systematically study properties different local rules across different network types. concept locality completely general applies equally well networks spiking neurons non-spiking neurons. analyses speciﬁc local learning rules sections conducted non-spiking neurons extensions spiking neurons possible material sections general applicable networks spiking units. main reason sections concerned primarily propagation information targets output layer back deeper layers regardless information encoded regardless whether non-spiking spiking neurons used forward backward directions. subsection essential follow rest paper initially skipped. studying local learning rules important look effects coordinate transformations various symmetries learning rules. complete treatment operations beyond scope give several speciﬁc examples below. general applying coordinate changes symmetries bring light important properties learning rule shows general function considered narrowly rather member class. instance consider narrow deﬁnition hebb’s rule ∆wij oioj applied threshold gates binary inputs. deﬁnition makes sense threshold gates deﬁned using formalism problematic formalism results ∆wij oioj three four cases always positive equal remaining fourth case. thus narrow deﬁnition hebb’s rule system modiﬁed using corresponding afﬁne transformation. however expression functional class i.e. case quadratic function activities. considerations apply sigmoid transfer functions used. speciﬁcally networks transformed networks transformation vice versa transformation easy show polynomial local rule type network transformed polynomial local rule degree type network. instance quadratic local rule coefﬁcients form learning rules less sensitive permutations order examples presented. order analyze behavior rules assume sensitive order examples presented generally case training examples treated equally on-line learning rate small changes slowly averages computed entire epochs learning rule applied isotropically important examine behavior symmetries network architecture applied. case instance hopﬁeld networks units connected symmetrically fully connected layers feedforward architecture. particular important examine whether differences inputs weight initializations lead symmetry breaking. also possible consider models different neurons different connections different rules rules class different coefﬁcients. taken account quadratic function induces acyclic orientation n-dimensional hypercube edge neighboring state spaces oriented patterns memories stored weights system applying simple hebb rule ∆wij oioj memories. thus given training produces corresponding weights thus corresponding energy function thus corresponding acyclic orientation hypercube. consider isometry n-dimensional hypercube i.e. one-to-one function preserves hamming distance. easy isometries generated composing kinds elementary operations permuting components; inverting sign component hypercube hopﬁeld network units yields symmetric weights corresponding quadratic energy function ultimately produces directed acyclic orientation hypercube directing dynamics network towards minima isometry hypercube yields vectors hence application hebb rule weights energy function acyclic orientation within general assumption ∆wij ∆wij supervised case must consider next functional form among things allows organize stratify space learning rules. seen above function cannot deﬁned narrowly must invariant certain changes thus primarily interested classes functions. paper focus exclusively case polynomial function degree local variables although functional forms could considered rational functions power functions rational exponents. rules found neural network literature correspond degree polynomial rules. thus consider functions comprising terms form αoni real coefﬁcient nwij non-negative integers satisfying nwij term apparent degree nwij effective degree higher depends also typically linear least around current value case effective degree term nwij finally denote highest effective degree among terms shall main numbers interest used stratify polynomial learning rules. expect able change word hebbian used recommendation used rest paper replace hebbian precise concept local learning rule assumes pre-existing deﬁnition variables considered local. within local learning rules easy general linear learning rules form ∆wij γwij useful functions thus local learning rule interesting must least quadratic. within quadratic learning rules could adopt position ∆wij oioj called hebbian. extreme could adopt position quadratic rules called hebbian. would include correlation rule case avoid terminological complexities result vagueness hebbian concept will focus concept locality; stratify local rules degrees reserve simple hebb rule ∆wij oioj avoiding hebb context. assume training consisting inputs unsupervised case input-target pairs supervised case. on-line learning local rules exhibit stochastic ﬂuctuations weights change on-line presentation. however small learning rate randomized order example presentation expect long term behavior dominated average values weight changes computed epoch. thus assume varies rapidly training data compared synaptic weights assumed remain constant epoch. difference time-scales enables analysis since assume weight remains essentially constant throughout epoch compute average changes induced training data entire epoch. instantaneous evolution weights governed relationship index entire epochs expectation taken corresponding epoch. thus analyses must ﬁrst compute expectation solve recurrence relation corresponding differential equation. article subdivided main parts. ﬁrst part focus hebbian learning precisely local learning rules. restricting learning rules polynomial form initial goal estimate expectations form supervised case. time-scale assumption within epoch assume constant therefore corresponding term factors expectation. thus left estimating terms form supervised case inputs output unit considered. follows ﬁrst consider linear case non-linear case give examples learning rules derived second part focus backpropagation. ﬁrst study limitations purely local learning shallow deep networks also called deep local learning beyond limitations naturally leads introduction local deep learning algorithms deep targets algorithms study properties backward learning channel optimality backpropagation study feedforward layered linear networks thus reduced study single linear unit form wiii. case understand behavior local learning rule must compute expectations encompasses also unsupervised case letting thus expectation polynomial weights coefﬁcients correspond statistical moments training data form polynomial linear weights learning equation solved exactly using standard methods. effective degree greater learning equation solved special cases general case. look analysis precisely assume learning rule uses data terms order less. thus means variances covariances necessary compute expectations learning rule. example term acceptable requires third-order moments data form compute expectation. compute necessary expectations systematically following notations. expectation matrix form written diagσii diagw thus cubic case expectation form diagw. likewise expectation matrix form also written thus cubic case expectation form note also bias term consider corresponding input constant clamped solving learning recurrence relation linear case effective degree satisﬁes recurrence relation provided equation linear value overall degree thus solved standard methods provided necessary data statistics available compute expectations. precisely computing expectation epoch leads relation table constant linear terms expectations scalar vector form. table contains constant linear terms degree equal depending ﬁrst order statistics data. horizontal double line separates unsupervised terms supervised terms terms sorted increasing values effective degree increasing values apparent degree table quadratic terms expectations scalar vector form. table contains quadratic terms terms depend ﬁrst second order statistics data. horizontal double line separates unsupervised terms supervised terms terms sorted increasing values effective degree increasing values apparent degree table cubic terms expectations scalar vector from. table contains terms degree depend ﬁrst second order statistics data. horizontal double line separates unsupervised terms supervised terms terms sorted increasing values effective degree increasing values apparent degree table simple terms order expectations scalar vector form. table contains terms degree depend ﬁrst second order statistics data. horizontal double line separates unsupervised terms supervised terms terms sorted increasing values effective degree increasing values apparent degree. thus linear setting many local learning rules lead divergent weights. notable exceptions however particular learning rule performing form gradient descent convex objective function. interesting example provided rule using table vector form rule convergent performs gradient descent quadratic error function converging general linear regression solution. summary dynamics learning rule solved exactly linear case entirely determined statistical moments data particular means variances covariances inputs targets case effective degree weights greater learning rule recurrence relation linear systematic solution general case. must noted however special cases result bernoulli riccati differential equation evolution weight solved reasons become clear later sections instance consider learning equation decrease asymptotically converge towards note also wobv wobv obvious constant solutions differential equation. solve riccati equation formally known obvious solutions introduce variable different solution). result easy variable satisﬁes linear differential equation. precisely simple calculation gives originally derived linear neuron. idea behind rule control growth weights induced simple hebb rule adding decay term. form decay term easily obtained requiring weights constant norm expanding corresponding constraint taylor series respect shown reasonable assumptions weight vector converge towards principal eigenvector input correlation matrix. converging learning rules discussed broadly section wiii) transfer function logistic hyperbolic tangent differentiable case corresponding threshold functions. expectations computed linear case involve variable computed exactly linear case. furthermore least case threshold gates easily deal powers case case. thus essence main challenge compute terms form non-linear. next show expectations approximated. vary term could ﬂuctuate. note however data centered often done practice approximate term constant equal across epochs. although cases approximation equation precise reasonable cases quite good. approximation origin dropout approximation represents normalized geometric mean. several related results proven transfer function hyperbolic tangent function approximation equation valid standard logistic function also logistic function slope form threshold functions approximated sigmoidal functions thus approximation used also threshold functions similar caveats. generally transfer function differentiable expanded taylor series around mean always have ′′)) thus ′′)v ars. thus small small approximation quality ﬁrst-order taylor approximation estimate term need assume terms depending independent terms dependent i.e. data covariances case thus weight vector tends align center gravity data. however provides direction weight vector continues grow inﬁnity along direction demonstrated figure figure single unit trained mnist data epochs learning rate using simple hebb rule unsupervised fashion. fan-in weights initialized normal distribution standard deviation left angle weight vector center gravity. right norm weight vector. consider single sigmoidal threshold unit trained using training input-output pairs supervised mode output clamped target value apply simple hebb rule output clamped target ηiit thus expectation constant across epochs depends data moments. general weights grow linearly number epochs unless case remain constant equal initial value short last example convergent rule provided logistic transfer function. rule convergent performs gradient descent relative entropy log). remarkably trivial scaling factor absorbed learning rate learning rule exactly form tanh function used range local learning framework also helpful discovering learning rules. principle could recursively enumerate polynomial learning rules rational coefﬁcients search rules satisfying particular properties. however necessary several reasons. practice interested polynomial learning rules relatively small degree direct approaches possible. provide example consider issue convergence derive convergent learning rules. ﬁrst note major concern hebbian rule even simple case ∆wij oioj weights tend diverge time towards large positive large negative values. ensure weights remain within ﬁnite range natural introduce decay term ∆wij oioj cwij decay coefﬁcient also adaptive long remains positive. exactly happens oja’s cubic learning rule postsynaptic neuron target consider clamped gradient descent version rules. clamped cases occurrences equations replaced target gradient descent version occurrences equations replaced corresponding list rules given appendix derive additional convergent learning rules take different approach introducing saturation effect weights. ensure weights remain range assume weights calculated applying hyperbolic tangent function. thus consider system trained using simple hebb rule ∆wij oioj. keep weights previous sections focused study local learning rules stratiﬁed degree shallow networks. section begin look local learning rules applied deep feedforward networks partially address question locally learnable feedforward networks. speciﬁcally want consider shallow local learning deep local learning deﬁned learning deep feedforward layered architectures learning rules form ∆wij applied successively layers starting input layer possibly followed supervised learning rule form ∆wij layer alone targets available layer would like understand input-output functions learnt examples using strategy whether provides viable alternative back-propagation. begin simulation experiments motivate analyses. conduct experiments using various local learning rules learn boolean functions small architectures adaptive layers. experiments purposely carried show simulations simple cases raise false hopes learnability local rules extend large complex functions shown later section. speciﬁcally train binary threshold gates learn boolean functions inputs using simple hebb rule rule rule corresponding equation supervised version. sometimes multiple random initializations weights tried function considered learnable learnt least case. tables report results obtained shallow case trained supervised manner results obtained deep case adaptive input layer figure temporal evolution norm weight vector single threshold gate inputs bias trained supervised mode using randomly generated training examples using three different learning rules basic hebb rule. rule gracefully prevent unbounded growth weights. rule produces weight vector whose component fairly saturated total norm close figure learning rule results riccati differential equation. solution riccati equation tells weights converge typical weight shown. initialized randomly trained mnist resulting fan-in almost perfect agreement theoretical empirical curve. trained unsupervised manner adaptive output layer trained supervised manner. experiments inputs targets binary units bias learning rate decays linearly. figure independence assumption reasonable riccati equation describes dynamics learning used exact solution. typical weight shown randomly initialized trained mnist samples recognize digits classes. figure single neuron tanh activation trained recognize handwritten digit nine supervised learning rules. input data mnist images binary targets. weights initialized independently updated learning rate shown table possible boolean functions variables learnt using simple hebb rules. boolean functions cannot learnt course converse cannot implemented single layer network. using deep local learning two-layer networks three rules able learn boolean functions demonstrating least complex functions learnt combining unsupervised learning lower layer supervised learning layer. similar table shows similar results subset monotone boolean functions. reminder boolean function said monotone increasing total number input vector leave value output unchanged increase value equivalently boolean functions circuit comprising gates. recursive methods generating monotone boolean functions total number monotone boolean functions known dedekind number. instance monotone boolean functions inputs. these learnable single unit trained supervised fashion learnable two-layer network trained combination unsupervised supervised application three local rules. setting problem already solved least setting perceptron learning algorithm theorem obviously deﬁnition threshold gate implement exact functions linearly separable. perceptron learning algorithm simply states data linearly separable local gradient descent learning rule converge separating hyperplane. note true also case gates gradient descent rule form systems. training data linearly separable perceptron algorithm still well behaved sense algorithm converges relatively small compact region consider similar results slightly different supervised rule clamped form simple hebb rule consider supervised training consisting input-target pairs form input vectors n-dimensional vectors corresponding targets every linearly separable separating hyperplane threshold function. slightly simplify notation analysis throughout section allow ambiguous cases interest. framework linearly separable learnable given learning rule rule separating hyperplane. case without bias bias every case consistent every obviously consistency necessary condition separability learnability case bias. bias training canonical form ensuring targets replacing training pair form equivalent pair thus size learnable canonical training binary case every consider whether learnable supervised simple hebb rule corresponding clamped outputs ηiit ﬁrst case bias i.e. denote symmetric square matrix cosine values c)). easy applying supervised simple hebb rule vectors equivalent applying supervised simple hebb rule vectors leading weights. canonical form bias following properties. assume constant positive learning rate sufﬁcient number epochs effect initial conditions inequality ignored. alternatively examine regime decreasing learning rates using initial conditions close thus ignoring transient effect caused initial conditions separating terms corresponding learnt sufﬁcient number epochs thus cosines strictly positive training set). since training ﬁnite simply take maximum number epochs training examples inequality satisﬁed offset initial conditions. note expression equation invariant respect transformation preserves vector lengths angles changes sign angles. thus invariant respect rotations symmetries. special case also obvious equation note particular vectors chosen randomly essentially orthogonal thus learnable high probability large. training vectors length equation simply becomes property obvious. note easy construct counterexamples property true training vectors length. take instance small case adaptive bias bias starting training ﬁrst modify vector vector adding zero-th component equal otherwise. finally construct corresponding canonical case bias letting apply previous results easy check applying supervised simple hebb rule vectors equivalent applying supervised simple hebb rule vectors leading weights. theorem figure examples supervised simple hebb learning different training properties. linearly separable data random matrix binary values shape binary targets determined random hyperplane. orthogonal dataset simply identity matrix random binary targets. common orthant dataset created sampling features column either setting targets weights initialized independently weights updated learning rate summary strictly local learning single threshold gate sigmoidal function learn linearly separable function. powerful unit allows form learning limited sense learn small fraction possible functions. logarithm size possible boolean functions variables exponential equal whereas logarithm size total number linearly separable boolean functions scales polynomially like indeed total number threshold functions variables satisﬁes references therein). negative result holds also restricted class monotone boolean functions class exponential size. monotone boolean functions cannot learnt single linear threshold unit number monotone boolean functions variables known dedekind number satisﬁes results immediately true also polynomial threshold functions polynomials bounded degree similar counting arguments short linear bounded-polynomial threshold functions best learn vanishingly small fraction boolean functions subclass exponential size regardless learning rule used learning. fact local learning shallow networks signiﬁcant limitations seems consequence limitations shallow networks simply able implement complex function. alone preclude possibility iterated shallow learning applied deep architectures i.e. deep local learning able learn complex functions. would consistent observed simple simulations described function learnable shallow networks becomes learnable local rules network depth two. thus years many attempts made seek efﬁcient perhaps biologically plausible alternatives backpropagation learning complex data using local rules. example simplest cases could learn simple two-layer autoencoder using unsupervised local learning ﬁrst layer supervised local learning layer. broadly could example learn mnist benchmark data using purely local learning. simulations show however schemes fail regardless local learning rules used learning rates hyperparameters tuned forth. next section show attempts made direction bound fail. consider deep local learning deep layered feedforward architecture layers size layer input layer layer output layer. denote activity non-linear processing units fairly arbitrary. unit layer section sufﬁcient assume functions differentiable functions synaptic weights inputs. also possible extend analysis instance threshold gates taking limit steep differentiable sigmoidal functions. consider supervised learning framework training input-output vector pairs form goal minimize differentiable error function eerr. main learning constraint deep local learning figure deep local learning. local learning rules used unit. hidden units local learning rules unsupervised thus form output units local learning rules supervised since targets considered local variables thus form fact consider supervised learning problem deep feedforward architecture differentiable error function transfer functions. cases deep local learning cannot weights associated critical points error functions thus cannot locally globally optimal weights. backpropagated error unit layer depends particular targets weights layers layer likewise presynaptic activity unit layer depends inputs weights layers layer short gradient training examples product terms product term product target-dependent term inputdependent term. result cases deep weights must depend inputs targets well weights. particular must true local global optimum. however using strictly local learning scheme deep weights depend inputs only thus cannot correspond critical point. particular shows applying local hebbian learning feedforward architecture whether simple autoencoder architecture fukushima’s complex neocognitron architecture cannot achieve optimal weights regardless kind local hebbian rule used. reasons architecture consisting stack autoencoders trained using unlabeled data cannot optimal general even layer trained gradient descent. course possible local learning shallow deep autoencoders restricted boltzmann machines forth compress data initialize weights deep architecture. however steps alone cannot learn complex functions optimally learning complex function optimally necessitates reverse propagation information targets back deep layers. fact correct level would satisfy physicist consistent empirical evidence. completely tight mathematical standpoint phrase cases. expression meant exclude trivial cases important practice would difﬁcult capture exhaustively mathematical precision. include case training data trivial respect architecture loaded entirely weights layer even random weights lower layers data generated precisely artiﬁcially constructed architecture deep weights depend input data selected random. simple result signiﬁcant consequences. particular constrained feedforward architecture trained complex task optimal deep weights architecture must depend training inputs target outputs. thus physical implementation order able reach locally optimal architecture must exist physical learning channel conveys information targets back deep weights. raises three sets questions regarding nature backward learning channel; nature information transmitted channel; rate backward learning channel. questions addressed section focus information targets transmitted deep layers. seen previous section general optimal implementation weight inputs targets order learning remain local targets transmitted output layer weight rule form contrast deep local learning. main point previous section show local deep learning powerful deep local learning local deep learning necessary reaching optimal weights. figure local deep learning. general deep local learning cannot learn complex functions optimally since leads architectures weights layer depend targets. optimal learning information targets must transmitted synapse associated deep layer becomes local variable incorporated corresponding local learning rule deﬁnition within class local deep learning algorithms deﬁne subclass deep targets local learning algorithms information transmitted targets depends postsynaptic unit words also seen proper targets available efﬁcient local learning rules adapting weights unit. particular rule works well practice sigmoidal threshold transfer functions. thus deep learning problem principle solved providing good targets deep layers. introduce second deﬁnition deep targets algorithms true sigmoidal transfer functions threshold gates otherwise rule slightly modiﬁed accommodate transfer functions accordingly. combining equations solve target assuming presynaptic activity particular backpropagation viewed deep targets algorithm providing targets hidden layers according equation form figure deep targets learning. special case local deep learning transmitted information targets depend presynaptic unit target typical cases rule search alternative backpropagation thus investigate whether exists alternative deep targets algorithms generally deep targets algorithms rely assumptions availability algorithm optimizing layer unit holding rest architecture ﬁxed target provided; availability algorithm providing deep targets. maximization complete partial optimization taking place respect error measure speciﬁc layer architecture instance exact optimization algorithm obvious unconstrained boolean case layer threshold gates perceptron algorithm exact linearly separable case. layer artiﬁcial neurons differentiable transfer functions delta rule gradient descent general perform partial optimization. thus deep targets algorithms proceed according loops outer loop inner loop. inner loop used suitable targets. outer loop uses targets optimize weights cycles units layers architecture. figure deep architecture deep targets algorithm. algorithm visits various layers according schedule optimizes them. achieved deep targets algorithm capable providing suitable targets layer input assuming rest architecture ﬁxed. targets used modify target adaptive layer write overall input-output function ah+bhch− assume ﬁxed. input produces activation vector goal suitable vector target layer generate sample activity vectors layer sampling carried different ways instance sampling values training set; small random perturbations i.e. using random vectors sampled proximity vector bhch−; large random perturbation sampling uniformly; exhaustively finally sample several optimal vectors select random control size learning step. instance selecting vector minimizes output error also minimizes error ensure target vector close possible current layer activity hence minimizes corresponding perturbation. learning optimization algorithms algorithmic details varied training instance progressively reducing size learning steps learning progresses note algorithm described natural generalization algorithm introduced unrestricted boolean autoencoder speciﬁcally optimization lower layer. related reinforcement learning algorithms connectionist networks stochastic units found present result simulation show sampling deep target algorithms work even applied case non-differentiable networks back propagation cannot applied directly. different application deep targets idea developed four-adjustable-layer perceptron autoencoder threshold gate units hamming distance error layers. input output layers units each three hidden layers units. units layer fully connected units layer below plus bias term. weights initialized randomly uniform distribution traveling along physical forward connections reverse direction; using separate different channel. essence implementation typically emulated digital computers using transpose forward matrices backpropagation algorithm. however ﬁrst thing observe even channel used forward backward direction signal need nature. figure sampling deep target algorithm used train simple autoencoder network threshold gates thus purely comprised non-differentiable transfer functions. axis correspond average hamming error component example. instance forward propagation could electrical backward propagation could chemical. congruent observation signals different time scales forward propagation fast compared learning occur longer time scales. biological neural networks evidence existence complex molecular signaling cascades traveling synapses neuron nucleus capable activating epigenetics modiﬁcations gene expression conversely molecular signals traveling synapses. least principle chemical electrical signals could traverse synaptic clefts directions. short direct evidence supporting physical connections directions biological neural systems possibility cannot ruled entirely time conceivably could used hardware embodiments. noted deep targets algorithm feedback reaches soma unit leads simpler feedback channel puts burden unit propagate central message soma synapses. separate channel used channel must allow transfer information output layer targets deep weights. transfer information could occur direct connections output layer deep layer staged process propagation level obviously combination processes also possible. either case channel implements form feedback. note learning feedback slow distinct terms signal also terms channel usual feedback recurrent networks typically used rapid dynamic mode tune rapid response e.g. helping combine generative model bottom recognition model. biological neuronal circuits plenty feedback connections different processing stages connections could serve primary channel carrying feedback signal necessary learning. must noted however given synaptic weight feedback could typically reach either dendrites neuron dendrites neuron both. general dendrites presynaptic neuron physically away synapse associated located dendritic tree post-synaptic neuron raising problem information transmission within neuron incoming synapses dendritic tree synapses associated output arborization axon. feedback reaching dendrites neuron could principle much closer site implemented although requires substantial degree organization synapses along dendrites neuron spatially combining synapses originating feedforward neurons synapses originating feedback neurons together biochemical mechanisms required local transmission feedback information spatially close synapses. short nature feedback channel depends physical implementation. clear understanding biological neural systems implement learning still reach framework presented here–in particular notions local learning deep targets algorithms–clariﬁes aspects potential challenges associated feedback channel complex geometry neurons. important related issue issue symmetry weights. backpropagation uses symmetric weights forward backward directions order compute exact gradients. physical implementation especially uses different channels forward backward propagation information difﬁcult instantiate weights precisely symmetric. however simulations seem indicate that instance random weights used backward direction without affecting much speed learning quality solutions. random weights general result matrices maximum rank allowed size layers thus transmit much information possible reverse direction least globally level entire layers. global transmission information allow precise learning entirely clear. least simple case network hidden layer output unit easy give mathematical proof random weights support convergence learning provided random weights sign forward weights. plausible biological networks could non-symmetric connections connections could possibly random random sign forward connections. regardless nature channel next must consider meaning information transmitted deep weights well amount. whatever information targets back ultimately used within epoch change weights form gradient hessian higher order remainder. denote total number weights system full hessian entries thus general computable large case interest here. thus limiting expansion ﬁrst order ij)) unit vector associated unit vector associated weight adjustments thus ﬁrst order approximation information sent back deep weights interpreted terms well approximates gradient many bits information provides gradient. weights precision level d-bits real number gradient contains bits information. turn split bits magnitude ||g|| gradient bits specify direction corresponding unit vector using real numbers plus determine sign remaining component. thus information gradient high-dimensional space contained direction. information determines conveys essentially many bits vector conveys close full budget bits weight gradient computed bits precision deﬁnes around true gradient vector cone possible unitary directions thus expectation provides measure well gradient approximated good corresponding optimization step conceivably also look regimes even information gradient transmitted backward channel. could include instance second order information curvature error function. however mentioned above case large deep networks seems problematic since weights procedure would scale like approximations essentially compute diagonal hessian matrix thus additional numbers considered using procedure similar backpropagation scales like operations. methods introduced purposes seem signiﬁcant practically useful improvements deep learning methods. furthermore change essence following scaling computations. want compare several on-line learning algorithms information targets transmitted back deep weights deﬁne compute notion transmission rate backward channel. interested estimating number important quantities limit large networks. estimates need precise primarily interested expectations scaling behavior. estimates computed adjustment weights given training example thus would multiplied factor complete epoch. particular given training example want estimate scaling number computations required transmit backward information network weight. estimates computed terms number elementary operations assumed ﬁxed unit cost. elementary operations include addition multiplication computing value transfer function computing value derivative transfer function. also assume costs forward backward propagation information. obviously assumptions essence capture implementation neural networks digital computers could revised considering completely different physical implementations. assumptions total number computations required forward pass backpropagation network scales like thus forward backward pass. amount information sent back weight. case deep targets algorithms also consider amount information sent back hidden unit value also derived. denote number bits used represent real number given implementation. thus instance backpropagation algorithm provides bits information unit weight training example associated corresponding derivative. deﬁne rate backward channel learning algorithm /cw. number bits transmitted weight backward channel divided number operations required compute/transmit information weight. note rate bounded maximal information transmitted actual gradient corresponding bits weight minimal computational/transmission cost must least operation weight. also useful consider improvement expected improvement normalized version algorithms considered ultimately lead learning step global learning rate vector weight changes. ﬁrst order approximation corresponding improvement computed taking product gradient case gradient descent η||g||. gradient descent gradient provides direction magnitude corresponding optimization step. perturbation algorithms described perturbation stochastically produces direction natural notion magnitude. since large information gradient direction compare various algorithms simply compare directions vector producedin particular relation direction gradient. avoid unnecessary mathematical complications associated generation random vectors unit length uniformly distributed high-dimensional sphere approximate process assuming mean also variance either case square norm tends normally distributed central limit theorem expectation simple calculation shows variance given gaussian case uniform case. thus case tends normally distributed mean variance c||g||/w η√c||g||/√w calculations also require components positive. case easier assume components i.i.d. uniform tends normally distributed central limit theorem expectation variance thus case tends normally distributed mean variance ||g||/ addition backpropagation implementing stochastic gradient descent consider stochastic descent algorithms associated small perturbations weights activities. algorithms identiﬁed name form a}{l g}{b r}{∅ perturbation applied weights deep targets algorithms activities perturbation either local applied single weight activity global applied weights activities. feedback provided network either binary indicating whether perturbation leads improvement real indicating magnitude improvement. finally presence indicates corresponding perturbation repeated times. brevity focus following main cases error decreases perturbation accepted. error increases perturbation rejected. alternatively opposite perturbation accepted since decrease error however detail since best speeds things factor two. pwlr stochastic descent algorithm weight turn perturbed small amount feedback provided real number representing change error. thus algorithm corresponds computation derivative error respect weight using deﬁnition derivative. short corresponds also stochastic gradient descent provides different mechanism computing derivative. deep targets algorithm. pwlb binary version pwlr whether error increases decreases transmitted back weight upon small perturbation. thus essence algorithms provides sign component gradient orthant gradient located magnitude. cycling weights random descent unit vector generated corresponding orthant palr deep targets version pwlr activity unit turn perturbed small amount thus providing derivative error respect activity turn used compute derivative error respect weight. pwgbk similar pwgb except small global perturbations produced rather single one. case binary feedback provides information perturbation leads largest decrease error. pwgrk similar pwgbk except perturbation real number corresponding change error back. corresponds providing value product gradient different unit vector directions. small global perturbation weights algorithm transmits single back weights corresponding whether error increases decreases. deep targets algorithm. perturbation deﬁnition derivative. derivative ∂eerr/∂wh also computed directly ﬁrst perturbing small amount propagating forward measuring ∆eerr eerr eerr using ∂eerr/∂wij ∆eerr/ǫ. deep target algorithm. algorithm computes gradient thus propagates bits back weight total computational cost scales like weight since essentially requires forward propagation weight. thus deep target algorithm. algorithm provides single information back weigh requires forward propagation without loss generality assume components ﬁnal random descent vector must positive. thus deep target algorithm computation derivative respect activity unit derive gradient. provides bits feedback unit well weight. algorithm requires total forward propagations unit resulting total computational cost weight. thus version algorithm information backpropagated perturbation leads best improvement corresponding bits weights. total cost forward propagations. note perturbations constrain gradient intersection hyperplanes corresponds information retaining best perturbation. however gain small enough reﬁned version algorithm corresponding calculations worth effort. thus best perturbation. seen perturbation product corresponding unit vector essentially normally distributed mean variance c||g||/w maximum samples normal distribution algorithm provides bits feedback total kd/w weight requires forward propagations. terms improvements consider algorithms generates random unit vector directions produces products high dimensions random directions reason algorithms better backpropagation found rate backpropagation greater equal alternatives considered true also improvement furthermore close second algorithms discussed section fall considerably behind backpropagation least dimension. ﬁnally unlikely algorithm exists rate improvement higher backpropagation backpropagation achieves maximal possible rate maximal possible improvement multiplicative constants. thus conclusion following theorem theorem rate backpropagation equal rate algorithms described achieves maximum possible value improvement backpropagation equal improvement algorithms described achieves maximum possible value remarkably results previous sections extended recurrent well recursive networks attempt implementing backpropagation recurrent networks using hebbian learning). this consider recurrent network connection weights connections form directed cycles. network unfolded time time steps obtains deep feedforward network sets original weights recurrent networks used update unit activations layer next. thus unfolded version weights shared times. recurrent case targets units time steps often targets available time steps possibly units. regardless pattern available targets argument used section expose limitations deep local learning exposes limitations local learning recurrent networks. precisely assumption error function differentiable function weights algorithm capable reaching optimal weights– partial derivatives zero–must capable backpropagating information provided target time step weights capable inﬂuencing corresponding activation. target unit time appear partial derivative weight present recurrent network capable inﬂuencing activity unity steps less. thus general implementation capable reaching optimal weights must channel unfolded back weights layers inﬂuence network capable transmitting information activity unity layer large recurrent network maximal amount information sent back full gradient minimal number operations required typically scales like thus shows backpropagation time algorithm optimal sense providing information i.e. full gradient least number computations boltzmann machines viewed particular class recurrent networks symmetric connections hidden nodes thus considered deep. although main learning algorithm viewed form simple hebbian learning exception previous analyses. connections boltzmann machines provide channel allowing information targets obtained visible units propagate back towards deep units. furthermore well known learning rule precisely implements gradient descent respect relative divergence true observed distributions data measured visible units. thus hebbian learning rule boltzmann machines implements form local deep learning principle capable transmitting maximal amount information visible units deep units equal gradient error function. perhaps less clear computational cost scales total number weights since learning rule principle requires achievement equilibrium distributions. finally nature learning channel temporal dynamics physical recurrent networks including biological neural networks important beyond scope paper. however analysis provided already useful clarifying backward recurrent connections could serve least three different roles fast role dynamically combine bottom-up top-down activity instance sensory processing; slower role help carry signals learning feedforward connections; slower role help carry signals learning backward connections. concept hebbian learning played important role computational neuroscience neural networks machine learning decades. however vagueness concept hampered systematic investigations overall progress. redress situation beneﬁcial expose separate notions locality learning rules functional form. learning rules viewed mathematical expressions computing adjustment variables describing synapses learning function variables which physical system must local. within framework studied space polynomial learning rules linear non-linear feedforward neural networks. many cases behavior rules estimated analytically reveals rules capable extracting relevant statistical information data. however general deep local learning associated stacking local learning rules deep feedforward networks sufﬁcient learn complex input-output functions even targets available layer. learning complex input-output functions requires learning channel capable propagating information targets deep weights resulting local deep learning. physical implementation learning channel either forward connections reverse direction separate connections. furthermore large networks information carried feedback channel interpreted terms number bits information gradient provided weight. capacity feedback channel deﬁned terms number bits provided gradient weight divided number required operations weight. capacity many possible algorithms calculated calculations show backpropagation outperforms algorithms achieves maximum possible capacity. true feedforward recurrent networks. must noted however results obtained using somewhat rough estimates–up multiplicative constants–and interesting algorithms scale similarly backpropagation. figure left recurrent neural network three neurons connection weights. right network unfolded time steps producing deep feedforward network weights shared time steps. original weight replicated times targets available subset layers order reach optimal weights learning algorithm must allow individual target inﬂuence copies weights leading corresponding unit. remarkable optimality backpropagation suggests deploying learning systems computer environments speciﬁc constraints budgets backpropagation provides upper bound achieved model emulated approximated systems. likewise optimality backpropgation leads wonder also whether necessity biological neural systems must discovered form stochastic gradient descent course evolution. question whether results presented biological relevance interesting several points must taken consideration. first analyses carried simpliﬁed supervised learning setting meant closely match biological systems learn. whether supervised learning setting approximate least essential aspects biological learning open question related question extending theory local learning forms learning reinforcement learning second analyses carried using artiﬁcial neural network models. question whether networks capture essential properties biological networks settled. obviously biological neurons complex biophysical information processing machines complex neurons used here. hand several examples literature important biological properties seem captured artiﬁcial neural network models. even substantially unrelated biology artiﬁcial neural networks still provide best simple model connectionist style computation information storage entirely different style digital computers information scattered superimposed across synapses intertwined processing rather stored speciﬁc memory addresses segregated processing. case realistic biological modeling complex geometry neurons dendritic trees must taken consideration. instance signiﬁcant feedback error signal arrive soma neuron available local variable away synapse located dentritic tree neuron words must become local variable synapse wij. using factor introduction rescales synapse size could correspond tens even hundreds meters. furthermore biological physical system must worry locality space also time e.g. close must time? third issues coordination learning across different brain components regions must also taken consideration ﬁnally complete model biological learning would include target signals backpropagated electrically ultimately also complex slower biochemical processes involved synaptic modiﬁcation including gene expression epigenetic modiﬁcations complex production transport sequestration degradation protein molecular species however deﬁnitive evidence favor stochastic gradient descent biological neural systems obtaining evidence remains challenge biological deep learning must follow locality principle thus theory local learning provides framework investigating fundamental question. hopﬁeld model viewed network threshold gates connected symmetrically dynamics network stochastic asynchronous updates converges local minima energy function. self-connections result network quadratic energy function −pij wijoioj given memory vectors simple hebb rule used produce energy function store memories local minima energy function induces acyclic orientation dimensional hypercube isometry hamming distance simple hebb rule learning rules property? consider learning rules thus must ∆wij polynomial function. hypercube thus need consider case αoioj however learning rule must symmetric preserve symmetry wji. therefore form αoioj finally isometric invariance must true memories easy construct examples speciﬁc sets force thus sense simple hebb rule ∆wij αoioj isometric invariant learning rule hopﬁeld model. similar results derived spin models higher-order interactions energy function polynomial degree spin variables many situations given input vector corresponding distinct activity vector oh−. however sometimes function injective cases several input vectors ﬁnal targets mapped onto activity vector ch−) ch−). case procedure determining target vector need adjusted slightly follows. first sample activity generated propagated forward using function non-injective case. however selection best output vector sample take consideration targets rather isolated target associated current input example. instance best output vector chosen minimize errors respect targets. procedure generalization procedure used train unrestricted boolean autoencoder depending schedule outer loop sampling approach optimization algorithm used inner loop well implementation details description provides family algorithms rather single algorithm. examples schedules outerloop include single pass layer layer alternating up-and passes along architecture cycling layers order variations. sampling deep targets approach combined tricks backpropagation weight sharing convolutional architectures momentum dropout forth. adjustable learning rates used different adjustment rules different learning phases sampling deep targets approach easily combined also backpropagation. instance targets provided every layer rather every layer backpropagation used train pairs adjacent layers. also possible interleave layers backpropagations applied better stitch shallow components together sampling layer focused using optimal output sample derive target. possible instead leverage additional information contained entire distribution samples. practice algorithm converges least local minima error function. general convergence monotonic occasional uphill jumps beneﬁcial avoiding poor local minima. convergence proved mathematically several cases. instance optimization procedure hidden activity corresponding target entire training overall training error guaranteed decrease stay constant optimization step hence converge stable value. unrestricted boolean case exhaustive sampling hidden layer algorithm also shown convergent. finally also shown convergent framework stochastic learning stochastic component optimization different kind deep targets algorithm output targets used targets hidden layers described goal case force successive hidden layers reﬁne predictions towards ﬁnal target. acknowledgments work supported part grant iis- google faculty research award also grateful hardware gift nvdia corporation. work presented keynote talk iclr conference preliminary version posted arxiv title flow deep learning.", "year": 2015}