{"title": "Bayesian representation learning with oracle constraints", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Representation learning systems typically rely on massive amounts of labeled data in order to be trained to high accuracy. Recently, high-dimensional parametric models like neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. However, the semantic structure inherent in observations is oftentimes lost in the process. Human perception excels at understanding semantics but cannot always be expressed in terms of labels. Thus, \\emph{oracles} or \\emph{human-in-the-loop systems}, for example crowdsourcing, are often employed to generate similarity constraints using an implicit similarity function encoded in human perception. In this work we propose to combine \\emph{generative unsupervised feature learning} with a \\emph{probabilistic treatment of oracle information like triplets} in order to transfer implicit privileged oracle knowledge into explicit nonlinear Bayesian latent factor models of the observations. We use a fast variational algorithm to learn the joint model and demonstrate applicability to a well-known image dataset. We show how implicit triplet information can provide rich information to learn representations that outperform previous metric learning approaches as well as generative models without this side-information in a variety of predictive tasks. In addition, we illustrate that the proposed approach compartmentalizes the latent spaces semantically which allows interpretation of the latent variables.", "text": "representation learning systems typically rely massive amounts labeled data order trained high accuracy. recently high-dimensional parametric models like neural networks succeeded building rich representations using either compressive reconstructive supervised criteria. however semantic structure inherent observations oftentimes lost process. human perception excels understanding semantics cannot always expressed terms labels. thus oracles human-in-the-loop systems example crowdsourcing often employed generate similarity constraints using implicit similarity function encoded human perception. work propose combine generative unsupervised feature learning probabilistic treatment oracle information like triplets order transfer implicit privileged oracle knowledge explicit nonlinear bayesian latent factor models observations. fast variational algorithm learn joint model demonstrate applicability well-known image dataset. show implicit triplet information provide rich information learn representations outperform previous metric learning approaches well generative models without side-information variety predictive tasks. addition illustrate proposed approach compartmentalizes latent spaces semantically allows interpretation latent variables. machine learning excels ability model large quantities data layered non-linear feature-learning systems purposes classiﬁcation understanding images scenes videos text structured objects. commonly many successes owed excessive availability labels coupled supervised learning. successful cases structure data used means hard-code wiring models instance modeling video using slowness convolutions images. oftentimes especially case perception real structure data generating process unknown hard explicitly model well large amounts accurate labels hard come even inadequate knowledge representation. incoporate information query oracles like crowds gather cheap labels collect auxiliary information like similarity constraints accoring undeﬁned perceptual biases crowd aware labeling noisy inadequate represent knowledge similarity constraints present robust encode implicit information various properties stimuli. propose take advantage auxiliary information provided oracles means learn ﬂexible graphical models latent variables. examples oracles include crowds implicit structural knowledge data structural multi-modal constraints without access explicit features encoded triplet constraints critically consider oracle similarity constraints implicit observations generated unknown process include model order capture subtle knowledge similarity oracle. idea helps shape explicit interpretable latent spaces exceed performance purely unsupervised learning applied cases labels sparse undesirable. latent spaces also used explicitly inspect implicit knowledge passed oracle model. goal infer latent factor model learns jointly triplets observed data transfers implicit biases encoded triplets explicit latent space captures semantics triplet-generating process better simple density estimation provide detailed review related work section explain relationship model context triplet-loss based metric learning approaches generative models ﬁrst describe contributions needed perform described task. section introduce novel probabilistic generative model oracle observations. extend principled approach multi-query oracles using masked subspaces section second contribution described section propose principled approach combining probabilistic oracle model graphical model performing nonlinear feature learning order transfer implicit triplet knowledge explicit parametric model. section introduce fast variational inference algorithm learn posterior latent spaces respecting observations data constraints. finally sections present experimental results benchmarking proposed approaches illustrating properties discussing beneﬁts confer competing approaches. consider unknown -similarity function computes distance objects respect query based semantic information associated objects. consider internal conceptual representation oracle uses apply similarity function unobserved feature space addition consider case directly observe similarity function observe orderings similarities i.e. either greater smaller deﬁne oracle triplets related query access exhaustive sample k-times using oracle yield ﬁnite sample {tk}k illustrative example process human perceptual judgement similarities heavily relies internal representations abstracted concepts evaluate similarities purely using level image statistics. frequently used oracle crowd. systems like amazon mechanical turk used obtain triplet samples explore human perceptual prior oracle. however oracles also naturally arize data structure temporal spatial orderings known semantic structure. another type oracle access privileged information extra features implicitly available triplets sentiments associated visual features. shared property oracles provide weak natural constraints similarity without explicitly quantifying model likelihood tijl triplet contained draw bernoulli distribution states true false parametrized using softmax-function. consider denote likelihood rest paper. since jensen shannon divergence commonly intractable discuss alternatives used experiments supplement subtlety acquisition process triplets oracles asked provide distances binary statements similarity rankings based prompted question. thus triplet fulﬁlls statement made deﬁnition valid. model relaxation truncated frequently oracle information conﬂicting especially using multiple oracles. instance consider colored geometric shapes triangle circle blue circle. oracle compare shapes triplets furthermore another oracle compare colors conﬂicting oracle constraints. circles similar shapes circle triangle similar colors. generated oracle constraints cannot easily jointly fulﬁlled uniform global constraints. extend presented model oracle observations incorporating masks dimensions latent space weigh/select dimensions oracle constraints must hold. since latent variables typically higher dimension approach aims learning semantic latent subspaces speciﬁc variables encode features correspond semantic information associated oracle. subspaces general entirely private oracle question share information multiple questions. leads compartmentalization semantic representation. figure give another conceptual illustration example also results. formally h-dimensional latent variables deﬁne corresponding h-dimensional global mask-variable shared samples speciﬁc question/oracle using masks adapt yield masked oracle model figure conceptually observing oracle information multiple oracles/questions allows otherwise fully unsupervised unstructured model identify semantically compartmentalized generative process using masked subspaces results model images illuminated faces show using oracle observations latent space factorizes automatically subspaces related different semantic aspects. variational belief networks apart modeling oracle triplets deﬁned latent representations interested modeling observations well. learn graphical model maximize using h-dimensional latent variables latent variables drawn exponential family distribution simplifying cases inference learning exist many continuous distributions. write model follows exponential family likelihood parameters given function also function parametrized here focus gaussian distribution prior dimensions note small adaptations inference procedure distributions feasible. case predict means variances fθσd. good estimator learning parameters model assuming approximate conditional posterior suggested kingma welling rezende mnih gregor understood instances doubly stochastic variational inference. estimator forms variational lower bound wainwright jordan jordan marginal likelihood. performing coordinate ascent respect variational parameters corresponds minimizing divergence true approximate posterior −kl||pθ) eqφ|z)]. established observation model triplets observation model proceed introduce full generative process joint model observables. instead relying supervised model taking observations input prefer using generative approach models joint density data triplets provide unsupervised model requires input data samples oracle trained. advantage generative latent variable encoding throw away information observations leading models explicitly need capture latent factors generating data. observing oracle-samples learning leads explicit factors information oracle observations. belief network introduced section model observations connect latent variables oracle observation-term triplets introduced section triplets require multiple samples prior drawn deﬁned multiple objects jointly. similar inference model siamese network necessitates multiple instances model shared parameters work coordination generate triplet. sketch generative model figure proposed joint-model refer opbn consider datapoints triplets deﬁned them figure shown proposed joint model. models observations triplets oracle observed variables. latent space variables causes shaded variables thus captures information necessary modeling both. triplets together multiple datapoints capture dependencies latent representations. effect attaching higher-order potentials latent space model uses regularization guidance. noteworthy learning consists maximizing marginal likelihood integrating latent z’s. directly maximizes evidence coming oracle observations maintaining ﬂexibility model used in-between. model balances reconstruction cost datapoints generative cost triplets prior latent variables generating samples. goal maximize marginal likelihood evidence logpθ order learn good mapping capturing dependencies observations triplets involves integrating latent variables general analytically intractable highly ﬂexible model classes. order perform efﬁcient learning inference model given resort approximate inference methods employ doubly stochastic variational inference variational inference requires approximate distributions posterior latent variables. amortized inference employing inference network learn conditional variational distribution parametrized inference model predicts variational approximation posterior latent variables input data point. evidence lower bound looks follows kl||p) kijl acts index matrix selecting corresponding datapoints. theoretically performing coordinate ascend lower bound sufﬁcient infer parameters model inference network however expectations latent variables present elbo intractable. resort reparametrization trick perform doubly stochastic variational inference drawing unbiased samples expectations using identity predicted variational parameters using inference network unbiased samples unit gaussian. differentiable bound takes shape objective perform gradient-based learning following respect global variational parameters perform stochastic gradient descent drawing minibatches datapoints triplets time. learning masks section infer posterior distribution masks given observed data variational inference analogously learn approximate distribution adding loss masks elbo taking account state mask variable triplet likelihood. upon close inspection detect components form variational autoencoder parameters distilled triplet information. also clariﬁes transfer implicit information triplets learned parametric model happens. simple terms formulation model forces inference network learn encodings respecting triplets model decodings account shared information. relationship work similarity-based learning instance crowdsourcing tackled various ways community before. notably crowd-kernels inferred used various vision tasks using maaten weinberger assumes ﬁxed student-t structure produce embedding using similarity constraints crowd learn adaptive latent representation input features. chechik metric respecting particular distances similarity learned. differs case studying assumes speciﬁc distances similarities observed hard weak oracle. tamuz probabilistic treatment triplets introduced adaptive crowd kernel learned without speciﬁc visual features mind. also adopt probabilistic treatment triplets learn adaptive feature representation comparing images crowd well. flexible nonlinear models employed variety situations learn representations data. result relation work siamese network uses discriminatively learned features reﬁnes using loss attached encodings multiply winged networks compared images. similar version later also developed uses oracle triplets supervision instead reﬁning supervised version features setting also consider similar approaches used wang schroff usage supervised features crowd-inferred similarities boosts performance face veriﬁcation generic ﬁne-grained visual categorization tasks. difference work two-fold focus probabilistic encoder-decoder approach features learned images without labels image information thrown away. feature learning guided additionally oracle introduce probabilistic generative model provides joint model components interactions including semantic masking. forces model learn explicit latent factors capture knowledge oracle rather learning invariant thus constitutes harder comprehensive task. bayesian generative models proposed crowd-sourcing tasks model differs introduce latent variables generating observations evaluate triplet constraints observation oracle. setup better facilitates implicit knowledge transfer posterior regularization. generative models representation learning recently made rapid progress using variational inference techniques allow fast learning directed graphical models major stepping stone combining deep learning graphical models. brieﬂy review variational autoencoders section notably kingma approaches used achieve state-of-the results semi-supervised learning explicit labels. identify related setting ours using oracle obtain weak implicit supervision form similarity constraints sparse subset data generalize that data subject oracle constraints. furthermore similarly learn functionally compact subspaces semantic roles generation. cheung deep generative models used functional constraints latent space increase speciﬁcity latent variables goal share tackle using oracle-information model-based semantic regularizer. disentangling information structuring models semantically theme recent work constraints latent variables models otherwise unsupervised setting also found early usage context gaussian processes using backconstraints. using side-knowledge regularizer posterior latent variables explored settings simpler latent variable models ganchev take inspiration work. interesting link also exists formulation triplet likelihood using jensen shannon divergence generative adversarial networks bernoulli likelihood employ using softmax conceptually adapted classiﬁer match framework goodfellow finally intuitive connection also exists vapnik’s privileged learning framework supervised setting improved classiﬁers learned privileged information form additional features present training time. borrowing terminology consider similarity constraints sparse privilege conveyed oracle unobserved structure learning student model improves understanding data. generative interpretation setting ultimately leads approach learning pseudo-causal inverse model data guided oracle information modeling factors variation instead learning invariances hadsell chopra section illustrate properties proposed algorithms. start describing dataset preliminaries section ﬁrst part quantitatively compare methods baseline state-of-the-art methods. second part illustrate model variant masks factorizes latent spaces distinct semantic units. preliminaries relatively small dataset however well-suited illustrate features algorithm facilitates interpretation factorized latent spaces yale faces dataset version used comprises images individuals different light conditions. split test images training images. images taken controlled conditions using lighting allows light sources varied speciﬁc ways. azimuth elevation light relation depicted face changed values degrees degrees respectively. resulting images dramatic variability appearance shading apart variability identity depicted person. proceeded series oracle simulations. particularity simulate three different questions upon presenting random triplets images evaluations below. questions used following ﬁrst question similar typical classiﬁcation setting answering accurately actually require ability understand light variation well. question concern complex qualities images related visual physics. used yale faces dataset provides metadata images metadata simulate number triplets oracle. comparison baseline state-of-the-art methods evaluation quantitative understanding models good limitations. consider metric learning network analogous hadsell purely unsupervised variational autoencoder comparators. refer metricl respectively. independent oracle triplets experiments since works entirely unsupervised. learned representations model assess quality respect different evaluation measures. particular representations predict identity face azimuth degree elevation degree latter assess well models capture physical properties images. evaluations done held-out test data using logistic regression model. addition measure well model able predict triplets test data i.e. predict whether triplet true. provide information model details experimental setup appendix results reveal opbn variants average best-performing method. metricl effectively learns classiﬁer setting. table generative model competes metric learning method terms classiﬁcation informed identity oracle maintaining error rates tasks related image physics. outperforms classiﬁcation accuracy loss image understanding. believe shows incorporates oracle knowledge shape alternative latent spaces compared vae. table test well model incorporate subtle oracle knowledge. inform models using light condition oracle expected metric learning performance collapses tasks except targeted oracle-task. maintains performance since agnostic oracle information. opbn hand maintains good performance tasks beneﬁts predicting light conditions unsupervised vae. complex setting also give models oracle-information available questions jointly test performance. show table opbn best-performing method average. metric learning approach cannot incorporate variability available information usefully using triplets setting report using triplets achieves good performance classiﬁcation elevation prediction. main difference performance opbn classiﬁcation ability predict triplets correlates observation training triplets would satisﬁed approach. opbn learns equally competitive clearly different latent space captures semantics oracle better predictive unseen triplets test data. however beneﬁts oracles incorporating multiple queries underwhelming comparison single oracles. address issue table comparison metric learning networks variational autoencoders proposed model without masks train model triplets identity oracle azimuth oracle best results results bold face. second best results italic. observe opbn predicts properties reasonably well metricl task trained for. works well predicting lighting conditions. details main text. table comparison metric learning networks variational autoencoders three variants model triplets three oracles identity azimuth elevation. observe obpn masks performs much better evaluations. using triplets leads improvements. numbers marked considered care since metricl aware differences oracles. main text details. experiment using masked-opbn. observe masked opbn exhibits greatly improved quantitative performance tasks yields representations predictive image-properties class held triplets models. test capacity seeing opbn stalls improved performance extra experiment times triplets masked opbn. observe masking allows model continue improving triplets added. opbn masks thus shows greatest promise incorporate heterogeneous information oracles latent spaces. observed masked version opbn shows greatly improved ability learn complex oracles multiple heterogeneous queries compared discussed approaches model otherwise equal parametric capacity fundamental inference machinery constitutes surprising observation. section illustrate effects learned masks contribute performance improvements. non-masked models triplet likelihoods global. learning local likelihoods masking subspaces allow model decide parts space uses query. variational compression leads solutions least possible amounts used variables. figure show fully learned mask yale faces model query. learning jointly queries leads factorization latent space task speciﬁc partially shared latent variables. also observed strong quantitative footprint usage masks performance predictive tasks models otherwise equal capacity improves across board leading models capture light conditions class better jointly. consider effect knowledge transfer oracle/crowd allowing model identify semantic latent variable systems strive high likelihoods pixels also help model oracle triplets. also observe models masks improve dramatically availability triplets. bayesian objective helps compress latent spaces semantic variables. order inspect latent spaces induced masks embed respective subspaces latent variable encoding using t-sne figure reveals learn ﬁne-grained class clusters using identity subspace continuous smooth embeddings azimuth elevation pointing understanding model continuous nature light placements images. results point fact oracle-informed model able learn semantics light placement facial structure dedicated subspaces increasing semanticness learned space signiﬁcantly. helps identify semantic variables explicitly never observed sketched earlier figure ﬁnally present example using masks sample synthesized images figure illustrates controlled transfer subtle imaging-physics properties image next using model. figure show oracle-speciﬁc masks latent space learning multiple oracles once. evident model learns virtually switch different dimensions question learns factorize latent space compartmentalized task-relevant subspaces without ever explicitly receiving supervision instance factorize light face class. figure t-sne visualizations latent subspaces identiﬁed model shown weights figure visualization dimensions weight greater oracle. observe identity subspace clearly separates faces different persons azimuth degree elevation degree light exposure. figure illustrate factorization latent spaces using masked models following. take images training-set project latent space. encoding face given identity mask image combine latent features given azimuth mask applied encoding image resulting image blend expected approximates facial features image especially mouth region facial shape. blended image furthermore exhibits light properties similar image ﬁnally show unobserved test image shows face light conditions comparison. facial transfer perfect eyebrows still taken image skin shading blend images. fair mistakes since eyebrows frequently shaded mixed light conditions dataset. expect improve bigger datasets using oracle samples. discussion introduced joint unsupervised generative model observations triplet-constraints given oracle. contributions ﬁrst fully probabilistic treatment triplets latent variable models joint unsupervised setting using variational belief networks. show joint learning allows implicit knowledge oracle human crowd transferred rich parametric model resulting improved classiﬁcation scores improved ability predict triplets interpretability crowd biases. useful framework encode expert knowledge probabilistic reasoning systems exact model unknown labels hard obtain. second introduce information theoretic distance measures triplets generalizing commonly used euclidian distances. furthermore introduce notion question speciﬁc masks latent space force model identify interpretable features relevance speciﬁc type oracle constraint enabling model learn multiple types questions boosting performance further. approach using variational inference triplet likelihood limited belief networks thus interesting framework conjunction ﬂexible probabilistic models gaussian processes inﬁnite partition models. highlight fact using framework supervised pre-training features needed learn problem speciﬁc nonlinear feature-spaces adapted available information. showed approach compares favorably state-of-the-art metric learning models fully unsupervised method generic application using feedforward networks. model trivially extendable convolutional de-convolutional networks used high-dimensional data. interesting combine learning approach structure temporal spatially constrained models encode relationships like topological unobserved constraints taste food images. oracle side future work regarding accurate crowd-modeling different bias noise regimes promising conjunction use-cases amazon mechanical turk. model also particularly amenable active learning probing oracles optimally. finally wish mention potential framework assist perceptual applications biases human visual system studied assisted generative models. references bergstra james breuleux olivier bastien fr´ed´eric lamblin pascal pascanu razvan desjardins guillaume turian joseph warde-farley david bengio yoshua. theano math expression compiler. proceedings python scientiﬁc computing conference june oral presentation. chopra sumit hadsell raia lecun yann. learning similarity metric discriminatively application face veriﬁcation. computer vision pattern recognition cvpr ieee computer society conference volume ieee ganchev kuzman grac¸a joao gillenwater jennifer taskar ben. posterior regularization structured latent variable models. journal machine learning research goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. advances neural information processing systems hadsell raia chopra sumit lecun yann. dimensionality reduction learning invariant mapping. computer vision pattern recognition ieee computer society conference volume ieee kingma diederik mohamed shakir rezende danilo jimenez welling max. semisupervised learning deep generative models. advances neural information processing systems lawrence neil qui˜nonero-candela joaquin. local distance preservation gp-lvm back constraints. proceedings international conference machine learning kuang-chih jeffrey kriegman david. acquiring linear subspaces face recognition variable lighting. pattern analysis machine intelligence ieee transactions reed scott sohn kihyuk zhang yuting honglak. learning disentangle factors variation manifold interaction. proceedings international conference machine learning titsias michalis l´azaro-gredilla miguel. doubly stochastic variational bayes nonconjugate inference. proceedings international conference machine learning wang jiang song yang leung thomas rosenberg chuck wang jingbin philbin james chen ying. learning ﬁne-grained image similarity deep ranking. computer vision pattern recognition ieee conference ieee give experimental details. experiments used diagonal normal distributions priors latent space rmsprop momentum adam optimizer. experiments graphics processing units using theano implementation take hours each. simulate oracle question using annotations provided dataset. question sample label distribution checking match produce answers triplets generated. question three resort sampling relative distances target angles given angles produce triplet information. ﬁnally generate different simulated oracles oracleid oracleaz oracleall correspond asking ﬁrst question second three mixed. sample triplets oracle-question random repeat process times account sampling bias report means reruns experiments. extra illustrative example combining three oracles samples triplets question total triplets inform model. numbers sound high note large combinatorial space possible triplets explored. proceed learn fully unsupervised models images using architecture hidden deterministic units latent variables. deterministic layers tanh nonlinearities. analogous metricl model without generative path supervised learning model optimizing triplet embeddings given images euclidian loss function. order compute likelihood triplet likelihood need calculate expensive divergence term using information theoretic quantity jensen shannon divergence deﬁned section practice term typically intractable analytically since involves divergence involving mixture possibly disjoint distributions. order evaluate divergence exhaustive sampling methods need used. order avoid expensive sampling steps training explore approximations term presented experiments used approximation inaccurate globally empirically fast yields better results divergence eucilidian distance becomes accurate limit closeby distributions. clearly using full beneﬁcial model yields stronger posterior regularization allowing learn efﬁciently triplets especially combination full covariance latent spaces. overview previous approximations related given tried previously known monte carlo-based approximations explore novel deterministic approximations term expect show empirical performance update paper follow-up work. trained model yale faces hidden units used similarly masked experiments main paper. space supplement show samples figure form image algebra adding components various images together. figure select trainings image face select latent encoding corresponding identify. middle select three training images another face different light conditions select light variables according mask. bottom synthesize images face light images clamped observations noisy faces generated look like face light conditions like middle one. sample another batch triplets yale dataset query rerun opbn-masked varying number triplets clarify effect. show results table evident queries improve triplets. want note numbers based single sampling triplets thus subject sampling noise. chance less good triplets contained set. generated perturbed version mnist dataset order show settings proposed approach used interestingly. normal mnist letter generated depending class. eyeballing style variations seen captured meta-data order used evaluation. proceed take mnist digits equal proportions class rotate progressively increasing positive angles. creates effect pushing digits fall towards right side trajectory shown figure questions simulated oracles following chose include order labels similarity digit-images could also deﬁned value digit could used reasoning tasks performing mathematical operations inferred values ordered manifold. setting assume similarity implied label dissimilarity else. future work plan exploit semantic oracles understand order reasoning-related tasks. experiment using deterministic layers hidden units tanh nonlinearities latent variables opbn masked outperform table mask variables also manage factorize latent space sharply variables query setting triplets. also observed using less triplets second query suffered performance whoch makes intuitive sense since learning rotation harder task learning match labels. terms ﬁnal performance observe opbn strongly reduce predictive errors tasks although synthetic tasks actually quite hard. train metric learning opbn triplets datapoints variety oracle settings. oracles perturbed noise meaning fraction equal triplets ﬂipped thus wrong. experiment illustrates robustness generative aspect model gives whereas evident metric learning approaches lose performance since cannot beneﬁt modeling observations directly. observe opbn learns signiﬁcantly better representations predict azimuth elevation classiﬁcation label. opbn performs similarly good terms triplet prediction.", "year": 2015}