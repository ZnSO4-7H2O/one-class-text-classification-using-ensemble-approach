{"title": "Predicting Abnormal Returns From News Using Text Classification", "tag": ["cs.LG", "cs.AI"], "abstract": "We show how text from news articles can be used to predict intraday price movements of financial assets using support vector machines. Multiple kernel learning is used to combine equity returns with text as predictive features to increase classification performance and we develop an analytic center cutting plane method to solve the kernel learning problem efficiently. We observe that while the direction of returns is not predictable using either text or returns, their size is, with text features producing significantly better performance than historical returns alone.", "text": "show text news articles used predict intraday price movements ﬁnancial assets using support vector machines. multiple kernel learning used combine equity returns text predictive features increase classiﬁcation performance develop analytic center cutting plane method solve kernel learning problem efﬁciently. observe direction returns predictable using either text returns size text features producing signiﬁcantly better performance historical returns alone. asset pricing models often describe arrival novel information jump process characteristics underlying jump process coarsely related underlying source information. similarly time series models arch garch developed forecast volatility using asset returns data methods also ignore source market volatility ﬁnancial news. objective show text classiﬁcation techniques allow much reﬁned analysis impact news asset prices. empirical studies examine stock return predictability traced back fama among others showed signiﬁcant autocorrelation daily returns thirty stocks dow-jones industrial average. similar studies conducted taylor ding signiﬁcant autocorrelation squared absolute returns effects also observed intraday volatility patterns demonstrated wood andersen bollerslev absolute returns. ﬁndings tend demonstrate that given solely historical stock returns future stock returns predictable volatility impact news articles also studied extensively. ederington example studied price ﬂuctuations interest rate foreign exchange futures markets following macroeconomic announcements showed prices mostly adjusted within minute major announcements. mitchell mulherin aggregated daily announcements jones company single variable found correlation market absolute returns weak correlation ﬁrm-speciﬁc absolute returns. however kalev aggregated intraday news concerning companies listed australian stock exchange exogenous variable garch model found signiﬁcant predictive power. ﬁndings attributed conditioning volatility news. results improved restricting type news articles included. common techniques forecasting volatility often based autoregressive conditional heteroskedasticity generalized arch models mentioned above. example intraday volatility foreign exchange equity markets modeled ma-garch andersen bollerslev arch taylor bollerslev survey arch garch models various applications. machine learning techniques neural networks support vector machines also used forecast volatility. neural networks used malliaris salchenberger forecast implied volatility options index support vector machines used forecast volatility index using daily returns gavrishchaka banerjee here show information press releases used predict intraday abnormal returns relatively high accuracy. consistent taylor ding however direction returns found predictable. form text classiﬁcation problem press releases labeled positive absolute return jumps time news made public. support vector machines used solve classiﬁcation problem using equity returns word frequencies press releases. furthermore multiple kernel learning optimally combine equity returns text predictive features increase classiﬁcation performance. text classiﬁcation well-studied problem machine learning joachims among many others show signiﬁcantly outperform classic methods naive bayes). initially naive bayes classiﬁers used wuthrich three-class classiﬁcation index using daily returns labels. news taken several sources reuters wall street journal. five-class classiﬁcation naive bayes classiﬁers used lavrenko classify intraday price trends articles published yahoofinance website. support vector machines also used classify intraday price trends fung using reuters articles m.-a.mittermayer knolmayer four-class classiﬁcation stock returns using press releases prnewswire. text classiﬁcation also used directly predict volatility survey trading systems text). recently robertson used predict articles bloomberg service followed abnormally large volatility; articles deemed important aggregated variable used garch model similar kalev kogan support vector regression forecast stock return volatility based text mandated reports. found reports published sarbanes-oxley improved forecasts baseline methods text. generating trading rules genetic programming another incorporate text ﬁnancial trading systems. trading rules created dempster jones using foreign exchange markets based technical indicators extended austin combine technical indicators non-publicly available information. ensemble methods used thomas create rules based headlines posted yahoo internet message boards. contribution twofold. first abnormal returns predicted using text classiﬁcation techniques similar m.-a.mittermayer knolmayer given press release predict whether abnormal return occur next minutes using text past absolute returns. algorithm m.-a.mittermayer knolmayer uses text predict whether returns jump remain within bounds unclear within minutes press release. consider nine months subset eight years press releases used here. experiments analyze predictability absolute returns many horizons demonstrate signiﬁcant initial intraday predictability decreases throughout trading day. second optimally combine text information asset price time series signiﬁcantly enhance classiﬁcation performance using multiple kernel learning analytic center cutting plane method solve resulting problem. accpm particularly efﬁcient problems objective function gradient hard evaluate whose feasible simple enough analytic centers computed efﬁciently. furthermore suffer conditioning issues accpm achieve higher precision targets ﬁrst-order methods. rest paper organized follows. section details text classiﬁcation problem solve provides predictability results using using either text absolute returns features. section describes multiple kernel learning framework details analytic center cutting plane algorithm used solve resulting optimization problem. finally enhance prediction performance. here describe support vector machines used make binary predictions equity returns. experimental setup follows results text stock return data separately make predictions. support vector machines form linear classiﬁer maximizing distance known margin parallel hyperplanes separate groups data detailed reference svm). illustrated figure linear classiﬁer deﬁned hyperplane midway separating hyperplanes. given linear classiﬁer ﬁnding maximum margin classiﬁer formulated margin computed explicitly linearly constrained quadratic program variables data point features label points. ﬁrst constraint dictates points equivalent labels side line. slack variable allows data misclassiﬁed penalized rate objective svms also handle nonseparable data. optimal objective value viewed upper bound probability misclassiﬁcation given task. results readily extended nonlinear classiﬁcation. given nonlinear classiﬁcation task function maps data input space linearly separable feature space linear classiﬁcation performed. problem becomes numerically difﬁcult high dimensional feature spaces crucially complexity solving dual variables depend dimension feature space. input problem matrix given mapping need speciﬁed hence l-dimensional linearly constrained quadratic program suffer high figure input space feature space. nonlinear classiﬁcation data mapped input space feature space. linear classiﬁcation performed support vector machines mapped data feature space. data features entirely described matrix called kernel must satisfy i.e. positive-semideﬁnite exists mapping thus svms require input kernel function table lists several classic kernel functions used text classiﬁcation corresponding different implicit mapping feature space. many efﬁcient algorithms developed solving quadratic program common technique uses sequential minimal optimization coordinate descent variables ﬁxed remaining two-dimensional problem solved explicitly. experiments paper libsvm package implementing method. data vectors following experiments formed using text features equity returns features. text features extracted press releases bag-of-words. ﬁxed important words referred dictionary predetermined; instance words increase decrease acqui lead down bankrupt powerful potential integrat considered. stems words used words acquired acquisition considered identical. following microsoft press release bag-of-words representation figure example. here number times word dictionary occurs press release. london dec. microsoft corp. acquired multimap united kingdoms technology companies leading online mapping services world. acquisition gives microsoft powerful location mapping technology complement existing offerings virtual earth live search windows live services aquantive advertising platform future integration potential range microsoft products platforms. terms deal disclosed. number times term occurs document number documents term appears. weighting increases importance words show often within document also decreases importance terms appear many documents useful discrimination. advanced text representations include latent semantic analysis probabilistic latent semantic analysis latent dirichlet allocation regards equity return features corresponds time series returns based equity prices leading time press release published. press releases published thus sufﬁcient stock price data create equity returns features used experiments consider news published experiments based press releases issued eight year period prnewswire. focus news related publicly traded companies issued least press releases prnewswire time frame. press releases tagged multiple stock tickers discarded experiments. intraday price data taken nyse trade quote database wharton research data services. eight year horizon divided monthly data. order simulate practical environment decision models calibrated year press release data used make predictions articles released following month; thus tests out-of-sample. making predictions particular month year training window slides forward month month test window. price data exists minutes following news particular article discarded experiments make predictions time horizons longer minutes. overall means training testing data sizes decrease forecasting horizon. figure displays overall amount testing data average amount training testing data used time window figure aggregate amount test press releases average training/testing window average training testing windows year month respectively. aggregated test data windows used calculate performance measures. kernel functions table contain parameters requiring calibration. reasonable values parameter chosen combination parameter values perform n-fold cross-validation optimize parameter values. training data separated equal folds. fold pulled successively model trained remaining data tested extracted fold. predeﬁned classiﬁcation performance measure averaged test folds optimal parameters determined give best performance. since distribution words occurring press releases change time perform chronological one-fold cross validation here. training data ordered according release dates model trained news published ﬁxed date tested remaining press releases several potential measures deﬁned table note problem also parameter must calibrated using cross-validation. beyond standard accuracy recall measures measure prediction performance ﬁnancially intuitive metric sharpe ratio deﬁned ratio expected return standard deviation returns following trading strategy every time news article released made stock return either lose according whether prediction correct. daily returns computed return playing game press release published given day. sharpe ratio estimated using mean standard deviation daily returns annualized. additional results given using classic performance measure accuracy deﬁned table performance measures. number periods year expected return period given trading strategy standard deviation binary classiﬁcation respectively true positives true negatives false positives false negatives. percentage correct predictions made however results based cross-validating sharpe ratios. accuracy displayed intuitive meaning binary classiﬁcation direct ﬁnancial interpretation. another potential measure recall deﬁned percentage positive data points predicted positive. general tradeoff accuracy recall would used measure cross-validation. instead tradeoff risk versus returns optimizing sharpe ratio. support vector machines used make predictions stock returns news regarding company published. section input feature vector either bag-of-words text vector time series past equity returns inputs single feature vector. predictions considered every minute interval following release article either maximum minutes close business day; i.e. article comes make predictions equity returns articles released business considered here. different classiﬁcation tasks performed. experiment direction returns predicted labeling press releases according whether future return positive negative. experiment predict abnormal returns deﬁned absolute return greater predeﬁned threshold. different thresholds correspond different classiﬁcation tasks expect larger jumps easier predict smaller ones latter correspond true abnormal returns. veriﬁed experiments below. performance predicting direction equity returns following press releases displayed figure shows weakest performance using either time series returns text features. predictability found direction equity returns consistent literature regarding stock return predictability. results displayed linear kernels single feature type. instead ﬁctitious trading strategy used abnormal return predictions directional results sell strategy based true equity returns. similar performance using gaussian kernels observed independent experiments. predicting direction returns difﬁcult task abnormal returns appear predictable using either time series absolute returns text press releases. figure shows time series absolute returns contains useful information intraday predictions even better predictions made using text threshold deﬁning abnormal returns window percentile absolute returns observed training data. performance using text kernels given full data press released business well reduced data compare experiments returns features. performance full data also broken according press released difference curves labeled former trains models using complete data including articles released open business latter ﬁrst minutes news train models. difference performance might attributed importance articles. sharpe ratio using reduced data greater news published fewer articles published ﬁrst minutes published remainder business day. note high sharpe ratio likely simple strategy traded here; imply high sharpe ratio generated practice rather indicates potential statistical arbitrage. decreasing trend observed performance measures intraday time horizon intuitive public information absorbed prices time hence articles slowly lose predictive power prediction horizon increases. figure compares performance predicting abnormal returns threshold taken either percentile absolute returns within training set. results using linear kernels annualized sharpe ratios shown here. decreasing threshold percentile slightly decreases performance using absolute returns. however huge decrease performance using text. increasing threshold percentile improves performance relative percentile measures. demonstrates sensitivity performance respect threshold. percentile absolute returns data large enough deﬁne true abnormal return whereas percentiles deﬁne abnormal jumps. absolute returns known predictability small movements question remains text poor source information predicting small jumps. figure illustrates impact percentile threshold performance. predictions made minutes future. press releases news bigger impact future returns past market data. publicly available information aside returns text considered predicting movements equity returns. time strong impact absolute returns demonstrated andersen bollerslev figure shows time effect following release press prnewswire data set. clear absolute returns following press released early average much higher midday. between. linear kernels created features used experiments absolute returns text features results displayed figure note gaussian kernels exactly performance using binary features. done analysis text data performance shown using press released business well reduced data news published training data beginning clearly important since curve labeled weakest performance. improved performance curve labeled attributed pattern seen figure training full data allows model distinguish absolute returns early versus midday. similar experiments using week features showed figure accuracy annualized daily sharpe ratio predicting abnormal returns direction returns using returns text data linear kernels. performance using text given full data well reduced data used experiments returns features. curves labeled trains models using complete data including articles released open business curved labeled ﬁrst minutes news train models. point x-axis corresponds predicting abnormal return minutes press release issued. percentile absolute returns observed training data used threshold deﬁning abnormal return. weak performance thus displayed. time effect exhibits predictability note experiments text absolute returns data time stamp features hence performance text absolute returns attributed time effects. furthermore experiments combining different pieces publicly available information show time effects less useful text returns data. course related market microstructure effects could useful predictability amount news released throughout industry respective companies. figure accuracy annualized daily sharpe ratio predicting abnormal returns using returns text data linear kernels. point x-axis corresponds predicting abnormal return minutes press release issued. percentile absolute returns observed training data used thresholds deﬁning abnormal returns. figure accuracy annualized sharpe ratio predicting abnormal returns minutes future percentile thresholds increased linear kernels absolute returns text used. press releases news bigger impact future returns past market data. main focus intraday movements next text absolute returns make daily predictions abnormal returns show trade predictions. experiments text data subset companies returns data also intraday time series above computed minute return prior figure accuracy annualized daily sharpe ratio predicting abnormal returns using time day. performance using time given full data well reduced data used experiments returns features. curves labeled trains models using complete news data including articles released open business curved labeled ﬁrst minutes news train models. point x-axis corresponds predicting abnormal return minutes press release issued. percentile absolute returns observed training data used thresholds deﬁning abnormal returns. rather ﬁctitious trading strategy above delta-hedged covered call options used abnormal returns order occurrence abnormal return strategy takes long position call option since direction price movement position kept delta neutral taking short position delta shares stock position exited following going short call option long delta shares stock. abnormal return takes opposite positions. equity positions closing prices following release press closing price following day. option prices average highest closing lowest closing price observed press release. normalize size positions always take position delta times worth respective stock proper amount call option. proﬁt loss strategies displayed figure using equity options data. left side shows predicting abnormal return occur right side shows predicting price movement. potentially large upside predicting abnormal returns however limited upside predicting movement incorrect prediction movement potentially large downside. text features used related experiments ﬁgures using returns features exhibit similar patterns. figure proﬁt loss trading delta-hedged covered call options. left ﬁgure displays trading predictions abnormal return follows release press right displays resulting predictions abnormal return occurs. potentially large upside predicting abnormal returns however limited upside predicting movement incorrect prediction movement potentially large downside. text features used related experiments ﬁgures using returns features exhibit similar patterns. table displays results three strategies. trade makes appropriate trade based predictions long takes positions abnormal return predicted short takes positions price movement predicted. percentile absolute returns observed training data used thresholds deﬁning abnormal returns. results imply downside predicting movement greatly decreases performance. long strategy performs best large upside limited downside. addition number movement predictions made using absolute returns features much larger using text. likely cause negative sharpe ratio trade absolute returns. results using higher thresholds show similar performance trends associated ﬁgures even clearer u-shaped patterns table performance delta-hedged covered call option strategies. trade makes appropriate trade based predictions long takes positions abnormal return predicted short takes positions price movement predicted. percentile absolute returns observed training data used thresholds deﬁning abnormal returns. discuss multiple kernel learning provides method optimally combining text return data order make predictions. cutting plane algorithm amenable large-scale kernels described compared another recent method mkl. multiple kernel learning seeks minimize upper bound misclassiﬁcation probability learning optimal linear combination kernels lanckriet bach sonnenberg rakotomamonjy zien micchelli pontil kernel learning problem formulated lanckriet written minimum problem viewed upper bound probability misclassiﬁcation. general sets enforcing mercer’s condition kernel makes kernel learning computationally challenging task. problem lanckriet particular instance kernel learning solves problem predeﬁned kernels. note cross-validation kernel parameters longer required kernel included desired parameters; however calibration parameter still necessary. kernel learning problem written semideﬁnite program nonnegativity constraints kernel weights shown lanckriet currently semideﬁnite programming solvers handle large kernel learning problem instances efﬁciently. restriction enforces mercer’s condition reduces problem quadratically constrained optimization problem dual differentiable problem constraints svm. sequential minimal optimization algorithm iteratively optimizes pairs variables used solve problem approaches solving larger scale problems written wrapper around computation. example approach detailed sonnenberg solves semi-inﬁnite linear program formulation variables problem derived moving objective constraints. algorithm iteratively adds cutting planes approximate inﬁnite linear constraints solution found. found solving using current kernel diki. formulation adapted multiclass zien similar silp solved. latest formulation rakotomamonjy optimal solution using kernel diki. becomes smooth minimization problem subject constraints linear equality constraint solved using reduced gradient method line search. computation objective gradient requires solving svm. experiments rakotomamonjy show method efﬁcient compared semiinﬁnite linear program solved above. svms required warm-starting makes method somewhat faster. still reduced gradient method suffers numerically large kernels requires computing many gradients hence solving many numerically expensive classiﬁcation problems. next detail efﬁcient algorithm solving problem requires less computations gradient descent methods. analytic center cutting plane method iteratively reduces volume localizing containing optimum using cuts derived ﬁrst order convexity property volume reduced localizing converges target precision. iteration center computed smaller localizing point added split create li+. method modiﬁed according center selected; case center selected analytic center deﬁned below. note method require differentiability still exhibits linear convergence. write rn|ad ﬁrst localization optimal solution. method described algorithm complete reference cutting plane methods). complexity iteration breaks follows. complexity. accpm provably convergent iterations using elimination scheme atkinson vaidya keeps complexity localization bounded. schemes available slightly different complexities achieved gofﬁn vial using approximate centers example. practice accpm usually converges linearly seen figure uses kernels dimension text data. illustrate affect increasing number kernels analytic center problem figure shows time increasing number kernels increases. figure convergence semilog plot accpm shows average versus iteration number. plot time ﬁrst iterations versus number kernels plots give averages experiments dashed lines plus minus standard deviation. experiments accpm converges linearly high precision. gradient methods reduced gradient method used simplemkl converge linearly require expensive line searches. therefore gradient methods sometimes converge linearly faster rate accpm certain problems often much slower need solve many problems iteration. empirically gradient methods tend require many gradient evaluations localization techniques discussed here. accpm computes objective gradient exactly iteration analytic center problem remains relatively cheap respect computation dimension analytic centering problem small application. thresholding small kernel weights zero reduce dimension analytic center problem. described above accpm computes computation iteration converges linearly. compare method denote accpmmkl simplemkl algorithm uses reduced gradient method also converges fast computes many svms perform line searches. svms line search speeded using warm-starting described rakotomamonjy practice observe savings warm-starting often sufﬁce make gradient method efﬁcient accpm. kernels usually required kernels eliminated efﬁciently beforehand using cross-validation hence several families kernels kernels family. experiment uses linear kernel number gaussian polynomial kernels giving total kernels experiment. duality experiment order compare algorithms identical problems. fairness compare simplemkl implementation accpmmkl using package simplemkl allows warm-starting implemented matlab.). ﬁnal column also give running time accpmmkl using libsvm solver without warm-starting. following tables demonstrate computational efﬁciency show predictive performance; algorithms solve optimization problem stopping criterion. high precision signiﬁcantly increase prediction performance. results averages experiments done linux -bit servers cpus. table shows accpm efﬁcient multiple kernel learning problem text classiﬁcation example. savings warm-starting simplemkl overcome beneﬁt fewer computations iteration accpmmkl. furthermore using faster solver libsvm produces better performance even without warm-starting. number kernels used accpmmkl higher simplemkl loose duality here. reduced gradient method simplemkl often stops much higher precision checked line search achieve high precision single iteration higher precision reduces number kernels. however slightly higher precision simplemkl often stall converge slowly; method sensitive target precision. accpmmkl method stops desired duality checked iteration linear convergence; however convergence much stable consistent data sets. accpmmkl number svms equivalent number iterations. table shows example accpmmkl outperformed simplemkl. occurs classiﬁcation task extremely easy optimal kernels singleton. case simplemkl converges fewer svms. note though accpmmkl libsvm still faster here. examples illustrate simplemkl trains many svms whenever optimal kernels includes table numerical performance simplemkl versus accpmmkl classiﬁcation text classiﬁcation data. accpmmkl outperforms simplemkl terms iterations time. using libsvm solve problems enhances performance. results averages runs. experiments done using solver simplemkl toolbox except ﬁnal column uses libsvm. time seconds. number training samples kernel. input kernel. overall accpmmkl advantages consistent convergence rates data sets fewer computations relevant data sets ability achieve high precision targets. table numerical performance simplemkl versus accpmmkl classiﬁcation mushroom data. simplemkl outperforms accpmmkl classiﬁcation task easy demonstrated optimality single kernel otherwise performs slower. experiments done using solver simplemkl toolbox except ﬁnal column uses libsvm. time seconds. number training instances kernel. multiple kernel learning used combine text returns data order predict abnormal equity returns. kernels created using text features done section additional kernels created time series absolute returns. experiments linear four gaussian kernels normalized unit trace feature type. problem solved using ...kd linear kernels based time week additional identity matrix described hence obtain single optimal kernel convex combination input kernels. technique applied lanckriet combine protein sequences gene expression data order recognize different protein classes. performance using percentile absolute returns threshold abnormality displayed figure results section text absolute returns linear kernels superimposed performance combining text absolute returns time stamps. predictions using text returns exhibit good performance combining signiﬁcantly improves performance accuracy annualized daily sharpe ratio. figure accuracy sharpe ratio using multiple kernels. mixes possible kernels point x-axis corresponds predicting abnormal return minutes press release issued. percentile absolute returns observed training data used threshold deﬁning abnormal return. next analyze impact various kernels. figure displays optimal kernel weights found solving time horizon kernel weights represented colored fractions single length one. kernels largest coefﬁcients gaussian text kernels linear text kernel identity kernel gaussian absolute returns kernels. note magnitudes coefﬁcients perfectly indicative importance respective features. hence optimal kernels supports evidence mixing news absolute returns improves performance. another important observation kernel weights remain relatively constant time. kernel weights corresponds independent classiﬁcation task persistent kernel weights imply combining important kernels detects meaningful signal beyond found using text return figure optimal kernel coefﬁcients using using possible kernels percentile threshold deﬁne abnormal returns. kernels labeled. point x-axis corresponds predicting abnormal return minutes press release issued. figure shows performance using multiple kernels predicting abnormal returns change threshold percentiles absolute returns training data. cases slight improvement performance using single kernels. figure displays optimal kernel weights experiments indeed experiments text absolute returns. previously text shown predictability higher threshold absolute returns performed better lower threshold. kernel weights versus percentile threshold reﬂect observation. successful performance using multiple kernel learning highly dependent proper choice input kernels. here show high accuracy optimal kernels crucial good performance including optimal kernels necessary. addition show insensitive inclusion kernels information following four experiments different kernels sets exemplify observations. first linear kernels using text absolute returns time week included. next equal weighting thirteen kernels used. another test performs using thirteen kernels addition three random kernels ﬁnal experiment uses four gaussian kernels figure displays accuracy sharpe ratios experiments. performance using linear kernels high since linear kernels achieved equivalent performance gaussian kernels using svm. adding three random kernels thirteen kernels achieve high performance signiﬁcantly figure optimal kernel coefﬁcients using possible kernels percentiles thresholds. kernels labeled. point x-axis corresponds predicting abnormal return minutes press release issued. figure accuracy annualized daily sharpe ratio predicting abnormal returns using multiple kernels. point x-axis corresponds predicting abnormal return minutes press release issued. percentiles absolute returns used deﬁne abnormal returns. impact results either. three random kernels negligible coefﬁcients across horizon noticeable decrease performance seen using equally weighted kernels even signiﬁcant decrease observed using highly suboptimal kernels. small data showed even smaller decrease performance equally weighted kernels. demonstrates need solved high tolerance order achieve good performance figure accuracy sharpe ratio different kernel sets. linear kerns uses linear kernels. equal coeffs uses equally weighted kernels. rand kerns adds random kernels kernels. kerns uses gaussian kernels misspeciﬁed constants percentile used threshold deﬁne abnormal returns. found signiﬁcant performance predicting abnormal returns using text absolute returns features. addition multiple kernel learning introduced application greatly improved performance. finally cutting plane algorithm solving large-scale problems described efﬁciency relative current solvers demonstrated. experiments could course reﬁned implementing tradeable strategy based abnormal return predictions done daily predictions section unfortunately equity options liquid assets would produce realistic performance metrics intraday options prices publicly available. important direction research feature selection i.e. choosing words dictionary. experiments simple handpicked words. techniques recursive feature elimination used select words performance similar results using handpicked dictionary. advanced methods latent semantic analysis probabilistic latent semantic analysis latent dirichlet allocation considered. additionally industry-speciﬁc dictionaries developed used associated subset companies. another natural extension work regression analysis. support vector regressions regression counterpart extend mkl. text combined returns order forecast intraday volatility abnormal returns using mkl. authors grateful jonathan lange kevin superb research assistance. would also like acknowledge support grant dms- grant ses- career award peek junior faculty fellowship howard wentz junior faculty award.", "year": 2008}