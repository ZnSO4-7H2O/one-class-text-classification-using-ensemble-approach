{"title": "An efficient algorithm for contextual bandits with knapsacks, and an  extension to concave objectives", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We consider a contextual version of multi-armed bandit problem with global knapsack constraints. In each round, the outcome of pulling an arm is a scalar reward and a resource consumption vector, both dependent on the context, and the global knapsack constraints require the total consumption for each resource to be below some pre-fixed budget. The learning agent competes with an arbitrary set of context-dependent policies. This problem was introduced by Badanidiyuru et al. (2014), who gave a computationally inefficient algorithm with near-optimal regret bounds for it. We give a computationally efficient algorithm for this problem with slightly better regret bounds, by generalizing the approach of Agarwal et al. (2014) for the non-constrained version of the problem. The computational time of our algorithm scales logarithmically in the size of the policy space. This answers the main open question of Badanidiyuru et al. (2014). We also extend our results to a variant where there are no knapsack constraints but the objective is an arbitrary Lipschitz concave function of the sum of outcome vectors.", "text": "consider contextual version multi-armed bandit problem global knapsack constraints. round outcome pulling scalar reward resource consumption vector dependent context global knapsack constraints require total consumption resource pre-ﬁxed budget. learning agent competes arbitrary context-dependent policies. problem introduced badanidiyuru gave computationally inefﬁcient algorithm near-optimal regret bounds give computationally efﬁcient algorithm problem slightly better regret bounds generalizing approach agarwal non-constrained version problem. computational time algorithm scales logarithmically size policy space. answers main open question badanidiyuru also extend results variant knapsack constraints objective arbitrary lipschitz concave function outcome vectors. multi-armed bandits classic model studying explorationexploitation tradeoff faced decision-making agent learns maximize cumulative reward sequential experimentation initially unknown environment. contextual bandit problem also known associative reinforcement learning generalizes multi-armed bandits allowing agent take actions based contextual information every round agent observes current context takes action observes reward random variable distribution conditioned context taken action. despite many recent advances successful applications bandits major limitations standard setting lack global constraints common many important real-world applications. example actions taken robot different levels power consumption total power consumed limited capacity battery. online advertising advertiser budget advertisement cannot shown certain number times. dynamic pricing certain number objects sale seller offers prices sequence buyers goal maximizing revenue number sales limited supply. recently papers started address limitation considering special cases single resource budget constraint application-speciﬁc bandit problems ones motivated online advertising dynamic pricing crowdsourcing subsequently badanidiyuru introduced general problem capturing previous formulations. problem called bandits knapsacks different resources pre-speciﬁed budget. action taken agent results d-dimensional resource consumption vector addition regular reward. goal agent maximize total reward keeping cumulative resource consumption budget. model generalized bwcr model agrawal devanur allows arbitrary concave objective convex constraints resource consumption vectors rounds. papers adapted popular upper conﬁdence bound technique obtain near-optimal regret guarantees. however focus non-contextual setting. signiﬁcant recent progress algorithms general contextual bandits context reward arbitrary correlation algorithm competes arbitrary context-dependent policies. dud´ık achieved optimal regret bound remarkably general contextual bandits problem assuming access policy linear optimization oracle instead explicit enumeration policies previous work however algorithm presented dud´ık tractable practice makes many calls optimization oracle. agarwal presented simpler computationally efﬁcient algorithm running time scales square-root logarithm policy space size achieves optimal regret bound. combining contexts resource constraints agrawal devanur also considered static linear contextual version bwcr expected reward linear context. considered special case random linear contextual bandits single budget constraint gave near-optimal regret guarantees badanidiyuru extended general contextual version bandits arbitrary policy sets allow budget constraints thus obtaining contextual version problem called resourceful contextual bandits refer problem cbwk consistent naming related problems deﬁned paper. gave computationally inefﬁcient algorithm based dud´ık regret optimal regimes. algorithm deﬁned mapping history context action computational issue ﬁnding mapping addressed. posed open question achieving computational efﬁciency maintaining similar even sub-optimal regret. main contributions. paper present simple computationally efﬁcient algorithm cbwk/rcb based algorithm agarwal similar agarwal running time algorithm scales square-root logarithm size policy thus resolving main open question posed badanidiyuru algorithm even improves regret bound badanidiyuru factor another improvement badanidiyuru need know marginal distribution contexts algorithm not. feature techniques need modify algorithm agarwal minimal almost blackbox fashion thus retaining structural simplicity algorithm obtaining substantially general results. extend algorithm variant problem call contextual bandits concave rewards every round agent observes context takes actions observes d-dimensional outcome vector goal maximize arbitrary lipschitz concave function average outcome vectors; constraints. allows many interesting applications discussed agrawal devanur setting also substantially general contextual version considered agrawal devanur context ﬁxed dependence assumed linear. organization. section deﬁne cbwk problem state regret bound theorem algorithm detailed section overview regret analysis section section present cbwr problem concave rewards state guaranteed regret bounds outline differences algorithm analysis. complete proofs details provided appendices. ﬁnite actions space possible contexts begin with algorithm given budget proceed rounds every round algorithm observes context chooses action observes reward d-dimensional consumption vector objective take actions maximize total algorithm stops either rounds budget exceeded dimensions whichever occurs ﬁrst. assume actions no-op action i.e. always gives reward consumption vector furthermore make stochastic assumption context reward consumption vectors drawn i.i.d. distribution d×a. distribution unknown algorithm. rithms compete arbitrary policies. ﬁnite policies contexts actions assume policy contains no-op policy always selects no-op action regardless context. global constraints distributions policies could strictly powerful policy itself. algorithms compete powerful stronger guarantee simply competing ﬁxed policies purpose deﬁne pπ∈π convex randomized policy selects action probability =pπ∈ππ=a therefore also refer policy. similarly deﬁne pπ∈π non-negative benchmark regret. benchmark problem optimal static mixed policy budgets required satisﬁed expectation only. e∼d)]] e∼d)]] denote respectively expected reward consumption vector policy call policy feasible policy note always exists feasible policy no-op policy. deﬁne optimal policy feasible policy maximizes expected reward amo. since policy extremely large interesting applications accessing explicit enumeration impractical. purpose efﬁcient implementation instead access maximization oracle. employing oracle common considering contextual bandits arbitrary policies following previous work call oracle oracle amo. policies randomized general results assume without loss generality deterministic. observed badanidiyuru replace randomized policies deterministic policies appending random seed context. blows size context space appear regret bounds. main results. main result computationally efﬁcient low-regret algorithm cbwk. furthermore improve regret bound badanidiyuru factor; present detailed discussion optimality dependence bound. previous work multi-armed bandits know challenges ﬁnding right policy concentrate fast enough empirically best policy probability choosing action must large enough enable sufﬁcient exploration efﬁciently computable. agarwal show addressed solving properly deﬁned optimization problem help amo. additional technical challenge dealing global constraints. mentioned earlier complication arises right away knapsack constraints algorithm compete best mixed policy rather best pure policy. following highlight main technical difﬁculties encounter solution difﬁculties. deﬁnitions place describe algorithm. denote history chosen actions observations time consisting records form denote respectively context action taken reward consumption vector observed time denotes probability action taken. although contains observation vectors chosen actions completed using trick importance sampling every deﬁne ﬁctitious observation vectors given estimates construct optimization problem aims mixed policy small empirical regret time provides sufﬁcient exploration good policies. optimization problem uses quantity dregt empirical regret policy characterize good policies. agarwal deﬁne dregt simply difference empirical reward estimate policy policy highest empirical reward. thus good policies characterized high reward. problem however policy could high reward consumption violates knapsack constraints large margin. policy considered good policy. challenge problem therefore deﬁne single quantity captures goodness policy appropriately combining rewards consumption vectors. combining regret reward constraint violation using multiplier multiplier captures sensitivity problem violation knapsack constraints. easy observe increasing knapsack size increase optimal atmost opt. follows policy violates knapsack constraint achieve rounds pure exploration notational convenience algorithm description index initial exploration rounds major component algorithm started runs following lemma provides bound estimate. proof appears appendix large enough constant speciﬁed later. here budget needed decreased account budget consumed ﬁrst exploration rounds. smaller budget amount ensure high probability algorithm abort time horizon budget violation. vector denote amount vector violates budget i.e. aims mixed policy equivalent ﬁnding returning αq′. denote smoothed projection assigning minimum probability every action depends history time parameter algorithm. rest paper convenience deﬁne constant shown analysis upper bound variance estimates ˆvt. constraints critical deriving regret bound section give algorithm efﬁciently ﬁnds feasible solution ready describe full algorithm summarized algorithm main body algorithm shares structure ilovetoconbandits algorithm contextual bandits important changes necessary deal knapsack constraints. ﬁrst rounds pure exploration calculate given lemma algorithm proceeds epochs pre-deﬁned lengths; epoch consists time steps indexed inclusively. algorithm work epoch schedule satisﬁes results hold schedule however algorithm choose solve frequently lower regret cost higher computational time. epoch computes mixed policy solving instance used entire next epoch. additionally every epoch algorithm computes empirically best policy deﬁned equation algorithm uses default policy sampling process deﬁned below. chosen arbitrarily e.g. uniform policy. sampling process sample step samples action computed mixed policy. takes following input current epoch) since proper distribution sample ﬁrst computes distribution deﬁned algorithm requires solving every epoch. agarwal gave algorithm solves using access amo. similar algorithm except calls replaced calls knapsack constrained optimization problem empirical distribution. optimization problem identical structure optimization problem deﬁning need solve also. solve problems using outlined below. algorithm adapted ilovetoconbandits input epoch schedule allowed failure probability initialize weights epoch round compute lemma round observe context sample. select action observe reward consumption recently gave fast algorithm solve problems kind above given access oracles solve linear optimization problems algorithm makes calls oracles takes additional running time. linear optimization problem equivalent amo; linear function deﬁnes rewards optimizes for. linear optimization problem trivial solve. aside solution output algorithm support equal policies output algorithm hence size section provides outline proof theorem provides bound regret algorithm proof structure similar proof agarwal major differences coming changes necessary deal mixed policies constraint alternately could algorithms vaidya solve problem slightly weaker polynomial running time. here hides terms order logo accuracy needed solution. these rewards afﬁne transformation rewards bring without changing solution. different analysis agarwal difference highlighted proof outline lemma below. bound show implies bound actual reward well probability violating knapsack constraints. start proving empirical average reward consumption vector mixed policy close true averages respectively. deﬁne initial epochs recall minimum probability playing action epoch deﬁned step algorithm therefore initial epochs variance importance sampling estimates small obtain stronger bound estimation error. subsequent epochs decreases error bounds terms variance estimates policy across epochs time deﬁned fact second constraint optimization problem seeks bound variance. proof outline. proof lemma induction using second constraint bound variance below prove base case. proof demonstrates importance appropriately chosing consider epoch completes base case. remaining proof induction using bounds provided lemma epochs terms variance bound variance provided second constraint epoch; inductive hypothesis used proof obtain bounds terms reg. next bound regret epoch using bound reg. simplicity discussion outline steps bounding regret rewards sampled policy epoch note precise following ways. first therefore proper distribution second actual sampling process picks action smoothed projection ˜qµm− ˜qt. however ignore technicalities order across intuition behind proof; technicalities dealt rigorously complete proof provided appendix then using bound kψµm obtain bound equation summing bound epochs using jensen’s inequality convexity obtain bound violation budget constraint converted high probability bound using azuma-hoeffding inequality. section consider version problem concave objective function show efﬁcient algorithm cbwr problem identical cbwk problem except following. outcome round simply vector goal algorithm maximize given algorithm ahead time. optimum mixed policy deﬁned main result section regret bound problem. note regret scales rather since problem deﬁned terms average vectors rather sum. assume represented solve optimization problems following form polynomial time. given remark. special case problem constraints case could deﬁned negative distance constraint set. further could handle concave objective function convex constraints follows. suppose wish maximize every epoch. similar convex optimization problem needs solved every iteration coordinate descent algorithm solving cases problems cast form problem form instance linear optimization oracle deﬁnition show efﬁciently solve convex optimization problem using cutting plane methods making calls oracle. details appendix prove algorithm deﬁnition ofdregt achieves regret bounds theorem empirical regret dregt actual regret close every therefore ﬁrst constraint bounds empirical regret dregt computed policy implies bound actual regret ignoring technicalities sampling process assuming policy used epoch provides bound regret every epoch. regret across epochs combined using jensen’s inequality bounds regret expectation. using azuma-hoeffding’s inequality bound deviation expected reward vector actual reward vector obtain high probability regret bound stated theorem", "year": 2015}