{"title": "Sliced Wasserstein Generative Models", "tag": ["cs.CV", "stat.ML"], "abstract": "In the paper, we introduce a model of sliced optimal transport (SOT), which measures the distribution affinity with sliced Wasserstein distance (SWD). Since SWD enjoys the property of factorizing high-dimensional joint distributions into their multiple one-dimensional marginal distributions, its dual and primal forms can be approximated easier compared to Wasserstein distance (WD). Thus, we propose two types of differentiable SOT blocks to equip modern generative frameworks---Auto-Encoders (AEs) and Generative Adversarial Networks (GANs)---with the primal and dual forms of SWD. The superiority of our SWAE and SWGAN over the state-of-the-art generative models is studied both qualitatively and quantitatively on standard benchmarks.", "text": "miyato wasserstein —proposed make optimal transport theory measure distribution distance wasserstein distance proved better properties divergences employed early generative models. nevertheless bottlenecks. example high-dimensional space known primal form intractable although works attempted propose relaxed unconstrained versions primal form. dual form derived easier still suffers challenges approximate k-lipschitz constraint required wasserstein metric particularly weight clipping technique used merely covers subset k-lipschitz functions studied adopted -lipschitz gradient penalty relies limited samples approximate k-lipschitz constraint high-dimensional domain overcome weakness original generative models alternatively exploit generative modeling scheme viewpoint sliced better distribution transfer. since resulting sliced wasserstein distance obtainable decomposing single ndimensional distribution one-dimensional marginal distributions well approximated independently studied theory able offer favorable model state-of-the-art generative models employing particular introduce based network blocks primal dual forms incorporate modern generative frameworks contributions summarized follows propose novel swae model equips classic differentiable blocks. blocks approximate primal form encoder distribution prior distribution progressively end-to-end network learning fashion. paper introduce model sliced optimal transport measures distribution afﬁnity sliced wasserstein distance since enjoys property factorizing high-dimensional joint distributions multiple one-dimensional marginal distributions dual primal forms approximated easier compared wasserstein distance thus propose types differentiable blocks equip modern generative frameworks—auto-encoders generative adversarial networks —with primal dual forms swd. superiority swae swgan state-of-theart generative models studied qualitatively quantitatively standard benchmarks. domain unsupervised learning experienced tremendous advances potential capitalize large pools unlabeled data. promising approaches generative modeling typically estimates real data distribution either variational inference using auto-encoders adversarial process generative adversarial networks words rather estimating real data distribution directly often learn parameterized distribution high-dimensional real data distribution. learning process statistical divergence kullbackleibler jensen-shannon divergence model distribution real data distribution minimized low-dimensional manifold latent space visually pleasing samples generated. *equal contribution computer vision zurich switzerland visics leuven belgium. correspondence jiqing <jiqing.wuvision.ee.ethz.ch> zhiwu huang <zhiwu.huangvision.ee.ethz.ch>. order train radon transform matrices embedded blocks generalize standard network optimization algorithm stiefel manifolds. evaluate swae swgan models standard benchmarks report visual results quantitative scores fr´echet inception distance achieve superior performances compared state-of-the-art generative models. other leading worse reconstruction making generative models produce blurry samples. solve problem waes apply cost latent variable models reaching state-of-theart results among ae-based generative models speciﬁcally waes proposed relaxed formulation estimate primal form decoding introduced additional divergence regularize encoding literature generative modeling often need measure agreement probability distributions many ways among classical kullback-leibler jensenshannon divergences widely used. recently introduced powerful family statistical distances namely wasserstein distance address mode collapse instability issues. originally studied context optimal transport theory primal formulation denotes joint distributions whose marginals respectively general cost function. general normally assumed metric space metric cost function case p-th root respect so-called p-wd. show kantorovich’s dual ex∼px -lipschitz functions. side note dual becomes replace lipk wasserstein paradigm classic aebased generative models vaes employs variational inference minimize upper bound match between model distribution decoder real image distribution imposing regularization match model distribution encoder prior distribution. unfortunately constraint typically makes variational problem hard solve. studied latent codes different samples generally forced stay close random noise nonparametric encoders cost function hyperparameter divergence marginal distribution prior distribution ﬁnally instantiated maximum mean discrepancy gans regarded distribution matching strategy differs employed plan decoding since indeed non-trivial apply primal form constraint encoding especially latent codes hundreds dimensions. wasserstein gans following generative adversarial networks wgans also established min-max adversarial game competing networks generator network maps source noise input space discriminator network receives either generated sample true data sample must distinguish them. stabilize training better image generation original wgan proposed approximate dual form adopting weight clipping strategy however satisﬁes k-lipschitz constraint poorly. alleviate problem approximation dual form improved training wasserstein proposed penalize norm gradient critic respect limited number input samples. particular gradient penalty simply added basic wgan loss following full objective along straight lines pairs points sampled gradient w.r.t. pointed sufﬁcient approximate k-lipschitz constraint high-dimensional domain limited samples. idea underlying sliced wasserstein distance slice plane lines passing origin project measures onto lines distances computed integrate distances possible lines. formally deﬁned results explicit beneﬁt simpler onedimensional density estimation compared direct highdimensional density estimation meanwhile several works exploit fact one-dimensional p-wd closed form solution plan. speciﬁcally exists unique monotonically increasing transport corresponding cumulative distribution functions probability density functions computed transport one-dimensional p-wd distributions consequently computed furthermore proven valid distance equivalent accordingly contrast original tends promising potential enhance modern generative modeling especially access samples high-dimensional pdfs. better approximation achieved iterative distribution transfer technique proved converge well optimal iteration number large enough. specially iteration ﬁrst randomly samples radon transform matrix rn×n satisfying orthogonality approximation radon transform leads series one-dimensional marginal pdfs transfers current distribution source data target distribution matching one-dimensional marginals minimize progressively iteration typically repeated number times. studying advantages section make ﬁrst attempt introduce plan literature generative modeling. mentioned before practice existing methods including typically demand large number iterations achieve favorable solution. furthermore non-trivial apply directly context neural network optimization. overcome limitations propose novel model adapts technique network setting. speciﬁcally paper proposes limited number differentiable blocks optimization primal form context aes. improved approximation primal form latent space using proposed primal blocks leads better generation results. also propose variant blocks dual case context gans. beneﬁting radon transform effectively factorizes high-dimensional joint distributions one-dimensional marginal distributions dual form estimated easier leading better metric generation. state-of-the-art ae-based generative model proposed based constraint encoding relaxed plan decoding. moderates problem proximate latent codes impose consistent constraint encoding leading sub-optimal solution. since highly non-trivial approximate primal form original latent codes propose express probabilistic encoders plan instead aims match encoder distribution prior distribution latent space primal form swd. particular introduce implicit approximation primal form encoders full objective whole model avoids explicit regularization. formally objective swae model optimization primal form prior encoder distributions design type differentiable blocks consist radon transform matching shown fig. inspired show carefully selected sequence radon transform matrices leads faster convergence optimal stack limited number differentiable blocks encoder learn favorable sequence radon transform matrices realizing plan deep learning manner. begin with let’s denote input data rn×n encoder ﬁrst uses common encoder applies designed primal blocks output denoted rk×n data dimension sample number respectively. primal block implemented following steps. step fig. first project latent codes radon transform matrix rk×k indicates radon transform projecting k-dimensional distribution -dimensional marginal distributions. cdfs input data input noise respectively) solved using discrete look-up tables. practice make process differentiable propose employ piece-wise interpolation. specially approximate cdfs ﬁrst estimate pdfs. technically pdfs estimated histogram assignment target data center however make operation differentiable context backpropagation propose soft assignment version instead assigns weight target data cluster proportional proximity relative proximities centres. ranges highest weight assigned closest cluster center. parameter controls decay response magnitude distance. remark setting returns original histogram hard assignment closest center bins. practice note estimation performed one-dimensional data thus sufﬁces estimate distribution moderate amount samples. supported universal approximation theorem neural network expected dual blocks well approximate one-dimensional dual respect arbitrary angle. also intimately relates fact easily satisfy k-lipschitz constraint imposing k-lipschitz gradient penalty long reasonable activation function. eventually computing bi)) approximate integral complete discriminator avoid gradient explosion vanishing implicitly constraint gradients imposing gradient regularizer entire ﬁnal objective thus follow given radon transform matrix orthogonal throughout training cannot directly apply standard optimization algorithm. meanwhile widely known space orthogonal matrices actually stiefel manifold. hence need update curved manifold instead euclidean space. building upon manifold-valued weight update rule well-studied generalize optimization algorithm stiefel manifolds. following standard optimization riemannian manifolds ﬁrst employ parallel transport transport euclidean gradient tangent space anchored orthogonal matrix tangent space orthogonal matrix θt+. resulting euclidean gradient subtracted normal component euclidean gradient loss k-th models desirable. standard framework relies adversarial training generator discriminator. latter typically required explicitly compute useful distance fake real data distribution proposed primal blocks transfer source distribution target distribution implicitly measuring swd. address issue resort dual form extending design blocks dual version. though k-lipschitz gradient penalty sufﬁcient high dimensional space relatively easy satisfy k-lipschitz constraint one-dimensional space potential advantage applying dual form -swd i.e. marginal distributions obtained radon transform thus instead approximating dual n-dimensional propose sliced version wasserstein gans estimate dual one-dimensional marginal distributions required -swd. since real data distribution supported low-dimensional manifold follow setting classic gans’ generator ﬁrst encode inputs data rn×n lower-dimensional latent codes rk×n indicates encoder. then apply dual block approximate optimal step fig. ﬁrst project latent codes radon transform matrices rk×k corresponds radon transform projecting k-dimensional distribution -dimensional marginal distributions. layer subsequently searching along tangential direction leads update tangent space stiefel manifold. resulting update projected back stiefel manifold retraction operation details riemannian geometry stiefel manifolds retraction operation riemannian manifolds refer readers accordingly update current orthogonal matrix stiefel manifold respects following form denotes retraction operation actually corresponds decomposition learning rate denotes standard optimization ∇lθtt normal component euclidean gradient ∇lθt computed conventional backpropagation. experimental study favor updating radon transfer matrices adam optimization generalized stiefel manifold discussed above rest weights updated standard adam optimization. conduct various experiments three standard benchmarks including cifar- celeba lsun evaluate proposed swae swgan models. recently introduced fr´echet inception distance measure difference fake real data distribution veriﬁed measurement similar human judgment inception score later conducted thorough large-scale investigation original variants evaluating scores. therefore present visual results also evaluate scores datasets justify superiority models. compare swae wae-mmd wae-gan equivalent cost function also compare swgan dcgan wgan wgangp compared methods follow default hyperparameters recommended authors. ae-based generative models including swae applied common convolutional architectures suggested decoder except difference shallow encoder contable comparison models. studied original wgan achieve good performance various architectures. wgan results unable reach scores comparable reported however inﬂuence ﬁnal conclusion. taining downscaling linear transform layer instead several convolutional blocks models including swgan follow ofﬁcial implementation employ standard resnet structure generator discriminator apply leakyrelu activation dual block based experimental tuning. learning rate swgan swae determined critic swgan lsun celeba cifar- determine cross validation. swgan achieves performances well quantitatively exhibits advantages models. lately reported competitive score– cifar- relying extra label information. best knowledge swgan reached lowest cifar- among existing generative models whereas celeba lsun mildly outperformed model employs sophisticated scheme time-scale update rule. furthermore visual results reported fig. consistent scores. particular swgan obtains visually pleasing images compared wgan wgan-gp terms better facial semantics sufﬁcient diversity. conclusion drawn cifar- lsun well. empirically proves dual model works better original model employed state-of-the-art wgans. believe mainly caused easier independent approximation multiple one-dimensional marginal distributions training data estimation original directly works samples higher dimensions. addition also study properties swae swgan. first show curve objective curve training verify effectiveness terms quantitative qualitative measurement. ﬁrst plot fig. fig. demonstrates training swae swgan stable models terms fid. meanwhile also witness proposed objective functions faithfully reﬂect image quality generated samples training iterations increase. second evaluate impact caused number designed blocks terms metric. turns merely primal dual block sufﬁcient achieve performance conﬁrms intuition instead randomly sampling long sequence radon transform matrices possible learn short sequence better matches distributions. additionally study impact k-lipschitz constraint swgan. fig. shows swgan favors relatively small optimal choice finally fig. shows interpolation results swae swgan justify capable generating reasonable geometry latent manifold. comparing state-of-the-art ae-based models tab. demonstrates proposed swae outperforms pure models sufﬁcient margin meanwhile score also competitive model additionally leverages adversarial training gans takes advantage better generalization ability. nevertheless adversarial training generally unstable studied model stable training using simple reconstruction loss without regularizations comparing recently published wae-mmd method observe swae shows clear advantage terms visual results scores. veriﬁes primal blocks encoding work better divergence constraints employed wae. evaluated ae-based models successful test lsun dataset include results paper. paper introduced novel model sliced optimal transport generative modeling. particular endowed modern ae-based models proposed blocks better approximation either primal dual form sliced wasserstein distance serves measurement model distribution data distribution. qualitative results demonstrated clear advantages existing models. future works include theoretical analysis approximation ratio proposed model primal dual forms extension model context progressive growing networks better generation. bonneel nicolas rabin julien peyr´e gabriel pﬁster hanspeter. sliced radon wasserstein barycenters measures. journal mathematical imaging vision kolouri soheil park thorpe matthew slepcev dejan rohde gustavo optimal mass transport signal processing machine-learning applications. ieee signal processing magazine goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. nips heusel martin ramsauer hubert unterthiner thomas nessler bernhard hochreiter sepp. gans trained time-scale update rule converge local nash equilibrium. advances neural information processing systems mescheder lars nowozin sebastian geiger andreas. adversarial variational bayes unifying variational autoencoders generative adversarial networks. icml radford alec metz luke chintala soumith. unsupervised representation learning deep convolutional generative adversarial networks. arxiv preprint arxiv. salimans goodfellow zaremba wojciech cheung vicki radford alec chen improved techniques training gans. advances neural information processing systems fisher seff zhang yinda song shuran funkhouser thomas xiao jianxiong. lsun construction large-scale image dataset using deep arxiv preprint learning humans loop. arxiv.", "year": 2017}