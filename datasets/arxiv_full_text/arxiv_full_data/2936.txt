{"title": "Self-Supervised Intrinsic Image Decomposition", "tag": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "abstract": "Intrinsic decomposition from a single image is a highly challenging task, due to its inherent ambiguity and the scarcity of training data. In contrast to traditional fully supervised learning approaches, in this paper we propose learning intrinsic image decomposition by explaining the input image. Our model, the Rendered Intrinsics Network (RIN), joins together an image decomposition pipeline, which predicts reflectance, shape, and lighting conditions given a single image, with a recombination function, a learned shading model used to recompose the original input based off of intrinsic image predictions. Our network can then use unsupervised reconstruction error as an additional signal to improve its intermediate representations. This allows large-scale unlabeled data to be useful during training, and also enables transferring learned knowledge to images of unseen object categories, lighting conditions, and shapes. Extensive experiments demonstrate that our method performs well on both intrinsic image decomposition and knowledge transfer.", "text": "intrinsic decomposition single image highly challenging task inherent ambiguity scarcity training data. contrast traditional fully supervised learning approaches paper propose learning intrinsic image decomposition explaining input image. model rendered intrinsics network joins together image decomposition pipeline predicts reﬂectance shape lighting conditions given single image recombination function learned shading model used recompose original input based intrinsic image predictions. network unsupervised reconstruction error additional signal improve intermediate representations. allows large-scale unlabeled data useful training also enables transferring learned knowledge images unseen object categories lighting conditions shapes. extensive experiments demonstrate method performs well intrinsic image decomposition knowledge transfer. remarkable progress computer vision particularly answering questions what where? given images. progress possible large labeled training sets representation learning techniques convolutional neural networks however general problem visual scene understanding require algorithms extract object identities locations also shape reﬂectance interactions intuitively disentangling contributions three components intrinsic light. images major triumph human vision perception. conferring type intuition algorithm though proven difﬁcult task constituting major open problem computer vision. problem challenging particular fundamentally underconstrained. consider porcelain vase figure individuals would difﬁculty identifying true colors shape vase along estimating lighting conditions resultant shading object shown however alternatives posits shape unnatural lighting entirely consistent compose form correct observed vase task ﬁnding appropriate intrinsic images object question simply ﬁnding valid answer countless factorizations would equivalent terms rendered combination rather ﬁnding probable answer. roughly speaking methods tackling problem model must either employ handcrafted priors reﬂectance shape lighting conditions found natural world order assign probabilities figure porcelain vase along three predictions underlying intrinsic images. assumes contribution shading negligible predicting completely rather rounded shape. reﬂectance therefore indistinguishable observed image. includes correct shape assumes lighting much brighter blue color regions affected shading. decomposition much inuitively pleasing either alternatives options valid combine exactly form observed vase. shows sphere visualized normals shape reference. intrinsic image proposals access library ground truth intrinsic images corresponding composite images. unfortunately limitations methods. although success ﬁrst route past strong priors often difﬁcult hand-tune generally useful fashion. hand requiring access complete high quality ground truth intrinsic images real world scenes also limiting creating training requires enormous amount human effort millions crowd-sourced annotations paper propose deep structured autoencoder rendered instrinsics network disentangles intrinsic image representations uses reconstruct input. decomposition model consists shared convolutional encoder observation three separate decoders reﬂectance shape lighting. shape lighting predictions used train differentiable shading function. output shader combined reﬂectance prediction reproduce observation. minimal structure imposed model namely intrinsic images provide natural disentangling real images provide enough information used input graphics engine makes autoencoder useful intermediate representations. structure also exploits natural sources supervision applied intermediate representations themselves reconstructed image. provides improve representations unlabeled data. avoiding need intrinsic image labels images dataset adapt types inputs even absence ground truth data. demonstrate utility approach three transfer experiments. ﬁrst trained simple geometric primitives supervised manner transferred common computer vision test objects. next trained dataset skewed underlying lighting distribution ﬁlls missing lighting conditions basis unlabeled observations. finally trained single shapenet category transferred separate highly dissimilar category. contributions three-fold. first propose novel formulation intrinsic image decomposition incorporating differentiable unsupervised reconstruction loss loop. second instantiate approach model uses convolutional neural networks intrinsic image prediction recombination learned shading function. also ﬁrst work apply deep learning full decomposition reﬂectance shape lights shading prior work focused reﬂectance-shading decomposition. finally show make unlabeled data improve intermediate intrinsic image representations transfer knowledge objects unseen training. figure contains convolutional encoder-decoders used predicting intrinsic images input another predicting shading stemming light source applied shape. networks together function larger structured autoencoder forcing speciﬁc type intermediate representation order reconstruct input image. intrinsic images introduced barrow tenenbaum useful mid-level scene descriptors model posits image expressed pointwise product contributions true colors object reﬂectance contributions shading object decomposing step further shading expressed function object’s shape ambient lighting conditions. exact nature shading function varies implementation. early work intrinsic image decomposition based insights land’s retinex theory horn separated images true colors shading using assumption large image gradients tend correspond reﬂectance changes small gradients lighting changes. assumption works well hypothetical mondrian world colors always hold natural images. particular weiss found model reﬂectance lighting rarely true outdoor scenes. recently barron malik developed iterative algorithm called sirfs maximizes likelihood intrinsic image proposals priors derived regularities natural images. sirfs proposes shape lighting estimates combines spherical harmonics renderer produce shading image. lombardi nishino oxholm nishino proposed bayesian formulation optimization procedure also formulating priors based distribution material properties physics lighting real world. researchers also explored reconstructing full shapes intrinsic images making richer generative models tang combined lambertian reﬂectance assumptions deep belief networks learn prior reﬂectance greyscale images applied deep lambertian network one-shot face recognition. narihira applied deep learning intrinsic images ﬁrst using human judgments real images later context animated movie frames rematas hold-geoffroy also used convolutional neural networks estimate reﬂectance maps illumination parameters respectively unconstrained outdoor settings. innamorati generalized intrinsic image decomposition considering contributions specularity occlusion direction-dependent model. found improved performance full decomposition incorporating skip layer connections network architecture used generate much crisper images. work seen figure contrast simple lambertian shading techniques learned shading model handle shadows cast objects. inputs shader shape lighting parameter pairs. extension models aims relax need complete ground truth data modeling image combination process nalbach incorporating domain-speciﬁc decoder reconstruct input images explored hinton transforming autoencoders also learned natural representations images vision community. work differs type representation question namely images rather descriptors like afﬁne transformations positions. kulkarni also interested learning disentangled representations autoencoder achieved selective gradient updates training. similarly chen showed mutual information objective could drive disentanglement deep network’s intermediate representation. solve intrinsic images match observed image. sirfs example predicts shading solves equation reﬂectance given prediction input ensures intrinsic image estimations combine form exactly observed image also deprives model reconstruction error. data-driven techniques rely solely ground truth labelings approaches assume access ground truth labels inputs explicitly model reconstruction input image based intrinsic image predictions. making reconstruction task previously unexplored error signal difﬁcult interpret. erroneous intrinsic images combine reconstruct input exactly cannot assume reconstruction error implies accurate intrinsic images. even simpler degenerate solution yields zero reconstruction error decomposes observation reﬂectance shape lighting conditions. opposed models estimate reﬂectance shading make direct equation generate reconstruction must employ function transforms shape lighting predictions shading estimate. linear lambertian assumptions could reduce function straightforward product would produce shading function incapable modeling lighting conditions drastically change across image ray-tracing purposes casting shadows. figure shading model’s outputs training synthetic models shapenet dataset shows effect panning light horizontally shows effect changing intensity light. input lights visualized rendering onto sphere. even though shader trained synthetic data generalizes well real shapes training. shape input estimated normals beethoven bust instead learn shading model. model limited pre-deﬁned shading function would evidenced shadows cast objects learning shader also beneﬁt allowing different representations lighting conditions. experiments lights deﬁned position three-dimensional space magnitude alternate representations radius orientation color spotlight could easily adopted. work employs shading engine sirfs instead learning shader similar disentanglement context sirfs engine represents lights spherical harmonics coefﬁcient vectors. model consists convolutional encoder-decoder networks ﬁrst predicts intrinsic images observed image second approximates shading process rendering engine. networks employ mirror-link connections introduced connect layers encoder decoder size. connections yield sharper results blurred outputs characteristic many deconvolutional models. ﬁrst network single encoder observation three separate decoders reﬂectance lighting shape. unlike link layers decoders possible update weights decoders without substantially affecting others useful transfer learning experiments. encoder convolutional layers ﬁlters size stride batch normalization relu activation applied every convolutional layer. layers reﬂectance shape decoders number features encoder reverse order plus ﬁnal layer output channels. spatial upsampling applied convolutional layers decoders. lighting decoder simple linear layer output dimension four shape passed input shading encoder directly. lighting estimate passed fully-connected layer output dimensionality matching shading encoder’s output concatenated encoded shading representation. shading decoder architecture ﬁrst network. ﬁnal component learnable parameters componentwise multiplication output shading network predicted reﬂectance. figure intrinsic image prediction model objects training category well example outside category quality airplane intrinsic images signiﬁcantly lower reﬂected reconstruction allows reconstruction drive improvement intermediate intrinsic image representations. predictions sirfs shown comparison. note reﬂectance sirfs deﬁned based difference observation shading prediction analogous reconstruction. table model sirfs test shapenet motorbikes category used train airplanes held-out class. lighting representation sirfs sufﬁciently different model attempt compare performance directly. instead visualization lights makes unlabeled data comparing reconstruction original input image. because shading model fully differentiable opposed shaders involve ray-tracing reconstruction error backpropagated intrinsic image predictions optimized standard coordinate ascent algorithm. shared encoder intrinsic images three separate decoders appropriate decoder updated others held ﬁxed. following experiments ﬁrst train dataset ground truth labels intrinsic images. treated standard supervised learning problem using mean squared error intrinsic image predictions loss. model trained additional unlabeled data using reconstruction loss error signal. refer self-supervised transfer. modes learning optimize using adam transfer half minibatch consist unlabeled transfer data half come labeled data. ensures representations shift learned initial supervised phase underconstrained nature problem drive model degenerate solutions. evaluating model test data outputs three decoders learned shader directly; enforce predictions must explain input exactly. below demonstrate model effectively transfer different shapes lighting conditions object categories without ground truth intrinsic images. however unsupervised transfer yield beneﬁts must sufﬁcient number examples unlabeled data. example intrinsic images dataset containing twenty real-world images large enough unsupervised learning affect representations model. absence unsupervised training model similar adapted predict full intrinsic images. data majority data generated shapenet objects rendered blender. labeled datasets rendered composite images accompanied object’s reﬂectance surface normals point parameters lamp used light scene. surface normals visualized mapping components normals appropriate ranges. following supervised learning experiments used dataset size images. intrinsic image decomposition model trained shapenet motorbikes. although accurately predicts intrinsic images train class performance drops tested classes. particular shape predictions suffer most dissimilar anything seen training set. crucially poor intrinsic image predictions reﬂected reconstruction input image. motivates reconstruction error drive improvement intrinsic images ground truth data. shading model contrast intrinsic image decomposition shading prediction generalized well outside training set. shader trained shapes lights rendered synthetic cars above. even though represents narrow distribution figure predictions trained left-lit images self-supervised learning right-lit images. uncovers updated lighting distribution without external supervision ground truth data. shapes found shader produced plausible predictions even real-world objects shader generalized without effort parameters never updated self-supervised training. freezing parameters shader prevents model producing nonsensical shading images. data generated dataset shape primitives viewed random orientations using blender rendering engine. images used supervised training. three common reference shapes used unlabeled transfer class. isolate effects shape mismatch labeled versus unlabeled data eight shapes rendered random monochromatic materials uniform distribution lighting positions within contained region space front object. datasets consisted shape rendered different colors colored shape viewed orientations. results updating weights shape decoder self-supervised transfer predictions held-out shapes improves shape affects rendered image shading improvement shapes comes alongside improvement shading predictions well. shape-speciﬁc results given table visualized data cars shapenet model repository rendered random orientations scales. labeled data left side. unlabeled data left right. results self-supervised training unlabeled data model’s distribution lighting predictions mirrored labeled training set. tested images right then tended predict centered lighting. updating lighting decoder based reconstruction error right-lit images though model’s lighting predictions accurately reﬂected distribution lighting mean-squared error reduces lighting predictions along reconstructions right-lit images shown previous transfer experiments intrinsic image mismatched labeled unlabeled data rin’s decoders needed updating transfer. transferring object categories though guarantee. although might expected model trained sufﬁciently many object categories would learn generally-useful distribution reﬂectances difﬁcult ensure case. interested sorts figure ﬁrst trained shapenet airplanes tested cars. airplanes white reﬂectance predictions washed even colorful cars. ﬁxed mismatch datasets without ground truth intrinsic images cars. table trained shapenet airplanes self-supervised transfer cars. although improves shading predictions necessarily driven improvement shape prediction. scenarios determine well self-supervised transfer works decoder needs updated account unlabeled data. data datasets shapenet cars airplanes created analogously section airplanes completely different color distribution cars mostly white whereas cars varied reﬂectance distribution. airplanes used labeled category ensure mismatch train transfer data. results transfer category allowed updates three decoders. pronounced improvements shading predictions accompanied modest improvements reﬂectances shading predictions always caused improved shape estimates. many-to-one mapping shape shading possible shape predictions worsen order improve shading estimates. lighting predictions also remained largely unchanged although opposite reason lighting region intentionally left training data lighting predictions adequate transfer classes even without self-supervised learning. paper proposed rendered intrinsics network intrinsic image prediction. showed learning image decomposition recombination functions make reconstruction loss improve intermediate representations. allowed unlabeled data used training demonstrated variety transfer tasks driven solely self-supervision. existed mismatch underlying intrinsic images labeled unlabeled data could also adapt predictions order better explain unlabeled examples.", "year": 2017}