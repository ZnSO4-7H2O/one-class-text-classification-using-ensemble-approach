{"title": "Improving Deep Neural Networks with Probabilistic Maxout Units", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN).", "text": "present probabilistic variant recently introduced maxout unit. success deep neural networks utilizing maxout partly attributed favorable performance dropout compared rectiﬁed linear units. however also depends fact maxout unit performs pooling operation group linear transformations thus partially invariant changes input. starting observation question desirable properties maxout units preserved improving invariance properties argue probabilistic maxout units successfully achieve balance. quantitatively verify claim report classiﬁcation performance matching exceeding current state three challenging image classiﬁcation benchmarks regularization large neural networks stochastic model averaging recently shown effective tool overﬁtting supervised classiﬁcation tasks. dropout ﬁrst stochastic methods improved performance several benchmarks ranging small large scale classiﬁcation problems idea behind dropout randomly drop activation unit within network probability seen extreme form bagging parameters shared among models number trained models exponential number model parameters. testing approximation used average large number models without instantiating them. combined efﬁcient parallel implementations procedure opened possibility train large neural networks millions parameters back-propagation inspired success number stochastic regularization techniques recently developed. includes work dropconnect generalization dropout connections units rather activation dropped random. adaptive dropout recently introduced variant dropout stochastic regularization performed binary belief network learned alongside neural network decrease information content hidden units. stochastic pooling technique applicable convolutional networks pooling operation replaced sampling procedure. instead changing regularizer authors searched activation function dropout performs well. result introduced maxout unit seen generalization rectiﬁed linear units especially suited model averaging performed dropout. success maxout partly attributed fact maxout aids optimization procedure partially preventing units becoming inactive; artifact caused thresholding performed rectiﬁed linear unit. additionally similar relus argue equally important property maxout unit however activation function seen performing pooling operation subspace linear feature mappings result subspace pooling operation maxout unit partially invariant changes within input. natural question arising observation thus whether could beneﬁcial replace maximum operation used maxout units pooling operations pooling. utility different subspace pooling operations already explored context unsupervised learning e.g. l-pooling known give rise interesting invariances work generalizing maxout replacing max-operation general lp-pooling exists deviation standard maximum operation comes price discarding desirable properties maxout unit. example abandoning piecewise linearity restricting units positive values introduction saturation regimes potentially worsen accuracy approximate model averaging performed dropout. based observations propose stochastic generalization maxout unit preserves desirable properties improving subspace pooling operation unit. additional beneﬁt training neural network using proposed probabilistic maxout units gradient training error evenly distributed among linear feature mappings unit. contrast maxout network helps gradient maxout units linear feature mappings. compared maxout probabilistic units thus learn better utilize full k-dimensional subspace. evaluate classiﬁcation performance model consisting units show matches state performance three challenging classiﬁcation benchmarks. deﬁning probabilistic maxout unit brieﬂy review notation used following deﬁning deep neural network models. adopt standard feed-forward neural network formulation given input desired output network realizes function computing c-dimensional vector number classes predicting desired output. prediction computed ﬁrst sequentially mapping input hierarchy hidwithin hidden layer hierarchy realizes layers unit function mapping inputs activation using weight bias parameters finally prediction computed based last layer output prediction realized using softmax layer tmax weights bias parameters learned minimizing cross entropy loss beformalized like becomes clear maxout unit interpreted performing pooling operation k-dimensional subspace linear units representing transformation input similar spatial max-pooling commonly employed convolutional neural networks. however unlike figure schematic different pooling operations. exemplary input image taken imagenet dataset together depiction spatial pooling region well input maxout probout unit spatial max-pooling proceeds computing maximum ﬁlter response four different positions maxout computes pooled response linear ﬁlter mappings applied input patch. activation probout unit computed sampling linear responses according probability. spatial pooling maxout unit pools subspace different linear transformations applied input contrast this spatial max-pooling linear feature maps would compute pooling linear transformation applied different inputs. schematic difference several pooling operations given fig. maxout thus similar subspace pooling operations used example topographic known result partial invariance changes within input. basis observation propose stochastic generalization maxout unit preserves desirable properties improving gradient propagation among linear feature mappings well invariance properties unit. following call generalized units probout units since direct probabilistic generalization maxout. derive probout unit activation function maxout formulation replacing maximum operation probabilistic sampling procedure. speciﬁcally assume boltzmann distribution linear feature mappings sample activation activation corresponding subspace units. ﬁrst deﬁne probability linear units subspace comparing both bounded activation always given linear feature mappings within subspace. probout unit hence preserves properties maxout unit replacing sub-unit selection mechanism. reduces maxout activation values probout unit behave similarly maxout activation linear unit subspace dominates. however activation multiple linear units differs slightly selected almost equal probability. futhermore active linear unit chance selected. sampling approach therefore ensures gradient ﬂows linear subspace units given probout unit examples hence argue probout units learn better utilize full k-dimensional subspace. practice want combine probout units described dropout regularizing learned model. achieve directly include dropout probabilistic sampling step idea using stochastic pooling operation explored context spatial pooling within machine learning literature before. among work approach similar authors introduced probabilistic pooling approach order derive convolutional deep believe network also boltzmann distribution based unit activations calculate sampling probability. main difference work calculate probability sampling unit different spatial locations whereas calculate probability sampling unit among units forming subspace spatial location. another difference forward propagate sampled activation whereas calculated probability activate binary stochastic unit. another approach closely related work stochastic pooling presented stochastic pooling operation samples activation pooling unit proportionally activation rectiﬁed linear unit computed different spatial positions. similar sense activation sampled different activations. similar however differs sampling performed spatial locations rather activations different units. noted work also bears resemblance recent work training stochastic units embedded autoencoder network back-propagation contrast work aims using stochastic neurons train generative model embrace stochasticity subspace pooling operation effective means regularize discriminative model. test time need account stochastic nature neural network containing probout units. forward pass network value probout unit sampled values according probability. output forward pass thus always represents different instantiations trained probout network; number probout units network. combined dropout number possible instantiations increases evaluating possible models test time therefore clearly infeasible. dropout formulation deals large amount possible models removing dropout test time halving weights unit. network consists softmax layer modiﬁed network performs exact model averaging general models computation merely approximation true model average which however performs well practice deep relu networks maxout model adopt procedure halving weights removing inﬂuence dropout testi= effectively replacing sampling observe models remaining removing dropout models instantiated high probability. therefore resort sampling small number outputs networks softmax layer average values. evaluation exact effect model averaging found section figure visualization pairs ﬁrst layer linear ﬁlters learned maxout model well probout model contrast maxout ﬁlters ﬁlter pairs learned probout model appear mostly transformed versions other. datasets. experiments performed using implementation based theano pylearn library using fast convoltion code mini-batch stochastic gradient descent batch size datasets start network used retaining hyperparameter choices ensure comparability results. replace maxout units network probout units choose crossvalidation layer preliminary experiment cifar-. begin experiments cifar- dataset. consists training images test images grouped categories. images size pixels contains color channels. maxout known yield good performance dataset making ideal starting point evaluating difference maxout probout units. conducted preliminary experiment evaluate effect probout parameters performance compare standard maxout model. purpose layer model consisting three convolutional layers probout units respectively pool linear units each. penultimate layer consists probout units pooling subspace linear units. ﬁnal layer standard softmax layer mapping units penultimate layer classes cifar-. receptive ﬁelds units convolutional layers respectively. additionally spatial max-pooling performed convolutional layer pooling size using stride layers. split cifar- training data retaining ﬁrst samples training using last samples validation set. start evaluation using probout units everywhere network cross-validate choice inverse-temperature parameters keeping hyperparameters ﬁxed. annealing parameter training lower value improved performance hence linearly decrease value lower initial cases. shown fig. best classiﬁcation performance achieved allow higher variance sampling ﬁrst layers speciﬁcally third well fully connected layer observe performance increase chosen meaning sampling procedure selects maximum value high probability. indicates probabilistic sampling effective lower layers. veriﬁed replacing probout units last layers maxout units signiﬁcantly decrease classiﬁcation accuracy. hypothesize increasing probability sampling maximal linear unit subspace pulls units subspace closer together forces network become more invariant changes within subspace. property desired lower layers might turn detrimental higher layers model averaging effect maxout important achieving invariance. sampling units non-maximal activation could result unwanted correlation submodels. qualitatively verify claim plot ﬁrst layer linear ﬁlters learned using probout units alongside ﬁlters learned model consisting maxout figure validation parameter layers cifar-. plot error validation training evaluating choice second parameter ﬁxed likewise experiments regarding evolution classiﬁcation error standard deviation cifar dataset changing number model evaluations. average activation softmax layer evaluations compute predicted class label maximum maxi∈{...c} standard deviation computed runs model evaluations. units fig. inspecting ﬁlters many ﬁlters belonging subspace formed probout unit seem transformed versions other resembling quadrature pairs ﬁlters. among linear ﬁlters learned maxout model also appear encode invariance local transformations. ﬁlters contained subspace however seemingly unrelated. support observation empirically probed changes feature vectors different layers applied translated rotated images validation set. similar calculate normalized euclidean distance feature vectors extracted unchanged image transformed version. plot distances several exemplary images well mean randomly sampled images. result experiment given fig. showing introducing probout units network moderate positive effect invariance translation rotations. finally evaluate computational cost model averaging procedure described section test time. depicted fig. classiﬁcation error probout model decreases model evaluations saturating moderate amount evaluations reached. conversely using sampling test time conjunction standard maxout model signiﬁcantly decreases performance. indicates maxout model highly optimized maximum responses cannot deal noise introduced sampling procedure. additionally also tried replace model averaging mechanism cheaper approximations. replacing sampling probout units maximum operation test time resulted decrease performance reaching also tried probability weighting testing however performed even worse achieving next step evaluate performance model full cifar- benchmark. follow protocol train probout model. ﬁrst preprocess images applying contrast normalization followed whitening. train model using ﬁrst examples training using last examples validation set. training proceeds validation error stops decreasing. retrain model complete training amount epochs took reach best validation error. half table shows result training model well recent results. achieve error slightly better statistically tied previous state given maxout model. also evaluated performance model training data augmented additional transformed training examples. purpose train model using original training images well randomly translated horizontally ﬂipped versions images. bottom half table shows comparison different results training cifar- additional data augmentation. using augmentation process achieve classiﬁcation error matching outperforming maxout result. images contained cifar- dataset cifar- images taken subset -million images database. dataset contains training test examples size pixels each. dataset hence similar cifar- size image content. however differs cifar- label distribution. concretely cifar- contains images classes grouped super-classes. training data therefore contains training images class times less examples class cifar- accompanied examples test-set. make super-classes train model using similar setup experiments carried cifar-. speciﬁcally preprocessing training procedure network section used experiment again architecture used thus ensuring comparability results. testing model evaluations average sampled probout units. result experiment given table agreement cifar- results model performs marginally better maxout model also shown table current best method cifar- achieves classiﬁcation error using larger convolutional neural network together tree-based prior classes formed utilizing super-classes. similar performance increase could potentially achieved combining tree-based prior model. street view house numbers dataset collection images depicting digits obtained google street view images. dataset comes variants restrict containing cropped pixel images. similar well known mnist dataset task dataset classify image digits range task considerably difﬁcult mnist since images cropped natural image data. images thus contain color information show signiﬁcant contrast variwhile writing manuscript came attention experiments cifar- carried using different preprocessing mentioned original paper. ensure substantially effect comparison experiment using preprocessing used experiments. resulted slightly improved classiﬁcation error training test contain labeled examples respectively. addition data extra labeled digits somewhat less difﬁcult differentiate used additional training data. build validation selecting examples class training examples class extra dataset. conﬂate remaining training images large images training. model trained task consists three convolutional layers containing units respectively pooling dimensional subspace. followed fully connected softmax layer fully connected layer contains units pooling dimensional subspace. yields classiﬁcation error matching current state model trained svhn without data augmentation achieved maxout model comparison results found table includes current best result data augmentation obtained using generalization dropout conjunction large network containing rectiﬁed linear units table classiﬁcation error different models svhn dataset. half shows comparison result current state achieved without data augmentation. bottom half gives best performance achieved data augmentation additional reference. presented probabilistic version recently introduced maxout unit. model built using units shown yield competitive performance three challenging datasets stands replacing maxout units probout units computationally expensive test time. problem could diminished developing approximate inference scheme similar interesting possibility future work. approach part larger body work exploring utility learning complex cell like units give rise interesting invariances neural networks. paradigm extensively studied unsupervised learning less explored supervised scenario. believe work towards building activation functions incorporating invariance properties time designed efﬁcient model averaging techniques dropout worthwhile endeavor advancing ﬁeld.", "year": 2013}