{"title": "Dynamic Optimization of Neural Network Structures Using Probabilistic  Modeling", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Deep neural networks (DNNs) are powerful machine learning models and have succeeded in various artificial intelligence tasks. Although various architectures and modules for the DNNs have been proposed, selecting and designing the appropriate network structure for a target problem is a challenging task. In this paper, we propose a method to simultaneously optimize the network structure and weight parameters during neural network training. We consider a probability distribution that generates network structures, and optimize the parameters of the distribution instead of directly optimizing the network structure. The proposed method can apply to the various network structure optimization problems under the same framework. We apply the proposed method to several structure optimization problems such as selection of layers, selection of unit types, and selection of connections using the MNIST, CIFAR-10, and CIFAR-100 datasets. The experimental results show that the proposed method can find the appropriate and competitive network structures.", "text": "popular approach network structure optimization treat network structure hyper-parameters dnns optimize black-box optimization technique bayesian optimization evolutionary algorithms given network conﬁguration training done certain period trained network evaluated based accuracy loss validation dataset. black-box optimizer treats hyper-parameter vector resulting accuracy/loss design variables objective/cost function. recently methods automatic network design construct ﬂexible network structures conventional hyper-parameter optimization approaches proposed. zoph deﬁned recurrent neural networks generate neural network architectures target problem found state-of-the-art architectures optimizing using policy gradient method works optimize connections types layers evolutionary algorithms construct high-performance architecture. methods succeeded ﬁnding state-of-the-art conﬁgurations dnns. view approaches static optimization network structures. main disadvantage static optimization efﬁciency since repeats training different network conﬁgurations ﬁnds reasonable conﬁguration. dynamic optimization network structures side learns connection weights structure network simultaneously. typical example represent network structure parameters learnable parameters optimize stochastic gradient descent carrying weight training srinivas babu introduce tri-state relu activation differentiable parameters prune units layers using back-propagation. frey binary belief network overlaying neural network decide dropout rate jointly train networks. sparse compact network structures dynamically learned using regularization techniques well. methods require loss function differentiable respect structure parameters deep neural networks powerful machine learning models succeeded various artiﬁcial intelligence tasks. although various architectures modules dnns proposed selecting designing appropriate network structure target problem challenging task. paper propose method simultaneously optimize network structure weight parameters neural network training. consider probability distribution generates network structures optimize parameters distribution instead directly optimizing network structure. proposed method apply various network structure optimization problems under framework. apply proposed method several structure optimization problems selection layers selection unit types selection connections using mnist cifar- cifar- datasets. experimental results show proposed method appropriate competitive network structures. deep neural networks become popular machine-learning model seen great success various tasks image recognition natural language processing. date variety models proposed. considering convolutional neural networks visual object recognition example variety deep complex models developed model residual networks skip connections dense convolutional networks easy users select appropriate network structure including hyperparameters depth network type unit connection layers since performance depends tasks data. however appropriate conﬁguration structures importance high performance dnns. therefore developing efﬁcient methods optimize structure dnns important topic. camera-ready version following paper shirakawa iwata akimoto dynamic optimization neural network structures using probabilistic modeling thirtysecond aaai conference artiﬁcial intelligence so-called natural gradient log-likelihood fisher information matrix note natural gradient generally requires estimation inverse fisher information matrix standard machine learning setting however analytically compute natural gradient loglikelihood w.r.t. since full access distribution example exponential family sufﬁcient statistics expectation parameterization admits natural gradient loglikelihood setting similar natural policy gradient method parameter-based exploration reinforcement learning information geometric optimization algorithm simulation based black-box optimization. natural gradient gives reasonable scaling gradient case compared vanilla gradient. step training phase receive mini-batch training data i.e. zn}. loss imated sample average loss shall write cost gradient cost replaced respectively. situation need estimate cost gradient structure parameter consider following different ways mini-batches training data heuristic optimization techniques. dynamic optimization approach computationally efﬁcient since optimizes connection weights network structure within single training loop though compromises ﬂexibility learnable structures compared static optimization approaches. paper propose general framework dynamically optimizing network structures connection weights simultaneously. achieve ﬂexibility learnable structures introduce parametric distribution generating network structure treat distribution parameters hyper-parameters. objective function weights hyper-parameters deﬁned expectation loss function distribution. then gradient based search algorithms applied. demonstrate ﬂexibility efﬁciency framework consider bernoulli distributions paper show proposed method dynamically optimize various network structure parameters framework. method computationally efﬁcient static optimization approach ﬂexible conventional dynamic optimization approach directly optimizing structure parameters conduct four experiments selection layers selection activation functions adaptation stochastic network selection connections. experimental results show proposed method appropriate unusual network structures. generic framework following consider neural network modeled parameter vectors vector consisting connection weights vector consisting hyperparameters determine structure network connectivity unit type activation function unit weights general real valued structure parameters live arbitrary space. original objective minimize loss often deﬁned dataset loss function given data respectively. consider family probability distributions parametrized real vector instead directly optimizing consider minimize expected loss namely lpθdm reference measure note minimizer admits minimizer sense concentrated minimizer long distribution included given family probability distributions. remark domain continuous objective likely differentiable whereas necessarily continuous since discrete consider following. hand latter possibly advantage computational time since mini-batch size network times smaller. advantage disappear capable processing data original mini-batch parallel. hand since latter uses different batches compute loss different networks resulting loss function lead racing situation. optimization viewpoint former preferred. variations compared experiment instantiation bernoulli distribution following focus cases structure variables binary i.e. element consider bernoulli distribution probability mass function probability parameter vector lives since exponential family expectation parameterization natural gradient log-likelihood given parameters updated taking approximated gradient steps learning rates stochastic gradient optimizer used update well standard neural network training. however since bounded needs constrain remains simple practically attractive treatment rescale loss function training step. done transforming loss value ranking based utility value guarantee stay neither adaptation constraint handling. moreover restrict range within leave open possibility generating values i.e. replace value boundary value updated beyond boundary. optimization procedure proposed method displayed algorithm apply trained network test data options deterministic stochastic predictions. deterministic prediction indicates random variables stochastic averages values model predictions using samples stochastic prediction requires high computational cost proportion number samples increases. report results predictions experiments samples stochastic prediction. algorithm optimization procedure method instantiated bernoulli distribution. input training data output optimized parameters procedure initialize weights bernoulli parameters stopping criterion satisﬁed relation stochastic network models since method uses stochastic network structures describe relation stochastic networks dropout stochastically zeros output hidden units training prevent overﬁtting. also stochastic networks dropconnect drops connections stochastic depth skips layers resnets developed. swapout generalization dropout stochastic depth randomly chooses unit behavior four types dropped feedforward skipped residual network unit. dropout techniques contribute reducing generalization error. stochastic behavior decided based bernoulli distributions typically binary vector drawn bernoulli distributions used decide whether unit drops method method regarded adaptation dropout ratio. therefore method also applied adaptation parameters existing stochastic network models. relation difference optimizing parameters probability distribution proposed method based view generalization speciﬁc estimation distribution algorithms population based incremental learning compact genetic algorithm discrete optimization. moreover generalizes covariance matrix adaptation evolution strategy nowadays recognized stat-of-the-art black-box continuous optimizer. update rule similar cga. standard edas optimizer updates parameters distribution. contrary paper weight parameters neural network simultaneously updated different mechanism differently applying update parameters time update distribution parameters weights updated using gradient loss function since gradient available leads faster learning compared direct search igo. viewpoint updating distribution parameters i.e. optimizing network structures landscape loss function dynamically changes algorithmic iteration weight parameters well minibatch change. reason call methods optimize structure weight parameters time dynamic structure optimization. apply methodology following four situations selection layers selection activation functions adaptation stochastic network selection connections densely connected cnns. algorithms implemented chainer framework nvidia geforce experiments nvidia titan experiment experiments nesterov momentum weight decay used optimize weight parameters. learning rate divided maximum number epochs. setting based literature selection layers experimental setting base network consists fully connected hidden layers units layer rectiﬁed linear unit mnist handwritten digits dataset containing training examples test examples gray-scale images. input output layers correspond input pixel values class labels respectively. cross entropy error softmax activation loss function binary vector decide whether processing corresponding layer skipped skip processing l-th layer re-connect layer layer precisely denoting l-th layer’s processing layer’s input vector becomes possible number units layer same. gradient computed straight-forward components gradient corresponding skipped layers zero. skip processing skip-forward deﬁned number -bits implies number hidden layers. ensure skip processing skip ﬁrst hidden layer decide whether second hidden layers skipped based binary vector. setting dimension purpose experiment investigate difference type approximation loss check whether proposed method appropriate number layers. neural network structure mentioned ﬁxed layer size following optimization setting training work properly number layers greater therefore proposed method needs less layers training. denote proposed methods using adaptivelayer using adaptivelayer vary parameter adaptivelayer adaptivelayer report results using deterministic stochastic predictions mentioned above. data sample size number epochs adaptivelayer respectively algorithms. number iterations algorithms. beginning training initialize learning rate bernoulli parameters θinit θinit verify impact initialization. also method using ﬁxed bernoulli parameters denoted stochasticlayer check effectiveness optimizing experiments conducted trials settings. result discussion table shows test error method ﬁnal iteration. observe adaptivelayer initialization θinit artiﬁcially poor initialization. setting check impact initialization. recommend tuning θinit θinit assumes prior knowledge. ters effective. preliminary study found best number layers whose test error experimental setting. layer size converges however ﬁnal test error inferior. observation conclude goodness proposed method optimal network conﬁguration reasonable conﬁguration within single training loop. improve convenience deep learning practice. based observation method using showed best performance even case initial condition adopt setting following experiments. selection activation functions experimental setting binary vector select activation function unit. different activation functions mixed layer. activation function i-th unit relu frelu hyperbolic tangent ftanh words activation function deﬁned mifrelu ftanh denotes input activation i-th unit. base network structure used experiment consists three fully connected hidden layers units layer. number activation functions decided mnist dataset. experiment report result method using denote adaptiveactivation. also method using ﬁxed bernoulli parameters ones using relu hyperbolic tangent activations units; denote stochasticactivation relu tanh respectively. data sample size number epochs adaptiveactivation respectively algorithms. note number epochs greater previous experiment. number bits optimized signiﬁcantly greater previous setting initialize learning rate bernoulli parameters θinit experiments conducted trials using settings. result discussion table shows test error training time algorithm. observe adaptiveactivation outperforms stochasticactivation bernoulli parameters stay constant suggesting optimization parameters method effective. predictive performance adaptiveactivation competitive stochasticactivation computationally efﬁcient stochastic prediction. addition obtained networks adaptiveactivation better classiﬁcation performance compared uniform activations relu hyperbolic tangent. comparing training time observe proposed method needs twice computational time training compared ﬁxed structured neural networks. method additionally requires computation regarding bernoulli distributions switch structure. implementation shows best performance among proposed methods performances adaptivelayer become signiﬁcantly worse initialization used. reason loss approximation used adaptivelayer evaluates sample mini-batch leads accurate comparison samples comparing deterministic stochastic predictions performance differences signiﬁcant values distributed close shown figure values converge learnable number layers early iteration even case initial condition. experiments observed signiﬁcant difference computational time adaptivelayer table mean test errors trials ﬁnal iteration experiment selection activation functions. values parentheses denote standard deviation. training time typical single reported. l-th hidden layer skipped underlying probability distribution munit then parameter rl−. vector bayesian optimization implementation adopt default parameter setting. bernoulli parameters stochastic network mentioned optimized hyper-parameter. problem dimension range search space training data split training validation ratio nine one; validation used evaluate hyper-paremter training neural network candidate hyper-parameter. parameters bernoulli distribution network training. searching hyper-parameter retrain model using training data report error test data. fair comparison include vector figure illustrates transitions test errors adaptiveactivation relu tanh. observe convergence adaptiveactivation slow achieves better results last iterations. iterations needed method tune structure weight parameters simultaneously. figure shows example histograms layer training. setting larger value means tends become relu. interestingly histogram ﬁrst layer biased toward relu. observed number units increases training. adaptation stochastic network experimental setting proposed framework applied optimize types hyperparameters. demonstrate this adapt dropout ratio well layer-skip ratio time. mnist dataset experiment. network model deﬁned follows. consider fully connected network consisting hidden layers units layer base network. conﬁguration network identiﬁed binary parameters ﬁrst bits denoted mlayer determine whether figure histograms layer obtained adaptiveactivation training. larger value means tends become relu. histograms created certain obtained histograms runs similar this. table test errors computational time proposed method bayesian optimization different budgets experiment adaptation stochastic network. mean values trials reported proposed method value parentheses denotes standard deviation. bayesian optimization result single reported. result discussion table shows test errors stochastic networks obtained proposed method bayesian optimization different budgets budget indicates number hyper-parameters evaluated. stochastic prediction samples calculate test errors. obviously observe computational time bayesian optimization proportionally increases number budgets method computationally efﬁcient. proposed method competitive stochastic network reasonable computational time. observed networks obtained proposed method skip seven layers units dropped high probability. also observed tendency network obtained bayesian optimization. although bayesian optimization could better conﬁguration case within several budgets probably needs many budgets dimension hyper-parameters increases setting experiment state-of-the-art architectures image classiﬁcation base network structure. densenets contain several dense blocks transition layers. dense block comprises lblock layers implements nonlinear transformation batch normalization followed relu activation convolution. size output feature-maps layer input feature-maps. number output feature-maps layer called growth rate; l-th layer dense block receives feature-maps indicates number input feature-maps dense block. thus number output feature-maps dense block klblock transition layer located dense blocks consists batch normalization relu activation convolutional layer followed average pooling layer. detailed architecture densenets found decide existence connections layers dense block according binary vector namely remove connection corresponding equals zero. denote k-th layer’s output feature-maps then input feature-maps layer computed simple densenet consisting depth reported base network structure containing three dense blocks transition layers. setting dimension becomes experiment cifar- cifar- datasets numbers classes respectively. numbers training test images respectively size images normalize data using per-channel means standard deviations preprocessing. data augmentation method based padding pixels side followed choosing random crop padded image random horizontal ﬂips cropped image. report results method using also normal densenet comparison. data sample size number epochs adaptiveconnection respectively epochs normal densenet. initialize weight parameters using method described learning rate initial bernoulli parameters respectively. conduct experiments settings trials adaptiveconnection normal densenet respectively. result discussion table shows test errors adaptiveconnection normal densenet ﬁnal iteration. case stochastic prediction slightly better deterministic difference signiﬁcant. difference predictive performances adaptiveconnection normal densenet signiﬁcant cifar- datasets whereas adaptiveconnection inferior cifar- dataset. however observed obtained bernoulli parameters distributed close figure observed connections removed high probability datasets. counting weight parameters removed connections found reduce weight parameters without suffering performance deterioration cifar-. paper proposed methodology dynamically indirectly optimizes network structure parameters using probabilistic models. instantiated proposed method using bernoulli distributions simultaneously optimized parameters network weights. conducted experiments optimized four different network components layer skips activation functions layer skips unit dropouts connections. observed proposed method could learnable layer size appropriate rate activation functions. also showed method dynamically optimize type hyper-parameters obtain competitive results reasonable training time. experiment connection selection densenets proposed method shown competitive results smaller number connections. proposed method computationally efﬁcient static structure optimization general validated experiment static optimization method bayesian optimization better hyper-parameter conﬁguration takes time. also observed table parameters optimize within standard stochastic gradient descent framework whereas proposed method optimize network structure necessarily differentiable parametric probability distributions. although paper focuses bernoulli distributions proposed framework used distributions categorical distributions represent several categorical variables indeed rather easy derive update distribution parameters distributions exponential families. since difﬁcult design model represent categorical variables differentiable parameters proposed framework ﬂexible existing dynamic optimization methods sense handily treat wider range structural optimization problems. direction future work extend proposed method treat variables binary variables i.e. categorical variables optimize larger complex networks. another direction future work introduce prior distribution incorporate regularization term obtain sparse compact representation prior distribution", "year": 2018}