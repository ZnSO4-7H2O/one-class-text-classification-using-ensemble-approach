{"title": "Video Highlight Prediction Using Audience Chat Reactions", "tag": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "abstract": "Sports channel video portals offer an exciting domain for research on multimodal, multilingual analysis. We present methods addressing the problem of automatic video highlight prediction based on joint visual features and textual analysis of the real-world audience discourse with complex slang, in both English and traditional Chinese. We present a novel dataset based on League of Legends championships recorded from North American and Taiwanese Twitch.tv channels (will be released for further research), and demonstrate strong results on these using multimodal, character-level CNN-RNN model architectures.", "text": "paper builds wealth interaction around esports develop predictive models match video highlights based audience’s online chat discourse well visual recordings matches themselves. esports journalists fans create highlight videos important moments matches. using ground truth explore automatic prediction highlights multimodal cnn+rnn models multiple languages. appealingly task natural community already produces ground truth global allowing multilingual multimodal grounding. highlight prediction capturing exciting moments speciﬁc video depends context state play players. task predicting exciting moments hence different summarizing entire match story summary. hence highlight prediction beneﬁt available real-time text commentary fans valuable exposing abstract background context accessible sports channel video portals offer exciting domain research multimodal multilingual analysis. present methods addressing problem automatic video highlight prediction based joint visual features textual analysis real-world audience discourse complex slang english traditional chinese. present novel dataset based league legends championships recorded north american taiwanese twitch.tv channels demonstrate strong results using multimodal character-level cnn-rnn model architectures. on-line esports events provide setting observing large-scale social interaction focused visual story evolves time—a video game. watching sporting competitions major source entertainment millennia signiﬁcant part today’s culture esports brings level several fronts. global reach games played around world across cultures speakers several languages. another scale on-line text-based discourse matches public amendable analysis. popular games league legends drew million views world series ﬁnal matches peak concurrent viewership million. finally players interact screen fans exactly views. computer vision techniques easily identify aspects state play. example computer vision understand michael jordan’s dunk highlight another player concurrent commentary might reveal this. collect dataset twitch.tv live-streaming platforms integrates comments largest live-streaming platform video games. record matches game league legends largest esports game subsets spring season north american league legends championship series league legends master series hosted taiwan/macau/hongkong chat comments english traditional chinese respectively. community created highlights label frame match highlight not. addition dataset present several experiments multilingual characterbased models deep-learning based vision models either per-frame tied together videosequence lstm-rnn combinations language vision models. results indicate surprisingly visual models generally outperform language-based models still build reasonably useful language models help disambiguate difﬁcult cases vision models combining sources effective model brieﬂy discuss small sample related work language vision datasets summarization highlight prediction. surge vision language datasets focusing captions last years followed efforts focus speciﬁc parts images referring expressions broader context video similar efforts collected descriptions others existing descriptive video service sources beyond descriptions datasets questions relate images language approach extended movies tapaswi related problem visually summarizing videos produced datasets holiday sports events multiple users making summary videos multiple users selecting summary key-frames short videos. language-based summarization extractive models generate summaries selecting important sentences assembling these abstractive models generate/rewrite summaries scratch. closer setting work highlight prediction football basketball based audio broadcasts commentators outsized impact visual features spirit study work looking tweets during sporting events tweets immediate well aligned games esports comments. closely related work song collects videos heroes storm league legends dota online broadcasting websites around hours total. also provide highlight labeling annotated four annotators. method hand similar scale data existing highlights also employ textual audience chat commentary thus providing resource task language vision research. summary present ﬁrst languagevision dataset video highlighting contains audience reactions chat format multiple languages. community produced ground truth provides labels frame used supervised learning. language side dataset presents interesting challenges related real-world internet-style slang. dataset covers videos nalcs total videos week week spring series tournament. week matches nalcs matches lms. matches best consist games three games. ﬁrst third games used training. second games ﬁrst weeks used validafigure highlight labeling feature representation frame calculated averaging color channel subregion. after template matching shows maximum similarity matching frame highlight bottom labeling result video. game’s video ranges minutes length contains image chat data linked speciﬁc timestamp game. average number chats video standard deviation high value standard deviation mostly fact nalcs simultaneously broadcasts matches different channels often leads majority users watching channel relatively popular team causing imbalance number chats. consider broadcasts single channel average number chats standard deviation number viewers game averages number unique users type chat average i.e. roughly viewers. highlight labeling game collected community generated highlights ranging minutes minutes length. purpose consistency within data collected highlights single youtube channel onivia provided highlights championship tournaments consistent arrangement. expect consistency model better pick characteristics determining highlights. next need align position frames highlight video frames full game video. this adopted template matching approach. frame video highlight divide regions average value color channel region feature. feature representation frame ends -dim vector shown figure frame highlight similar frame video calculating distance vectors. however matching single frame another suffers noise. therefore alternatively concatenate following frames form window template matching best matching location video. found window size frames gives consistent high quality results. frame result contains best matching score also location match video. figure illustrates matching process. section explain proposed models components. ﬁrst describe notation deﬁnition problem plus evaluation metric used. next explain vision model vcnn-lstm language model l-char-lstm. finally describe joint multimodal model lv-lstm. problem deﬁnition basic task determine frame full input video labeled part output highlight not. simplify notation denote sequence features frames. chats expressed chat comes timestamp methods take image features and/or chats predict labels frames yt}. evaluation metric refer frames positive ground truth label when window contains moment clip transition highlights best matching score appears low. used separate clips highlight. starting locations clip label video. v-cnn resnet- model represent frames motivated strong results imagenet challenge naive v-cnn model uses features pre-trained version network directly make prediction frame v-cnn-lstm order exploit visual video information sequentially time memory-based lstm-rnn image features model long-term dependencies. videos fps. difference between consecutive frames usually minor prediction every frame evaluation interpolate predictions frames. during training memory constraints unfold lstm cell times. therefore image window size around -seconds hidden state last cell used v-cnnlstm feature. process shown figure l-word-lstm l-char-lstm next discuss language-based models using audience chat text. word-level lstm-rnn models common approach embedding sentences. unfortunately internet-slang style language irregularities mispelled words emojis abbreviations marks onomatopoeic cases therefore alternatively model audience chat character-level lstm-rnn model characters language chinese english emojis expanded multiple ascii characters according two-character unicode representations used chat servers. encode -hot vector ascii input character. frame chats occur next seconds called text window size form input l-charlstm. concatenate chats window separating special stop character -layer l-char-lstm model. model shown figure following setting sec. evaluate text window size seconds seconds following accuracies.% achieved best results text window size seconds used rest experiments. joint lv-lstm model combines best vision language models v-cnn-lstm l-char-lstm. vision language models extract features v-cnn-lstn lchar-lstm respectively. concatenate feed -layer mlp. completed model shown figure expect room improve approach using involved representations e.g. bilinear pooling memory networks attention models future work. number stop characters encoding number chats window. therefore l-char-lstm could learn chats information useful feature. also content deleted twitch.tv channel usage improper words. symbol replace cases. training details development ablation studies train splits data nalcs evaluate models section ﬁnal results models retrained combination train data performance measured test set. separate highlight prediction three different tasks based using different input data videos chats videos+chats. details dataset split section code implemented pytorch. deal large number frames total sample positive negative examples epoch. batch size epochs experiments. weight decay learning rate ﬁrst epochs that. cross entropy loss used. highlights generated fans consist clips. match clip happened full match call highlight clip action interest often happens later part highlight clip clip contains additional context action help stage. experimental settings used heuristic including last frames every highlight clip positive training examples. evaluation used frames highlight clip. ablation study table shows performance module separately set. basic l-char-lstm v-cnn models using last frames highlight clips training works best. order evaluate performance l-char-lstm model also train word-lstm model tokenizing chats test results test results shown table somewhat surprisingly vision model accurate language model despite real-time nature comment stream. perhaps visual form game highlight events similar animations. however including language vision lv-lstm model signiﬁcantly improves vision alone comments exhibit additional contextual information. comparing results ablation ﬁnal test seems data contributes higher accuracy. effect apparent vision models perhaps complexity. moreover l-char-lstm performs better english compared traditional chinese. numbers given section variation number chats nalcs much higher expect critical effect language model. however results seem suggest l-char-lstm model pickup factors chat data instead counting number chats. expect different language model suitable traditional chinese language able improve results data. presented dataset multimodal methods highlight prediction based visual cues textual audience chat reactions multiple languages. hope dataset encourage multilingual multimodal research. references stanislaw antol aishwarya agrawal jiasen margaret mitchell dhruv batra lawrence zitnick devi parikh. visual question answering. iccv. sandra avila lopes antonio arnaldo arajo. vsumm mechanism designed produce static video sumpattern maries novel evaluation method. recognition letters. akira fukui dong park daylen yang anna rohrbach trevor darrell marcus rohrbach. multimodal compact bilinear pooling visual question answering visual grounding. emnlp. ranjay krishna yuke oliver groth justin johnson kenji hata joshua kravitz stephanie chen yannis kalantidis li-jia david shamma michael bernstein fei-fei. visual genome connecting language vision using crowdsourced dense image annotations. arxiv.. tsung-yi michael maire serge belongie james hays pietro perona deva ramanan piotr dollr lawrence zitnick. microsoft coco common objects context. eccv. olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei. imagenet large scale visual recognition challenge. ijcv. francis ferraro nasrin mostafazadeh ishan misra jacob devlin aishwarya agrawal ross girshick xiaodong pushmeet kohli dhruv batra visual storytelling. naacl. makarand tapaswi yukun rainer stiefelhagen antonio torralba raquel urtasun sanja fidler. movieqa understanding stories movies question-answering. cvpr. atousa torabi christopher hugo larochelle aaron courville. using descriptive video services create large data source video annotation research. arxiv.v.", "year": 2017}