{"title": "Combinatorial Topic Models using Small-Variance Asymptotics", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "Topic models have emerged as fundamental tools in unsupervised machine learning. Most modern topic modeling algorithms take a probabilistic view and derive inference algorithms based on Latent Dirichlet Allocation (LDA) or its variants. In contrast, we study topic modeling as a combinatorial optimization problem, and propose a new objective function derived from LDA by passing to the small-variance limit. We minimize the derived objective by using ideas from combinatorial optimization, which results in a new, fast, and high-quality topic modeling algorithm. In particular, we show that our results are competitive with popular LDA-based topic modeling approaches, and also discuss the (dis)similarities between our approach and its probabilistic counterparts.", "text": "topic models emerged fundamental tools unsupervised machine learning. modern topic modeling algorithms take probabilistic view derive inference algorithms based latent dirichlet allocation variants. contrast study topic modeling combinatorial optimization problem propose objective function derived passing smallvariance limit. minimize derived objective using ideas combinatorial optimization results fast high-quality topic modeling algorithm. particular show results competitive popular lda-based topic modeling approaches also discuss similarities approach probabilistic counterparts. topic modeling long fundamental unsupervised learning large document collections. though roots topic modeling date back latent semantic indexing probabilistic latent semantic indexing arrival latent dirichlet allocation turning point transformed community’s thinking topic modeling. several followups address limitations original model also helped pave subsequent advances bayesian learning methods including variational inference methods nonparametric bayesian models among others. family topic models almost exclusively cast probabilistic models. consequently vast majority techniques developed topic modeling—collapsed gibbs sampling variational methods factorization approaches theoretical guarantees —are centered around performing inference underlying probabilistic models. limiting purely probabilistic viewpoint missing important opportunities grounded combinatorial thinking. realization leads central question paper obtain combinatorial topic model competes lda? answer question afﬁrmative. particular propose combinatorial optimization formulation topic modeling derived using small-variance asymptotics model. produces limiting versions various probabilistic learning models solved combinatorial optimization problems. analogy worth keeping mind k-means solves combinatorial problem arises upon letting variances zero gaussian mixtures. techniques proved quite fruitful recently e.g. cluster evolution hidden markov models feature learning supervised learning hierarchical clustering others common theme examples computational advantages good empirical performance k-means carry richer based models. indeed compelling example demonstrate hard cluster evolution algorithm obtained orders magnitude faster competing sampling-based methods still signiﬁcantly accurate competing probabilistic inference algorithms benchmark data. merely using obtain combinatorial topic model sufﬁce. need effective algorithms optimize resulting model. unfortunately direct application greedy combinatorial procedures lda-based model fails compete usual probabilistic methods. setback necessitates idea. surprisingly local reﬁnement procedure combined improved word assignment technique transforms approach competitive topic modeling algorithm. contributions. summary main contributions paper following perform standard model obtain combinatorial topic model. develop optimization procedure optimizing derived combinatorial model utilizing demonstrate approach competes favorably existing state-of-the-art topic modeling algorithms; particular approach orders magnitude faster sampling-based approaches comparable better accuracy. proceeding outline technical details make important comment regarding evaluation topic models. connection approach standard viewed analogously connection k-means gaussian mixture model. such evaluation nontrivial; topic models evaluated using predictive log-likelihood related measures. light hard-vs-soft analogy predictive log-likelihood score misleading evaluate performance k-means algorithm clustering comparisons typically focus ground-truth accuracy lack available ground truth data assess combinatorial model must resort synthetic data sampled model enable meaningful quantitative comparisons; line common practice also present results real-world data hard soft predictive log-likelihoods. algorithms. many techniques developed efﬁcient inference lda. popular perhaps mcmc-based methods notably collapsed gibbs sampler variational inference methods among mcmc variational techniques typically yields excellent results guaranteed sample desired posterior sufﬁciently many samples. running time slow many samples required convergence. since topic models often used large collections signiﬁcant effort made scaling algorithms. recent example presents massively distributed implementation. methods outside focus paper focuses combinatorial model quatitatively compete probabilistic model. ultimately model amenable fast distributed solvers obtaining solvers model important part future work. complementary line algorithms starts consider certain separability assumptions input data circumvent np-hardness basic model. works shown performance competitive gibbs sampling scenarios also featuring theoretical guarantees. recent viewpoints offered small-variance asymptotics noted above recently emerged powerful tool obtaining scalable algorithms objective functions hardening probabilistic models. similar connections known instance dimensionality reduction multi-view learning classiﬁcation structured prediction starting dirichlet process mixtures thread research considered applying richer bayesian nonparametric models. applications include clustering feature learning evolutionary clustering inﬁnite hidden markov models markov jump processes inﬁnite svms hierarchical clustering methods related thread research considers apply methods data likelihood gaussian precisely scenario falls. shown applied long likelihood member exponential family distributions. work considers topic modeling potential application develop algorithmic tools without fails succeed topic models; present paper ﬁxes using stronger word assignment algorithm introducing local reﬁnement. combinatorial optimization. developing effective algorithms topic modeling borrow ideas large literature combinatorial optimization algorithms. particular k-means community signiﬁcant effort made improve upon basic k-means algorithm known prone local optima; techniques include local search methods good initialization strategies also borrow ideas approximation algorithms notably algorithms based facility location problem detail combinatorial approach topic modeling. start derivation underlying objective function basis work. objective derived model applying contains terms. ﬁrst similar k-means clustering objective seeks assign words topics particular sense close. second term arising dirichlet prior per-document topic distributions places penalty number topics document. recall standard model. choose topic weights document choose word weights topic then word document choose topic word cat. scalars denote vector words documents topic indicators words documents concatenation variables concatenation variables. also total number word tokens document vectors length number topics. vectors length size vocabulary. write full joint likelihood model factored form number word tokens document assigned topic following obtain objective taking logarithm likelihood letting variance zero. given space considerations summarize derivation; full details available appendix consider ﬁrst bracketed term taking logs yields terms form terms form noting latter multinomial distribution thus member exponential family appeal results introduce parameter scaling variance. particular write bregman divergence form exp) refers discrete kl-divergence ˜wjt indicator vector word token wjt. straightforward verify ψzjtwjt. next introduce parameter scales variance appropriately write resulting distribution proportional exp). expected value distribution remains ﬁxed variance goes zero exactly require. this consider second bracketed term scale appropriately well; ensures hierarchical form model retained asymptotically. particular write exp. manipulation distribution conclude negative dirichlet multinomial term becomes asymptotically number topics document i.e. number topics currently used document formalize denote following terms negative log-likelihood together. terms vanish asymptotically since scaling thus remaining terms objective ones arising word likelihoods dirichlet-multinomial. using bregman divergence representation additional parameter conclude negative log-likelihood asymptotically yields following remind reader ψzjtwjt. thus obtain k-means-like term says words documents close assigned topic terms kl-divergence also many topics represented document. note. scale obtain simple objective parameter words scaling natural approach integrate joint likelihood done collapsed gibbs sampler. would obtain additional dirichlet-multinomial distributions properly scaling discussed would yield simple objective places penalties number topics document well number words topic. optimization would performed respect topic assignment matrix. future work consider effectiveness objective function topic modeling. combinatorial objective hand ready develop algorithms optimize particular discuss locally-convergent algorithm similar k-means hard topic modeling algorithm then introduce powerful techniques word-level assignment method arises connections proposed objective function facility location problem; incremental topic reﬁnement method inspired local-search methods developed k-means. despite apparent complexity algorithms show per-iteration time matches collapsed gibbs sampler ﬁrst describe basic iterative algorithm optimizing combinatorial hard objective derived previous section basic algorithm follows k-means style—we perform alternate optimization ﬁrst minimizing respect topic indicators word minimizing respect topics consider ﬁrst minimization respect ﬁxed. case penalty term objective function number topics document relevant minimization. therefore minimization performed closed form computing means based assignments known properties kl-divergence; proposition case topic vectors computed follows entry corresponding topic word simply equal number occurrences word assigned topic normalized total number word tokens assigned topic next consider minimization respect ﬁxed follow strategy similar dp-means particular compute kl-divergence word token every topic log. then topic currently occupied word token document i.e. tokens document penalize distance next obtain assignments reassigning word token topic corresponding smallest divergence continue alternating strategy convergence. running time batch algorithm shown iteration total number word tokens number topics. also show algorithm guaranteed converge local optimum similar k-means dp-means. basic algorithm advantage achieves local convergence. however quite sensitive initialization analogous standard k-means. section discuss analyze alternative assignment technique used initialization locally-convergent basic algorithm replace completely. algorithm details alternate assignment strategy tokens. inspiration greedy algorithm arises fact view assignment problem given instance uncapacitated facility location problem recall problem aims open facilities potential locations. given clients distance function cost function problem aims subset minimizes assignment problem combinatorial topic modeling consider problem assigning word tokens topics ﬁxed document topics correspond facilities clients correspond word tokens. facility distances clients facilities given corresponding kl-divergences detailed earlier. objective corresponds exactly assignment problem topic modeling. algorithm greedy algorithm shown achieve constant factor approximation guarantees distances clients facilities forms metric algorithm appears computationally expensive requiring multiple rounds marking round requires minimizer exponentially-sized sets. surprisingly mild assumptions structure problem derive efﬁcient implementation algorithm runs total time details efﬁcient implementation presented appendix unlike traditional clustering problems topic modeling hierarchical word level assignments mini-topics explicitly reﬁning mini-topics help achieving better word-coassignment within document. inspired local search techniques clustering literature take similar approach here. however traditional approaches directly apply setting; therefore adapt local search techniques clustering topic modeling problem. speciﬁcally consider incremental topic reﬁnement scheme works follows. given document consider swapping word tokens assigned topic within document another topic. compute change objective function would occur updated topic assignments tokens updated resulting topic vectors. speciﬁcally document mini-topic formed word tokens assigned topic objective function change computed updated topics topics used document accept move mini=i update topics assignments accordingly. continue next mini-topic hence term incremental. note accept moves improve objective function instead single best move traditional approaches since updated every objective-decreasing move randomly permute processing order documents iteration. usually helps obtaining better results practice. algorithm details. ﬁrst glance appears incremental topic reﬁnement strategy computationally expensive. however computing global change objective function performed time topics maintained count matrices. counts involving words mini-topic total counts affected. since compute change across topics across mini-topics total running time incremental topic reﬁnement seen basic batch algorithm facility location assignment algorithm. ﬁrst experiments simulated data. compare three versions algorithms—basic batch improved word assignment improved word topic reﬁnement collapsed gibbs sampler standard variational inference algorithm http//psiexp.ss.uci.edu/research/programs data/toolbox.htm http//scikit-learn.org/stable/modules/generated/sklearn.decomposition.latentdirichletallocation.html figure left running time comparison iteration facility location improved word algorithm local reﬁnement data sets different sizes. word/cgs refine/cgs refer ratio word refine cgs. larger datasets word takes roughly gibbs iterations refine takes roughly gibbs iteration. right comparison topic reconstruction errors different algorithms different sizes synthb. methodology. lack ground truth data topic modeling following benchmark synthetic data. train algorithms following data sets. documents sampled model topics vocabulary size document length documents sampled model topics vocabulary size document length collapsed gibbs sampler collect samples iterations thinning burn-in iterations. variational inference runs iterations. word algorithm replaces basic word assignment improved word assignment step within batch algorithm word+refine alternates improved word incremental topic reﬁnement steps. word word+refine iterations respectively. basic word word+refine experiments best results presented stated otherwise. contrast true parameters provided input algorithms whenever applicable. note heavily handicapped methods setup since algorithms designed speciﬁcally data model. assignment accuracy. gibbs sampler algorithms provide word-level topic assignments. thus compare training accuracy assignments shown table result gibbs sampler given highest among samples selected. accuracy shown terms normalized mutual information score adjusted rand index range standard evaluation metrics clustering problems. plots performance word+refine matches slightly outperforms gibbs sampler wide range values. topic reconstruction error. look reconstruction error true topic-word distributions learned distributions. particular given learned topic matrix true matrix hungarian algorithm align topics evaluate distance pair topics. figure presents mean reconstruction errors topic different learning algorithms varying number documents. baseline also include results k-means algorithm kl-divergence document assigned single topic. that data anchor word+refine methods perform best; appendix results discussion. table predictive word log-likelihood documents enron nytimes datasets ﬁxed value. hard short hard predictive word log-likelihood computed using word-topic assignments inferred word algorithm original short original predictive word log-likelihood computed using document-topic distributions inferred sampler short symmetric kl-divergence. word+refine algorithm word assignments facility location local reﬁnement step relative runnings times improve data sizes gets larger large data sets iteration refine roughly equivalent gibbs iteration iteration word roughly equivalent gibbs iterations. since typically runs thousands gibbs iterations observe several orders magnitude improvement speed algorithm. further running times could signiﬁcantly enhanced noting facility location algorithm trivially parallellizes. addition results found per-iteration running times consistently faster consider real-world data sets different properties random subset enron emails subset york times articles documents reserved predictive performance assessment datasets. following metrics hard predictive word log-likelihood standard probabilistic predictive word artist painting museum century show collection history french exhibition painting exhibition portrait drawing object photograph gallery artist plane ﬂight airport passenger pilot aircraft crew planes ﬂight plane passenger airport pilot airline aircraft planes airlines money million fund donation dollar contribution donor raising ﬁnancial fund raising contribution donation raised donor soft raise ﬁnance foundation driver truck vehicles vehicle ford seat wheel driving drive driver vehicles vehicle truck wheel fuel engine drive ford log-likelihood documents. topic assignments documents either perform iteration word algorithm used compute hard predictive log-likelihood mcmc sample assignments learned topic matrix. hard log-likelihood viewed natural analogue standard predictive log-likelihood setting. also compute symmetric kl-divergence learned topics. make fair comparisons tune value resulting number topics document comparable sampler. remind reader issues raised introduction namely combinatorial approach longer probabilistic therefore would necessarily expected perform well standard likelihood-based score. table shows results enron nytimes datasets. approach excels hard predictive word log-likelihood lags standard mixture-view predictive word log-likelihood line objectives reminiscent differences k-means gmms. table shows sample topics generated method. appendix results predictive log-likelihood including comparisons approaches cgs. goal groundwork combinatorial optimization view topic modeling alternative standard probabilistic framework. small-variance asymptotics provides natural obtain underlying objective function using k-means connection gaussian mixtures analogy. potential future work includes distributed implementations scalability adapting k-means-based semi-supervised clustering techniques setting extensions k-means++ derive explicit performance bounds problem.", "year": 2016}