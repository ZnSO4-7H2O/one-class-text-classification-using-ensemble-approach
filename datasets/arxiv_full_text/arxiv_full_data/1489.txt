{"title": "Pay Attention to Those Sets! Learning Quantification from Images", "tag": ["cs.CL", "cs.AI", "cs.CV"], "abstract": "Major advances have recently been made in merging language and vision representations. But most tasks considered so far have confined themselves to the processing of objects and lexicalised relations amongst objects (content words). We know, however, that humans (even pre-school children) can abstract over raw data to perform certain types of higher-level reasoning, expressed in natural language by function words. A case in point is given by their ability to learn quantifiers, i.e. expressions like 'few', 'some' and 'all'. From formal semantics and cognitive linguistics, we know that quantifiers are relations over sets which, as a simplification, we can see as proportions. For instance, in 'most fish are red', most encodes the proportion of fish which are red fish. In this paper, we study how well current language and vision strategies model such relations. We show that state-of-the-art attention mechanisms coupled with a traditional linguistic formalisation of quantifiers gives best performance on the task. Additionally, we provide insights on the role of 'gist' representations in quantification. A 'logical' strategy to tackle the task would be to first obtain a numerosity estimation for the two involved sets and then compare their cardinalities. We however argue that precisely identifying the composition of the sets is not only beyond current state-of-the-art models but perhaps even detrimental to a task that is most efficiently performed by refining the approximate numerosity estimator of the system.", "text": "major advances recently made merging language vision representations. tasks considered conﬁned processing objects lexicalised relations amongst objects know however humans abstract data perform certain types higherlevel reasoning expressed natural language function words. case point given ability learn quantiﬁers i.e. expressions like all. formal semantics cognitive linguistics know quantiﬁers relations sets which simpliﬁcation proportions. instance encodes proportion ﬁsh. paper study well current language vision strategies model relations. show state-of-theart attention mechanisms coupled traditional linguistic formalisation quantiﬁers gives best performance task. additionally provide insights role ‘gist’ representations quantiﬁcation. ‘logical’ strategy tackle task would ﬁrst obtain numerosity estimation involved sets compare cardinalities. however argue precisely identifying composition sets beyond current state-of-the-art models perhaps even detrimental task eﬃciently performed reﬁning approximate numerosity estimator system. natural language sentences built complex interactions content words function words well-founded broad-coverage semantics therefore jointly model lexical items functional operators computational work language vision however mostly focused lexicon topical representations text fragments. strand work concentrates content word representations nouns particular whilst another interested approximate sentence representation image captioning visual question answering tasks systems perform poorly type questions requires exact numerosity estimation although recent work shows might possible adapt counting task paper focus complementary phenomenon considering quantiﬁers involve approximate number estimation mechanism; quantiﬁcation comparison step i.e. computation proportion sets. instance given images figure want quantify proportion ﬁsh. endeavour argue below simply investigation diﬀerent type quantiﬁer. claim speciﬁc problem interesting opportunity reﬂect build neural network architectures. linguistic level formal semanticists extensively studied expressions described relations restrictor selects target objects state-of-aﬀairs scope selects subset target satisﬁes certain property. alternatively seen proportions selected sets proportional property generalised quantiﬁers necessitates operation level abstraction which think interestingly diﬀerent level shallow reasoning needed process content words simple cardinals. intuition behind conjecture explained considering following. let’s assume want correct quantiﬁer particular concept-feature pair given speciﬁc image want network learn certain quantiﬁers tirely contained true overlap less correlation learnt diﬀerent conﬁgurations particular quantiﬁers conﬁgurations abstractions linguistic visual data comparison takes place irrelevant whether cream scoops indeed many exactly observed. fact argue below trying integrate information quantiﬁcation decision detrimental system. quantiﬁers operators applied regardless composition whether matches statistics observed category level. attempting category-level information result failure generalise randomly sampled subsets small cardinality. illustrates point knowledge redness enough predictive power system. similarly amount overlap sets associated particular quantiﬁers regardless cardinality sets matter proportion. ideal model learn abstract cardinality information too. straightforward eﬃcient strategy learn quantify could divide task subtasks learning generalize correlation data abstract representation latter quantiﬁers. high results obtained trained quantify synthetic scenarios coloured dots suggest able learn second subtask quite easily. paper study current strategies integrate language vision modalities suitable work full task involving quantiﬁcation real-life images. revisit stateof-the-art models considering features aﬀect model deals high-level process. particular focus role sequential processing modalities attention mechanisms within across modalities core many state-of-the-art systems. show that case content words attention mechanisms help obtaining salient representation linguistic visual input useful processing quantiﬁers. observed above contrast content words functional operators sets. approximate visually-grounded representation sets obtained exploiting logical structure linguistic query combined attention. concretely show dealing quantiﬁers instead computing composed representation linguistic query note special cases correlation certain conceptproperty pairs quantiﬁcation particular deﬁnitional properties correspond universal quantiﬁcation however special cases apply universal quantiﬁcation. attend image better reach multimodal composition using linguistic representation restrictor guide visual representation scenarios latter guide composition linguistic representation restrictor linguistic representation scope. results highlight using output lstm language side attend relevant parts image less successful attention mechanism. additionally provide insights role image gist representation built attention models quantiﬁcation task. ‘logical’ strategy tackle quantiﬁcation task would ﬁrst obtain numerosity estimation involved sets compare quantities. method could implemented aiming extract fully abstract representation sets data. however argue that given inherent diﬃculty identifying objects even more properties approximate representation form visual gist eﬃcient cognitively plausible strategy. finally mention work touches current debate balancing datasets natural images. example demonstrated simple bag-of-word baseline concatenates visual textual inputs achieve decent overall performance task. performance model excellent ability network encode certain types correlations either within across modalities. part results might language prior discovered dataset addressed either using abstract scenes carefully building dataset similar natural images corresponding diﬀerent answers quantiﬁcation dataset propose computational models quantiﬁers problem algorithmically describing logical quantiﬁers ﬁrst addressed using automata. following ﬁrst eﬀorts work done computational formal semantics model quantiﬁers language overview). recently distributional semantics turned problem demonstrating entailment relations hold quantiﬁer vectors obtained large corpora mapping distributional vector space formal space quantiﬁcation concept-property pair predicted. line work however considers linguistic modality without attention vision. parallel formal linguistic models psycholinguistics studied function words statistical perspective using architectures. showed approximate numerosity could extracted visual input without serial counting bringing computational evidence psycholinguistic observation infants develop numerosity abilities able count. particular interest aimed grounding linguistic quantiﬁers perception. quantiﬁers studied several many lots system trained human annotations images consisting white stripy ﬁsh. given image model predict proportion stripy using given quantiﬁers. authors showed spacing number objects played role prediction. studies touching upon interesting research avenue models available time powerful enough full investigation. meantime interesting progress modelling acquisition quantiﬁers bayesian probabilistic framework reported recently shown perform well tasks related quantiﬁcation counting simulating approximate number sense segu´ı instance explore task counting occurrences object image using convolutional demonstrate object identiﬁcation learnt surrogate counting. stoianov zorzi show emerges statistical property images deep networks learn hierarchical generative model visual input. interesting models also proposed focus issue counting everyday objects visual scenes using subitising strategies observed humans. similarly focusing subitising process address issue salient object detection show models discriminate images salient objects. discussed salient object detection task highly depends various properties images like uniformity various regions complexity foreground background close salient objects diﬀer size. models present paper seen continuation previous work linguistic quantiﬁers. systems evaluate rely explicit counting gist objects image produce appropriate quantiﬁer given scenario. also follow investigation ‘vague’ linguistic quantiﬁers train evaluate system real images rather examples. unlike them however investigate object position image start bounding boxes. knowledge recent attempt model non-cardinals visual quantiﬁcation task using neural networks. focus diﬀerence acquisition cardinals quantiﬁers showing modelled diﬀerent operations within network learning function cardinal/quantiﬁer. paper seen extending work augmenting list logical quantiﬁers proportional ones moving artiﬁcial scenarios geometric ﬁgures real images; importantly treating quantiﬁers relations sets objects amongst number distractors datasets numerosity annotation coco-qa ﬁrst dataset images associated number questions. coco-qa consists around images extracted questions generated automatically image descriptions. number questions four question categories make overall questions training test datasets. category authors observe evaluated models sometimes count however ability fairly weak count correctly presented unknown object types. starting built dataset aiming increase diversity knowledge kinds reasoning needed provide correct answers. consists around images questions ground truth answers. contains open-ended free-form questions answers provided humans. evaluation models dataset conﬁrmed number questions hard answered good combined understanding language vision modalities essential. diﬃculty number questions highlighted authors introduced clevr dataset allowing in-depth evaluation current models various visual reasoning tasks. reasoning skills investigated close task propose. show state-of-the-art systems perform poorly situations requiring short-term memory focusing subitising phenomenon salient object subitising dataset proposed contains everyday images annotated respect numerosity salient objects images gathered various sources ﬁltered create balanced distribution images containing obviously salient objects. eliminate bias unbalanced number distribution authors used cut-and-paste strategy generated synthetic image data. none datasets meets needs quantiﬁcation task. images salient objects category properties annotated. small numerosities represented. contain annotated objects diﬀerent categories provide properties annotation. recentlyhowever version coco-qa coco attribute-qa released. contains images annotated objects properties consists images unique objects attributes total objectattribute pairs. take starting point create dataset natural malinowski learn perform given task end-to-end fashion. ﬁrst proposed tackle based combination global visual feature vectors extracted convolutional neural network text feature vectors extracted using long-short term memory network various lstm-cnn models proposed diﬀer regard types features combined mere concatenation complex operations like element-wise multiplication multimodal compact bilinear pooling proposals also made architecture. lstm jointly model image question treat image word appended question image processed model output frozen training process. recently opposite site convolutional architecture used learn types feature interaction signiﬁcant progress made task introducing memory attention components taken areas lavi. instance introduced attention-based framework problem image caption generation. memory networks used tackle tasks involving reasoning natural language text combination memory attention components proposed e.g. recently applied challenge dynamic memory network stacked attention networks further combine dynamic properties previous models compositionality process natural language questions reinforcement learning. build previous work porting insights task quantiﬁcation task. particular investigate role lstms combination cnns simple concatenation within stacked attention mechanisms. propose model combines formal semantics intruitions quantiﬁers latest ﬁndings models attention mechanisms. restrictor scope quantiﬁer respectively. scenario image containing objects type restrictor property expressed scope. refer objects required property target objects. take quantiﬁers stand ﬁxed relations hence take relevant sets objects correct answer scenarios target objects restrictor respectively. deﬁne most prevalence estimates reported low-prevalence majority predications. particular assign ratios lower equal ratios equal greater ratios ranging values assigned some. coco-attribute dataset comprehensive property annotation. contains image ms-coco objects marked region coordinates/bounding boxes properties annotated humans. total object categories properties annotated regions average properties annotated object. objects image annotated respect properties included object categories bounding boxes provided. hence cannot exploit full image must restrict annotated regions. illustrated figure construct q-coco scenarios data following procedure described below. first ﬁlter images containing less annotated objects thus obtaining unique images. choice motivated fact lowest restrictor cardinality allows quantiﬁers represented data given ratios assigned them. clarify point objects would case would would some; secondly images properties associated annotated object extracted. compute overall frequency property avoid data sparsity retain properties frequency object ‘banana’ given image originally annotated properties frequent ones included obtain unique properties. finally retain images containing least annotated objects belong category reported table resulting dataset includes unique images depicting annotated objects. average image contains annotated objects average properties. mentioned above scenarios q-coco consist bounding boxes extracted images object/property annotations. figure reports distribution scenarios respect number annotated objects included. noted scenarios containing objects vast majority using annotations unique scenarios generate possible queries corresponding ground-truth answers following ratios deﬁned above. avoid including implausible queries like e.g. ‘metallic banana’ generating queries whose answer ensure properties occur together target object least annotation included. shows queries generated annotation real image included dataset. unique objects unique properties properties object objects property objects scenario objects scenario object object property property total images total total queries linguistic modality. check whether apply q-coco analyse sample datapoints randomly selecting balanced number cases quantiﬁer. query compute number times occurs paired given quantiﬁer e.g. ‘black all’ sample dataset. divide frequency total number times query ‘black dog’ appears sample dataset. obtain ratio describing bias query toward quantiﬁer. ‘black dog’ appears times dataset cases equally split among quantiﬁers dataset considered perfectly balanced around cases quantiﬁer. cases correspond speciﬁc quantiﬁers dataset biased. plot distribution ratios relative quantiﬁer. seen cases particularly biased meaning model could simply learn correlations object-properties quantiﬁers order give right answer tested seen query. limitation dataset cannot easily solved since real-image dataset likely contain correlations depend object-property relations. illustrate ‘banana metallic’ present likely appear quantiﬁer all. ﬁnding illustrates general issue since carrying quantiﬁcation tasks using real images might always aﬀected regularities object-property distributions real world. argued introducgiven inherent bias object distribution real images also investigate synthetic dataset. this select imagenet background visual corpus since contains object categories annotated properties visual imagenet contains images annotated bounding boxes. object categories also provided human annotations properties representing total unique attributes. imagenet images rather diﬀerent coco attribute time don’t contain multiple objects. q-coco create q-imagenet scenarios bounding boxes image. bounding extracted image. result dataset diﬀers q-coco merges together bounding boxes originally belong image giving leeway overcome bias found real scenes. build synthetic scenarios made diﬀerent bbs. choice motivated reasons. first q-coco images contain less objects considered reasonable ‘realistic’ upper bound. second number allows fairly large variability respect cardinalities restrictor scope. objects associated annotated property labels corresponding images. select imagenet items annotated least properties extract bounding human annotation performed. results bounding boxes. subsequently ﬁltered according criterion property words must occur least times ukwac corpus ensures quality corresponding word embeddings. reported table ﬁltering process diﬀerent objects associated bounding boxes labelled properties. average object unique bounding boxes assigned properties property shared objects. ages. assign ground-truth answer scenario-query combination. further make synthetic scenarios plausible possible also constraint distractor images datapoint. association measure based ms-coco captions evaluates chance objects appear together real image. idea objects likely occur together make realistic scenarios thus preferred generation process compute proxy likelihood objects co-occur image objects number times words cooccur single caption frequency captions ms-coco overall number words captions. object’s label occur captions considered probability co-occurrence objects. selecting distractors object interest particular scenario randomly pick according likelihood co-occurrence object given calculation. check whether q-imagenet contains language bias applying method used q-coco. plot distribution datapoints respect proportion cases given query occur given quantiﬁer. noticed distribution much better compared real-image dataset. average datapoints always around chance level indicating almost equal number cases quantiﬁer occur given query. uncontrolled whole generated datapoints randomly select balanced number cases quantiﬁer. setting possible encounter known scenarios queries test time scenario-query combinations unseen. unseen objects setting tests generalisation power models scenarios containing unseen objects. randomly divide list concepts pick training testing/validation. concept randomly select balanced number diﬀerently quantiﬁed datapoints. unseen properties similarly unsobj setting tests generalisation power models respect properties. procedure followed obtain dataset used setting unsobj setting except split datapoints according properties. seen combinations object property. instance system sees black training black test time. build setting ﬁrst randomly select object property tuples training tuples testexperiment seven diﬀerent models understand contribution various mechanisms architectures quantiﬁcation task. ﬁrst models ‘blind’ bow+cnn simple baselines literature show language-only model performs one-hot representations simple concatenation one-hot language vectors image representations. next models ‘blind’ lstm lstm+cnn check contribution sequential processing task language-only system using modalities. expect sequential processing somewhat account composition restrictor scope components query whereas play relevant role visual inputs since sets bounding boxes order relevant. turn attention mechanisms adapt stacked attention network hoping attention allow system focus relevant sets individuals quantifying. using insights formal linguistics also propose model quantiﬁcation memory network clearly creates separate representations scope restrictor quantiﬁer following hypothesis quantiﬁcation operates deﬁned representations. finally combine insights investigated models general system name quantiﬁcation stacked attention network qsan seen linguistically-motivated architecture based speciﬁcally designed quantiﬁcation task. visual input bounding scenario extract visual representation using convolutional neural network vgg- model pre-trained imagenet ilsvrc data matconvnet toolbox features extraction. bounding represented -dimension vector extracted fully connected layer computational eﬃciency subsequently reduce vectors dimensions applying singular value decomposition linguistic input similarly word query represented -dimension vector built wordvec cbow architecture using parameters shown perform best corpus used building semantic space billion tokens concatenation web-based ukwac mid- dump english wikipedia british national corpus baselines consider models shown remarkable accuracy task given simplicity ibowimg implement minor adaptations models suit quantiﬁcation task described below. ‘blind’ language-only model. network input layer size overall vocabulary query ﬁrst converted one-hot bag-ofwords vector transformed ‘word feature’ embedding dimensions. combined features sent softmax layer predicts answer assigning appropriate weights output layer node corresponds quantiﬁers. cnn+bow model adaptation ibowimg. uses linguistic input above concatenated visual input. query question ﬁrst converted one-hot bag-of-words vector transformed ‘word feature’ embedding. linguistic embedding concatenated ‘image feature’ obtained convolutional neural network resulting embedding sent softmax classiﬁer predicts quantiﬁers above. order single vector visual input simply concatenate visual vectors individual bounding boxes scenarios. q-coco dataset number objects contained images ranges concatenate ‘frozen’ visual vectors -dimension vector ‘empty’ cells scenario zero vectors. q-imagenet dataset number objects ﬁxed concatenate visual vectors -dimension vector ‘blind’ lstm graphic representation lstm provided model receives input linguistic embeddings query. then input processed lstm module cells hope might simulate composition restrictor scope components query; output linearly mapped -dimension vector. softmax classiﬁer applied vector order output correct quantiﬁer. cnn+lstm shown figure visual features processed lstm output last cell combined linguistic information provided ‘blind lstm’ module processing query stacked attention network stacked attention network proposed motivated idea might require step reasoning. model supposed particular attention image regions relevant query attention layer. diagram presented figure zooms main module network attention layer. layer sums visual vector linguistic representation applies tanh softmax functions result obtain weighted average initial visual vectors gist thus encodes information question image. consistently purpose architecture namely performing multi-step reasoning attention layer used twice san. shown ﬁrst pass applies representation query obtained lstm module visual input. second pass main module takes linguistic input original linguistic representation output ﬁrst pass. ﬁnal gist softmax classiﬁer obtain predicted quantiﬁer. model adaptation memory network originally proposed achieved state-of-the-art performance synthetic question answering language modelling. model shown main feature explicitly encodes retrieval sets assumed formal semantics model quantiﬁers model implements idea quantiﬁcation model steps ﬁrst step produces representation relevant sets individuals second step computes relation sets step shown diagram visual linguistic vectors datapoints linearly mapped -dimension space. visual vectors memory cells cell compute similarity value visual vector linguistic vector representing query restrictor calculating product normalized using euclidean norm. result either -dimension ‘similarity vector figure calculate weighted vectors individual multiplying memory cells associated similarity values gives representation amount ‘dogness’ object. representation restrictor calculated summing memory cells weighted vectors obtaining restrictor gist. represents much ‘dogness’ found given scenario. calculate product weighted vectors scope linguistic vector normalise values using euclidean norm. again result -dimension ‘similarity vector second weighted vector obtained multiplying gives amount ‘black-dogness’ object. representation overlap restrictor scope memory cells. represents much ‘black-dogness’ found given scenario. model composition restrictor scope components operationalised model lstm module accomplished using probability similarity vector weight visual vectors vector linearly transformed vector. apply softmax classiﬁer resulting vector returns probability distribution quantiﬁers. concatenation restrictor gist step model re-implemented main diﬀerence given linguistic information restrictor e.g. embedding word dog. refer part model restrictor module output restrictor gist. network takes probabilities obtained softmax layer restrictor module uses probabilities weight initial visual vectors. assume operation attend correct regions visual scenario restrictor composition restrictor scope obtained weighting visual vectors bounding boxes restrictor probability feeding scope module. weighted visual vectors scope results models q-coco settings reported table ‘blind’ lstm model turns best-performing model setting even simpler ‘blind’ achieving remarkable good accuracy outcome consistent previous discussion bias dataset towards linguistic modality. models capitalising solely linguistic associations objects properties eﬀective similarly eﬀective relatively complex state-of-the-art models integrate modalities. words adding visual information result accuracy improvements setting. expected language-only models particularly eﬀective predicting cases object-property distributional associations might intuitively play higher role compared quantiﬁers. particular ‘blind’ lstm achieves accuracy accuracy all. unsprop setting models’ accuracies around chance level. recap setting train models properties test unseen properties. seen table none models able generalise unseen properties. conﬁrms task really challenging suggests visual information provided features tuned task object classiﬁcation might informative properties concerned. intuition partially conﬁrmed results unsobj accuracies increase even though performance blind models almost best-performing qsan model noted generalising unseen objects slightly feasible task compared unseen properties. moreover improvement obtained models might indicative object bias encoded visual vectors. ﬁnal setting unsque qsan best model followed ‘blind’ lstm fairly large best attention network models suggests qsan extent able generalise unseen queries. contrast blind models’ accuracies drop compared thus indicating moving q-imagenet dataset observe table that dataset harder q-coco since accuracies generally lower across settings; attention models i.e. qsan turn overall best across settings qsan outperforming slightly worse san. conﬁrms crucial role using restrictor guide attention image instead composing restrictor scope linguistic level only done lstm model. particular qsan model best-predicting settings namely unsobj unsprop second-best unsque. slightly worse qsan unsobj better qmn. finally outperforms qsan unsque second-best performing unsprop. starting setting table shows qsan outperforms almost cnn+lstm almost visual representation results provided shows accuracies best-performing models relative quantiﬁer. noticed qsan model outperforms models whereas best predicted best predicted san. ﬁrst glance noted average accuracies qsan constant across quantiﬁers whereas others drops corresponding speciﬁc quantiﬁers table report quantitative analysis errors made qsan san. ﬁrst thing noticed qsan correctly predicts target quantiﬁer often predicts wrong ones. contrast hold predicts often actual target quantiﬁer. second errors made qsan always ‘plausible’ meaning network wrong tends predict quantiﬁers adjacent target one. wrongly outputs often some target quantiﬁer all. contrast errors follow pattern network indeed outputs often correct quantiﬁer some. third noticed tends rather ‘negative’ predictions meaning generally outputs answers left’ quantiﬁer scale. illustrate wrongly outputs often some even target quantiﬁer most. qsan best-performing system followed attention model contrast q-coco models obtain fairly high accuracies unsque setting dataset none models reaches accuracy qsan however best second-best respectively. datasets probably highest repetition objects q-coco compared q-imagenet comparably much lower number object categories included even though properties almost halved latter compared former conjecture lower number object categories q-coco plays crucial role helping model ‘recognise’ better given object scenario. thus seen often object training help seen often property better understand results obtained qsan perform kinds analysis. ﬁrst aimed testing whether task predicting correct quantiﬁer harder scenario contains increasing number distractors queried property. instance query black could case model confounded high number black objects present amongst distractors. check computing total number cases cardinality distractors queried property well number cases correctly predicted qsan q-imagenet cardinality. proportion second analysis aimed checking whether accuracy qsan qimagenet aﬀected actual ratio targets restrictor objects. hypothesis model might confounded ratios boundaries diﬀerent quantiﬁers perform better ratio undoubtedly associated given quantiﬁer analysing model’s accuracy respect whole span ratios ranging clear ‘peaks’. accordingly model’s predictions stable across quantiﬁers shown however could case local patterns ﬂuctuation found within quantiﬁer’s ratios. clear zoom three ones deﬁned ranges. notice expected trend clearly visible plots. particular peak observed some slightly fuzzier ﬂuctuation however still consistent hypothesis. third general analysis aims understanding extent quantiﬁcation made harder deal ‘real’ concepts images. wish check whether purely logical part quantiﬁer computes ratio sets easily learnt network. this reduce uncontrolled q-imagenet dataset simplest instance white dots black dots images gray background. build simple classiﬁer data training scratch shallow convolutional neural network convolution layer. system obtains accuracy conﬁrming learn quantiﬁcation comparison step nearly perfectly completely abstract representation given. interesting result conﬁrms actual challenge visual quantiﬁcation right strategies deal uncertainty object property recognipaper investigated task quantifying visual scenes using natural language quantiﬁers. discussed section assigning quantiﬁer scenario involves steps approximate number estimation mechanism acting relevant sets image; quantiﬁcation comparison step. straightforward logical strategy learn two-step operation would divide task subtasks learning correlation data abstract representation latter quantiﬁers. high results obtained trained quantify synthetic scenarios coloured dots suggest able learn second subtask quite easily. experiments using shallow convolution layer abstract images conﬁrms this. however know previous work object identiﬁcation particular property identiﬁcation solved problem. task single mistake identiﬁcation dramatic consequences especially considering sets small cardinalities also unclear exact object identiﬁcation performed humans quantify therefore ﬁrst showed letting network compose scope restrictor language side using representation attend image resulted underperforming models. instead using linguistic representation quantiﬁer relation sets guiding attention mechanism produced much better accuracy illustrated qsan models. take result show that considering complex high-level phenomena useful correlate insights formal linguistics targeted mechanisms. hope study encourage work building linguistically-motivated neural architectures.", "year": 2017}