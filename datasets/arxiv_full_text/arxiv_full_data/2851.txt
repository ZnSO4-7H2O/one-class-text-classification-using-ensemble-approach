{"title": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves, such as a polygonal chain with only one bend, over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10 and CIFAR-100, using state-of-the-art deep residual networks. On ImageNet we improve the top-1 error-rate of a pre-trained ResNet by 0.56% by running FGE for just 5 epochs.", "text": "loss functions deep neural networks complex geometric properties well understood. show optima complex loss functions fact connected simple curves polygonal chain bend training test accuracy nearly constant. introduce training procedure discover high-accuracy pathways between modes. inspired geometric insight also propose ensembling method entitled fast geometric ensembling using train high-performing ensembles time required train single model. achieve improved performance compared recent state-of-the-art snapshot ensembles cifar- cifar- using state-of-the-art deep residual networks. imagenet improve top- error-rate pre-trained resnet running epochs. loss surfaces deep neural networks highly non-convex depend millions parameters. geometric properties loss surfaces well understood. even simple networks number local optima saddle points large grow exponentially number parameters moreover loss high along line segment connecting optima observations suggest local optima isolated. paper provide training procedure fact paths near-constant accuracy modes large deep neural networks. furthermore show wide range architectures paths form simple polygonal chain line segments. consider example figure illus*equal contribution moscow state university cornell university higher school economics. correspondence andrew gordon wilson <andrewcornell.edu>. trates resnet- l-regularized cross-entropy loss cifar- three different planes. left panel shows afﬁne combinations weights three independently trained networks. plane optima isolated corresponds standard intuition. however middle right panels show different paths near-constant accuracy modes weight space discovered proposed training procedure. endpoints paths independently trained dnns corresponding lower modes left panel. believe geometric discovery major implications research multilayer networks including improving efﬁciency reliability accuracy training creating better ensembles deriving effective posterior approximation families bayesian deep learning. indeed paper inspired geometric insight propose ensembling procedure efﬁciently discover multiple high-performing diverse deep neural networks. show paths merely exist overparametrization instead correspond meaningfully different representations efﬁciently ensembled increased accuracy. inspired observations propose fast geometric ensembling outperforms existing ensembling techniques including recent snapshot ensembles cifar- cifar- using state-of-the-art deep neural networks vgg- wide resnet-- resnet-. imagenet achieve error-rate improvement pretrained resnet- model running epochs. figure cross-entropy loss surface deep residual network cifar- function network weights two-dimensional subspace. panel horizontal axis ﬁxed attached optima independently trained networks. vertical axis changes panels change planes. left three optima independently trained networks. middle right quadratic bezier curve polygonal chain bend connecting lower optima left panel along path near-constant loss. notice panel direct linear path mode would incur high loss. rest paper organized follows. section discusses existing literature loss geometry ensembling techniques. section introduces proposed method curves train loss test error local optima investigate empirically section section introduces proposed ensembling technique empirically compare alternatives section finally section discuss connections ﬁelds directions future work. despite success deep learning across many application domains loss surfaces deep neural networks well understood. loss surfaces active area research falls distinct categories. ﬁrst category explores local structure minima found modiﬁcations. researchers typically distinguish sharp wide local minima respectively found using large small mini-batch sizes training. hochreiter schmidhuber keskar example claim minima lead strong generalization sharp minima deliver poor results test dataset. however recently dinh argue existing notions ﬂatness cannot directly explain generalization. better understand local structure loss minima proposed visualization method loss surface near minima found sgd. applying method variety different architectures showed loss surfaces modern residual networks seemingly smoother vgg-like models. neural networks able overcome poor local optima. choromanska investigated link loss function simple fully-connected network hamiltonian spherical spin-glass model. strong simplifying assumptions showed values investigated loss function local optima within well-deﬁned bound. research showed mild conditions gradient descent almost surely converges local minimizer saddle point starting random initialization. recent work freeman bruna theoretically showed local minima neural network hidden layer relu activations connected curve loss upper-bounded along curve constant depends number parameters network smoothness data. using dynamic programming approach able construct polygonal chain alexnet cifar- test accuracy stays along curve. polygonal chain constructed needed least bends. contrast propose much simpler training procedure near-constant accuracy polygonal chains bend optima even modern state-of-theart architectures. inspired properties loss function discovered procedure also propose stateof-the-art ensembling method trained time required train single dnn. proposed related ensembling approach gathers outputs neural networks different epochs training stabilize ﬁnal predictions. recently huang proposed snapshot ensembles cosine cyclical learning rate save snapshots model training times learning rate achieves minimum. experiments compare geometrically inspired approach huang showing improved performance. uniform distribution difference latter expectation loss respect uniform distribution expectation respect uniform distribution curve. losses coincide example deﬁnes polygonal chain line segments equal length parametrization segments linear minimize iteration sample uniform distribution make gradient step respect loss obtain unbiased estimates gradients et∼u ∇θl) ∇θet∼u simplest parametric curve consider polygonal chain trained networks serve endpoints chain bends chain parameters curve parametrization. consider simplest case chain bend draxler simultaneously independently discovered existence curves connecting local optima loss landscapes. curves used different approach inspired nudged elastic band method quantum chemistry. describe method minimize training error along path connects points space weights. section introduces general procedure arbitrary parametric curves section describes polygonal chains bezier curves example parametrizations curves. describe section apply batch normalization test time points curves. supplementary material discuss computational complexity proposed approach. connection procedure r|net| sets weights corresponding neural networks independently trained minimizing user-speciﬁed loss crossentropy loss. here |net| number weights dnn. moreover r|net| continuous piecewise smooth parametric curve parameters figure error function point curves found proposed method using resnet- cifar-. left train error. bottom left test error; dashed lines correspond quality ensemble constructed curve points logits rescaling. right train loss regularized cross-entropy). bottom right cross-entropy logits rescaling polygonal chain. mean standard deviation output constant numerical stability free parameters. training computed separately mini-batch test time statistics aggregated training used. connecting dnns batch normalization along curve compute given mini-batches training usual. order apply batch-normalization network curve test stage compute statistics additional pass data running averages networks collected training. show proposed training procedure section indeed high accuracy paths connecting different modes across range architectures datasets. moreover investigate properties curves showing correspond meaningfully different representations ensembled improved accuracy. insights propose improved ensembling procedure section empirically validate section particular test vgg- datasets -layer wide resnet widening factor -layer resnet cifar- -layer resnet-bottleneck cifar-. cifar- cifar- standard data augmentation huang provide additional results including experiments fully connected recurrent networks appendix. model dataset train networks different random initializations modes. proposed algorithm section path connecting modes weight space quadratic bezier curve polygonal chain bend. also connect modes line segment comparison. experiments optimize loss bezier curves gradient loss intractable polygonal chains found loss stable. figures show results proposed mode connecting procedure resnet- cifar-. loss refers regularized cross-entropy loss. bezier curve polygonal chain train error test error indeed nearly constant bezier curve performing slightly better polygonal chain. hand train test loss somewhat increase towards though increase profound test loss. examine trend section appendix include table summarizing path ﬁnding experiments cifar- cifar- vggs resnets wide resnets follow general trends. hand resnet- worst test error along segment wide resnet-- worst error ﬁnding suggests loss surfaces state-of-theart residual networks indeed regular classical models like corresponds recent observations investigate relation observed connectedness local optima number parameters neural network. start network three convolutional layers followed three fullyconnected layers layer neurons. vary value train networks connect bezier curve using proposed procedure section value figure shows worst training loss along curve maximum losses endpoints ratio length curve line segment connecting modes. increasing number parameters able reduce difference worst value loss along curve loss single models used endpoints. ratio length found curve length line segment connecting modes also decreases monotonically result intuitive since greater parametrization allows ﬂexibility navigate loss surfaces. additionally section show curves correspond redundancy parametrization since points curve meaningfully ensembled. appendix provide construction would exploit redundancy parametrization provide curves correspond networks make identical predictions thus could used ensembling. figure worst train loss along curve maximum losses endpoints ratio length curve line segment connecting modes function scaling factor sizes fully-connected layers. explore ensembles constructed points sampled high accuracy curves. particular train polygonal chain bend connecting independently trained resnet- networks cifar- construct ensemble networks corresponding points placed equally-spaced grid curve. resulting ensemble error-rate test dataset. error-rate ensemble constructed endpoints curve ensemble three independently trained networks error rate thus ensemble networks curve outperformed ensemble endpoints implying curves found proposed method actually passing diverse networks produce predictions different produced endpoints curve. moreover ensemble based polygonal chain number parameters three independent networks comparable performance. furthermore improve ensemble chain without adding additional parameters computational expense accounting pattern increased training test loss towards centres linear paths shown figure training test accuracy relatively constant pattern loss shared across train test sets indicates overconﬁdence away three points deﬁning curve region networks tend output probabilities closer sometimes wrong answers. overconﬁdence decreases performance ensembles constructed networks sampled curves. order correct overconﬁdence improve ensembling performance temperature scaling inversely proportional loss. figure bottom right illustrates test loss resnet- cifar- temperature scaling. rescaling predictions networks test loss along curve decreases ﬂattens. further test error-rate ensemble constructed points curve went applying temperature scaling outperforming independently trained networks. however directly ensembling curves requires manual intervention temperature scaling additional pass training data networks test time perform batch normalization described section moreover also need train least networks endpoints curve. section propose different ensembling method inspired connectedness optima limitations. function resnet- cifar-. test error starts going error ensemble approximately error ensemble independently trained networks used endpoints curve. thus even moving away endpoint relatively small distance along curve network produces meaningfully different predictions network endpoint. line segment connecting endpoints curve error goes near second mode random direction weight space ensemble error never goes error single network. observed similar patterns architectures. suppose weights corresponding mode loss. cannot explicitly construct path before know multiple paths passing exist thus possible move away weight space without increasing loss. further know diverse networks providing meaningfully different predictions making relatively small steps weight space. inspired observations propose fast geometric ensembling method aims diverse networks relatively small steps weight space without leaving region corresponds test error. alg. provides outline algorithm. first initialize copy network weights equal weights trained network force move away without substantially decreasing prediction accuracy adopt cyclical learning rate schedule learning rate iteration deﬁned learning rates number iterations cycle given iteration mean processing mini-batch data. train network using standard l-regularized cross-entropy loss function proposed learning rate schedule iterations. middle learning rate cycle learning rate reaches minimum value collect checkpoints weights training ﬁnished ensemble collected models. figure illustrates adopted learning rate schedule. during periods learning rate large exploring weight space larger steps sacriﬁcing test error. learning rate small exploitation phase steps become smaller test error goes down. cycle length usually epochs method efﬁciently figure error two-network ensemble consisting endpoint curve point curve segment line segment connecting modes found sgd. polychain polygonal chain connecting endpoints. random segment straight line following endpoint curve random direction. distinct sets weights corresponding optima obtained independently training times. section showed exists path connecting test accuracy high along path. parametrize path section ensembling points curve achieve improved performance compared ensemble endpoints examine need move along curve point produces substantially different still useful predictions. particular investigate performance ensemble networks endpoint curve point curve corresponding figure shows test error ensemble section compare proposed fast geometric ensembling technique ensembles independently trained networks snapshot ensembles recent state-of-the-art fast ensembling approach. ensembling experiments -layer preactivation-resnet addition vgg- wide resnet-- models. links implementations models found appendix. release code upon publication. compare accuracy method function computational budget. network architecture dataset denote number epochs required train single model budget times random initializations ensemble models gathered runs. experiments vgg- wide resnet- models resnet- since epochs typically sufﬁcient train model. note runtime epoch same total computation associated budgets ensembling approaches. initial learning rate resnet wide resnet vgg. cycle length epochs total models ﬁnal ensemble. resnet wide resnet epochs total number models ﬁnal ensemble wide resnets resnets. procedure average predictions results ensembles. followed huang varied initial learning rate number snapshots report best results achieved corresponded resnet wide resnet vgg. total number models ensemble constrained network choice computational budget. experimental details appendix. table summarizes results experiments. conducted experiments outperforms particularly increase computational budget. performance improvement noticeable cifar-. large number classes models less likely make predictions. moreover greater uncertainty representation cifar- since number classes increased figure plot learning rate test error distance initial value function iteration preactivation-resnet- cifar-. circles indicate times save models ensembling. good initialization proposed procedure ﬁrst train network standard learning rate schedule time required train single model. pre-training ﬁnished initialize proposed fast ensembling algorithm remaining computational budget. order diverse samples algorithm several times smaller number iterations initializing different checkpoints saved training ensemble models gathered across runs. cyclical learning rates also recently considered smith topin huang proposed method perhaps closely related snapshot ensembles several distinctive features inspired geometric insights. particular snapshot ensembles adopt cyclical learning rates cycle length scale epochs beginning training trying large steps weight space. however according analysis curves sufﬁcient relatively small steps weight space diverse networks employ cyclical learning rates small cycle length scale epochs last stage training. illustrated figure step sizes made saving models scale preactivation-resnet- cifar-. snapshot ensembles model distance snapshots scale also piecewise linear cyclical learning rate schedule following smith topin opposed cosine schedule cifar- primary focus ensemble experiments. however also include imagenet results proposed procedure using resnet- architecture. used pretrained model top- test error initialize procedure. epochs cycle length epochs learning rates top- test error-rate ﬁnal ensemble thus epochs could improve accuracy model using fge. ﬁnal ensemble contains models despite harder setting epochs construct ensemble performs comparably best result reported huang imagenet error also achieved using resnet-. shown optima deep neural networks connected simple pathways polygonal chain single bend near constant accuracy. introduced training procedure pathways user-speciﬁc curve choice. inspired insights propose practical ensembling approach fast geometric ensembling achieves state-of-the-art results cifar- cifar- imagenet. many exciting future directions research. high level shown even though loss surfaces deep neural networks complex relatively simple structure connecting different optima. indeed move towards thinking valleys loss rather isolated modes. figure ensemble performance function training time using resnet- cifar- crosses represent performance separate snapshot models diamonds show performance ensembles constructed models available given time. tenfold cifar- number training examples held constant. thus smart ensembling strategies especially important dataset. indeed experiments cifar- outperformed methods. cifar- consistently improved upon budgets architectures. also improved training budgets similar performance cifar- using resnets. figure illustrates results preactivation-resnet- cifar- training budgets. snapshot ensembles cyclical learning rate beginning training gather models ensemble throughout training. good initialization standard independent training ﬁrst epochs applying described section supplement. case whole ensemble gathered last epochs runs. epochs able gather diverse enough networks outperform snapshot ensembles valleys could inspire directions approximate bayesian inference stochastic mcmc approaches could jump along bridges modes rather getting stuck exploring single mode. could similarly derive proposal distributions variational inference exploiting ﬂatness pathways. geometric insights could also used accelerate convergence stability accuracy optimization procedures like helping understand trajectories along optimizer moves making possible develop procedures search structured spaces high accuracy. could also paths construct methods robust adversarial attacks using arbitrary collection diverse models described high accuracy curve returning predictions different model query adversary. also property create better visualizations multilayer loss surfaces. indeed using proposed training procedure able produce types visualizations showing connectivity modes normally depicted isolated. also could continue build training procedure proposed here curves particularly desirable properties diversity networks. indeed could start entirely loss functions line surface integrals cross-entropy across structured regions weight space. references abadi mart´ın agarwal ashish barham paul brevdo eugene chen zhifeng citro craig corrado greg davis andy dean jeffrey devin matthieu tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. dinh laurent pascanu razvan bengio samy bengio yoshua. sharp minima generalize deep nets. precup doina whye proceedings international conference machine learning volume proceedings machine learning research international convention centre sydney australia pmlr. http//proceedings.mlr. press/v/dinhb.html. topology ingeometry half-rectiﬁed network optimization. ternational conference learning representations https//openreview.net/forum? id=bkfwvcgx. goodfellow vinyals oriol saxe andrew qualitatively characterizing neural network optimization problems. international conference learning representations kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition choromanska anna henaff mikael mathieu michael arous g´erard lecun yann. loss surfaces artiﬁcial intelligence multilayer networks. statistics ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning dauphin yann pascanu razvan gulcehre caglar kyunghyun ganguli surya bengio yoshua. identifying attacking saddle point problem highdimensional non-convex optimization. advances jonsson hannes mills greg jacobsen karsten nudged elastic band method ﬁnding minimum energy paths transitions. classical quantum dynamics condensed phase simulations keskar nitish shirish mudigere dheevatsa nocedal jorge smelyanskiy mikhail tang ping pelarge-batch training deep learning ter. internageneralization sharp minima. tional conference learning representations https//openreview.net/references/ pdf?id=hoyrlygg. russakovsky olga deng krause jonathan satheesh sanjeev sean huang zhiheng karpathy andrej khosla aditya bernstein michael imagenet large scale visual recognition challenge. international journal computer vision appendix organized follows. section provides formulas polygonal chain bezier curve bends. section discusses computational complexity proposed curve ﬁnding method. section provides details results additional experiments curve ﬁnding. section provides visualizations train loss test accuracy surfaces. section provides details experiments fge. finally section discusses trivial forward pass proposed method consists steps computing point passing minibatch data corresponding point. similarly backward pass consists ﬁrst computing gradient loss respect multiplying result jacobian second step forward pass ﬁrst step backward pass exactly forward backward pass training single model. additional computational complexity procedure compared single model training comes ﬁrst step forward pass second step backward pass general depends parametrization curve. experiments curve parametrizations speciﬁc form. general formula curve bend given thus additional computational complexity backward pass also need multiply gradient respect constant. thus total additional computational complexity practice observe time-complexity table summarizes results curve ﬁnding experiments datasets architectures. models report properties train loss error train test datasets. metrics report values maximum values metric along curve numerical approximation resents train loss error train test dataset minimum value error curve. represents mean uniform distribution curve train loss coincides loss paper. equally-spaced grid points estimate values int. trapezoidal rule estimate integral. dataset architecture report performance single models used endpoints curve single performance line segment connecting single networks segment performance quadratic bezier curve bezier performance polygonal chain bend polychain. finally curve report ratio length length line segment connecting modes. besides convolutional fully-connected architectures also apply approach architecture next word prediction task dataset base model used implementation available https//www.tensorflow.org/ strategy resnet routine summarized alg. epoch usual training epochs. total training time thus epochs. wide resnet models pre-training procedure epochs initialize fge. epochs starting checkpoints corresponding epochs ensemble gathered models. total training time thus epochs. cycle length epochs means total number models ﬁnal ensemble resnet wide resnet epochs total number models ﬁnal ensemble wide resnets resnets. convolutional networks relu activations withbatch normalization construct path connecting points weight space accuracy point curve least good minimum accuracies endpoints. unlike paths found procedure paths trivial merely exploit redundancies second part completely analogous. weights network bi}≤i≤n weights biases i-th layer total number layers. throughout derivation consider inputs network ﬁxed. output i-th layer wirelu corresponds ﬁrst layer corresponds logits construct figure l-regularized cross-entropy train loss test error surfaces deep residual network cifar-. left three optima independently trained networks. middle right quadratic bezier curve polygonal chain bend connecting lower optima left panel along path near-constant loss. notice panel direct linear path mode would incur high loss. bi}≤i≤n following way. biti. easy logits network weights equal tnon note predicted labels corresponding logits same accuracy networks corresponding same.", "year": 2018}