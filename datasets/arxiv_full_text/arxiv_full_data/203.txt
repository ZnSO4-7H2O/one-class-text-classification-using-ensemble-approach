{"title": "Zero-bias autoencoders and the benefits of co-adapting features", "tag": ["stat.ML", "cs.CV", "cs.LG", "cs.NE"], "abstract": "Regularized training of an autoencoder typically results in hidden unit biases that take on large negative values. We show that negative biases are a natural result of using a hidden layer whose responsibility is to both represent the input data and act as a selection mechanism that ensures sparsity of the representation. We then show that negative biases impede the learning of data distributions whose intrinsic dimensionality is high. We also propose a new activation function that decouples the two roles of the hidden layer and that allows us to learn representations on data with very high intrinsic dimensionality, where standard autoencoders typically fail. Since the decoupled activation function acts like an implicit regularizer, the model can be trained by minimizing the reconstruction error of training data, without requiring any additional regularization.", "text": "regularized training autoencoder typically results hidden unit biases take large negative values. show negative biases natural result using hidden layer whose responsibility represent input data selection mechanism ensures sparsity representation. show negative biases impede learning data distributions whose intrinsic dimensionality high. also propose activation function decouples roles hidden layer allows learn representations data high intrinsic dimensionality standard autoencoders typically fail. since decoupled activation function acts like implicit regularizer model trained minimizing reconstruction error training data without requiring additional regularization. autoencoders popular models used learning features pretraining deep networks. simplest form based minimizing squared error observation non-linear reconstruction deﬁned schemes used prevent trivial solutions using large number hidden units. include corrupting inputs learning vincent adding contraction penalty forces derivatives hidden unit activations small rifai using sparsity penalties coates work motivated empirical observation across wide range applications hidden biases tend take large negative values training autoencoder mentioned regularization schemes. figure left filters learned sigmoid contractive autoencoder rifai relu denoising autoencoder vincent cifar- patches resulting histograms learned hidden unit biases. right classication accuracy permutation invariant cifar- data using multiple different inference schemes. plots paper best viewed color. consequence fact hidden units autoencoder dual function selecting weight vectors take part reconstructing given training point representing coefﬁcients selected weight vectors combined reconstruct input overcome detrimental effects negative biases propose activation function allows disentangle roles. show yields features increasingly outperform regularized autoencoders recognition tasks increasingly high dimensionality. since regularization built activation function allows train autoencoder without additional regularization like contraction denoising simply minimizing reconstruction error. also show using encoding without negative biases test-time model contractive autoencoder achieves state-of-the-art performance permutation-invariant cifar- dataset. analysis help explain network linear hidden units optimal number units tends relatively small frey makhzani frey training thresholding introduce section loosely related dropout hinton forces features align high-density regions. contrast dropout thresholding scheme stochastic. hidden activations reconstructions deterministic function input. related work work goroshin lecun introduce variety activation functions training autoencoders argue shrinking non-linearities small activations zero. contrast work show possible train autoencoders without additional regularization using right type shrinkage function. work also loosely related martens discuss limitations rbms binary observations. work motivated observation regularized training common autoencoder models tends yield hidden unit biases negative. figure shows experimental demonstration effect using whitened -cifar- color patches krizhevsky hinton negative biases sparse hidden units also shown important obtaining good features hinton negative biases arguably important training autoencoders especially overcomplete ones help constrain capacity localize features. several undesirable consequences encoding shall discuss. consider effect negative bias hidden unit one-sided activation functions relu sigmoid contrast-normalized data like selection function zeroing activities points whose inner product weight vector small. result region hypersphere activates hidden unit spherical whose size determined size weight vector bias. activations deﬁned spherical caps model effectively deﬁnes radial basis function network hypersphere. long regions weight vectors become active overlap equivalent clustering. contrast clustering regions course overlap autoencoder. however show following basis autoencoder relu hidden units negative biases even active regions merge model resemble clustering learn point attractor represent region. words model able multiple hidden units collaborate deﬁne multidimensional region constant density. consider points reconstructed perfectly autoencoder. points viewed mode true data generating density true manifold autoencoder ﬁnd. input deﬁne active hidden units yield positive response denote weight matrix restricted active units. contains columns weight vectors associated active hidden units data point ﬁxed point condition relu autoencoder written inhomogeneous linear equations whose solutions given speciﬁc solution null-space given eigenvectors corplus null-space responding unit eigenvalues number unit eigenvalues equal number orthonormal weight vectors would enforce although minimizing reconstruction error without bias orthogonality hidden units active together learning ﬁxed non-zero amounts minimizing reconstruction error shifted projection orthonormal solution longer optimal account non-zero translation wsb). case sigmoid activation function harder analyze sigmoid never exactly zero notion active cannot used. characterize manifold learned sigmoid autoencoder analysis suggests even though negative biases required achieve sparsity detrimental effect make difﬁcult learn non-trivial manifold. observation sparsity detrimental already discussed example ranzato kavukcuoglu authors give sparsity test-time show improves recognition performance. similarly coates saxe showed good classiﬁcation performance achieved using linear classiﬁer applied features using relu activation without bias. also showed classiﬁcation scheme robust wrt. choice learning method used obtaining features figure conﬁrm ﬁnding show still true features represent whole cifar- images ﬁgure shows classiﬁcation performance standard contractive autoencoder sigmoid hidden units trained permutation-invariant cifar- training dataset using linear classiﬁer applied hidden activations. shows much better classiﬁcation performance achieved replacing sigmoid activations used training zero-bias relu activation test-time light preceding analysis hidden units promote sparsity learning becoming active small region input space hidden unit active linear afﬁne encoding. furthermore sparsity-promoting process removed test time. satisfy criteria suggest separating selection function sparsiﬁes hiddens encoding deﬁnes representation linear. deﬁne autoencoder reconstruction product selection function linear representation selection function negative bias achieve sparsity active hidden unit uses linear activation deﬁne coefﬁcients reconstruction. activation function reminiscent spike-and-slab models deﬁne probability distributions hidden variables product binary spike variable real-valued code. case product come probabilistic interpretation serves deﬁne deterministic activation function supports linear encoding. activation function differentiable almost everywhere back-propagate learning. activation function also related adaptive dropout however differentiable thus cannot trained back-prop. coates so-called triangle activation used instead relu inference method k-means. amounts setting activations mean activation zero almost identical zero-bias relu since mean linear preactivation close zero average. shown figure unlike relu non-differentiability activation function also non-continuity. common relu activations train stochastic gradient descent ignore non-differentiability optimization. refer activation function truncated rectiﬁed following. experiments unlikely optimal found work well often with better than traditional regularized autoencoders like denoising contractive autoencoder. truncation contrast negativebias relu also viewed hard-thresholding operator inversion fairly well-understood boche note trec activation function simply peculiar activation function training. training amounts minimizing squared error without kind regularization. drop thresholding testing simply rectiﬁed linear response. performs linear reconstruction preactivation either large negative active region subspace rather convex cone. contrast rectiﬁed version refer activation function thresholded linear below also known hardthresholding literature rozell figure illustration. trec tlin activation functions allow hidden units linear rather afﬁne encoding. shall refer autoencoders activation functions zero-bias autoencoder following. overcomplete representations orthonormality longer hold. however weight vectors span data space form frame analysis weights exist exact reconstruction written vectors general identical related matrix multiplication ˜wk. matrix known frame operator frame {wk}k given weight vectors ˜wk}k dual frame associated frame operator identity case minimizing reconstruction error make frames {wk}k ˜wk}k approximately duals another approximately hold. interestingly autoencoder tied weights minimizing reconstruction error would frame approximate parseval chose cifar- dataset study ability various models learn high dimensional input data. contains color images size pixels assigned different classes. number samples training testing consider permutation invariant recognition task method unaware spatial structure input. evaluated several models along ours namely contractive autoencoder standout autoencoder k-means. evaluation based classiﬁcation performance. input data size contrast normalized dimensionally reduced using whitening retaining variance. also evaluated second method dimensionality reduction using without whitening whitening mean normalizing variances i.e. dividing dimension square-root eigenvalues projection. number features model models trained stochastic gradient descent. experiments section chose learning rate initial training epochs increased ensure scaling issues initializing dealt outset help avoid blow-ups training. model trained epochs total ﬁxed momentum inference rectiﬁed linear units without bias models. classify resulting representation using logistic regression weight decay classiﬁcation weight cost parameter estimated using cross-validation subset training samples size threshold parameter ﬁxed trec tlin autoencoder. tried regularization strengths .−.; latter uncontraction. case standout results reported plots figure learned ﬁlters shown figure plots figure observable results line discussions earlier sections. note particular trec tlin autoencoders perform well even hidden units. number hidden units increases performance models tend tile input space tends improve. second experiment evaluate impact different input sizes ﬁxed number features. experiment training data given image patches size cropped center training image cifar- dataset. yields patch size training samples test samples. different patch sizes evaluated well original image size number features preprocessing classiﬁcation procedure used report performance. results shown figure using preprocessed input data directly classiﬁcation performance increased increasing patch size would expect. figure shows smaller patch sizes models perform equally well. performance tlin based model improves monotonically patch size increased. model’s performances suffer patch size gets large. among these model using trec activation suffers least expected. figure features different models trained cifar- data. whitening preprocessing. bottom whitening preprocessing. denotes regularization strength. figure classiﬁcation accuracy permutation invariant cifar- data function number features. whitening without whitening used preproceesing. bottom classiﬁcation accuracy cifar- data features features function input patch size. whitening used preprocessing. also experimented initializing neural network features trained models. single hidden layer relu units input hidden weights initialized features trained models hidden output weights logistic regression models hyperparameter search yielding optimal threshold along supervised tuning helps increase best performance case trec observed case performance went slightly down. thus using trec followed supervised ﬁne-tuning dropout regularization yields accuracy regularization strength yields best knowledge results beat current state-of-the-art performance permutation invariant cifar- recognition task trec slightly outperforming cae. cases without whitening used preprocessing. contrast krizhevsky hinton train extra data none models provided knowledge task beyond preprocessed training set. dataset high intrinsic dimensionality videos show transforming random dots used memisevic hinton subsequent work data example vectorized video whose ﬁrst frame random image whose subsequent frames show transformations ﬁrst frame. video represented concatenating vectorized frames large vector. data intrinsic dimensionality least high dimensionality ﬁrst frame. high ﬁrst frame random image. widely assumed bi-linear models memisevic hinton related models able learn useful representations data. interpretation data terms high intrinsic dimensionality suggests simple autoencoder able learn reasonable features long uses linear activation function hidden units span larger regions. found indeed case training rotating random dots proposed memisevic hinton model hiddens trained vectorized -frame random videos size frame. figure depicts ﬁlters learned shows model learns represent structure data developing phase-shifted rotational fourier components discussed context bi-linear models. able learn features distinguishable noise line existing results chose activity recognition perform quantitative evaluation observation. intrinsic dimensionality real world movies probably lower random movies higher still images. used recognition pipeline proposed konda evaluated hollywood dataset marszałek dataset consists training videos test videos classes human actions. models trained pca-whitened input patches size cropped randomly training videos. number training patches number features models. recognition pipeline blocks size patch size cropped super-blocks using stride super block results blocks. concatenation block ﬁlter responses dimensionally reduced performing super block descriptor second layer k-means learns vocabulary spatio-temporal words classiﬁed konda experiments plug features learned different models pipeline. performances models reported figure show trec tlin autoencoders clearly outperform localized models. surprisingly also outperform sophisticated gating models memisevic hinton previous sections discussed importance rectiﬁed linear inference. experimentally show using rectiﬁed linear inference yields best performance among different inference schemes. model ﬁxed number hiddens trained cifar- images evaluate performance quantizing input space tiles proportional quantity data density arguably best represent data given enough training data enough tiles allows approximate function reasonably well using subsequent linear layer. however data high intrinsic dimensionality limited number hidden units choice summarize regions using responses invariant changes input. invariance perspective necessary evil goal itself. increasingly important increasingly high dimensional inputs. selection made hidden units active given data example linear coefﬁcients used reconstruction. similar gating square pooling models memisevic hinton ranzato courville deﬁne reconstruction response hidden unit models deﬁned multiplying ﬁlter response squaring followed non-linearity. reconstruct input output hidden unit multiplied ﬁlter response itself making model bi-linear. result reconstructions deﬁned feature vectors weighted linear coefﬁcients active hiddens. suggest interpreting fact models work well videos high-dimensional data result using linear zero-bias hidden units too. saxe andrew pang chen zhenghao bhand maneesh suresh bipin andrew. random weights unsupervised feature learning. proceedings international conference machine learning taylor graham fergus lecun yann bregler christoph. convolutional learning spatiotemporal features. proceedings european conference computer vision part eccv’ vincent pascal larochelle hugo bengio yoshua manzagol pierre-antoine. extracting composing robust features denoising autoencoders. proceedings international conference machine learning", "year": 2014}