{"title": "Lower bounds over Boolean inputs for deep neural networks with ReLU  gates", "tag": ["cs.CC", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Motivated by the resurgence of neural networks in being able to solve complex learning tasks we undertake a study of high depth networks using ReLU gates which implement the function $x \\mapsto \\max\\{0,x\\}$. We try to understand the role of depth in such neural networks by showing size lowerbounds against such network architectures in parameter regimes hitherto unexplored. In particular we show the following two main results about neural nets computing Boolean functions of input dimension $n$,  1. We use the method of random restrictions to show almost linear, $\\Omega(\\epsilon^{2(1-\\delta)}n^{1-\\delta})$, lower bound for completely weight unrestricted LTF-of-ReLU circuits to match the Andreev function on at least $\\frac{1}{2} +\\epsilon$ fraction of the inputs for $\\epsilon > \\sqrt{2\\frac{\\log^{\\frac {2}{2-\\delta}}(n)}{n}}$ for any $\\delta \\in (0,\\frac 1 2)$  2. We use the method of sign-rank to show exponential in dimension lower bounds for ReLU circuits ending in a LTF gate and of depths upto $O(n^{\\xi})$ with $\\xi < \\frac{1}{8}$ with some restrictions on the weights in the bottom most layer. All other weights in these circuits are kept unrestricted. This in turns also implies the same lowerbounds for LTF circuits with the same architecture and the same weight restrictions on their bottom most layer.  Along the way we also show that there exists a $\\mathbb{R}^ n\\rightarrow \\mathbb{R}$ Sum-of-ReLU-of-ReLU function which Sum-of-ReLU neural nets can never represent no matter how large they are allowed to be.", "text": "motivated resurgence neural networks able solve complex learning tasks undertake study high depth networks using relu gates implement function size lowerbounds network architectures parameter regimes hitherto unexplored. particular show following main results neural nets computing boolean functions input dimension method sign-rank show exponential dimension lower bounds relu circuits ending gate depths upto restrictions weights bottom layer. weights circuits kept unrestricted. turns also implies lowerbounds circuits architecture weight restrictions bottom layer. recent surge activity using neural networks complex artiﬁcial intelligence tasks power neural nets). rekindled interest understanding neural networks complexity theory perspective. myriad hard mathematical questions surfaced attempts rigorously explain power neural networks comprehensive overview found recent three part series articles center brains minds machines .there rich literature investigating complexity function classes represented neural networks various kinds gates many papers canonical example classic paper maass establish complexity results entire class functions represented circuits gates ∗department applied mathematics statistics johns hopkins university email amukhejhu.edu †department applied mathematics statistics johns hopkins university email basu.amitabhjhu.edu come general family. complemented papers study speciﬁc family gates sigmoid gate gate many associated results also found reviews recent circuit complexity results stand signiﬁcant improvements known lower bounds circuit complexity threshold gates. results maass also show general families neural networks converted circuits gates constant factor blow depth polynomial blow size circuits. last years particular family gates called rectiﬁed linear unit gates reported signiﬁcant advantages traditional gates practical applications neural networks. gate real inputs computes following output prior results apply general gates ones also apply relu gates results apply gates compute piecewise polynomial function however witnessed results gates usually make much stronger claims speciﬁc classes gates. best knowledge prior results obtained relu gates perspective boolean complexity theory i.e. study circuits restricted boolean inputs. main focus although aware analysis lower bounds relu circuits applied boolean inputs recent work analysis circuits viewed function know exponential lowerbounds size sum-ofrelu circuits certain easy sum-of-relu-of-relu functions depth size tradeoffs circuits recently also studied recent paper current authors. best knowledge lowerbounds scaling exponentially dimension known analog deep neural networks depths follows depth circuit length longest path output node input variable size circuit total number gates circuit. also notation sum-of-relu refer circuits whose inputs feed single layer relu gates whose outputs combined weighted give ﬁnal output. similarly sum-ofrelu-of-relu denotes circuit depth output node simple weighted intermediate gates relu gates hidden\" layers. analogously deﬁne sum-of-ltf ltf-of-ltf ltf-of-relu ltf-of-ltf-of-ltf ltf-of-relu-of-relu also notation ltf-of-k circuit form ltf-of-relu-of-relu-. .-relu levels relu gates. boolean real inputs. begin study following observation shows relu circuits markedly different behaviour inputs restricted boolean opposed arbitrary real inputs. since gates implemented relu gates follows boolean function implemented relu-of-relu circuit. fact hard show something slightly stronger implemented relu gates thus restrict size circuit sum-of-relu circuits represent pseudo-boolean function. contrast show allows real inputs exist functions inputs cannot represented sum-of-relu circuit matter large. understand strength relu gates vis-a-vis gates. hard circuit gates simulated circuit relu gates constant blow-up size question whether relu gates signiﬁcantly better gates terms depth and/or size. result follows following facts linear function implementable relu gates sum-of-ltf circuit gates gives piecewise constant function takes different values. since takes different values need context preliminary results state main contributions. next result recall deﬁnition andreev function previously many times used prove computational lower bounds particularly inspired recent andreev function kane williams ﬁrst super linear lower bounds approximating using ltf-of-ltf circuits. give almost linear lower bound size ltf-of-relu circuits approximating andreev function restriction weights gate. well known proving lower bounds without restrictions weights much fact recent results ﬁrst challenging even context circuits. superlinear lower bounds circuits restrictions weights. restrictions weights e.g. assuming poly bounds weights certain layers exponential lower bounds established circuits next results ﬂavor certain kinds weight restrictions prove exponential size lower bounds size ltf-of-d− circuits. thing note weight restrictions assumed bottom layer layers gates unbounded weights. nevertheless weight restrictions somewhat unconventional. note arkadev-nikhil function represented size ltf-of-ltf circuit restrictions weights light fact theorem somesurprising shows purpose representing boolean functions deep relu circuit gate exponentially weakened bottom layer weights restricted deﬁnition even integers allowed super-polynomially large. moreover lower bounds also hold circuits arbitrary depth weight restrictions bottom layer. unaware exponential lower bounds circuits arbitrary depth kind weight restrictions. used great effect diverse ﬁelds communication complexity circuit complexity learning theory. explicit matrices high sign-rank known till breakthrough work forster forster showed elegant complexity measure show exponential lowerbounds ltf-of-maj circuits previous literature sign-rank reviewed book satya lokam recently following result obtained arkadev nikhil leading proof strict containment ltf-of-maj ltf-of-ltf. classic method random restrictions\" show lowerbound weight unrestricted ltf-of-relu circuits representing andreev function. basic philosophy method take arbitrary ltf-of-relu circuit supposedly matches andreev function large fraction inputs randomly values input coordinates also ﬁxing coordinates input andreev function. show upon restriction andreev function collapses arbitrary boolean function remaining inputs hand show ltf-of-relu collapses circuit small size high-probability cannot possibly approximate randomly chosen boolean function remaining inputs. contradiction leads lowerbound. important concepts towards implementing idea. first precisely deﬁne relu gate upon partial restriction inputs considered removable circuit. notion clariﬁed automatically turn random restrictions relu random restriction gate recently done secondly needs true ﬁxed size ltf-of-relu circuits cannot represent many boolean functions possible input dimension. speciﬁc case ltf-of-relu circuits relu gates necessarily fan-out theorem applies ltf-of-relu circuits n−bits relu gates represent log) log) number boolean functions. note slightly departing usual convention neural networks work wolfgaang mass allows direct wires input nodes deﬁnition denote arbitrary ltf-of-relu circuits ⌊log equation deﬁnition upperbound given earlier number ltf-of-relu functions ﬁxed circuit size whereby last inequality assumed assumption legitimate want estimate certain large asymptotics. arbitrarily chosen constant −cǫk constant large enough would hence arbitrary member truth-table restrictions input part input sampling restrictions uniformly random restrictions type different instances differ matrix left unﬁxed values entries ltf-of-relu boolean circuit size note relu gate bits upon random restriction becomes redundant linear argument either reduces non-positive deﬁnite function positive deﬁnite function. former case gate computing constant function zero later case computing linear function simply implemented introducing wires connecting inputs directly output gate. thus cases resultant function needs relu gate computed. combining cases note conditions collapse relu gate identical conditions collapse gate linear argument. hence corresponding random restrictions directly utilize random restriction lemma that compare deﬁnitions observe probability least circuit type event equation deﬁnition andreev function follows an|ρ truth table given hence speciﬁes function hence well write ltf-of-relu circuit given weights network inputs threshold function gate real numbers inputs distance largest negative number gate ever gets evaluated. increasing bias last gate quantity less ensure input gate entire circuit still computes boolean function originally. assume without loss generality input threshold function gate never also recall weights bottom layer constrained integers magnitude widths relu layers depths indexed increasing increasing distance input. thus output gate gets inputs; j-th input output circuit depth composed relu gates. pseudo-boolean function implemented thus output overall ltf-of-d− circuit lemma natural numbers. consider circuit inputs single output consisting relu gates depth relu gates depth corresponding layer closest input large enough would have sign-rank upper bound sign-rank bottom layer weight restricted ltf-of-d− widths {wk}d− follows represent arkadev-nikhil function would need max{ha since entries assumed integers bounded terms take different values since arrange rows columns increasing order partition rows columns contiguously claim ﬁxed natural numbers. matrices rows columns partitioned contiguously blocks constant valued within blocks. .+aw matrix whose rows columns partitioned contiguously proof. partition rows contiguous blocks equivalent choice lines lines. matrices reﬁned partition selection lines lines giving contiguous blocks. argument would like thank aurko extensive discussions methods used questions addressed work. also thank nikhil mande piyush srivastava helpful conversations circuit complexity. amitabh basu anirbit mukherjee gratefully acknowledge support grant cmmi.", "year": 2017}