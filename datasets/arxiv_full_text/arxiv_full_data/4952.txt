{"title": "Online learnability of Statistical Relational Learning in anomaly  detection", "tag": ["cs.LG", "cs.AI", "I.2.6"], "abstract": "Statistical Relational Learning (SRL) methods for anomaly detection are introduced via a security-related application. Operational requirements for online learning stability are outlined and compared to mathematical definitions as applied to the learning process of a representative SRL method - Bayesian Logic Programs (BLP). Since a formal proof of online stability appears to be impossible, tentative common sense requirements are formulated and tested by theoretical and experimental analysis of a simple and analytically tractable BLP model. It is found that learning algorithms in initial stages of online learning can lock on unstable false predictors that nevertheless comply with our tentative stability requirements and thus masquerade as bona fide solutions. The very expressiveness of SRL seems to cause significant stability issues in settings with many variables and scarce data. We conclude that reliable anomaly detection with SRL-methods requires monitoring by an overarching framework that may involve a comprehensive context knowledge base or human supervision.", "text": "abstractâ€” statistical relational learning methods anomaly detection introduced security-related application. operational requirements online learning stability outlined compared mathematical definitions applied learning process representative method bayesian logic programs since formal proof online stability appears impossible tentative common sense requirements formulated tested theoretical experimental analysis simple analytically tractable model. found learning algorithms initial stages online learning lock unstable false predictors nevertheless comply tentative stability requirements thus masquerade bona fide solutions. expressiveness seems cause significant stability issues settings many variables scarce data. conclude reliable anomaly detection srl-methods requires monitoring overarching framework involve comprehensive context knowledge base human supervision. real-time abnormal behavior detection important component many security applications today. simple example facility surveillance system wants detect anomalies take appropriate action either prevent mitigate consequences. generally necessary combine information many different sources order detect anomalies; hence fusion necessary component surveillance systems. anomalies interest could range terrorists planning attack pedestrian situations theft employees customers port facility. even though terrorist cases presently attention important forget possibilities detect everyday crime using systems. research described paper motivated applications projects supply chain security .there several different aspects supply chain security perhaps obvious scanning screening cargo bags take place airports. similar scanning done containers ports postal items sorting facilities. ideally cargo/containers scanned economic realities favor risk-based approach risk computing risk indicators abnormal behavior detection available data regarding item. include information sensors sources. instance relevant information containers might include senders receivers goods packaged inside container owner crew ship transporting journey details ship. information highly heterogeneous complicates task fusing analyzing furthermore analysis constitutes threat static change dynamically. hence need methods fusion heterogeneous information anomaly detection adapt changing threat profiles. equally important application anomaly detection supply chain security facility protection surveillance. abnormal behavior detection crowds common mass transport security applications several similar issues arise also facility surveillance. consider port passengers employees large. anomaly detection used detect individuals appear preparing criminal acts. here threat patterns evolving quickly need online learning adapt anomaly detection system changing conditions. full description anomaly detection system scope paper. however assume system consists sensors give information current state world. information fused produce higher-level situation descriptions anomaly detection system. paper consider anomaly detection form classification system learns training including normal anomalous situations. trained system classifies situations either normal anomalous operators alerted detected anomalies. note availability labeled data including normal anomalous events often issue supervised classification approach anomaly detection employed here. simplicity assume type anomaly although practical applications typically include many anomaly types. alternative paradigms considered here include unsupervised learning lowprobability events considered anomalous. clauses constitute proof viewed network predicates nodes inference transforming predicates random variables inference relations statistical dependency relations proof converted bayesian network called support network query. node support network inherits bayesian clause template node. codifies dependent variable parents. specific statistical parameters support network hence instantiated generic parameters blp. summary queries system requests estimates marginal probability distribution random variables. system random variables predicates attempts prove predicates converts proof support network estimates requested marginal probability distribution support network. main challenge learn bayesian clauses data. presently done two-stage loop space relations first explored purpose identifying candidate sets clauses. second step strives find optimal statistical parameters candidate clauses. selecting best bayesian clauses starting point next iteration learning process eventually converges. learning discussed section introduction online learning learning processes including learning algorithm outlined previous section primarily designed batch processing learning process executed using available data. many applications including surveillance scenario natural consider online learning system learns continuously stream incoming data simultaneously must respond queries. disregarding computational efficiency memory issues possible batch learning process accumulated data soon data arrives. however vital online learning stable security-critical systems. single training example could otherwise cause core anomaly detection system assumed based statistical relational learning requirement classification performance continuously learning anomaly detection system stable. paper therefore investigate online stability context application scenario. statistical relational learning extending machine learning take account rich relational structure real-world problems goal emerging science comprehensive overview). many different methods proposed including bayesian logic programs method selected present investigation. methods describe broad class probability density functions using two-tier representation consisting structural part parametric part. structural part usually written language first order logic parametric part expresses statistical significance relational structure. explicit representation typically form bayesian network markov random field generated models response query. explicit representation used compute answer query e.g. probability observed event anomalous. enables potentially wide range powerful applications profuse expressiveness also source stability issues discussed following. consists ground terms logical atoms so-called bayesian clauses represent relations. ground terms represent known objects. logical atoms well-founded facts relations including particular universally quantified relations. ground terms together logical atoms form base-line logical knowledge base bayesian clause describes dependency relation form logical clause associated probabilistic parameters describing statistical significance relation. statistical parameters take form conditional probability table bayesian clauses constitute extended knowledge base kbb. total knowledge base formed merging kbb. consider object e.g. discovered surveillance system. first ground term observed facts added knowledge base. system estimates degree anomalousness object three steps. knowledge base first used construct logical proof predicate anomalous. simplicity assume precisely proof found. ref. describes combination rules used handling multiple proofs. logical proof discussion online learning literature seems focus computational complexity aspects. online learning statistical parameters markov logic networks considered huynh mooney address online learning structural statistical aspects introduce regularization method suppresses creation clauses learning online operative requirements online stability anomaly detection generic critical properties anomaly detection systems false alarm rate pfalse missed alarm rate pmissed. former quantity probability erroneously detecting anomaly normal situation latter quantity probability detecting anomaly anomalous situation. pfalse false positive rate machine learning pmissed false negative rate. anomaly detection system useful pfalse pmissed fall thresholds tfalse tmissed respectively threshold values depend contingent operative requirements. furthermore system able reliable good performance maintenance check-points. assume system runs unattended except regularly scheduled maintenance occasions operator monitors performance system decides relied means online learning algorithm comply performance thresholds least training examples given maximum rate incoming training examples maintenance period. guaranteed stable value operational parameters tfalse tmissed system property universally stable since adding training example system satisfactory performance system. circumstances reasons spelled following universal stability online learning chimera present state art. strive enhancing online stability never reach epitome universal stability. section provides detailed description learning brief review online stability machine learning discussion simplified model sources online instability identified discussed. learning following review learning described goal learning infer consists training examples training example list random variables values assigned variables e.g. {flies colour white father true mother true inductive logic programming first applied create initial clauses data. purpose training example viewed list logical assertions random variables transformed predicates assigned values ignored. training example becomes example assertion {flies colour father mother}. using transformed data optionally background knowledge engine generates universally quantified horn clauses codifying generic relations learnt note clauses typically unique. delivers hence structure initial horn clause becomes bayesian clause unknown statistical parameters. bayesian support networks generated instantiating universally quantified variables ground terms training set. statistical parameters bayesian clauses computed maximizing combined likelihood support networks respect value combined likelihood optimal statistical parameters becomes utility initial blp. refinement operators applied initial candidate blps generated. presently studied implementation refinement operators employed that applied body clause either adds removes universally quantified predicate. viewed horn clauses candidate required consistent viewed candidate scored computing statistical parameters evaluating utility summing combined maximum likelihood support networks. candidate highest utility selected next preferred solution. selected. significant improvement utility achieved learning process deemed converged. note process must constrained acyclic support networks generated maximum likelihood computation must handle combining rules. section intended brief overview learning rests completely ref. consulted details. ross bagnell provide up-to-date review known stability conditions online learnability context general setting learning online version algorithm observes sequence time algorithm data points must select hypothesis received subsequence suffer penalty given loss function applied next data point symbols denote space data hypotheses respectively. problem online stability anomaly detection discussed section strictly problem since several different objectives problem therefore belongs class multiobjective optimization problems transform stability requirements problem must define computationally feasible loss function captures combined operational constraints expressed terms available data. ross bagnell discuss online learnability major classes machine learning algorithms. known method online learnable according definition ross bagnell. even assume algorithm regret hence online learnable would able system comply operational stability requirements sufficiently large without knowing much data actually needed given system situation. assess system online stable specific finite also need estimate convergence rate however known special classes algorithms notably convex loss function. convexity means local optima greedy gradient ascent algorithm guaranteed find global optimum. problems typically feature jagged fitness landscape violates convexity condition. learning process outlined previous section involves greedy gradient search respect structure parameters. since unknown online learnable convergence rate anyway unknown appears presently difficult formal online stability conditions ross bagnell basis testing practical online stability applications. tentative rule thumb online stability great potential advantages using methods tempting ignore lack formal stability assurance embrace cavalier approach according following sketch. since real-valued statistical parameters always vary impact accumulating training samples appears permanence digital clause structure best signature stability. clause structure hence monitored according following guidelines make sure minor refinements structure occurred last modifications solution. reasonable assume small simpifications specializations logical structure occurr system converges bona fide solution. tentative rule thumb amounts accepting solution operational conditions satisfied. note requirement small structural changes used verifying stability. early learning phases include large structural changes system breaks local minima. iii. analysis online stabilty simplified model purpose better understanding online stability similar concepts discuss highly simplified model stability issues tractable generic learning. model also basis numerical experiments test rule thumb previous section. training consists complete samples variables drawn ground truth pdf. interested learning training predict value values variables test sample. prepare learning consider training example values value random variable value dummy random variable index example. bayesian clause fully models ground truth probability anomalousness indicator structural perspective universally quantified logical variable representing training example label describes relevant probability distribution. addition redundant independent binary variables. following called blpt ground truth blp. consider learning process according section initial model good measure taken identical blpt. note learning process outlined section normally would generate multiple initial clauses. purpose creating analytically tractable problem assume initial structure learning algorithm found ideal starting point hence clause used learning process. sufficiently small initial training includes redundant variables accidentally strongly correlated total likelihood training improved applying expanding refinement operator include variables body clause example following sequence step increases combined likelihood training therefore valid learning iteration according section present batch training examples redundant variables body happens code values training accuracy since combination values either maps given value occur training set. applying reductive refinement operator next learning iteration hence produces accurate applying test obviously gives much worse performance blpt thus illustrating learning least situations might reduce performance overfitting statistical fluctuations training sample. together associated example false predictor variables accidentally codes another variable training although variables really independent. section analyze false predictors impact online learning process. facilitate analysis shall simplify model assuming cpts associated bayesian clause consist binary-valued conditional structure single bayesian probabilities clause model represented list body variables define pattern pattern body variable values e.g. x=}. binary bayesian clause variables body hence represented rows consisting pattern associated conditional means probability certainty. total number different cpts given structure size countable model amounts variables decoupled called redundant variables. redundant structure structure consists redundant variables. false predictor bayesian clause consisting redundant structure clause classifies values training accuracy. assumptions model average number false predictors containing variables drawn variables applied training consisting examples binomial coefficient equation corresponds number ways structure variables selected total variables. factor corresponds total number cpts given structure. training example even chance falsifying false predictors thus explaining negative factor exponent. approximating binomial coefficient easy large certain show conditions false predictors hence ubiquitous e.g. note term exponent characteristic rich hypothesis space relational algorithms would absent non-relational machine learning methods. collapse true solution found. general expect sharp shift structural size online learning process exits false predictor phase transits highly specialized false predictor relevant solution. life time selected redundant structure also increase training expands. larger structures correspond larger supply possible cpts require training examples falsify. increased structural life time therefore indicator convergence stable relevant solution. expect average size fairly constant learning process false predictor phase. learning process favors minimal hops distribution false predictors fitness landscape uniform. relax model restriction binary cpts allow real-valued probabilistic parameters. expect general behavior smoother fitness landscape since expanded model less constrained. single training example able falsify false predictor weaken likelihood hypothesis thus extending life span. complex relational structure fundamentally change picture. size life time preferred redundant structures still increase increasing size training set. hence expect behavior false predictors discussed context model qualitatively remain generic online learning similar models. confirm theoretical arguments section performed numerical experiments model described section used generating training samples. apply learning algorithm described section modified constraint binary cpts. loss function inverse likelihood training data according hypothesis. training best fitting hypothesis found. training incremented sample learning process repeated expanded training set. structural size structural life time size measured averaged many online learning histories. table quantifies evolution structural size life time progression training sizes. table presents stability indicators given range training sizes averaged online training histories data drawn ground truth distribution. assume simulated observations accurate uncertainty data. experiment used model redundant variables. initial online learning phase fitness landscape loss function includes many false minima associated false predictor. phase called false predictor phase. online learning process proceeds decimated. life time selected hypothesis short learning process rapidly switch false optima. relevant solutions found almost false predictors falsified. model probability falsified approximately training examples. generic problems general impossible estimate much data required suppress false predictors since prior knowledge true joint probability distribution typically available. model shall endeavor extrapolate generic insights online learning. particular evaluate rule thumb section studying evolution structure online learning. preparation investigation quantities characterize structure life cycle defined. life time structure number training examples online learning point structure selected learning algorithm point discarded replaced different structure. minimum life time one. redundant structure long-lived individual bayesian clauses incorporate structure since structure correspond many bayesian clauses different cpts probability false predictor. structural size measure logical complexity blp. example structural size simply number variables body clause. structural distance models minimum number refinement operations required transform models identical model. bayesian clauses head structural distance number predicates differ clauses. size difference structural distance consecutive solutions. number training examples increases online learning false predictors depleted rate independently structure smaller structures associated fewer cpts hence options sooner. starting structurally simple hypothesis expect average structural size preferred solution increase gradually online learning false predictor phase. example structural size would suddenly table average structural size structural life time preferred solution according learning algorithm described section standard deviation tabulated values indicated data batched range training sizes order make trends visible spite large variance structural life time. note false predictors contribute data shown table learning process terminated soon system finds ground truth hypothesis. equation gives idea many training samples needed suppress false predictors algorithm captures relevant solution. experiments found average number training samples needed exiting false predictor phase standard deviation indicated. experiments also measured size found average size always close unity independently training size. experiments illustrate confirm effects presaged section namely size life time redundant structures increases increasing training size false predictor phase. comparing theoretical experimental results model rule thumb suggested section note evolution solution structure zone false predictors dangerously close satisfying suggested stability requirements. false predictors masquerade relevant solutions since life time redundant structures increases number redundant variables might longer maintenance interval. real-life situation impossible assess case since true joint probability distribution unknown relational structure much complex model. redundant structures imitate expectations valid solutions mainly structures exponentially larger number associated cpts take progressivly longer time falsify. obviously case binary cpts talk number cpts survivablity redundant structures actually expected increase real-valued cpts since parameter space less constrained. masquerading effect hence expected preavail generic models effect fact unique relational models. specialized redundant hypotheses general easier falsify data classical machine learning specialized redundant structures harder falsify. learning algorithm briefly reviewed section comprehensibly described could amended sophisticated search regularization methods although scope improvements severely limited computational efficiency issues. replacing greedy hill climbing search techniques designed rough fitness landscape simulated annealing would reduce risk ending false optimum. note however false predictors typically global optima false predictor phase hence prevail even robust search techniques applied. learning algorithm kreator toolkit includes simple regularization since default setting limits number predicates body bayesian clause three. wide range alternative methods discriminating complex solutions tried including replacing hard cut-off soft weighting also entropy-based methods. however often difficult estimate complexity wanted solution. strict regularization could emasculate expressive power srl. foreseeable modification learning algorithm hence panacea online stability issue. better search methods certain remedy since false predictors lower loss function value valid solutions. regularization favors simple solutions main argument ability model complex real-world relational structures simplicity cannot driven far. presently appears online learning stability guaranteed selecting non-relational machine learning methods convex loss functions form convergence rate known. relational algorithms provable online stable analysis indicates false predictor structures behave bona fide solutions according rules thumb section online learning stability algorithms therefore doubt since real-life scenarios often include large number variables limited supply training examples little priori knowledge ground truth probability distribution. since underlying probability distribution unknown difficult estimate much initial training data needed eradicate false predictors generate relevant hypothesis. life time false predictors unknown difficult know learning process converged hypothesis trusted sufficiently stable according operational helpful advice learning kreator toolkit matthias thimm gratefully acknowledged. research supported securitylink strategic research centre well european commission grant mihalkova r.j. mooney learning disambiguate search queries short sessions proc. european conf. machine learning knowledge discovery databases pp.-. t.n. huynh r.j. mooney online structure learning markov logic networks proc. european conf. machine learning principles practice knowledge discovery databases finthammer thimm towards toolbox relational probabilistic knowledge representation proc. workshop relational approaches knowledge representation learning requirements discussed section problems remain even ideal case algorithm fortuitously supplied correct model example human expert. algorithm continues learn online still lost jagged fitness landscape first requirement ensures online training directed improving operational performance second requirement reduces risk launching live system learning process still thrashing false predictor phase. although appears human monitoring help establish least partial control online stability practical applications often include pressing stability issues. paper tacitly assumed stable underlying joint probability distribution explored growing data set. many applications include probability distributions change multiple initially unknown time scales. concept stochastic modeling also challenged intelligent adversarial opposition. appears good reasons investigating stability online learning conditions relevant real-life applications.", "year": 2017}