{"title": "Robustly Leveraging Prior Knowledge in Text Classification", "tag": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "abstract": "Prior knowledge has been shown very useful to address many natural language processing tasks. Many approaches have been proposed to formalise a variety of knowledge, however, whether the proposed approach is robust or sensitive to the knowledge supplied to the model has rarely been discussed. In this paper, we propose three regularization terms on top of generalized expectation criteria, and conduct extensive experiments to justify the robustness of the proposed methods. Experimental results demonstrate that our proposed methods obtain remarkable improvements and are much more robust than baselines.", "text": "prior knowledge shown useful address many natural language processing tasks. many approaches proposed formalise variety knowledge however whether proposed approach robust sensitive knowledge supplied model rarely discussed. paper propose three regularization terms generalized expectation criteria conduct extensive experiments justify robustness proposed methods. experimental results demonstrate proposed methods obtain remarkable improvements much robust baselines. posses wealth prior knowledge many natural language processing tasks. example text categorization know words player basketball strong indicators sports category words like terrible boring messing indicate negative polarity words like perfect exciting moving suggest positive polarity sentiment classiﬁcation. problem arisen here leverage knowledge guide learning process interesting problem machine learning communities. previous studies addressing problem fall several lines. first leverage prior knowledge label data second encode prior knowledge prior parameters commonly seen many bayesian approaches third formalise prior knowledge additional variables dependencies last prior knowledge control distributions latent output variables makes output variables easily interpretable. however crucial problem rarely addressed bias prior knowledge supply learning model. would model robust sensitive prior knowledge? kind knowledge appropriate task? let’s example baseball unfamiliar hockey provide number feature words baseball much less hockey baseball-hockey classiﬁcation task. prior knowledge mislead model heavy bias baseball. model cannot handle situation appropriately performance undesirable. paper investigate problem framework generalized expectation criteria study aims reveal factors reducing sensibility prior knowledge therefore make model robust practical. introduce auxiliary regularization terms prior knowledge formalized distribution output variables. recall example mentioned though enough knowledge provide features class hockey easy provide neutral words namely words strong generalized expectation criteria generalized expectation criteria provides natural directly constrain model preferred direction. example know proportion class dataset classiﬁcation task guide model predict pre-speciﬁed class distribution. parameter estimation objective function term expresses preferences value constraint functions model’s expectation. given constraint function conditional model distribution empirical distribution input samples score function term expressed follows learning labeled features druck proposed ge-fl learn labeled features using generalized expectation criteria. given labeled features reference distribution classes features denoted ge-fl introduces divergence reference distribution model predicted distribution term objective function speciﬁcally explore three regularization terms address problem regularization term associated neutral features; maximum entropy class distribution regularization term; divergence reference predicted class distribution. ﬁrst manner simply common features neutral features assume neutral features distributed uniformly class labels. second third assume knowledge class distribution detailed soon later. explore three regularization terms make model robust regularization term associated neutral features maximum entropy class distribution regularization term divergence reference predicted class distribution. rest paper structured follows section brieﬂy describe generalized expectation criteria present proposed regularization terms. section conduct extensive experiments justify proposed methods. survey related work section summarize work section address robustness problem gefl method leverages labeled features prior knowledge. labeled feature strong indicator speciﬁc class manually provided classiﬁer. example words like amazing exciting labeled features class positive sentiment classiﬁcation. predicted class distribution given control inﬂuence term overall objective function tune according difference number labeled features class. paper simply proportional total number labeled features β|k|. maximum entropy term derived setting constraint function therefore epθ] model distribution expectation empirical distribution simply average input samples namely takes maximum entropy form derive objective function above. sometimes already much knowledge corpus estimate class distribution roughly without labeling instances. therefore introduce divergence predicted reference class distributions objective function. similarly β|k|. divergence term derived setting constraint function setting distributions. note regularization term involves reference class distribution discussed later. section ﬁrst justify approach exists unbalance number labeled features class distribution. then test inﬂuence conduct experiments method incorporates divergence regularization terms ge-fl reduces heavy load instance annotation performs well provide prior knowledge bias. experiments observe comparable numbers labeled features class supplied. mentioned before often case able provide enough knowledge classes. baseball-hockey classiﬁcation task shown before ge-fl predict instances baseball. section show three terms make model robust. neutral features features informative indicator classes instance word player baseball-hockey classiﬁcation task. features usually frequent words across categories. preference distribution neutral features uniform distributed neutral features prevent model biasing class dominate number labeled features. note need manual annotation provide neutral features. simple take common features neutral features. experimental results show strategy works successfully. data preparation evaluate methods several commonly used datasets whose themes range sentiment webpage science medical healthcare. bag-of-words feature remove stopwords preprocess stage. though labels documents learning process instead label features. movie dataset task classify movie reviews positive negtive used testing proposed approaches unbalanced labeled features unbalanced datasets different parameters. unbalanced datasets constructed based movie dataset randomly removing documents positive class. experiment conduct -fold cross validation. described ways obtain labeled features. ﬁrst information gain. ﬁrst calculate mutual information features according labels documents select labeled features class feature pool. note using information gain requires document label simulate human provide prior knowledge model. second select features. selection process ﬁrst train dataset select probable features topic probability word given topic tj). similar estimate reference distribution labeled features using heuristic strategy. classes total classes associated feature probability feature related classes also experimented datasets observed simneutral features frequent words after removing stop words reference distributions uniformly distributed. frequent words neutral features experiments. incorporating neutral features performs similarly maximum entropy since assume neutral words uniformly distributed. accuracy decreases slowly number labeled features becomes larger suggesting model gradually biases class labeled features like ge-fl. incorporating divergence class distribution performs much better ge-fl balanced unbalanced datasets. shows effective control unbalance labeled features dataset. also compare baseline labeled features balanced. similar experiment above labeled features obtained information gain. settings experimented with figure performance unbalanced labeled features tested movie dataset. randomly select feature pool labeled features class select feature other. unbalanced datasets constructed randomly removing positive documents. figure performance balanced labeled features tested movie dataset. randomly select feature pool labeled features class. unbalanced datasets constructed randomly removing positive documents. results shown figure dataset balanced little difference between ge-fl methods. reason proposed regularization terms provide additional knowledge model bias labeled features. unbalanced dataset incorporating divergence much better ge-fl since provide additional knowledge maximum entropy neutral features much worse forcing model approach uniform distribumethods also evaluated datasets different unbalanced class distributions. manually construct several movie datasets class distributions randomly removing positive documents. original balanced movie dataset used control group. test balanced unbalanced labeled features. balanced case randomly select features feature pool class unbalanced case select features class feature other. results shown figure inﬂuence unbalanced class distribution tested movie dataset. provide labeled features class class three unbalanced datasets constructed randomly removing documents positive class. figure inﬂuence tested movie dataset. randomly select feature pool labeled features class keep feature other. unbalanced datasets constructed randomly removing documents positive class. figure shows dataset labeled features balanced little difference methods ge-fl). class distribution becomes unbalanced difference becomes remarkable. performance neutral features maximum entropy decrease signiﬁcantly incorporating divergence increases remarkably. suggests accurate knowledge class distribution divergence guide model right direction. present inﬂuence method incorporates divergence section. since simply β|k| tune here. note newly introduced regularization term disappeared thus model actually ge-fl. again test method different settings compare methods ge-fl datasets section. instead using features obtained information gain select labeled features. unlike information gain employ instance labels labeled features. setting build classiﬁcation models without instance annotation labeled features. table shows three methods signiﬁcantly outperform ge-fl. incorporating neutral features performs better ge-fl datasets maximum entropy better datasets divergence better datasets. selects predictive features labeled features without considering balance among classes. ge-fl exert control issue performance severely suffered. methods introduce auxiliary regularization terms control bias problem thus promote model signiﬁcantly. much work incorporate prior knowledge learning related lines surveyed here. prior knowledge label unlabeled instances apply standard learning algorithm. constrain model directly prior knowledge. manually labeled features highly predictive unsupervised clustering assignments label unlabeled data. chang proposed constraint driven learning. ﬁrst used constraints learned model annotate unlabeled instances updated model newly labeled data. daum´e proposed self training method several models trained dataset unlabeled instances satisfy cross task knowledge constraints used self training process. macallum proposed generalized expectation criteria formalised knowledge constraint terms expectation model objective function.grac¸a proposed posterior regularization framework projects model’s posterior onto distributions satisfy auxiliary constraints. druck explored constraints labeled features framework forcing model’s predicted feature distribution approach reference distribution. andrzejewski proposed framework general domain knowledge easily incorporated lda. altendorf explored monotonicity constraints improve accuracy learning sparse data. chen tried learn comprekl divergence performs much better unbalanced corpora methods. reason divergence utilizes reference class distribution doesn’t make assumptions. fact suggests additional knowledge beneﬁt model. however divergence term requires providing true class distribution. sometimes exact knowledge true distribution sometimes not. fortunately model insensitive true distribution therefore rough estimation true distribution sufﬁcient. experiments true class distribution reference class distribution .//. accuracy ././. respectively. provides possibility perform simple computing corpus obtain distribution reality. distribution roughly domain expertise. mann mccallum incorporated labeled features also knowledge like class distribution objective function ge-fl. discussed semisupervised perspective investigate robustness problem unlike addressed paper. also active learning methods trying prior knowledge. raghavan proposed feedback instances features interlacedly demonstrated feedback features boosts model much. druck proposed active learning method solicits labels features rather instances used ge-fl train model. paper investigates problem leverage prior knowledge robustly learning models. propose three regularization terms generalized expectation criteria. demonstrated experimental results performance considerably improved taking account factors. comparative results show proposed methods effective works robustly baselines. best knowledge ﬁrst work address robustness problem leveraging knowledge inspire research. present detailed discussions three regularization methods. incorporating neutral features simplest regularization doesn’t require modiﬁcation ge-fl ﬁnding common features. figure shows using neutral features strong enough handle extremely unbalanced labeled features. method doesn’t need extra knowledge thus suitable know nothing corpus. method assumes categories uniformly distributed case practice degraded performance assumption violated figure aria haghighi klein. prototype-driven learning sequence models. proceedings main conference human language technology conference north american chapter association computational linguistics pages association computational linguistics. gideon mann andrew mccallum. simple robust scalable semisupervised learning expectation regularization. proceedings international conference machine learning pages acm. gideon mann andrew mccallum. generalized expectation criteria semi-supervised learning weakly labeled data. journal machine learning research hema raghavan james allan. interactive algorithm asking incorporating feature feedback support vector maproceedings annual internachines. tional sigir conference research development information retrieval pages acm.", "year": 2015}