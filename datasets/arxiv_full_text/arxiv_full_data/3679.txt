{"title": "Variational Inference with Normalizing Flows", "tag": ["stat.ML", "cs.AI", "cs.LG", "stat.CO", "stat.ME"], "abstract": "The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.", "text": "choice approximate posterior distribution core problems variational inference. applications variational inference employ simple families posterior approximations order allow efﬁcient inference focusing mean-ﬁeld simple structured approximations. restriction signiﬁcant impact quality inferences made using variational methods. introduce approach specifying ﬂexible arbitrarily complex scalable approximate posterior distributions. approximations distributions constructed normalizing whereby simple initial density transformed complex applying sequence invertible transformations desired level complexity attained. view normalizing ﬂows develop categories ﬁnite inﬁnitesimal ﬂows provide uniﬁed view approaches constructing rich posterior approximations. demonstrate theoretical advantages posteriors better match true posterior combined scalability amortized variational approaches provides clear improvement performance applicability variational inference. great deal renewed interest variational inference means scaling probabilistic modeling increasingly complex problems increasingly larger data sets. variational inference lies core large-scale topic models text provides state-of-the-art semi-supervised classiﬁcation drives models currently produce realistic generative models images default tool understanding many physical chemical systems. despite successes ongoing advances number disadvantages variational methods limit power hamper wider adoption default method statistical inference. limitations choice posterior approximation address paper. variational inference requires intractable posterior distributions approximated class known probability distributions search best approximation true posterior. class approximations used often limited e.g. mean-ﬁeld approximations implying solution ever able resemble true posterior distribution. widely raised objection variational methods unlike inferential methods mcmc even asymptotic regime unable recover true posterior distribution. much evidence richer faithful posterior approximations result better performance. example compared sigmoid belief networks make mean-ﬁeld approximations deep auto-regressive networks posterior approximation autoregressive dependency structure provides clear improvement performance also large body evidence describes detrimental effect limited posterior approximations. turner sahani provide exposition commonly experienced problems. ﬁrst widely-observed problem under-estimation variance posterior distribution result poor predictions unreliable decisions based chosen posterior approximation. second limited capacity posterior approximation also result biases estimates model parameters number proposals rich posterior approximations explored typically based structured meanﬁeld approximations incorporate basic form dependency within approximate posterior. another potentially powerful alternative would specify approximate posterior mixture model developed jaakkola jordan jordan gershman mixture approach limits potential scalability variational inference since requires evaluation log-likelihood gradients mixture component parameter update typically computationally expensive. paper presents approach specifying approximate posterior distributions variational inference. begin reviewing current best practice inference general directed graphical models based amortized variational inference efﬁcient monte carlo gradient estimation section make following contributions propose speciﬁcation approximate posterior distributions using normalizing ﬂows tool constructing complex distributions transforming probability density series invertible mappings inference normalizing ﬂows provides tighter modiﬁed variational lower bound additional terms terms linear time complexity show normalizing ﬂows admit inﬁnitesimal ﬂows allow specify class posterior approximations asymptotic regime able recover true posterior distribution overcoming oft-quoted limitation variational inference. present uniﬁed view related approaches improved posterior approximation application special types normalizing ﬂows show experimentally general normalizing ﬂows systematically outperforms competing approaches posterior approximation. perform inference sufﬁcient reason using marginal likelihood probabilistic model requires marginalization missing latent variables model. integration typically intractable instead optimize lower bound marginal likelihood. consider general probabilistic model observations latent variables must integrate model parameters introduce approximate posterior distribution latent variables follow variational principle obtain bound marginal likelihood used jensen’s inequality obtain ﬁnal equation likelihood function prior latent variables. easily extend formulation posterior inference parameters focus inference latent variables only. bound often referred negative free energy evidence lower bound consists terms ﬁrst divergence approximate posterior prior distribution second reconstruction error. bound provides uniﬁed objective function optimization parameters model variational approximation respectively. current best practice variational inference performs optimization using mini-batches stochastic gradient descent allows variational inference scaled problems large data sets. problems must addressed successfully variational approach efﬁcient computation derivatives expected loglikelihood ∇φeqφ] choosing richest computationally-feasible approximate posterior distribution second problem focus paper. address ﬁrst problem make tools monte carlo gradient estimation inference networks used together refer amortized variational inference. bulk research variational inference years ways compute gradient expected log-likelihood ∇φeqφ]. whereas would previously resorted local variational methods general always compute expectations using monte carlo approximations forms aptly named doublystochastic estimation since source stochasticity minibatch second monte carlo approximation expectation. focus models continuous latent variables approach take computes required gradients using non-centered reparameterization expectation combined monte carlo approximation referred stochastic backpropagation approach also referred stochastic gradient variational bayes afﬁne variational inference reparameterization. reparameterize latent variable terms known base distribution differentiable transformation example gaussian distribution location-scale conditioned also parameterized deep neural network model class general includes models factor analysis non-linear factor analysis non-linear gaussian belief networks special cases dlgms continuous latent variables model class perfectly suited fast amortized variational inference using lower bound stochastic backpropagation. end-to-end system dlgm inference network viewed encoder-decoder architecture perspective taken kingma welling present combination model inference strategy variational auto-encoder. inference networks used kingma welling rezende simple diagonal diagonal-plus-low rank gaussian distributions. true posterior distribution complex assumption allows deﬁning multimodal constrained posterior approximations scalable manner remains signiﬁcant open problem variational inference. examining bound optimal variational distribution allows idklp] i.e. matches true posterior distribution. possibility obviously realizable given typically used distributions independent gaussians mean-ﬁeld approximations. indeed limitation variational methodology available choices approximating families even asymptotic regime obtain true posterior. thus ideal family variational distributions highly ﬂexible preferably ﬂexible enough contain true posterior solution. path towards ideal based principle normalizing ﬂows normalizing describes transformation probability density sequence invertible mappings. repeatedly applying rule change variables initial density ‘ﬂows’ sequence invertible mappings. sequence obtain valid probability distribution hence type referred normalizing ﬂow. basic rule transformation densities considers invertible smooth mapping inverse composition i.e. mapping transform random variable distribution resulting random variable backpropagation monte carlo. parameters differentiate w.r.t. variational distribution using monte carlo approximation draws base distribution number general purpose approaches based monte carlo control variate estimators exist alternative stochastic backpropagation allow gradient computation latent variables continuous discrete important advantage stochastic backpropagation that models continuous latent variables lowest variance among competing estimators. second important practice approximate posterior distribution represented using recognition model inference network inference network model learns inverse observations latent variables. using inference network avoid need compute data point variational parameters instead compute global variational parameters valid inference training test time. allows amortize cost inference generalizing posterior estimates latent variables parameters inference network. simplest inference models diagonal gaussian densities diag)) mean function standard-deviation function speciﬁed using deep neural networks. paper study deep latent gaussian models general class deep directed graphical models consist hierarchy layers gaussian latent variables layer layer latent variables dependent layer non-linear dlgms non-linear dependency speciﬁed deep neural networks. joint probability model partial differential equation describing initial density evolves ‘time’ describes continuous-time dynamics. langevin flow. important family ﬂows given langevin stochastic differential equation wiener process eξj)] δijδ) drift vector diffusion matrix. transform random variable initial density langevin rules transformation densities given fokker-planck equation density transformed samples time evolve machine learning often langevin −∇zl unnormalised log-density model. importantly case stationary solution given boltzmann distribution e−l. start initial density evolve samples langevin resulting points distributed according i.e. true posterior. approach explored sampling complex densities welling suykens hamiltonian flow. hamiltonian monte carlo also described terms normalizing augmented space dynamics resulting hamiltonian ωmω; also widely used machine learning e.g. neal hamiltonian make connection recently introduced hamiltonian variational approach salimans section allow scalable inference using ﬁnite normalizing ﬂows must specify class invertible transformations used efﬁcient mechanism computing determinant jacobian. straightforward build invertible parametric functions equation e.g. invertible neural networks approaches typically complexity computing jacobian determinant scales dimension hidden layers number hidden layers used. furthermore computing gradients jacobian determinant involves several additional operations also last equality seen applying chain rule property jacobians invertible functions. construct arbitrarily complex densities composing several simple maps successively applying density obtained successively transforming random variable distribution chain transformations equation used throughout paper shorthand composition fk)). path traversed random variables initial distribution called path formed successive distributions normalizing ﬂow. property transformations often referred unconscious statistician expectations w.r.t. transformed density computed without explicitly knowing expectation written expectation require computation logdetjacobian terms depend understand effect invertible ﬂows sequence expansions contractions initial density. expansion pulls points away region reducing density region increasing density outside region. conversely contraction pushes points towards interior region increasing density interior reducing density outside. formalism normalizing ﬂows gives systematic specifying approximate posterior distributions required variational inference. appropriate choice transformations initially simple factorized distributions independent gaussian apply normalizing ﬂows different lengths obtain increasingly complex multi-modal distributions. deﬁned transformation modiﬁes initial density applying series contractions expansions direction perpendicular hyperplane wz+b hence refer maps planar ﬂows. alternative consider family transformations modify initial density around reference point transformation family parameters ir}. family also allows linear-time computation determinant. applies radial contractions expansions around reference point thus referred radial ﬂows. show effect expansions contractions uniform gaussian initial density using ﬂows ﬁgure visualization shows transform spherical gaussian distribution bimodal distribution applying successive transformations. figure inference generative models. left inference network maps observations parameters ﬂow; right generative model receives posterior samples inference network training time. round containers represent layers stochastic variables whereas square containers represent deterministic layers. normalizing ﬂows free energy bound used variational optimization scheme including generalized variational amortized variational inference construct inference model using deep neural network build mapping observations parameters initial density well parameters work able form single computational graph allows easy computation gradients parameters inference network generative model. estimated gradients used conjunction preconditioned stochastic gradient-based optimization methods rmsprop adagrad parameter updates form diagonal preconditioning matrix adaptively scales gradients faster minimization. algorithmic complexity jointly sampling computing log-det-jacobian terms inference model scales number deterministic layers used data parameters average hidden layer size ﬂow-length dimension latent variables. thus overall algorithm quadratic making overall approach competitive large-scale systems used practice. using framework normalizing ﬂows provide uniﬁed view recent proposals designing ﬂexible posterior approximations. outset distinguish types mechanisms differ jacobian handled. work paper considers general normalizing ﬂows presents method linearcontrast volumetime computation jacobian. preserving ﬂows design jacobiandeterminant equal still allowing rich posterior distributions. categories allow ﬂows ﬁnite inﬁnitesimal. non-linear independent components estimation developed dinh instance ﬁnite volume-preserving ﬂow. transformations used neural networks easy compute inverses form neural network parameters form results jacobian zero upper triangular part resulting determinant order build transformation capable mixing components initial random variable ﬂows must alternate between different partitionings resulting density using forward inverse transformations given compare nice general transformation approach described section dinh assume partitioning form enhance mixing components introduce mechanisms mixing components separating disjoint subgroups ﬁrst mechanism applies random permutation second applies random orthogonal transformation hamiltonian variational approximation developed salimans instance inﬁnitesimal volume-preserving ﬂow. consider posterior approximations make additional auxiliary variables latent variables independent auxiliary variables using change variables rule resulting distribution |j|qq using transformation salimans obtain volume-preserving invertible transformation exploiting transition operators mcmc literature particular methods langevin hybrid monte carlo. extremely elegant approach since know number iterations transition function tends inﬁnity distribution tend true distribution alternative make hamiltonian inﬁnitesimal described section disadvantage using langevin hamiltonian require evaluations likelihood gradients iteration training test time. throughout section evaluate effect using normalizing ﬂow-based posterior approximations inference deep latent gaussian models training performed following monte carlo estimate gradient annealed version free energy respect model parameters variational parameters using stochastic backpropoagation. monte random orthogonal transformations generated sampling matrix independent unit-gaussian entries performing qr-factorization. resulting q-matrix random orthogonal matrix deep neural networks form conditional probability random variables consist deterministic layers hidden units using maxout nonlinearity windows variables brieﬂy maxout non-linearity windowsize takes input vector computes maxoutk maxi∈{∆k∆} d/∆. mini-batches data points rmsprop optimization results collected parameter updates. experiment repeated times different random seeds report averaged scores standard errors. true marginal likelihood estimated importance sampling using samples inference network provide insight representative power density approximations based normalizing ﬂows parameterize unnormalized densities exp] listed table ﬁgure show true distribution four cases figure approximating four non-gaussian distributions. images represent densities energy function table range true posterior; approx posterior using normalizing approx posterior using nice summary results comparing kl-divergences between true approximated densities ﬁrst cases. show distributions characteristics multi-modality periodicity cannot captured typically-used posterior approximations. figure shows performance normalizing approximations densities using lengths transformations. non-linearity tanh equation used mapping initial distribution diagonal gaussian substantial improvement approximation quality increase length. figure shows approximation using volumepreserving transformation used nice number transformations. show summary statistics planar nice random orthogonal matrices random permutation matrices found nice planar achieve asymptotic performance grow ﬂow-length planar requires fewer parameters. presumably parameters learned contrast nice requires extra mechanism mixing components learned randomly initialized. observe substantial difference using random orthogonal matrices random permutation matrices nice. performance dlgm using normalizing approximation compared volume-preserving approaches using nice exactly model different ﬂow-lengths summarize performance ﬁgure graph shows increase ﬂow-length systematically improves bound shown ﬁgure reduces kl-divergence approximate posterior true posterior distribution also shows approach using general normalizing ﬂows outperforms nice. also show wider comparison table results included hamiltonian variational approach well model speciﬁcation different thus gives indication attainable performance approach data set. cifar- natural images dataset consists training test images size pixels extract random patches. color levels converted range used similar dlgms used mnist experiment latent variables. since data non-binary logit-normal observation likelihood summarize results table able show increase length systematically improves test log-likelihoods resulting better posterior approximations. work developed simple approach learning highly non-gaussian posterior densities learning transformations simple densities complex ones normalizing ﬂow. combined amortized approach variational inference using inference networks efﬁcient monte carlo gradient estimation able show clear improvements simple approximations different problems. using view normalizing ﬂows able provide uniﬁed perspective closely related methods ﬂexible posterior estimation points wide spectrum approaches designing powerful posterior approximations different statistical computational tradeoffs. important conclusion discussion section exist classes normalizing ﬂows allow create extremely rich posterior approximations variational inference. normalizing ﬂows able show asymptotic regime space solutions rich enough contain true posterior distribution. combine local convergence consistency results maximum likelihood parameter estimation certain classes latent variables models able overcome objections using variational inference competitive default approach statistical inference. making statements rigorous important line future research. normalizing ﬂows allow control complexity posterior run-time simply increasing length sequence. approach presented considered normalizing ﬂows based simple transformations form many maps used alternative transforms designed posterior approximations require constraints e.g. restricted support. important avenue future research lies describing classes transformations allow different characteristics posterior still allow efﬁcient linear-time computation. ackowledgements thank charles blundell theophane weber daan wierstra helpful discussions. gregor danihelka mnih blundell wierstra deep autoregressive networks. icml graves alex jimenez rezende danilo wierstra daan. draw recurrent neural network image generation. icml turner sahani problems variational expectation maximisation time-series models. barber cemgil chiappa bayesian time series models chapter cambridge university press functions form always invertible depending non-linearity parameters chosen. using tanh sufﬁcient condition invertible seen splitting vector perpendicular vector parallel substituting gives sufﬁcient condition invertible w.r.t r.h.s non-decreasing function. corresponds condition since sufﬁces enforce constraint taking arbitrary vector modifying component parallel producing vector modiﬁed vector compactly written", "year": 2015}