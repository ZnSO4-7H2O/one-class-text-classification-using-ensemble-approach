{"title": "Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA", "tag": ["cs.NE", "cs.AI", "cs.LG", "I.2.6; I.2.8; G.1.6"], "abstract": "An automated technique has recently been proposed to transfer learning in the hierarchical Bayesian optimization algorithm (hBOA) based on distance-based statistics. The technique enables practitioners to improve hBOA efficiency by collecting statistics from probabilistic models obtained in previous hBOA runs and using the obtained statistics to bias future hBOA runs on similar problems. The purpose of this paper is threefold: (1) test the technique on several classes of NP-complete problems, including MAXSAT, spin glasses and minimum vertex cover; (2) demonstrate that the technique is effective even when previous runs were done on problems of different size; (3) provide empirical evidence that combining transfer learning with other efficiency enhancement techniques can often yield nearly multiplicative speedups.", "text": "automated technique recently proposed transfer learning hierarchical bayesian optimization algorithm based distance-based statistics. technique enables practitioners improve hboa eﬃciency collecting statistics probabilistic models obtained previous hboa runs using obtained statistics bias future hboa runs similar problems. purpose paper threefold test technique several classes np-complete problems including maxsat spin glasses minimum vertex cover; demonstrate technique eﬀective even previous runs done problems diﬀerent size; provide empirical evidence combining transfer learning eﬃciency enhancement techniques often yield nearly multiplicative speedups. transfer learning inductive transfer learning experience estimation distribution algorithms hierarchical bayesian optimization algorithm decomposable problems eﬃciency enhancement. missouri estimation distribution algorithms laboratory department mathematics computer science university missouri–st. louis university blvd. louis e-mail medalmedal-lab.org http//medal-lab.org/ automated technique recently proposed transfer learning hierarchical bayesian optimization algorithm based distance-based statistics. technique enables practitioners improve hboa eﬃciency collecting statistics probabilistic models obtained previous hboa runs using obtained statistics bias future hboa runs similar problems. purpose paper threefold test technique several classes np-complete problems including maxsat spin glasses minimum vertex cover; demonstrate technique eﬀective even previous runs done problems diﬀerent size; provide empirical evidence combining transfer learning eﬃciency enhancement techniques often yield nearly multiplicative speedups. keywords transfer learning inductive transfer learning experience estimation distribution algorithms hierarchical bayesian optimization algorithm decomposable problems eﬃciency enhancement. estimation distribution algorithms guide search optimum building sampling probabilistic models candidate solutions. probabilistic models edas provides basis incorporating prior knowledge problem learning previous runs order solve problem instances similar type increased speed accuracy reliability however much prior work area based hand-crafted constraints probabilistic models diﬃcult design even detrimental eﬃciency scalability recently pelikan hauschild proposed automated technique capable learning previous runs hierarchical bayesian optimization algorithm order improve eﬃciency future hboa runs problems similar type. basic idea approach design distance metric problem variables correlates expected strength dependencies variables collect statistics hboa models respect values distance metric collected statistics bias model building hboa solving future problem instances similar type. distance metric strongly related problem solved aforementioned study described rather general metric applied practically problem objective function represented additively decomposable function. however prior study evaluated proposed technique classes problems demonstrate several features technique. purpose paper threefold demonstrate technique ref. classes challenging optimization problems demonstrate ability technique learn problem instances size order introduce bias instances another size demonstrate potential beneﬁts combining technique eﬃciency enhancement techniques sporadic model building test problems paper considers several classes np-complete additively decomposable problems including maxsat three-dimensional ising spin glass minimum vertex cover. results together results published prior work provide strong evidence broad applicability great potential technique learning experience edas. paper organized follows. section outlines hboa. section discusses eﬃciency enhancement estimation distribution algorithms using inductive transfer main focus hboa distance-based bias section presents discusses experimental results. section summarizes concludes paper. hierarchical bayesian optimization algorithm works population candidate solutions represented ﬁxed-length strings ﬁnite alphabet. paper candidate solutions represented n-bit binary strings. initial population binary strings generated random according uniform distribution candidate solutions. iteration starts selecting promising solutions current population; binary tournament selection without replacement used. next hboa learns bayesian network local structures selected solutions generates candidate solutions sampling distribution encoded built network. maintain useful diversity population candidate solutions incorporated original population using restricted tournament selection terminated termination criteria met. paper terminated either global optimum found maximum number iterations reached. hboa represents probabilistic models candidate solutions bayesian networks local structures bayesian network deﬁned components acyclic directed graph problem variables specifying direct dependencies variables conditional probabilities specifying probability distribution variable given values variable’s pari= variable parents underlying graph. represent conditional probabilities variable given variable’s parents hboa uses decision trees internal node decision tree speciﬁes variable subtrees node correspond diﬀerent values variable. leaf decision tree particular variable deﬁnes probability distribution variable given condition speciﬁed constraints given path root tree leaf build probabilistic models hboa typically uses greedy algorithm initializes decision tree problem variable single-node tree encodes unconditional probability distribution iteration model building algorithm tests much model would improve splitting leaf decision tree variable already located path leaf. algorithm executes split provides improvement process repeated improvement possible. models evaluated using bayesian-dirichlet metric penalty model complexity estimates goodness bayesian network structure given data background knowledge bayesianhboa edas based complex probabilistic models building accurate probabilistic model crucial success however building complex probabilistic models time consuming require rather large populations solutions much eﬀort enhancing eﬃciency model building edas improving quality models even smaller populations learning experience represents approach addressing issue. basic idea learning experience gather information problem examining previous runs optimization algorithm obtained information bias search problem instances. bias based results learning tasks also commonplace machine learning referred inductive transfer transfer learning since learning model structure often computationally expensive task model building learning experience often focuses identifying regularities model structure using regularities bias structural learning future runs. analyzing probabilistic models built hboa edas straightforward. challenging facet implementing learning experience practice must make sure collected statistics meaningful respect problem solved. make learning experience work ensure pairs variables classiﬁed categories pairs category common expected either correlated independent simultaneously section describes approach deﬁnition distance variables used paper well ref. follows work hauschild given deﬁne distance variables using graph nodes node variable. variables subset create edge nodes denoting number edges along shortest path deﬁne distance variables distance measure makes variables subproblem close other whereas remaining variables distances correspond length chain subproblems relate variables. distance maximal variables completely independent since interactions problem variables encoded mainly subproblems additive problem decomposition distance metric typically correspond closely likelihood dependencies problem variables probabilistic models discovered edas. speciﬁcally variables located closer respect metric likely interact other. observation conﬁrmed numerous experimental studies across number important problem domains spin glasses distributed ﬁnite-dimensional lattice landscapes section describes approach learning experience developed pelikan hauschild inspired mainly work hauschild assume hboa models prior hboa runs similar problems. applying bias based prior runs hboa models ﬁrst processed generate data serve basis introducing bias. processing starts analyzing models determine number splits variable decision tree variable model then values used compute probability split variable distance dependency tree given parts prior probability network structure posterior probability data given pelikan hauschild proposed prior probability distribution introduce bias based distance-based statistics previous hboa runs represented setting denotes number splits variable used tune strength bias normalization constant. since log-likelihood typically used evaluate model quality evaluating contribution particular split change prior probability network structure still done constant time. evolutionary algorithms three-dimensional ising spin glasses considered couplings periodic boundary conditions problem sizes used spins spins unique problem instances minimum vertex cover considered random graphs ﬁxed ratio number edges number nodes ratios problem sizes used unique problem instances combination maxsat considered mapped instances graph coloring graphs created combining regular maximum number iterations problem instance number bits problem; according preliminary experiments upper bound suﬃcient. terminated either global optimum found population consisted copies single candidate solution maximum number iterations reached. problem instance used bisection ensure population size within minimum population size optimum independent runs. bit-ﬂip hill climbing incorporated hboa improve performance test problems except minimum vertex cover; used improve every solution population. minimum vertex cover repair operator based ref. incorporated instead. strength ensure problem instances used deﬁning bias well testing -fold crossvalidation used evaluating eﬀects distance-based bias derived problem instances size. problems problem instances randomly split equally sized subsets. round crossvalidation subset instances left hboa remaining subsets instances. runs subsets produced models analyzed order obtain probabilities bias based obtained values used hboa runs remaining subset instances. procedure repeated subset; overall rounds crossvalidation performed instances. evaluating eﬀects distance-based bias derived problem instances smaller size crossvalidation case runs done diﬀerent problem instances importantly every experiment models used generate statistics hboa bias obtained hboa runs diﬀerent problem instances. experiments performed across variety computer architectures conﬁgurations base case bias case bias always computational node; results runs could therefore compared respect actual time. evaluate hboa performance focus multiplicative speedup respect execution time run; speedup deﬁned multiplicative factor execution time improves distance-based bias compared base case. example executiontime speedup indicates bias allowed hboa optimum using half execution time compared base case without bias. also report percentage runs execution time strictly improved addition speedups achieved various values examine ability distance-based bias based prior runs apply across range problem sizes; done using previous runs instances size bias runs instances another size. since maxsat used instances size facet examined problem classes. using distance-based bias well sporadic model building recording speedups respect base case. ideally would expect speedups sources multiply. time requirements solving maxsat combined eﬀects studied remaining problem classes. settings; remainder discussion focus cases distancebased bias yielded substantial speedups best speedups obtained minimum vertex cover. cases performance least problem instances strictly improved terms execution time; cases improvements observed much greater majority instances. speedups substantial even bias based prior runs problem instances diﬀerent smaller size; fact speedups obtained bias nearly identical speedups bias based instances size. results thus provide clear empirical evidence distance-based bias applicable even problem instances vary size argued main advantages distance-based bias prior work area demonstrated. finally results show nearly multiplicative eﬀect distance-based bias sporadic model building providing support importance distance-based bias; combined speedups ranged paper extended prior work eﬃciency enhancement hierarchical bayesian optimization algorithm using distance-based bias derived prior hboa runs paper demonstrated distance-based bias yields substantial speedups several previously untested classes challenging np-complete problems approach applicable even prior runs executed problem instances diﬀerent size approach yield nearly multiplicative speedups combined eﬃciency enhancement techniques. summary results presented paper together prior work provide clear evidence learning experience using distance-based bias great potential improve eﬃciency hboa particular estimation distribution algorithms general. several topics central importance future work. approach adapted model-directed optimization techniques including edas genetic algorithms linkage learning. approach also modiﬁed introduce bias problems cannot formulated using additive decomposition straightforward manner decomposition practical. finally important study limitations proposed approach create theoretical models automatically tune strength bias predict expected speedups. project sponsored national science foundation grants ecs- iis- univ. missouri–st. louis high performance computing collaboratory sponsored information technology services. experiments performed beowulf cluster maintained univ. missouri louis resources university missouri bioinformatics consortium. opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect views national science foundation.", "year": 2012}