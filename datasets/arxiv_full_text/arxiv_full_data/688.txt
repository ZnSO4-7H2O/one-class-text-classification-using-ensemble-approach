{"title": "The Predictron: End-To-End Learning and Planning", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple \"imagined\" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.", "text": "paper introduce architecture call predictron integrates learning planning end-to-end training procedure. every step model applied internal state produce next state reward discount value estimate. model completely abstract goal facilitate accurate value prediction. example plan effectively game agent must able predict score. model makes accurate predictions optimal plan respect model also optimal underlying game even model uses different state space action space rewards even time-step require trajectories abstract model produce scores consistent trajectories real environment. achieved training predictron end-to-end make value estimates accurate possible. ideal model could generalise many different prediction tasks rather overﬁtting single task; could learn rich variety feedback signals single extrinsic reward. therefore train predictron predict host different value functions variety pseudo-reward functions discount factors. pseudo-rewards encode event aspect environment agent care about e.g. staying alive reaching next room. focus upon prediction task estimating value functions environments uncontrolled dynamics. case predictron implemented deep neural network recurrent core. predictron unrolls core multiple steps accumulates rewards overall estimate value. challenges artiﬁcial intelligence learn models effective context planning. document introduce predictron architecture. predictron consists fully abstract model represented markov reward process rolled forward multiple imagined planning steps. forward pass predictron accumulates internal rewards values multiple planning depths. predictron trained end-toend make accumulated values accurately approximate true value function. applied predictron procedurally generated random mazes simulator game pool. predictron yielded signiﬁcantly accurate predictions conventional deep neural network architectures. central idea model-based reinforcement learning decompose problem subproblems learning model environment planning model. model typically represented markov reward process decision process planning component uses model evaluate select among possible strategies. typically achieved rolling forward model construct value function estimates cumulative reward. prior work model trained essentially independently within planner. result model well-matched overall objective agent. prior deep reinforcement learning methods successfully constructed models unroll near pixel-perfect reconstructions correspondence david silver <davidsilvergoogle.com> hado hasselt <hadogoogle.com> matteo hessel <mtthssgoogle.com> schaul <schaulgoogle.com> arthur guez <aguezgoogle.com>. model maps internal state subsequent internal state internal rewards internal discounts γγγ. third value function outputs internal values representing remaining internal return internal state onwards. predictron applied unrolling model multiple planning steps produce internal rewards discounts values. superscripts indicate internal steps model finally internal rewards discounts values combined together accumulator overall estimate value whole predictron input state output viewed value function approximator external targets consider k-step λ-weighted accumulators. k-step predictron rolls internal model forward steps -step predictron return simply ﬁrst value -step preturn r+γγγv. generally kstep predictron return internal return obtained accumulating model steps plus discounted ﬁnal value step λ-predictron combines together many k-step preturns. speciﬁcally computes diagonal weight matrix λλλk internal state accumulator uses weights λλλk aggregate k-step preturns output combined value call λ-preturn mazes simulated pool domain directly pixel inputs. cases predictron signiﬁcantly outperformed model-free algorithms conventional deep network architectures; much robust architectural choices depth. consider environments deﬁned states deﬁned function next state reward discount factor instance represent non-termination probability transition. process stochastic given noise return cumulative discounted reward single trajectory γt+rt+ γt+γt+rt+ vary time-step. consider generalisation setting includes vector-valued rewards diagonal-matrix discounts vector-valued returns deﬁnitions otherwise identical above. bold font notation closely match familiar scalar case; majority paper comfortably understood reading rewards scalars discount factors scalar constant i.e. value function expected return state vector case known general value functions value function consistent environment satisﬁes following bellman equation model-based reinforcement learning approximation environment learned. uncontrolled setting model normally maps state subsequent state additionally outputs rewards discounts γγγ; model stochastic given source noise value function consistent model satisﬁes bellman equation respect model conventionally model-based methods focus ﬁnding value function consistent separately learned model figure k-step predictron architecture. ﬁrst three columns illustrate -step pathways predictron. -step preturn reduces standard model-free value function approximation; preturns imagine additional steps internal model. pathway outputs k-step preturn accumulates discounted rewards along ﬁnal value estimate. practice k-step preturns computed single forward pass. λ-predictron architecture. λ-parameters gate different preturns. output λ-preturn mixture k-step preturns. example recover -step preturn discount factors γγγk λ-parameters λλλk dependent state dependence shown ﬁgure. summary joint parameters state representation model value function updated make k-step preturns similar target parameters λ-accumulator updated learn weights aggregate λpreturn becomes similar target acts gate computation λ-preturn value λλλk truncate λ-preturn layer value λλλk utilise deeper layers based additional steps model ﬁnal weight always individual λλλk weights depend corresponding abstract state differ prediction. enables predictron compute adaptive depth depending internal state learning dynamics network. ﬁrst consider updates optimise joint parameters state representation model value function. begin k-step predictron. update k-step preturn towards target outcome e.g. montecarlo return real environment minimising mean-squared error loss gradient sample loss update parameters e.g. stochastic gradient descent. stochastic models independent samples required unbiased samples gradient figure sample mazes random-maze domain. light blue cells empty darker blue cells contain wall. maze connected top-left bottom-right not. bottom example trajectory pool domain selected maximising prediction predictron pocketing balls. second domain simulation game pool using four balls four pockets. simulator implemented physics engine mujoco generate sequences frames starting random arrangement balls table. goal simultaneously learn predict future events four balls given frames input. events include collision ball collision boundary table entering quadrant located quadrant entering pocket events provides binary pseudo-reward combine different discount factors predict cumulative discounted various time spans. yields total general value functions. example trajectory shown figure domains inputs presented minibatches i.i.d. samples regression targets. additional domain details provided appendix. ﬁrst experiment trained predictron predict trajectories generated simple deterministic policy random mazes random starting positions. figure shows weighted preturns wkgk resultk wkgk example inputs targets. predictions almost perfect—the training error close zero. full prediction composed weighted preturns decompose trajectory piece piece starting start position ﬁrst step often multiple policy steps added planning step. predictron informed sequential build targets—it never sees policy model-based reinforcement learning architectures dyna value functions updated using real imagined trajectories. reﬁnement value estimates based imagined trajectories often referred planning. similar opportunity arises context predictron. rollout predictron generates trajectory abstract space alongside rewards discounts values. furthermore predictron aggregates components multiple value estimates consistency updates require labels samples environment. result applied states associated ‘real’ outcome update value estimates self-consistent other. especially relevant semi-supervised setting consistency updates allow exploit unlabelled inputs. conducted experiments domains. ﬁrst domain consists randomly generated mazes. location either empty contains wall. mazes considered tasks. ﬁrst task input maze random initial position goal predict trajectory generated simple ﬁxed deterministic policy. target vector element cell maze either cell reached policy zero. second random-maze task goal predict cells diagonal maze whether connected bottom-right corner. locations maze considered connected empty reach moving horizontally vertically adjacent empty cells. cases predictions would seem easier could learn simple algorithm form search ﬂood ﬁll; hypothesis internal model learn structure model. case internal rewards discounts learned. non- case corresponds vanilla hidden-tohidden neural network module internal rewards discounts ignored ﬁxing values second dimension whether k-step accumulator λ-accumulator used aggregate preturns. λaccumulator used λ-preturn computed described section otherwise intermediate preturns ignored ﬁxing λλλk case overall output predictron maximum-depth preturn third dimension labelled usage weighting deﬁnes loss used update parameters consider options preturn losses either weighted uniformly update preturn weighted according weight determines much used λ-predictron’s overoutput. call latter loss ‘usage weighted’. note architectures without λ-accumulator thus usage weighting implies backpropagating loss ﬁnal preturn variants utilise convolutional core intermediate hidden layers; parameters updated supervised learning root mean squared prediction errors architecture aggregated predictions shown figure corresponds random mazes bottom pool domain. main conclusion learning model improved performance greatly. inclusion weights helped well especially pool. usage weighting improved performance. figure indication planning. sampled mazes start positions shown superimposed bottom. corresponding target vector arranged matrix visual clarity shown top. ensembled predick wkgk gλλλ shown target—the prediction near perfect. weighted preturns wkgk make prediction shown gλλλ. full predicted trajectory built steps starting start position planning trajectory sequence. walking maze resulting trajectories— sequential plans emerged spontaneously. notice also easier trajectory right predicted steps thinking steps used complex trajectories. third experiments compares predictron feedforward recurrent deep learning architectures without skip connections. compare corners cube depicted left figure based three different binary dimensions. next experiments tackle problem predicting connectivity multiple pairs locations random maze problem learning many different value functions simulator game pool. challenging domains examine three binary dimensions differentiate predictron standard deep networks. compare eight predictron variants corresponding corners cube left figure ﬁrst dimension second cube whether predictron deep network internal model output learn intermediate predictions. effective predictron previous section i.e. predictron usage weighting. second dimension whether cores share weights core uses separate weights non-λ variants predictron correspond standard feedforward recurrent neural networks respectively. figure exploring predictron variants. aggregated prediction errors predictions eight predictron variants corresponding cube left random mazes pool line median rmse seeds; shaded regions encompass seeds. full -prediction consistently performed best. third dimension whether include skip connections. equivalent deﬁning model step output change current state deﬁning non-linear function— case relu max. deep network skip connections variant resnet root mean squared prediction errors architecture shown figure -predictrons outperformed corresponding feedforward recurrent baselines random mazes pool. also investigated effect changing depth networks predictron outperformed corresponding feedforward recurrent baselines depths without skip connections. consider predictron semisupervised learning training model combination labelled unlabelled random mazes. semi-supervised learning important common bottleneck applying machine learning real world difﬁculty collecting labelled data whereas often large quantities unlabelled data exist. trained full -predictron alternating standard supervised updates consistency updates obtained stochastically minimizing consistency loss additional unlabelled samples drawn distribution. supervised update apply either consistency updates. figure shows perforprinciple predictron adapt depth ‘think more’ predictions others perhaps depending complexity underlying target. indications figure investigate looking qualitatively different prediction types pool ball collisions rail collisions pocketing balls entering staying quadrants. prediction type consider several different time-spans figure shows distributions depth type prediction. ‘depth’ predictron deﬁned effective number model steps. predictron relies fully ﬁrst value counts steps. instead learns place equal weight rewards ﬁnal value counts steps. concretely depth deﬁned recursively λλλk note even input state prediction separate depth. depth distributions exhibit three properties. first different types predictions used different depths. second depth correlated real-world discount ﬁrst four prediction types. third distributions strongly peaked implies depth differ input even single real-world discount prediction type. control experiment used figure comparing predictron baselines. aggregated prediction errors random mazes pool predictions eight architectures corresponding cube left. line median rmse seeds; shaded regions encompass seeds. full -predictron consistently outperformed conventional deep network architectures without skips without weight sharing. figure semi-supervised learning. prediction errors -predictrons using consistency updates every update labelled data plotted function number labels consumed. learning performance improves consistency updates. test quality predictions pool domain evaluate whether well-suited making decisions. sampled pool position consider different initial conditions likely lead pocketing coloured balls. initial condition apply -predictron obtain predictions ensemble predictions associated pocketing ball discounts select condition maximises sum. roll forward pool simulator number pocketing events. figure shows sampled rollout using predictron pick providing choice angles velocities initial conditions procedure resulted pocketing coloured balls episodes. using procedure equally deep convolutional network resulted pocketing events. results suggest lower loss learned -predictron translated meaningful improvements informing decisions. video rollouts selected predictron available following https//youtu.be/ bealdancq. introduced neural network architecture classiﬁcations branch intermediate hidden layers. important difference respect λ-predictron weights hand-tuned hyper-parameters whereas predictron weights learnt importantly conditional input. another difference loss auxiliary classiﬁcations used speed learning classiﬁcations combined aggregate prediction; output model deepest prediction. graves introduced architecture adaptive computation time discrete decision halt aggregating outputs pondering step. related weights obtains depth different way; notable difference λ-predictron different pondering depths predictions. value iteration networks also learn value functions end-to-end using internal model similar predictron. however vins plan convolutional operations full input state space; whereas predictron plans imagined trajectories abstract state space. allow predictron architecture scale much effectively domains natural two-dimensional encoding state space. notion learning many predictions future relates work predictive state representations general value functions nexting predictions shown useful representations transfer however none considered learning abstract models. schmidhuber discusses learning abstract models maintains separate losses model controller suggests training model unsupervised compactly encode entire history observations predictive coding. predictron’s abstract model instead trained end-to-end obtain accurate values. predictron single differentiable architecture rolls forward internal model estimate external values. internal model given structure semantics traditional reinforcement learning models. unlike approaches model-based reinforcement learning model fully abstract need correspond real environment human understandable fashion long rolled-forward plans accurately predict outcomes true environment. predictron viewed novel network architecture incorporates several separable ideas. first predictron outputs value accumulating rewards series internal planning steps. second forward pass predictron outputs values multiple planning depths. third values combined together also within single forward pass output overall ensemble value. finally different values output predictron encouraged self-consistent other provide additional signal learning. experiments demonstrate differences result accurate predictions value reinforcement learning environments conventional network architectures. focused value prediction tasks uncontrolled environments. however ideas transfer control setting example using predictron qnetwork even intriguing possibility learning internal abstract internal actions rather considered paper. explore ideas future work. schmidhuber juergen. learning think algorithmic information theory novel combinations reinforcement learnarxiv controllers recurrent neural world models. preprint arxiv. sutton integrated architectures learning planning reacting based dynamic programming. machine learning proceedings seventh international workshop sutton richard modayil joseph delp michael degris thomas pilarski patrick white adam precup doina. horde scalable real-time architecture learning knowledge unsupervised sensorimotor interaction. international conference autonomous agents multiagent systems-volume international foundation autonomous agents multiagent systems todorov emanuel erez tassa yuval. mujoco physics engine model-based control. ieee/rsj international conference intelligent robots systems ieee ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing internal covariate shift. arxiv preprint arxiv. mnih badia puigdom`enech mirza graves lillicrap harley silver kavukcuoglu asynchronous methods deep reinforcement learning. international conference machine learning mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg petersen stig beattie charles sadik amir antonoglou ioannis king helen kumaran dharshan wierstra daan legg shane hassabis demis. human-level control deep reinforcement learning. nature modayil joseph white adam sutton richard multitimescale nexting reinforcement learning robot. international conference simulation adaptive behavior springer junhyuk xiaoxiao honglak lewis richard singh satinder. action-conditional video prediction using deep networks atari games. advances neural information processing systems compute values rewards discounts lambdas composed convolution stride channels fully connected hidden layer size rest network architecture described above. training experiments used supervised update described section except semi-supervised experiment used consistency update described section update parameters applying adam optimiser stochastic gradients corresponding loss functions. return normalised dividing standard deviation experiments learning rate parameters adam optimiser used mini-batches samples. investigated effect changing depth networks without skip connections. figure shows skip connections make conventional architectures robust depth predictron outperforms corresponding feedforward recurrent baselines depths without skips. capacity comparisons section present additional experiments comparing predictron conventional deep networks. purposes experiments show conclusions obtained depend precise architecture used show structure network—whether predictron not—is important number parameters. speciﬁcally consider random mazes pool task described main text. described section results paper used encoder preserved size input plans mazes pool. convolution channels therefore abstract states mazes pool. consider different architecture longer convolutions used encoder. mazes still layers stride- convolutions means planes reduce size means abstract states third smaller. pool three stride- convolutions bring well. abstract states equal size experiments. pool approximately two-thirds reduction helps reduce compute needed model. parameters predictron fully connected layers. previously ﬁrst fully connected layer internal values rewards discounts λ-parameters would take ﬂattened abstract state hidden nodes. means number parameters layer mazes architecture state representation two-layer convolutional neural network core based convolutions combines model λ-network single repeatable module γγγk+ λλλk core deterministic duplicated times predictron shared weights. finally value network fully connected neural network computes concretely core consists ﬁrst convolutional layer maps intermediate layer. layer another convolutions compute next abstract state predictron. additionally hidden layer ﬂattened three separate networks fully connected layers each. outputs three networks represent internal rewards discounts lambdas. similar small network also hangs internal states addition core computes values. convolutions ﬁlters stride padding retain size feature maps. feature maps channels. hidden layers within mlps hidden units. figure convolutional layers schematically drawn three channels ﬂattening represented curly brakets arrows represent small multi-layer perceptrons compute values rewards discounts lambdas. allow model steps experiments resulting -layer deep networks—two convolutional layers state representations convolutional layers core steps fully-connected layers values ﬁnal state. layers apply batch normalization followed relu non-linearity value reward networks linear layer whereas discount λ-networks additionally sigmoid non-linearity ensure quantities illustrative maze experiment section smaller network architecture employed model steps convolutional feature maps channels. additionally subnetworks figure comparing depths. comparing -predictron conventional deep networks various depths lighter colours correspond shallower networks. dashed lines correspond networks skip connections. figure shows setting—on mazes pool without shared cores—both. predictrons always performed better deep networks. includes node predictron compared node deep network even though latter approximately times many parameters means number parameters mattered less whether predictron. generate sequences pool domain initial locations balls different colours sampled random. white ball moving initially. velocity norm sampled uniformly initial angle sampled uniformly range initial condition mujoco simulation forward balls stopped moving; sequences last frames rejected generated replacement. frame rendered mujoco image subsequently downsampled bilinear interpolation input since signals described section targets pool experiments different levels sparsity resulting values different scales normalised pseudo returns. normalization procedure consisted dividing targets standard deviation empirically measured across initial sequences. pool. predictron shared core would four layers internal values rewards discounts compared deep network values. change ways. first convolution stride channels ﬁrst fully connected layer outputs. reduces number channels therefore number parameters subsequent fullyconnected layer fourth. second tested three different numbers hidden nodes deep network hidden nodes values exact number parameters -predictron hidden nodes outputs. before deep network fewer parameters kept number ﬁxed across experiments. opens question whether improved performance predictron artifact parameters. tested hypothesis results figure comparing depths. comparing -predictron conventional deep networks different numbers hidden nodes fully connected layers therefore different total numbers parameters. deep networks nodes respectively parameters total. predictrons nodes respectively parameters total. note number parameters node predictrons exactly equal number parameters node deep networks. mazes generated ensuring around locations walls. policy takes observation wall conﬁguration four locations adjacent position maps conﬁgurations action. maze policy stepped steps uniformly random start location. target indicates whether trajectory traversed maze location. generate mazes ﬁrst determine stochastic line search number walls top-left corner connected bottom-right corner approximately mazes. shufﬂe walls uniformly randomly. mazes means locations empty contain walls. googol different", "year": 2016}