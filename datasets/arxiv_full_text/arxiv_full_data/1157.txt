{"title": "Deep Adaptive Network: An Efficient Deep Neural Network with Sparse  Binary Connections", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Deep neural networks are state-of-the-art models for understanding the content of images, video and raw input data. However, implementing a deep neural network in embedded systems is a challenging task, because a typical deep neural network, such as a Deep Belief Network using 128x128 images as input, could exhaust Giga bytes of memory and result in bandwidth and computing bottleneck. To address this challenge, this paper presents a hardware-oriented deep learning algorithm, named as the Deep Adaptive Network, which attempts to exploit the sparsity in the neural connections. The proposed method adaptively reduces the weights associated with negligible features to zero, leading to sparse feedforward network architecture. Furthermore, since the small proportion of important weights are significantly larger than zero, they can be robustly thresholded and represented using single-bit integers (-1 and +1), leading to implementations of deep neural networks with sparse and binary connections. Our experiments showed that, for the application of recognizing MNIST handwritten digits, the features extracted by a two-layer Deep Adaptive Network with about 25% reserved important connections achieved 97.2% classification accuracy, which was almost the same with the standard Deep Belief Network (97.3%). Furthermore, for efficient hardware implementations, the sparse-and-binary-weighted deep neural network could save about 99.3% memory and 99.9% computation units without significant loss of classification accuracy for pattern recognition applications.", "text": "abstract—deep neural networks state-of-the-art models understanding content images video input data. however implementing deep neural network embedded systems challenging task typical deep neural network deep belief network using images input could exhaust giga bytes memory result bandwidth computing bottleneck. address challenge paper presents hardware-oriented deep learning algorithm named deep adaptive network attempts exploit sparsity neural connections. proposed method adaptively reduces weights associated negligible features zero leading sparse feedforward network architecture. furthermore since small proportion important weights significantly larger zero robustly thresholded represented using single-bit integers leading implementations deep neural networks sparse binary connections. experiments showed that application recognizing mnist handwritten digits features extracted two-layer deep adaptive network reserved important connections achieved classification accuracy almost standard deep belief network furthermore efficient hardware implementations sparse-and-binary-weighted deep neural network could save memory computation units without significant loss classification accuracy pattern recognition applications. index terms—efficient deep learning binary quantization sparse connections deep adaptive network deep neural network deep belief network hardware embedded system approach deep belief networks demonstrated remarkable performance feature extraction pattern recognition last years fundamental computations feedforward deep belief network large number floating-point multiplications matrix weight parameters input data implemented using clusters central processing units general purpose graphics processing units powerful computers cloud servers however zhou* tang chongqing engineering laboratory high performance integrated circuits college communications engineering chongqing university chongqing china embedded computer vision cognitive applications memory power budget limited efficient network architecture training algorithm would ideal designed efficient hardware computing requires fix-point computations much less number parameters allowing larger networks implemented using embedded systems cognitive applications. last years researchers invented different digital hardware accelerate feedforward neural computations real-time feature extraction pattern recognition himavathi digital implementations feedforward neural networks using reconfigurable field-programmable gate arrays sanni presented fpga-based implementation deep belief network character recognition using stochastic computation improve efficiency processing power morgan jonghong chen proposed respective digital architectures implementations using application specific integrated circuits recently merolla colleagues company took advantage cutting-edge integrated-circuit technology built large-scale neuromorphic chip known truenorth chip feature extraction pattern recognition applications truenorth chip breakthrough integrates million neurons million synapses single asic; however computer vision researchers indicated chip limitations high-dimensional computer vision pattern recognition applications truenorth chip adopted simple quantization strategy represent weight parameters classic deep belief network twobit integers saved memory footprint resulted notable loss classification accuracy through research custom designed hardware deep neural networks booming know attempts made algorithmic level optimize training learning process network parameters efficient hardware implementation efficiently implement feedforward neural network algorithmic consideration number neural connections equals number weight parameters number multiplication operations. deep belief network fully connected neurons adjacent approximately memory computational complexity relieve memory computational bottleneck exploited sparsity neural connections. adopting usual assumption features extracted deep neural networks negligible classification pattern recognition approach named deep adaptive fig. deep neural networks implemented cognitive applications. deep belief network full-connected adjacent layers whereas deep adaptive network connections zero weights removed leading sparser architecture efficient hardware implementation. layer classic neural network concatenated deep neural networks classification. fig. weight parameters trained images mnist handwritten digits. weight matrix lined column column fashion. values weights sparse concentrated important neural connections leading robust sparse binary representation neural connections. second algorithmic consideration efficient hardware implementation data representation essential tradeoff accuracy cost. digital implementations feedforward neural networks usually fix-point representations. researches that deep neural networks trained limited-precision could suffer significant loss accuracy gupta indicated that cases -bit width necessary training deep neural network achieve convergence adequate accuracy however since sparse weights proposed deep adaptive network naturally separated roughly three groups i.e. close-to-zero weights large positive weights negative weights large absolute values robustly threshold represent weight parameters using single-bit integers without notably influence pattern recognition accuracy. proposed method could potentially reduce memory computational complexity feedforward neural network main contributions follows introduce deep adaptive network method train deep neural network sparse connections incorporating novel mixed-norm weight-decay regularization process. algorithm adaptively reduce negligible weight parameters zero omitted efficient implementations. remained important weight parameters robustly thresholded represented binary integers without significantly loss classification accuracy. binary weights significantly reduce memory complexity replace complicated floating-point multiplications simple logic operations. deep adaptive network extract sparse features designed reduce weight parameters associated unimportant features. negligible features become close constants efficient hardware implementation. rest paper organized follows. section introduces related work. section presents formulation algorithm estimate sparse-binary weights features. section presents experiments recognizing handwritten digits mnist dataset. conclude paper section early attempts incorporating sparsity neural networks inspired researches visual cortex neural activity found sparse learning algorithms sparse deep neural networks ranzato proposed encoder-decoder architecture learn sparse representations recently developed variant deep belief network learn sparse representations input images found selected sparse features properties similar visual area proposed sparse-response based rate-distortion theory attempted encode original data using bits possible generally speaking researches focused exploiting sparsity extracted features however motivated learning efficient architectures deep neural network embedded implementations work focused exploiting sparsity neural connections. binary weights would bring great benefits neural network implementations replacing huge number multiplyaccumulate operations simple accumulations floatingpoint multipliers space power-hungry components digital neural network implementations therefore training deep neural networks binary weights subject recent works. daniel courbariaux trained deep neural network backpropagation expectation backpropagation respectively comparison instead training binary weights method aims learn sparse weights reduce small weights zero. result weight parameters adaptively separated three groups robustly thresholded ternarized zero weights omitted small proportion important weight parameters represented using single-bit integers. computation deep neural networks pattern recognition consists stages i.e. training stage evaluating stage. training algorithms usually approximate parameters offline using high performance computers. evaluating stage multi-layer feedforward neural network implemented based approximated parameters realtime feature extraction. many hardware accelerators recently proposed implement feedforward neural network; represents gaussian distribution. since generative model suppose contains parameters rbm. parameters calculated performing stochastic gradient descent loglikelihood training samples. probability network assigns sample given summing possible hidden vectors efficient embedded implementations propose sparsely weighted variant named adaptive adds extra regularization term shrink weights. introducing adarbm algorithm first define mixed norm matrix mixed matrix norm defined adds vector norms rows matrix minimizing mixed norm could reduce lengths matrix’s rows. worth noting shrinking process apply evenly rows. specifically stochastic gradient descent process shorter rows shrink faster rows larger weights. weights short rows tend shrink zero finite iterations. similarly minimizing mixed norm transposed matrix like practice training algorithms deep neural networks designed extract important features classification. reducing mixed-norm matrices adarbm could shrink weight parameters short rows columns zero therefore could used select important input output features. suppose weight matrix reduced zero output affected input feature hand suppose weight parameters associated reduced zero according given evaluating samples，the probability stay close constant leading negligible output feature higher level abstraction classification. however researches focused training algorithm efficient hardware implementations. section propose training algorithm addresses challenge memory computation bottleneck building feedforward neural network. explain formulation algorithm first briefly introduce original algorithm. state training algorithm deep neural networks widely used feature extraction. constructed stacking multiple layers restricted boltzmann machines using output previous input next higher layers tend encode abstract features typically informative classification tasks. standard consists layers binary units matrix rn×d defined connection weights represents connection visible unit hidden unit parameters bias visible hidden unit respectively. given vector forms hidden units visible units bias energy configuration written since direct connections hidden units hidden units conditioned independent other. similarly visible units conditioned also independent other. units binary hidden layer conditioned visible layer independent bernoulli random variables. binary state hidden unit probability sigmoid function. visible units binary visible units conditioned hidden layer also independent bernoulli random variables. case binary state visible unit probability multiple layers adarbms stacked compose consists multiple hidden layers adjacent layers forming adarbm. similar training process trained layer-bylayer style. adarbm training algorithm repeated several times learn deep hierarchical model. specifically could train bottom adaptive training data. corresponding parameters frozen hidden unit values inferred. inferred values serve input data train next higher layer network model hidden layer representations first-layer adarbm. process repeated yield deep architecture unsupervised model training distribution. deep adaptive network built stacked adaptive rbms designed efficient hardware implementation. reducing using mixed-norm regularization algorithm could build multilayer feedforward neural network sparse connections robustly thresholded efficient binary representation. technically calculates sparse-binary weights three steps calculates real-value sparse parameters according algorithm weight parameters thresholded small value weights small absolute values omitted; remained positive negative weights binarized respectively. challenge implementing using embedded system memory computation bottleneck. first weight parameters exhaust gigabytes memory bandwidth processing units memory. secondly multiplications weight parameters input features time-consuming feedforward neural network causes bottleneck computation. thirdly typical hardware high-precision multiplier usually consists hundreds logic gates made challenge implementing large deep neural networks hardware. addresses challenge learning efficient hardware-oriented network architecture featuring properties neural connections sparse. adaptively reduce weight parameters associated negligible neural connections zero omitted efficient embedded hardware implementation. neural connections represented binary integers. since sparse weights well separated therefore weights robustly thresholded neural connection represented using single-bit. single-bit binary weight parameters could save major proportion memory relieve furthermore feedforward neural networks single-bit representation allows replace complicated high-precision multipliers fast area-efficient computation units relieved computation bottleneck digital implementations. generally objective function log-likelihood term regularization term. derivatives probability regularization term respect parameters expressed ⟨·⟩p expectation distribution unfortunately similar training process equations tractable computations expectations difficult; however could contrastive divergence approximate optimal parameters iterative way. iteration apply contrastive divergence update rule followed step gradient descent regularization term table worth noting that according weight decay process shrinks weight parameters unevenly. hundreds iterations weights short rows columns could reduced values close zero leads sparse weight parameters. fig. empirical distributions weight parameters hidden units two-layer diagram show weight distribution diagram show distribution hidden units. models trained handwritten images randomly selected mnist benchmark. figure illustrates weight parameters features. implemented two-layer deep neural networks experiment select features mnist dataset. networks width since adopts mixed-norm regularization training process weights close zero similar previous sparse variants could used extract sparse features. specifically training phase adarbm uses probability output feature higher levels. since weights short columns weight matrix reduced zero output features associated zero columns become close constant independent input features. experiment subtracting hidden units stacked adaptive rbms became notably sparse summary attempts select sparse features input samples exploiting sparsity weight parameters deep neural network. shown fig. weight parameters become sparser second layer through motivated different reasons norm regularization approach could also used calculate sparse weight parameters training process however experiments mnist dataset show that features extracted l-norm regularization notably lower classification accuracy dan; moreover l-norm weight-decay approach robust single-bit representation. specifically applying thresholding binarization weight parameters calculated l-norm regularization results major drop classification accuracy less hand algorithm robust binary weight representation achieves classification accuracy mnist dataset. illustrated fig. networks built extract features images handwritten digits mnist dataset. layer visible units layers hidden units comprised layers stacked rbms. first-layer rbms visible units hidden units second-layer rbms visible hidden units. mnist handwritten digits images pixels. randomly selected images mnist dataset train simultaneously. experiments implemented sparse binary connected three steps. step feedforward neural network became efficient. distinguish networks give feedforward neural networks different levels precision respective names danb sparse-connected binary-weighted calculated substituting positive negative weights dans respectively. danb sparse-connected binary-weighted whose hidden units adarbm binarized using threshold fig. illustrates danb parameters hidden units learned images mnist dataset. weight parameters shown subgraph weight matrices first-layer second-layer sparse binary. third layer full-connected neural network connected classification. weight matrix classifier classic neural network real-valued. example input image indicates number weights whose absolute values smaller threshold specifically sparsity measurement reaches max. sparsity measurement important trade-off efficiency classification accuracy typical range weight parameters mnist dataset. different classifiers could applied recognizing handwritten digits. since classic neural networks naturally parallel hardware accelerators adopts neural network classifier pattern recognition experiment classified features extracted using fully-connected -by- neural network. classification accuracy neural network randomly selected test images calculated evaluate features extracted dan. training algorithm parameters i.e. parameter controls sparsity weight parameters. first experiment changed parameter examine relation classification accuracy. four types deep neural networks compared i.e. original l-norm weight-decay l-norm weight-decay changed parameter shown fig. similar classification accuracy smaller however notably lower accuracy algorithms. experiment shows show better pattern recognition results original parameter equal smaller reason that suggested hinton might adding small regularization term could reduce overfitting improve classification accuracy deep adaptive network. examined relation ratio reserved weights value typical threshold value changed fig. shows ratio reserved weights drops significantly less minst dataset. worth noting second layer sparser first layer. combining results classification accuracy weight sparsity recommended range parameter mnist dataset. increase flexibility second parameter added formulation designed second group experiments examine parameter series dans different trained observe parameter affected classification accuracy sparsity weights. parameter changed step size shown fig. classification accuracy robust varied figure classification accuracy thresholded sparse dbns norm regularization norm regularization weights dans dbns thresholded proportion weights largest absolute values reserved. base line classification accuracy original high-precision parameters. figure classification accuracy sparse-binary-weighted dbns norm regularization terms. weights dbns thresholded significant weights reserved reserved represented according signs. base line classification accuracy original without threshold operation. sparsity examined relation ratio reserved weights value typical value changed step size. fig. shows first layer adarbm preferable value achieves sparsity. second layer adarbm preferable value leads sparsity weight parameters. since parameter controls trade-off sparsity column sparsity weight matrix examined weights dans different fig. compares row-wise mixed norm ||w||m column-wise mixed norm ||wt||m observations. first ||w||m increases increases whereas ||wt||m drops increases. observation coherent optimization formulation second weight matrix second layer adarbm smaller mixed norm first-layer adarbm. observation indicates second-layer weights sparser first-layer weights coherent results fig. since weight parameters sparse separated roughly three groups robustly threshold weights using small value proportion reserved weights decreases increases. omitting weights smaller efficient implemented almost influence pattern recognition performance. fig. compares classification accuracy classic neural network proceeded different variants deep belief networks. base line shows classification accuracy original -bit highprecision weights. accuracy thresholded sparse weights increases average proportion reserved weights first layer second layer increases fig. compares dans original l-norm weight decay l-norm weight decay number multiplications weight parameters features. achieve highest efficiency digital implementations could binary features well binary weights. fig. shows influence adopting binary features classification accuracy. danb deep adaptive network binary sparse connections features danb thresholded binarized using probability shown fig. classification accuracy robust adopting binary weights features. specifically binary weights reserved classification accuracy danb reaches mnist dataset. worth noting that adopting binary weights features could replace complicated complicated floating-point multipliers usually consists hundreds logic gates merely logic gate leading three four orders magnitude higher time area efficiency digital hardware implementations. bridge hardware algorithmic researches paper demonstrated deep learning algorithm specifically designed digital hardware modeling. proposed deep adaptive network attempt learn deep neural network sparse discrete connections quantized using single-bit integers. impact method specialized hardware implementations deep networks could major removing need memory store huge number weight parameters replace typically implemented using hundreds logic gates fix-point accumulator even simple logic gate neural connection could potentially reduce footprints computing units furthermore simpler computing requirement significantly less parameters latency feedforward neural computing data transaction significantly reduced leading faster larger efficient hardware implementation neural networks. since images handwritten digits relatively sparse algorithms shows classification accuracy ratio reserved rates exceeds however given lower ratio reserved weights e.g. dans shows notably higher accuracy original since weight parameters naturally separated significant positive negative weights dans could represented integers without significant influence classification accuracy. fig. compares robustness binarization operation thresholded dans dbns norm regularization. dans thresholded binary weights represented danb dbns’ weights also thresholded binarized. sparse robust binarization operation. single-bit weights reserved classification accuracy danb whereas classification accuracy drops mnist dataset. danb reaches highest accuracy binary weights reserved less original high-precision all-reserved weights. efficient original terms memory consumption. fig. shows memory used danb original dbn. memory complexity deep neural networks increases number neurons network increases. original dbns generally implemented using high-precision representation previous research however sparse binary weights danb generally improve memory efficiency three orders magnitude. danb also improves computational efficiency replacing complicated floating-point operations simple fix-point fundamental computations feedforward neural network huge vamien mckalin computer chip 'truenorth' designed mimic human brain facebook researcher skeptical tech times august available http//www.techtimes.com/articles///new -ibm-computer-chip-truenorth-is-designed-to-mimic-thehuman-brain-facebook-researcher-is-skeptical.htm draghici sorin. capabilities neural networks using limited precision weights.\" neural networks official journal international neural network society soudry daniel hubara meir. \"expectation backpropagation parameter-free training multilayer neural networks continuous discrete weights.\" advances neural information processing systems chen zhao jia. \"spectral–spatial classification hyperspectral data based deep belief network.\" ieee journal selected topics applied earth observations remote sensing vinyals oriol ravuri. \"comparing multilayer perceptron deep belief network tandem features robust asr.\" ieee international conference acoustics sanni \"fpga implementation deep belief network architecture character recognition using stochastic computation.\" information sciences systems ieee morgan ferguson bolouri. \"costperformance analysis fpga vlsi implementations neural network.\"microelectronics neural networks fuzzy systems proceedings fourth internation jonghong hwang sung. realtime phoneme recognition vlsi using feedforward deep ieee neural international conference acoustics speech signal processing", "year": 2016}