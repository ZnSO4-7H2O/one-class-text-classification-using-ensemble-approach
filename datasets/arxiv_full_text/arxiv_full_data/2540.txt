{"title": "Adaptive Skills, Adaptive Partitions (ASAP)", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We introduce the Adaptive Skills, Adaptive Partitions (ASAP) framework that (1) learns skills (i.e., temporally extended actions or options) as well as (2) where to apply them. We believe that both (1) and (2) are necessary for a truly general skill learning framework, which is a key building block needed to scale up to lifelong learning agents. The ASAP framework can also solve related new tasks simply by adapting where it applies its existing learned skills. We prove that ASAP converges to a local optimum under natural conditions. Finally, our experimental results, which include a RoboCup domain, demonstrate the ability of ASAP to learn where to reuse skills as well as solve multiple tasks with considerably less experience than solving each task from scratch.", "text": "introduce adaptive skills adaptive partitions framework learns skills well apply them. believe necessary truly general skill learning framework building block needed scale lifelong learning agents. asap framework also solve related tasks simply adapting applies existing learned skills. prove asap converges local optimum natural conditions. finally experimental results include robocup domain demonstrate ability asap learn reuse skills well solve multiple tasks considerably less experience solving task scratch. human-decision making involves decomposing task course action. course action typically composed abstract high-level actions execute different timescales decision-maker chooses actions execute solve task. actions need reused different points task. addition actions need used across multiple related tasks. consider example task building city. course action building city involve building foundations laying sewage pipes well building houses shopping malls. action operates multiple timescales certain actions need reused additional units required. addition actions reused neighboring city needs developed reinforcement learning represents actions last multiple timescales temporally extended actions also referred options skills macro-actions shown experimentally precup sutton sutton silver ciosek theoretically teas speed convergence rates planning algorithms. teas seen potentially viable solution making truly scalable. teas become popular many domains including robocup soccer video games robotics here decomposing domains temporally extended courses action generated impressive solutions. refer teas skills. course action deﬁned policy. policy solution markov decision process deﬁned mapping states probability distribution actions. tells agent action perform given agent’s current state. refer inter-skill policy policy tells agent skill execute given current state. truly general skill learning framework must learn skills well automatically compose together determine skill executed framework also determine skills reused different parts state space adapt changes task itself. finally also able correct model misspeciﬁcation model misspeciﬁcation deﬁned unsatisfactory skills inter-skill policy provide sub-optimal solution given task. skill learning framework able correct misspeciﬁed skills inter-skill policy obtain near-optimal solution. number works addressed issues separately shown table however work best knowledge combined elements truly general skill-learning framework. framework entitled ‘adaptive skills adaptive partitions ﬁrst kind incorporate above-mentioned elements single framework shown table solve continuous state mdps. receives input misspeciﬁed model asap framework corrects misspeciﬁcation simultaneously learning near-optimal skill-set inter-skill policy stored bayesian-like manner within asap policy. addition asap automatically composes skills together learns reuse learns skills across multiple tasks. main contributions adaptive skills adaptive partitions algorithm automatically corrects misspeciﬁed model. learns near-optimal skills automatically composes skills together learns inter-skill policy solve given task. learning skills multiple different tasks automatically adapting inter-skill policy skill set. asap determine skills reused state space. theoretical convergence guarantees. background reinforcement learning problem markov decision process deﬁned -tuple state space action space bounded reward function discount factor transition probability function mdp. solution policy function mapping states probability distribution actions. optimal policy determines best actions take maximize expected reward. value function ea∼π policy state optimal expected reward following optimal policy state policy gradient policy gradient methods enjoyed success recent years especially ﬁelds robotics goal learn policy trajectories probability trajectory reward obtained particular trajectory. here state deﬁned timestep trajectory; action timestep; trajectory length. policy general formulation policy gradient parameterized parameters idea update policy parameters using stochastic gradient descent leading update rule policy parameters timestep gradient objective function respect parameters step size. skills skill parameterized temporally extended action power skill incorporates generalization temporal abstraction. skills special case options therefore inherit many useful theoretical properties deﬁnition skill consists two-tuple parameterized intra-skill policy parameters termination probability distribution skill. skill partitions skill deﬁnition performs specialized task sub-region state space. refer sub-regions skill partitions necessary skills specialize learning process. given covering state space effectively deﬁne inter-skill policy determine skill executed. partitions unknown a-priori generated using intersections hyperplane half-spaces. hyperplanes provide natural automatically compose skills together. addition skill executed agent needs select actions skill’s intra-skill policy next utilize intra-skill policy skill construct asap policy deﬁned section deﬁne skill hyperplane. deﬁnition skill hyperplane vector features depend state environment vector hyperplane parameters. skill hyperplane deﬁned work interpret hyperplanes mean intersection skill hyperplane half spaces form sub-regions state space called skill partitions deﬁning skill executed. figure contains example skill hyperplanes skill executed deﬁned intersection positive half-space negative half-space argument applies refer skill interchangeably index skill hyperplanes functions automatically compose skills together creating chainable skills desired bacon precup deﬁne enable derive probability executing skill given state first need able uniquely identify skill. deﬁne binary vector bernoulli random variable number skill hyperplanes. deﬁne skill index k−bk bernoulli random variables note generate skill partitions. principle setup deﬁnes skills practice fewer skills typically used furthermore complexity governed vc-dimension. deﬁne probability executing skill bernoulli likelihood equation made logistic sigmoid function ensure valid probabilities xmβk skill hyperplane temperature parameter. intuition skill skill hyperplane xmβk meaning skill’s partition positive half-space hyperplane. similarly xmβk corresponding negative half-space. using skill example hyperplanes figure would deﬁne bernoulli likelihood executing intra-skill policy deﬁne probability executing skill based deﬁne intra-skill policy skill. gibb’s distribution commonly used function deﬁne policies therefore deﬁne intra-skill policy skill parameterized here temperature feature vector depends current state action deﬁnition probability executing skill intra-skill policy need incorporate distributions policy gradient setting using generalized trajectory. generalized trajectory generalized trajectory necessary derive policy gradient update rules respect parameters shown section typical trajectory usually deﬁned length trajectory. generalized trajectory algorithm emits class timestep denotes skill executed. generalized trajectory deﬁned probability generalized trajectory extension trajectory section pβσθi probability skill executed given state environment time probability executing action time given executing skill generalized trajectory function parameter vectors adaptive skills adaptive partitions framework simultaneously learns near-optimal skills given initially misspeciﬁed model. asap automatically composes skills together allows multi-task setting incorporates environment hyperplane feature set. previously deﬁned important distributions respectively. distributions used collectively deﬁne asap policy presented below. using notion generalized trajectory asap policy learned policy gradient setting. asap policy assume given probability distribution mdps d-dimensional state-action space z-dimensional vector describing mdp. deﬁne matrix column represents skill hyperplane matrix column parameterizes intra-skill policy. using previously deﬁned distributions deﬁne asap policy. deﬁnition given skill hyperplanes skills {ζi|i state space actions hypothesis space mdps asap policy deﬁned powerful description policy resembles bayesian approach policy takes account uncertainty skills executing well actions skill’s intra-skill policy chooses. deﬁne asap objective respect asap policy. asap objective deﬁned policy respect hypothesis space mdps. need deﬁne objective function takes hypothesis space account. since assume provided distribution possible models d-dimensional state-action space incorporate asap objective function asap policy expected return respect asap policy. simplify notation group parameters single parameter vector vec]. deﬁne expected reward generalized trajectories pωrdg reward obtained particular trajectory slight variation original policy gradient objective deﬁned section insert equation asap objective function expected return policy next need derive gradient update rules learn parameters optimal policy asap gradients learn intra-skill policy parameters matrix well hyperplane parameters matrix derive update rule policy gradient framework generalized trajectories. derivation supplementary material. ﬁrst step involves calculating gradient asap objective function yielding asap gradient theorem suppose asap objective function estimate gradient ∇ωρ. able derive refer clear context. turns possible derive term result generalized trajectory. yields gradients theorems respectively. derivations found supplementary material. theorem suppose matrix column parameterizes intra-skill policy. gradient ∇θit corresponding intra-skill temperature parameter φxtat rd×k feature vector current state current action theorem suppose matrix column represents skill hyperplane. gradient corresponding parameters hyperplane ∇βkz hyperplane temperature parameter skill hyperplane corresponds locations binary vector equal corresponds locations binary vector equal using gradient updates order gradients vector update intra-skill policy parameters hyperplane parameters given task note updates occur single time scale. formally stated asap algorithm. present asap algorithm dynamically simultaneously learns skills inter-skill policy automatically composes skills together learning sps. skills initially arbitrary therefore form misspeciﬁed model. line combines skill hyperplane parameters single parameter vector lines learns skill hyperplane parameters line generalized trajectory generated using current asap policy. gradient estimated line trajectory updates parameters line repeated skill hyperplane parameters converged thus correcting misspeciﬁed model. theorem provides convergence guarantee asap local optimum algorithm asap require {state-action feature vector} {skill hyperplane feature vector} {the number hyperplanes} rd×k arbitrary skill matrix} arbitrary skill hyperplane matrix} distribution tasks} theorem convergence asap given asap policy asap objective models well asap gradient update rules. step-size satisﬁes k→∞ηk second derivative policy bounded bounded rewards. experiments performed four different continuous domains rooms domain flipped domain three rooms domain robocup domains include one-on-one scenario striker goalkeeper two-on-one scenario striker goalkeeper defender striker defenders goalkeeper experiment asap provided misspeciﬁed model; skills achieve degenerate sub-optimal performance. asap corrects misspeciﬁed model case learn near-optimal skills sps. experiment implement asap using actor-critic policy gradient learning algorithm two-room flipped room domains domains agent needs reach goal location shortest amount time. agent receives constant negatives rewards upon reaching goal receives large positive reward. wall dividing environment creates rooms. state space -tuple consisting continuous xagent yagent location agent xgoal ygoal location center goal. agent move four cardinal directions. experiment involving room domains single hyperplane learned linear feature vector representation addition skill learned sps. intra-skill policies represented probability distribution actions. automated hyperplane skill learning using asap agent learned intuitive skills seen figure colored region corresponds white arrows figure intersection skill hyperplanes form four partitions deﬁnes skill’s execution region flipped robocup domains learned skills skill partitions flipped across multiple tasks. superimposed onto ﬁgures indicate skills learned since intra-skill policy probability distribution actions skill unable solve entire task own. asap taken account positioned hyperplane accordingly given skill representation solve task. figure shows asap improves upon initial misspeciﬁed partitioning attain near-optimal performance compared executing asap ﬁxed initial misspeciﬁed partitioning ﬁxed approximately optimal partitioning. multiple hyperplanes analyzed asap framework learning multiple hyperplanes room domain. seen figure increasing number hyperplanes impact ﬁnal solution terms average reward. however increase computational complexity algorithm since skills need learned. approximate points convergence marked ﬁgure respectively. addition skills dominate case producing similar partitions seen figure indicating asap learns skills necessary solve task. multitask learning ﬁrst applied asap domain attained near optimal average reward took approximately episodes near-optimal performance resulted skill shown figure using learned skills asap able adapt learn skills solve different task episodes indicating parameters learned task provided good initialization task. knowledge transfer seen figure signiﬁcantly change tasks skills completely relearned. also wanted whether could sps; switch sign hyperplane parameters learned domain whether asap solve flipped domain without additional learning. symmetry domains asap indeed able solve domain attained near-optimal performance exciting result many problems especially navigation tasks possess symmetrical characteristics. insight could dramatically reduce sample complexity problems. three-room domain domain similar domain regarding goal state-space available actions rewards. however case walls dividing state space three rooms. hyperplane feature vector consists single fourier feature. intra-skill policy probability distribution actions. resulting learned hyperplane partitioning skill shown figure using partitioning asap achieved near optimal performance experiment shows insightful unexpected result. reusable skills using hyperplane representation asap able learn intra-skill policies also skill needed reused different parts state space asap therefore shows potential automatically create reusable skills. robocup domain robocup soccer simulation domain soccer ﬁeld opposing teams. utilized three robocup sub-domains mentioned previously. sub-domains striker needs learn dribble ball score goals past goalkeeper. state space domain continuous locations striker xstriker ystriker ball xball yball goalkeeper xgoalkeeper ygoalkeeper constant goal location xgoal ygoal. domain addition defender’s location xdef ender ydef ender state space. domain locations defenders. features domain tested linear degree figure average reward learned asap policy compared approximately optimal skill well initial misspeciﬁed model. learning across multiple tasks without learning ﬂipping hyperplane. average reward learned asap policy varying number hyperplanes. learned skill domain. learned using polynomial hyperplane linear hyperplane representation. learned using polynomial hyperplane representation without defender’s location feature defender’s location location location feature dribbling behavior striker taking defender’s location account. average reward domain. polynomial feature representation hyperplanes. domains also utilized degree polynomial hyperplane feature representation. actions striker three actions move ball move ball dribble towards goal move ball shoot towards goal rewards reward setup consistent logical football strategies small negative rewards shooting outside dribbling inside box. large negative rewards losing possession kicking ball bounds. large positive reward scoring. different optimas since asap attains locally optimal solution sometimes learn different sps. polynomial hyperplane feature representation asap attained different solutions shown figure well figures respectively. achieve near optimal performance compared approximately optimal scoring controller linear feature representation skill figure obtained achieved near-optimal performance outperforming polynomial representation. sensitivity domain additional player added game. expected presence defender affect shape learned sps. asap learns intuitive sps. however shape learned change based pre-deﬁned hyperplane feature vector ψmx. figure shows learned location defender used hyperplane feature. location defender utilized ‘ﬂatter’ learned figure using location defender hyperplane feature causes hyperplane offset shown figure striker learning dribble around defender order score goal seen figure finally taking location defender account results ‘squashed’ shown figure clearly showing sensitivity adaptability asap dynamic factors environment. discussion presented adaptive skills adaptive partitions framework able automatically compose skills together learns near-optimal skill skill partitions simultaneously correct initially misspeciﬁed model. derived gradient update rules skill skill hyperplane parameters incorporated policy gradient framework. possible deﬁnition generalized trajectory. addition asap shown potential learn across multiple tasks well automatically reuse skills. necessary requirements truly general skill learning framework applied lifelong learning problems exciting extension work incorporate deep reinforcement learning framework skills asap policy represented deep networks. mann timothy mannor shie. scaling approximate value iteration options better policies fewer iterations. proceedings international conference machine learning precup doina sutton richard singh satinder. theoretical results reinforcement learning temporally abstract options. machine learning ecml- springer", "year": 2016}