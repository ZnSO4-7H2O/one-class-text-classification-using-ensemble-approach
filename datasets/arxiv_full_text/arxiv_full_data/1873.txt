{"title": "Support Vector Machine Active Learning Algorithms with  Query-by-Committee versus Closest-to-Hyperplane Selection", "tag": ["cs.LG", "cs.CL", "cs.IR", "stat.ML", "H.3.3; I.2.6; I.2.7; I.5.4"], "abstract": "This paper investigates and evaluates support vector machine active learning algorithms for use with imbalanced datasets, which commonly arise in many applications such as information extraction applications. Algorithms based on closest-to-hyperplane selection and query-by-committee selection are combined with methods for addressing imbalance such as positive amplification based on prevalence statistics from initial random samples. Three algorithms (ClosestPA, QBagPA, and QBoostPA) are presented and carefully evaluated on datasets for text classification and relation extraction. The ClosestPA algorithm is shown to consistently outperform the other two in a variety of ways and insights are provided as to why this is the case.", "text": "dealing imbalance al-svm denoted initpa. paper refer algorithm combines closest-tohyperplane selection initpa cost modeling closestpa. query committee another active learning algorithm shown effective number settings practical ways build committees include using boosting bagging imbalanced data situations algorithms also beneﬁt suitably adapted version initpa cost modeling technique. adapted algorithms called qboostpa qbagpa. paper carefully compares closestpa qboostpa qbagpa alleviating training data annotation burden applications three previously mentioned characteristics. experimental results provided multiple applications datasets different levels naturally occurring imbalance might encountered realistic data mining settings. closestpa shown superior choices insights provided case. section explains algorithms detail section contains empirical evaluation algorithms section contains insights closestpa outperforms methods section contains related work section concludes. abstract—this paper investigates evaluates support vector machine active learning algorithms imbalanced datasets commonly arise many applications information extraction applications. algorithms based closestto-hyperplane selection query-by-committee selection combined methods addressing imbalance positive ampliﬁcation based prevalence statistics initial random samples. three algorithms presented carefully evaluated datasets text classiﬁcation relation extraction. closestpa algorithm shown consistently outperform variety ways insights provided case. active learning received interest reducing annotation costs text speech processing applications many applications following three characteristics characteristics suggest al-svm support vector machines). previous work presented al-svm algorithm selects examples closest current model’s hyperplane closest-based algorithm shown need modiﬁcation imbalanced data situations previous work presented method adapting imbalanced data situations context alsvm using asymmetric cost factors model training asymmetric cost model shown effective model based prevalence statistics unbiased initial sample data serves positive ampliﬁcation minority positive examples. method typical many applications positive target examples minority class overwhelmed number negative examples. sometimes types settings referred colloquially needle-inthe-haystack settings. approach based approximating binomial distribution normal distribution. rule thumb used statisticians approximation justiﬁed sample least positive least negative examples. situations sample sizes considered substantial enough meet conditions using normal approximation. sampling distribution proportion assumed approximately normally distributed determine need know values value determined looking standard normal table appropriate z-score corresponding level conﬁdence desired. sampling error amount error willing accept estimating population proportion. population parameter trying estimate don’t know exact value. pages possibilities suggested. past information relevant experience available enables educated estimate estimate used. past information relevant experience available used since never underestimate seen equation product occurs numerator noting product maximized user-deﬁned cost factors trade separating data large margin misclassifying training examples. cost misclassifying positive training example cost misclassifying negative training example. stands positive ampliﬁcation. often allowed default refer nopa. imbalanced datasets prudent actively greater positive examples minority class less positive examples majority class. increasing typically shifts learned hyperplane recall increased precision decreased cost model sets well signiﬁcant beneﬁcial impact measure. morik describe effectively passive learning imbalanced datasets using cost model based distribution negative positive training examples. bloodgood vijay-shanker explain distribution skewed al-svm show integrate asymmetric cost factors al-svm imbalanced datasets estimating small initial sample unbiased. since interested evaluating active learners imbalanced datasets technique integrate asymmetric cost factors active learners evaluate. technique referred initpa technique. order appropriately want proportion positive instances small initial labeled data accurate estimate proportion positive instances entire original pool data. determine sample size required estimate proportion positives ﬁnite population within sampling error desired level conﬁdence using approach described next paragraphs. measure standard metric used evaluating performance systems perform search text classiﬁcation relation extraction systems. measure deﬁned harmonic mean precision recall. precision proportion predicted positives indeed true positives. recall proportion positives predicted system positive. note ﬁrst term right-hand side equation estimate sample size would obtained using equation assumed inﬁnite population thus employ ﬁnite population correction factor. deﬁne uncorrected sample size estimate i.e. then summarizing determined using twostep process. first determine uncorrected sample size estimate using equation determine ﬁnite population-corrected sample size estimate using equation case used specify many initial points labeled. example carrying sample size determination computations aimed dataset shows size enables conﬁdent proportion estimate within true proportion. experiments used initial labeled size commonly used selection strategy used al-svm closest-based example selection strategy previous works select unlabeled examples closest current model’s hyperplane query labels. different theoretical motivations using closest-based selection strategy. intuitive argument examples closest hyperplane ones model unsure therefore knowing labels examples provide greatest beneﬁt. another commonly used active learning selection strategy query committee strategy works using committee models querying unlabeled examples committee disagrees label. common form committees ensemble learning method bagging boosting bagging used resulting active learner called qbag boosting used it’s called qboost. section evaluates algorithms described section various datasets relation extraction text classiﬁcation. applications training data annotation shown major bottleneck development systems successful active learning make signiﬁcant contribution. applications modeled binary classiﬁcation tasks give rise datasets imbalanced sense negative examples positive examples. experiment datasets exhibit range variations along following dimensions total number examples available level class imbalance performance levels achieved passive learning data sparsity solution. datasets svms shown provide top-performing systems using traditional passive learning setup. thus natural explore al-svm datasets. leading state al-svm algorithms never evaluated head-to-head following subsections. results show closestpa active learner performs better active learners terms data efﬁciency achieving target measure terms measure achieved corresponding points along learning curve. performance equally important points along learning curve. beginning models rapidly improving performance less important you’re going stop there. also near active learning simulation available data eventually labeled performance different learners becomes similar. points relevant long performance leveled successful active learning would stopped much earlier. middle performance starting level effective stopping criteria stop active learning performance differences matter most. closestpa shown signiﬁcantly outperform algorithms important points along learning curve. active learning intended reduce amount annotated data required induce top-performing model. evaluate effectiveness active learners deﬁne target performance performance baseline active learner achieve given dataset determined averaging performance points learning curve corresponding last training examples. record smallest number labeled examples required baseline active learner achieve target performance. data utilization ratio number labeled examples required active learner achieve target performance divided number required baseline active learner. metric reﬂects efﬁciently active learner using data similar metrics used previous research addition data utilization metric also report graphs full learning curves order realize potential performance enabled efﬁcient selection queries effective stopping criterion must used thus also report results based performance measurements focused area learning curve expected stop practice. finally paired tests signiﬁcance level perform statistical signiﬁcance tests following subsections. unless otherwise mentioned approaches experiments committee size ﬁve. conduct experiments aimed corpus previously used training protein interaction extraction systems protein interactions aimed examples domain-speciﬁc relations exist demand systems built manual annotation expensive. consistent previous work cast binary classiﬁcation task. pair annotated proteins occurring sentence constitutes instance. proteins labeled interacting it’s positive instance. otherwise it’s negative instance. together instances positive negative. note dataset imbalanced positives constituting instances. baseline active learner. numbers table show many labeled points various learners need achieve target measure. thus lower numbers mean better data efﬁciency. observe qbagpa qboostpa closestpa better data utilization randompa. also note closestpa best data utilization active learners. difference data utilization qboostpa qbagpa statistically signiﬁcant difference between closestpa qboostpa statistically signiﬁcant closestp statistically signiﬁcantly better qbagpa fig. plots learning curves qboostpa qbagpa. plot rest plots subsection show performance averaged folds cross validation. observe difference performance small. late stages qbagpa performs little better largely irrelevant since intention stop querying labels much earlier. fig. fig. plot learning curves closestpa versus qbagpa closestpa versus qboostpa respectively. contrast fig. note ﬁgures difference performance larger. cases closestpa stronger performance especially relevant area region stop practice. fig. fig. plot close-ups regions fig. fig. would reasonable stop querying annotations. dashed vertical line plots stopping detection method indicates stop active learning. observe closestpa outperforms qbc-based approaches important region. text classiﬁcation datasets contain multiple categories. treat binary classiﬁcation tasks using one-versus-the-rest classiﬁcation category averaging results. ﬁrst dataset reuters- distribution modapte split. dataset training documents test documents. keeping past practice experiment largest categories. linear kernel svmlight binary features word occurs training data least three times. dataset used previous al-svm research main contributions past work closestnopa algorithm. table reports performance active learners respect baseline closestnopa active learner. observe closestpa highest performance active learners. performance statistically signiﬁcantly better closestnopa qboostpa. performance statistically signiﬁcantly better qbagpa qbagpa abysmal outlier performance classiﬁcation documents category ‘crude’. might think would help make closestpa statistically signiﬁcantly better qbagpa actually makes harder sample estimate standard deviation matched differences much higher. removing performance ‘crude’ category rerunning statistical signiﬁcance test nine categories indicates closestpa statistically signiﬁcantly better qbagpa. addition statistical signiﬁcance observe closestpa requires practically signiﬁcantly smaller numbers labeled examples obtain target measure algorithms require. second text classiﬁcation dataset ohsumed collection dataset training documents test documents. keeping past practice report results largest categories. linear kernel svmlight binary features word occurs training data least three times. data utilization ratio results ohsumed reported table iii. results reinforce ﬁndings datasets closestpa outperforming algorithms again. following experiments explore potential increasing performance methods increasing committee size. results show increases performance worth extra computational expenses. slower factor equal size committee. applications speed important don’t want annotators wait system deciding examples annotated. thus large committee sizes impractical. query boosting parallelize training different committee members. query bagging parallelization possible resources available. qbc-based approaches shown work slightly better committee size increases diminishing returns committee size increased figure shows performance qbagpa committee size versus committee size representative ohsumed category immunologic diseases fig. shows performance qboostpa committee size versus committee size representative reuters category moneyfx. performance categories similar. figure indicates increasing committee size small improvement qbagpa qboostpa. small gain worth additional computational burden increasing committee size section explores possible reasons closestpa outperforms qbagpa qboostpa al-svm. recall main goals reduce amount labeled data requesting labels useful points points help base learner learn better model. successful current labeled data consist almost exclusively points relevant learning redundant points. veriﬁed empirically al-svm experiments almost every training point stopping area becomes support vector resulting model. consider following scenario. earlier rounds point selected annotation placed labeled data also exists current unlabeled pool point label similar round occur. closestpa used training thus likely selected instead different informative point selected. hurt selecting it’s similar model trained already correctly predicts correct. qbagpa/qboostpa it’s plausible sampled training bags others. since selected means committee disagreed label wasn’t included since similar committee members don’t training likely disagree label though committee members training likely label correctly overall disagreement could still high enough gets selected annotation expense choosing different point. during latter stages simulation ones presented longer true algorithm eventually forced select redundant examples unlabeled pool data exhausted latter stages irrelevant would stopped practice. initiated made practically viable showing committees could formed using boosting/bagging melville mooney decorate form committees decision trees base learners. decorate works generating artiﬁcial examples labeling according distribution labels inversely proportional current ensemble’s predictions. decorate interesting approach consider since helps create committee diversity. however straightforward extend decorate svms would seem require explicitly representing feature vectors generating artiﬁcial examples. goes important appealing aspects using svms ability large number features computationally efﬁcient manner implicit feature representation kernel many applications svms number features manageable ﬁnite number exploring performance al-svm decorate applications possibility future work. mccallum nigam integrate expectation maximization algorithm select queries based combination committee disagreement example density. using svms base learner straightforward algorithm seems require probabilistic method integrate algorithm. none svms base learner compare learners closestbased active learners. al-svms closest-based strategy explored works compare closest-based al-svm approach compare approaches winnow naive bayes base learners. found closest-based al-svm approach worked better approaches cannot draw conclusions closestbased al-svm approach compares qbc-based approaches. additionally none consider class imbalance. experimental results show closestpa consistently outperforms qboostpa qbagpa algorithms. closestpa computationally faster learners important scenarios annotators wait active learner select examples request labels. corresponding points learning curves closestpa higher performance qboostpa qbagpa. also points around stops important ones examine closestpa outperforms qbagpa qboostpa. lastly terms data utilization ratio often used evaluating approaches closestpa achieves superior results. mairesse gasic jurcicek keizer thomson young phrase-based statistical language generation using graphical models active learning proceedings annual meeting association computational linguistics. uppsala sweden association computational linguistics july available http//www.aclweb.org/anthology/p- bloodgood callison-burch bucking trend largescale cost-focused active learning statistical machine translation proceedings association computational linguistics. uppsala sweden association computational linguistics july available http//www.aclweb.org/anthology/p- s.-w. zhang zhou h.-c. translation model size reduction hierarchical phrase-based statistical machine translation proceedings association computational linguistics jeju island korea association computational linguistics july available http//www.aclweb.org/ anthology/p- miura neubig paul nakamura selecting syntactic non-redundant segments active learning machine translation proceedings north american association computational linguistics human chapter language technologies. diego california association computational linguistics june available http//www.aclweb.org/anthology/n- bloodgood filtering tweets social unrest proceedings semantic computing ieee ieee january available http//ieeexplore.ieee.org/stamp/stamp.jsp?tp= &arnumber=&isnumber= bloodgood vijay-shanker taking account differences actively passively acquired data case active learning support vector machines imbalanced datasets proceedings human language technologies annual conference association computational linguistics companion volume short papers. boulder colorado association computational linguistics june seung opper sompolinsky query committee colt proceedings fifth annual workshop computational learning theory. pittsburgh pennsylvania united states mccallum nigam employing pool-based active learning text classiﬁcation proceedings icml- international conference machine learning shavlik madison morgan kaufmann publishers francisco vapnik statistical learning theory. cristianini shawe-taylor introduction support vector machines kernel-based learning methods. cambridge cambridge university press morik brockhausen joachims combining statistical learning knowledge-based approach case study intensive care monitoring proceedings international conference machine learning bled slovenia beatty kochis bloodgood impact batch size stopping active learning text classiﬁcation proceedings ieee international conference semantic computing laguna hills ieee january bloodgood grothendieck analysis stopping active learning based stabilizing predictions proceedings seventeenth conference computational natural language learning. soﬁa bulgaria association computational linguistics august bloodgood vijay-shanker method stopping active learning based stabilizing predictions need user-adjustable stopping proceedings thirteenth conference computational natural language learning boulder colorado association computational linguistics june giuliano lavelli romano exploiting shallow linguistic information relation extraction biomedical literature proceedings conference european chapter association computational linguistics trento italy april dumais platt heckerman sahami inductive learning algorithms representations text categorization cikm proceedings seventh international conference information knowledge management bethesda maryland united states joachims text categorization suport vector machines learning many relevant features ecml ser. lecture notes computer science nedellec rouveirol eds. vol. springer hersh buckley leone hickman ohsumed interactive retrieval evaluation large text collection research proceedings sigir- international conference research development information retrieval croft rijsbergen eds. dublin springer verlag heidelberg joachims text categorization support vector machines learning many relevant features universit¨at dortmund dortmund germany tech. rep. viii-report melville mooney constructing diverse classiﬁer ensembles using artiﬁcial training examples proceedings eighteenth international joint conference artiﬁcial intelligence mexico august", "year": 2018}