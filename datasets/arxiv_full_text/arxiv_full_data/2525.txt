{"title": "Holographic Embeddings of Knowledge Graphs", "tag": ["cs.AI", "cs.LG", "stat.ML", "I.2.6; I.2.4"], "abstract": "Learning embeddings of entities and relations is an efficient and versatile method to perform machine learning on relational data such as knowledge graphs. In this work, we propose holographic embeddings (HolE) to learn compositional vector space representations of entire knowledge graphs. The proposed method is related to holographic models of associative memory in that it employs circular correlation to create compositional representations. By using correlation as the compositional operator HolE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. In extensive experiments we show that holographic embeddings are able to outperform state-of-the-art methods for link prediction in knowledge graphs and relational learning benchmark datasets.", "text": "extract taxonomies probabilistic question answering furthermore embeddings used support machine reading assess trustworthiness sites however existing embedding models capture rich interactions relational data often limited scalability. vice versa models computed efﬁciently often considerably less expressive. work approach learning within framework compositional vector space models. introduce holographic embeddings circular correlation entity embeddings create compositional representations binary relational data. using correlation compositional operator hole capture rich interactions simultaneously remains efﬁcient compute easy train scalable large datasets. show experimentally hole able outperform state-of-the-art embedding models various benchmark datasets learning kgs. compositional vector space models also considered cognitive science natural language processing e.g. model symbolic structures represent semantic meaning phrases models associative memory plate mitchell lapata socher work draw inspiration models also highlight connections hole holographic models associative memory. section introduce compositional vector space models general learning setting related work. denote entities relation types domain. binary relation subset pairs entities higher-arity relations deﬁned analogously. characteristic function relation indicates possible pair entities whether part denote relation instances denote ﬁrst second argument asymmetric relation refer subject object triples. compositional vector space models provide elegant learn characteristic functions relations learning embeddings entities relations efﬁcient versatile method perform machine learning relational data knowledge graphs. work propose holographic embeddings learn compositional vector space representations entire knowledge graphs. proposed method related holographic models associative memory employs circular correlation create compositional representations. using correlation compositional operator hole capture rich interactions simultaneously remains efﬁcient compute easy train scalable large datasets. experimentally show holographic embeddings able outperform state-ofthe-art methods link prediction knowledge graphs relational learning benchmark datasets. relations concept artiﬁcial intelligence cognitive science. many structures humans impose world logical reasoning analogies taxonomies based entities concepts relationships. hence learning relational knowledge representations long considered important task artiﬁcial intelligence muggleton gentner kemp richardson domingos work concerned learning knowledge graphs i.e. knowledge bases model facts instances binary relations form knowledge representation interpreted multigraph entities correspond nodes facts correspond typed edges type edge indicates kind relation. modern knowledge graphs yago dbpedia freebase contain billions facts millions entities found important applications question answering structured search digital assistants. recently vector space embeddings knowledge graphs received considerable attention used create statistical models entire i.e. predict probability possible relation instance graph. models used derive knowledge known facts disambiguate entities intuitively feature tuple representation corresponding features entities allows capture relational patterns liberal persons typically members liberal parties since single feature encode subject liberal person object liberal party. compositional models using tensor product rescal neural tensor network shown state-of-the-art performance learning kgs. furthermore miller liang proposed rescal-based model learn paths kgs. smolensky introduced tensor product create compositional vector representations. tensor product allows capture rich interactions main problem compositional operator lies fact requires large number parameters. since explicitly models pairwise interactions must size problematic terms overﬁtting computational demands. instance nickel jiang tresp showed linear tensor factorization require large model certain relations. since rpeo yang proposed diagonal rp’s reduce number parameters. however approach model symmetric relations suitable model general knowledge graphs concatenation projection non-linearity another compute composite representations concatenation projection subsequent application non-linear function. rd+d denote concatenation non-linear function tanh. composite tuple representation given projection matrix rh×d learned combination entity relation embeddings. intuitively feature tuple representation least corresponding features advantage compositional operator mapping entity embeddings representations pairs learned adaptively matrix however resulting composite representations also less rich consider direct interactions features. socher noted non-linearity provides weak interactions leading harder optimization problem. variant compositional operator also includes relation embedding composite representation used er-mlp model knowledge vault vector representations relations entities; denotes logistic function; {ei}ne denotes embeddings; denotes compositional operator creates composite vector representation pair embeddings discuss possible compositional operators below. denote triple denote label. given dataset true false relation instances want learn representations entities relations best explain according instance done minimizing logistic loss relational data minimizing logistic loss additional advantage help dimensional embeddings complex relational patterns however cases store true triples non-existing triples either missing false case negative examples generated heuristics local closed world assumption alternatively pairwise ranking loss important property compositional models meaning representation entities vary regard position compositional representation since representations entities relations learned jointly eqs. property allows propagate information triples capture global dependencies data enable desired relational learning effect. review machine learning knowledge graphs also nickel non-compositional methods another class models form compositional representations predicts existence triples similarity vector space embeddings. particular transe models score fact distance relation-speciﬁc translations entity embeddings score) dist major appeal transe requires parameters moreover easy train. however simplicity comes also cost modeling power. wang proposed transh transr respectively improve performance transe -to-n n-to- n-to-n relations. unfortunately models lose simplicity efﬁciency transe. section propose novel compositional model relational data. combine expressive power tensor product efﬁciency simplicity transe circular correlation vectors represent pairs entities i.e. compositional operator compositional operator circular correlation interpreted compression tensor product. tensor product assigns separate component aibj pairwise interaction entity features correlation component corresponds ﬁxed partition pairwise interactions intuitively feature tuple representation least partition subject-object-interactions form compression effective since allows share weights semantically similar interactions. example model relational patterns partyof relation might sufﬁcient know whether subject object liberal person liberal party conservative person conservative party. interactions grouped partition. additionally typically case subset possible interactions latent features relevant model relational patterns. irrelevant interactions grouped partitions collectively assigned small weight please note partitioning learned ﬁxed advance correlation operator. possible entity representations learned latent features thus assigned best partition learning. compared tensor product circular correlation important advantage increase dimensionality composite representation memory complexity tuple representation therefore linear dimensionality entity representations. moreover runtime complexity quasilinear circular correlation computed figure circular correlation compression tensor product. arrows indicate summation patterns nodes indicate elements tensor product. adapted plate denotes hadamard product. computational complexity table summarizes improvements circular correlation tensor product. table lists memory complexity hole comparison embedding models. circular convolution operation closely related circular correlation deﬁned comparison convolution correlation main advantages used compositional operator commutative correlation unlike convolution commutative i.e. non-commutativity necessary model asymmetric relations compositional representations. similiarity component correlation single aibi corresponds product existence component helpful model relations similarity entities important. component exists convolution compute representations entities relations minimize either stochastic gradient descent {ei}ne denote embedding single entity relation fspo eo)). gradients given section outline connections holographic models associative memory. models employ sequence convolutions correlations used holography store retrieve information poggio particular holographic reduced representations store association circular convolution holds retrieve noisy version denoising pass retrieved vector clean-up memory returns stored item highest similarity item retrieved instance perform clean-up hence acts scheme memory stores associations vectors stored retrieved using circular convolution correlation. consider following model associative memory relational data subject-predicate indices relation true. next store existing relations convolution superposition representation scheme compositional representation would analogous retrieving stored predicates exist similarly computing eo)) analogous computing probability included retrieved relations i.e. seen triple norm constraints either enforced directly regularization embeddings depend regularization parameter). important difference hole associative memory memorize generalizes well deﬁned associative memory given embeddings store associations directly typically hebbian learning hole simply store associations instead learn embeddings best explain observed data. iterating update embeddings objects denotes learning rate. please note analogous association predicate subject holographic associative memory. hence interpret adapting memory retrieval observed facts improved. analogy holds updates however roles correlation convolution storage retrieval reversed. moreover minimizing estimating probability distribution possible states knowledge graph allows predict probability possible triple graph nickel knowledge graphs evaluate performance link prediction knowledge graphs compared hole state-of-the-art models commonly used benchmark datasets task wordnet groups words synonyms provides lexical relationships words. dataset consists subset wordnet containing entities relation types triples. datasets used ﬁxed training- validation- test-splits provided bordes baseline methods used rescal transe transr er-mlp. facilitate fair comparison reimplemented models used identical loss optimization method training i.e. adagrad ranking loss improved results transe rescal signiﬁcantly datasets compared results reported bordes following bordes generated negative relation instances training corrupting positive triples used following evaluation protocol true triple test replace subject entity compute score rank instances scores decreasing order. since exist multiple true triples test given predicate-object pair remove instances ranking i.e. consider transe original implementation used without ranking test instance among wrong instances repeat procedure replacing object measure quality ranking mean reciprocal rank commonly used information retrieval contrast mean rank less sensitive outliers. addition report ratio occurs within ﬁrst results optimized hyperparameters models extensive grid search selected model best ﬁltered score validation set. results experiments shown table seen hole able outperform considered baseline methods signiﬁcantly consistently datasets. instance transe transr rank test instance cases likely triple contrast hole ranks test instance cases likely instance. less pronounced similar results observed fbk. table report dimensionality resulting number parameters selected models. seen hole efﬁcient number parameters compared tensor product model rescal. although dimensionality hole embedding larger rescal’s overall number parameters signiﬁcantly reduced memory complexity depends linearly also hole typically fast compute. standard hardware .ghz) runtime compute probability single triple around compute embeddings single epoch takes around typically need epochs arrive best estimates embeddings. relational learning shown hole predict triples successfully knowledge graphs. additional experiments wanted test relational learning capabilities compositional representation. purpose used countries dataset bouchard singh trouillon consists countries subregions regions country located exactly region subregion subregion located exactly region country number countries neighbors. data created relational representation predicates locatedin neighborof. task experiment predict locatedin instances ranges countries regions data. evaluation protocol following first split countries randomly train validation test country test least neighbor training set. next removed triples test validation three different settings basic setting locatedin missing countries test/valid. set. setting correct addition triples locatedin missing countries test/valid. subregions data. setting correct triples predicted from neighborof locatedin locatedin harder task since country multiple neighbors different regions. addition triples locatedin missing neighbors countries test/valid. regions data. setting correct triples predicted from illustration data structure test settings. measured prediction quality area under precision-recall curve results experiments shown table seen hole successful learning tasks. missing triples predicted nearly perfectly. moreover even difﬁcult task hole achieves good results especially since every country’s region predicted neighbors poorer results rescal er-mlp likely explained overﬁtting since difference hole reduced hyperparameters optimized test instead validation set. observed similar results experiment commonly used benchmark datasets statistical relational learning. space constraints report experiments supplementary material. correlation context creates ﬁxed-width representations meaning compositional representation dimensionality representation constituents. hole exploited property create compositional model capture rich interactions relational data simultaneously remains efﬁcient compute easy train scalable. experimentally showed hole provides state-of-the-art performance variety benchmark datasets model complex relational patterns economical number parameters. moreover highlighted connections hole holographic models associative memory discussed interpreted context. creates link relational learning associative memory also allows principled ways query model instance question answering. future work plan exploit ﬁxed-width representations holographic embeddings complex scenarios since especially suitable model higher-arity relations facts facts material based upon work supported center brains minds machines funded award ccf-. code models experiments used paper available https//github.com/ mnick/holographic-embeddings.", "year": 2015}