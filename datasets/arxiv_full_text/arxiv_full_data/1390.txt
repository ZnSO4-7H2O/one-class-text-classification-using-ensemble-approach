{"title": "Systematic evaluation of CNN advances on the ImageNet", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "The paper systematically studies the impact of a range of recent advances in CNN architectures and learning methods on the object categorization (ILSVRC) problem. The evalution tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatibility with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc.  The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is bigger than the observed improvement when all modifications are introduced, but the \"deficit\" is small suggesting independence of their benefits. We show that the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.", "text": "paper systematically studies impact range recent advances architectures learning methods object categorization problem. evalution tests inﬂuence following choices architecture non-linearity pooling variants network width classiﬁer design image pre-processing learning parameters learning rate batch size cleanliness data etc. performance gains proposed modiﬁcations ﬁrst tested individually combination. individual gains bigger observed improvement modiﬁcations introduced deﬁcit small suggesting independence beneﬁts. show pixel images suﬃcient make qualitative conclusions optimal network structure hold full size caﬀe nets. results obtained order magnitude faster standard pixel images. deep convolution networks become mainstream method solving various computer vision tasks image classiﬁcation object detection semantic segmentation image retrieval tracking text detection stereo matching many other. besides classic works training neural networks still highly relevant little guidance theory plethora design choices hyper-parameter settings cnns consequent researchers proceed trial-and-error experimentation architecture copying sticking established types. good results imagenet competition improvements many components architecture like nonlinearity type pooling structure learning recently proposed. first applied ilsvrc competition adopted diﬀerent research areas. contributions recent improvements interaction systematically evaluated. survey recent developments perform large scale experimental study considers choice nonlinearity pooling learning rate policy classiﬁer design network width batch normalization include resnets recent development achieving excellent results since well covered papers three main contributions paper. first survey present baseline results wide variety architectures design choices alone combination. based large-scale evaluation provide novel recommendations insights construction deep convolutional network. second present imagenet-px fast reliable benchmark relative order results popular architectures change compared common image size even pixels. last least benchmark fully reproducible scripts data available online. paper structured follows. section explain validate experiment design. section inﬂuence range hyper-parameters evaluated isolation. related literature review corresponding experiment sections. section devoted combination best hyperparameter setting squeezing-the-last-percentage-points given architecture recommendation. paper concluded section tested networks trained object category classiﬁcation problem imagenet dataset consists image training image validation image test set. test used experiments. commonly used pre-processing includes imvariants linear tanh sigmoid relu vlrelu rrelu prelu maxout combination non-linearity. non-linearity linear tanh sigmoid relu vlrelu rrelu prelu maxout average stochastic max+average strided convolution zero-padding step square square root linear input size reduction validated training caﬀenet googlenet vggnet reduced standard image sizes. results shown figure reduction input image size leads consistent drop top- accuracy around popular architectures change relative order accuracy diﬀerence. order decrease probability overﬁtting make experiments less demanding memory another change caﬀenet made. number ﬁlters fully-connected layers reduced factor results validating resolution reduction presented figure parameters architecture standard caﬀenet shown table experiments used caﬀenet thinner fully-connected layers named caﬀenet-fc. architecture denoted notation fully-connected layers equivalent image input size ﬁxed default activation function relu every convolution layer except last -way softmax classiﬁer. momentum used learning initial learning rate decreased factor iterations learning stops iterations. weight decay convolutional weights applied bias. dropout probability used last layers. networks initialized lsuv biases initialized zero. since lsuv initialization works assumption preserving unit variance input pixel intensities scaled subtracting mean pixel values activation functions neural networks topic many functions proposed since relu discovery ﬁrst group related relu i.e. leakyrelu leaky relu rrelu prelu generalized version others based diﬀerent ideas e.g. maxout etc. however best knowledge small fraction activation functions evaluated imagenetscale dataset. have e.g. network architecture used evaluation designed speciﬁcally experiment commonly used. tested popular activation functions available trivial implementations relu tanh sigmoid vlrelu rrelu prelu linear maxout softplus. formulas references given table selected maxout linear pieces. maxout tested modiﬁcations maxw eﬀective network width doubles number parameters computation costs linear pieces maxs computational complexity proposed lecun also tried train sigmoid network initial loss never decreased. finally proposed swietojanski et.al tested combination relu ﬁrst layers maxout last layers network. results shown figure best single performing activation function similar complexity relu elu. parametric prelu performed par. performance centered softplus elu. surprisingly leaky relu popular dcgan networks small datasets outperforms vanilla relu. interesting network swietojanski et.al hypothesis maxout power ﬁnal layers conﬁrmed combined maxout shows best performance among non-linearities speed close relu. wide maxout outperforms rest competitors higher computational cost. pooling combined striding common archive degree invariance together reduction spatial size feature maps. popular options pooling average pooling. among recent advances stochastic pooling lp-norm pooling tree-gated pooling authors last paper tested pooling imagenet. pooling receptive ﬁeld another design choice. krizhevskiy etal. claimed superiority overlapping pooling window size stride vggnet uses non-overlapping window. tested average stochastic proposed average pooling skipping pooling replacing strided convolutions proposed springenberd also tried tree gated poolings encountered convergence problems results strongly depend input image size. know problem implementation method therefore omitted results. figure top- accuracy gain relu caﬀenet- architecture. maxs stands maxout compexity maxw maxout width csoftplus centered softplus. baseline i.e. relu accuracy pooling dropout trained network without dropout. decreased accuracy even more. best results obtained combination average pooling. guess pooling brings selectivity invariance average pooling allows using gradients ﬁlters instead throwing away information done non-overlapping pooling. second experiment receptive ﬁeld size. results shown figure right. overlapping pooling inferior non-overlapping window wins zero-padding done. explained fact better results obtained larger outputs; pooling leads spatial size pool feature leads pool observation means speed performance trade-oﬀ. figure top- accuracy gain pooling caﬀenet- architecture. left diﬀerent pooling methods right diﬀerent receptive ﬁeld sizes. stoch stands stochastic pooling stoch dropout network stochastic pooling turned drop drop layers. learning rate important hyper-parameters inﬂuences ﬁnal performance. surprisingly commonly used learning rate decay policy reduce learning rate validation error stops decreasing adopted parameter search. works well practice lazy policy sub-optimal. tested four learning rate policies step quadratic square root decay linear decay. actual learning rate dynamics shown figure left. validation accuracy shown right. linear decay gives best results. table learning rate decay policies tested paper. initial learning rate number learning iterations current iteration step iteration. decay coeﬃcient commonly used input pixels commonly adopted recommendation pre-processing. much research optimal colorspace pre-processing techniques cnn. rachmadi purnama explored diﬀerent colorspaces vehicle color identiﬁcation dong et.al compared ycrcb channels image superresolution graham extracted local average color retina images winning solution kaggle competition. pre-processing experiment divided parts. first tested popular handcrafted image pre-processing methods colorspaces. since transformations done on-the-ﬂy ﬁrst tested calculation mean pixel variance training replaced applying batch normalization input images. decreases ﬁnal accuracy seen baseline methods. tested ycrcb single-channel grayscale colorspaces. results shown figure experiment conﬁrms best suitable colorspace cnns. labbased network improved initial loss iterations. removing color information images costs accuracy second network learn transformation convolution pixel neighbors involved. mini-networks architectures described table learning process joint main network seen extending caﬀenet architecture several convolutions input. best performing network gave absolute accuracy gain without signiﬁcant computational cost. batch normalization recent method solves gradient exploding/vanishing problem guarantees near-optimal learning regime layer following batch normalized one. following ﬁrst tested diﬀerent options non-linearity. results presented table surprisingly contradictory caﬀenet architecture prefers convrelu-bn-conv googlenet conv-bn-relu-conv placement. moreover results googlenet inferior plain network. diﬀerence changed parameters except using original paper authors decreased regularization changed learning rate decay policy applied additional training re-shuﬄing. also googlenet behavior seems diﬀerent caﬀenet vggnet w.r.t. modiﬁcation section next experiment activations selected placement non-linearity. results shown figure batch normalization washes diﬀerences relu-family variants need architecture seen integration feature detector following classiﬁer. proposed consider convolutional layers alexnet feature extractor fully-connected layers -layer classiﬁer. argued fully-connected layers optimal design explored various architectures instead. considered pre-trained hogs feature extractor explored mostly transfer learning scenario network weights frozen. also explored architectures additional convolution layers seen better classiﬁer enhancement feature extractor. three popular approaches classiﬁer design. first ﬁnal layer feature extractor pooling layer classiﬁer layer done lenet alexnet vggnet second spatial pooling pyramid layer instead pooling layer followed layer mlp. third architecture consist average pooling layer squashing spatial dimensions followed softmax classiﬁer without feature transform. variant used googlenet resnet explored following variants default -layer sppnet pyramid levels removing pool-layer treating fully-connected layers convolutional allows zero-padding therefore increase eﬀective number training examples layer averaging features softmax layer averaging spatial predictions softmax layer results shown figure best results predictions averaged spatial positions layers treated convolution zero padding. advantage standard pooling less pronounced. mini-batch size always trade-oﬀ computation eﬃciency architecture prefers large enough accuracy; early work wilson martinez shows superiority online training batchtraining. explore inﬂuence mini-batch size ﬁnal accuracy. experiments show keeping constant learning rate diﬀerent mini-batch sizes negative impact performance. also tested heuristic proposed krizhevskiy suggests keep product mini-batch size learning rate constant. results shown figure. heuristics works large mini-batch sizes leads quite signiﬁcant decrease performance. extreme online training bring accuracy gains signiﬁcantly slows training wall-clock time. advances imagenet competition caused architectural improvement. best knowledge study network width ﬁnal accuracy dependence. canziani et.al comparative analysis imagenet winner terms accuracy number parameters computational complexity comparison diﬀerent architectures. subsection evaluate increasing caﬀenet width changes. results shown figure original architecture close optimal accuracy flops sense decrease number ﬁlters leads quick signiﬁcant accuracy drop making network thicker brings gains saturates quickly. making network thicker times leads limited accuracy gain. input image size brings additional information training samples convolution ﬁlters plays important role. initial experiment showed figure indicates caﬀenet trained images compete much complex googlenet architecture trained smaller performed experiment diﬀerent input image sizes pixels wide. results presented figure news accuracy depends image size linearly needed computations grow quadratically expensive performance gain. second part experiment kept spatial output size pool layer ﬁxed changing input image size. archieve this respectively change stride ﬁlter size conv layer. results show gain large image size mostly comes larger spatial size deeper layers unseen image details. performance current deep neural network highly dependent dataset size. unfortunately much research published topic. deepface authors shows dataset reduction leads accuracy drop. similar dependence shown schroﬀ et.al extra-large dataset decreasing dataset size leads accuracy drop datasets private experiments reproducible. another important property dataset cleanliness data. example estimate human accuracy imagenet top- create imagenet image voted diﬀerent people imagenet. dataset size experiment thousand examples random chosen full training set. reduced dataset caﬀenet trained scratch. cleanliness test replaced labes random incorrect examples. labels ﬁxed unlike recent work disturbing labels regularization method results shown figure clearly shows bigger dataset brings improvement. minimum size performance quickly degrades. less clean data outperforms noisy ones clean dataset images performs dataset correct images. conducted simple experiment importance bias convolution fully-connected layers. first network trained usual second biases initialized zeros bias learning rate zero. network without biases shows less accuracy default table finally test improvements increase computational cost perform together. combine learned colorspace transform non-linearity convolution layers maxout fully-connected layers linear learning rate decay policy average plus pooling. improvements applied caﬀenet caﬀenet vggnet googlenet. ﬁrst three demonstrated consistent performance growth googlenet performance degraded found batch normalization. possibly complex optimized structure figure applying improvements change feature maps size linear learning rate decay policy colorspace transformation nonlinearity convolution layers maxout non-linearity fully-connected layers average pooling. compared systematically recent advances large scale dataset. shown benchmarking done aﬀordable time computation cost. summary recommendations non-linearity without batchnorm relu apply learned colorspace transformation rgb. linear learning rate decay policy. average pooling layers. mini-batch size around fully-connected layers convolutional average predictions investing increasing training size check plateau cleanliness data important size. cannot increase input image size reduce stride network complex highly optimized architecture like e.g. linear tanh vrelu relu rrelu maxout prelu maxout non-linearity non-linearity linear tanh sigmoid maxout relu rrelu prelu stochastic dropout average stochastic strided convolution max+average pad= step square square root linear grayscale learned histogram equalized ycrcb clahe pooling-fc-fc-clf spp-fc-fc-clf pooling-c-c-clf-maxpool spp-fc-fc-clf pooling-c-c-avepool-clf c-c-clf-avepool pooling-c-c-clf-avepool lr=. lr=. lr=. lr=. lr=. lr=. lr=. lr=. lr=. lr=. lr=. lr=. without caﬀenet caﬀenetall caﬀenet caﬀenetall vggnet- vggnet-all googlenet googlenetall googlenet", "year": 2016}