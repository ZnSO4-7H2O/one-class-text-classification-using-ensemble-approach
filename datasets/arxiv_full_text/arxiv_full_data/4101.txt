{"title": "Learning from Synthetic Data Using a Stacked Multichannel Autoencoder", "tag": ["cs.CV", "cs.AI"], "abstract": "Learning from synthetic data has many important and practical applications. An example of application is photo-sketch recognition. Using synthetic data is challenging due to the differences in feature distributions between synthetic and real data, a phenomenon we term synthetic gap. In this paper, we investigate and formalize a general framework-Stacked Multichannel Autoencoder (SMCAE) that enables bridging the synthetic gap and learning from synthetic data more efficiently. In particular, we show that our SMCAE can not only transform and use synthetic data on the challenging face-sketch recognition task, but that it can also help simulate real images, which can be used for training classifiers for recognition. Preliminary experiments validate the effectiveness of the framework.", "text": "abstract—learning synthetic data many important practical applications example application photosketch recognition. using synthetic data challenging differences feature distributions synthetic real data phenomenon term synthetic gap. paper investigate formalize general framework stacked multichannel autoencoder enables bridging synthetic learning synthetic data efﬁciently. particular show smcae transform synthetic data challenging face-sketch recognition task also help simulate real images used training classiﬁers recognition. preliminary experiments validate effectiveness framework. modern supervised learning algorithms need plenty data help train classiﬁers. data higher quality always desired real-world applications; sometimes beneﬁcial turn synthetic data. example help identify criminals many criminal investigations rely synthetic face sketch rather facial photograph suspect available. synthetic face data normally drawn expert based descriptions eyewitnesses and/or victim. several photo-sketch examples shown fig. application recognition based synthetic data crucial. directly using synthetic data learning algorithm unfortunately challenging since synthetic data different real data least extent e.g. exaggerated facial shapes sketch images fig. compared real images. result feature distributions synthetic data shifted away real data illustrated fig. term shift distributions synthetic gap. synthetic largely caused generating process synthetic data whereas synthetic data generated replicating principal patterns eyes mouth nose hairstyle rather replicating every detail real data. synthetic major obstacle using synthetic data recognition problems since synthetic data fail simulate potentially useful patterns real data important successful recognition. solve problem associate synthetic data real data jointly learn stacked multichannel autoencoder help t-sne visualization distribution histogram oriented fig. gradients features data cufsf dataset left synthetic observed photo sketch features; right synthetic bridged smcae. paper addresses problem learning mapping synthetic data real data. speciﬁcally propose novel framework smcae. training process smcae facilitates bridging synthetic real synthetic data learning transform synthetic real data real real data. model learns essential ‘characters’ ‘patterns’ real data learns augment synthetic data best reproduce distribution real data. tasks learned simultaneously shared parameters essential ‘characteristics’ learned help regularize results vice versa illustrate handwritten digit experiments. main contributions paper best knowledge ﬁrst attempt address problem synthetic demonstrating synthetic data could used improve performance recognition task. propose stacked multichannel autoencoder model bridge synthetic jointly learn real synthetic data. transfer learning aims extract knowledge more source tasks apply target task. transfer learning used many different applications page classiﬁcation zero-shot classiﬁcation detailed survey transfer learning given method speciﬁc form transfer learning termed domain adaptation nonetheless different previous domain adaptation approaches assume synthetic caused shift feature distribution synthetic data real data assume main ’characters’ ’patterns’ strongly co-exist synthetic real data. smcae thus developed based assumption. autoencoder special type neural network output vectors dimensionality input vectors autoencoder different variants shown successful learning transferring shared knowledge among data source different domains thus beneﬁt machine learning tasks. framework borrows idea autoencoder jointly learn different related tasks mapping synthetic real data; real real data. worth noting multimodal autoencoder structure similar proposed. multimodal autoencoder normal autoencoders together sharing hidden layer. structure data input output fully symmetric modal data occupy branch antuencoder. contrast structure proposed smcae composes structure normal autoencoder denoising autoencoder. composition branch smcae capable exploring intrinsic features data domain another branch smcae going transfer data domain another domain using features discovered branches. structure smcae could easily expanded branches compensate complicated multi-task learning problems. experiments show smcae better autoencoders regard. learning synthetic templates. recent works learning synthetic data mostly generate synthetic data either applying simple geometric transformation adding image degradation real data. help ofﬂine recognition handwritten text perturbation model combined morphological operation applied real data. enhance quality degraded document degradation models brightness degradation blurring degradation noise degradation texture-blending degradation used create training dataset handwritten text recognition problem. methods address synthetic problem thus limited small performance improvements using synthetic data. computer graphics models used ease training data generation. simulate pedestrian picture authors track volunteers pose multiple views human bodies reshaped using morphable human model. reshaped picture human bodies later composed real world backgrounds. idea adopted addition render model simulate object real scene features extracted synthetic data adapted better train object detector. propose smace model learn mapping synthetic real data. learn mapping smcae model formulated stacked structure multichannel autoencoders facilitates efﬁcient ﬂexible jointly learning synthetic real data. structure conﬁguration smcae illustrated fig. speciﬁcally left right tasks channels smcae respectively. left task illustrated left channel fig. takes synthetic data input real data reconstruction target; right task right channel fig. uses real data input reconstruction target. between-layer connections colored gray shared tasks channels. smcae structured attempts transform synthetic data real data left task using representation learned real data right task. fig. illustration smcae black edges layers linked shared tasks; blue links separately connected left right task respectively. zoom-in structure smcae single hidden layer. regularization term novel contribution smcae. basically penalizes situation difference learning errors channels large. since conﬁguration smcae data input output channels symmetric learning error resulted optimizing learning process channels different. objective prevent situation optimization channel dominates entire smcae help smcae better leverage learning process compromising balance channels. importance objective show learning results setting different fig. minimization achieved back propagation stochastic gradient descent using quasi-newton method lbfgs. smcae balance regularization added objective difference opposed sparse autoencoder gradient computation unknown parameters clarify differences following equations ﬁrst illustarte setup single layer channel smcae. single channel smcae basically autoencoder assume input dataset encode input instances {xi}n sigmoid data decoding parameters encoded representations weight decay term added improve generalisation autoencoder leverages importance term. avoid learning identity mapping autoencoder penalizes over-activation nodes hidden layer added. averaged activation nodes hidden layer computed thus objective single channel updated structure smcae model extended autoencoder simultaneously deal tasks left right channels. speciﬁcally notation denote conﬁguration input data reconstruction target output layer channel smcae. thus label tasks left right channels smcae oxrl oxrr individually indicate left right channel branch smcae. stand synthetic real data respectively. tasks channels share parameters hidden layers enforces autoencoder learn common structures tasks. output layer divide smcae separate channels parameters target minimize reconstruction error tasks smcae together taking account balance channels. objective function smcae thus train smcae greedy manner layer gets trained time. conﬁguration training layer smcae shown fig. output trained layer sent input next layer training. ﬁne-tuning implemented entire stacked structure layers trained. thus smcae trained transform synthetic data data sent left channel smcae oxrl. take output process transformed synthetic data. shown fig. compare smcae conﬁguration three alternative conﬁgurations smcae-ii places separate channels structure i.e. oxsl oxrr. stacked autoencoder type-i merges tasks single channel stacked autoencoder conﬁguration ixsxr oxrxr. stacked autoencoder type-ii simply transforms source data target data conﬁgures oxr. compared sae-i sae-ii channel structures endow ﬂexibility. critically single channel models force synthetic data real data causes synthetic data lose information become less useful recognition. contrast smcae explore ‘characters’ ‘patterns’ common synthetic real data. intrinsically smcae ﬁrst encodes synthetic real data common hidden layers model common information useful recognition. decoding process transforms synthetic data better simulate real data. although smcae-ii branches structure learn transformation synthetic data real data. evaluation metric proposed rank- recognition accuracy. features. similar cufsf dataset histogram oriented gradients reduce computational cost resolution photos sketches reduced cell size features hwduci dataset uses features cell size classiﬁers. cufsf dataset nearest-neighbor search euclidean metric used retrieving similar photo query sketch. handwritten digit classiﬁcation support vector machine kernel used experiments. ﬁrst compare smcae challenging task facesketch recognition using cufsf dataset. show smcae better alternative conﬁgurations. validate efﬁcacy framework train smcae handwritten digit images generate synthetic data simulate real images. show synthetic data help train classiﬁers recognition. dataset. conduct experiments different datasets cufsf dataset containing photos sketches people lighting variations. employ standard split deﬁned selects persons training remaining persons testing set. handwritten digits dataset containing instances total samples used training samples used testing. handwritten digits dataset collected people contributed training test set. experiments empirically number hidden layers smcae layer nodes. settings used make smcae smcae-ii sae-i sae-ii comparable. evaluation metrics. report following metrics available f-score deﬁned receiving operator characteristic curves vr.%far performance veriﬁcation rate false acceptance rate vr.%far standard experiments dataset features sketch images ﬁrst transformed smcae used queries. ﬁrst compare results photo-sketch matching using feature transformed smcae smcae-ii saesae-ii. results reported curve starting vr.%far. dissimilarity photo sketch computed euclidean distance descriptors. curves vr.%far shown fig. clearly proposed smcae achieves highest results values vr.%far accuracy signiﬁcantly outperforms alternative conﬁgurations. note also report state-of-the-art approaches vr.%far including lfda cite classic eigenfaces. previous works better result could obtained combining multiple features. example multiple cite features generated random forest used batter matching photos sketches. here enable comparison fairness focus comparison matching results obtained using uncombined feature only. several reasons smcae outperform approaches. first compared smcae-ii conﬁguration smcae involves task handles transformation synthetic real data thus better eliminates distance them. second compared sae-i rather merging tasks single channel smcae employs channels better clarify task reconstructing main ‘characters’ ‘patterns’ co-existing tasks. thus synthetic data easily transformed real data less error. finally smcae better sae-ii smcae learns features real data task oxrr. features better compensate difference synthetic data real data transformation. validate results using rank- recognition accuracy also reported results shown fig. methods comparable smcae. method employed discriminant common subspace maximize between-class variations minimize within-class variations. method used structure composed autoencoders. seen fig. smcae outperforms methods. parameter validation validate signiﬁcance different values report rank- accuracy particularly takes times longer smcae converge compared used work rank- accuracy dropped validates importance term discussed sec. qualitative results. qualitative results shown fig. shows sketch transformed smcae similar ground truth photo hog. handwritten digit recognition generating synthetic data. synthetic version real character generated variant centralized model learned real characters. centralized model digit shaped control points {ci}n settled boundary digit. technique called migration used locate corresponding control points real digit image. synthetic digit image could generated ﬁlling areas closed control points examples generated synthetic digits shown fig. generate synthetic data used train classiﬁer transformed trained smcae assume locations control points follow multivariate normal distribution estimated using control points synthetic digit images. digit synthetic images generated randomly drawing samples compare smcae smcae-ii sae-i saeii lenet- best results reported data set. classiﬁcation performance evaluated f-score. support vector machine classiﬁer kernel used experiments. smcae smcae-ii sae-i sae-ii test real training data together transformed synthetic data used train svm. shown fig. classiﬁer smcae better alternative methods. validates effectiveness framework generating synthetic data better help training classiﬁer. demonstrate transformed synthetic data improve classiﬁcation results conducted evaluations training classiﬁers using different combinations training sets fig. particularly four combinations training sets used. first performance baseline trained using real data only. investigate much improvement could obtain classiﬁcation using autoencoder model. smcae multiple channels structure extension standard autoencoder. show smcae bridges synthetic real data synthetic data also jointly learns real synthetic data. gungor gady agam ophir frieder gideon frieder. interactive degraded document enhancement ground truth generation. electronic imaging international society optics photonics shai ben-david john blitzer koby crammer alex kulesza fernando pereira jennifer wortman vaughan. theory learning different domains. mach. learn. trends machine learning yoshua bengio. deep learning representations unsupervised transfer learning. unsupervised transfer learning challenges machine learning volume page deng zixing zhang erik marchi bjorn schuller. sparse autoencoder-based feature transfer learning speech emotion recognition. affective computing intelligent interaction erik miller nicholas matsakis paul viola. learning computer example shared densities transforms. vision pattern recognition proceedings. ieee conference volume pages ieee leonid pishchulin arjun jain christian wojek mykhaylo andriluka thorsten thormählen bernt schiele. learning people detection computer vision pattern models training samples. recognition ieee conference pages ieee fig. comparison classiﬁcation results f-score different methods different training datasets left ﬁgure methods test used training dataset combines original real data transformed synthetic data. trained transformed synthetic data compare trained synthetic data transformed synthetic data respectively. best performance obtained trained real data together transformed synthetic data. synthetic training data generated smcae gain large margin improvement classiﬁcation. notice result using transformed synthetic real+transformed synthetic separately fig. highlights effectiveness smcae transforming synthetic data simulate real data. finally interesting evaluate amount synthetic data affects classiﬁcation results. increasingly transformed synthetic data training svm. classiﬁcation results reported fig. curve shows ascending trend adding samples means transformed synthetic data added test highly effective useful classiﬁcation. baochen kate saenko. virtual reality fast adaptation virtual object detectors real domains. proceedings british machine vision conference. bmva press laurens maaten geoffrey hinton. visualizing data using t-sne. journal machine learning research tamás varga horst bunke. effects training expansion conf. tamás varga horst bunke. comparing natural synthetic training data off-line cursive handwriting recognition. frontiers handwriting recognition iwfhr- ninth international workshop pages ieee pascal vincent hugo larochelle yoshua bengio pierre-antoine manzagol. extracting composing robust features denoising autoencoders. proceedings international conference machine learning pages synthetic data represented parametric model control points edges associated points images. control points synthetic images could generated simulate real images terms convergence order minimize distance synthetic images generated control points real image. annotate control points edges associated {ci}n control points algorithm optimizecontrolpoints situation given algorithm number steps algorithm. step step examples given fig. zoom example showing control points moved", "year": 2015}