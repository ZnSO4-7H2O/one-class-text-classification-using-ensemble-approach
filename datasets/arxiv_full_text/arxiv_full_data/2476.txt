{"title": "Unbiased Offline Evaluation of Contextual-bandit-based News Article  Recommendation Algorithms", "tag": ["cs.LG", "cs.AI", "cs.RO", "stat.ML", "H.3.5; I.2.6"], "abstract": "Contextual bandit algorithms have become popular for online recommendation systems such as Digg, Yahoo! Buzz, and news recommendation in general. \\emph{Offline} evaluation of the effectiveness of new algorithms in these applications is critical for protecting online user experiences but very challenging due to their \"partial-label\" nature. Common practice is to create a simulator which simulates the online environment for the problem at hand and then run an algorithm against this simulator. However, creating simulator itself is often difficult and modeling bias is usually unavoidably introduced. In this paper, we introduce a \\emph{replay} methodology for contextual bandit algorithm evaluation. Different from simulator-based approaches, our method is completely data-driven and very easy to adapt to different applications. More importantly, our method can provide provably unbiased evaluations. Our empirical results on a large-scale news article recommendation dataset collected from Yahoo! Front Page conform well with our theoretical results. Furthermore, comparisons between our offline replay and online bucket evaluation of several contextual bandit algorithms show accuracy and effectiveness of our offline evaluation method.", "text": "user activities clicks identify attractive contents. inherent challenge score newly generated contents breaking news especially news ﬁrst emerges little data available. personalized service tailor contents towards individual users desirable challenging. distinct feature applications partiallabel nature observe user feedback article article displayed. challenge thus arises known exploration/exploitation tradeoﬀ hand want exploit than explore balance exploration exploitation modeled contextual bandit subclass reinforcement learning problems also present many important web-based applications online display search query suggestion etc. ideal evaluate contextual-bandit algorithm conduct bucket test algorithm serve fraction live user traﬃc real recommendation system. however method expensive requiring substantial engineering eﬀorts deploying method real system also negative impacts user experience. furthermore easy guarantee replicable comparison using bucket tests online metrics vary signiﬁcantly time. oﬄine evaluation contextual-bandit algorighms thus becomes valuable optimize online recommendation system. although benchmark datasets supervised learning repository proved valuable empirical comparison algorithms collecting benchmark data towards reliable oﬄine evaluation diﬃcult bandit problems. application news article recommendation yahoo front page example user visit results following information stored user information displayed news article user feedback using data form evaluate bandit algorithm oﬄine user feedback algorithm recommends diﬀerent news article stored log. words data bandit-style applications contain user feedback recommendations actually displayed user candidates. partial-label nature raises diﬃculty diﬀerence evaluation bandit algorithms supervised learning ones. abstract contextual bandit algorithms become popular online recommendation systems digg yahoo buzz news recommendation general. oﬄine evaluation eﬀectiveness algorithms applications critical protecting online user experiences challenging partial-label nature. common practice create simulator simulates online environment problem hand algorithm simulator. however creating simulator often diﬃcult modeling bias usually unavoidably introduced. paper introduce replay methodology contextual bandit algorithm evaluation. diﬀerent simulator-based approaches method completely data-driven easy adapt diﬀerent applications. importantly method provide provably unbiased evaluations. empirical results large-scale news article recommendation dataset collected yahoo front page conform well theoretical results. furthermore comparisons oﬄine replay online bucket evaluation several contextual bandit algorithms show accuracy eﬀectiveness oﬄine evaluation method. permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copy otherwise republish post servers redistribute lists requires prior speciﬁc permission and/or fee. wsdm’ february hong kong china. copyright ----// ..... common practice evaluating bandit algorithms create simulator algorithm approach evaluate bandit algorithm without real system. unfortunately major drawbacks approach. first creating simulator challenging time-consuming practical problems. second evaluation results based artiﬁcial simulators reﬂect actual performance since simulators rough approximations real problems unavoidably contains modeling bias. contributions two-fold. first describe study oﬄine evaluation method bandit algorithms enjoys valuable theoretical guarantees including unbiasedness accuracy. second verify method’s eﬀectiveness comparing evaluation results online bucket results using large volume data recorded yahoo front page. positive results encourage wide proposed method web-baesd applications also suggest promising solution create benchmark datasets real-world applications bandit algorithms. unbiased evaluation studied different settings. unbiased evaluation method brieﬂy sketched earlier paper interpreted special case exploration scavenging technique conduct thorough investigation work including improved theoretical guarantees positive empirical evidence using online bucket data. multi-armed bandit problem classic popular model studying exploration-exploitation tradeoﬀ. despite simplicity model found wide applications important problems like medical treatment allocation recently challenging large-scale problems like content optimization diﬀerent classic multi-armed bandit problems particularly concerned interesting setting round contextual information available decision making. notation purpose paper consider multi-armed bandit problem contextual information. following previous work call contextual bandit problem. formally deﬁne arms current context chooses literature contextual bandits sometimes called bandits covariate associative reinforcement learning bandits expert advice bandits side information associative bandits much research multi-armed bandit problems devoted developing algorithms large total payoﬀ. formally search algorithm minimizing regret respect optimal arm-selection strategy here -trial regret algorithm respect deﬁned important special case general contextual bandit problem well-known k-armed bandit context remains constant since contexts constant every trial eﬀect bandit algorithm also refer type bandit context-free bandit. example news article recommendation view articles pool arms t-th user visit article chosen serve user. served article clicked payoﬀ incurred; otherwise payoﬀ deﬁnition payoﬀ expected payoﬀ article precisely click-through rate choosing article maximum equivalent maximizing expected number clicks users turn maximizing total expected payoﬀ bandit formulation. existing bandit algorithms fundamental challenge bandit problems need balancing exploration exploitation. minimize regret equation algorithm exploits past experience select appears best. hand seemingly optimal fact suboptimal imprecision knowledge. order avoid modeling step often expensive diﬃcult importantly often introduces modeling bias simulator making hard justify reliability obtained evaluation results. contrast propose approach unbiased grounded data simple implement. section describe sound technique carrying evaluation assuming individual events i.i.d. logging policy chose time step uniformly random. although omit details latter assumption weakened considerably randomized logging policy allowed algorithm modiﬁed accordingly using rejection sampling cost decreased data eﬃciency. precisely suppose unknown distribution tuples drawn i.i.d. form consisting observed context unobserved payoﬀs arms. also posit access long sequence logged events resulting interaction uniformly random logging policy world. event consists context vector selected resulting observed payoﬀ crucially logged data partially labeled sense payoﬀ observed single chosen uniformly random. events together current context. therefore data serves benchmark people evaluate compare diﬀerent bandit algorithms. supervised learning benchmark sets allow easier replicable comparisons algorithms real-life data. noted section focuses contextual bandit problems constant sets size assumption leads easier exposition analysis satisﬁed practice. example news article recommendation problem studied section arms ﬁxed arms become available arms dismissed. consequently events independent drawn non-identical distributions. investigate setting formally although possible generalize setting section variable case. empirically evaluator stable. subsection simplicity exposition take sequence logged events inﬁnitely long stream. also give explicit bounds actual ﬁnite number events required evaluation method. variation ﬁnite data streams studied next subsection. policy evaluator shown algorithm method takes input bandit algorithm desired number valid events base evaluation. step stream logged events one. given current history happens policy chooses selected logging policy event retained total payoﬀ updated. otherwise policy selects diﬀerent taken logging policy event entirely ignored algorithm proceeds next event without change state. undesired situation explore world actually choosing seemingly suboptimal arms gather information exploration increase short-term regret since suboptimal arms chosen. however obtaining information arms’ average payoﬀs reﬁne estimate arms’ payoﬀs turn reduce long-term regret. clearly neither purely exploring purely exploiting algorithm works best general good tradeoﬀ needed. roughly classes bandit algorithms. ﬁrst class algorithms attempt minimize regret number steps increases. formally algorithms ensure quantity ra/t vanishes time grows. low-regret algorithms extensively studied context-free k-armed bandit problem general contextual bandit problem remained challenging. another class algorithms based bayes rule gittins index methods bayesian approaches competitive performance appropriate prior distributions often computationally prohibitive without coupling approximation appendix describes representative low-regret algorithms used experiments noted method algorithm independent applied evaluate bayesian algorithms well. compared machine learning standard supervised learning setting evaluation methods contextual bandit setting frustratingly diﬃcult. goal measure performance bandit algorithm rule selecting time step based preceding interactions current context here t-th action chosen general depends previous contexts actions observed rewards. interactive nature problem would seem evaluation unbiasedly actually algorithm online live data. however practice approach likely infeasible serious logistical challenges extensive engineering resources potential risks user experiences. rather oﬄine data available collected previous time using entirely diﬀerent logging policy. payoﬀs observed arms chosen logging policy likely diﬀer chosen algorithm evaluated clear evaluate based logged data. evaluation problem viewed special case so-called oﬀ-policy policy evaluation problem reinforcement-learning literature multiarmed bandit setting however need temporal credit assignment thus eﬃcient solutions possible. next consider situation relevant practical evaluation static policy ﬁnite data containing logged events. roughly speaking algorithm steps every event algorithm obtains estimate policy’s average pertrial payoﬀ based random number valid events. detailed pseudocode algorithm algorithm similar algorithm diﬀerence number valid events denoted pseudocode random number mean l/k. reason output algorithm unbiased estimate true per-trial payoﬀ however next theorem shows ﬁnal value arbitrarily close high probability long large enough. using fact theorem shows returned value algorithm accurate estimate true per-trial payoﬀ high probability ﬁxed policy chooses action independent history ht−. emphasize ﬁxed policy following theorem proof instead result similar oﬄine evaluation algorithm similarly provides sharpened analysis special case policy evaluation reinforcement learning section provides empirical evidence matching bound. uniformly random event retained algorithm probability exactly independent everything else. means events retained distribution selected result prove processes equivalent ﬁrst evaluating policy real-world events second evaluating policy using policy evaluator stream logged events. theorem formalizes intuition. theorem says every history identical probability real world policy evaluator. statistics histories estimated per-trial payoﬀ ˆga/t returned algorithm therefore unbiased estimates respective quantities algorithm hence repeating algorithm multiple times averaging returned per-trial payoﬀs accurately estimate total per-trial payoﬀ algorithm respective conﬁdence intervals. further theorem guarantees that high probability logged events suﬃcient retain sample size proof. ﬁrst statement proved mathematical induction time steps event streams second since event stream retained probability exactly expected number required retain events exactly finally high-probability bound application multiplicative form chernoﬀ’s inequality. given unbiasedness guarantee expect concentration also guaranteed; evaluator becomes accurate increases. unfortunately conjecture false general bandit algorithms explained example next section. dependent history. hope lost though— known algorithms deviation bounds provable; example epoch-greedy algorithm exp.p furthermore commented earlier always repeat evaluation process multiple times average outcomes accurate estimate algorithm’s performance. next section show empirically algorithm returns highly stable results algorithms tried. denote probability expectation respect randomness generated t-th event stream indicator matches chosen policy context vtrtat returned value algorithm ˆgπ/t bound denominator numerator respectively. section apply oﬄine evaluation method previous section large-scale real-world problem variable sets validate eﬀectiveness oﬄine evaluation methodology. speciﬁcally provide empirical evidence unbiasedness guarantee theorem convergence rate theorem variance evaluation result eﬀectiveness evaluation method change time. proposed evaluation methodology applied application focus eﬀectiveness oﬄine evaluation method itself. importantly also provide empirical evidence unbiasedness ﬁxed policies also learning algorithms relating oﬄine evaluation metric online performance large-scale production buckets yahoo front page. ﬁrst describe application show modeled contextual bandit problem. second compare oﬄine evaluation result policy online evaluation show evaluation approach indeed unbiased gives results asymptotically consistent number valid events large. third provide empirical evidence oﬄine evaluation method gives stable results representative algorithms. finally study relationship oﬄine evaluation results online bucket performance three bandit algorithms. given theorem might wonder similar result holds general bandit algorithms. unfortunately following example shows concentration result impossible general. suppose deﬁned uniform random coin ﬂip. algorithm operates follows algorithm chooses otherwise always chooses therefore expected per-trial payoﬀ however individual algorithm -step total reward either pages internet; snapshot figure default featured today module highlights four high-quality news articles selected hourly-refreshed article pool maintained human editors. illustrated figure four articles footer positions indexed f–f. article represented small picture title. four articles highlighted story position featured large picture title short summary along related links. default article highlighted story position. user click highlighted article story position read details interested article. event recorded story click. draw visitors’ attention would like rank available articles according individual interests highlight attractive article visitor story position. paper focus selecting articles story position. problem naturally modeled contextual bandit problem. here reasonable assume user visits click probabilities articles i.i.d. furthermore user features click probability speciﬁc article inferred; features contextual information used bandit process. finally view articles pool arms payoﬀ user clicks article otherwise. deﬁnition payoﬀ expected payoﬀ article precisely choosing article maximum equivalent maximizing expected number clicks users turn maximizing per-trial payoﬀ bandit formulation. setup cookie-based buckets evaluation. bucket consists certain amount visitors. cookie string letters randomly generated browser identiﬁer. specify cookie pattern create bucket. example could users starting letter cookies fall bucket. cookiebased bucket user served policy unless user changes cookie belongs another bucket. oﬄine evaluation millions events collected random bucket nov. nov. random bucket articles randomly selected article pool serve users. million events oﬄine evaluation data articles available pool every moment. focused user interactions story article story position only. user interactions recorded types events user visit event story click event. chose metric interest deﬁned ratio number story click events number user visits. protect business-sensitive information report relative ctrs deﬁned ratio true ctrs hidden constant. unbiasedness analysis given policy unbiasedness oﬄine evaluation methodology empirically veriﬁed comparing oﬄine metrics online performance. another cookiebased bucket noted serving bucket evaluate online performance. serving bucket spatio-temporal algorithm deployed estimate article ctrs. article highest estimate used serve users. extracted serving policy serving bucket i.e. best article every minutes nov. nov. note period time oﬄine evaluation data ensuring sets available arms serving random buckets. then used algorithm evaluate serving policy events random bucket oﬄine metric. noted outcome experiments foregone conclusion mathematics presented setting diﬀers ways i.i.d. assumption made theorems typical real-world applications. particular events exchangeable since articles leave system ones enter sometimes unlogged business rule constraints serving policy users course behave independently repeatedly visit site. ﬁnesse away last issue ﬁrst still valid. serving bucket winner article usually remains best while. winning time user repeatedly sees article. time users random bucket likely diﬀerent articles user visit events random serving policy. conceivable user views article less likely user clicks article. conditional eﬀect violates i.i.d. assumption theorem fortunately discrepancy removed considering distinct views. user consecutive events viewing article counted user visit only. distinct views serving bucket measures user interactions winner articles across whole session. regarding oﬄine evaluation metric algorithm subset events sampled random bucket also measures user interactions winner articles across whole session. ﬁrst compared online oﬄine per-article ctrs. winner articles viewed times serving bucket used plot online ctrs accurate enough treated ground truth. figure shows metric evaluated oﬄine close estimated online. sets results corroborate unbiasedness guarantee theorem property particular importance practice almost impossible simulator-based evaluation methods. therefore evaluation method provides solution accurate without cost risk running policy real system. study diﬀerence oﬄine online ctrs decreases data show convergence rate present estimated error versus number samples used oﬄine evaluation. formally deﬁne estimated form upper conﬁdence bound. results suggest that practice observe error decay rate predicted theorem reasonably stable algorithms evaluated. variance evaluation results algorithms parameter ǫgreedy linucb details). ﬁxed parameters reasonable values collected user visits random bucket evaluate variance subsampled data event used probability algorithm times independently subsampled events measure returned using algorithm table summarizes statistics estimates three algorithms. shows evaluation results highly consistent across diﬀerent random runs. speciﬁcally ratio standard deviation mean ǫ-greedy linucb known algorithm-speciﬁc deviation bounds. experiment demonstrates empirically evaluation method give results small variance natural algorithms despite artiﬁcial counterexample section suggesting large datasets result obtained evaluation method already quite reliable. sections give evidence accuracy oﬄine evaluation method applied static policy ﬁxed time. section show complimentary accuracy results learning algorithms viewed history-dependent non-ﬁxed policies. particular show consistency oﬄine evaluation online evaluation three ǫ-greedy bandit models clusters based age/gender information. estimate available articles within cluster user cluster serve article highest ctr. note users’ feedback change serving policy cluster future trials; personalized model. model deﬁne separate context user based gender etc. available article maintain logistic regression model predict given user context. user comes cemp bucket estimate ctrs articles user select article highest estimated display. users diﬀerent contexts served diﬀerent articles bucket user’s feedback three bandit models three online bcookie-based buckets deploy three bandit models respectively. also another bucket collect random exploration data. random data used update states three online bandit models also used oﬄine evaluation. given period obtain per-trial payoﬀs gonline random exploration bucket oﬄine evaluation three models period per-trial payoﬀs goﬄine important note unlogged businessrule constraints online serving buckets today module; instance article forced shown given time window. fortunately data analysis suggested business rules roughly multiplicative impact algorithm’s online although multiplicative factor vary across diﬀerent days. remove eﬀects caused business rules report ratio oﬄine estimate online model goﬄine oﬄine evaluation metric truthful algorithm’s online metric absence business rules expected that given period time like remain constant ideally depend algorithm figure present scatter plot ρemp ρsemp days views four online buckets. scatter plot indicates strong linear correlation. slope least squares linear regression standard deviation residue vector observed business rules give almost impact buckets serving policies. semp relatively simple bandit algorithm similar emp. next experiment study online/oﬄine correlation complicated contextual bandit serving policy cemp ctrs estimated using logistic regression online bucket. scatter plot indicates strong linear correlation. comparison slope standard deviation residue vector respectively. shows diﬀerence oﬄine online evaluation caused business rules systemic factors e.g. time-out user feature retrieval delays model update comparable across bandit models. although daily factor unpredictable relative performance bandit models oﬄine evaluation reserved online buckets. thus oﬄine evaluator provide reliable comparison diﬀerent models historical data even presence business rules. paper studies oﬄine evaluation method bandit algorithms relies data directly rather simulator. requirement method data generated i.i.d. arms chosen random policy. show evaluation method gives unbiased estimates quantities like total payoﬀs also provide sample complexity bound estimated error algorithm ﬁxed policy. evaluation method empirically validated using real-world data collected yahoo front page challenging application online news article recommendation. empirical results verify theoretical guarantees demonstrate accuracy stability method using real online bucket results. encouraging results suggest usefulness evaluation method easily applied related applications online reﬁnement ranking results display. finite-time analysis multiarmed bandit problem. machine learning peter auer nicol`o cesa-bianchi yoav freund robert schapire. nonstochastic multiarmed bandit problem. siam journal computing reyzin robert schapire. contextual bandit algorithms supervised learning guarantees. proceedings fourteenth international conference artiﬁcial intelligence statistics pages lihong reyzin robert schapire. contextual bandits linear payoﬀ functions. proceedings fourteenth international conference artiﬁcial intelligence statistics pages shawe-taylor. gaussian processes modelling dependencies multi-armed bandit problems. proceedings tenth international symposium operational research pages data expensive obtain. furthermore risksensitive applications inject randomness data collection uniformly random policy might much hope practical constraints mentioned earlier evaluation method extended work data collected random policy rejection sampling enjoys similar unbiasedness guarantees reduces data eﬃciency time. interesting future direction therefore exploiting problem-speciﬁc structures avoid exploration full space. related question make non-random data reliable oﬄine evaluation recent progress made ǫ-greedy strategy unguided since picks random exploration. intuitive clearly suboptimal need explored. contrast another class algorithms generally known upper conﬁdence bound algorithms smarter balance exploration exploitation. particular trial algorithms estimate mean payoﬀ ˆµta well corresponding conﬁdence interval lect achieves highest upper conﬁdence bound maxa tunable parameter increase slowly time. words algorithms choose either high payoﬀ estimate high estimation uncertainty measure data collected reﬁne payoﬀ estimate conﬁdence interval vanishes algorithms behave greedily. appropriately deﬁned conﬁdence intervals parameter shown algorithms small total -trial regret logarithmic total number trials context-free k-armed bandits extensively studied well understood general contextual bandit problem largely remained open. algorithm variants exponential weighting technique achieve regret expectation odef= even sequence contexts payoﬀs chosen adversarial world computational complexity exponential number features general. another general contextual bandit algorithm epoch-greedy algorithm similar ǫ-greedy adaptively shrinking assuming sequence contexts i.i.d. algorithm computationally eﬃcient given oracle empirical risk minimizer weaker regret guarantee general stronger guarantees various special cases. zhaohui zheng chang. online learning recency search ranking using real-time user feedback. proceedings nineteenth international conference knowledge management littman haym hirsh. experience-eﬃcient learning associative bandit problems. proceedings twenty-third international conference machine learning pages", "year": 2010}