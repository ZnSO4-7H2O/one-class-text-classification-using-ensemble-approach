{"title": "Building Memory with Concept Learning Capabilities from Large-scale  Knowledge Base", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "We present a new perspective on neural knowledge base (KB) embeddings, from which we build a framework that can model symbolic knowledge in the KB together with its learning process. We show that this framework well regularizes previous neural KB embedding model for superior performance in reasoning tasks, while having the capabilities of dealing with unseen entities, that is, to learn their embeddings from natural language descriptions, which is very like human's behavior of learning semantic concepts.", "text": "present perspective neural knowledge base embeddings build framework model symbolic knowledge together learning process. show framework well regularizes previous neural embedding model superior performance reasoning tasks capabilities dealing unseen entities learn embeddings natural language descriptions like human’s behavior learning semantic concepts. recent years seen great advances neural networks applications modeling images natural languages. deep neural networks people able achieve superior performance various machine learning tasks relational learning aims modeling relational data user-item relations recommendation systems social networks knowledge base etc. paper mainly focus knowledge base. generally knowledge base consists triplets like denote left entity right entity denotes relation them. previous works neural embeddings model entities relations distributed representation i.e. vectors matrices learn prove scalable approaches relational learning. experiments also show neural embedding models obtain state-of-art performance reasoning tasks like link prediction. section cover related work. although methods neural modeling shown promising results reasoning tasks limitations addressing known entities appear training generalize well settings unseen entities. know embedding representations entities cannot establish relations them. hand capability learn concepts entities speciﬁcally learn certain name used human means obviously highly useful particularly kb-based dialog system. observe conversations human task ﬁrst asking explanation establishing knowledge concept peoples’ natural language descriptions. inspired framework modeling human’s cognitive process learning concepts conversations i.e. process natural language description concept memory. neural embedding model model memory concepts. given description text concept framework directly transforms entity embedding captures semantic information concept. entity embedding stored later used concept learning cognitive science usually refers cognitive process people grow abstract generalizations several example objects concept learning represent different behavior. semantic tasks. details framework described section show efﬁciency framework modeling entity relationships involve natural language understanding reasoning. perspective modeling symbolic knowledge learning process main advantages. first enables incorporate natural language descriptions augment modeling relational data human’s behavior learning concepts conversations well. second also utilize large number symbolic facts knowledge base labeled information guide semantic modeling natural language. novel perspective together framework contributions work. statistical relational learning long important topic machine learning. traditional methods markov logic networks often suffer scalability issues intractable inference. following success rank models collaborative ﬁltering tensor factorization proposed general form deal multi-relational learning another perspective regard elements factorized tensors probabilistic latent features entities. leads methods apply nonparametric bayesian inference learn latent features link prediction. also attempts made address interpretability latent feature based models framework bayesian clustering recently noticeable achievements neural embedding models like word vectors natural language processing area various neural embedding models relational data proposed strong competitors scalability predictability reasoning tasks. methods model relational data latent-feature assumption common perspective machine learning gain high performance prediction tasks. however models leave latent features learnt data suffers substantial increments model complexity applying large-scale knowledge bases. example seen feature vector entity factorized tensors also represents entities separate vectors embeddings thus number parameters scales linearly number entities. large number parameters models often increases risk overﬁtting works proposed effective regularization techniques address hand applying models real world tasks shared limitation entities unseen training cannot dealt with complete relations known entities human’s ability learning concepts achieve. perspective develop general framework capable modeling symbolic knowledge together learning process detailed section framework consists parts. ﬁrst part memory storage embedding representations. model large-scale symbolic knowledge thought memory concepts. part concept learning module accepts natural language descriptions concepts input transforms entity embeddings space memory storage. paper translating embedding model memory storage neural networks concept learning module. ﬁrst describe translating embedding model memory storage concepts. transe relationships represented translations embedding space. suppose true facts training set. fact true transe requires close formally deﬁne entity vectors relation vectors distance measure either norm. transe minimizes margin loss score true facts margin. note loss favors lower distance translated left entities right entities training facts random generated facts model optimized stochastic gradient descent mini-batch. besides transe forces norms entity embeddings essential perform well according prevents training process trivially minimizing loss increasing entity embedding norms. advantages using embeddings instead symbolic representations cognitive tasks. example it’s kind easier ﬁgure person violinist play violin tell father’s name. however symbolic representations like knowledge base former fact play violin> deduced reasoning process facts profession violinist> <violinist play violin> two-step procedure latter result acquired step fact father look transe embeddings task ﬁgure plays violin ﬁnding nearest neighbors embedding play’s embedding costs amount time ﬁnding father claim supported ﬁndings cognitive science general properties concepts strongly bound object speciﬁc properties mentioned earlier concept learning module accepts natural language descriptions concepts input outputs corresponding entity embeddings. requires natural language understanding knowledge transferred module neural networks good candidates task. explore kinds neural network architectures concept learning module including multi-layer perceptrons convolutional neural networks hidden layer neurons relu activations. fullyconnected cannot afford computational cost input length long. large scale datasets vocabulary size often millions means bag-of-words features cannot used. here bag-of-n-grams features inputs given word example word ﬁrst starting ending marks like word break -grams suppose kinds -grams training set. input description count numbers kinds -grams text form -dimensional feature vector control scale input dimension instead input features. feed vector output corresponding entity embedding description. since bag-of-n-grams features loses information word order little sense semantics. even word level fails identify words similar meanings. point view explore convolutional architecture i.e. together word vector features. ww...wk paragraph concept description vector representation word experiments paper initialize wi’s word vector pretrained large scale corpus using methods input matrix deﬁned kernel stride kernel stride pooling size stride kernel stride kernel stride pooling size stride kernel stride pooling size stride kernel stride pooling size stride size normalization layer form choose size axis feature maps ﬁrst layer input size kernel size last axes dimension word vectors. ﬁrst layer last axes feature maps layer remain vectors. list layers table kernels described output channels axis axis. note neural networks output entity embeddings according section embedding model requires l-norms entity embeddings leads special normalization layer designed purpose. given output second last layer deﬁne last layer output embedding. it’s easy show throughout experiments found trick plays essential role making joint training whole framework work. describe training process section jointly train embedding model concept learning module together stochastic gradient descent mini-batch nesterov momentum using loss deﬁned equation entity embeddings given outputs concept learning module. mini-batch back-propagate error gradients neural network ﬁnally word vectors. relation embeddings also updated re-normalize iteration make l-norms stay since public datasets satisfy need built datasets test method make public research use. ﬁrst dataset based released dump natural language descriptions entities freebase stored relation /common/topic/description. refer dataset fbk-desc. dataset also freebase make much larger. fact include entities descriptions freebase remove triplets relations ﬁlter set. relations ﬁlter schema relations like /type/object/key. dataset entities call fbm-desc. statistics datasets presented table fbk-desc available http//ml.cs.tsinghua.edu.cn/˜ jiaxin/fbk desc.tar.gz fbm-desc available http//ml.cs.tsinghua.edu.cn/˜jiaxin/fbm desc.tar.gz note scale difference datasets. also differ splitting criteria. fbk-desc follows fbk’s original partition training validation test sets entities validation test sets already seen training set. fbm-desc goes contrary designed test concept learning ability framework. facts validation test sets include entity side seen training set. evaluated fbm-desc good embedding concept rely information natural language description knowledge transferred concept learning module. ﬁrst describe task link prediction. given relation entity side task predict entity side. natural reasoning procedure happens thoughts time. following previous work evaluation protocol task. test triplet removed replaced entities training turn. neural embedding model give scores corrupted triplets. rank correct entity stored. report mean predicted ranks test left mean rank. procedure repeated corrupting right mean rank. proportion correct entities ranked another index refer hits. test link prediction performance fbk-desc report table type concept learning module cnn. note triplets training validation test sets fbk-desc list transe’s results table. compared transe cannot make information descriptions model performs much better terms mean rank hits. stated section entities test contained training which together results shows framework well regularizes embedding model forcing embeddings reﬂect information natural language descriptions. demonstrate concept learning capability next subsection. shown section framework well regularizes neural embedding model memory storage. next fbm-desc evaluate capability framework learning concepts performing reasoning based learnt embeddings. report link prediction performance fbm-desc table note test contains millions triples time-consuming ranking-based evaluation. randomly sample triplets test report evaluation statistics. consistently outperforms terms mean rank hits. triplets test fbm-desc include entity unseen training side requiring model understand natural language descriptions reasoning based know traditional knowledge base embedding model compete task claims novelty framework. ajeyo assamese language drama directed jahnu barua ajeyo depicts struggles honest ideal revolutionary youth gajen keot fought social evils rural assam freedom movement india. best feature film assamese award national film awards /people/person/profession writer /people/person/profession author /people/person/gender female /people/person/nationality united states /film/film/country india finally show examples table illustrate framework’s capability learning concepts natural language descriptions. ﬁrst example framework able infer <lily burana profession author> sentence lily burana american writer. kind reasoning requires correct understanding original sentence knowledge writer author synonyms. third example limited information description framework hits correct facts almost purely based knowledge astronomy demonstrating robustness approach. present novel perspective knowledge base embeddings enables build framework concept learning capabilities large-scale based previous neural embedding models. evaluate framework newly constructed datasets freebase results show framework well regularizes neural embedding model give superior performance ability learn concepts newly learnt embeddings deal semantic tasks future work include consistently improving performance learnt concept embeddings large-scale datasets like fbm-desc. applications think framework promising solving problems unknown entities kb-powered dialog systems. dialog system users description meeting unknown entity natural behavior even human conversations.", "year": 2015}