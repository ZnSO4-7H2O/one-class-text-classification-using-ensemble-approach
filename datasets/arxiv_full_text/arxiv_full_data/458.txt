{"title": "Constructing a Natural Language Inference Dataset using Generative  Neural Networks", "tag": ["cs.AI", "cs.CL", "cs.NE"], "abstract": "Natural Language Inference is an important task for Natural Language Understanding. It is concerned with classifying the logical relation between two sentences. In this paper, we propose several text generative neural networks for generating text hypothesis, which allows construction of new Natural Language Inference datasets. To evaluate the models, we propose a new metric -- the accuracy of the classifier trained on the generated dataset. The accuracy obtained by our best generative model is only 2.7% lower than the accuracy of the classifier trained on the original, human crafted dataset. Furthermore, the best generated dataset combined with the original dataset achieves the highest accuracy. The best model learns a mapping embedding for each training example. By comparing various metrics we show that datasets that obtain higher ROUGE or METEOR scores do not necessarily yield higher classification accuracies. We also provide analysis of what are the characteristics of a good dataset including the distinguishability of the generated datasets from the original one.", "text": "natural language inference important task natural language understanding. concerned classifying logical relation sentences. paper propose several text generative neural networks generating text hypothesis allows construction natural language inference datasets. evaluate models propose metric accuracy classiﬁer trained generated dataset. accuracy obtained best generative model lower accuracy classiﬁer trained original human crafted dataset. furthermore best generated dataset combined original dataset achieves highest accuracy. best model learns mapping embedding training example. comparing various metrics show datasets obtain higher rouge meteor scores necessarily yield higher classiﬁcation accuracies. also provide analysis characteristics good dataset including distinguishability generated datasets original one. keywords natural language inference natural language generation machine learning dataset construction generative neural network recurrent neural network challenge natural language inference also known recognizing textual entailment correctly decide whether sentence entails contradicts neutral respect another sentence classiﬁcation task requires various natural language comprehension skills. paper focused following natural language generation task based nli. given premise goal generate stream hypotheses comply label addition reading capabilities task also requires language generation capabilities. stanford natural language inference corpus dataset contains half million examples. size dataset suﬃcient train powerful neural networks. several successful classiﬁcation neural networks already proposed paper utilize snli train generative neural networks. example dataset consist human-written sentences premise hypothesis corresponding label describes relationship them. examples presented table proposed generative networks trained generate hypothesis given premise label allow construct unseen examples. generative models build generate single optimal response given input. models applied machine translation image caption generation dialogue systems another type premise person throwing yellow ball air. ball sails person throwing yellow ball air. person throws square person throwing yellow ball air. ball heavy generative models autoencoders generate stream random samples original distribution. instance autoencoders used generate text images setting combine approaches generate stream random responses comply input good stream hypotheses? argue good stream contains diverse comprehensible accurate non-trivial hypotheses. hypothesis comprehensible grammatical semantically makes sense. accurate clearly expresses relationship premise. finally non-trivial trivial determine relationship hypothesis premise. instance given premise drives label entailment hypothesis drives trivial person sitting vehicle. next question automatically measure quality generated hypotheses. metrics standard text generation tasks instance rouge bleu meteor. metrics estimate similarity generated text original reference text. task used comparing generated reference hypotheses premise label. main issue metrics penalize diversity since penalize generated hypotheses dissimilar reference hypothesis. alternative metric classiﬁer test generated hypothesis input label correct respect premise. perfect classiﬁer would penalize diverse hypotheses would reward accurate comprehensible hypotheses. however would reward non-trivial hypotheses. based hypothesis propose following approach evaluation generative models also presented figure first generative model trained original training dataset. then premise label example original dataset taken input generative model generate random hypothesis. generated hypothesis combined premise label form unseen example. done every example original dataset construct dataset. next classiﬁer trained dataset. finally classiﬁer evaluated original test set. accuracy classiﬁer proposed quality metric generative model. compared accuracy classiﬁer trained original training tested original test set. generative models learn solely original training regenerate dataset. thus model learns distribution original dataset. furthermore generated dataset random sample estimated distribution. determine well generative model learn distribution observe close accuracy classiﬁer trained generated dataset approach accuracy classiﬁer trained original dataset. ﬂagship generative network embeddecoder works similar fashion encoder-decoder networks encoder used transform input low-dimensional latent representation decoder reconstructs input. diﬀerence embeddecoder consists decoder latent representation learned embedding training example separately. main contributions novel generative neural network consist decoder learns mapping embedding training example separately procedure generating datasets automatically iii) novel evaluation metric generative models accuracy classiﬁer trained generated dataset. section present related work. section considered neural networks presented. besides main generative networks also present classiﬁcation discriminative networks used evaluation. results presented section generative models evaluated compared. experiments best dataset generated attention-based model embeddecoder. classiﬁer dataset achieved accuracy less accuracy achieved original dataset. also investigate inﬂuence latent dimensionality performance compare diﬀerent evaluation metrics provide deeper insights generated datasets. conclusion presented section focal point recognizing textual entailment challenges goal determine premise entails hypothesis not. proposed approaches include bag-of-words matching approach matching predicate argument structure approach logical inference approach another rule-based inference approach proposed bar-haim approach allows generation hypotheses transforming parse trees premise maintaining entailment. hickl proposes approach constructing training datasets extracting sentences news articles tend entailment relationship. snli dataset released several neural network approaches classiﬁcation emerged. state-of-the-art model achieves accuracy snli dataset. similar generation approach proposed kolesnyk goal work generating entailment inference chains examples entailment label used. task unstructured text label. side spectrum tasks deal solely unstructured text like machine translation summarization conversational dialogue systems another recently popular task generating captions images advancement deep learning many neural network approaches introduced generating sequences. recurrent neural network language model simplest neural architectures generating text. approach extended encoder-decoder architecture generate sequence input sequence. hierarchical recurrent encoder-decoder architecture generates sequences several input sequences. models oﬀer little variety output sequences. obtained modeling output distribution language model. introduce variety models based variational autoencoder proposed. models stochastic random variables source variety. latent variable used initial generates sentences variational recurrent neural network models dependencies latent variables across subsequent steps rnn. latent variable hierarchical recurrent encoder-decoder extends hred incorporating latent variables learned similarly vae. latent variables like models used represent mappings sequences. conditional variational autoencoders used generate images continuous visual attributes. attributes conditional information models like discrete label models. recognized evaluation metrics text-generating models fall evaluation three categories manual evaluation automatic evaluation metrics task-based evaluation. based human judgment generated textual example inspected manually. automatic evaluation metrics like rouge bleu meteor compare human texts generated texts. shows meteor strongest correlation human judgments image description evaluation. last category task-based evaluation impact generated texts particular task measured. type evaluation usually involves costly lengthy human involvement like measuring eﬀectiveness smoking-cessation letters hand task evaluation classiﬁcation automatic. ranking used automatic task-based evaluation associating images captions. section present several neural networks used experiments. start variants recurrent neural networks essential layers models. then present classiﬁcation networks needed evaluation generative neural networks presented following section. next present generative networks generate hypothesis. finally present discriminative networks used evaluation analysis hypotheses. represented word embeddings respectively. e-dimensional vector represents corresponding word length premise length hypothesis. labels represented -dimensional vector label output model label input model. recurrent neural networks neural networks suitable processing sequences. basic building block networks. variants rnns long short term memory network attention-based extension lstm mlstm lstm tends learn long-term dependencies better vanilla rnns. input lstm sequence vectors output sequence sigmoid function element-wise multiplication operator rd×e rd×d parameter matrices parameter vectors input vector dimension output vector dimension. vectors zero standard setting however cases models value result previous layers. mlstm attention-based model input sequences premise hypothesis case nli. word premise matched word hypothesis soft alignment sentences. mlstm based lstm remembers important matches forgets less important. input lstm inside mlstm time step attention vector represents weighted premise sequence weights present degree token premise aligned t-th token concatenation operator. details mlstm presented hidden states models d-dimensional unless otherwise noted. hidden states input mlstm layer. output mlstm hidden states although last state used. fully connected layer transforms -dimensional vector softmax function applied obtain probabilities labels. goal proposed generative models generate diverse stream hypotheses given premise label. section present four variants generative models variants embeddecoder model presented figure variants encoderdecoder model presented figure figure generative models architecture. rounded boxes represent trainable parameters blue boxes inputs green boxes outputs orange represents mapping embeddings. -hypo denotes shifted <null>-started hypothesis. note encoderdecoder model latent representation hidden layer emebeddecoder trainable parameter matrix. models learn latent representation represents mapping premise label side hypothesis side. embeddecoder models learn latent representation learning embedding mapping training example separately. embedding i-th training example z-dimensional trainable parameter vector. consequentely rn×z attembeddecoder presented figure attention based variant embeddecoder. mlstm layer used classiﬁcation model. however initial cell state mlstm constructed latent vector label input. premise hypothesis ﬁrst processed lstm mlstm like classiﬁcation model however hypothesis shifted. ﬁrst word hypothesis input empty token <null> symbolizing empty input sequence predicting ﬁrst word. output mlstm hidden state represents output word. obtain probabilities words vocabulary ﬁrst transformed vocabulary-sized vector softmax function applied. size vocabulary. large size vocabulary two-level hierarchical softmax used instead regular softmax reduce number parameters updated training step. <null> generating step ignored. embeddecoder model without attention baseembeddecoder mlstm replaced regular lstm. input lstm shifted hypothesis. premise provided initial cell state speciﬁcally last hidden state premise merged class input latent representation lstm. also present variants encoderdecoder models regular baseencodedecoder regularized varencoderdecoder based variational bayesian approach. presented figure information available encoder whose output latent representation hand decoder provided premise label hypothesis shifted. forces encoder learn encode missing information mapping premise-label pair hypothesis. encoder similar structure classiﬁcation model figure except label connected initial cell state mlstm varencoderdecoder models based variational autoencoder instead using single points latent representation previous models latent representation varencoderdecoder presented continuous variable thus mappings presented soft elliptical regions latent space instead single points forces model latent space calculated form output encoder using diﬀerent fully connected layers. generation phase decoder trained generative model used. generates hypothesis given premise label randomly selected latent vector single word generated step becomes hypothesis input next step. also used beam search optimize hypothesis generation. similarly small number hypotheses generated given single input best selected. k-beam search time step best partial hypotheses expanded words vocabulary producing partial hypothesis. best partial hypotheses selected next step according joint probability partial hypothesis. thus procedure presented generation ends <null> symbol encountered maximum hypothesis length discriminative model used measure distinguishability original human written sentences generated ones. higher error rate model means generative distribution similar original distribution goals generative model. model based generative adversarial nets single network generative part tires trick discriminative part generating images similar original images discriminative part tries distinguish original generated images. discreteness words diﬃcult connect discriminative generative part single diﬀerentiable network thus construct separately. generative models already deﬁned section deﬁne discriminative model. training step original sequence xoriginal generated sequence xgenerated processed discriminative model. optimization function maximizes following objective construct dataset ﬁrst generative model trained training original dataset. then dataset constructed generating hypotheses generative model. premises labels examples original dataset taken input generative model. hypotheses replace training hypotheses dataset. experiments performed snli dataset. examples dataset divided training development test set. development test contain around examples. examples labeled means enough consensus them. examples excluded. also speed computation excluded examples premise longer words hypothesis longer words. still remaining examples. premises hypothesis padded <null> symbols premises consisted words hypotheses consisted tokens. -dimensional word vectors trained glove words withpretrained embeddings embeddings randomly selected normal distribution. word embeddings updated training. generative models trained epochs since turned none stopping criteria useful. generative model dataset created. dataset consists training generated using examples original training development generated original development set. beam size beam search details decision presented section datasets constructed ﬁltering generated datasets according various thresholds. thus generated datasets constructed contain enough examples ﬁltered datasets least number examples original dataset. datasets trimmed size original dataset selecting samples sequentially beginning dataset right size. also datasets ﬁltered labels represented equally. models including classiﬁcation discriminative models trained hidden dimension unless otherwise noted. first classiﬁcation model origclass trained original dataset. model used throughout experiments ﬁltering datasets comparison etc. notice assumed origclass ground truth purpose experiments. however accuracy model original test less attained mlstm model models similar including experimental settings however trained evaluated slightly smaller dataset. trained used generate datasets. couple generated examples presented table figure shows accuracies generated development datasets evaluated origclass. maximum accuracy achieved embeddecoder accuracy decreasing number dimensions latent variable. analysis label shows accuracy contradiction neutral labels quite stable accuracy entailment examples drops http//nlp.stanford.edu/data/glove.b.zip suggested beta http//keras.io latent dimension largest dimension reduction dimensionality equation therefore person throwing yellow ball air. someone playing basketball. neutral contradiction person sleeping chair. entailment neutral contradiction person sitting bleachers. entailment neutral contradiction person reading bank london. entailment neutral contradiction women playing basketball. entailment neutral contradiction women naked. entailment neutral contradiction women gossiping sandy beach. entailment signiﬁcantly latent dimensionality. reason hypothesis space entailment label smaller spaces labels. thus dimensionality higher creative examples generated examples less often comply entailment label. since none generated datasets’ accuracies high accuracy origclass original test used origclass ﬁlter datasets subject various prediction thresholds. examples generated dataset classiﬁed origclass probability label example exceeded threshold example retained. ﬁltered dataset classiﬁer trained. figure shows accuracies classiﬁers original test set. filtering examples incorrect labels improves accuracy classiﬁer. however threshold high accuracy drops since dataset contains examples trivial. figure represents accuracy classiﬁers corresponding generated development sets shows trade-oﬀ accuracy triviality examples. classiﬁers trained datasets latent dimension high ﬁltering threshold higher accuracies. notice training dataset test dataset generated generative model. unﬁltered datasets evaluated metrics besides classiﬁcation accuracy. results presented figure whole ﬁgure shows eﬀect latent dimensionality models diﬀerent metrics. main purpose ﬁgure show absolute values metrics compare metrics’ curves curve main metric accuracy classiﬁer. ﬁrst metric premise-hypothesis distance represents average jaccard distance premise generated hypothesis. datasets generated latent dimensions hypotheses similar premises indicates generated hypotheses trivial less diverse hypothesis generated higher latent dimensions. also evaluated models standard language generation metrics rouge-l meteor. metrics negatively correlated accuracy classiﬁer. believe metrics reward hypotheses similar reference hypothesis. however classiﬁer better trained diverse hypotheses. figure accuracies unﬁltered generated datasets classiﬁed origclass. dataset generated generative model diﬀerent latent dimension dataset examples classiﬁed origclass. predicted labels taken golden truth compared labels generated dataset measure accuracy. accuracies measured labels together label separately. unﬁltered datasets datasets created ﬁltering according various prediction thresholds also represent chart lines. classiﬁer trained datasets. point represents accuracy single classiﬁer. classiﬁers evaluated original test figure classiﬁer evaluated corresponding generated development figure next metric log-likelihood hypotheses development set. metric negative training loss function. log-likelihood improves dimensionality since easier hypotheses training step dimensions. consequently hypothesis generating step conﬁdent lower log-likelihood. last metric discriminative error rate calculated discriminative model. model trained hypotheses unﬁltered generated dataset side original hypotheses side. error rate calculated development sets. higher error rate indicates diﬃcult discriminative model distinguish generated original hypotheses suggests original generating distribution distribution generative model similar. discriminative model detects dimensional generative models generate trivial examples also indicated distance premise hypotheses. hand also detects hypotheses high dimensional models frequently contain grammatic semantic errors. positive correlation discriminative error rate accuracy classiﬁer. observation experiment generated dataset ﬁltered according prediction probability discriminative model. disjoint ﬁltered datasets created. hypotheses high probability come original distribution probability. however accuracies classiﬁers trained datasets similar accuracy classiﬁer unﬁltered dataset. similar test also done log-likelihood metric. examples higher log-likelihood similar performance ones lower log-likelihood. accept hypothesis section shown quality dataset requires accurate examples showing ﬁltering dataset original classiﬁer improves performance next shown non-trivial examples also required. ﬁltering threshold high examples excluded accuracy drops. also trivial examples produced low-dimensional models indicated lower premise-hypothesis distances lower discriminative error rate finally quality dataset requires comprehensible examples. high dimensional models produce less comprehensible hypotheses. detected discriminative model also compared attembeddecoder model models. table presents results. models latent dimension previously shown best dimensions. models number total parameters relatively high however portion parameters updated time. attembeddecoder model best model according main metric accuracy classiﬁer trained generated dataset. hidden dimension baseembeddecoder selected model comparable attembeddecoder terms number parameters accuracies classiﬁers generated baseembeddecoder still lower accuracies classiﬁers generated attembeddecoder shows attention mechanism helps models. table shows performance generated datasets compared original one. best generated dataset generated attembeddecoder. accuracy classiﬁer lower accuracy classiﬁer generated original human crafted dataset. comparison best generated dataset original dataset shows datasets identical examples. average length hypothesis original dataset generated dataset respectively. another experiment generated dataset original dataset merged train classiﬁer. thus merged dataset contained twice many examples datasets. accuracy classiﬁer better classiﬁer trained solely original training set. however lowest average loss achieved classiﬁer trained original dataset. figure comparison unﬁltered generated datasets using various metrics. dataset generated model diﬀerent latent dimension metric applied dataset. metrics classiﬁer accuracy discriminator error rate metric applied example average calculated dataset. number parameters updated training example. thus hierarchical softmax latent representation parameters excluded measure. columns acc. acc. represent accuracy classiﬁer trained unﬁltered dataset dataset ﬁltered threshold respectively. column acc-data presents accuracy unﬁltered development dataset evaluated origclass. column presents negative log-likelihood unﬁltered development dataset. error rates discriminative models presented disc-er. table performance classiﬁers trained original generated datasets. classiﬁers tested original test set. generated datasets generated models table generated datasets ﬁltered threshold also qualitative evaluation generated hypothesis. hypotheses mostly grammatically sound. sometimes models incorrectly indeﬁnite articles instance phone possessive pronouns uses umbrella. fact system must learn right indeﬁnite article every word separately. hand models sometimes generate hypotheses showcase advanced grammatical patterns. instance hypothesis woman cake family shows model correctly plural non-trivial setting. generative neural networks tendency repeat words sometimes make sentences meaningless like drinking coﬀee even ungrammatical like several people shown previously larger latent dimension creative hypotheses generated. however creativity semantic errors emerge. hypotheses correct unlikely written human like shirtless holding guitar woman woman. others present improbable events like girls sitting park watching even impossible events instance child waiting wife. type errors arise models learned enough common sense logic. finally hypotheses make sense. instance women grassy beach tennis equipment. contrary models able generate non-trivial hypotheses. original premise band performing girl singing next singing well playing guitar model generated hypotheses contain concepts explicitly found premise. instance people playing instruments band entirely silent girl playing concert regarding compliance hypotheses label premise observed many generated hypotheses complying label however would good example diﬀerent label. instance generated hypotheses represent entailment instead contradiction. also explains accuracy generated dataset measured original classiﬁer figure hand models generate examples ambiguous clear original dataset. examples harder classify even human. instance relationship premise hitting baseball baseball ﬁeld hypothesis baseball player trying ball either interpreted either entailment verb intepreted miss discriminative error rates encoderdecoder models embeddecoder models table signiﬁcant. investigate experiment performed human evaluator discriminative model. time sample examples. recap model human asked select generated hypothesis given random original generated hypothesis without knowing which. human evaluation conﬁrms attembeddecoder hypotheses diﬃcult separate original hypotheses vaeencoderdecoder. table presents results. discriminative model discriminates better human evaluator. fact discriminative model learned large training human shown training examples. human evaluation shown generated hypotheses positively recognized contain grammatical semantic error. even generated hypothesis contain errors sometimes reveals sophisticated original example. hand discriminative model always recognize discrepancies. relies diﬀerences distributions learned form training set. true number non-distinguishable examples even higher indicated human discriminator error rate since human correctly guessed examples could distinguish. paper proposed several generative neural networks generating hypothesis using dataset. evaluate models propose accuracy classiﬁer trained generated dataset main metric. best model achieved accuracy less accuracy classiﬁer trained original human written dataset best dataset combined original dataset achieved highest accuracy. model learns decoder mapping embedding training example. outperforms standard encoder-decoder networks. although parameters needed trained less updated batch. also shown attention mechanism improves model. analysis conﬁrmed hypothesis good dataset contains accurate non-trivial comprehensible examples. examine quality generated hypothesis compared original human written hypotheses. discriminative evaluation shows cases human evaluator incorrectly distinguished original generated hypothesis. discriminative model actually better distinguishing. also compared accuracy classiﬁer metrics. standard text generation metrics rouge meteor indicate generated dataset good training classiﬁer. obtain higher accuracies generated datasets need ﬁltered generative models produce examples whose label always accurate. thus propose future work incorporating classiﬁer generative model similar fashion done images network could also include discriminative model generate examples distribution similar original training distribution. finally constructing dataset requires intensive manual work mainly consists writing text creativity. extend original dataset human users could validate correct generated examples. would like develop active learning methods identify incorrect generated examples would improve dataset corrected.", "year": 2016}