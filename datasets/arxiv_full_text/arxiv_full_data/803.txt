{"title": "Emergence of foveal image sampling from learning to attend in visual  scenes", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "We describe a neural attention model with a learnable retinal sampling lattice. The model is trained on a visual search task requiring the classification of an object embedded in a visual scene amidst background distractors using the smallest number of fixations. We explore the tiling properties that emerge in the model's retinal sampling lattice after training. Specifically, we show that this lattice resembles the eccentricity dependent sampling lattice of the primate retina, with a high resolution region in the fovea surrounded by a low resolution periphery. Furthermore, we find conditions where these emergent properties are amplified or eliminated providing clues to their function.", "text": "describe neural attention model learnable retinal sampling lattice. model trained visual search task requiring classiﬁcation object embedded visual scene amidst background distractors using smallest number ﬁxations. explore tiling properties emerge model’s retinal sampling lattice training. speciﬁcally show lattice resembles eccentricity dependent sampling lattice primate retina high resolution region fovea surrounded resolution periphery. furthermore conditions emergent properties ampliﬁed eliminated providing clues function. striking design feature primate retina manner images spatially sampled retinal ganglion cells. sample spacing receptive ﬁelds smallest fovea increase linearly eccentricity shown figure thus highest spatial resolution center ﬁxation lowest resolution periphery gradual fall-off resolution proceeds center periphery. question attempt address retina designed manner i.e. beneﬁcial vision? commonly accepted explanation eccentricity dependent sampling provides high resolution broad coverage visual ﬁeld limited amount neural resources. human retina contains million ganglion cells whose axons form sole output retina. essentially constitute distinct samples image multiplicity cell types coding different aspects channels packed uniformly highest resolution would subtend image area spanning deg. thus would high-resolution essentially tunnel vision. alternatively spread uniformly entire monocular visual ﬁeld spanning roughly would wide ﬁeld coverage blurry vision sample subtending thus primate solution makes intuitive sense achieve best worlds. however still lacking quantitative demonstration sampling strategy emerges optimal design subserving visual tasks. here explore optimal retinal sampling lattice attentional system performing simple visual search task requiring classiﬁcation object. propose learnable retinal sampling lattice explore properties best suited task. evolutionary pressure tuned retinal conﬁgurations found primate retina instead utilize gradient descent optimization in-silico model constructing fully differentiable dynamically controlled model attention. choice visual search task follows paradigm widely used study overt attention humans primates many forms task single target randomly located display among distractor objects. goal subject target rapidly possible. itti koch propose selection mechanism based manually neural attention models applied successfully variety engineering applications little work relating properties attention mechanisms back biological vision. important property distinguishes neural networks neurobiological models ability learn internal features directly data. existing neural network models specify input sampling lattice priori. larochelle hinton employ eccentricity dependent sampling lattice mimicking primate retina mnih utilize multi scale glimpse window’ forms piece-wise approximation scheme. seems reasonable think design choices contribute good performance systems remains seen arrangement emerges optimal solution. extend learning paradigm neural networks structural features glimpse mechanism attention model. explore emergent properties learned retinal conﬁgurations train artiﬁcial datasets factors variation easily controllable. despite departure biology natural stimuli model learns create eccentricity dependent layout distinct central region high acuity emerges surrounded acuity periphery. show properties layout highly dependent variations present task constraints. depart physiology augmenting attention model ability spatially rescale zoom input model learns uniform layout properties similar glimpse window proposed jaderberg gregor ﬁndings help understand task conditions constraints eccentricity dependent sampling lattice emerges. attention neural networks formulated terms differentiable feedforward function. allows parameters models trained jointly backpropagation. formulations visual attention input image assume structure kernel ﬁlters. example recent attention models proposed jaderberg mnih gregor assume kernel ﬁlter lies rectangular grid. create learnable retinal sampling lattice relax assumption allowing kernels tile image independently. interpret glimpse form routing subset visual scene sampled form smaller output glimpse routing deﬁned kernels kernel speciﬁes part input contribute particular output control variable centers kernel ﬁlter calculated respect control variables learnable offset control variables specify position zoom entire glimpse. specify position spread respectively individual kernel parameters learned training backpropagation. describe control variables computed next section. kernels thus speciﬁed follows assume kernel ﬁlters factorize horizontal vertical dimensions input image. factorization shown equation kernel deﬁned isotropic gaussian kernel ﬁlter given center scalar variance dimensional gaussian deﬁned input image shown figure gaussian kernel ﬁlters thought simpliﬁed approximation receptive ﬁelds retinal ganglion cells primates factored formulation reduces space possible transformations input output still form many different mappings input output figure shows possible windows input image mapped output yellow circles denote central location particular kernel size denotes standard deviation. kernel maps outputs positional control considered analogous motor control signals executes saccades whereas would correspond controlling zoom lens contrast training deﬁnes structural adjustments individual kernels include position lattice well variance. adjustments possible training ﬁxed afterwards.training adjustments considered analagous incremental adjustments layout retinal sampling lattice occur many generations directed evolutionary pressure biology. figure starting initial lattice conﬁguration uniform grid kernels learn optmized conﬁguration data. attentional ﬁxations generated inference model shown unrolled time prior training kernel ﬁlters initialized grid tiling uniformly central region input image creating retinal sampling lattice shown figure training. recurrent network frnn layer traditional recurrent network units. control network fcontrol fully-connected network units model shown figure differentiable trained end-to-end backpropagation time. note allows train control network indirectly signals backpropagated task cost. stochastic gradient descent optimization adam construct models theano example images dataset shown figure handwritten digits original mnist dataset lecun cortes randomly placed image varying amounts distractors distractors generated extracting random segments nontarget mnist digits placed randomly uniform probability image. contrast cluttered mnist dataset proposed mnih number distractors image varies randomly pieces. prevents attention model learning solution depends number ‘on’ pixels given region. addition create another dataset additional factor variation original mnist digit randomly resized factor examples dataset shown second figure deﬁne visual search task recognition task cluttered scene. recurrent attention model propose must output class single mnist digit appearing image prediction network fpredict. task loss speciﬁed equation minimize classiﬁcation error cross-entropy cost analolgous visual search experiments performed physiological studies pressure attention model accomplish visual search quickly possible. applying task loss every timepoint model forced accurately recognize localize target mnist digit iterations possible. classiﬁcation experiments model given glimpses. figure sampling lattice shown four different stages training translation model initial condition ﬁnal solution radius corresponds standard deviation kernel. figure learned sampling lattices four different model conﬁgurations. middle resolution bottom kernel standard deviation function eccentricity model conﬁguration. figure shows layouts learned kernels translation model different stages training. ﬁlters smoothly transforming uniform grid kernels eccentricity dependent lattice. furthermore kernel ﬁlters spread individual centers create sampling lattice covers full image. sensible target mnist digit appear anywhere image uniform probability. include variable sized digits additional factor dataset translation model shows even greater diversity variances kernel ﬁlters. shown visually ﬁrst figure furthermore second shows highly dependent relationship sampling interval standard deviatoin retinal sampling lattice eccentricity center. dependency increases training variable sized mnist digits proposed attention model able zoom retinal sampling lattice different layout emerges. much less diversity distribution kernel ﬁlter variances evidenced figure sampling interval standard deviation retinal sampling lattice less dependence eccentricity. shown last column figure also trained model variable sized digits noticed signiﬁcant differences sampling lattice conﬁguration. figure shows model variant makes retinal sampling lattice training. strategy variant adopts solve visual search task helps explain drastic difference lattice conﬁguration. translation variant simply translates high acuity region recognize localize target digit. translation zoom model rescales translates sampling lattice target digit. remarkably figure shows models detect digit early make minor corrective adjustments following iterations. table compares classiﬁcation performance model variant cluttered mnist dataset ﬁxed sized digits signiﬁcant drop performance retinal sampling lattice ﬁxed learnable conﬁrming model beneﬁtting learning high-acuity region. classiﬁcation performance translation translation zoom model competitive. supports hypothesis functionality high acuity region resolution periphery similar zoom. constrained glimpse window translate only similar kernels converge sampling lattice similar found primate retina layout composed high acuity region center surrounded wider region acuity. essen anderson postulate linear relationship eccentricity sampling interval leads form scale invariance primate retina. results translation model variable sized digits supports conclusion. additionally observe zoom appears supplant need learn high acuity region visual search task. implies high acuity region serves purpose resembling zoomable sampling lattice. acuity periphery used detect search target high acuity ‘fovea’ ﬁnely recognizes localizes target. results obtained admittedly simpliﬁed domain visual scenes point possibility using deep learning tool explore optimal sample tiling retinal data driven task-dependent manner. exploring results change challenging tasks naturalistic visual scenes future goal research. would like acknowledge everyone redwood center helpful discussion comments. gratefully acknowledge support nvidia corporation donation tesla gpus used research. fr´ed´eric bastien pascal lamblin razvan pascanu james bergstra goodfellow arnaud bergeron nicolas bouchard david warde-farley yoshua bengio. theano features speed improvements. arxiv preprint arxiv. hugo larochelle geoffrey hinton. learning combine foveal glimpses third-order boltzmann machine. advances neural information processing systems kelvin jimmy ryan kiros aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation visual attention. arxiv preprint arxiv.", "year": 2016}