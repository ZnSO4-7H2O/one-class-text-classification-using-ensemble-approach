{"title": "Semi-supervised Learning with GANs: Manifold Invariance with Improved  Inference", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Semi-supervised learning methods using Generative Adversarial Networks (GANs) have shown promising empirical success recently. Most of these methods use a shared discriminator/classifier which discriminates real examples from fake while also predicting the class label. Motivated by the ability of the GANs generator to capture the data manifold well, we propose to estimate the tangent space to the data manifold using GANs and employ it to inject invariances into the classifier. In the process, we propose enhancements over existing methods for learning the inverse mapping (i.e., the encoder) which greatly improves in terms of semantic similarity of the reconstructed sample with the input sample. We observe considerable empirical gains in semi-supervised learning over baselines, particularly in the cases when the number of labeled examples is low. We also provide insights into how fake examples influence the semi-supervised learning procedure.", "text": "semi-supervised learning methods using generative adversarial networks shown promising empirical success recently. methods shared discriminator/classiﬁer discriminates real examples fake also predicting class label. motivated ability gans generator capture data manifold well propose estimate tangent space data manifold using gans employ inject invariances classiﬁer. process propose enhancements existing methods learning inverse mapping greatly improves terms semantic similarity reconstructed sample input sample. observe considerable empirical gains semi-supervised learning baselines particularly cases number labeled examples low. also provide insights fake examples inﬂuence semi-supervised learning procedure. deep generative models well prescribed become widely popular generative modeling data. generative adversarial networks particular shown remarkable success generating realistic images several cases generator seen learning nonlinear parametric mapping data manifold. applications interest dim. distribution space combined mapping induces distribution space sample distribution obtained ancestral sampling i.e. gans adversarial training discriminator approximates divergence measure real data distribution solving optimization problem generator tries minimize also seen another perspective discriminator tries tell apart real examples fake examples minimizing appropriate loss function generator tries generate samples maximize loss primary motivations studying deep generative models semi-supervised learning. indeed several recent works shown promising empirical results semi-supervised learning implicit well prescribed generative models state-ofthe-art semi-supervised learning methods using gans discriminator classiﬁer outputs probabilities generator trained produces realistic images argued capture data manifold well whose properties used semi-supervised learning. particular tangent spaces manifold inform desirable invariances wish inject classiﬁer work make following contributions propose tangents generator’s mapping automatically infer desired invariances improve semi-supervised learning. contrasted methods assume knowledge invariances estimating tangents real sample requires learn encoder maps data latent space i.e. propose enhancements existing methods learning encoder improve semantic match counter problem class-switching. further provide insights workings based semi-supervised learning methods existing methods semi-supervised learning using gans modify regular discriminator outputs corresponding real classes cases output corresponds fake samples generator generator mainly used source additional data discriminator tries classify label. propose generator obtain tangents image manifold inject invariances classiﬁer earlier work used contractive autoencoders estimate local tangent space point caes optimize regular autoencoder loss augmented additional -norm penalty jacobian encoder mapping. rifai intuitively reason encoder trained fashion sensitive tangent directions dominant singular vectors jacobian encoder tangents. this however involves extra computational overhead every training sample avoid based approach. gans also established generate better quality samples prescribed models like vaes hence argued learn accurate parameterization image manifold. trained generator serves parametric mapping dimensional space manifold embedded higher dimensional space open subset open subset standard topologies respectively surjective range restricted assume smooth injective mapping embedded manifold. jacobian function matrix partial derivatives jacobian provides mapping tangent space tangent space i.e. txx. noted isomorphic isomorphic however mapping surjective range restricted tangent space manifold denoted gans capable generating realistic samples argue approximates true data manifold well hence tangents obtained using close tangents true data manifold. problem learning smooth manifold ﬁnite samples studied literature interesting problem right study manifold approximation error gans minimize chosen divergence measure data distribution fake distribution using ﬁnite samples however outside scope current work. given data sample need corresponding latent representation tangents manifold current discussion assume availability so-called encoder deﬁnition write avoid unnecessary burden manifold terminologies still technically correct. also enables jacobian regular matrix rd×d instead working differential taken jacobian generator used tangent directions manifold point following lemma speciﬁes conditions existence encoder shows encoder also used tangent directions. later come back issues involved training encoder. lemma jacobian full rank locally invertible open neighborhood exists smooth case jacobian spans tangent space proof. refer reader standard textbooks multivariate calculus differentiable manifolds ﬁrst statement lemma second statement easily deduced looking jacobian composition functions id×d since implies span coincides column span jzg. columns span tangent space rows jxh. training inverse mapping estimate tangents given real data point need corresponding latent representation ideal scenario. however practice learn approximation true data manifold mapping like projection manifold yielding approximation error. projection orthogonal i.e. nearest point nevertheless desirable semantically close least class label preserved mapping studied following three approaches training inverse regard desideratum decoupled training. similar approach outlined donahue generator trained ﬁrst ﬁxed thereafter encoder trained optimizing suitable reconstruction loss space approach yield good results observe time semantically similar given real sample change class label. reasons noted donahue encoder never sees real samples training. address this also experimented combined objective minh lz)) lh)) however yield signiﬁcant improvements early explorations. bigan. donahue propose jointly train encoder generator using adversarial training pair considered fake example pair considered real example discriminator. similar approach proposed dumoulin gives parameters posterior stochastic sample posterior paired taken real example. bigan work modiﬁcation feature matching loss discriminator i.e. ezf) optimize generator encoder found greatly help convergence observe better results terms semantic match decoupled training approach however still observe considerable fraction instances class changed augmented-bigan. address still-persistent problem class-switching reconstructed samples propose construct third pair also considered discriminator fake example addition augmented-bigan objective given ex∼px probability pair real example assigned discriminator optimize discriminator using objective generator encoder optimized using feature matching loss intermediate layer discriminator i.e. ezf) help convergence. minimizing note recently proposed methods training gans based integral probability metrics make similar case bigan however discriminator tries make features layer difﬁcult achieve directly optimizing third term objective results improved semantic similarity empirically evaluate approaches regard similarity quantitatively qualitatively observing augmented-bigan works signiﬁcantly better bigan. note also problems semantic mismatch class switching reconstructed samples reported authors stochastic version proposed third term objective potentially help well investigation left future work. trained encoder good approximation good approximation either estimate tangent space. speciﬁcally columns rows directions approximately span tangent space data manifold almost deep learning packages implement reverse mode differentiation computationally cheaper forward mode differentiation computing jacobian output dimension function hence experiments tangents. approximation errors several places preferable consider dominant tangent directions span jxh. obtained using matrix taking right singular vectors corresponding singular values done trained using contractive auto-encoder. however process expensive needs done independently every data sample. adopt alternative approach dominant tangent direction take pre-trained model encoder-generator-discriminator triple insert extra functions learned optimizing minp exg)− g))) x−)) x−))))] kept ﬁxed pre-trained model. note discriminator pipelines latent data respectively share parameters last layers last layer loss. enables learn nonlinear approximation space g))) close jacobian estimate dominant tangent directions tangent propagation approach make classiﬁer invariant estimated tangent directions previous section. apart form regular classiﬁcation loss jxic rk×d jacobian classiﬁer function tangent directions want classiﬁer invariant term penalizes linearized variations classiﬁer output along tangent directions. simard tangent directions using slight rotations translations images whereas estimate tangents data manifold. step make classiﬁer invariant small perturbations directions emanating point leads regularizer dual norm j-norm. reduces squared frobenius norm jacobian matrix penalty training results approximation data manifold. hence ﬁrst learn approximate smooth manifold parameterized generator using dominant tangent directions classiﬁer outputs approximately estimates yields using gradient kl||c] calling virtual adversarial example uses kl||c] regularizer classiﬁer objective. replace kl-divergence total-variation distance optimize ﬁrst-order approximation becomes equivalent regularizer practice computationally expensive optimize jacobian based regularizers. hence experiments stochastic ﬁnite difference approximation jacobian based regularizers. tangentprop randomly sampled tangents every time example visited sgd. jacobian-norm regularizer every time example visited approximates upper bound expectation recent works used gans semi-supervised learning discriminator also serves classiﬁer semi-supervised learning problem classes discriminator outputs output corresponding fake examples originating generator gan. loss discriminator given term probability fake example probability real example loss component unsup regular discriminator loss modiﬁcation probabilities real fake compiled outputs. salimans proposed training generator using feature matching generator minimizes mean discrepancy features real fake examples obtained intermediate layer discriminator i.e. ezf) using feature matching loss generator empirically shown result much better accuracy semi-supervised learning compared training methods including minibatch discrimination regular generator loss attempt develop intuitive understanding fake examples inﬂuence learning classiﬁer feature matching loss work much better semi-supervised learning compared regular gan. term classiﬁer discriminator interchangeably based context however really network mentioned earlier. following assume logit ﬁxed subtracting term logits change softmax probabilities. rewriting unlabeled loss terms logits minimizing unsup move parameters decrease increase rate increase also modulated results warping functions around real example warping around examples current model conﬁdent belong class becomes locally concave around real examples loosely scattered around consider following three cases weak fake examples. fake examples coming generator weak hence gradient rendering unlabeled data almost useless semi-supervised learning. strong fake examples. fake examples strong imax note case would smaller since product probabilities. examples close imax maxi maxi optimization push limax amount pull limax larger amount. want consider cases here classiﬁer enough capacity classiﬁer enough capacity make curvature limax around really high locally concave around since close. results over-ﬁtting around unlabeled examples test example closer model likely misclassify controlled-capacity classiﬁer suppose capacity classiﬁer controlled adequate regularization. case curvature function limax around cannot increase beyond point. however results limax pulled optimization process since pronounced examples classiﬁer conﬁdent although still assigning highest probability class imax) since becomes higher. examples entropy distribution {p}k actually increase training proceeds hurt test performance. moderate fake examples. fake examples generator neither weak strong current discriminator unsupervised gradient push limax pulling limax down giving rise moderate curvature around real examples since sufﬁciently apart results smooth decision function around real unlabeled examples. again curvatures around classes current classiﬁer trust example affected much. further less case fake examples strong. similarly less case strong fake examples. hence norm gradient lower contribution unlabeled data overall gradient lower case strong fake examples. intuitively seems beneﬁcial classiﬁer gets ample opportunity learn supervised loss conﬁdent right class unlabeled examples boost conﬁdence slowly using gradient training proceeds. experimented regular loss feature matching loss generator plotting several quantities interest discussed mnist svhn datasets fig.. generator trained feature matching loss corresponds case moderate fake examples discussed generator trained regular loss corresponds case strong fake examples discussed above. plot aimax imax max≤i≤k =imax≤k separately look behavior imax logit. similarly plot exbt separately true label unlabeled example quantities plots selfexplanatory. expected unlabeled loss unsup regular becomes quite high early implying fake examples strong. aimax also higher regular pointing towards case strong fake examples controlled-capacity classiﬁer discussed above. indeed average entropies distributions much lower feature-matching compared regular test errors mnist regular fm-gan respectively. test errors svhn respectively. also emphasized semi-supervised learning heavily depends generator dynamically adapting fake examples current discriminator observed freezing training generator point results discriminator able classify easily thus stopping contribution unlabeled examples learning. ﬁnal loss semi-supervised learning. feature matching semi-supervised loss classiﬁer objective incorporate invariances sec. ﬁnal objective discriminator third term objective makes classiﬁer decision function change slowly along tangent directions around real example mentioned sec. stochastic ﬁnite difference approximation jacobian terms computational reasons. experiments implementation details. architecture endoder generator discriminator closely follow network structures remove stochastic layer encoder deterministic). estimating dominant tangents employ fully connected two-layer network tanh non-linearly hidden layer represent output taken hidden layer. batch normalization replaced weight normalization modules make output dependent given input whole minibatch. necessary make jacobians independent examples minibatch. replaced relu nonlinearities encoder generator exponential linear units ensure smoothness functions follow completely optimization learning rates generators models trained using feature matching loss. also experimented minibatch-discrimination minibatch features suited classiﬁcation prediction example adversely affected features examples indeed notice training error md-gan regular fm-gan. md-gan gave similar test error regular-gan. figure comparing bigan augmented bigan based classiﬁcation error reconstructed test images. left column cifar right column svhn. images corresponds original images followed bigan reconstructions middle augmented bigan reconstructions bottom row. images found appendix. figure visualizing tangents. cifar bottom svhn. rows tangents using method estimating dominant tangent space. even rows tangents using jxh. first column original image. second column reconstructed image using third column reconstructed image using columns tangents using encoder. columns tangents using generator. semantic similarity. image samples reconstructions bigan augemented-bigan seen fig. quantitatively measure semantic similarity reconstructions original images learn supervised classiﬁer using full training obtain classiﬁcation accuracy reconstructions test images. architectures classiﬁer cifar svhn similar corresponding discriminator architectures have. lower error rates augmented-bigan suggest leads reconstructions reduced class-switching. tangent approximations. tangents cifar svhn shown fig. show visual comparison tangents jp)g followed dominant tangents. seen proposed method getting dominant tangent directions gives similar tangents svd. tangents generator look different tangents encoder though trace boundaries objects image also empirically quantify method dominant tangent subspace estimation estimation computing geodesic distances principal angles estimations. results shown table semi-supervised learning results. table shows results svhn cifar various number labeled examples. experiments tangent regularizer cifar svhn tangents. hyperparameters obtain signiﬁcant improvements baselines particularly svhn case table test error semi-supervised learning svhn cifar- results proposed methods obtained training model epochs svhn epochs cifar averaged runs. table dominant tangent subspace approximation quality columns show geodesic distance principal angles subspaces. shows results randomly sampled -dimensional subspaces -dimensional space middle bottom rows show results dominant subspace obtained using dominant subspace obtained using method cifar- svhn respectively. numbers averages randomly sampled test examples. labeled examples. good results cifar fact encoder cifar still able approximate inverse generator well hence tangents good enough. think obtaining better estimates tangents cifar potential improving results. accuracy cifar also close results however results obtained running optimization epochs slower learning rate mentioned temporal ensembling using explicit data augmentation assuming knowledge class-preserving transformations input method estimates transformations data manifold form tangent vectors. outperforms method signiﬁcant margin cifar- could fact uses horizontal ﬂipping based augmentation cifar- cannot learned tangents non-smooth transformation. temporal ensembling conjunction method potential improving semi-supervised learning results. discussion empirical results show using tangents data manifold inject invariances classiﬁer improves performance semi-supevised learning tasks. particular observe impressive accuracy gains svhn tangents obtained good quality. also observe improvements cifar though impressive svhn. think improving quality tangents cifar potential improving results there direction future explorations. also shed light effect fake examples common framework used semi-supervised learning gans discriminator predicts real class labels along fake label. explicitly controlling difﬁculty level fake examples hence indirectly effective semi-supervised learning another direction future work. possible distortion model real examples whose strength controlled effective semi-supervised learning. bernstein kuleshov. data-based manifold reconstruction tangent bundle manifold learning. icml- topological methods machine learning workshop beijing volume pages guangliang chen anna little mauro maggioni lorenzo rosasco. recent advances multiscale geometric analysis point clouds. wavelets multiscale analysis pages springer goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems pages diederik kingma shakir mohamed danilo jimenez rezende welling. semisupervised learning deep generative models. advances neural information processing systems pages youssef mroueh stephen voinea tomaso poggio. learning group invariant features kernel perspective. advances neural information processing systems pages sebastian nowozin botond cseke ryota tomioka. f-gan training generative neural samplers using variational divergence minimization. advances neural information processing systems pages alec radford luke metz soumith chintala. unsupervised representation learning deep convolutional generative adversarial networks. arxiv preprint arxiv. antti rasmus mathias berglund mikko honkala harri valpola tapani raiko. semisupervised learning ladder networks. advances neural information processing systems pages salimans goodfellow wojciech zaremba vicki cheung alec radford chen. improved techniques training gans. advances neural information processing systems cicero nogueira santos kahini wadhawan bowen zhou. learning loss functions semi-supervised learning discriminative adversarial networks. arxiv preprint arxiv. patrice simard yann lecun john denker bernard victorri. transformation invariance pattern recognition—tangent distance tangent propagation. neural networks tricks trade pages springer figure cifar tangents. rows tangents using method estimating dominant tangent space. even rows tangents using jxh. first column original image. second column reconstructed image using third column reconstructed image using columns tangents using encoder. columns tangents using generator. figure svhn tangents. rows tangents using method estimating dominant tangent space. even rows tangents using jxh. first column original image. second column reconstructed image using third column reconstructed image using columns tangents using encoder. columns tangents using generator. figure cifar reconstructions comparing bigan reconstructions augmented-bigan reconstructions. shows original images followed bigan reconstructions augmented-bigan reconstructions row. reconstructions shown total randomly sampled images test test set. figure svhn reconstructions comparing bigan reconstructions augmented-bigan reconstructions. shows original images followed bigan reconstructions augmented-bigan reconstructions row. reconstructions shown total randomly sampled images test", "year": 2017}