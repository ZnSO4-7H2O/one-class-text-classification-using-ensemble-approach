{"title": "Learning Distributed Representations of Texts and Entities from  Knowledge Base", "tag": ["cs.CL", "cs.NE"], "abstract": "We describe a neural network model that jointly learns distributed representations of texts and knowledge base (KB) entities. Given a text in the KB, we train our proposed model to predict entities that are relevant to the text. Our model is designed to be generic with the ability to address various NLP tasks with ease. We train the model using a large corpus of texts and their entity annotations extracted from Wikipedia. We evaluated the model on three important NLP tasks (i.e., sentence textual similarity, entity linking, and factoid question answering) involving both unsupervised and supervised settings. As a result, we achieved state-of-the-art results on all three of these tasks. Our code and trained models are publicly available for further academic research.", "text": "describe neural network model jointly learns distributed representations texts knowledge base entities. given text train proposed model predict entities relevant text. model designed generic ability address various tasks ease. train model using large corpus texts entity annotations extracted wikipedia. evaluated model three important tasks involving unsupervised supervised settings. result achieved state-of-the-art results three tasks. code trained models publicly available academic research. methods capable learning distributed representations arbitrary-length texts sentences paragraphs recently attracted considerable attention methods learn generic representations useful across domains similar word embedding methods wordvec glove base wikipedia freebase. methods encode information entities continuous vector space. shown effective various kb-related tasks entity search entity linking link prediction paper describe novel method bridge different approaches. particular propose neural text-entity encoder neural network model jointly learn distributed representations texts entities. every text model aims predict relevant entities places text relevant entities close continuous vector space. humanedited entity annotations obtained wikipedia supervised data relevant entities texts containing annotations. note that entities conventionally used model semantics texts. representative example explicit semantic analysis represents semantics text using sparse vector space dimension corresponds relevance score text entity. essentially shows text accurately represented using small relevant entities. based entity annotations wikipedia viewed supervised data relevant entities wikipedia instructs contributors create annotations relevant manual https//en.wikipedia.org/ wiki/wikipediamanual_of_style model given text train model predict entities appear formally probability represents likelihood entity appearing deﬁned following softmax function entities vector representations entity text respectively. compute using element-wise word vectors normalization fully connected layer. denote vector word vectors fact hypothesize annotations relevant entities supervised data learning text representations. furthermore also consider placing texts entities vector space enables easily compute similarity texts entities beneﬁcial various kb-related tasks. order test hypothesis conduct three experiments involving unsupervised supervised tasks. first standard semantic textual similarity datasets evaluate quality learned text representations method unsupervised fashion. result method clearly outperformed state-of-the-art methods. effectiveness method perform kb-related tasks address following important problems supervised setting entity linking factoid question answering tasks adopt simple multi-layer perceptron classiﬁer learned representations features. tested method using standard datasets task popular factoid dataset based quiz bowl quiz game factoid task. result method outperformed recent state-of-the-art methods factoid tasks. contributions summarized follows propose neural network model jointly learns vector representations texts entities. train model using large amount entity annotations extracted directly wikipedia. demonstrate proposed representations surprisingly effective various tasks. particular apply proposed model three different tasks namely semantic textual similarity entity linking factoid question answering achieve stateof-the-art results three tasks. problem training model denominator computationally expensive involves summation entities address problem replacing union positive entity randomly chosen negative entities appear method viewed negative sampling uniform negative distribution. parameters learned model vector representations words entities vocabulary weight matrix bias vector consequently total number parameters model initialize representations words entities using pre-trained representations reduce training time. skip-gram model wordvec negative sampling trained wikipedia articles. order create corpus skip-gram model wikipedia simply replace name entity annotation wikipedia articles unique identiﬁer entity annotation refers simple method enables easily train distributed representations words entities simultaneously. used wikipedia dump generated july hyper-parameters skip-gram model used standard parameters context window size size negative samples used python wordvec implementation gensim. additionally entity representations normalized unit length used pre-trained representations. trained model using english dbpedia abstract corpus open corpus wikipedia texts entity annotations manually created wikipedia contributors. extracted ﬁrst introductory sections million wikipedia articles. train model iterating texts entity annotations corpus. used words appear times entities appear three times corpus simply ignored words entities. result vocabulary consisted words entities. further number valid words entity annotations approximately million million respectively. additionally also introduce heuristic method generate entity annotations. text pseudo-annotation points entity page source text. because every page describes corresponding entity typically contains many mentions referring entity. however hyper-linking page make sense kinds mentions cannot observed annotations wikipedia. therefore aforementioned heuristic method address problem. model implemented using python theano training took approximately days using nvidia gpu. trained model using stochastic gradient descent learning rate controlled rmsprop order evaluate model presented previous section conduct experiments three important tasks using representations learned model. first conduct experiment semantic textual similarity task order evaluate quality learned text representations. next conduct experiments important problems order test effectiveness proposed representations features downstream tasks. finally qualitatively analyze learned representations. semantic textual similarity semantic textual similarity aims test well model reﬂects human judgments semantic similarity sentence pairs. task used standard method evaluate quality distributed representations sentences past work experimental setup follows previously published experiment standard datasets dataset consisting sentence pairs human ratings different sources sick dataset consisting pairs sentences human ratings. datasets ratings take values between rating indicates sentence pair related rating means highly related. sentence pairs except sick trial pairs used experiments. train model experimenting paragraphs sentences. further introduce another training setting parameters word representations entity representations ﬁxed throughout training. wordvec popular word embedding model. compute sentence representation element-wise addition vectors words skip-gram cbow models baselines. train model hyper-parameters wikipedia corpus explained section thus skip-gram model equivalent pre-trained representations used model. furthermore order conduct fair comparison skip-gram model model also skip-gram skip-gram model trained using different corpus. particular corpus augmented using texts dbpedia abstract corpus entity annotations treated regular text phrases skip-thought model trained predict adjacent sentences given sentence corpus. sentences encoded using recurrent neural network gated recurrent units ntee models able outperform state-of-the-art models datasets terms pearson’s moreover ﬁxed ntee models outperformed ntee models several datasets skip-gram models datasets. further model trained sentences consistently outperformed model trained paragraphs. additionally skip-gram models performed mostly similarly regardless difference corpus. note that word representations entity representations training ﬁxed ntee models difference ﬁxed ntee models skip-gram model merely presence learned fully connected layer. model places text representation representations relevant entities close other function layer recognized afﬁne transformation word-based text representation entity-based text representation. consider reason ﬁxed ntee model performed well among datasets entity-based text representations semantic contain less noise word-based text representations thus much suitable addressing task. entity linking entity linking task resolving ambiguous mentions entities referent entities recently received considerable attention effectiveness various tasks information extraction semantic search. task challenging ambiguity meaning entity mentions improve performance accurately model semantic context entity mentions. model learns likelihood entity appearance given text naturally used modeling context experimental setup follows setup described past work standard datasets conll dataset dataset. conll dataset proposed hoffart includes training development test sets consisting documents respectively. training train method test measuring performance method. report standard micro macro accuracies top-ranked candidate entities. dataset another dataset constructed text analysis conference dataset comprises training test sets containing documents respectively. mentions valid entry report micro-accuracy score top-ranked candidate entities. evaluate method mentions contained test set. further randomly select documents training documents development set. additionally collected measures frequently used past work entity popularity prior probability. entity popularity entity deﬁned anchors point prior probability mention referring entity deﬁned |aem|/|a∗m| represents anchors surface subset points measures collected directly wikipedia dump described section candidate generation candidate generation candidates referent entities generated mention. candidate generation method proposed yamada sake compatibility state-of-the-art results. particular public dataset proposed pershina conll dataset. dataset dictionary directly built wikipedia dump explained section retrieved possible mention surfaces entity title entity title another entity redirecting entity names anchors point entity. furthermore improve recall also tokenize title entity treat resulted tokens possible mention surfaces corresponding entity. sort entity candidates according entity popularities retain candidates computational efﬁciency. recall canmention disambiguation address mention disambiguation task using multi-layer perceptron single hidden layer. figure shows architecture neural network model. model selects entity among entity candidates mention document entity candidate input vector vector document product small number features described below. features stack hidden layer nonlinearity using rectiﬁed linear units dropout. also output layer onto hidden layer select relevant entity using softmax entity candidates. similar past work include small number features model. first following three standard features entity popularity prior probability referring maximum prior probability mentions addition optionally features representing string similarities title surface similarities include whether title exactly equals contains surface whether title starts ends surface tuned following hyper-parameters using micro-accuracy development dataset number units hidden layer dropout probability. results listed table further trained model using stochastic gradient descent learning rate controlled rmsprop mini-batch size also used micro-accuracy development locate best epoch testing. tested ntee model ﬁxed ntee model initialize parameters representations furthermore also tested simple methods using pre-trained representations ﬁrst method representations words entities initialized using pre-trained representations presented section parameters initialized randomly second method method sg-proj except training corpus pre-trained representations augmented using dbpedia abstract corpus regarding ntee ﬁxed ntee models sentences used train proposed representations superior performance approach conll datasets. further update representations words entities training method updating generally improve performance. additionally used vector ﬁlled zeros representations entities contained vocabulary. augmented corpus simply concatenating wikipedia corpus dbpedia abstract corpus. similar wikipedia corpus replaced entity annotation dbpedia abstract corpus unique identiﬁer entity referred annotation. table compares results method obtained state-of-the-art methods. method achieved strong results conll datasets. particular ntee model clearly outperformed proposed models. also tested performance ntee model without using string similarity features found features also contributed performance. furthermore method successfully outperformed recent strong state-of-the-art methods datasets. remarkable state-of-the-art methods including baseline methods except adopt global approaches entity mentions document simultaneously disambiguated based coherence among disambiguation decisions. method depends local context available target document. thus performance likely improved combining global model local model frequently observed past work also conducted brief error analysis using ntee model test conll dataset randomly inspecting errors. result errors mentions referent entities contained vocabulary. case method could incorporate contextual information thus likely resulting disambiguation errors. major types errors mentions location names. dataset contains many location names referring sports team entities appeared method neglected distinguish whether location name refers location sports team. particular method often wrongly resolved mentions referring sports team entities corresponding location entities vice versa. accounted total number errors respectively. moreover observed several difﬁcult cases selecting hindu instead hindu nationalism christian instead catholicism york city instead york forth. question answering central problems research last decades. factoid typical types aims predict entity discussed given question. quiz bowl popular trivia quiz game players asked questions consisting sentence questions describing entities. dataset quiz bowl frequently used evaluating factoid methods recent literature followed existing method experimental setup. used public quiz bowl dataset proposed iyyer following past work used questions belonging history literature categories used answers appeared least times. questions referring answer sampled development test sets remaining training set. result obtained training development test questions history training development test questions literature. number possible answers history literature categories respectively. following past work address task classiﬁcation problem selects relevant answer possible answers observed dataset. adopt neural network architecture described section following three features vector vector question product note include features task. unlike task updated parameters including representations words entities training method. used stochastic gradient descent train model. minibatch size ﬁxed learning rate controlled rmsprop. used accuracy dataset downloaded https//cs.umd. edu/˜miyyer/qblearn/. note public dataset signiﬁcantly smaller used past work also used proprietary dataset addition public dataset. similar task tested four models initialize representations i.e. ntee ﬁxed ntee sg-proj sgproj-dbp models. further representations ntee model ﬁxed ntee model trained sentences overall superior accuracy compared trained paragraphs. qanta approach based recursive neural network derive distributed representations questions. method also uses classiﬁer derived representations features. fts-brnn based bidirectional recurrent neural network gated recurrent units similar qanta method adopts classiﬁer derived representations features. table shows results methods compared baseline methods. results bow-dt qanta obtained also include result reported iyyer used signiﬁcantly larger dataset training testing. particular despite simplicity neural network architecture method compared state-of-the-art methods method clearly outperformed methods. demonstrates effectiveness proposed representations background knowledge task. also conducted brief error analysis using test history dataset. observations indicated method mostly performed perfect terms predicting types target answers however method erred delicate cases predicting henry england instead henry england syracuse sicily instead sicily. qualitative analysis order investigate happens inside model conducted qualitative analysis using proposed representations trained sentences. ﬁrst inspected word representations model pre-trained representations computing similar words words using cosine similarity. results presented table interestingly model somespeciﬁc skip-gram model. example word whose cosine similarity word model whereas corresponding similar words skip-gram model satisfy condition. observe similar trend similar words dry. furthermore words similar tennis strictly related sport model whereas corresponding similar words skip-gram model contain broader words ball sports similar trend observed similar words spanish moon. similarly also compared entity representations pre-trained representations computing similar entities entities respect cosine similarity. table contains results. entities europe golf observe similar trends word representations. particularly model similar entities europe golf eastern europe golf course respectively whereas skip-gram model asia tennis respectively. however similar entities entities appear similar model skipgram model. various neural network models learn distributed representations arbitrary-length texts recently proposed. models aimed produce general-purpose text representations used ease various downstream tasks. although models learn text representations unstructured text corpus also proposed models learn text representations leveraging structured linguistic resources. instance wieting trained model using large number noisy phrase pairs retrieved paraphrase database hill several public dictionaries train model mapping deﬁnition texts dictionary representations words explained texts. knowledge work ﬁrst work learn generic text representations supervision entity annotations. several methods also proposed extending word embedding methods. example levy goldberg proposed method train word embedding dependency-based conskip-gram mother moist drier drying moister badminton hardcourt volleyball racquetball squash spain portuguese french catalan mexican lunar moons earth sadasaa texts luan used semantic role labeling generating contexts train word embedding. moreover recent studies learning entity embedding based word embedding methods reported models typically based skip-gram model directly model semantic relatedness entities. work differs studies learn representations arbitrary-length texts addition entities. another related approach relational embedding encodes entities continuous vectors relations operations vector space vector addition. models typically learn representations large graphs consisting entities relations. similarly universal schema finally yamada recently proposed method jointly learn embeddings words entities wikipedia using skip-gram model applied method differs method method directly model arbitrary-length texts proved highly effective various tasks paper. moreover also showed joint embedding texts entities applied also wider applications semantic textual similarity factoid paper presented novel model capable jointly learning distributed representations texts entities large number entity annotations wikipedia. construct proposed general-purpose model enables practitioners address various tasks ease. achieved state-of-the-art results three important tasks clearly demonstrated effectiveness model. furthermore qualitative analysis showed characteristics learned representations apparently differ conventional word embedding model plan investigate detail future. moreover make code trained models publicly available future research. future work includes analyzing model extensively exploring effectiveness model terms tasks. also test expressive neural network models derive text representations. furthermore believe promising directions would incorporate rich structural data relationships entities links entities hierarchical category structure entities. model eastern europe western europe central europe asia north america golf course tour lpga professional golfer u.s. open coffee green black camellia sinensis spice tablet computer mobile device personal digital assistant android iphone kirsten dunst anne hathaway cameron diaz natalie portman jessica biel hobbit tolkien silmarillion fellowship ring lord rings skip-gram asia western europe north america central europe americas tennis lpga tour golf course nicklaus design coffee green black camellia sinensis spice tablet computer personal digital assistant mobile device android feature phone anne hathaway natalie portman kirsten dunst cameron diaz kate beckinsale hobbit tolkien silmarillion fellowship ring elvish languages", "year": 2017}