{"title": "Non-Markovian Control with Gated End-to-End Memory Policy Networks", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Partially observable environments present an important open challenge in the domain of sequential control learning with delayed rewards. Despite numerous attempts during the two last decades, the majority of reinforcement learning algorithms and associated approximate models, applied to this context, still assume Markovian state transitions. In this paper, we explore the use of a recently proposed attention-based model, the Gated End-to-End Memory Network, for sequential control. We call the resulting model the Gated End-to-End Memory Policy Network. More precisely, we use a model-free value-based algorithm to learn policies for partially observed domains using this memory-enhanced neural network. This model is end-to-end learnable and it features unbounded memory. Indeed, because of its attention mechanism and associated non-parametric memory, the proposed model allows us to define an attention mechanism over the observation stream unlike recurrent models. We show encouraging results that illustrate the capability of our attention-based model in the context of the continuous-state non-stationary control problem of stock trading. We also present an OpenAI Gym environment for simulated stock exchange and explain its relevance as a benchmark for the field of non-Markovian decision process learning.", "text": "partially observable environments present important open challenge domain sequential control learning delayed rewards. despite numerous attempts last decades majority reinforcement learning algorithms associated approximate models applied context still assume markovian state transitions. paper explore recently proposed attention-based model gated end-to-end memory network sequential control. call resulting model gated end-to-end memory policy network. precisely model-free value-based algorithm learn policies partially observed domains using memory-enhanced neural network. model end-to-end learnable features unbounded memory. indeed attention mechanism associated non-parametric memory proposed model allows deﬁne attention mechanism observation stream unlike recurrent models. show encouraging results illustrate capability attention-based model context continuous-state non-stationary control problem stock trading. also present openai environment simulated stock exchange explain relevance benchmark ﬁeld non-markovian decision process learning. reinforcement learning methods realistic environments typically need deal incomplete noisy state information resulting partial observability formalized partially observable markov decision processes addition often need deal non-markovian problems signiﬁcant dependencies earlier states. pomdps non-markovian problems largely defy traditional fully parametric value function policy based approaches currently require handcrafted state estimators based accurate knowledge system. context neural networks used value function policy reinforcement learning paradigm solving continuous control problems long history. several recent papers successfully apply model-free direct policy search methods problem learning neural network control policies challenging continuous domains many degrees freedom however work still assumes fully observed state. naive alternative using memory learning reactive stochastic policies simply observations probabilities actions. underlying assumption state-information play crucial role parts problem using random actions prevent policy getting stuck endless loop ambiguous observations. often strategy optimal algorithms form memory remain necessary. summary perfect model precisely estimated state assumed optimal policy likely memory-based. however works policy gradient methods memory rare largely limited ﬁnite-state controllers recently work proposed partially observable instance atari framework work makes attempt provide attention mechanism accumulative memory.rather authors suggest ﬁxed size memory model long short term memory control learning. paper extend lstm approaches sophisticated policy representations capable representing observed state using memory enhanced architecture called gated endto-end memory policy network. model policy gradient type algorithm effectively learn policies pomdps using unbounded memory leveraging attention mechanism past observations. result policy updates depend event history. show method outperforms methods proposed benchmark task continuous control non-markovian trading environments. paper organized follows section formulates pomdps discusses reinforcement learning pomdps. section proposes usage gated end-to-end memory networks memory-enhanced reinforcement learning. section derivation model policy network presented. then section describes trading optimized execution tasks chosen evaluation purposes. pertinence environment developed using openai framework discussed. finally section presents results task using real indices. standard paradigm reinforcement learning agent interacts environment during potentially inﬁnite number discrete time steps. time step agent observes state chooses action admissible actions using policy function states actions result agent observes next state receives scalar reward process continues agent reaches terminal state. deﬁne return γkrt+k total accumulated return time step discount factor goal agent maximize expected return state action value expected return selecting action state following policy optimal value function maxπ gives maximum action value state action achievable policy. similarly value state policy deﬁned simply expected return following policy state value-based model-free reinforcement learning methods action value function often modeled using function approximator neural network. approximate action-value function parameters updates deﬁned variety reinforcement learning algorithms. well known example algorithm q-learning aims directly approximate optimal action value function one-step q-learning parameters action value function learned iteratively minimizing sequence loss functions loss function deﬁned asli state encountered state standard formulation problem called markov decision process. assumes environment markovian means transition state conditioned pair. formally pomdp described -tuple respectively states actions transition function reward function markov decision process addition agent longer access true system state receives observation instead. observation generated underlying system state according probability distribution goal agent infer policy order maximize cumulative reward. formalism pomdp well captures dynamics many real world environments explicitly acknowledging perception received agent offers partial glimpse underlying state. realistic world environments reasnoable assume full state system provided resolution partial observability also called perceptual aliasing non-trivial existing methods roughly divided classes. ﬁrst class approaches explicitly maintain belief state corresponds distribution world states given previous observations. assuming model-free hypothesis i.e. assumption taken transition reward functions policy derived state estimation using reinforcement learning methods like value based methods e.g. q-learning policy based methods e.g. policy-gradient approaches. major disadvantages mentioned ﬁrst need model support state inference task. second computational cost typically associated update belief state second class approaches learn form memories based interactions environment. methods challenging since priori unknown features observations relevant later associations formed many steps. here differentiable mechanism learn dependencies experience becomes desirable. reason model free approaches tend assume full observability. practice partial observability often solved hand-crafting sufﬁcient state representation observations. example video-games estimate velocity consecutive frames mentioned before ﬁrst attempts deep q-network experimented atari videogames explicit mechanism inferring underlying state sequence pomdp thus effective contiguous series past observations reﬂect underlying system states general case learning q-function ﬁxed observation window arbitrarily since ﬁxed. recently deep recurrent q-learning proposed method uses recurrent network namely long short term memory memorization capability previously proposed model. drawback proposal comes necessity selecting priori using cross-validation dimension hidden context vectors recurrent model determine memorization capacity. another point discussion might concern experimental setting used last work. authors propose develop artiﬁcial flickered version atari platform order mask parts entire current frame given period order force model memorize. performance model environment transformed non-markovian measured. finally closest reference work. authors experiment off-the-shelf memory network policy task exploration path-ﬁnding virtual environment. paper make propositions. first investigate gated attention mechanism coupled deep recurrent q-network. suggest mechanism allow qnetwork better estimate underlying system state narrowing indeed following sections show attention enhanced deep qnetworks better approximate actual q-values sequences observations leading better policies partially observed environments. second contribution present simple simulated environment stock trading evaluating proposed model. compare fully connected neural networks lstm tasks stock exchange simpliﬁed realistic task optimized execution brieﬂy present. ﬁeld algorithmic trading regroups large family methods proposed perform autonomous decision models global ﬁnancial market. discipline roughly decomposed categories. ﬁrst hand predictive methods deterministic policies consist learning indicators used support deterministic stochastic stationary decision schema methods consist learning actionable patterns used trigger buying selling actions based history identiﬁed trading signals external macroeconomical informations. hand policy learning investigated learn investement portfolio management policy directly stock market history also macro-economical events recently task optimized execution also studied context action space reduced either selling buying. indeed actual policy determined independent system optimized execution algorithm charge applying order market leveraging constant ﬂuctuation share prices order maximize proﬁtability chosen operation. context paper ambition challenge highly priori knowledge enriched partially handcrafted portfolio management policies currently implemented real market place. however believe execution context novel fruitful environment experiment conducting research non-markovian decision policy learning. model-free approach non-markovian observation state transitions require decision model store observations resulting interaction environment order gather sufﬁcient information support decision. recently recurrent models like lstm investigated incorporate memorization capability decision model direct policy value function learning drawback approach necessity deﬁning hyper-parameter dimension hidden state vector network limits memory model. furthermore recurrent model explicitly learn focus attention different parts growing memory long temporal dependencies occur observation space. alternative attention-based models already provided encouraging alternative several sequential decision tasks immediate reward maximization like natural language translation end-to-end dialog systems. former domain types approaches investigated. so-called sequence-to sequence model aims memorizing overall source sentence deciding target sentence words sequentially attention-based model aims iteratively constructing representation unbounded memory conditioned current state target sentence word generator motivated recent empirical success latter method investigate approach based recently proposed gated memory network model. depicted next section originally line research focused text-based applications like natural language understanding dialog management machine reading. propose adapt extend model policy learning. end-to-end memory network architecture consists main components supporting memories ﬁnal answer prediction. supporting memories turn comprised input output memory representations memory cells. input output obtained transforming input observations memory cells denoted using embedding matrices size embedding size dimension observations gathered environment function maps input real-valued space dimension similarly original memnn model question encoded using another embedding matrix rd×dq resulting question embedding input memories together embedding question utilized determine relevance observations context yielding vector attention weights softmax difﬁcult tasks requiring multiple supporting memories model extended include input/output memories stacking number memory layers. setting memory layer named takes input output lastly ﬁnal step prediction answer question performed softmax) predicted answer distribution parameter matrix model learn total number hops. suggested equation considered form residual working residual function shortcut connection. however discussed contrast hard-wired skip connection residual networks advantages highway networks adaptive gating mechanism capable learning dynamically control information based current input. therefore adopt idea adaptive gating mechanism highway networks integrate memnn. resulting model named gated end-to-end memory networks illustrated figure capable dynamically conditioning memory reading operation controller state hop. concretely reformulate equation into case policy learning memory cells ﬁlled past observations collected past interactions environment question input carry current state information relevant agent independent environment observations. context stock trading optimized execution memory blocks carry past values traded signal question block carry current budget portfolio composition agent. finally assuming discrete action output model answer expected reward associated eligible action. figure summarizes elements gated end-to-end memory policy network. limitation memory networks compared types attention-based models like applied machine translation necessity encode temporal information memory blocks. indeed commutative nature equation information regarding order observations embedded memory blocks encoded beforehand. dynamic memory network hidden state lstm used encode values memory blocks computing attention values them. original end-to-end memory network encoding done using deterministic function transforms sequence word embeddings sentence putting memory blocks. propose embed signal using denoising predictive neural auto-encoder. speciﬁcally single hidden layer perceptron reconstructing noisy input time frame placed memory blocks. hand model denoiser hand adding output model neural network predict future windows regarding encoded time frame. approach related context-dependent word vectorization consider policies represented gated memory networks. model builds vector controller state representing latent state multiple attention-based readings memory blocks environment observations stored. latent state begins ﬁxed state time-step network takes input series observations computes internal state according differentiable function outputs distribution actions according differentiable function denotes output memory network time-step past work deﬁned principled method updating parameters policy reinforcement learning using stochastic gradient descent△θd ▽θlogπθgt. update unbiased practice known suffer high variance converge rate. ▽θlogπθ baseline arbitrary function states visited episode. using general framework policy-gradient learning gated memory network deﬁne control model using approach described context language modeling application constant deﬁned network produces output distribution vocabulary. kind approach parallel control model deep q-learning proposed convolutional neural network takes input contiguous sliding window video game screens output q-values associated ﬁnite eligible actions. finally stability learning parametric policies asynchronous deep q-learning reinforcement learning algorithm described algorithm proposed evaluation environment developed simpliﬁed portfolio management platform. following settings proposed decision space trading consists three discrete actions {buy hold sell} assuming ﬁxed amount stock exchanged action. observation space current value stocks considered trading. transaction ﬁxed transaction cost associated. realistic setting transaction cost likely function type amount stocks involved decision step. experiments consider task speculative trading means reward measured increase budget given time step result evolution market shares. realistic settings dividends part companies beneﬁce distributed share holders also considered potential source income especially multi-year scale multi-stocks management settings. second task studied litterature optimized execution setting. consists either selling buying given amount stock ﬁxed amount time described optimized buying case goal consists buying desired amount stock cheapest price given period time. optimized selling case goal consists following acquisition strategy allows sell higher possible price given period. simulation platform developed openai environment planned published open-source package. purpose encourage research community non-markovian reinforcement learning framework reusable experimental testbed. non-markovian control possibility also generate synthetic series. comparison virtual environment like first person shooter atari control required memory capacity perform proﬁtable control deﬁned estimating markovian order series. indeed context games memory capacity hardly related partially observable maze ﬁrst person shooter function size maze. however case trading memory capacity requirement deﬁned order time series. experiments choose focus real indices taken main market places europe asia. gated end-to-end memory policy network takes input past observations traded series. time step computes expected reward eligible actions. model optimized policy gradient prioritized experience replay double q-learning order cope inherent instability learning process. beyond stability convergence rate compared q-learning model allows implement boltzmann type policy reward expectation using forward pass model. concernint parameterization decision model. suggested adjacent weight tying temporal encoding random noise used. learning rate initially assigned value exponential decay applied every epochs epochs reached. linear start used experiments proposed linear start softmax memory layer removed re-inserted epochs. batch size gradients norm larger divided scalar norm weights initialized randomly gaussian distribution zero mean except transform gate bias empirically mean experiments embedding size since memory-based models sensitive parameter initialization repeat training times choose best system based performance validation set. temporal neural encoders learnt individually training series used test preprocess observation sequences placed memory block policy network. hidden layer dimension encoder cross-validation optimized using adam then baseline neural policy network composed hidden layers hidden units rectiﬁed linear activation linear output projection. baseline lstm model hidden representation dimensions. hyperparameters estimated cross-validation. concerning policy learning algorithm. reward function episodic. episode agent receives reward difference budget period initial budget. network trained using consecutive days daily opening values. training phase consists trading episodes sequences values. training given series represents approximatively hour core nvidia tesla gpu. experiment policies learnt independently series another. testing phase trading experiment performed using consecutive days market. case optimized trading testing corresponds roll-outs. resulting policies follows bolzmann distribution reward predicted policy network. finally update period double q-learning mechanism action steps. table computes proﬁtability ratio corresponds number days test period agent proﬁtable. trading qualiﬁed proﬁtable difference corresponding current budget initial budget agent positive. evaluation makes sense speculative strategy maximizing amount positive market exit opportunities given period time excepted maximized. ﬁrst results conﬁrms utility control policy equipped memorization capability. then control policy equipped attention mechanism proposed work seems conﬁrm. policy network fcnn lstm memnn gmemnn fcnn lstm memnn gmemnn fcnn lstm memnn gmemnn fcnn lstm memnn gmemnn fcnn lstm memnn gmemnn fcnn lstm memnn gmemnn fcnn lstm memnn gmemnn fcnn lstm memnn gmemnn evaluation proposed policy optimized selling task also depicted. setting authorized actions reduced {hold sell}. agent starts episode stocks sell trading period. reward resulting accumulated budget period. trading policies evaluated testing series trading days. settings proposed policy show encouraging result conﬁrm beneﬁt attention based mechanism memory management learning differentiable policies nonmarkovian environment. experiments series absolute values max-normalized order accelerate gradient descent control gradient magnitude. finally necessity memory task seems conﬁrmed inferior performance memory-less fully connected layer model. paper studied question non-markovian decision processes attention-based policy network called gated end-to-end memory policy network. task stock exchange optimized execution used experimental testbed illustrate capability model. addition proposed model think trading environment produce fruitful research domain non-markovian control future. indeed settings stock exchange revenue maximization allow study behavior policy learning algorithms policy models signals exhibiting different requirement memorization. furthermore tasks resource allocation scheduling easily related formal setting. finally comparison current results using parametric memories like gated rectiﬁed units long short term memory believe attention-based models already demonstrated advantages domain sequence prediction natural language processing like machine translation machine reading ﬁrst importance general case", "year": 2017}