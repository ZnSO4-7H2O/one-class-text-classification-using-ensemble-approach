{"title": "Data Smashing", "tag": ["cs.LG", "cs.AI", "cs.CE", "cs.IT", "math.IT", "stat.ML"], "abstract": "Investigation of the underlying physics or biology from empirical data requires a quantifiable notion of similarity - when do two observed data sets indicate nearly identical generating processes, and when they do not. The discriminating characteristics to look for in data is often determined by heuristics designed by experts, $e.g.$, distinct shapes of \"folded\" lightcurves may be used as \"features\" to classify variable stars, while determination of pathological brain states might require a Fourier analysis of brainwave activity. Finding good features is non-trivial. Here, we propose a universal solution to this problem: we delineate a principle for quantifying similarity between sources of arbitrary data streams, without a priori knowledge, features or training. We uncover an algebraic structure on a space of symbolic models for quantized data, and show that such stochastic generators may be added and uniquely inverted; and that a model and its inverse always sum to the generator of flat white noise. Therefore, every data stream has an anti-stream: data generated by the inverse model. Similarity between two streams, then, is the degree to which one, when summed to the other's anti-stream, mutually annihilates all statistical structure to noise. We call this data smashing. We present diverse applications, including disambiguation of brainwaves pertaining to epileptic seizures, detection of anomalous cardiac rhythms, and classification of astronomical objects from raw photometry. In our examples, the data smashing principle, without access to any domain knowledge, meets or exceeds the performance of specialized algorithms tuned by domain experts.", "text": "fig. data smashing determining similarity data streams data mining process relies heavily humanprescribed criteria. data smashing ﬁrst encodes data stream collides inverse other. randomness resulting stream reﬂects similarity original streams leading cascade downstream applications involving classiﬁcation decision optimization. notion data smashing applies data form ordered series digits symbols acoustic waves microphone light intensity time telescope traﬃc density along road network activity router. anti-stream contains opposite information original data stream produced algorithmically inverting statistical distribution symbol sequences appearing original stream. example sequences digits common original stream rare anti-stream vice versa. streams anti-streams algorithmically collided systematically cancels common statistical structure original streams leaving information relating statistically signiﬁcant diﬀerences. call principle information annihilation data smashing involves data streams proceeds three steps data streams ﬁrst quantized converting continuous value string characters symbols. simplest example quantization positive values mapped symbol negative values thus generating string bits. next select quantized input streams generate anti-stream. finally annihilate anti-stream remaining quantized input stream measure information remains. remaining information estimated deviation resultant stream white noise since data stream perfectly annihilated correct realization anti-stream deviation collision product noise quantiﬁes statistical dissimilarity. using causal similarity metric cluster streams classify them identify stream segments unusual diﬀerent. algorithms linear input data implying applied eﬃciently streams near-real time. importantly data smashing applied without understanding streams generated encoded represent. ultimately collection data streams pairwise similarities possible automatically back underlying metric embedding data revealing hidden structure traditional machine learning methods. abstract—investigation underlying physics biology empirical data requires quantiﬁable notion similarity observed data sets indicate nearly identical generating processes not. discriminating characteristics look data often determined heuristics designed experts distinct shapes folded lightcurves used features classify variable stars determination pathological brain states might require fourier analysis brainwave activity. finding good features non-trivial. here propose universal solution problem delineate principle quantifying similarity sources arbitrary data streams without priori knowledge features training. uncover algebraic structure space symbolic models quantized data show stochastic generators added uniqely inverted; model inverse always generator white noise. therefore every data stream anti-stream data generated inverse model. similarity streams then degree summed other’s anti-stream mutually annihilates statistical structure noise. call data smashing. present diverse applications including disambiguation brainwaves pertaining epileptic seizures detection anomalous cardiac rhythms classiﬁcation astronomical objects photometry. examples data smashing principle without access domain knowledge meets exceeds performance specialized algorithms tuned domain experts. term data smashing might conjure images erasing information destroying hard drives. smashing atoms reveal composition colliding quantitative data streams reveal hidden structure. describe principle quantitative data streams corresponding anti-streams inspite non-unique tied stream’s unique statistical structure. describe data smashing process streams anti-streams algorithmically collided reveal diﬀerences diﬃcult detect using conventional techniques. establish principle formally describe implemented practice report performance number real-world cases. results show without access domain knowledge data smashing meets exceeds accuracy achieved specialized algorithms heuristics devised domain experts. nearly automated discovery systems today rely core ability compare data automatic image recognition discovering astronomical objects systems must able compare contrast data records order group them classify them identify odd-one-out. despite rapid growth amount data collected increasing rate processed analysis quantitative data streams still relies heavily knowing look for. time data mining algorithm searches beyond simple correlations human expert must help deﬁne notion similarity specifying important distinguishing features data compare training learning algorithms using copious amounts examples. data smashing principle removes reliance expert-deﬁned features examples many cases faster better accuracy traditional methods. paper organized follows sections i-vi describe concepts along brief complete description approach. mathematical details including proﬀs correctness presenetd sections vii-ix. qunatization schemes discussed section comparisons standard notions statistical dependencies carried section paper concluded secion xii. fig. calculation causal similarity using information annihilation. quantize signals symbolic sequences chosen alphabet compute causal similarity sequences. underlying theory established assuming existence generative probabilistic automata sequences algorithms require explicit model construction priori knowledge structures. concept stream inversion; group inverse given pfsa algebraically also transform generated sequence directly represents inverse model without constructing model itself. summing pfsas inverse yields zero pfsa carry annihilation purely sequence level white noise. circuit allows measure similarity distance streams computation given threshold sufﬁcient data stream additionally conclude stochastic source high probability distinct concepts. former measures dependence streams; latter computes distance generative processes themselves. sequences independent coin-ﬂips necessarily zero mutual information data smashing identify streams similar; generated stochastic process. moreover smashing works correctly streams independent nearly similarity computed data smashing clearly function statistical information buried input streams. however might easy right statistical tool reveals hidden information particularly without domain knowledge without ﬁrst constructing good system model describe detail process computing antistreams process comparing information. section vii-ix provide theoretical bounds conﬁdence levels minimal data lengths required reliable analysis scalability process function signal encodings. limitations. data smashing directly applicable learning tasks depend require notion similarity identifying speciﬁc time instant event interest transpired within data predicting next step time series. even problems smashing applicable claim strictly superior quantitative performance state-ofart applications; carefully chosen approaches tuned speciﬁc problems certainly well better. claim uniformly outperform existing methods evidenced multiple example applications; without requiring expert knowledge training set. additionally technical reasons preclude applicability data strictly deterministic systems notion universal comparison metric makes sense context featureless approach considers pairwise similarity individual measurement sets. however advantage considering notion similarity data sets instead feature vectors recognized deﬁnition similarity measures remained intrinsically heuristic application dependent possibility universal metric summarily rejected. show universal comparison indeed realizable least general assumptions nature generating process. consider sequential observations time series sensor data. ﬁrst step mapping possibly continuous-valued sensory observations discrete symbols pre-speciﬁed quantization data range symbol represents slice data range total number slices deﬁne symbol alphabet coarsest quantization binary alphabet consisting ﬁner quantizations larger alphabets also possible. observed data stream thus mapped symbolic sequence pre-speciﬁed alphabet. algorithmic procedure generate stream read current symbol write output move read positions step right step operation required internally stream inversion. generate independent copies sj\u0006j\u0000 read current symbols move read positions step right step ysee section proof correctness zsee deﬁnition propositions section symbolic derivatives underlie rigorous proofs. however actual implementation needed ﬁnal step compute deviation quantization involves information loss reduced ﬁner alphabets expense increased computational complexity quantization schemes require domain expertise. quantized stochastic processes capture statistical structure symbolic streams modeled using probabilistic automata provided processes ergodic stationary purpose computing similarity metric require number states automata ﬁnite attempt construct explicit models require knowledge either exact number states explicit bound thereof slightly restricted subset space pfsa ﬁxed alphabet admits abelian group structure wherein operations commutative addition inversion well-deﬁned. trivial example abelian group reals usual addition operation; addition real numbers commutative real number unique inverse summed produce unique identity previously discussed abelian group structure pfsas context model selection here show group operations necessary classiﬁcation carried observed sequences alone without state synchronization reference hidden generators sequences. existence group structure implies given pfsas sums unique inverses welldeﬁned. individual symbols notion sign hence models generators sign-inverted sequences would make sense generated sequences symbol streams. example anti-stream sequence fragment inverted statistical properties terms occurrence patterns symbols pfsa unique inverse pfsa added yields group identity zero model. note zero model characterized property arbitrary pfsa group ﬁxed alphabet size zero model unique singlestate pfsa generates symbols consecutive realizations independent random variables uniform distribution symbol alphabet. thus generates white noise entropy rate achieves theoretical upper bound among sequences generated arbitrary pfsa model space. pfsas identical addition abelian group pfsa space admits metric structure distance models thus interpreted deviation group-theoretic diﬀerence process. information annihilation exploits possibility estimating causal similarity observed data streams estimating distance observed sequences alone without requiring models themselves. estimate distance hidden generative model given observed stream achieved function intuitively given observed sequence fragment ﬁrst compute deviation distribution next symbol uniform distribution alphabet. deviations historical fragments length weighted =j\u0006jjxj. weighted ensures deviation distributions longer smaller contribution addresses issue occurrence frequencies longer sequences variable. comparable result available literature. however reveals manifold structure dataset additional assumptions nature hidden processes fails yield insight. insight following sets sequential observations generative process inverted copy annihilate statistical information contained other. claim given symbol streams check underlying pfsas satisfy annihilation equality without explicitly knowing constructing models themselves. data smashing predicated able invert streams compare streams noise. inversion generates stream given stream pfsa source source summation collides streams given streams generate stream realization hidden models satisfy finally deviation stream generated process calculated directly. importantly stream inverted stream unique. symbol stream generated inverse model qualiﬁes inverse thus anti-streams non-unique. indeed unique generating inverse pfsa model. since technique compares hidden stochastic processes possibly non-unique realizations non-uniqueness anti-streams problematic. despite possibility mis-synchronization hidden model states applicability algorithms shown table disambiguation hidden dynamics valid. show section algorithms evaluate distinct models distinct nearly identical hidden models nearly identical. length must necessarily occur frequency process; simply estimate deviation behavior observed sequence. tasks carried selective erasure symbols input stream example summation streams realized follows given streams read symbol stream match copy output ignore symbols read match. thus data smashing allows manipulate streams selective erasure estimate distance hidden stochastic sources. speciﬁcally estimate degree stream anti-stream brings entropy rate resultant stream close theoretical upper bound. contemporary research machine learning dominated search good features typically understood heuristically chosen discriminative attributes characterizing objects phenomena interest. finding attributes easy moreover number characterizing features size feature needs relatively small avoid intractability subsequent learning algorithms. additionally heuristic deﬁnition precludes notion optimality; impossible quantify quality given feature absolute terms; compare performs context speciﬁc task selected variations. addition heuristic nature feature selection machine learning algorithms typically necessitate choice distance metric feature space. example classic nearest neighbor k-nn classiﬁer requires deﬁnition proximity fig. computational complexity convergence rates information annihilation. illustrates exponential convergence self-annihilation error small data series different applications data plate heart sound recordings plate photometry). computation times carrying annihilation using circuit shown fig. function length input streams different alphabet sizes note asymptotic time complexity obtaining similarity distances scales length shorter input streams. k-means algorithm depends pairwise distances feature space clustering. side-step heuristic metric problem recent approaches often learn appropriate metrics directly data attempting back metric side information labeled constraints unsupervised approaches dimensionality reduction embedding strategies uncover geometric structure geodesics feature space however automatically inferred data geometry feature space again strongly dependent initial choice features. since euclidean distances feature vectors often misleading heuristic features make impossible conceive task-independent universal metric. contrast smashing based application-independent notion similarity quantized sample paths observed hidden stochastic processes. universal metric quantiﬁes degree summation inverted copy stream annihilates existing statistical dependencies leaving behind white noise. circumvent need features altogether require training. despite fact estimation similarities data streams performed absence knowledge underlying source structure parameters establish universal metric causal suﬃcient data converges welldeﬁned distance hidden stochastic sources themselves without ever knowing explicitly. statistical process characteristics dictate amount data required estimation proposed distance. access hidden models cannot estimate required data length priori; however possible check data-suﬃciency speciﬁed error threshold self-annihilation. since proposed metric causal distance independent samples source always converges zero. estimate degree selfannihilation achieved order determine data suﬃciency; stream suﬃciently long suﬃciently annihilate inverted self-copy fwn. self-annihilation based data-suﬃciency test consists steps given observed symbolic sequence ﬁrst generate independent copy independent stream copy operation carried selective symbol erasure without knowledge source itself. check inverted version annihilates pre-speciﬁed degree. particular generate stream inversion stream summation produce ﬁnal output stream check less speciﬁed threshold show considering histories length computation suﬃcient self-annihilation error also useful rank eﬀectiveness diﬀerent quantization schemes. better quantization schemes able produce better selfannihilation maintaining ability discriminate diﬀerent streams given data streams construct matrix represents estimated distance streams thus diagonal elements self-annihilation errors oﬀ-diagonal elements represent inter-stream similarity estimates given positive threshold self-annihilation tests passed suﬃcient data streams identical sources high probability constructed determine clusters rearranging prominent diagonal blocks. standard technique used clustering; information annihilation used causal distances observed data streams resultant distance matrix used input state-of-theart clustering methodologies ﬁnding geometric structures induced similarity metric data sources. matrix obtained setting diagonal entries zero estimates distance matrix. euclidean embedding leads deeper insight geometry space hidden generators case data time series’ describe one-dimensional manifold data similar phenomena clustered together along curve pass self-annihilation test data stream must suﬃciently long; required length input speciﬁed threshold dictated characteristics generating process. selective erasure annihilation implies output tested shorter compared input stream expected shortening ratio explicitly computed refer annihilation eﬃciency since words required length data stream achieve self-annihilation error scales important note analysis shows annihilation eﬃciency independent descriptional complexity process fig. self-annihilation error simpler state process converges faster four state process. however convergence rate always scales data smashing useful problems require notion similarity predicting future course time series analyzing data pinpoint occurrence time event interest. fig. data smashing applications. pairwise distance matrices identiﬁed clusters projections euclidean embeddings epileptic pathology identiﬁcation identiﬁcation heart murmur classiﬁcation variable stars photometry applications relevant clusters found unsupervised. models explicitly. follows actually assume particular modeling framework systems interest satisfy properties ergodicity stationarity ﬁnite number states practice technique performs well even properties approximately satisﬁed algebraic structure space pfsas information annihilation principle; however argue quantized ergodic stationary stochastic process indeed representable probabilistic automata data smashing applicable data strictly deterministic systems. systems representable probabilistic automata; however transitions occur probabilities either pfsas zero-probability transitions non-invertible invalidates underlying theoretical guarantees similarly data streams alphabet symbol exceedingly rare would diﬃcult invert symbolization invariably introduces quantization error. made small using larger alphabets. however larger alphabet sizes demand longer observed sequences implying length observation limits quantization granularity process limits degree quantization error mitigated. importantly coarse quantizations distinct processes evaluate similar. however identical processes still evaluate identical provided streams pass self-annihilation test. self-annihilation test thus oﬀers application-independent compare rank quantization schemes data smashing begins quantizing streams symbolic sequences followed annihilation circuit compute pairwise causal similarities. details quantization schemes computed distance matrices identiﬁed clusters euclidean embeddings summarized table fig. ﬁrst application classiﬁcation brain electrical activity diﬀerent physiological pathological brain states used sets electroencephalographic data series consisting surface recordings healthy volunteers eyes closed open intracranial recordings epilepsy patients starting data series electric potentials generated sequences relative changes consecutive values quantization. step allows common alphabet sequences wide variability sequence mean values. distance matrix pairwise smashing yielded clear clusters corresponding seizure normal eyes open normal eyes closed epileptic pathology non-seizure conditions. embedding distance matrix yields one-dimensional manifold contiguous segments corresponding diﬀerent brain states right hand side plane correspond epileptic pathology. provides particularly insightful picture eludes complex non-linear modeling. next classify cardiac rhythms noisy heat-sound data recorded using digital stethoscope analyzed data series corresponding healthy rhythms murmur verify could identify clusters without supervision correspond expert-assigned labels. found clusters distance matrix consisted mainly data murmur rest consisting mainly healthy rhythms classiﬁcation precision murmur noted table embedding distance matrix revealed dimensional manifold next problem classiﬁcation variable stars using light intensity series optical gravitational lensing experiment survey supervised classiﬁcation photometry proceeds ﬁrst folding light-curve known period correct phase mismatches. ﬁrst analysis started folded light-curves; generated data series relative changes consecutive brightness values curves quantization allows common alphabet light curves wide variability mean brightness values. using data cepheids rrls obtained classiﬁcation accuracy marginally outperforms state clear clusters corresponding classes seen computed distance matrix projection euclidean embedding embedding nearly constrained within manifold additionally second analysis asked data smashing work without knowledge period variable star; skipping folding step. smashing photometry data yielded classiﬁcation accuracy classes direct approach beyond state techniques. fourth application biometric authentication using visually evoked potentials public database used considered subjects exposed pictures objects chosen standardized snodgrass note application supervised actual training involved; merely mark randomly chosen subject-speciﬁc data series library representing individual subject. unknown test data series smashed element libraries corresponding individual subjects expected data series subject annihilate correctly diﬀerent subjects fail extent. outperformed state based approaches independent speaker identiﬁcation using elsdsr database includes recording speakers before training involved specifying library series speaker. computed distance matrix smashing library data series other trained simple euclidean embedding distance matrix. test data yielded classiﬁcation accuracy beat state ﬁgure snippets recording data suceeding sections develop mathematical details information annihilatin principle establish correctness data smashing algorithm. section presents theory probabilistic automata modeling framework ergodic stationary quantized stochastic processes. section viii describes relevant algebraic structures including abelian group deﬁnable space probabilistic automata. central notion anti-streams. section establishes stream operations delineated table indeed correct. section discusses quantization schemes; speciﬁcally describing choose granularity quantization. section expounds diﬀerences data smashing approach speciﬁc standard notions often used quantify statistical dependencies mutual information data streams. also discuss speciﬁc example illustrate simple statistical features miss important dynamical artifacts data easily revealed data smashing. paper summarized concluded section xii. establish correctness data smashing algorithm ﬁrst establish possibility using probabilistic automata model stationary ergodic processes. automata models distinct reported literature include brief overview sake completeness. notation denotes ﬁnite alphabet symbols. ﬁnite possibly unbounded strings denoted ﬁnite strings form concatenative monoid empty word identity. strictly inﬁnite strings denoted denotes ﬁrst transﬁnite cardinal. string denotes length denotes cardinality. deﬁnition discrete time \u0006-valued strictly stationary ergodic stochastic process next formalize connection qsps pfsa generators. develop theory assuming multiple realizations ﬁxed initial conditions. using ergodicity able apply construction single suﬃciently long realization initial conditions cease matter. deﬁnition inﬁnite strings deﬁne smallest \u001b-algebra generated family sets \u0006?g. lemma every induces probability space classically automaton states equivalence classes nerode relation; strings equivalent ﬁnite extension strings either language consideration neither probabilistic extension deﬁnition induces equivalence relation ﬁnite strings deﬁned ergodic stationary whereas initial-marked pfsas designated initial state. next introduce canonical representations remove initial-state dependence. stationary distribution states note exist string leads distribution beginning stationary distribution thus equivalence class strings unique. deﬁnition initial-marked pfsa construct stationary distribution using transition probabilities markov chain induced include ﬁrst element note transition matrix row-stochastic matrix jqj\u0002jqj right-invariant equivalence always induces automaton structure; hence probabilistic nerode relation induces probabilistic automaton states equivalence classes transition structure arises follows states formalizing construction introduce notion probabilistic automata initial ﬁnal states. deﬁnition initial marked probabilistic ﬁnite state automaton quintuple note probability null word unity state. current state next symbol speciﬁed next state ﬁxed; similar probabilistic deterministic automata however unlike latter lack ﬁnal states model. additionally assume graphs strongly connected. notion canonical representations along symbolic derivatives used establish correctness stream operations section note canonical representation free notion initial states; intuitively translates ability carry stream operations without knowledge initial states hidden models. notion symbolic derivatives along proposition establishes derivatives computed suﬃciently long observed sequences match closely underlying generative pfsas also close. detailed formulation proves conclude distance underlying models small high probability fig. synchronizable non-synchronizable machines. synchronization determination current state observed past symbols. pfsas synchronizable machine synchronizable bottom not. note history symbol sufﬁces determine current state synchronizable machine ﬁnite history non-synchronizable machine \u000f-synchronizing string always exists pfsa true deterministic automata initial states unimportant; denote initial-marked pfsa induced initial marking removed refer simply pfsa. states representable states elements next show always encounter state arbitrarily close element canonical construction starting stationary distribution states next introduce notion \u000f-synchronization probabilistic automata synchronization automata ﬁxing determining current state. pfsas synchronizable \u000f-synchronizable deﬁnition string \u000fsynchronizing pfsa next introduce notion symbolic derivatives note that pfsa states observable; observe symbols generated hidden states. symbolic derivative given string speciﬁes distribution next symbol alphabet. notation denote probability distributions ﬁnite cardinality deﬁnition string count function string generated symbolic derivative deﬁned formulation section indicates symbolic dynamical process probabilistic ﬁnite state description corresponding nerode equivalence ﬁnite index. deﬁnition space pfsa given symbol alphabet denoted space probability measures inducing ﬁnite-index probabilistic nerode equivalence corresponding measure space denoted expected close relationship possibly many pfsa realizations encode probability measure existence non-minimal realizations state relabeling; neither aﬀect underlying encoded measure. perspective notion pfsa equivalence introduced follows deﬁnition pfsa deﬁned equivalent case underlying graphs. assertion crucial development sequel since binary operation deﬁned pfsa identical structure extended general case account deﬁnition proposition next show restricted pfsa subspace assigned algebraic structure abelian group. ﬁrst construct abelian group subspace probability measures induce group structure subspace pfsa isomorphism spaces. deﬁnition proof closure property commutativity obvious. associativity existence identity existence inverse element established next. associativity have independence claim follows immediately noting random erasure executed stated algorithm eliminates possibility synchronization states underlying model limit inﬁnite string lengths consider pfsas jsj; bring structure transformations respectively denote transition matrices pfsas transformed representations respectively. denoting claim show independent stream copy operation produces independent realization pseudo-copy pfsa model generating input stream. first formalize notion pseudocopies. notation given string underlying pfsa generator denoted given pfsa realization generated note automatically implies referring pfsa generator since cannot unique generator bounded strings. proposition given symbol stream hidden pfsa generator stream generated generate stream read current symbol write output move read positions step right step then have proof probability ﬁrst symbol input stream recorded output. since stream conclude also constant probability symbol recorded. thus assuming symbolic derivatives computed exact assume without loss generality canonical representations thus streams assumed start states canonical representation maps corresponding stationary distribution states corresponding initial-marked pfsas also note delete arbitrary preﬁxes still assume start states. thus delete preﬁxes point next symbols match ﬁrst output symbol. also since assumed graph without loss generality follows next hidden states seeing ﬁrst output symbol still synchronized. thus conclude ﬁrst observed symbol probability next symbol also given stream summation algorithm thought producing output sequence traversing arcs canonical representation augmented jumps unreported unlabeled transitions back state corresponding equivalence class state whenever mismatch jump back probabilities back transitions easily computed important here. however implies state canonical representation stream summation proposition given symbol streams hidden pfsa generators stream generated read current symbols write output move read positions step right step then denoting generator have completes proof. remark note lower bound established claim obviously tight; since exact summation whereas bound factor j\u0006j. remark proposition establishes stream summation algorithm works perfectly summands state machine includes fwn. arbitrary inputs deviation realized small deviation original models small; conversely deviation realized large deviation original models large. stream inversion lemma streams independent realizations pfsa deﬁned alphabet generated follows read current symbols distinct write output move read positions step right step then have next consider following construction consider pfsa streams traversing transitions symbollabeled arcs initialized state note symbol output current symbols input streams distinct; occur possible ways synchronization; symbols generated distinct second case assume re-initialization occurs; streams jump back state distinct symbols generated causing output symbol. note next symbol produced silent jump state additionally probability jump occurs speciﬁc state explicit function parameters pfsa however need compute probabilities; simply conclude that fig. upper bound annihilation efﬁciency alphabet size j\u0006j. illustrates quantization would necessitate large amounts data pass self-annihilation test. proof algorithmic steps operations stream copy stream summation proceed symbol-by-symbol fashion memory previous symbols. also step involves constant number integer comparisons. assuming symbol processes involved generated constant complexity conclude asymptotic time complexity stream summation stream copy pass self-annihilation test data stream must suﬃciently long; required length input speciﬁed threshold dictated characteristics generating process. thus rate convergence self-annihilation error function quantiﬁes sample complexity information annihilation. obtained stream inversion obtained stream summation then follows always realization process uniform probability generating symbol point. thus vectors empirical distributions converge distribution additionally central limit theorem dictates process input however selective erasure annihilation implies expected shortening ratio indeed depend generating process. refer annihilation eﬃciency since convergence fig. convergence rate self-annihilation error shown scale dictated central limit theorem. convergence rates depend directly descriptional complexity generating processes; note data state process slower convergence rate compared four state process. discussed section computational complexity convergence finally establish causality claim deviation estimated function ﬁnite observed string consideration ﬁnite histories length bounded converges deviation underlying process limit inﬁnite data thus follows distance calculated annihilating stream second stream converges absolute deviation generator proof generate ﬁrst need generate independent stream copies clear stated algorithm independent stream copy expected length copies probability obtaining symbol output comparing streams simply probability seeing diﬀerent symbol copied streams denoting probability have harmonic mean probability vector thus expected length jsj. ﬁnal step stream summation obtain note probability seeing symbol follows stream summation would result expected length combined completes proof. information annihilation operates symbolic sequences. thus need specify quantization scheme possibly continuousvalued data streams symbolic sequences. accomplished choice symbol alphabet letter alphabet denotes slice data range. given particular quantization scheme continuous-valued observation symbol representing slice data range observation belongs. thus chosen quantization scheme incurs error made small using quantization large alphabet. however length observed data limits size alphabet use. direct consequence fact annihilation eﬃciency falls rapidly alphabet size thus input stream obtained stream inversion output stream summation expected ratio lengths jsj=jsj falls rapidly alphabet size increased making estimation deviation diﬃcult. since convergence rate self-annihilation error frequency rarest symbol quantized data streams small. ensure symbols represented faithfully according generation probability state hidden model; occurrences particular symbol represent statistical ﬂuctuations rather generation probabilities. large approach choosing quantization satisﬁes stated properties following restrict maximum-entropy quantizations schemes symbol occurs frequency data set. fig. plates show three maximum-entropy schemes eeg-dataset. alphabet size increased choose slices datarange slice contains approximately equal number data points. example plate fig. four slices contains approximately total number observations data set. maximum-entropy schemes guarantee property satisﬁed. remaining properties plot mean self-annihilation error mean discrimination alphabet size. expected ﬁner alphabets lead high average discrimination time incur high average self-annihilation errors ratio quantities useful fig. note trinary maximum-entropy quantization minimizes ratio; implying high discrimination self-annihilation error. fig. plates- maximum-entropy quantization schemes dataset alphabet sizes respectively explained text maximum-entropy context refers fact data slice contains approximately equal number observations. plate shows average selfannihilation error well average discrimination different data streams increases exponentially alphabet size. plate shows ratio average self-annihilation error average discrimination minimum alphabet size fig. distinct pfsa models initial sections generated sample streams streams generated independent runs model near zero mutual information correctly evaluated similar generators data smashing also runs different models also near-zero mutual information smashing correctly reveals signiﬁcant difference generators. note chosen quantization coarse distinct processes evaluate similar. however coarse alphabet produces errors direction; identical processes still evaluate identical provided streams pass self-annihilation test. smashing ﬁnite quantized data streams manipulates statistical information contained them. notions information-theoretic interdependence sequential data investigated literature extensively; concept mutual information between streams. discrete random variables mutual information quantiﬁes amount information random variable contains another. formally discrete random variables alphabets probability mass function robfx robfy also considering single vector vector-valued random variable mutual information related notion entropy entropy random variable measure amount information required average describe random variable; mutual information amount information variable contains other; precisely degree uncertainty reduced knowing other. thus sharing common generative process imply high mutual information; conversely high mutual information indicative sort statistical synchronization generative processes; diﬀerent themselves. thus concept mutual information data smashing \"orthogonal\" sense measure statistical dependence computing former streams need statistically independent latter work. note computation anti-stream generated streams approximate independent copies input stream manipulated yield inverse. algorithm requires independence; absence proposition falls apart. illustrate points simple example consider simple one-state pfsas diﬀerent event probabilities generated streams note simply running\" given pfsa twice choosing start state randomly generating symbols accordance event probabilities implies generations independent. smash streams other compute pairwise distance matrix shown table iii. note streams annihilate nearly perfectly stream streams fail results clearly clustered values table correctly indicate streams identical generators diﬀer signiﬁcantly other. pairwise computation mutual information streams expected reveal generative structure. since streams generated independently mutual information distinct streams would zero generate time series data using gillespie’s stochastic simulation algorithm. plates show prey numbers varying time three different runs different values. thus summarize mutual information measures degree statistical dependence data streams; data smashing computes distance generative processes provided data streams independent nearly pairwise distances computed data smashing clearly function statistical information buried streams. however might easy right statistical tool mine information particular problem. section provide example dynamical system data smashing able recover meaningful nontrivial structure missed simple statistical measures. consider lotka-volterra system stochastic reactions modeling simple closed eco-system species preys other. deterministic diﬀerential equation models system exist realistic model three simple reactions primarily ability model stochastic component. generally accepted method solve systems produce time traces population numbers gillespie’s stochastic simulation algorithm. consider propensity reactions parameterized ranges steps reaction parameters simulate system times maximum using gillespie’s algorithm. simulation initialize system fig. compute distances simulated time series population numbers species first column obtained pairwise smashing. second column obtained absolute difference means third column obtained absolute difference variances. second illustrates effect normalizing data make time series zero mean. third illustrates effect normalization make series zero mean unit variance. data smashing yields clear clusters preserved normalization; whereas simple statistical measures not. note probability reaction point time proportional number combinatorial ways particular reaction transpire well propensity reaction itself. combinatorial number function current population count species; hence reaction probabilities strongly dependent current state vector. since simulation terminates species becomes extinct cannot assume stationary behavior. also initial state least partially dictates length time ecosystem survives implying non-ergodicity. note removing restriction strictly positive integervalued population count might result well-behaved system. given parameterization actually distinct systems diﬀerent sets parameters system index propensity third reaction since third reaction models predator death expect increasing propensity make predator degradation probable. thus clearly expect smaller number predators larger number prey average increased. however truly interesting structure would uncovered behaviors exhibit sort clustering; opposed simply monotonic dependence aanalyzed r-parameterized dynamical systems follows also generated pairwise absolute diﬀerences means variances. cases ijth entry corresponding matrix absolute diﬀerence corresponding statistical measure data series fig. notably smashing matrix plate fig. shows clear clusters whereas matrices corresponding mean variance show trivial monotonic dependence parameter ascertain clustering obtained data smashing dependent mean variance input data streams redid analysis after ﬁrst case zeroing mean makes clusters appear prominently fig. additionally normalizing variance little eﬀect fig. none changes allow simple statistical measures recover clear clusters obtained data smashing. lotka-volterra system rich dynamical regimes would surprising measures fail capture complexity. eﬀect plotted minimum number predators simulation fig. mapping clusters recovered data smashing meaningful dynamical feature. plotting minimum number predators remaining system ﬁxed simulation time illustrates clusters almost perfect correspond monotonic domains function. clusters identiﬁed data smashing seen correspond almost perfectly monotonic domain function. illustrates data smashing ﬁnds meaningful categorization simple statistical tools miss. diﬀerences discovered smashing obviously function statistical structure observed data. however preceding example illustrates easy right statistical tool. data smashing approach alleviates challenge considerable degree. introduced data smashing measure causal similarity series sequential observations. demonstrated insight allows feature-less model-free classiﬁcation diverse applications without need training expert tuned heuristics. non-equal length time-series missing data possible phase mismatches consequence. better classiﬁcation algorithms exist speciﬁc problem domains algorithms diﬃcult tune. strength data smashing lies ability circumvent need expert-deﬁned heuristic features expensive training; eliminating bottlenecks contemporary data challenges.", "year": 2014}