{"title": "DRAW: A Recurrent Neural Network For Image Generation", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.", "text": "figure trained draw network generating mnist digits. shows successive stages generation single digit. note lines composing digits appear drawn network. rectangle delimits area attended network time-step focal precision indicated width rectangle border. core draw architecture pair recurrent neural networks encoder network compresses real images presented training decoder reconstitutes images receiving codes. combined system trained end-to-end stochastic gradient descent loss function variational upper bound log-likelihood data. therefore belongs family variational auto-encoders recently emerged hybrid deep learning variational inference signiﬁcant advances generative modelling draw differs siblings that rather generatpaper introduces deep recurrent attentive writer neural network architecture image generation. draw networks combine novel spatial attention mechanism mimics foveation human sequential variational auto-encoding framework allows iterative construction complex images. system substantially improves state generative models mnist trained street view house numbers dataset generates images cannot distinguished real data naked eye. person asked draw paint otherwise recreate visual scene naturally sequential iterative fashion reassessing handiwork modiﬁcation. rough outlines gradually replaced precise forms lines sharpened darkened erased shapes altered ﬁnal picture emerges. approaches automatic image generation however generate entire scenes once. context generative neural networks typically means pixels conditioned single latent distribution well precluding possibility iterative self-correction shot approach fundamentally difﬁcult scale large images. deep recurrent attentive writer architecture represents shift towards natural form image construction parts scene created independently others approximate sketches successively reﬁned. obvious correlate generating images step step ability selectively attend parts scene ignoring others. wealth results past years suggest visual structure better captured sequence partial glimpses foveations single sweep entire image main challenge faced sequential attention models learning look addressed reinforcement learning techniques policy gradients attention model draw however fully differentiable making possible train standard backpropagation. sense resembles selective read write operations developed neural turing machine following section deﬁnes draw architecture along loss function used training procedure image generation. section presents selective attention model shows applied reading modifying images. section provides experimental results mnist street view house numbers cifar- datasets examples generated images; concluding remarks given section lastly would like direct reader video accompanying paper contains examples draw networks reading generating images. basic structure draw network similar variational auto-encoders encoder network determines distribution latent codes capture salient information input data; decoder network receives samples code distribuion uses condition distribution images. however three differences. firstly encoder decoder recurrent networks draw sequence code samples exchanged them; moreover encoder privy decoder’s previous outputs allowing tailor codes sends according decoder’s behaviour far. secondly decoder’s outputs successively added distribution ultimately generate data opposed emitting distribution single step. thirdly dynamically updated attention mechanism used restrict input region observed encoder output region modiﬁed decoder. simple terms network decides time-step where read where write well figure left conventional variational auto-encoder. during generation sample drawn prior passed feedforward decoder network compute probability input given sample. inference input passed encoder network producing approximate posterior latent variables. training sampled used compute total deminimised stochastic gradient descent. right draw network. time-step sample prior passed recurrent decoder network modiﬁes part canvas matrix. ﬁnal canvas matrix used compute inference input read every timestep result passed encoder rnn. rnns previous time-step specify read. output encoder used compute approximate posterior latent variables time-step. function enacted encoder network single time-step. output time similarly output encoder hidden vector henc decoder hidden vector hdec general encoder decoder implemented recurrent neural network. experiments long short-term memory architecture both extended form forget gates favour lstm proven track record handling long-range dependencies real sequential data throughout paper notation denote linear weight matrix bias vector vector time-step encoder receives input image previous decoder hidden vector precise form encoder input depends hdec read operation deﬁned next section. encoder used parameterise output henc latent vector distribution number nats required decoder reconstruct given total loss therefore equivalent expected compression data decoder prior. image generated draw network iteratively picking latent samples prior running decoder update canvas matrix ˜ct. repetitions process generated image sample bernoulli distributions common gaussians latent variables auto-encoders however great advantage gaussian latents gradient function samples respect distribution parameters easily obtained using so-called reparameterization trick makes straightforward back-propagate unbiased variance stochastic gradients loss function latent distribution. drawn time-step sample cumulative canvas matrix ultimately used reconstruct image. total number time-steps consumed network before performing reconstruction free parameter must speciﬁed advance. error image concatenation vectors single vector denotes +exp. note logistic sigmoid function depends henc history previous latent samples. sometimes make dependency explicit writing shown fig. henc also passed input read operation; however helped performance therefore omitted ﬁnal canvas matrix used parameterise model input data. input binary natural choice bernoulli distribution means given reconstruction loss deﬁned simplest instantiation draw entire input image passed encoder every time-step decoder modiﬁes entire canvas matrix every time-step. case read write operations reduce however approach allow encoder focus part input creating latent distribution; allow decoder modify part canvas vector. words provide network explicit selective attention mechanism believe crucial large scale image generation. refer conﬁguration draw without attention. endow network selective attention without sacriﬁcing beneﬁts gradient descent training take inspiration differentiable attention mechanisms recently used handwriting synthesis neural turing machines unlike aforementioned works consider explicitly twodimensional form attention array gaussian ﬁlters applied image yielding image ‘patch’ smoothly varying location zoom. conﬁguration refer simply draw someresembles afﬁne transformations used computer graphics-based autoencoders illustrated fig. grid gaussian ﬁlters positioned image specifying co-ordinates grid centre stride distance adjacent ﬁlters. stride controls ‘zoom’ patch; larger stride larger area original image visible attention patch lower effective resolution patch grid centre stride determine mean location ﬁlter column patch follows parameters required fully specify attention model isotropic variance gaussian ﬁlters scalar intensity multiplies ﬁlter response. given input image attention parameters dynamically determined time step figure left grid ﬁlters superimposed image. stride centre location indicated. right three patches extracted image green rectangles left indicate boundary precision patches patches shown right. patch small high giving zoomed-in blurry view centre digit; middle patch large effectively downsampling whole image; bottom patch high generated network always novel virtually indistinguishable real data mnist svhn; generated cifar images somewhat blurry still contain recognisable structure natural scenes. binarized mnist results substantially improve state art. preliminary exercise also evaluate attention module draw network cluttered mnist classiﬁcation. experiments model input data bernoulli distribution means given mnist experiments reconstruction loss usual binary cross-entropy term. svhn cifar- experiments green blue pixel intensities represented numbers interpreted independent colour emission probabilities. reconstruction loss therefore cross-entropy pixel intensities model probabilities. although approach worked well practice means training loss correspond true compression cost images. experiments network hyper-parameters presented table adam optimisation algorithm used throughout. examples generation sequences mnist svhn provided accompanying video test classiﬁcation efﬁcacy draw attention mechanism evaluate performance cluttered translated mnist task image cluttered mnist contains many digit-like fragments visual clutter network must distinguish true digit classiﬁed. illustrated fig. iterative attention model allows network progressively zoom relevant region image ignore clutter outside model consists lstm recurrent network receives ‘glimpse’ input image time-step using selective read operation deﬁned section ﬁxed number glimpses network uses softmax layer classify mnist digit. network similar recently introduced recurrent attention model except attention method differentiable; therefore refer differentiable ram. figure zooming. left original image. middle patch extracted gaussian ﬁlters. right reconstructed image applying transposed ﬁlters patch. bottom gaussian ﬁlters displayed. ﬁrst used produce top-left patch feature. last ﬁlter used produce bottom-right patch feature. using different ﬁlter weights attention moved different location. note ﬁlterbanks used image error image. write operation distinct attention parameters extracted hdec order transposition reversed intensity inverted assess ability draw generate realisticlooking images training three datasets progressively increasing visual complexity mnist street view house numbers cifar- images table negative log-likelihood test-set example binarised mnist data set. right hand column present gives upper bound negative loglikelihood. previous results figure cluttered mnist classiﬁcation attention. sequence shows succession four glimpses taken network classifying cluttered translated mnist. green rectangle indicates size location attention patch line width represents variance ﬁlters. trained full draw network generative model binarized mnist dataset dataset widely studied literature allowing compare numerical performance draw existing methods. table shows draw without selective attention performs comparably recent generative models darn nade dbms draw attention considerably improves state art. figure generated mnist images. digits generated draw except rightmost column shows training images closest column second right note network trained binary samples generated images mean probabilities. draw network trained generated mnist digits following method section examples presented fig. fig. illustrates image generation sequence draw network withselective attention interesting compare generation sequence draw attention depicted fig. whereas without attention progressively sharpens blurred image global main motivation using attention-based generative model large images built iteratively adding small part image time. test capability controlled fashion trained draw generate images mnist images chosen random placed random locations black background. cases digits overlap pixel intensities added together point clipped greater one. examples generated data shown fig. network typically generates digit other suggesting ability recreate composite scenes simple pieces. mnist digits simplistic terms visual structure keen well draw performed natural images. ﬁrst natural image generation experiment used multi-digit street view house numbers dataset used preprocessing yielding house number image training example. network trained using patches extracted random locations preprocessed images. svhn training contains images validation contains images. figure generated svhn images. rightmost column shows training images closest generated images beside them. note columns visually similar numbers generally different. figure svhn generation sequences. rectangle indicates attention patch. notice network draws digone time moves scales writing patch produce numbers different slopes sizes. figure training validation cost svhn. validation cost consistently lower validation patches extracted image centre network never able overﬁt training data. paper introduced deep recurrent attentive writer neural network architecture demonstrated ability generate highly realistic natural images photographs house numbers well improving best known results binarized mnist generation. also established two-dimensional differentiable attention mechanism embedded draw beneﬁcial image generation also image classiﬁcation. many assisted creating paper especially thankful koray kavukcuoglu volodymyr mnih jimmy yaroslav bulatov greg wayne andrei rusu shakir mohamed. gregor karol danihelka mnih andriy blundell charles wierstra daan. deep autoregressive networks. proceedings international conference machine learning larochelle hugo hinton geoffrey learning combine foveal glimpses third-order boltzmann machine. advances neural information processing systems murray iain salakhutdinov ruslan. evaluating probabilities high-dimensional latent variable models. advances neural information processing systems raiko tapani kyunghyun bengio yoshua. iterative neural autoregressive distribution estimator nade-k. advances neural information processing systems rezende danilo mohamed shakir wierstra daan. stochastic backpropagation approximate inference deep generative models. proceedings international conference machine learning salakhutdinov ruslan murray iain. quantitative analysis deep belief networks. proceedings annual international conference machine learning omnipress zheng zemel richard zhang yu-jin larochelle hugo. neural autoregressive approach attention-based recognition. international journal computer vision", "year": 2015}