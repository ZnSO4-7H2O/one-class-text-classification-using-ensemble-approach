{"title": "Collaborative Representation for Classification, Sparse or Non-sparse?", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Sparse representation based classification (SRC) has been proved to be a simple, effective and robust solution to face recognition. As it gets popular, doubts on the necessity of enforcing sparsity starts coming up, and primary experimental results showed that simply changing the $l_1$-norm based regularization to the computationally much more efficient $l_2$-norm based non-sparse version would lead to a similar or even better performance. However, that's not always the case. Given a new classification task, it's still unclear which regularization strategy (i.e., making the coefficients sparse or non-sparse) is a better choice without trying both for comparison. In this paper, we present as far as we know the first study on solving this issue, based on plenty of diverse classification experiments. We propose a scoring function for pre-selecting the regularization strategy using only the dataset size, the feature dimensionality and a discrimination score derived from a given feature representation. Moreover, we show that when dictionary learning is taking into account, non-sparse representation has a more significant superiority to sparse representation. This work is expected to enrich our understanding of sparse/non-sparse collaborative representation for classification and motivate further research activities.", "text": "abstract—sparse representation based classiﬁcation proved simple effective robust solution face recognition. gets popular doubts necessity enforcing sparsity starts coming primary experimental results showed simply changing l-norm based regularization computationally much efﬁcient l-norm based non-sparse version would lead similar even better performance. however that’s always case. given classiﬁcation task it’s still unclear regularization strategy better choice without trying comparison. paper present know ﬁrst study solving issue based plenty diverse classiﬁcation experiments. propose scoring function pre-selecting regularization strategy using dataset size feature dimensionality discrimination score derived given feature representation. moreover show dictionary learning taking account non-sparse representation signiﬁcant superiority sparse representation. work expected enrich understanding sparse/non-sparse collaborative representation classiﬁcation motivate research activities. introduction recently simple approach called sparse representation based classiﬁcation shown quite impressive results face recognition also classiﬁcation tasks minimizes l-norm based error reconstructing test sample linear combination training samples whilst limiting sparsity reconstruction coefﬁcients. sparsity term tends force larger coefﬁcients assigned training samples class test sample belongs making coefﬁcients discriminative classiﬁcation. since ideal l-norm modeling sparsity leads computationally expensive even infeasible combinatorial optimization problem adopts l-norm approximate l-norm though still time consuming unavoidable iterative optimization. main weakness preconditions ensuring good performance training samples need carefully controlled number samples class sufﬁciently large lack quantitative criteria verifying whether satisﬁed not. later research argued src’s success lies collaborative representation using training samples l-norm based regularization makes representation coefﬁcients sparse shown l-norm replaced computationally much efﬁcient lnorm without sacriﬁcing performance. therefore better understanding comparison models treated collaborative representation based classiﬁcation jarich department intelligence science technology graduate school informatics kyoto university kyoto japan. e-mail vansteenbergemm.media.kyoto-u.ac.jp. regularization term used differentiate them. follow notation name standing sparse representation non-sparse representation respectively representation coefﬁcients regularized l-norm widely-regarded sparse ones regularized l-norm generally non-sparse. since birth attention paid l-norm based regularization collaborative representation attractive effectiveness efﬁciency. though experiments extensions shown superiority sparse representation competitors counterexamples reported well uncertain relative superiority sparse non-sparse models confuses people limited research experiences them still lack in-depth reliable criterion preselecting promising model given task. paper presents study solving problem. speciﬁcally contribute aspects propose analytic scoring function depending given feature vectors size dataset predicting whether model sparse non-sparse. extensive representative experiments various classiﬁcation tasks demonstrated effectiveness function. discuss important direction extending collaborative representation dictionary learning whilst proposing simple dictionary learning approach non-sparse collaborative representation aware ﬁrst kind. extensive experiments shown dl-nscr generally superior similar well state-of-the-art dictionary learning models rest parts paper organized follows. brief introduction background knowledge collaborative representation given section commenting related work section section presents details dlnscr. experiments results stated analyzed section conclusions future work given section related work comparison proposing zhang et.al. done experiments comparing along robust versions handling occlusions/corruptions. concluded relative superiority depends feature dimensionality data. supposed high dimensionality corresponds high discrimination power case coefﬁcients tend naturally passively sparse even without sparse regularization better dimension lead opposite result. agree point favors higher dimension don’t think relative superiority depends feature dimensionality. note assumption correspondence feature dimensionality data discrimination power zhang et.al.’s work unreliable arbitrarily different features quite different discrimination abilities given dimensionality. even feature vectors projected different spaces using certain dimension reduction approach still counterexamples effectiveness using dimensionality indicator shown besides issue another drawback zhang et.al.’s experiments limited face datasets though different subsets/variations tested comparison extended models debate sparse non-sparse collaborative representation limited simplest models. recently model called extended added another generic dictionary based thirdparty dataset handling within-class variations. since additional dictionary able cover possibly large changes test sample corresponding training samples class esrc applied single-shot recognition problems single training sample available class. later non-sparse version extended published. difference ecrc esrc ecrc uses lnorm instead l-norm coefﬁcients regularization. experimental results several widely used face recognition datasets showed ecrc much faster effective esrc. though interesting valuable comparison limitation models depend third-party data brings problems relative important unexplored area dictionary learning another important inﬂuential direction enhancing collaborative representation models dictionary learning i.e. learning discriminative dictionary training data instead directly using dictionary. generally speaking dictionary learning signiﬁcantly improve discrimination generalization abilities models without relying additional data. quite publications found sparse representation aware model ever proposed non-sparse representation. existing models roughly grouped three categories making dictionary discriminative learning discriminative model classiﬁcation coefﬁcients simultaneously optimizing dictionary coefﬁcients-based classiﬁcation model. ﬁrst group follow classiﬁcation model using class-speciﬁc reconstruction error directly targeting discriminative dictionary. second group learn discriminative classiﬁcation models sparse representation coefﬁcients including logistic regression linear regression lc-ksvd adds linear regression term d-ksvd enhance label consistency within class). third group contain representative work called fisher discrimination dictionary learning discriminative ﬁdelity term minimizes reconstruction error using global dictionary classspeciﬁc sub-dictionaries time minimizes ability sub-dictionaries reconstructing samples different classes. besides that fddl also discriminative coefﬁcient term utilizes fisher discriminant make coefﬁcients discriminative. recently model called dl-copar developed idea proposed dlsi exploring common bases subdictionaries explicitly separating particularity commonality dictionary learning. meanwhile also inherited incoherence term third part ﬁdelity term fddl make class-speciﬁc sub-dictionaries discriminative possible. despite differences learning discriminative model approaches enforce sparsity coefﬁcients using either l-norm l-norm usually computationally expensive. collaborative representation dictionary learning sparse representation suppose training dataset rd×n given denotes total number samples; denotes rd×k learned dictionary denotes reconstruction coefﬁcients denotes learned parameters discriminative model classiﬁcation discriminative reconstruction model deﬁned trade-off parameters. though existing dictionary learning models covered general model term) vary detailed design resulting different performances speeds. however proposed dictionary learning models iteratively optimize model parameters coefﬁcients difﬁculty optimizing simultaneously. mentioned before explored. dictionary learning non-sparse representation proposed dl-nscr model inherits design term fddl discards timeconsuming term keep model light. note difference existing dictionary learning models adopts computationally efﬁcient l-norm regularize k·kf denotes frobenius norm denotes coefﬁcients corresponding sub-dictionary class samples model ﬁrst term overall reconstruction error second term class-speciﬁc reconstruction error third term confusion factor easy tell deﬁnition force discriminative. feature dimension; number classes; denotes samples belonging class seeks reconstruction test sample linear combination training samples time minimizes l-norm sparsity reconstruction coefﬁcients. coding model formulated concatenated reconstruction coefﬁcients corresponding training samples different classes regulatory parameter weighting regularization classiﬁcation computes representation residual class according recent arguments model collaborative representation classes l-norm regularization term truly contributes good face recognition performance. therefore proposed following non-sparse scheme replaces equation attractively solution linear projection independent pre-computed given training data doesn’t require separate optimization process test sample demands. though l-norm based regularizer equation longer sparsity constraint coefﬁcients still potential induce competition among training samples candidate classes cause right class relatively smaller reconstruction error larger lnorm values coefﬁcients. therefore computes normalized residuals instead using training data reconstruction dictionary dictionary learning techniques seek learn compact over-complete dictionary training data scale large amounts training samples note optimizing depends given means updated used update d\\i. chicken-and-egg problem straightforward solution consists updating iteratively convergence. however since iterating optimizing updating converged soon changed recomputed. therefore implementation ignored inner-iteration optimization found still worked quite well. experiments results conduct experiments visual recognition tasks using nine public benchmark datasets. chosen cover different scenarios appearance-based face recognition controlled environment texture recognition focusing texture information leaf categorization using shape information food categorization stuffs rich highly varying color texture shape information appearancebased across-camera person re-identiﬁcation uncontrolled environment. besides that follow varying number samples class adopted reidentiﬁcation datasets different values investigate factor inﬂuences performance tested models. therefore totally different data settings experiments. denoting dictionary size matrices selecting speciﬁc subdictionaries denote zero matrix identity matrix respectively. optimization problem rewritten simpler form optimizing given ﬁxed ﬁxing term becomes constant however still impossible optimized whole objective function equation terms functions sub-dictionaries overall dictionary therefore optimize one-by-one assuming others ﬁxed. concretely extensive diverse experiments extend scope collaborative representation face recognition general recognition/classiﬁcation tasks various statistics properties feature representations. different many experiments literature artiﬁcially generated versions dataset artiﬁcial data override true factors we’re looking for. methods compared experiment used exactly features data splits regulatory parameter sparse/non-sparse regularization term concerned models fair comparison. speciﬁcally person re-identiﬁcation datasets others proved good choice. notice ﬁnding best value regulatory parameter analyzing sensitivity method important open issues focus paper. face recognition. face recognition choose widely-used datasets extended yale extended yale dataset contains frontal-face images belonging individuals. images captured different lighting conditions various facial expressions. preprocessed data according using cropped images size pixels; randomly selecting half samples training testing projecting image -dimensional feature space using random matrix generated zeromean normal distribution rows normalized size class-speciﬁc dictionary dictionary learning models suggested dl-copar compared with common sub-dictionary size dataset contains variations extended yale dataset including illumination expression disguises changes. following subset subjects images containing illumination expression changes -dimensional eigenfaces feature representation. dl-copar common sub-dictionary assigned size since training data test data ﬁxed round experiment conducted. texture recognition. work representative datasets kth-tips dataset curet dataset many samples class great within-class variations including illumination viewpoint scale changes. also different sense kthtips greater within-class variations curet signiﬁcantly classes. adopt -dimensional pri-colbp feature descriptor proposed high performances. chosen datasets suggested common sub-dictionary size dl-copar leaf categorization. popular swedish leaf dataset used leaf recognition. carefully built leaves well-aligned open complete. side leaf photographed clean background. though strict settings make problem much easier might real applications advantage making problem clean focused i.e. distinguishing different leaf species mainly shapes. dataset contains species leaves images them. following state-of-the-art model spatial pact images class sampled training rest left testing. common sub-dictionary size dl-copar again pri-colbp feature descriptor. food categorization. relatively less popular visual recognition problem difﬁculty lack good benchmark datasets. recently released pittsburgh food image dataset might good starting point. owns fast food images videos collected chain restaurants acquired realistic settings. following yang et.al. focus categories speciﬁc food items background removed. three different instances category bought different chain stores different days. instance images taken different viewpoints. follow standard experimental protocol using images instances training images third instance testing. allows -fold cross-validation. -dimensional pri-colbp feature extracted color channel thus whole feature vector dimensional. dictionary learning models common sub-dictionary dl-copar size person re-identiﬁcation. experiment three recently built datasets ilids-ma ilids-aa caviarreid. representatives cross-camera re-identiﬁcation non-overlapping views real scenarios. ﬁrst collected i-lids video surveillance data captured airport third consists several sequences ﬁlmed shopping centre. ilidsdataset persons exactly manually cropped images camera person resulting images total. unlike ilids-ma ilids-aa dataset extracted automatically using hog-based detector instead manual annotation. property simulates localization errors human detection tracking real systems. moreover ilids-aa also much bigger ilids-ma. contains many individuals totaling images. since automatically annotated number images person varies three different versions dataset named ilidsma ilids-ma etc. note ilids-ma data sampling result. compared ilidsilids-aa datasets caviarreid broader resolution changes larger pose variations. follow training speciﬁed subjects testing subjects. contains randomly sampled images. like perform multipleshot re-identiﬁcation treat set-based classiﬁcation problem. therefore set-based classiﬁcation model dlnscr used. person re-identiﬁcation problem usually treated ranking problem desired correct match given querying individual appears top-ranked candidates used cumulative recognition rate rank effectiveness measure. used exactly -dimensional color texture histograms based features adopted methods. chosen number samples class ilids-ma dataset caviarreid dataset ilids-aa dataset common sub-dictionary size dl-copar value datasets. sparse non-sparse representation? justifying whether collaborative representation sparse non-sparse compare l-norm regularization l-norm regularization i.e. compare results shown table clearly none completely outperforms other though wins datasets. therefore question becomes shall choose want better performance expect answer applying data. easy comparison them propose relative superiority measure called error reduction rate deﬁned denote error rate accuracy rate concerned model respectively. shows much performance improvement replacing therefore positive values indicate performs better negative ones stand opposite. larger value data favors non-sparse representation. order predict much speciﬁc dataset might favor non-sparse representation knowing value necessary design scoring function coincides err. function statistics properties dataset. therefore list representative statistics datasets shown table including feature dimensionality number classes number training samples class total number training samples besides that believe datasets properties independent simple statistics. important properties quality features directly inﬂuences performance classiﬁcation model. intuitive feeling larger related better features better features generally enable samples class stay relatively closer thus make easier generate discriminative collaborative representation even non-sparse coefﬁcients. therefore design feature discrimination rate simplest minimum point-wise distance based classiﬁer chance method randomly guessing class label test sample. greater details uses euclidean distance point-wise distance measurement treats minimum point-wise distance test sample training samples belong speciﬁc class dissimilarity them. directly uses dissimilarity classiﬁcation. since performance purely relies feature space makes good measure features’ discriminative power. note uses top- recognition accuracy different err. mpd’s accuracy rates values datasets given table however proportional err. recall many evidences literature showing larger lead higher also test effectiveness fdr×d. unfortunately still coincide err. enhance propose total number training samples involved inversely proportional reason quite simple smaller generally means lower redundancy training samples representation coefﬁcients likely denser sparser. prove that simply test fdr/n already differentiate negative values positive ones. however insist fdr×d/n better choice. four special datasets considered fdr×d/n clearly deﬁnite relationship w.r.t terms linear power function fdr/n shown figure note threshold fdr×d/n experiments differentiating positive values negative ones accurate enough generalization datasets believe good reference. dictionary learning compare simple dictionary learning model nonsparse representation inﬂuential dictionary learning models sparse representation generally complex dl-nscr statistics datasets prediction criteria inferring shall collaborative representation sparse non-sparse comparison actual performances. denote feature dimensionality number testing kl). seen dl-nscr scales less linearly nearly proportionally thus predetermined scales well table presents actual running time concerned methods four representative datasets. clearly dl-nscr fastest dictionary learning model testing time also comparable fastest dictionary learning based sparse representation model. conclusion future work shown promising scoring function pre-selecting sparse non-sparse collaborative representation models. dlnscr simple dictionary learning model non-sparse collaborative representation demonstrated superiority sparse non-sparse collaborative representation models state-of-the-art dictionary learning models sparse representation. still large room enhancing better dictionary learning models interesting comprehensive comparison existing dictionary learning models. including fddl lc-ksvd dl-copar results shown table clearly show even dl-nscr signiﬁcantly promote performance datasets also outperforms competitors classiﬁcation tasks except person re-identiﬁcation task. relatively lower performance boost dl-nscr re-identiﬁcation datasets probably difﬁculty data limits room improvement even though interesting dlnscr performs best datasets favors moreover focus fddl similar dl-nscr wins dl-nscr datasets therefore generally speaking dictionary learning taken account non-sparse representation looks promising sparse representation. note dlnscr simple example promising valuable explore better dictionary learning models nonsparse collaborative representation. complexity running time dl-nscr takes alternative optimization model theoretical guarantee global convergence local qlinear convergence speed experiments always converges within several steps. considering iteration contains basic matrix georghiades belhumeur kriegman from many illumination cone models face recognition variable lighting pose ieee trans. pattern anal. machine intell acknowledgments work supported program implementation anti-crime anti-terrorism technologies safe secure society funds integrated promotion social system reform research development ministry education culture sports science technology japanese government. wright yang ganesh sastry robust face recognition sparse representation ieee trans. pattern anal. machine intell vol. wright mairal spairo huang sparse representation computer vision pattern recognition proceedings ieee yang zhang yang zhang metaface learning sparse representation based face recognition ieee internat. conf. image processing ramirez sprechmann sapiro classiﬁcation clustering dictionary learning structured incoherence shared features ieee conf. computer vision pattern recognition minoh mukunoki collaborative sparse approximation multiple-shot across-camera person reidentiﬁcation ieee international conference advanced video signal-based surveillance", "year": 2014}