{"title": "Progressive EM for Latent Tree Models and Hierarchical Topic Detection", "tag": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "abstract": "Hierarchical latent tree analysis (HLTA) is recently proposed as a new method for topic detection. It differs fundamentally from the LDA-based methods in terms of topic definition, topic-document relationship, and learning method. It has been shown to discover significantly more coherent topics and better topic hierarchies. However, HLTA relies on the Expectation-Maximization (EM) algorithm for parameter estimation and hence is not efficient enough to deal with large datasets. In this paper, we propose a method to drastically speed up HLTA using a technique inspired by recent advances in the moments method. Empirical experiments show that our method greatly improves the efficiency of HLTA. It is as efficient as the state-of-the-art LDA-based method for hierarchical topic detection and finds substantially better topics and topic hierarchies.", "text": "hierarchical latent tree analysis recently proposed method topic detection. differs fundamentally lda-based methods terms topic deﬁnition topic-document relationship learning method. shown discover signiﬁcantly coherent topics better topic hierarchies. however hlta relies expectation-maximization algorithm parameter estimation hence efﬁcient enough deal large datasets. paper propose method drastically speed hlta using technique inspired recent advances moments method. empirical experiments show method greatly improves efﬁciency hlta. efﬁcient state-of-the-art lda-based method hierarchical topic detection ﬁnds substantially better topics topic hierarchies. detecting topics topic hierarchies document collections along many potential applications major research area machine learning. currently predominant approach topic detection latent dirichlet allocation developed detect topics model relationships among them including topic correlations topic hierarchies topic evolution collectively name methods lda-based methods. methods topic probability distribution vocabulary document mixture topics. therefore type mixture membership model. totally different approach hierarchical topic detection recently proposed called hierarchical latent tree analysis topics organized hierarchically latent tree model hlta topic state latent variable corresponds collection documents document belong multiple topics. hlta therefore type multiple membership model. empirical results indicate hlta ﬁnds signiﬁcantly better topics topic hierarchies hierarchical latent dirichlet allocation ﬁrst lda-based method hierarchical topic detection. however hlta scale well. took instance hours process nips dataset consists fewer documents distinct words computational bottleneck hlta lies algorithm parameter estimation. paper propose progressive replacement scale hlta. motivated moments method parameters determined solving equations involves small number model parameters related three observed variables similarly works steps step focuses small part model parameters involves three four observed variables. algorithm hence named pem-hlta. drastically faster hlta. pem-hlta ﬁnished processing aforementioned nips dataset within minutes. took around hours single desktop computer analyze version york times dataset consists articles distinct words. pem-hlta also efﬁcient nhdp state-of-the-art lda-based method hierarchical topic detection signiﬁcantly outperforms nhdp well hlda terms quality topics topic hierarchies. latent tree model markov random ﬁeld undirected tree leaf nodes represent observed variables internal nodes represent latent variables paper assume variables ﬁnite cardinality i.e. ﬁnite number possible states. parameters consist potentials associated edges nodes product potentials joint distribution variables. pick potentials follows root model arbitrary latent node direct edges away root specify marginal distribution root conditional distribution nodes given parent. root parameters distributions forth. potentials picked ltms technically tree-structured bayesian networks ltms single latent variables known latent class models type ﬁnite mixture models discrete data. example model deﬁnes following mixture distribution observed variables cardinality model learned dataset data partitioned soft clusters represented state model latent variables. joint distribution reduced different related mixture distributions algorithm input pem-hlta algorithm collection documents represented binary vector vocabulary values vector indicate presence absence words document. output shown word variables bottom latent variables assumed binary form several levels hierarchy top. state latent variable corresponds cluster documents interpreted topic. level control pem-hlta given algorithm subroutines algorithms level control illustrate level control using example model learned dataset word variables. ﬁrst pass loop subroutine buildislands called partitions variables clusters uni-dimensional sense co-occurrences words cluster properly modeled using single latent variable. latent variable introduced cluster form lcm. metaphorically refer lcms islands latent variables level- latent variables. next step link islands done estimating mutual information every pair latent variables building chow-liu tree them form overall model result model middle subroutine hardassignment inference carried compute posterior distribution latent variable document. document assigned state maximum posterior probability. results dataset level- latent variables second pass loop level- latent variables partitioned groups islands created. islands linked form model shown line model stacked model middle give rise ﬁnal model subroutine stackmodels cuts links among level- latent variables. number nodes level threshold hence loop exited. ﬁnal model steps improve parameters building islands pseudo code subroutine buildislands given algorithm calls another subroutine oneisland identify uni-dimensional subset observed variables builds them. repeats process observed variables left create islands variables included islands. finally returns islands. uni-dimensionality test rely uni-dimensionality test determine whether variables uni-dimensional. idea compare ltms best model among lcms best model among ltms contain latent variables. model selection criterion used score uni-dimensional following inequality holds threshold. words considered uni-dimensional best two-latent variable model signiﬁcantly better best one-latent variable model. quantity left hand side equation large sample approximation natural logarithm bayes factor comparing according cut-off values bayes factor experiments. building island given dataset variables subroutine oneisland identiﬁes uni-dimensional subset variables builds them. deﬁne mutual information variable maxa∈s oneisland maintains working observed variables. initially contains pair variables highest among pairs third variable highest pair line learned three variables using subroutine learnlcm given appendix along subroutines. variables added ud-test fails. illustrate process using suppose initially consists three variables variable maximum among variables. suppose ud-test passes added next variable maximum ud-test performed models used test shown computational efﬁciency search best structure instead structure determined follows pick variable maximum group model model parameters estimated using subroutines pem-lcm pem-ltm-l explained next section. test fails removed remains model returned. test passes added process continues. progressive model construction pem-hlta conceptually consists model construction phase parameter estimation phase ﬁrst phase many intermediate models constructed. section present fast method estimating parameters intermediate models. moments method parameter estimation begin presenting property ltms motivates method. similar property hmms ﬁrst discovered chang introduce notations using since variables cardinality conditional distribution regarded square matrix denote pa|y similarly matrix representation joint distribution value pb|y vector presentation pabc matrix representation diag diagonal matrix components pb|y diagonal elements. equation implies model parameters eigenvalues matrix right hence obtained marginal distributions pabcpac. theorem used estimate conditions good data model data generated model sample size sufﬁciently large. case empirical marginal distributions computed data accurate estimates distributions model. form matrix pabcp determine pb|y eigenvalues matrix. called moments method. note theorem still applies replacing edges like paths. example estimated third observed variable chosen long path observed variable. theorem also used estimate parameters model first estimate using equation sub-model swapping roles variables also estimate sub-model. next consider sub-model estimate ﬁxed. finally consider sub-model estimate ﬁxed. note parameters estimated steps instead once. hence call scheme progressive parameter estimation. progressive moments method iterative hence drastically faster unfortunately produce high quality estimates model data well and/or sample size sufﬁciently large. cases empirical marginal distributions poor estimates distributions model. experiences method frequently gives negative estimates probability values context latent tree models. paper estimate parameters solving equation theorem however adopt progressive estimation scheme combine gives rise progress estimate parameters ﬁrst estimates running sub-model estimates running sub-model ﬁxed; ﬁnally estimates sub-model similarly. sub-models involve observed variables. ﬁrst estimates running sub-model estimates running latent variable sub-model d}-y -z-{c ﬁxed. note children used here model involves observed variables. intuitively moments method tries data rigid tries data elastic manner. never gives negative probability values. moreover still efﬁcient sub-models three four observed binary variables local maxima seldom issue using multiple starting points. island building aligned subroutine oneisland nicely subroutine adds variables working time. consider pass loop. beginning variables whose parameters estimated earlier. oneisland ﬁnds variable outside maximum variable inside maximum line oneisland adds create estimates parameters variable using subroutine pem-lcm. illustrate done using suppose model variable pem-lcm adds variable thereby creates estimate distribution pem-lcm creates temporary model keeping three observed variables variables maximum suppose pem-lcm estimates distribution running parameters ﬁxed. finally copies returns line oneisland adds learns two-latent variable model using subroutine pem-ltm-l. illustrate pem-ltm-l using foregoing example. pem-ltm-l creates model d}-y -z-{c estimate parameters pem-ltm-l creates temporary model d}-y -z-{c children maximum remain. pem-ltm-l estimates three distributions running parameters ﬁxed. finally copies distributions returns similarly subroutine bridgedislands method estimate parameters edges latent variables estimating keeping parameters ﬁxed. empirical results scaling hlta hence need empirically determine efﬁcient pem-hlta compared hlta. also compare pem-hlta nhdp state-of-the-art lda-based method hierarchical topic detection terms computational efﬁciency quality results. also included comparisons hlda method named corex builds hierarchical latent trees optimizing information-theoretic objective function. datasets used nips data newsgroup. three versions nips data vocabulary sizes created choosing words highest average tf-idf values referred nips-k nips-k nips-k. similarly versions newsgroup data created. note news-k included beyond capabilities three methods. comparisons pem-hlta nhdp large-scale data given separately section preprocessing nips newsgroup consist documents respectively. pem-hlta hlta corex data represented binary vectors whereas nhdp hlda represented bags-of-words. pem-hlta determines height hierarchy number nodes level automatically. nips newsgroup datasets produced hierarchies levels. nhdp hlda height hierarchy needs manually usually number nodes level nhdp hlda would yield roughly total number topics pem-hlta. corex conﬁgured similarly. pem-hlta implemented java. parameter settings described section implementations algorithms provided authors default parameter settings. experiments conducted desktop computer. table shows parts topic hierarchies obtained nhdp pem-hlta. left half displays top-level topics nhdp children. nhdp topic represented using words occurring highest probabilities topic. right half show top-level topics yielded pem-hlta children. topics extracted model learned pem-hlta follows latent binary variable model enumerate word variables subtree rooted descending order values leading words whose probabilities differ states hence used characterize states. state words occur less often overall regarded background topic reported state reported genuine topic. values show percentage documents belonging genuine topic. examine topics. refer topics left using letter followed topic numbers right using pem-hlta consists probability terms algorithm; gaussian mixtures generative distributions. combination variance noise separated next lower level. nhdp topic children also probability. however group well. topic image analysis ﬁrst four subtopics different aspects image analysis sources images pixels objects. also meaningful related well. placed another subgroup pemhlta. nhdp subtopics give clear spectrum aspects image analysis. topic speech recognition. subtopics different aspects speech recognition. group well. contrast subtopics present clear semantic hierarchy. meaningful. another topic related speech recognition placed elsewhere. overall topics topic hierarchy obtained pem-hlta meaningful nhdp. topic coherence model quality quantitatively measure quality topics topic coherence score proposed mimno metric depends number words used characterize topic. addition held-out likelihood assess quality models produced algorithms. dataset randomly partitioned training data test data. table shows average topic coherence scores topics produced algorithms. sign indicates running time exceeded hours. quality topics produced pem-hlta similar hlta nips-k news-k better nips-k. cases pemhlta produced signiﬁcantly better topics nhdp algorithms. held-out per-document loglikelihood statistics shown table likelihood values pem-hlta gaussian density likelihood bayesian frey hidden posterior chaining classifier classifiers confidence smola adaboost onoda mika svms speech context experts recognition speech word trained rules rule stack machine examples voicing syllable fault faults units rules hint table hidden structure syllable stress nucleus heavy maximization ghahramani expectation mixture gaussian mixtures covariance generative generafive generarive variance noise variances deviation variance variances deviation noise noisy robust robustness images image features detection face camera video imaging false tracked pixel pixels intensity intensities object objects shape views plane rotation invariant translation nearest neighbor kohonen neighbors word language vocabulary words sequence spoken acoustics utterances speakers string strings grammar symbol symbols retrieval search semantic searching phoneme phonetic phonemes waibel lang speech speaker acoustic hmms similar hlta showing replace inﬂuence model quality much. signiﬁcantly higher corex. note likelihood values table lda-based methods calculated bag-of-words data. still lower methods even calculated binary data three methods. general better model necessarily imply better topic noted that quality context hierarchical topic detection however pem-hlta leads better model also gives better topics better topic hierarchies. table shows running time statistics. pem-hlta drastically outperforms hlta difference increases vocabulary size. nips-k news-k hlta terminate days pem-hlta ﬁnished computation hours. pem-hlta also faster nhdp although difference decreases vocabulary size nhdp works stochastic moreover pem-hlta efﬁcient hlda corex. stochastic conceptually pem-hlta phases hierarchical model construction parameter estimation. second phase predeﬁned number steps initial parameter values ﬁrst phase. time-consuming sample size large. paisley faced similar problem nhdp. solve problem using stochastic inference. idea divide data subsets process subsets one. model parameters updated processing data subset overall goes entire data once.we adopt idea second phase pem-hlta call stochastic tested idea york times dataset consists articles. analyze data picked words using tf-idf randomly divided dataset equal-sized subsets. used subset ﬁrst phase pem-hlta. second phase current model using subset turn subsets utilized york times data compare pem-hlta nhdp since methods amenable processing large datasets observe table still trained nhdp model using documents bag-of-words form pem-hlta using documents binary vectors words. table reports running times topic coherence. pem-hlta took around hours little slower nhdp however pem-hlta produced coherent topics testiﬁed coherence score also resulting topic hierarchies. reader could clear picture superiority pem-hlta nhdp taking quick look model structure topic hierarchies submitted supplements. conclusions proposed investigated method scale hlta newly emerged method hierarchical topic detection. idea replace using progressive resulting algorithm pem-hlta reduces computation time hlta drastically handle much larger datasets. importantly outperforms nhdp state-of-the-art lda-based method hierarchical topic detection terms quality topics topic hierarchy comparable speed large-scale data. although show works hlta possibly used general models. pem-hlta also scaled parallelization used text classiﬁcation. plan investigate directions future. references animashree anandkumar kamalika chaudhuri daniel sham kakade song tong zhang. spectral methods learning multivariate latent tree structure. advances neural information processing systems pages jonathan chang jordan boyd-graber sean gerrish chong wang david blei. reading leaves humans interpret topic models. advances neural information processing systems volume pages teng-fei nevin zhang peixian chen april leonard k.m. poon wang. greedy learning latent tree models multidimensional clustering. machine learning tengfei nevin zhang peixian chen. hierarchical latent tree analysis topic detection. machine david mimno hanna wallach edmund talley miriam leenders andrew mccallum. optimizing semantic coherence topic models. proceedings conference empirical methods natural language processing pages association computational linguistics adrian raftery. bayesian model selection social research. sociological methodology gideon schwarz. estimating dimension model. annals statistics greg steeg aram galstyan. discovering structure high-dimensional data correlation expla-", "year": 2015}