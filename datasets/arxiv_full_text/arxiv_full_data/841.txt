{"title": "Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Deep Belief Networks (DBN) have been successfully applied on popular machine learning tasks. Specifically, when applied on hand-written digit recognition, DBNs have achieved approximate accuracy rates of 98.8%. In an effort to optimize the data representation achieved by the DBN and maximize their descriptive power, recent advances have focused on inducing sparse constraints at each layer of the DBN. In this paper we present a theoretical approach for sparse constraints in the DBN using the mixed norm for both non-overlapping and overlapping groups. We explore how these constraints affect the classification accuracy for digit recognition in three different datasets (MNIST, USPS, RIMES) and provide initial estimations of their usefulness by altering different parameters such as the group size and overlap percentage.", "text": "deep belief networks successfully applied popular machine learning tasks. speciﬁcally applied hand-written digit recognition dbns achieved approximate accuracy rates effort optimize data representation achieved maximize descriptive power recent advances focused inducing sparse constraints layer dbn. paper present theoretical approach sparse constraints using mixed norm non-overlapping overlapping groups. explore constraints affect classiﬁcation accuracy digit recognition three different datasets provide initial estimations usefulness altering different parameters group size overlap percentage. restricted boltzmann machines energy based models extensively used diverse machine learning applications mainly generative unsupervised learning framework. applications range image scene recognition generation video-sequence recognition dimensionality reduction equally important aspect rbms serve building blocks dbns favored machine learning community conditional independence hidden units allows efﬁcient computationally tractable implementation deep architectures. recent years sparsity become important requirement shallow deep architectures. although primarily used statistics optimization tasks order overcome curse dimensionality various applications also serves emulate biologically plausible models human visual cortex shown sparsity integral process hierarchical processing visual information moreover added beneﬁt using sparse constraints form mixed norm regularizers deep architectures alleviate restrictive nature allowing implicit interactions hidden units rbms. mixed norm regularizers extensively used statistics machine learning paper provide initial results inducing sparse constraints using mixed norm regularizer activation probabilities rbms. mixed norm applied non-overlapping overlapping groups. also show regularizer used train dbns offer results task digit recognition using several datasets. type layer neural network comprised visible layer represents observed data hidden layer represents hidden variables addition hidden units allows model increased capacity expressing underlying distribution observed data. visible unit biases hidden unit biases. common case using stochastic binary units visible hidden units conditional probabilities activation obtained intuitively observed data modeled hidden units expressed high conditional probability goal adding sparse constraints network allow salient activation hidden units based differences observed data. result achieve initial clustering observed data increase discriminative power model. general energy based models learnt performing gradient descent negative loglikelihood observed data. speciﬁcally learn parameters network need compute gradient provided given observed data denotes expectation respect evident gradient phases. positive phase tries lower energy training data negative phase tries increase energy model. assessing energy data intractable task given size network number possible conﬁgurations. order obtain approximation hinton successfully proposed contrastive divergence allows sample approximation expectation using gibbs sampling steps. empirically shown setting provide adequate approximation although follow theoretical gradient applying obtain following update equations parameters network. several attempts inducing sparse constraints successful increasing discriminative power models. examples sparse constraints range weight decay modiﬁed norm penalties paper focus generalized penalty mixed norm also provide theoretical practical implementation overlapping groups. refer generalized penalty applied expectations activation probabilities mixed norm mentioned before learning consists performing gradient descent negative log-likelihood. thus deﬁne cost function minimized −logp. applying mixed norm regularizer cost function takes general form regularizer constant. second term deﬁnes mixed norm penalty expectations hidden unit activation probabilities. order apply mixed norm assume hidden units divided groups. groups non-overlapping overlapping. result able penalize whole group individual hidden units. mnrbm non-overlapping groups given hidden units deﬁne partition hidden units groups ...m. groups non-overlapping equal size alleviate computational issues. mixed norm penalty data sample deﬁned practice desire behind application mixed norm penalty groups hidden units zero representing observed data forcing activation probabilities zero. result given observed data sample small number groups hidden units activated leading sparse representation. mnrbm overlapping groups given hidden units deﬁne partition hidden units groups ...m. groups overlapping equal size. depending percentage overlap obtain groups ...k. deﬁne augmented hidden units subsequently given hidden units consider deﬁnes non-overlapping equally sized groups mixed norm penalty data sample deﬁned similar order train mnrbm non-overlapping groups obtain model parameters need minimize cost function presented achieved performing coordinate descent obtained gradients regularizers. applied expectations activation probabilities mixed norm penalty follow trend forcing groups include members activation probabilities towards zero. norm denominator ensures groups activations pushed closer zero. general penalty allows manipulation constant regularizer group size percentage overlap obtain different types architectures. case sparsity induced group level hidden units whereby observed data represented small number groups hidden units. constant empirically determined based task hand. fig. shows sample weights mixed norm using non-overlapping overlapping groups. provides average probability activations hidden units given batch usps training data. seen ﬁgure activation probabilities hidden units appear towards left-hand side ﬁgure desired effect. however appears bimodality whereby large proportion activation probabilities high value non-overlapping groups mnrbm. attributed choice size groups applying mixed norm penalty. given activation probabilities pushed towards high values expect process adverse result classiﬁcation tasks since hidden units over-represent observed data. however case overlapping groups activations pushed towards zero. although goal adding sparse constraints force activations zero case dealing biased system actually represents data distribution. rimes data created asking volunteers write hand written letters different scenarios. paper used digit data base. total used comprised images different sizes information obtained www.rimes-database.fr. order achieve cross-training testing images resized size mnist dataset given extensive task. images also checked ensure orientations/translations uniform across data sets. pre-processing employed. example images three datasets seen ﬁgure figure average hidden unit activation probabilities mixed norm using batch usps data set. y-axis hidden unit activation probabilities mixed norm rbm. x-axis hidden unit activations vanilla rbms became increasingly popular hinton salakhudinov used building blocks creating pre-training efﬁcient dbns. proposed mnrbms utilized manner initialize dbns obtain sparse computationally efﬁcient representation observed data. order offer comparative view different architectures used hinton’s model digit recognition substituted vanilla proposed mnrbm. pre-trained tested three different data sets mnist rimes usps. continuing obtain classiﬁcation error rates added softmax layers posterior probabilities different classes. network ﬁne-tuned using conjugate gradient described constant regularizer empirically different models continuing mixed norm architecture non-overlapping groups used different group sizes hidden units respectively. case overlapping groups used group sizes results classiﬁcation accuracy computational cost models seen table table respectively. experiments performed core server core .ghz cache figure average activation probabilities vanilla mixed norm using batch usps data classiﬁcation accuracy usps data using different architectures table infer proposed mixed norm penalty offer ﬂexibility creating architectures able matchthe classiﬁcation accuracy models depending underlying distributions. appears task hand-written digit recognition distribution observed data favors larger non-overlapping group sizes mixed norm architectures. order better understanding impact different sparse constraints architectures figure depicts average probability density functions expectations activation probabilities mnist training data. interesting note proposed architectures utilize mixed norm penalty with overlapping groups tend aggressively push activation probabilities zero. however architectures also tend offer lower accuracy rates attributed inability models concisely capture underlying data. possible exploring phenomenon constrain penalty expectations seen work provided ﬁrst insights mixed norm sparse constraint dbns. performed experiments using three different data sets task hand written digit recognition offered practical approach overlapping groups mixed norm constraint. although initial experiments limited equal size overlapping groups could easily extended non-symmetric overlapping groups using similar methodology. inducing sparse constraints based speciﬁc geometries also provide better results case digit recognition offer interesting results tasks scene categorization.", "year": 2013}