{"title": "Resource Constrained Structured Prediction", "tag": ["stat.ML", "cs.CL", "cs.CV", "cs.LG"], "abstract": "We study the problem of structured prediction under test-time budget constraints. We propose a novel approach applicable to a wide range of structured prediction problems in computer vision and natural language processing. Our approach seeks to adaptively generate computationally costly features during test-time in order to reduce the computational cost of prediction while maintaining prediction performance. We show that training the adaptive feature generation system can be reduced to a series of structured learning problems, resulting in efficient training using existing structured learning algorithms. This framework provides theoretical justification for several existing heuristic approaches found in literature. We evaluate our proposed adaptive system on two structured prediction tasks, optical character recognition (OCR) and dependency parsing and show strong performance in reduction of the feature costs without degrading accuracy.", "text": "study problem structured prediction test-time budget constraints. propose novel approach applicable wide range structured prediction problems computer vision natural language processing. approach seeks adaptively generate computationally costly features test-time order reduce computational cost prediction maintaining prediction performance. show training adaptive feature generation system reduced series structured learning problems resulting efﬁcient training using existing structured learning algorithms. framework provides theoretical justiﬁcation several existing heuristic approaches found literature. evaluate proposed adaptive system structured prediction tasks optical character recognition dependency parsing show strong performance reduction feature costs without degrading accuracy. structured prediction powerful ﬂexible framework making joint prediction mutually dependent output variables. successfully applied wide range computer vision natural language processing tasks ranging text classiﬁcation human detection. however superior performance ﬂexibility structured predictors come cost computational complexity. order construct computationally efﬁcient algorithms trade-off must made expressiveness speed structured models. cost inference structured prediction broken three parts acquiring features evaluating part responses solving combinatorial optimization problem make prediction based part responses. past research focused evaluating part responses solving combinatorial optimization problem proposed efﬁcient inference algorithms speciﬁc structures general structures however methods overlook feature acquisition part response bottlenecks underlying structure relative simple efﬁciently solved. consider dependency parsing task goal create directed tree describes semantic relations words sentence. task formulated structured prediction problem inference problem concerns ﬁnding maximum spanning trees directed graphs node graph represents word directed edge represents likely depends fig. shows example dependency parse trade rich features prediction time. introducing complex features potential increase system performance however distinguish among small subset difﬁcult parts. therefore computing complex features parts every example computationally costly unnecessary achieve high levels performance. address problem structured prediction test-time budget constraints goal learn system computationally efﬁcient test-time little loss predictive performance. consider test-time costs associated computational cost evaluating feature transforms acquiring sensor measurements. intuitively goal learn system figure left predicting dependency tree dependencies easily resolved less need expressive features making prediction. right system diagram operating region. relatively feature inference costs insigniﬁcant policy must carefully balance overhead costs feedback predictor feature acquisition costs. identiﬁes parts example incorrectly classiﬁed/localized using cheap features additionally yield large reductions error entire structure given expensive features improved distinguishability relationships parts example. consider forms budgeted structured learning problem prediction expected budget constraints anytime prediction. cases consider streaming test-time scenario system operates test example without observation interaction test examples. expected budget constraint setting system chooses features acquire example minimize prediction error subject average feature cost constraint. ﬁxed budget given user training time test-time system tasked allocating resources well determining features acquired allocated resources. anytime structured prediction setting system chooses features acquired sequentially example minimize prediction error time step allowing accurate prediction anytime. budget speciﬁed user training time. insteadthe system sequentially chooses features minimize prediction error time features acquired. setting requires single system able predict budget constraint example. learn systems capable adaptive feature acquisition settings. propose learning policy functions exploit relationships parts adapt varying length examples. problem naturally reduces policy learning structured learning problem allowing original model used minor modiﬁcation. resulting systems reduce prediction cost test-time minimal loss predictive performance. summarize contributions follows formulation structured prediction expected budget constraints anytime prediction. reduction settings conventional structured prediction problems. demonstration structured models beneﬁt access features multiple complexities perform well subset parts expensive features. begin reviewing structured prediction problem formulating expected budget constraint. extend formulation anytime structured prediction. structured prediction goal structured prediction learn function maps input space structure space contrast multi-class classiﬁcation space outputs simply categorical instead assumed exponential space outputs containing underlying structure generally represented multiple parts relationships parts. example dependency parsing features representing sentence parse tree. structured prediction model mapping function often modeled maxy∈y scoring function. assume score broken sub-scores output assignment associated component number sub-components varies across examples. dependency parsing example edge directed graphs indicator variable whether edge parse tree. score parse tree consists scores edges. goal reduce cost prediction test-time consider case variety scoring functions available used component. additionally associated scoring function evaluation cost example deﬁne state space states deﬁned }k×|c| representing features used components prediction. state element indicates feature used prediction component cost evaluating feature single part. assume given structured prediction model maps features state structured label prediction predicted label loss maps predicted true structured label example state deﬁne modiﬁed loss represents error induced predicting label using sensors combined cost acquiring sensors trade-off pattern adjusted according budget small value encourages correct classiﬁcation expense feature cost whereas large value penalizes costly features enforcing tighter budget constraint. deﬁne policy maps feature space initial state state. ease reference refer policy feature selection policy. goal learn policy chosen family functions that given example maps state minimal expected modiﬁed loss argminπ∈π proofs found suppl. material. theorem maps policy learning problem weighted structured learning problem. example example/label pair created state importance weight representing savings lost choosing state unfortunately expansion cost function space states introduces summation combinatorial space states. avoid this instead introduce approximation objective using single indicator function formulate approximate policy pseudo-label deﬁned example weight deﬁned maxs∈s formulation reduces objective summation combinatorial states single indicator function example represents upper-bound original risk. theorem objective upper-bound objective note second term dependent thus theorem leads efﬁcient algorithm learning policy function solving importance-weighted structured learning problem combinatorial search space finding psuedo-label eqn. involves searching combinatorially large search space states computationally intractable. instead present trajectory-based parsimonious pseudo-labels approximating trajectory search trajectory-based pseudo-label greedy approximation optimization eqn. deﬁne ˆst− ˆst− k×|c| none argmins∈s features evaluated components. example obtain trajectory all-one state. choose pseudo-label trajectory note restricting search space states differing single component approximation needs perform polynomial search states opposed exhaustive combinatorial search eqn. observe modiﬁed loss strictly decreasing cost adding features outweigh reduction loss time. empirically approach computationally tractable shown produce strong results. parsimonious search rather trajectory search requires inference update acquire features consider alternative stage update here. idea look -step transitions potentially improve cost. simultaneously update features produce improvement. obviates need trajectory search. addition incorporate guaranteed loss improvement parsimonious search. argmins∈s )≥c+τ}. note potential candidate transitions non-unique thus generate collection potential state transitions obtain suppose margin replace cost-function loss function optimization relatively simple state simply collection transitions sub-components incorrect. finding parsinomious pseudo-label computationally efﬁcient empirically shows similar performance trajectory-based pseudo-label. choosing pseudo-label requires knowledge budget cost trade-off parameter budget unspeciﬁed varies time system capable adapting changing budget demands necessary. handle scenario propose anytime system next section. many applications budget constraint unknown priori varies example example changing resource availability expected budget system section yield feasible system. instead consider problem learning anytime system setting single system designed example arrives test-time features acquired arbitrary budget constraint particular example. note anytime system special case expected budget constrained system. instead expected budget instead hard per-example budget given. single system applied feasible budgets opposed learning unique systems budget constraint. model anytime learning problem sequential state selection. goal select trajectory states starting initial state k×|c| components features negligible cost. select trajectory states deﬁne policy functions function maps structured features current state state sequential selection system deﬁned policy functions example policy functions produce trajectory states deﬁned follows goal learn system small expected loss time formally deﬁne average modiﬁed loss system trajectory states user-speciﬁed family functions. unfortunately problem learning policy functions highly coupled dependence state trajectory policy functions. note ﬁxed budget choice dictates behavior anytime system. decreasing leads larger increases classiﬁcation performance expense budget granularity. propose greedy approximation policy learning problem sequentially learning policy functions minimize modiﬁed loss note selected take account future effect loss consider greedy approximation instead chosen minimize immediate loss time restrict output space states policy non-zero components previous state single feature added. space states deﬁned {s|d hamming distance. note mirrors trajectory used trajectory-based pseudo-label. section take empirical risk minimization approach learning policies. sequentially learn function minimizing risk enumerating space states policy example. note space states empty features acquired example step thm. problem learning sequence policy functions viewed weighted structured learning problem. theorem optimization problem equivalent solving importance weighted structured learning problem using indicator risk form theorem reduces problem learning policy importance weighted structured learning problem. replacement indicators upper-bounding convex surrogate functions results convex minimization problem learn policies particular hinge-loss surrogate converts problem commonly used structural svm. experimental results show signiﬁcant cost savings applying sequential policy. training algorithm presented algorithm time policy trained minimize immediate loss. given policy states examples time ﬁxed trained minimize immediate loss given states. algorithm continues learning policies every feature every example acquired. test-time system sequentially applies trained policy functions speciﬁed budget reaches shown algorithm multi-class prediction test-time budget received signiﬁcant attention fundamentally multi-class classiﬁcation based approaches cannot directly applied structured settings reasons structured feature selection policy unlike multiclass prediction structured setting many parts associated features costs part. often requires coupled adaptive part part feature selection policy applied varying structures; structured inference costs contrast multi-class prediction structured prediction requires solving constrained optimization problem test-time often computationally expensive must taken account. strubell improve speed parser operates search-based structured prediction models joint prediction decomposed sequence decisions. case resourceconstrained multi-class approaches applied however reduction applies search-based models fundamentally different graph-based models discussed applying policy case graphical models requires repeated inferences dramatically increasing computational cost inference slow. similar observations apply weiss present scheme adaptive feature selection assuming computational costs policy execution inference negligible. approach uses reinforcement learning scheme requiring inference step policy estimate rewards. complex inference tasks repeatedly executing policy negate computational gains induced adaptive feature selection imitation learning adaptively select features dependency parsing. approach viewed approximation eqn. parsimonious search. although policy avoids performing inference estimate reward multiple inferences required instance design action space. overhead avoided exploiting speciﬁc inference structure unclear generalized. methods increase speed inference proposed approaches incorporated approach reduce computational cost therefore complementary. focused research improved speed individual algorithms object detection using deformable parts models dependency parsing methods specialized failing generalize varying graph size and/or structures relying problem-speciﬁc heuristics algorithm-speciﬁc properties. adaptive features approaches designed improve accuracy including easy-ﬁrst decoding strategies however methods focus performance opposed computational cost. section demonstrate effectiveness proposed algorithm structured prediction tasks different domains dependency parsing ocr. report results anytime expected case policies refer latter one-shot policy. focus mainly policy achieving state performance either domains. high-level policies resource constrained structured prediction must manage tradeoff beneﬁts three resources namely feature acquisition costs intermediate inferencing costs policy overhead costs decides feature acquisition inferencing. methods described earlier account feature costs inference overhead costs. methods incorporate inference policy selecting features account resulting policy overhead. approach poses policy optimization structured learning problem turn jointly optimizes resources demonstrated empirically experiments. compare system q-learning approach baselines uniform policy myopic policy. uniform policy takes random part level actions. uniform policy help show performance policy come removing redundant features clever allocation among samples. second baseline adapt myopic policy used figure example word test dataset shown. note word initially incorrectly identiﬁed degradation letters \"n\". letter classiﬁcation accuracy increases policy acquires features strategic positions. figure performance one-shot policy compared uniform strategy policy weiss dataset. although policy complex features efﬁcient features simple feature policy lower total run-time budget region overhead additional inference. structured prediction case. myopic policy runs structured predictor initially cheap features looks total conﬁdence classiﬁer normalized sample size conﬁdence threshold chooses acquire expensive features positions. finally compare q-learning method proposed method requires global features structures varying size. refer features require access part complex features part level features simple features. case conﬁdence feedback structured predictor induces additional inference overhead policy. addition this straightforward apply approach part part feature selection structures varying sizes. adopt structured-svm solve policy learning problems expected anytime cases deﬁned respectively. structure policy graph edges simplicity. form policy learning problem written sample weighted svm. discuss details appendix space constraints. show following complex features indeed beneﬁt policy simple features perform better cases inference time feature costs comparable additional overhead unwanted. finally show part part selection outperforms global selection. optical character recognition tested algorithm sequence-label problem dataset composed handwritten words word represented sequence binary letter images. linear-chain markov model similar setup pixel values features cell size feature templates. split data percent used training used test. fig. shows average letter accuracy total running time. proposed system reduces budget performance levels savings percent performance. note weiss operate part part level graph structure varying. using complex part part selection signiﬁcant advantage using uniform feature templates. furthermore shows behavior policy individual example anytime model signiﬁcant gains accuracy made ﬁrst several steps correctly identifying noisy letters. dependency parsing follow setting conduct experiments english penn treebank corpus algorithms implemented based graph-based dependency parser illinois-sl library code optimized speed. sets feature templates considered parser. ﬁrst considers part-of-speech tags lexicons surrounding words considers features. complex features often contribute small performance improvement. adding complex redundant features easily yield arbitrarily large speedups comparing speedups different systems different accuracy levels meaningful addition greedy-style parser might faster nature. discussing different architecture features outside scope paper. figure left performance various adaptive policies varying budget levels compared uniform strategy word sentence level myopic policy section dataset. right distribution parse-tree depth words cheap expensive features anytime policy. time increases left right. group columns show distribution depths policy concentrated acquiring features lower depth words. sentence example also shows effect. easy identify parents adjectives determiner. however additional features required root subject object. policy assigns feature templates word sentence directed edges corresponding word share feature templates. ﬁrst feature template ψpos takes word second feature template ψfull takes word extract features compute edge scores. decoding chu–liu-edmonds algorithm word supporting hypothesis feature extraction makes signiﬁcant portion total running time inference time negligible. space limit present details experiment setting appendix. fig. shows test performance along inference time. one-shot policies perform similarly losing negligible accuracy using half available expensive features. apply length dictionary ﬁltering heuristic parser achieves section overall running time merely seconds obtains total speed-up losing comparing baseline. signiﬁcant speed-up efﬁcient implementation remarkable. although marginal one-shot policy greedy trajectory strongest performance budget regions. greedy trajectory search better granularity parsimonious search choosing positions decrease loss early anytime policy one-shot policy budget levels. discussed anytime policy constrained achieve ﬁxed budget examples. naive myopic policy performs worse uniform since inference samples conﬁdence times adding approximately seconds extra time full test dataset. explore effect importance weights greedy policy. notice small improvement. hypothesize policy functional complexity limiting factor. also conduct ablative studies better understand policy behavior. fig. shows distribution depth words expensive cheap features ground truth dependency tree. expect investing time low-depth words yield higher accuracy gains. observe phenomenon empirically policy concentrates extracting features close root. contrast baseline system slow three times. operating accuracy level figure shows ﬁnal system takes acknowledge different features policy settings hardware ours; therefore numbers might comparable. dependency parsing split corpus parts sections section training test sets. conduct modiﬁed cross-validation mechanism train feature selector dependency parser. note cost policy dependent structured predictor. therefore learning policy training predictor cause structured loss overly optimistic. follow cross validation scheme deal issue splitting training data folds. fold generate label predictions based structured predictor trained remaining folds. finally gather label predictions train policy complete data. dependency parser trained averaged structured perceptron modelwith learning rate number epochs respectively. setting achieves best test performance reported notice trained dependency models different feature sets separately scale edge scores different resulting sub-optimal test performance. issue generate data random edge features train model minimize joint loss states. finally found dependency parsing expensive features necessary several critical locations sentence. therefore budget levels turned unachievable feature-tradeoff parameter lambda pseudo-labels. obtain regions varied class weights feature templates training one-shot feature selector.", "year": 2016}