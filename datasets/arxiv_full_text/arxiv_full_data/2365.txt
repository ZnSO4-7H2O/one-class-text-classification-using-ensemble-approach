{"title": "A Bayesian Network Classifier that Combines a Finite Mixture Model and a  Naive Bayes Model", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this paper we present a new Bayesian network model for classification that combines the naive-Bayes (NB) classifier and the finite-mixture (FM) classifier. The resulting classifier aims at relaxing the strong assumptions on which the two component models are based, in an attempt to improve on their classification performance, both in terms of accuracy and in terms of calibration of the estimated probabilities. The proposed classifier is obtained by superimposing a finite mixture model on the set of feature variables of a naive Bayes model. We present experimental results that compare the predictive performance on real datasets of the new classifier with the predictive performance of the NB classifier and the FM classifier.", "text": "paper present net­ work model classification naive bayes lect optimal ability problem classifier ject active research several computer-aided proximately among others. probabilistic classification popular method object active research mark thermore often outperform decision classification performance degree probabilistic tors particular suasively bayesian directed-acyclic-graph probability functions conditional able given parent allows representation joint probability tion variables probability bayesian model selection model given data based proportionality purpose term probability pro­ vided input. term usually marginal idence high dimensional maximum likelihood parameters mode penalty term penalizes form differs likelihood cludes akaike information veloped cheeseman-stutz grated classification also scores tion described section rely availability mode parameter tation bayesian ters usually consists configuration maximum posteriori given complete variable missing multinomial distribution equation yields estimation finite mixture structure model class variable common parent node represents used model interaction ture variables ables class variable model represent distributions constraints cause model's loses \"preferential\" model. suggests general observable model could come expense predictive able. model classifier bines models described tion relaxing based. augmented superimposing feature model depicted hidden variable introduced probabilistic dependencies among {x;} captured time attempt model model variable among feature parameter tiation gorithm selection section setting metric model selection model data reduces marginal likelihood uninformative els' space. scoring proper number components make asymptotic described parameter straightforward pro­ cedure additional common parent equation conditional ture variable prior probability described mixture task induction respect encodes ables smaller model corresponding experimental consequence algorithm function vided observed usually section components conver­ gence parameter estimation model selection task. model similar discriminant analysis however restricted only distribution gaussian common variance/covariance mixture number components values. since number mixture supplied used measure fact monte carlo ditional distribution erated classifier. tropy minimized interpreted \"true\" area native sification variable rate function threshold independence classify area curve used measure sification perfect performance outcome. domains measure experiments. synthetic created databases discrete discrete induced score) parameterization data aimed maximizing resulting models. based distinct set/ testset irrespective induced totic approximations summary statistics alently half perform half). unequal size small difference cally significant performance icant. testing aimed testing ferences. former classification compared median differences used paired ratio confidence ratio confidence level. model outperformed calibration ratio plots area curve finally area curve overall ratio reduced differences confidence level results performed variables. outperformed cretized formed believe result distinct tioning eling conditional vari­ ables. noted component stan­ dard deviations freund schapire. generalization learning on-line boosting. friedman. curse-of-dimensionality. discovery friedman network friedman classification best discretization shavlik conference area receiver operating characteristic curve. hastie tibshirani. gaussian society kass raftery. american statistical kohavi. scaling classifiers conference kontkanen bayesian rithm. technical science mclachlan extensions. john wiley merz murphy. machine learning itory. university ment information http//vvv.ics.uci.edu/ monti cooper. bayesian classifier finite-mixture ftp//pogo.isp.pitt.edu/pub/smonti/tr.ps. pearl. probabilistic reasoning intelligent tems networks plausible inference. morgan kauf­ publishers provost algorithms. ference machine learning ridgeway pretable international salzberg. avoid recommended knowledge discovery schwarz. annals statistics vapnik. springerh. warner toronto veasey stephenson. diagnosis. tion", "year": 2013}