{"title": "Coordinate Descent Methods for Symmetric Nonnegative Matrix  Factorization", "tag": ["cs.NA", "cs.CV", "cs.LG", "math.OC", "stat.ML"], "abstract": "Given a symmetric nonnegative matrix $A$, symmetric nonnegative matrix factorization (symNMF) is the problem of finding a nonnegative matrix $H$, usually with much fewer columns than $A$, such that $A \\approx HH^T$. SymNMF can be used for data analysis and in particular for various clustering tasks. In this paper, we propose simple and very efficient coordinate descent schemes to solve this problem, and that can handle large and sparse input matrices. The effectiveness of our methods is illustrated on synthetic and real-world data sets, and we show that they perform favorably compared to recent state-of-the-art methods.", "text": "clustering tasks. paper propose simple eﬃcient coordinate descent schemes solve problem handle large sparse input matrices. eﬀectiveness methods illustrated synthetic real-world data sets show perform favorably compared recent state-of-the-art methods. many applications nonnegativity constraints lead sparse part-based representation better interpretability factors e.g. analyzing images documents paper work special case input matrix symmetric matrix usually matrix similarity matrix entry measure similarity data points. rather general framework user decide generate matrix data selecting appropriate metric compare data points. opposed interested symmetric approximation factor nonnegative–hence symnmf variant data points grouped clusters rank-one factor ideally correspond cluster present data set. fact symnmf used successfully many diﬀerent settings proved given nonnegative symmetric matrix factorization rank symnmf looks n-by-r nonnegative matrix error approximation measured diﬀerent ways focus paper frobenius norm arguably widely used practice. applying standard non-linear optimization schemes hope obtain stationary points since objective function highly nonconvex problem np-hard example methods approximate solutions proposed ﬁrst method newton-like algorithm exploits second-order information withprohibitive cost full newton method. iteration algorithm computational complexity operations. second algorithm adaptation alternating nonnegative least squares penalizing diﬀerence factors added objective function. idea used author developed methods solve penalized problem without available implementation comparison. paper analyze coordinate descent schemes motivation eﬃcient methods methods; references therein. reason behind success methods twofold updates written closed-form cheap compute interaction variables many variables expected equal zero stationary point paper organized follows. section focus rank-one problem present general framework implement exact method symnmf. main proposed algorithm described section section discusses initialization convergence issues. section presents extensive numerical experiments synthetic real data sets shows methods perform competitively recent state-of-the-art techniques symnmf. exact coordinate descent techniques among intuitive methods solve optimization problems. iteration variables ﬁxed variable updated optimal value. update variable time often computationally cheap easy implement. however little interest given methods recently approaches shown competitive certain classes problems; recent survey. fact entries nonnegative problem solved example truncated singular value decomposition; follows perron-frobenius eckart-young theorems. case residuals general negative entries–see –which makes problem np-hard general optimality conditions given variables ﬁxed complementary slackness condition optimal solution either solution equation root since roots third-degree polynomial computed closed form suﬃces ﬁrst compute roots evaluate roots order identify optimal solution algorithm based cardano’s method described algorithm runs time. therefore given known means updating variable cost operations necessary coordinates updating gradient. operations; section algorithm describes iteration applied problem words wants stationary point problem algorithm called convergence would correspond applying cyclic coordinate descent method lines quantities ai’s bi’s precomputed. product needed every takes time. then line line algorithm called every variable followed updates described finally algorithm computational cost operations. note cannot expect lower computational cost since computing gradient requires operations. tackle symnmf apply algorithm every column successively apply algorithm procedure simple describe algorithm implements exact cyclic method applied symnmf. easily check algorithm requires operations update entries once algorithm drawbacks. particular heavy computation residual matrix unpractical large sparse matrices next sections show tackle issues propose eﬃcient method symnmf applicable large sparse matrices. algorithm symnmf developed previous section unpractical input matrix large sparse; sense although stored memory algorithm memory large. fact residual matrix entries computed step algorithm general dense even sparse. sparse matrices usually non-zero entries large unpractical store entries section re-implement algorithm order avoid explicit computation residual matrix algorithm algorithm runs operations iteration requires space memory algorithm runs operations iteration requires space memory number non-zero entries hence dense algorithm asymptotic computational cost operations iteration algorithm however performs better practice exact number operations smaller. sparse algorithm runs operations iteration signiﬁcantly smaller algorithm applicable large sparse matrices. fact practice order millions usually smaller hundred. illustrated section numerical experiments text data sets. following ﬁrst assume dense accounting computational cost algorithm then show computational cost signiﬁcantly reduced sparse. since want avoid computation residual reducing problem rank-one subproblems solved desirable. evaluate gradient objective summary precomputing quantities khik khjk possible apply iteration variables operations. computational cost algorithm dense case residual matrix computed; algorithm line line precomputations performed time computing expensive part. loops iterate entries update variable once. computing bottleneck scheme part loops requires time. however matrix sparse cost computing computing drops number nonzero entries taking account term compute requires operations algorithm requires operations iteration. initialization previous works matrix initialized randomly using uniform distribution interval entry note that practice obtain unbiased initial point matrix multiplied constant allows initial approximation well scaled compared using initialization observed using random shuﬄing columns iteration performs general much better; section random initialization might seem reasonable especially scheme. fact ﬁrst step method optimal values entries ﬁrst column computed sequentially trying solve would arguably make sense initialize zero that optimizing entries ﬁrst step approximate matrix itself. turns simple strategy allows obtain faster initial convergence random initialization strategy. however observe following solution tends particular structure ﬁrst factor dense next ones sparser. explanation ﬁrst factor given importance since optimized ﬁrst hence close best rank-one approximation general positive hence initializing zero tends produce unbalanced factors. however might desirable cases next factors general signiﬁcantly sparser random initialization. illustrate this perform following numerical experiment cbcl face data contains facial images pixels each. construct nonnegative images. hence symnmf provide matrix column corresponds ‘cluster’ pixels sharing similarities. figure shows columns obtained zero initialization random initialization observe solutions diﬀerent although relative approximation iterations). depending application hand solutions might desirable example cbcl data seems solution obtained zero initialization easily interpretable facial features random initialization interpreted average/mean faces. example also illustrates sensitivity algorithm initialization diﬀerent initializations lead diﬀerent solutions. unavoidable feature algorithm trying good solution np-hard problem relatively computational cost. finally would like point ability initialize algorithm zero nice feature. fact since stationary point shows coordinate descent method escape ﬁrst-order stationary points uses higher-order unfortunately current form diﬃcult prove convergence algorithm stationary point. fact guarantee convergence exact cyclic coordinate method stationary point three suﬃcient conditions objective function continuously diﬀerentiable feasible sets blocks variables updated compact well convex minimum computed iteration given block variables uniquely attained prop. conditions algorithm unfortunately necessarily case minimizer fourth order polynomial unique. possible obtain convergence apply maximum block improvement method iteration update variable leads largest decrease objective function although theoretically appealing makes algorithm computationally much expensive hence much slower practice. numerical experiments always observed sequence iterates generated algorithm converged unique limit point. case prove limit point stationary point. note theorem useful practice since easily checked whether algorithm converges unique accumulation point plotting example norm diﬀerent iterates. section shows eﬀectiveness algorithm several data sets compared state-ofthe-art techniques. organized follows. section describe real data sets section tested symnmf algorithms. section describe settings compare symnmf algorithms. section provide discuss experimental results. exactly data sets space limitation give results value factorization rank numerical experiments available arxiv version paper authors four dense data sets sparse data sets compare several algorithms. section data sets generate similarity matrices construct entries equal inner products data points. table summarizes dense data sets corresponding widely used facial images data mining community. table summarizes characteristics diﬀerent sparse data sets corresponding document datasets described details http//www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html http//www.cs.toronto.edu/~roweis/data.html http//cbcl.mit.edu/cbcl/software-datasets/facedata.html objective function note anls drawback depend parameter nontrivial tune namely penalty parameter term objective function method recently introduced ﬁrst computes rank-r truncated contains ﬁrst singular vectors r-by-r diagonal matrix containing ﬁrst singular values diagonal. then instead solving authors solve ‘closeby’ optimization problem replacing truncated computed iteration method extremely cheap main computational cost matrix-matrix product urς/ r-by-r rotation matrix computed operations. note also initialization ﬂipped signs columns maximize norm nonnegative part algorithm presented based multiplicative updates note also implemented multiplicative update rules however report numerical results outperformed betasnmf numerical experiments observation already made algorithm either ‘cyclic’ ‘shuﬄe’ indicates whether columns optimized cyclic shuﬄed randomly iteration. initialization ‘rand’ random initialization zero initialization; section details. hence compare four variants algorithm cd-cyclic- cd-shuﬄe- cd-cyclic-rand cd-shuﬄe-rand. algorithm requires perform many loops matlab wellsuited language. therefore developed implementation called matlab note algorithms better suited matlab since main computational cost resides matrix-matrix products solving linear systems equations algorithms using random initializations matrix used initial matrices. note however that ﬁgures presented section display error ﬁrst iteration reason curves start value. normalized measure evolution objective function given algorithm given data set. advantage measure separates better diﬀerent algorithms using scale since goes zero best algorithm several random initializations; below). would like stress measure interpreted care. fact algorithm converges zero simply means algorithm able best solution among algorithms fact diﬀerent algorithms initialized diﬀerent initial points particular tsvd uses svd-based initialization. necessarily mean converges fastest compare convergence look values small. however measure allows better visualize diﬀerent algorithms. example displaying relative error dense image data sets figure displays results dense real data sets. table gives number iterations performed algorithm within seconds table ﬁnal function value among algorithms. might surprising since works approximation original data appears real dense data sets approximation computed eﬃciently allows tsvd converge extremely fast good solution. reasons tsvd eﬀective iteration times cheaper hence perform many iterations; table another crucial reason image data sets well approximated low-rank matrices therefore images tsvd best method provides good solution extremely fast. generate fast good solution. cases fastest generate solution relative error ﬁnal solution tsvd. moreover fact tsvd generate solution long truncated computed could critical larger data sets. example cbcl truncated takes seconds compute while mean time cd-cyclic- cd-shuﬄe- generate solution relative error ﬁnal solution obtained tsvd seconds. random shuﬄing. moreover cd-shuﬄe-rand converges initially slower cd-shuﬄe- often able converge better solution; particular umistim data sets. sparse document data sets figure displays results real sparse data sets. table gives number iterations performed algorithm within seconds table data sets computing truncated possible matlab within seconds hence tsvd able return solution; remark discussion. moreover newton displayed designed sparse matrices runs memory initialization order columns updated plays signiﬁcant role. however observe cases converges value never smaller value. means best solution always obtained random initialization average random initialization performs similarly initialization zero. remark noted that numerical experiments matrix constructed using formula columns matrix data points. words simple similarity measure data points case obtained hence made methods obtain rapidly good initial iterate. however looking figure table indicates would necessarily advantageous cd-based methods cases. example classic data tsvd would achieve relative error within seconds methods obtain similar relative error within computing time. hitech data however would rather helpful since tsvd would take second obtain relative error methods require seconds however goal paper provide eﬃcient algorithm general symnmf problem without assuming particular structure matrix therefore assumed matrix particular structure provide numerical comparison case. remark although sparse data sets usually rank still makes sense low-rank structure close given data often allows extract pertinent information. particular document classiﬁcation clustering low-rank models proven extremely useful; discussion introduction references therein. another important application low-rank models proven extremely useful although data sets usually low-rank recommender systems also refer reader recent survey section perform numerical experiments synthetic data sets. main motivation conﬁrm behavior observed real data tsvd performs extremely well low-rank matrices poorly full-rank matrices. low-rank input matrices natural generate nonnegative symmetric matrices given cp-rank generate randomly compute section matlab function rand entry generated uniformly random interval generated matrices rank figure displays average value measure emin since known optimal value. observe that cases tsvd outperforms methods. moreover seems svdbased initialization eﬀective. reason exactly rank hence best rank-r approximation exact. moreover tsvd works correct subspace belongs hence converges much faster methods. worth noting behavior observed real dense data sets present here cd-shuﬄe-rand performs better cd-cyclic-rand shuﬄing columns iteration play crucial role zero initialization. full-rank input matrices simple generate nonnegative symmetric matrices full rank generate matrix randomly compute section matlab function rand generated matrices rank figure displays average value measure figure displays results. clearly tsvd cd-based approaches eﬀective although anls sometimes performs competitively dense data sets. however tsvd performs extremely well input matrix rank close rank three cases performs poorly conclude cd-based approaches overall reliable eﬀective methods solve symnmf dense data sets initialization zero allows faster initial convergence cd-shuﬄe-rand generates average best solution cd-cyclic-rand perform well recommended. sparse data sets variants perform similarly outperform tested algorithms. algorithm easily adapted handle replacing bij’s fact derivative penalty term inﬂuences constant part gradient; however seems solutions sensitive parameter hence diﬃcult tune. note another identify sparser factors simply increase factorization rank sparsify input matrix references therein) fact sparser matrix induces sparser factors since", "year": 2015}