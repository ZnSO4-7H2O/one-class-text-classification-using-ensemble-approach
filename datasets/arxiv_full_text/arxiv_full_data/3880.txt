{"title": "Topology Reduction in Deep Convolutional Feature Extraction Networks", "tag": ["stat.ML", "cs.CV", "cs.IT", "cs.LG", "math.FA", "math.IT"], "abstract": "Deep convolutional neural networks (CNNs) used in practice employ potentially hundreds of layers and $10$,$000$s of nodes. Such network sizes entail significant computational complexity due to the large number of convolutions that need to be carried out; in addition, a large number of parameters needs to be learned and stored. Very deep and wide CNNs may therefore not be well suited to applications operating under severe resource constraints as is the case, e.g., in low-power embedded and mobile platforms. This paper aims at understanding the impact of CNN topology, specifically depth and width, on the network's feature extraction capabilities. We address this question for the class of scattering networks that employ either Weyl-Heisenberg filters or wavelets, the modulus non-linearity, and no pooling. The exponential feature map energy decay results in Wiatowski et al., 2017, are generalized to $\\mathcal{O}(a^{-N})$, where an arbitrary decay factor $a>1$ can be realized through suitable choice of the Weyl-Heisenberg prototype function or the mother wavelet. We then show how networks of fixed (possibly small) depth $N$ can be designed to guarantee that $((1-\\varepsilon)\\cdot 100)\\%$ of the input signal's energy are contained in the feature vector. Based on the notion of operationally significant nodes, we characterize, partly rigorously and partly heuristically, the topology-reducing effects of (effectively) band-limited input signals, band-limited filters, and feature map symmetries. Finally, for networks based on Weyl-Heisenberg filters, we determine the prototype function bandwidth that minimizes---for fixed network depth $N$---the average number of operationally significant nodes per layer.", "text": "deep convolutional neural networks used practice employ potentially hundreds layers nodes. network sizes entail signiﬁcant computational complexity large number convolutions need carried out; addition large number parameters needs learned stored. deep wide cnns therefore well suited applications operating severe resource constraints case e.g. low-power embedded mobile platforms. paper aims understanding impact topology speciﬁcally depth width network’s feature extraction capabilities. address question class scattering networks employ either weyl-heisenberg ﬁlters wavelets modulus non-linearity pooling. exponential feature energy decay results wiatowski feature vector. based notion operationally signiﬁcant nodes characterize partly rigorously partly heuristically topology-reducing eﬀects band-limited input signals band-limited ﬁlters feature symmetries. finally networks based weyl-heisenberg ﬁlters determine prototype function bandwidth minimizes—for ﬁxed network depth —the average number operationally signiﬁcant nodes layer. feature extraction based deep convolutional neural networks applied signiﬁcant success wide range practical machine learning tasks many applications e.g. classiﬁcation images imagenet data employ deep networks potentially hundreds layers nodes depth average number nodes layer). network sizes entail formidable computational challenges training phase large number parameters learned million parameters) operating network large number convolutions need carried entails billion flops pass single image network). moreover storing learned network parameters requires large amounts memory. deep wide cnns therefore suited applications operating strong resource constraints case e.g. low-power embedded mobile platforms hence important understand impact topology speciﬁcally depth width network’s feature extraction capabilities. address question class scattering networks introduced extended scattering network-based feature extractors shown yield classiﬁcation performance competitive state-of-the-art various data sets moreover mathematical theory exists allows establish formally feature extractors are—under certain technical conditions—horizontally vertically translation-invariant energy-conserving deformation-stable sense exhibit limited sensitivity deformations input signal classes band-limited functions cartoon functions lipschitz functions energy contained feature vector—obtained aggregating ﬁltered versions propagated signals —were recently obtained results apply scattering networks employing modulus non-linearity pooling general ﬁlters analytic constitute parseval frames allowed diﬀerent diﬀerent network layers. main ﬁndings state feature contained feature vector. based notion operationally signiﬁcant nodes characterize partly rigorously partly heuristically topology-reducing eﬀects band-limited input signals bandlimited ﬁlters feature symmetries. results obtain suggest classiﬁcation shallow singlelayer constant-width expanding-width depth-pruned extremely narrow scattering networks. finally networks based ﬁlters determine prototype function bandwidth minimizes—for ﬁxed network depth —the average number operationally signiﬁcant nodes layer. general notation employed paper refer stage brieﬂy reviewing basics scattering network-based feature extractors. presentation follows closely throughout paper focus case employ module sequence χ}q∈λn features generated n-th network layer figure here corresponds root network. feature extractor∗ shown vertically translation-invariant provided though pooling employed pooling factors moreover exhibits limited sensitivity certain non-linear deformations input signal classes band-limited functions cartoon functions lipschitz functions recently shown energy-conserving sense energy contained feature vector proportional corresponding input signal conditions impose satisﬁed constructing function whose fourier transform b-spline from e.g. analytic meyer wavelet emphasize wavelet ﬁlters satisfy—by construction—the analyticity highpass condition well symmetry property sobolev functions encompass wide range practically relevant signal classes square-integrable functions strictly -band-limited functions functions proof. proof structurally similar hence presented detail. nutshell elements needed establish case replace so-called −ν|k| deﬁned modulation weights −ν|k| similarly wavelet case replace modulation weights identities show ﬁlter constructions propose indeed allow tune decay factor single parameter namely case wavelet case. reducing results faster energy decay particularizing recovers decay factors respectively established finally refer reader references therein overview previous work decay rate lower bound guarantees least input signal energy contained feature vector generated ﬁrst network layers. note establishing upper bound pose signiﬁcant diﬃculties follows straight results lower bound implies trivial null-set feature extractor thereby ensures signal mapped all-zeros feature vector emphasize energy decay results theorem pertain feature maps whereas energy conservation according applies feature vector results presented thus mathematically strict nature present section shall allow argue less formal level. energy decay conservation results established assume results doubling bandwidth heuristically argued below allow assume remainder paper feature maps eﬀectively band-limited. consequently sensible many nodes actually needed n-th network layer capture feature energy contained root network i.e. owing deﬁnition accounts topology reduction relative full tree figure caused feature symmetries below) reﬂected counting number distinct‡ feature maps only§ width pruning owing strict band-limitation ﬁlters hence eﬀective band-limitation feature maps note speciﬁc spectral structure e.g. multi-band structure lead topology reduction. eﬀect will however taken account remainder paper. honor dependence ﬁlters eﬀective bandwidth input signal employing notation ξwav wherever appropriate. emphasize location identical feature maps uniquely determined. practice therefore suﬃces indeed compute features arrange identical copies accordingly next determining requires studying spectral characteristics feature maps gk|. note that owing modulus non-linearity characterizing eﬀective spectral support non-trivial. however take behavior non-linearity therefore doubles spectral support demodulates sense spectrum located symmetrically around origin irrespectively spectral location following shall therefore allow work esuppf z\\{}. hasten statement based solely numerical evidence corresponding formal result. interesting observe sigmoid rectiﬁed linear unit hyperbolic tangent nonlinearities exhibit diﬀerent behavior regard feature maps increase decrease remain constant layer index increases. bandwidth expansion bandwidth contraction eﬀective bandwidths feature maps remain constant i.e. scattering network architecture deﬁned section tree topology inﬁnite number nodes layer. analysis previous section revealed however number operationally signiﬁcant nodes ﬁnite every network layer goal section determine characterize induced operationally signiﬁcant max{ logr− feature maps decreasing eventually smaller hence contained spectral means layer onwards non-zero signals propagated deeper layers. extremely-narrow network feature maps constant overlap spectral supports only) number operationally signiﬁcant nodes ξwav constant every network layer contributes elements feature vector. purpose section analyze impact topology reduction average number operationally signiﬁcant nodes layer. simplicity exposition throughout section focus case. take parameters ﬁxed assume eﬀective bandwidth input signal satisﬁes shallow feature extraction situation section ﬁrst recall that thanks decay factor tuned parameter speciﬁcally reducing bandwidth prototype function implies faster energy decay increasing implies slower energy decay eventually violating wiatowski tschannen stani´c grohs b¨olcskei discrete deep feature extraction theory architectures proc. international conference machine learning", "year": 2017}