{"title": "Learning without Forgetting", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.", "text": "abstract—when building uniﬁed vision system gradually adding capabilities system usual assumption training data tasks always available. however number tasks grows storing retraining data becomes infeasible. problem arises capabilities convolutional neural network training data existing capabilities unavailable. propose learning without forgetting method uses task data train network preserving original capabilities. method performs favorably compared commonly used feature extraction ﬁne-tuning adaption techniques performs similarly multitask learning uses original task data assume unavailable. surprising observation learning without forgetting able replace ﬁne-tuning similar task datasets improved task performance. visual capabilities maintaining performance existing ones. example robot delivered someone’s house default object recognition capabilities site-speciﬁc object models need added. construction safety system identify whether worker wearing safety vest hard superintendent wish ability detect improper footware. ideally tasks could learned sharing parameters ones without suffering catastrophic forgetting access training data. legacy data unrecorded proprietary simply cumbersome training task. problem similar spirit transfer multitask lifelong learning. developing simple effective strategy variety image classiﬁcation problems convolutional neural network classiﬁers. setting shared parameters architecture) task-speciﬁc parameters previously learned tasks classiﬁcation corresponding weights) randomly initialized taskspeciﬁc parameters tasks useful think classiﬁers operate features parameterized currently three common approaches learning beneﬁting previously learned also possible variation ﬁne-tuning part convolutional layers frozen prevent overﬁtting fully connected layers ﬁnetuned. seen compromise ﬁnetuning feature extraction. work call method fine-tuning stands fully connected. joint training parameters jointly optimized example interleaving samples task. method’s performance seen upper bound proposed method achieve. strategies major drawback. feature extraction typically underperforms task shared parameters fail represent information discriminative task. fine-tuning degrades performance previously learned tasks shared parameters change without guidance original task-speciﬁc prediction parameters. duplicating ﬁnetuning task results linearly increasing test time tasks added rather sharing computation shared parameters. fine-tuning show experiments still degrades performance task. joint training becomes increasingly cumbersome training tasks learned possible training data previously learned tasks unavailable. besides commonly used approaches methods emerged continually prediction tasks adapting shared parameters without access training data previously learned tasks. paper expand previous work learning without forgetting using examples task optimize high accuracy task preservation responses existing tasks original network. method similar joint training except method need task’s images labels. clearly network preserved produces exactly outputs relevant images task accuracy fig. wish prediction tasks existing vision system without requiring access training data existing tasks. table shows relative advantages method compared commonly used methods. original network. practice images task provide poor sampling original task domain experiments show preserving outputs examples still effective strategy preserve performance task also unexpected beneﬁt acting regularizer improve performance task. learning without forgetting approach several advantages classiﬁcation performance learning without forgetting outperforms feature extraction surprisingly ﬁne-tuning task greatly outperforming using ﬁne-tuned parameters task. method also generally perform better experiments recent alternatives computational efﬁciency training time faster joint training slightly slower ﬁne-tuning test time faster uses multiple ﬁnetuned networks different tasks. compared previous work conduct extensive experiments. compare additional methods ﬁne-tune commonly used baseline less forgetting learning recently proposed method. experiment adjusting balance old-new task losses providing thorough intuitive comparison related methods switch obsolete places newer places-standard dataset. perform stricter careful hyperparameter selection process slightly changed results. also include detailed explanation method. finally perform experiment application video object tracking appendix related work multi-task learning transfer learning related methods long history. brief learning without forgetting approach could seen combination distillation networks ﬁne-tuning fine-tuning initializes parameters existing network trained related data-rich problem ﬁnds local minimum optimizing parameters task learning rate. idea distillation networks learn parameters simpler network produce outputs complex ensemble networks either original training large unlabeled data. approach differs solve parameters works compared methods feature extraction uses pre-trained deep compute features image. extracted features activations layer multiple layers given image. classiﬁers trained features achieve competitive results sometimes outperforming human-engineered features studies show hyper-parameters e.g. original network structure selected better performance. feature extraction modify original network allows tasks beneﬁt complex features learned previous tasks. however features specialized task often improved ﬁne-tuning. fine-tuning modiﬁes parameters existing train task. output layer extended randomly intialized weights task small learning rate used tune parameters original values minimize loss task. sometimes part network frozen prevent overﬁtting. using appropriate hyper-parameters training resulting model often outperforms feature extraction learning randomly initialized network fine-tuning adapts shared parameters make discriminative task learning rate indirect mechanism preserve representational structure learned original tasks. method provides direct preserve representations important original task improving original task performance relative ﬁne-tuning experiments. multitask learning aims improve tasks simultaneously combining common knowledge tasks. task provides extra training data parameters shared constrained serving form regularization tasks neural networks caruana gives detailed study multi-task learning. usually bottom layers network shared layers task-speciﬁc. multitask learning requires data tasks present method requires data tasks. propose deep block-modular neural networks fully-connected neural networks rusu propose progressive neural networks reinforcement learning. parameters original network untouched newly added nodes fully connected layer beneath them. methods downside substantially expanding number parameters network underperform ﬁne-tuning feature extraction insufﬁcient training data available learn parameters since require substantial number parameters trained scratch. experiment expanding fully connected layers original network expansion provide improvement original approach. topically relevant methods work also relates methods transfer knowledge networks. hinton propose knowledge distillation knowledge transferred large network network assembly smaller network efﬁcient deployment. smaller network trained using modiﬁed cross-entropy loss encourages large small responses original network similar. romero builds work transfer deeper network applying extra guidance middle layer. chen proposes netnet method immediately generates deeper wider network functionally equivalent existing one. technique quickly initialize networks faster hyper-parameter exploration. methods produce differently structured network approximates original network parameters original network structure approximate original outputs tuning shared parameters tasks. feature extraction ﬁne-tuning special cases domain adaptation transfer learning different multitask learning tasks simultaneously optimized. transfer learning uses knowledge task help another surveyed deep adaption network long matches rkhs embedding deep representation source target tasks reduce domain bias. another similar domain adaptation method tzeng encourages shared deep representation indistinguishable across domains. method also uses knowledge distillation help train domain instead preserving task. domain adaptation transfer learning require least unlabeled data present task domains. contrast interested case training data original tasks available. methods integrate knowledge time e.g. lifelong learning never ending learning also related. lifelong learning focuses ﬂexibly adding tasks transferring knowledge tasks. never ending learning focuses building diverse knowledge experience though topically related work methods provide preserve performance existing tasks without original training data. ruvolo describe method efﬁciently tasks multitask system cotraining tasks using task data. however method assumes weights classiﬁers regression models linearly decomposed bases. contrast method algorithm applies logistic linear regression engineered features features cannot made task-speciﬁc e.g. ﬁnetuning. a-ltm developed independently nearly identical method different experiments conclusions. main differences method weight decay regularization used training warm-up step prior full ﬁne-tuning. however large datasets train initial network extend tasks smaller datasets a-ltm uses small datasets task large datasets task. experiments a-ltm much larger loss ﬁne-tuning paper concludes maintaining data original task necessary maintain performance. experiments contrast show maintain good performance task performing well sometimes better ﬁnetuning task without access original task data. believe main difference choice old-task new-task pairs observe less drop oldtask performance ﬁne-tuning choice believe experiments start well-trained network tasks less training data available better motivated practical perspective. less forgetting learning also similar method preserves task performance discouraging shared representation change. method argues task-speciﬁc decision boundaries change keeps task’s ﬁnal layer unchanged method discourages task output change jointly optimizes shared representation ﬁnal layer. empirically show method outperforms less forgetting learning task. learning without forgetting given shared parameters task-speciﬁc parameters goal task-speciﬁc parameters task learn parameters work well tasks using images labels task algorithm outlined fig. network structure illustrated fig. experiments involve classiﬁcation responses label probabilities training image. nodes class added output layer fully connected layer beneath randomly initialized weights number parameters equal number classes times number nodes last shared layer typically small percent total number parameters. experiments also compare alternate ways modifying network task. next train network minimize loss tasks regularization using stochastic gradient descent. regularization corresponds simple weight decay training ﬁrst freeze train convergence then jointly train weights convergence warm-up step greatly enhances ﬁne-tuning’s oldtask performance crucial either method compared less forgetting learning still adopt technique learning without forgetting slight enhancement fair comparison. simplicity denote loss functions outputs ground truth single examples. total loss averaged images batch training. tasks loss encourages predictions consistent ground truth tasks experiments multiclass classiﬁcation common multinomial logistic loss softmax output network one-hot ground truth label vector. multiple tasks task multi-label classiﬁcation make true/false predictions label take losses across tasks labels. original task want output probabilities image close recorded output original network. knowledge distillation loss found hinton work well encouraging outputs network approximate outputs another. modiﬁed cross-entropy loss increases weight smaller probabilities multiple tasks task multi-label classiﬁcation take loss task label. hinton suggest setting increases weight smaller logit values encourages network better encode similarities among classes. according grid search held aligns authors’ recommendations. experiments knowledge distillation loss leads slightly better similar performance reasonable losses. therefore important constrain outputs original tasks similar original network similarity measure crucial. relationship joint training. mentioned before main difference joint training method need dataset. joint training uses task’s images labels training learning without forgetting longer uses them instead uses task images recorded responses substitutes. eliminates need require store dataset brings beneﬁt joint optimization shared also saves computation since images pass shared layers task task. however distribution images tasks different substitution potentially decrease performance. therefore joint training’s performance seen upper-bound method. efﬁciency comparison. computationally expensive part using neural network evaluating backpropagating shared parameters especially convolutional layers. training feature extraction fastest task parameters tuned. slightly slower ﬁne-tuning needs back-propagate tasks needs evaluate back-propagate once. joint training slowest different images used different tasks task requires separate backpropagation shared parameters. methods take approximately amount time evaluate test image. however duplicating network ﬁne-tuning task takes times long evaluate total number tasks. matconvnet train networks using stochastic gradient descent momentum dropout enabled fully connected layers. data normalization original task used task. resizing follows implementation original network alexnet pixels shortest edge aspect ratio preserved vgg. randomly jitter training data taking random ﬁxed-size crops resized images offset grid randomly mirroring crop adding variance values like alexnet data augmentation applied feature extraction too. training networks follow standard practices ﬁne-tuning existing networks. random initialization xavier initialization. learning rate much smaller training original network learning rates selected maximize task performance reasonable number epochs. scenario learning rate shared methods except feature extraction uses learning rate small number parameters. choose number epochs warmstep joint-optimize step based validation held-out set. look task performance validation. therefore selected hyperparameter favors task more. compared methods converge similar speeds used number epochs method fair comparison; however convergence speed heavily depend original network task pair validate number epoch separately scenario. perform stricter validation previous work number epochs generally longer scenario. exception imagenet→scene observe overﬁtting shorten training feature extraction. lower learning rate epoch held accuracy plateaus. make fair comparison intermediate network trained using method used starting point joint training fine tuning since speed training convergence. words experiment ﬁrst freeze train resulting parameters initialize method joint training ﬁne-tuning. feature extraction feature extraction baseline instead extracting features last hidden layer original network freeze shared parameters disable dropout layers two-layer network nodes hidden layer effect training -layer network extracted features. joint training loss task’s output nodes applied training images. number images subsampled every task epoch balance loss interleave batches different tasks gradient descent. experiments experiments designed evaluate whether learning without forgetting effective method learn task preserving performance tasks. compare common approaches feature extraction ﬁnetuning ﬁne-tuning also less forgetting learning methods leverage existing network task without requiring training data original tasks. feature extraction maintains exact performance original task. also compare joint training upper-bound possible task performance since joint training uses images labels original tasks uses images labels tasks. experiment variety image classiﬁcation problems varying degrees inter-task similarity. original task consider ilsvrc subset imagenet places-standard dataset. note previous work used places taster challenge ilsvrc earlier version places dataset deprecated publication. imagenet object category classes training images. places scene classes training images. large datasets also assume start well-trained network implies large-scale dataset. tasks consider pascal image classiﬁcation caltech-ucsd birds-- ﬁne-grained classiﬁcation indoor scene classiﬁcation datasets moderate number images training voc; cub; scenes. among these similar imagenet subcategories labels found imagenet classes. indoor scene dataset turn similar places. dissimilar both since includes birds requires capturing details image make valid prediction. experiment mnist task expecting method underperform since hand-written characters completely unrelated imagenet classes. mainly alexnet network structure because fast train well-studied community also verify similar results hold using -layer vggnet smaller experiments. network structures ﬁnal layer treated task-speciﬁc rest shared unless otherwise report center image crop mean average precision center image crop accuracy tasks. report accuracy validation imagenet places test scenes dataset. since test performance former three cannot evaluated frequently provide performance test sets experiment. randomness within training experiments three times report mean performance. experiments investigate adding single task network adding multiple tasks one-by-one. also examine effect dataset size network design. ablation studies examine alternative response-preserving losses utility expanding network structure ﬁne-tuning lower learning rate method preserve original task performance. note results multiple sources variance including random initialization training pre-determined termination etc. main experiments single task scenario. first compare results learning task among different task pairs different methods. table shows performance method relative performance methods compared using alexnet. also visualize old-new performance comparison task pairs figure make following observations task method consistently outperforms ﬁnetuning ﬁne-tuning feature extraction except imagenet→mnist places→cub using ﬁne-tuning. gain ﬁne-tuning unexpected indicates preserving outputs task effective regularizer. ﬁnding motivates replacing ﬁne-tuning standard approach adapting network task. task method performs better ﬁne-tuning often underperforms feature extraction ﬁne-tuning sometimes lfl. changing shared parameters ﬁne-tuning signiﬁcantly degrades performance task original network trained. jointly adapting generate similar outputs original network task performance loss greatly reduced. considering tasks figure shows adjusted perform better ﬁne-tuning task task performance ﬁrst task pair perform similarly second. indeed ﬁne-tuning gives performance ﬁnetuning feature extraction. provides freedom changing shared representation compared boosted task performance. method performs similarly joint training alexnet. method tends slightly outperform joint training task underperform task attribute different distribution scenarios performs comparably joint training uses task training data consider unavailable methods. task method greatly outperforms ﬁne-tuning achieves slightly worse performance joint training. exception imagenet-mnist task datasets. overall methods perform similarly positive result since method require access task training data faster train. note sometimes tasks’ performance degrade large small. suspect making large essentially increases task learning rate potentially making suboptimal making small lessens regularization. dissimilar tasks degrade task performance more. example dissimilar task places adapting network leads places accuracy loss ﬁne-tuning joint training. cases learning task causes considerable drift shared parameters cannot fully accounted distribution places images different. even joint training leads accuracy loss task cannot shared parameters works well tasks. method outperform ﬁne-tuning places→cub expected imagenet→mnist task since hand-written characters provide poor indirect supervision task. task accuracy drops substantially ﬁne-tuning though ﬁne-tuning. similar observations hold alexnet structures except joint training outperforms consistently performs worse task. indicates results likely hold network structures well though joint training larger beneﬁt networks representational power. among results diverges using stochastic gradient descent tuned multiple task scenario. second compare different methods cumulatively tasks system simulating scenario object scene categories gradually added prediction vocabulary. experiment gradually adding task alexnet trained places adding scene task alexnet trained imagenet. pairs moderate difference original task tasks. split task classes three parts according similarity transport animals objects scenes large rooms medium rooms small rooms. images scenes split three subsets. since multilabel dataset possible split images different categories labels split task images shared among tasks. time task added responses tasks re-computed emulate situation data original tasks unavailable. therefore older tasks changes time. feature extractor joint training cumulative training apply report performance ﬁnal stage tasks added. figure shows results dataset pairs. ﬁndings usually consistent single task experiment outperforms ﬁne-tuning feature extraction ﬁne-tuning newly added tasks. however performs similarly joint training newly added tasks underperforms joint training task tasks added. fig. performance task gradually adding tasks pre-trained network. different tasks shown different sub-graphs. x-axis labels indicate task added network time. error bars shows standard deviations runs different random initializations. markers jittered horizontally visualization line plots jittered facilitate comparison. tasks method degrades slower time ﬁne-tuning outperforms feature extraction scenarios. places→voc method performs comparably joint training. fig. inﬂuence subsampling task training compared methods. x-axis indicates diminishing training size. three runs experiments different random initialization dataset subsampling shown. scatter points jittered horizontally visualization line plots jittered facilitate comparison. differences compared methods task task decrease less data observations remain same. outperforms ﬁne-tuning despite change training size. imagenet alexnet. subsample dataset training network report result entire validation set. note joint training since dataset different size number images subsampled train tasks means smaller number imagenet images used time. results shown figure results show observations hold. method outperforms ﬁne-tuning tasks. differences methods tend increase data used although correlation deﬁnitive. choice task-speciﬁc layers. possible regard layers task-speciﬁc instead regarding output nodes task-speciﬁc. provide advantage tasks later layers tend task speciﬁc however requires storage parameters alexnet ﬁrst fully connected layers. table shows comparison three task pairs. results indicate advantage additional task-speciﬁc layers. fig. illustration alternative network modiﬁcation methods. fully connected layers task-speciﬁc rather shared. nodes multiple tasks connected way. also applied network expansion unfreezing nodes matching output responses tasks. figure illustrates method. nodes layer layers. weights nodes previous layer nodes current layer initialized netnet would expand layer copying nodes. weights nodes previous layer original nodes current layer initialized zero. layer weights nodes randomly re-initialized. either freeze existing weights ﬁne-tune weights task train using learning without forgetting note methods needs network scale quadratically respect number tasks. table shows comparison original method. network expansion performs better feature extraction well task. network expansion performs similarly additional computational cost complexity. effect lower learning rate shared parameters. investigate whether simply lowering learning rate shared parameters would preserve original task performance. result shown table reduced learning rate prevent ﬁne-tuning signiﬁcantly reducing original task performance reduces task performance. shows simply reducing learning rate shared layers insufﬁcient original task preservation. soft-constrained weights. perhaps obvious alternative keep network parameters close original. compare baseline adds ﬂattened vectors shared parameters original values. change coefﬁcient observe effect performance. places→voc imagenet→scene. fig. visualization task performance compared methods different weights losses. comparing methods; comparing losses. larger symbols signiﬁes larger i.e. heavier weight towards response-preserving loss. change) ﬁne-tuning believe regularizing output method maintains task performance better regularizing individual parameters since many small parameter changes could cause changes outputs. choice response preserving loss. compare cross-entropy loss knowledge distillation loss keeping similar. test task pairs before. figure shows results. results indicate knowledge distillation loss slightly outperforms compared losses although advantage large. address problem adapting vision system task preserving performance original tasks without access training data original tasks. propose learning without forgetting method convolutional neural networks seen hybrid knowledge distillation ﬁne-tuning learning parameters discriminative task preserving outputs original tasks training data. show effectiveness method number classiﬁcation tasks. another use-case example investigate using application tracking appendix build md-net views tracking template classiﬁcation task. classiﬁer transferred training videos ﬁnetuned online classify regions object background. propose replace ﬁne-tuning step learning without forgetting. leave details implementation appendix. observe improvements applying difference statistically signiﬁcant. work implications uses. first want expand possible predictions existing network method performs similarly joint training faster train require access training data previous tasks. second care performance task method often outperforms current standard practice ﬁne-tuning. fine-tuning approaches learning rate hopes parameters settle good local minimum original values. preserving outputs task direct interpretable retain important shared structures learned previous tasks. several directions future work. demonstrated effectiveness image classiﬁcation experiment tracking would like experiment semantic segmentation detection problems outside computer vision. additionally could explore variants approach maintaining unlabeled images serve representative examples previously learned tasks. theoretically would interesting bound task performance based preserving outputs sample drawn different distribution. generally need approaches suitable online learning across different tasks especially classes heavy tailed distributions. mccloskey cohen catastrophic interference connectionist networks sequential learning problem psychology learning motivation vol. goodfellow mirza xiao courville bengio empirical investigation catastrophic forgetting gradientbased neural networks arxiv preprint arxiv. russakovsky deng krause satheesh huang karpathy khosla bernstein berg fei-fei imagenet large scale visual recognition challenge international journal computer vision vol. donahue vinyals hoffman zhang tzeng darrell decaf deep convolutional activation feature generic visual recognition international conference machine learning girshick donahue darrell malik rich feature hierarchies accurate object detection semantic segmentation ieee conference computer vision pattern recognition june razavian azizpour sullivan carlsson features off-the-shelf astounding baseline recognition proceedings ieee conference computer vision pattern recognition workshops azizpour razavian sullivan maki carlsson factors transferability generic convnet representation ieee transactions pattern analysis machine intelligence mitchell cohen hruschka talukdar betteridge carlson dalvi gardner kisiel krishnamurthy mazaitis mohamed nakashole platanios ritter samadi settles wang wijaya gupta chen saparov greaves welling never-ending learning proceedings twenty-ninth aaai conference artiﬁcial intelligence everingham eslami gool williams winn zisserman pascal visual object classes challenge retrospective international journal computer vision vol. jan. kristan matas leonardis felsberg ˇcehovin fernandez vojir h¨ager nebehay pﬂugfelder gupta bibi lukeˇziˇc garcia-martin saffari petrosino montero varfolomieiev baskurt zhao ghanem martinez wang garcia zhang schmid huang prokhorov d.y. yeung ribeiro khan porikli bunyak seetharaman kieritz bischof possegger bogun chan jeong j.y. feng choi j.-w. lang martinez choi xing palaniappan lebeda alahari wong bertinetto pootschi maresca danelljan zhang arens valstar tang m.-c. chang khan wang miksik torr wang martin-nieto pelapur bowden laganiere moujtahid hare hadﬁeld s.-c. becker duffner hicks golodetz choi mauthner pridmore ¨ubner wang zhao shizeng chen huang chen zhang visual object tracking challenge results visual object tracking workshop iccv appendix tracking md-net using analyze ability learning without forgetting generalize beyond classiﬁcation tasks examine usecase improving general object tracking videos. task bounding tracked object image frame given ﬁrst frame’s groundtruth bounding known. usually algorithm causal i.e. result frame depend image frames onward. base method md-net state-of-theart tracker poses tracking template classiﬁcation task. unique uses ﬁne-tuning transfer general network jointly trained number videos classiﬁer speciﬁc test video. fine-tuning potentially cause undue drift original parameters. hypothesize replacing effective. experiment using slightly improves md-net difference statistically signiﬁcant. md-net md-net tracks object sampling bounding boxes proximity bounding last frame using classiﬁer classify foreground object background clutter. algorithm picks bounding highest foreground score apply bounding regression report regression result. uniqueness md-net comes classiﬁer trained. order obtain general representation objects suitable video tracking md-net pretrains -layer multidomain neural network classifying foreground versus background bounding boxes different sequences. convolutional layers initialized vgg-m network. data different sequences considered different domains therefore pretraining procedure joint training ﬁrst layers shared ﬁnal layer domain-speciﬁc thus name multi-domain convolutional neural network. topmost shared layer provides general representation tracked objects videos. test time ﬁnal layers discarded replaced randomly initialized layer test video. convolutional layers frozen rest network trained samples ﬁrst frame. bounding regression layer trained convolutional layers ﬁrst frame’s data kept unchanged. md-net starts track object consequent frames occasionally training fully-connected layers using data previous frames sampled hard-negative mining. refer readers original paper details. md-net evaluated among datasets general object tracking benchmark challenge. mainly uses expected average overlap measure combination tracking accuracy robustness evaluate trackers. refer readers report details. md-net online training method used test time seen ﬁne-tune baseline. since method outperforms ﬁne-tune task time experimented using learning without forgetting perform online training step. hopefully additional regularization beneﬁt updates since task data conﬁned space speciﬁcally pretrained network using code provided authors. test time instead throwing away task-speciﬁc ﬁnal layers keep task parameters. also keep copy original pretrained network compute responses tasks task data obtained online network changed. performing online training training data network compute responses learning without forgetting loss updated multi-task network. loss balance used. convolutional layers left frozen like md-net. rest training tracking testing procedure left unchanged. like md-net pretrain using excluding sequences appearing tracking algorithm tested runs. results. table shows performance method. methods start pre-trained network md-net reports slightly better performance possibly randomness pretraining step. observe method slightly improves md-net. however compute expected average overlap single runs scores vary greatly. observe improvement statistically signiﬁcant medium rooms bakery bookstore bowling buffet classroom clothingstore computerroom deli fastfood restaurant ﬂorist gameroom jewelleryshop kindergarden laboratorywet laundromat locker room meeting room ofﬁce pantry restaurant shoeshop toystore videostore. rooms artstudio bathroom bedroom children room closet corridor dentalofﬁce dining room elevator garage hairsalon hospitalroom kitchen livingroom nursery operating room prisoncell restaurant kitchen stairscase studiomusic studio waitingroom", "year": 2016}