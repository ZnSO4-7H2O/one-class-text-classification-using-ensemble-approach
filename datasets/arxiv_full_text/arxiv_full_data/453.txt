{"title": "Question Answering over Knowledge Base with Neural Attention Combining  Global Knowledge Information", "tag": ["cs.IR", "cs.AI", "cs.CL", "cs.NE"], "abstract": "With the rapid growth of knowledge bases (KBs) on the web, how to take full advantage of them becomes increasingly important. Knowledge base-based question answering (KB-QA) is one of the most promising approaches to access the substantial knowledge. Meantime, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is unable to express the proper information of the question. Hence, we present a neural attention-based model to represent the questions dynamically according to the different focuses of various candidate answer aspects. In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. And it also alleviates the out of vocabulary (OOV) problem, which helps the attention model to represent the question more precisely. The experimental results on WEBQUESTIONS demonstrate the effectiveness of the proposed approach.", "text": "rapid growth knowledge bases take full advantage becomes increasingly important. knowledge basebased question answering promising approaches access substantial knowledge. meantime neural networkbased methods develop nn-based kb-qa already achieved impressive results. however previous work emphasis question representation question converted ﬁxed vector regardless candidate answers. simple representation strategy unable express proper information question. hence present neural attention-based model represent questions dynamically according different focuses various candidate answer aspects. addition leverage global knowledge inside underlying aiming integrating rich information representation answers. also alleviates vocabulary problem helps attention model represent question precisely. experimental results webquestions demonstrate effectiveness proposed approach. introduction amount knowledge bases grows people paying attention seeking effective methods accessing precious intellectual resources. several tailor-made languages designed querying sparql however handle query languages users required familiar particular language grammars also aware vocabularies kbs. contrast knowledge base-based question answering takes natural language query language user-friendly solution become research focus recent years. mainstream research directions task i.e. semantic parsing-based information retrieve-based methods. sp-based methods usually focus constructing semantic parser could convert natural language questions structured expressions like logical forms. ir-based methods like search answers based information conveyed questions. here ranking techniques often adopted make correct selections candidate answers. general ir-based methods easier ﬂexible implement. proven irbased methods could acquire competitive performance compared sp-based methods experiments conducted freebase recently progress deep learning neural network-based methods introduced kb-qa task belong irbased methods. different previous methods nn-based methods represent questions answers semantic vectors. complex process kb-qa could converted similarity matching process input question candidate answers semantic space. candidates highest similarity score considered ﬁnal answers. adaptive robust nn-based methods attracted attention paper also focus using neural networks answer questions knowledge base. nn-based methods crucial step compute similarity score question candidate answer learn representations. previous methods emphasis learning representations answer end. example considers importance subgraph candidate makes context swers. type answers. contrast representation methods question oligotrophic. existing approaches often represent question single vector using simple bag-of-words model whereas relatedness answer neglected. argue question repreone candidate answers francois hollande example. dealing answer entity francois holland president france question focused question representation bias towards words. facing answer type /business/board member prominent word. obviously attention mechanism reﬂects focus answer aspects could inﬂuence representation question. learning representations questions make proper word question according different attention aspect candidate answer instead simply compressing ﬁxed vector. believe kind representations represents questions expressive. using three cnns different parameters dealing different answer aspects including answer path answer context answer type. think simply selecting three independent cnns mechanical inﬂexible. thus step further propose attentionbased neural network perform question answering different represent question differently according different answer resources allowing sharing network does. instance /business/board member /location/country answer types question representation different according different attention method. hand notice representations resources also limited previous work. speciﬁc often learned barely training data results limitations. deﬁciency global information previous methods merely utilize answer-related part i.e. answer path answer context learn representations resources. global information completely ignored. example question-answer pair appears training data global information implies similar denoted probable right. however current training mechanism cannot guarantee could learned. problem vocabulary limited coverage training data problem common testing many answer entities testing candidate never seen before. scenario representation unseen resources could learned precisely. attention resources become shared embedding harm proposed attention model. tackle problems additionally incorporates training data training embeddings besides original questionanswer pairs. global structure whole knowledge could captured problem could alleviated naturally. summary contributions paper follows. present novel attention-based model tailored kb-qa task considers inﬂuence answer aspects representing questions. goal kb-qa task could formulated follows. given natural language question return entity answers. architecture proposed kbqa system shown figure illustrates basic approach. first identify topic entity question generate candidate answers freebase. then candidate answers represented regard four aspects. next attention-based neural network employed represent question inﬂuence candidate answer aspects. finally similarity score between question corresponding candidate answer calculated candidates highest score selected ﬁnal answers. utilize freebase knowledge base. billion facts used supporting many tasks. freebase facts represented subject-property-object triples clarity call basic element resource could either entity relation. example describe capital france paris /m/flc /m/qtj entities denoting france paris respectively location.country.capital relation. answer aspect representation answer directly embedding answer aspect embedding matrix rd×vk. here means vocabulary size resources. embedding matrix randomly initialized learned training could enhanced help global information described section concretely employ four kinds answer aspects namely answer entity answer relation answer type answer context embeddings denoted respectively. worth noting answer context consists multiple resources denote ﬁrst acquire embeddings calculate average embedding attention model crucial part proposed approach attention mechanism. based assumption answer aspect different attention towards question. extent attention measured relatedness word representation answer aspect embedding propose following formulas calculate weights. approach candidate generation candidate answers entities freebase ideally practice time consuming really necessary. question freebase identify topic entity could simply understood main entity question. example france topic entity question president france?. freebase method able resolve many questions result getting topic entity collect entities directly connected ones connected -hop. entities constitute candidate proposed neural attention model present attention-based neural network represents question dynamically according different answer aspects. concretely aspect answer pays different attention question thus decides question represented. extent attention used weight word question. figure architecture model. illustrate system works follows. lstm first obtain representation word question. representations retain information question could serve following steps. suppose question expressed denotes word. shown figure ﬁrst look word embedding matrix rd×vw word embeddings randomly initialized updated training process. here means dimension embeddings denotes vocabulary size natural language words. then embeddings long short-term memory networks. lstm proven effective many natural language processing tasks machine translation dependency parsing adept harnessing long sentences. note unidirectional lstm outcome speciﬁc word contains information words whereas words taken account. avoid this employ bidirectional lstm does consists forward backward networks. forward lstm handles question left right backward lstm processes reverse order. thus could acquire hidden state sequences forward ...hn) backward concatenate forward hidden state backward hidden state word resulting hidden unit forward backward lstm concatenated vector dimension obtain representation word question. worth noting many questions answer improper smax ﬁnal answer. instead make margin loss function score candidate answer within margin compared smax ﬁnal answer set. combining global knowledge information section elaborate global information could leveraged. stated before take account complete structural information adopt transe model represent integrate representations training process. transe model entities relations represented dimensional embeddings. basic idea relations regarded translations embedding space. here consistency denote fact boldface denote embeddings. embedding tail entity close embedding head entity plus embedding relation i.e. energy triple equal dissimilarity deﬁned learn embeddings transe minimizes following loss function. facts corrupted facts composed positive facts either head tail replace random entity. loss function favors lower values energy positive facts negative facts. implementation ﬁlter completely unrelated facts save time. speciﬁc ﬁrst collect topic entities questions initial set. then expand adding direct connected -hop entities. finally facts entities appeared form positive set. negative facts randomly corrupted ones. compromise solution large scale freebase. combine global information training process adopts multi-task training strategy. speciﬁcally perform kb-qa training transe training turn. epoch kb-qa training epochs transe training conducted embeddings resources shared updated training processes. proposed training process ensures global information additional supervision interconnections among resources fully considered. addition resources involved problem relieved able bring additional beneﬁts attention model. intermediate matrix offset value. randomly initialized updated training. subsequently attention weights employed calculate weighted words resulting semantic vector represent question according speciﬁc answer aspect proposed attention model could also intuitively interpreted re-reading mechanism select correct answers candidate set. consider candidate answer suppose ﬁrst look type re-read question part question focused next aspect re-read question again aspects utilized. believe mechanism beneﬁcial system better understand question help answer aspects leads performance promotion. training since question-answer pairs supervision data candidate question divided subsets namely correct answer wrong answer correct answer randomly select wrong answers negative examples. topic entities enough wrong answers acquire wrong answers. under circumstance extend randomly selected candidate generated training data able make pairwise training. positive real number ensure margin positive negative examples. means max. intuition training strategy guarantee score positive question-answer pairs higher negative ones margin. resent questions. also note uses additional training data reverb original dataset simplequestions. employs three ﬁxed cnns represent questions able express focus unique answer aspect question representation. besides global information leveraged. believe results faithfully show proposed approach effective competitive methods. worth noting achieves much higher methods. staged system able address questions constraints aggregations. however approach applies numbers manually designed rules features come observations training questions. particular manual efforts reduce adaptability approach. model analysis lstm employs unidirectional lstm uses last hidden state question representation. lstm adopts bidirectional lstm. ...hn) denote forward lstm indicate backward lstm ﬁnal presentation question lstm+att bidirectional lstm neural attention lstm+gki denote bidirectional lstm model global information lstms+att+gki ours bidirectional lstm model attention model global information. lstm similarly lstm+att+gki improves lstm+att results indicate proposed training strategy successfully leverages global information underlying question-answer pairs training testing. questions collected google suggest answers labeled manually amazon mturk. answers freebase. three-quarter training data training remaining quarter validate set. score computed script provided select evaluation metric settings kb-qa training mini-batch stochastic gradient descent minimize pairwise training loss. minibatch size learning rate word embedding matrix embedding matrix normalized epoch. embedding size hidden unit size margin negative example number transe training process deﬁnes embeddings dimension mini-batch size also hyperparameters proposed network determined according performance validate set. demonstrate effectiveness proposed approach compare method previous nn-based methods. table shows results webquestions test set. methods listed table employ neural network kbqa. applies method obtain single vector questions answers. improves work proposing concept subgraph embeddings. besides answer path subgraph contains entities relations connected answer entity. ﬁnal vector also obtained strategy. follows sp-based manner uses embeddings entities relations resources question converted logical forms. jointly consider mapping process. uses three columns cnns represent questions corresponding three aspects answers namely answer context answer path answer type. puts kb-qa memory networks framework achieves state-of-the-art performance. represents proposed approach. results observe achieves best performance webquestions. utilize model represent questions takes advantage attention answer aspects dynamically repexample observe methods able capture attention properly. instructive ﬁgure attention part question dealing different answer aspects. heat help understand parts useful selecting correct answers. instance figure location.country paying great attention where indicating where much important parts question dealing type. words parts crucial since ‘where strongly implying question asking location. occasions generated attention weights unreasonable. instance question what songs justin bieber wrote? answer type /music/composition pays attention what rather songs. think bias training data believe errors could solved introducing instructive training data future. complex questions label errors another challenging problem complex questions example when last time knicks championship? actually last championship predicted answers give championships. model cannot learn last mean training process. addition label mistakes also inﬂuence evaluation example what college john nash teach at?. labeled answer princeton university massachusetts institute technology also answer proposed method able answer correctly. errors include topic entity generation error multiple answers error guess errors caused simplest implementations related steps method explain detail space limitation. related work neural network-based kb-qa ﬁrst applies nn-based method solve kb-qa problem. questions triples represented vectors dimensional space. thus cosine similarity could used possible answer. method employed obtain single vector questions answers. pairwise training utilized negative examples randomly selected facts. also present training data generation method i.e. using facts heuristics rules generate natural language questions. improves work proposing concept subgraph embeddings. idea involve much information answer end. besides answer triple subgraph contains entities relations connected answer entity. ﬁnal vector also obtained strategy. focuses single-relation questions. kb-qa task divided parts i.e. ﬁnding entity mention-entity mapping mapping remaining relation pattern relation. train models perform mapping processes. handles entity relation mapping joint procedures. strictly speaking methods follow sp-based manner take advantage neural networks obtain intermediate mapping results. similar work consider different aspects answers using three columns cnns represent questions respectively. difference approach uses attention mechanism unique answer aspect question representation ﬁxed three types. moreover utilize global information. attention-based model ﬁrst applies attention model nlp. improve encoder-decoder neural machine translation framework jointly learning alignment translation. argue representing source sentence ﬁxed vector unreasonable propose soft-align method could understood attention mechanism. also tackling machine translation task. propose attentions models i.e. global model local model. latter indicates small scope attend achieves better results. representing question propose novel attentionbased model kb-qa. speciﬁcally attention answer aspect word question used. kind dynamic representation precise ﬂexible. second leverage global information could take full advantage complete also could alleviate problem. extensive experiments demonstrate proposed approach could achieve better performance compared state-of-the-art nn-based methods. colin evans praveen paritosh sturge jamie taylor. freebase collaboratively created graph database structuring proceedings sigmod pages human knowledge. antoine bordes nicolas usunier alberto garcia-duran oksana yakhnenko. translating embeddings modeling multi-relational data. advances neural information processing systems pages antoine bordes jason weston nicolas usunier. open question answering weakly machine learning supervised embedding models. knowledge discovery databases pages springer chris dyer miguel ballesteros wang ling austin matthews noah smith. transitionbased dependency parsing stack long short-term memory. arxiv preprint arxiv. anthony fader stephen soderland oren etzioni. identifying relations open information extraction. proceedings emnlp pages association computational linguistics karl moritz hermann tomas kocisky edward grefenstette lasse espeholt mustafa suleyman phil blunsom. teaching machines read comprehend. advances neural information processing systems pages wen-tau xiaodong christopher meek. semantic parsing single-relation question answering. proceedings pages wen-tau ming-wei chang xiaodong jianfeng gao. semantic parsing staged query graph generation question answering knowledge base. proceedings ijcnlp pages luke zettlemoyer michael collins. learning sentences logical form structured classiﬁcation probabilistic categoproceedings pages rial grammars.", "year": 2016}