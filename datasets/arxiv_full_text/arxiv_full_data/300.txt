{"title": "Training Simplification and Model Simplification for Deep Learning: A  Minimal Effort Back Propagation Method", "tag": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "abstract": "We propose a simple yet effective technique to simplify the training and the resulting model of neural networks. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-$k$ elements (in terms of magnitude) are kept. As a result, only $k$ rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction in the computational cost. Based on the sparsified gradients, we further simplify the model by eliminating the rows or columns that are seldom updated, which will reduce the computational cost both in the training and decoding, and potentially accelerate decoding in real-world applications. Surprisingly, experimental results demonstrate that most of time we only need to update fewer than 5% of the weights at each back propagation pass. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given. The model simplification results show that we could adaptively simplify the model which could often be reduced by around 9x, without any loss on accuracy or even with improved accuracy.", "text": "abstract—we propose simple effective technique simplify training resulting model neural networks. back propagation small subset full gradient computed update model parameters. gradient vectors sparsiﬁed top-k elements kept. result rows columns weight matrix modiﬁed leading linear reduction computational cost. based sparsiﬁed gradients simplify model eliminating rows columns seldom updated reduce computational cost training decoding potentially accelerate decoding real-world applications. surprisingly experimental results demonstrate time need update fewer weights back propagation pass. interestingly accuracy resulting models actually improved rather degraded detailed analysis given. model simpliﬁcation results show could adaptively simplify model could often reduced around without loss accuracy even improved accuracy. propagation usually dominates computational cost learning process. back propagation entails high computational cost needs compute full gradients update model parameters learning step. uncommon neural network massive number model parameters. study propose minimal effort back propagation method call meprop neural network learning. idea compute small critical portion gradient information update corresponding minimal portion parameters learning step. leads sparsiﬁed gradients highly relevant parameters updated parameters stay untouched. sparsiﬁed back propagation leads linear reduction computational cost. meprop propose simplify trained model eliminating less relevant parameters discovered meprop computational cost decoding also reduced. name method mesimp idea record portion parameters updated learning step meprop gradually remove parameters less updated. leads simpliﬁed model costs wang school electronics engineering computer science peking university china laboratory computational linguistics peking university china. e-mail {xusun renxc shumingma weibz liweitj wanghf}pku.edu.cn motivations method suppose back propagation determine importance input features meprop essential features welltrained non-essential features less-trained robustness models improved overﬁtting reduced. essential features play important role ﬁnal model chances parameters related non-essential features could eliminated leads idea mesimp. classiﬁcation task essential features decisive classiﬁcation non-essential features helpful also distractions irrelevant features useful all. example classifying picture taxi taxi sign essential features color yellow often color taxi non-essential features. overﬁtting often occurs non-essential features given much importance model meprop intentionally focuses training probable essential features lessen risk overﬁtting. realize approaches need answer four questions. ﬁrst question highly relevant subset parameters current sample stochastic learning. propose top-k search method important parameters. interestingly experimental results demonstrate time need update fewer weights back propagation pass. result larger number training iterations. proposed method general-purpose independent speciﬁc models speciﬁc optimizers back propagation strategy would hurt accuracy trained models. show strategy degrade accuracy trained model even small portion parameters updated. interestingly experimental results reveal strategy actually improves model accuracy cases. based experiments probably minimal effort update modify weakly relevant parameters update according assumption makes overﬁtting less likely similar dropout effect. third question whether decoding cost model reduced meprop shorten training time. based meprop apply technique mesimp. observations simplifying strategy indeed shrink ﬁnal model usually around without loss accuracy. also supports assumption that fact many learned features essential ﬁnal correct prediction. ﬁnal question whether size simpliﬁed models needs explicitly advance. previous work ﬁnal model size pre-conﬁgured desired using heuristic rules making hard simplify models multiple layers naturally layer different dimension since captures different level abstraction. practice mesimp could adaptively reduce size hidden layers automatically decide features essential task different abstraction levels resulting model different hidden layer sizes. propose minimal effort back propagation technique neural network learning could automatically important features. small subset full gradient computed update model parameters used determine whether related parameters kept ﬁnal model. applying technique training simpliﬁcation strategy actually improve accuracy resulting models rather degraded even fewer weights updated back propagation pass time. technique entail larger number importantly applying technique model simpliﬁcation could potentially reduce time decoding. ability adaptively simplify layer model keep essential features resulting model could reduced around ninth original size equals around reduction decoding cost base accuracy loss even improved accuracy. it’s worth mentioning applied models multiple layers given single hyper-parameter mesimp could simplify hidden layer different extent alleviating need different hyperparameters different layers. minimal effort back propagation technique applied different types deep learning models applied various optimization methods works diverse tasks propose simple effective technique neural network learning. forward propagation computed usual. back propagation small subset full gradient computed update model parameters. gradient vectors quantized topk components terms magnitude kept. based technique propose simplify resulting models removing rows seldom updated according top-k indices. model simpliﬁed actively updated rows kept. ﬁrst present proposed methods describe implementation details. forward propagation neural network models including feedforward neural networks lstm consists linear transformations non-linear transformations. simplicity take computation unit linear transformation non-linear transformation example figure illustration meprop single computation unit neural models. original back propagation uses full gradient output vectors compute gradient parameters. proposed method selects top-k values gradient output vector back propagates loss corresponding subset total model parameters. proposed meprop uses approximate gradients keeping top-k elements based magnitude values. top-k elements largest absolute values kept. example suppose vector de’s top-k values note indices vector approximate gradient parameter matrix input vector proposed meprop selects top-k elements gradient approximate original gradient passes gradient computation graph according chain rule. hence gradient goes figure shows illustration computational meprop. forward propagation traditional forward propagation computes output vector matrix multiplication operation input tensors. original back propagation computes full gradient input vector weight matrix. meprop back propagation computes approximate gradient keeping top-k values backward ﬂowed gradient masking remaining values learning applications real life even important reduce computational cost decoding fact although training time consuming needs done once decoding needs done long request. section propose simplify model eliminating inactive paths deﬁne neurons whose gradients top-k. decoding cost would also reduced. major concerns proposal. main problem don’t know active path unseen examples advance don’t know gradient information examples. solution problem could obtain overall inactive paths inactive paths training samples could removed gradually training. second reduction dimension could lead performance degradation. surprisingly experimental results top-k gradient based method deteriorate model. instead appropriate conﬁguration resulting smaller model often performs better baseline large model even baseline model similar size. matter fact pruning performance drop. however following training performance regained. follows brieﬂy introduce inspiration proposed method model simpliﬁcation done. paths universal inactive paths. neurons updated training parameter values remain initialized values every reason believe would effective samples well. however number paths enough bring substantial contraction model. based previous ﬁndings generalize idea universal inactive paths prune paths less updated paths eliminate paths active number samples. realize idea keep record many times index top-k indices back propagation time meprop. several training steps take less active paths updated number samples e.g. prune rate results simpliﬁed model. record cleared pruning action. iteratively model size approach near stable end. algorithm describes method computation unit illustration shown figure important hyper-parameter method pruning threshold. determining threshold model size number examples pruning actions taken account. shown algorithm threshold could parameterized prune interval many samples pruning prune rate active path eliminated. note layer sizes determined adaptively multi-layer setting threshold needed model multiple layers different layer sizes. top-k indices different layers different iterations intersect differently back propagation. layers top-k indices similar hence results larger layer size compared layers top-k indices quite different iteration intersection happens often means lower hence resulting layer size smaller. layer simpliﬁed depends learning done accordance intuition. fig. illustration model simpliﬁcation ﬁgure shows three main stages simpliﬁed model training. first upper shows model trained using meprop several iterations record activeness paths kept indicated shades neurons. second middle shows model simpliﬁed based collected record inactive paths eliminated. third lower shows train simpliﬁed model also using meprop. repeat procedure goal training met. deep neural network it’s worth noticing simplifying hidden layer respective columns next layer could also removed values columns represent connection eliminated inputs outputs longer effective. could reduce model even further. however include implementation yet. extra considerations lstm models. lstm lasting linear memory four gates controlling modifying memory cells. makes sense pruning memory cells instead gates computation units deﬁned previously coherence memory gates. otherwise pruning would cause chaos mismatch dimensions gate size memory another dimension union gates. lstm models treat lstm module whole unit model simpliﬁcation instead treating gate lstm module unit simpliﬁcation. however top-k gradient selection takes place level gates rather memory cells. practice still obtain top-k indices gates merge top-k indices records gates record pruning memory cells related gates pruned well. model simpliﬁcation also propose kind cycle mechanism. experiments time simpliﬁcation drop performance recovers quickly within following training even supersede performance simpliﬁcation. makes wonder whether training simpliﬁcation critical performance improvement. propose divide training procedure several stages stage ﬁrst conduct training model simpliﬁcation conduct normal training. start stage also reinitialize optimizer historical information gradients stored. reason operation model simpliﬁcation dynamics neurons interacted changed previous gradient information interfere dynamics simpliﬁed network. cycle mechanism could improve resulting model’s performance even tasks. coded neural network models including lstm model part-of-speech tagging feedforward model transition-based dependency parsing mnist image recognition. optimizers automatically adaptive learning rates including adam adagrad implementation make modiﬁcation optimizers although many zero elements gradients. experiments conducted framework coded own. framework builds dynamic computation graph model sample making suitable data variable lengths. typical training procedure contains three parts forward propagation back propagation parameter update. also implementation based pytorch framework based experiments. focus method itself results based experiments presented appendices. proposed method aims reduce complexity back propagation reducing elements computationally intensive operations. preliminary observations matrix-matrix matrix-vector multiplication consumed time back propagation. implementation apply meprop back propagation output multiplication inputs. element-wise operations original back propagation procedure kept operations already fast enough compared matrix-matrix matrix-vector multiplication operations. multiple hidden layers top-k sparsiﬁcation needs applied every hidden layer sparsiﬁed gradient dense layer another. meprop gradients sparsiﬁed top-k operation output every hidden layer. apply meprop hidden layers using top-k usually output layer could different hidden layers output layer typically different dimension compared hidden layers. example tags mnist task dimension output layer hidden dimension thus best output layer could different hidden layers. implementation instead sorting entire vector well-known min-heap based top-k selection method slightly changed focus memory reuse. algorithm time complexity space complexity pytorch comes implementation certain paralleled top-k algorithm sure operation done exactly. transition-based dependency parsing following prior work english penn treebank evaluation. follow standard split corpus sections training section development section ﬁnal test evaluation metric unlabeled attachment score implement parser using following used baseline. part-of-speech tagging standard benchmark dataset prior work derived penn treebank corpus sections wall street journal training sections testing evaluation metric per-word accuracy. popular model task lstm model used baseline. mnist image recognition mnist handwritten digit dataset evaluation. mnist consists pixel training images additional test examples. image contains single numerical digit select ﬁrst images training images development rest meprop results based lstm/mlp models adam optimizers. time means averaged time iteration. iter means number iterations reach optimal score development data. model iteration used obtain test score. applying meprop dimension hidden layers tasks. parsing input dimension output dimension pos-tag input dimension output dimension mnist input dimension output dimension based development prior work mini-batch size parsing pos-tag mnist respectively. using transition examples parsing follows discussed section optimal topk output layer could different hidden layers dimensions could different. parsing mnist using output hidden layers works well simply another task pos-tag output layer different hidden layers. simplicity apply meprop output layer pos-tag task computational cost output layer almost negligible compared layers. experiments model simpliﬁcation adam optimizer tasks sake simplicity. addition also apply cycle mechanism reported results. note that simulate real scenario conﬁguration times different random seeds choose best model development report. hyperparameters tuned based development data. adam optimization method default hyperparameters work well development sets follows learning rate experiments conducted computer intel xeon .ghz cpu. experiments conducted nvidia geforce hidden layers presented later). conduct experiments different optimization methods including adagrad adam. since meprop applied linear transformations report linear transformation related backprop time backprop time. include non-linear activations usually less computational cost. total time back propagation including nonlinear activations reported overall backprop time. table shows results based different models different optimization methods. table meprop means applying meprop corresponding baseline model means hidden layer dimension means meprop uses top- elements back propagation. note that fair comparisons experiments ﬁrst conducted development data test data observable. then optimal number iterations decided based optimal score development data model iteration used upon test data obtain test scores. applying meprop substantially speed back propagation. provides linear reduction computational cost. surprisingly results demonstrate update fewer weights back propagation pass natural language processing tasks. result larger number training iterations. surprisingly accuracy resulting models actually improved rather decreased. main reason could minimal effort update modify weakly relevant parameters makes overﬁtting less likely similar dropout effect. important whether meprop applied different optimizers minimal effort technique spariﬁes gradient affects update parameters. adagrad learner learning rate parsing pos-tag mnist respectively shown table results consistent among adagrad adam. results demonstrate meprop independent speciﬁc optimization methods. simplicity following experiments adam. figure vary top-k meprop compare test accuracy different ratios meprop backprop. example means backprop ratio /=%. optimizer adam. meprop achieves consistently better accuracy baseline. interesting check role top-k elements. figure shows results top-k meprop random meprop. random meprop means random elements selected back propagation. top-k version works better random version. suggests top-k elements contain important information gradients. still question top-k meprop work well simply original model require dimension hidden layers? example meprop works simply lstm works well hidden dimension need hidden dimension examine this perform experiments using hidden dimension results shown table however results small hidden dimensions much worse meprop. addition figure shows detailed curves varying value ﬁgure different gives different backprop ratio meprop different hidden dimension ratio lstm/mlp. answer question negative meprop rely redundant hidden layer elements. another question whether meprop relies shallow models hidden layers. answer question also perform experiments hidden layers hidden layers hidden layers. setting dropout rate works well cases different numbers layers. simplicity comparison dropout rate experiment. table shows adding number hidden layers hurt performance meprop. since observed meprop reduce overﬁtting deep learning natural question meprop reducing type overﬁtting risk dropout. thus development data proper value dropout rate tasks meprop check improvement possible. meprop adding dropout technique. results show meprop could improve performance dropout suggesting meprop reducing different type overﬁtting comparing implementing meprop simplest solution treat entire mini-batch training example top-k operation based averaged values examples mini-batch. sparse matrix mini-batch consistent sparse patterns among examples consistent sparse matrix transformed small dense matrix removing zero values. call implementation simple uniﬁed top-k. experiment based pytorch. despite simplicity table shows good performance implementation based minibatch size also speedup less signiﬁcant hidden dimension low. reason gpu’s computational power fully consumed baseline normal back propagation already fast enough making hard meprop achieve substantial speedup. example supposing ﬁnish operations cycle could speed difference method method operations. indeed almost speed even forward propagation theoretically difference. forward propagation time respectively. provides evidence hypothesis fully consumed small hidden dimensions. thus speedup test meaningful heavy models baseline least fully consume gpu’s computational power. check this test speedup synthetic data matrix multiplication larger hidden dimension. indeed table shows meprop achieves much higher speed traditional backprop large hidden dimension. furthermore test speedup large hidden dimension table shows meprop also substantial speedup mnist large hidden dimension. experiment speedup based overall backprop time results demonstrate meprop achieve good speedup applied heavy models. implementation choices meprop gpu. example another natural solution sparse matrix represent sparsiﬁed gradient output mini-batch. then sparse matrix multiplication library used accelerate computation. could interesting direction future work. experiment simplify hidden layers model adam optimizer tasks. cycle tasks ﬁrst train model using mesimp epochs train model normally epochs repeat procedure till end. table shows model simpliﬁcatoin results based different models. table meprop means applying meprop corresponding baseline model mesimp means applying model compression meprop. means dimension model’s hidden layers means back propagation propagate top- elements prune means dimension updated less times statistical interval dropped. method capable reducing models relatively small size maintaining performance improving. hidden layers models reduced around parsing pos-tag mnist respectively. means simpliﬁed model deployed could achieve reason could minimal effort update captures important features simpliﬁed model enough data without minimal effort update model similar size treats feature equally start limiting ability learn data. show experimental results section results show simplifying method effective reducing model size thus bringing substantial reduction computational cost decoding real-world task. importantly accuracy original model kept even often improved. means model simplifying could make probable deploy deep learning system computation constrained environment. it’s worth noticing mesimp also able automatically determine appropriate size resulting model deep neural networks beginning conduct experiments neural network single hidden layer parsing promising result model size reduced original size. result paring makes wondering whether meprop could also simplify deeper networks continue experiments different models. experiments pos-tag lstm based bi-lstm forward lstm backward lstm regarding input sequences means often deep time series shown table forward backward lstms indeed gets different dimensions respectively. conduct experiments hidden layers result shows ﬁrst hidden layer second hidden layer also need remind readers mesimp need specify different hyper-parameters different layers previous work different layer sizes pursued different hyperparameter need separately hidden layer limiting ability simplifying models adaptively. natural important question simpliﬁed model perform compared model similar size. simpliﬁed models perform well normally trained models similar sizes simplifying method redundant unnecessary. results table shed light question. train baseline models sizes similar sizes simpliﬁed models report results table shown that simpliﬁed models perform better models trained similar sizes especially parsing task. results shows model simplifying training unnecessary simpliﬁed model achieves better accuracy model trained using small dimension. back propagation simpliﬁcation model simpliﬁcation results could approaches based active paths measured back propagation effective reducing overﬁtting. hypothesis neural network example small part neurons needed produce correct results gradients good identiﬁers detect identiﬁer. layer neuron’s accumulated absolute gradients accounts less speciﬁed percentage gradients neurons layer threshold number neurons layer threshold neuron considered inactive. paths active paths deactivated previous activation values last encounter training outputs thus effort activation minimized. sparse activation done forward propagation back propagation sparsiﬁed well obviously deactivate neurons contribute none results meaning gradients zeros requires computation. note method reduce size model example obtain active paths. test forward propagation done normally wouldn’t know active paths unseen examples. results shown table that mnist task average fewer neurons adequate achieve good results even better results. results shown figure that minimal effort activation accuracy rises baseline shows accuracy could beneﬁt training focused related neurons. accuracy improvement acquired investigate change parameters training. normally gradient used represent change. however adam optimizer update done directly using gradient consider change adam update rule change. many iterations epoch many parameters model average change parameters iteration means absolute update change parameter iteration means number parameters means number training iterations report average absolute change parameter iteration update. absolute update parameter would like much parameters modiﬁed training process change start end. shown figure update dropped sharply meact applied meaning accuracy improvement achieved little change parameters update normal training still high update meact suggesting many update redundant unnecessary could result model trying adapt noise data. regular pattern noise requires subtle update parameters noise much harder often affects training essential features thus leading lower accuracy method tries focus essential features example. results conﬁrm initial hypothesis example neurons required minimal effort technique provides simple effective train extract helpful neurons. fig. change accuracy average update parameters active path ﬁnding. isolate impact meact random seed means initialization parameters shufﬂe meact baseline lines coincide epoch training exactly same. meact applied accuracy rises indicates training focused relevant neurons could reduce overﬁtting update drops suggests later normal training update caused ﬁtting noises making already trained neurons constantly changing. examine hypothesis design algorithm call meact activates active path respect example forward propagation experimental results consistent hypothesis. realize idea example activate paths largest accumulated absolute gradients number chosen paths controlled threshold. speciﬁcally accumulate absolute gradients layer’s output example denoted neuron’s index layer example’s transition-based dependency parsing task existing approaches typically achieve score popular transition-based parsers maltparser uas. achieves using neural networks. method achieves mnist based approaches achieve accuracy often around method achieves help convolutional layers techniques accuracy improved method also improved additional techniques which however focus paper. shown meprop could improve accuracy convolutional neural network baseline reaching proposed direct adaptive method fast learning performs local adaptation weight update according behavior error function. also proposed adaptive acceleration strategy back propagation. dropout proposed improve training speed reduce risk overﬁtting. sparse coding class unsupervised methods learning sets over-complete bases represent data efﬁciently proposed sparse autoencoder model learning sparse over-complete features. proposed method quite different compared prior studies back propagation dropout sparse coding. sampled-output-loss methods limited softmax layer based random sampling method limitations. sparsely-gated mixture-of-experts sparsiﬁes mixture-of-experts gated layer limited speciﬁc setting mixture-of-experts method limitations. also prior studies focusing reducing communication cost distributed systems quantizing value gradient -bit ﬂoat -bit. settings also different ours. proposed reduce parameters family models masking parameters heuristically calculated threshold. method mainly designed make large scale family models smaller maintaining similar performance models feasibly deployed. model limited family models reduce training time model. propose distill expressive cumbersome model smaller model mimicking target cumbersome model adjusting temperature. claim method transfer knowledge learned cumbersome model simpler one. method doesn’t presume speciﬁc model. however ﬁnal model size preconﬁgured method could adaptively learn size model. proposed automatically choose size model parameters iteratively adding removing zero units. however method rather complicated limited batch normalization. propose densesparse-dense model ﬁrst eliminate units small absolute values reactivate units re-train them model achieve better performance. model really reduce size model purpose improve performance. note work could adaptively choose size layer deep neural networks single simplifying conﬁguration shown table previous work either related hyper parameter controlling ﬁnal model size needs layer model size needs directly methods lead trivial hyper-parameter tuning different hidden layer sizes pursued different layers representing different levels abstraction naturally different sizes know advance. work eliminates needs that hyper parameter mesimp automatically determine size needed represent useful information hidden layer thus leading varied dimensions hidden layers. minimal effort technique adopts top-k selection based back propagation determine relevant features leads sparsiﬁed gradients compute given training sample. experiments show meprop reduce computational cost back propagation orders magnitude updating fewer parameters improve model accuracy cases. propose remove seldom updated parameters simplify resulting model purpose reducing computational cost decoding. experiments reveal model size could reduced around ninth original models leading around computational cost reduction decoding natural language processing tasks improved accuracy. importantly mesimp could automatically decide appropriate sizes different hidden layers alleviating need hyper-parameter tuning. tion corr vol. abs/. duchi hazan singer adaptive subgradient methods online learning stochastic optimization journal machine learning research vol. collins discriminative training methods hidden markov models theory experiments perceptron algorithms proceedings emnlp’ hochreiter schmidhuber long short-term memory neural computation vol. dryden moon jacobs essen communication quantization data-parallel training deep neural networks proceedings workshop machine learning environments matsuzaki tsujii comparative parser performance analysis across grammar frameworks automatic tree conversion using synchronous grammars proceedings coling’ nivre hall nilsson chanev eryigit ¨ubler marinov marsi maltparser language-independent system data-driven dependency parsing natural language engineering vol. riedmiller braun direct adaptive method faster backpropagation learning rprop algorithm proceedings ieee international conference neural networks srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting journal machine learning research vol.", "year": 2017}