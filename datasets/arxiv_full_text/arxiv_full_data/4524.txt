{"title": "Multi-Instance Learning by Treating Instances As Non-I.I.D. Samples", "tag": ["cs.LG", "cs.AI"], "abstract": "Multi-instance learning attempts to learn from a training set consisting of labeled bags each containing many unlabeled instances. Previous studies typically treat the instances in the bags as independently and identically distributed. However, the instances in a bag are rarely independent, and therefore a better performance can be expected if the instances are treated in an non-i.i.d. way that exploits the relations among instances. In this paper, we propose a simple yet effective multi-instance learning method, which regards each bag as a graph and uses a specific kernel to distinguish the graphs by considering the features of the nodes as well as the features of the edges that convey some relations among instances. The effectiveness of the proposed method is validated by experiments.", "text": "abstract. previous studies multi-instance learning typically treated instances bags independently identically distributed. instances however rarely independent real tasks better performance expected instances treated non-i.i.d. exploits relations among instances. paper propose simple eﬀective methods. ﬁrst method explicitly every undirected graph design graph kernel distinguishing positive negative bags. second method implicitly construct graphs deriving aﬃnity matrices propose eﬃcient graph kernel considering clique information. eﬀectiveness proposed methods validated experiments. multi-instance learning training example instances. positive contains least positive instance negative otherwise. although labels training bags known however labels instances bags unknown. goal construct learner classify unseen bags. multi-instance learning found useful diverse domains image categorization image retrieval text categorization computer security face detection computer-aided medical diagnosis etc. prominent advantage multi-instance learning mainly lies fact many real objects inherent structures adopting multi-instance representation able represent objects naturally capture information simply using single-instance representation. example suppose partition image several parts. contrast representing whole image single-instance represent part instance partition information captured multi-instance representation; partition meaningful additional information captured multi-instance representation helpful make learning task easier deal with. obviously good idea apply multi-instance learning techniques everywhere since single-instance representation suﬃcient using multiinstance representation gilds lily. even tasks objects inherent structures keep mind power multi-instance representation exists ability capturing structure information. however zhou indicated previous studies multi-instance learning typically treated instances bags independently identically distributed; neglects fact relations among instances convey important structure information. considering image task again treating diﬀerent image parts inter-correlated samples evidently meaningful treating unrelated samples. actually instances rarely independent better performance expected instances treated non-i.i.d. exploits relations among instances. paper propose multi-instance learning methods treat instances i.i.d. samples. basic idea regard entity processed whole regard instances inter-correlated components entity. experiments show proposed methods achieve performances highly competitive state-of-the-art multi-instance learning methods. rest paper organized follows. brieﬂy review related work section propose methods section report experiments section conclude paper ﬁnally section many multi-instance learning methods developed past decade. name diverse density k-nearest neighbor algorithm citation-knn decision trees relic miti neural networks bp-mip rbf-mip rule learning algorithm ripper-mi ensemble algorithms miboosting milboosting logistic regression algorithm mi-lr etc. kernel methods multi-instance learning studied many researchers. g¨artner deﬁned mi-kernel regarding feature vectors applying kernel directly. andrews proposed mi-svm mi-svm. mi-svm tries identify maximal margin hyperplane instances subject constraints least instance positive locates positive half-space instances negative bags locate negative half-space; mi-svm tries identify maximal margin hyperplane bags regarding margin most positive instance margin bag. cheung kwok argued sign instead value margin positive instance important. deﬁned loss function allowed bags well instances participate optimization process used well-formed constrained concave-convex procedure perform optimization. later kwok cheung designed marginalized multi-instance kernels incorporating generative model kernel design. chen wang proposed ddsvm method employed diverse density learn instance prototypes maps bags feature space based instance prototypes. zhou proposed misssvm method regarding instances negative bags labeled examples positive bags unlabeled examples positive constraints. wang proposed ppmm kernel representing aggregate posteriors mixture model derived based unlabeled data. addition classiﬁcation multi-instance regression also studied diﬀerent versions generalized multi-instance learning deﬁned main diﬀerence standard multi-instance learning generalized multi-instance learning standard multi-instance learning single concept positive instance satisﬁes concept; generalized multi-instance learning multiple concepts positive concepts satisﬁed recently research multiinstance semi-supervised learning multi-instance active learning multi-instance multi-label learning also reported. paper mainly work standard multi-instance learning show methods also applicable multi-instance regression. actually also possible extend proposal variants multi-instance learning. zhou indicated instances treated i.i.d. samples paper provides solution. basic idea regard every entity processed whole. alternative ways realize idea paper work regarding graph. mcgovern jensen taken multi-instance learning tool handle relational data instance given graph. here working propositional data natural graph. contrast instances graphs regard every graph instance node graph. section propose migraph migraph methods. migraph method explicitly maps every undirected graph uses graph kernel distinguish positive negative bags. migraph method implicitly constructs graphs deriving aﬃnity matrices deﬁnes eﬃcient graph kernel considering clique information. presenting details give formal deﬁnition multi-instance learning following. denote instance space. given data xini} called label goal generate learner classify unseen bags. instance xijl value attribute number training bags number instances number attributes. exists positive instance positive thus otherwise concrete value index unknown. ﬁrst explain intuition proposed methods. here three example images shown fig. illustration. simplicity show marked patches ﬁgure assume image corresponds patch corresponds instance marked patches color similar instances treated independent samples fig. abstracted fig. typical taken previous multi-instance learning studies obviously three bags similar since contain identical number similar instances. however consider relations among instances ﬁrst bags blue marks close third blue marks scatter among orange marks thus ﬁrst bags similar third bag. case fig. abstracted fig. evident abstraction fig. desirable fig. essential that relation structures bags belonging class relatively similar belonging diﬀerent classes relatively dissimilar. describe migraph method. ﬁrst step construct graph bag. inspired shows ǫ-graph helpful discovering underlying manifold structure data establish ǫ-graph every bag. process quite straightforward. regard every instance node. then compute distance every pair nodes e.g. xiv. distance smaller pre-set threshold edge established nodes weight edge expresses aﬃnity nodes many distance measures used compute distances. according manifold property i.e. small local area approximately euclidean space euclidean distance establish ǫ-graph. categorical attributes involved complement. detail suppose ﬁrst attributes categorical remaining ones continuous attributes normalized h=j+ xh|)/ measure distance distance values categorical attribute computed mapping training bags graphs many options build classiﬁer. example build k-nearest neighbor classiﬁer employs graph edit distance design graph kernel capture similarity among graphs solve classiﬁcation problems kernel machines svm. migraph method takes second idea graph kernel illustrated fig. brieﬂy measure similarity bags shown left part fig. node kernel incorporate information conveyed nodes edge kernel incorporate information conveyed edges aggregate obtain ﬁnal graph kernel formally deﬁne follows. deﬁne feature vector describing edge. paper edge connecting nodes deﬁne degree node number edges connecting nodes. note normalized dividing total number edges graph corresponding degree node deﬁned similarly. deﬁned wuv/ numerator weight edge connecting xiv; weight edge connecting nodes thus denominator weights connecting xiu. evident conveys information important connection node node xiu. deﬁned similarly node xiv. intuition that edges similar properties ending nodes similar. deﬁned positive deﬁnite kernel used kinds graphs. computational complexity clearly satisﬁes four major properties considered graph kernel deﬁnition design simple next section proposed migraph method quite eﬀective. deﬁciency migraph computational complexity dominated number edges. bags containing instances exist large number edges migraph hard execute. desired method smaller computational tried apply existing graph kernels directly unfortunately results good. page limit comparison diﬀerent graph kernels reported longer version. calculate distance instances derive aﬃnity matrix comparing distances threshold example distance instances smaller element column otherwise. many ways derive paper calculate distances using gaussian distance average distance bag. migraph kernel deﬁned follows. understand intuition helpful consider good graph instances clique regarded belonging concept. cliques generally expensive large graphs viewed eﬃcient soft version clique-based graph kernel following principles evidently satisﬁed evident computational complexity similar multi-instance kernel shown i.e. note multi-instance kernel obtained gaussian distances every pair instances already calculated easy i’s. musk musk elephant tiger. musk contains positive negative bags musk contains positive negative bags three data sets contains positive negative bags. details data sets found compare migraph migraph mi-kernel times -fold cross validation methods gaussian kernel parameters determined cross validation training sets. average test accuracy standard deviations shown table table also shows performance several multi-instance kernel methods including mi-svm mi-svm misssvm ppmm kernel famous diverse density algorithm improvement em-dd results methods except diverse density obtained times -fold cross validation; best results reported literature since obtained diﬀerent studies standard deviations available results reference instead rigorous comparison. best performance data bolded. table shows performance migraph migraph quite good. musk worse ppmm kernel; note results ppmm kernel obtained exhaustive search prohibitive practice musk elephant migraph migraph respectively best second-best algorithms. pairwise t-tests signiﬁcance level indicate migraph signiﬁcantly better mi-kernel data sets except musk signiﬁcant diﬀerence. re-implemented mi-kernel since comparison mi-kernel clearly show whether helpful treat instances non-i.i.d. samples note performance mikernel implementation better reported image categorization successful applications multi-instance learning. data sets -image -image contain twenty categories corel images respectively category images. image regarded rois image regarded instances described nine features. details data sets found experimental routine described data randomly partition images within category half subset training testing. experiment repeated times random splits average results recorded. one-againstone strategy used migraph migraph mi-kernel multi-class task. following style present overall accuracy well conﬁdence intervals table reference table also shows best results multi-instance learning methods reported literature including mi-svm dd-svm kmeans-svm misssvm miles found table image categorization task proposed migraph migraph highly competitive state-of-the-art multi-instance learning methods. particular migraph best performed method. conﬁrms intuition migraph good choice contains instances migraph better contains instances. examining detail results -image found migraph migraph least better mi-kernel categories except african dinosaurs. might fact structure information examples belonging complicated concepts diﬃcult captured simple schemes used migraph migraph using incorrect structure information worse conservatively treating instances i.i.d. samples. alt.atheism comp.graphics comp.os.ms-windows.misc comp.sys.ibm.pc.hardware comp.sys.mac.harware comp.window.x misc.forsale rec.autos rec.motorcycles rec.sport.baseball rec.sport.hockey sci.crypt sci.electronics sci.med sci.space sci.religion.christian talk.politics.guns talk.politics.mideast talk.politics.misc talk.religion.misc twenty text categorization data sets derived newsgroups corpus popularly used text categorization. fifty positive ﬁfty negative bags generated news categories. positive contains posts randomly drawn target category instances randomly uniformly drawn categories. instance post represented tfidf features. data times -fold cross validation migraph return results reasonable time present average accuracy standard deviations migraph mi-kernel table best result data bolded. pairwise t-tests signiﬁcance level indicate that migraph significantly better mi-kernel text categorization data sets. impressive that examining detail results found consider time times -fold cross validation number win/tie/lose migraph versus mi-kernel data sets also compare migraph migraph mi-kernel four multi-instance regression data sets including lj-.. lj-..-s lj-.. lj..-s name lj-r.f.s number relevant features number features number scale factors used relevant features indicate importance features. suﬃx indicates data uses labels near details data sets found perform leave-one-out tests report results table reference table also shows leave-one-out results methods reported literature including diverse density bp-mip rbf-mip table best performance data bolded. evident proposed migraph migraph methods also work well multi-instance regression tasks. previous studies multi-instance learning typically treated instances bags i.i.d. samples neglecting fact instances within extracted object therefore instances rarely i.i.d. intrinsically relations among instances convey important information. paper propose methods treat instances non-i.i.d. way. experiments show proposed methods simple eﬀective performances highly competitive best performing methods several multi-instance classiﬁcation regression tasks. note methods also handle i.i.d. samples using identity matrix. interesting future issue design better graph kernel capture useful structure information multi-instance bags. applying graph edit distance metric learning methods graphs corresponding multi-instance bags also worth trying. success proposed methods also suggests possible improve multi-instance learning methods incorporating mechanisms exploit relations among instances opens promising future direction. moreover possible extend proposal settings generalized multi-instance learning multi-instance semi-supervised learning multi-instance active learning multi-instance multi-label learning etc.", "year": 2008}