{"title": "Effective Building Block Design for Deep Convolutional Neural Networks  using Search", "tag": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "abstract": "Deep learning has shown promising results on many machine learning tasks but DL models are often complex networks with large number of neurons and layers, and recently, complex layer structures known as building blocks. Finding the best deep model requires a combination of finding both the right architecture and the correct set of parameters appropriate for that architecture. In addition, this complexity (in terms of layer types, number of neurons, and number of layers) also present problems with generalization since larger networks are easier to overfit to the data. In this paper, we propose a search framework for finding effective architectural building blocks for convolutional neural networks (CNN). Our approach is much faster at finding models that are close to state-of-the-art in performance. In addition, the models discovered by our approach are also smaller than models discovered by similar techniques. We achieve these twin advantages by designing our search space in such a way that it searches over a reduced set of state-of-the-art building blocks for CNNs including residual block, inception block, inception-residual block, ResNeXt block and many others. We apply this technique to generate models for multiple image datasets and show that these models achieve performance comparable to state-of-the-art (and even surpassing the state-of-the-art in one case). We also show that learned models are transferable between datasets.", "text": "deep learning shown promising results many machine learning tasks models often complex networks large number neurons layers recently complex layer structures known building blocks. finding best deep model requires combination ﬁnding right architecture correct parameters appropriate architecture. addition complexity also present problems generalization since larger networks easier overﬁt data. paper propose search framework ﬁnding effective architectural building blocks convolutional neural networks approach much faster ﬁnding models close state-of-the-art performance. addition models discovered approach also smaller models discovered similar techniques. achieve twin advantages designing search space searches reduced state-of-the-art building blocks cnns including residual block inception block inception-residual block resnext block many others. apply technique generate models multiple image datasets show models achieve performance comparable state-of-the-art also show learned models transferable datasets. deep convolutional neural networks currently produce state-of-the-art accuracy many machine learning tasks including image classiﬁcation. early deep learning architectures alexnet used convolution fully connected and/or pooling operations still provided large improvements classical vision approaches. recent advances ﬁeld improved performance using several complex building blocks involve operations branching resnext blocks) skip connections resnext since operations used branch remains active area research correct building block involves searching possible conﬁgurations branch components. increase search space effectively means addition traditional deep hyperparameters layer size number ﬁlters training model includes searching various combinations involved constructing effective building block. increased complexity corresponds increased training time often means process ﬁnding right architecture conﬁguration remains result extensive search. recently research tackling issue automating architecture discovery process. consider methods falling categories. ﬁrst methods focus discovering entire architecture primary building blocks i.e. convolution layers pooling layers fully connected layers methods focus building architectures afore-mentioned complex blocks involving branching skip connections. goal second methods ﬁnding particular building block repeated many times create deep architecture. approaches techniques reinforcement learning evolutionary algorithms generally used search architecture space. drawback using search techniques computationally expensive. paper consider second approach designing effective architectural building block repeated create deeper network. motivated resnet inception module structure block design includes branching skip connections. resnet includes convolution operations block design inception includes convolution well row/column convolutions xception includes separable depth-wise convolution operations. also several techniques literature combining outputs different branches including concatenation adding summation stochastic afﬁne transformation propose search framework ﬁnding architectural building blocks cnns considering well-known operations branch along combination techniques. random search search space generate building blocks repeat block multiple times create deep network. architectures searched able architecture providing comparable performance respect state-ofart models multiple image recognition datasets including cifar- cifar- svhn fer. approach additional advantage search process much simpler previous approaches need many trials generate architectures comparable performance. finally models discovered approach smaller models discovered architecture discovery methods. deep cnns shown promising results many machine learning tasks including image classiﬁcation. starting alexnet modern resnet many architectural changes improve performance deep cnn. improvements seen among achievements drop error rate imagenet classiﬁcation task previous cnns included convolution layers pooling layers fully connected layers. layers stacked create deeper network. recently different kinds layers depthwise convolution separable convolution dilated convolution introduced. addition stacking layers other skip connections used pass gradients smoothly deeper models. disadvantage advances become difﬁcult select optimize best model practical application. consequence interest recently ﬁnding architectures automatically. zoph recurrent neural networks used generate model descriptions neural network trained recurrent neural network trained reinforcement learning maximize expected accuracy generated architectures validation dataset. predicts ﬁlter height ﬁlter width stride height stride width number ﬁlters layer repeats layer construct cnn. every prediction carried softmax classiﬁer next time-step input. generally process generating architecture stops number layers exceeds certain value. generating architecture neural network architecture built trained validated held-out validation set. validation performance used update policy generate better architectures time. approach involves minimal human intervention depends learn policy scratch. however approach also involves tuning hyperparameters model needs many samples architectures learn good policy. baker metaqnn meta-modeling algorithm based reinforcement learning automatically generates high performing architectures given learning task. learning agent sequentially chooses neural network layers layer types corresponding hyperparameters \u0001-greedy policy reaches termination state. generated architecture trained validated held-out validation set. validation performance used reward update q-learning network generate better architectures time. work extends architecture generation ability layer types still suffers similar drawbacks resulting reinforcement learning requires many samples accurately learn good policy. real approach uses simple evolutionary algorithm automatically discover high performance models. algorithm starts random architecture usually performs poorly progressively improves architecture navigating fairly unrestricted search space. evolved model trained validated held-out validation performance considered individual’s quality ﬁtness score. scheme uses repeated pairwise competitions random individuals. different mutations used reproduction steps child architectures allowed inherit parents’ weights whenever possible. given limited mutation space step still requires signiﬁcant amount computing resources reach optimal solution. miikkulainen codeepneat automated method optimizing deep learning architectures evolution. ﬁrst population minimal complexity neural networks generated many generations structure added incrementally mutation. node represents layer deep network contains table real binary valued hyperparameters. hyperparameters mutated uniform gaussian distribution random bit-ﬂipping respectively. hyperparameters determine type properties layer. evolved model trained validated held-out validation performance considered individual’s quality ﬁtness score. starts relatively complex architecture efﬁcient approaches begin cold start. however zoph nasnet considers learning architectural building block rather learning full architecture. learned building block repeated many times create deep architecture. used generate descriptions building block manner similar models types building blocks learned normal cell reduction cell blocks return size feature maps reduced size feature maps respectively. controller trained using proximal policy optimization generate better architectural building blocks time. approach also generates models transferable best building blocks learning task also showing good performance learning tasks well. however still lacks transferability needs retrained problem. also blocks learned scratch approach requires many trials achieve state-of-the-art performance. suganuma cartesian genetic programming used automatically construct architectures image classiﬁcation task. structure connectivity represented encoding method optimized maximize validation accuracy. architecture deﬁned trained validated held-out validation performance considered quality ﬁtness score architecture. however creates additional hyperparameters need searched best model. paper proposes reinforcement learning search framework action grow network depth layer width based current network architecture preserving functionality. shared encoder network used learn low-dimensional representation given architecture separate actor network generates certain types network transformation actions. steps transformations ﬁnal network along transferred weights trained validated held-out validation set. validation performance used update policy using policy gradient method. mutation either increase network depth layer width efﬁciently search deep architectures. however also limits ability less complex good-enough architecture. negrinho deeparchitect provides framework automatically designing training deep models. proposes extensible modular language allows human expert compactly represent complex search spaces architectures hyperparameters. random search monte carlo tree search sequential model-based optimization used explore residual building block made easier train much deeper neural networks producing state-of-the results. residual block simply adds input block output layer within block described formally input residual block output residual block output residual branch residual block. basic design contains convolution layers along batch normalization and/or rectiﬁed linear unit activation function. training deeper nets building block modiﬁed bottleneck design. contains layers instead layers responsible reducing increasing depth dimension reduce number parameters deeper nets. resnext building block performs operations whose outputs aggregated summation transformations topology. inception-resnet uses inception module skip connections. operations hyperparameters selected carefully achieve better performance imagenet classiﬁcation task. exact conﬁguration module varies throughnetwork. shake-shake residual nets branches residual function outputs branches combined standard summation stochastic afﬁne combination. design search space block throughout network allow operations inside block learned enables take combine functionalities models still limiting extent search space block structure. framework searches structure considered convolutional cell building block rather whole architecture. building block stacked many times create deep cnn. figure shows design building block. residual branch starts convolutional layer reduce feature depth factor respect output feature depth block. unlike resnet contains convolutional layer bottleneck layer create three branches. operation branch selected following operations linear unit activation function. strided operation spatial feature space reduction. case feature reduction convolution stride applied input feature block match dimension residual branch adding them. double number output units case spatial feature size reduction maintain constant hidden state dimension. block repeated times spatial feature size reduction. consider number repetitions number initial convolution ﬁlters hyperparameters. search strategy treat choices branches hyperparameters open many off-the-shelf optimization methods. random search simplest methods hyperparameter optimization compared iterating predeﬁned parameter combinations random search shows good balance exploration exploitation thus better convergence rate. also less sensitive prior assumptions distribution hyperparameters makes robust alternative applied many different problems. addition random search algorithm naively parallelizable dependency historical results. course advanced methods bayesian optimization evolutionary algorithms reinforcement learning applied hyperparameter optimization problem. case exploration exploitation techniques early stages explorative akin random search since approach limited search space uses small number trials random search enough effectively explore space. recent work also conﬁrmed early stage random search clearly outperforms optimization algorithms ﬁrst trials. clear methods e.g. monte carlo tree search sequential model-based optimization better given large uncertainty associated training validation results within ﬁrst hundred trials. target better architecture fewer trials choose random search hyperparameter optimization process work leave methods future investigation. would remiss mention search strategy focused improving search space associated hyperparameters architecture. leaves model parameters learning rate momentum initialization discovered. proposal extend hyperparameters techniques discover scope study. cifar- object recognition dataset training examples test examples categories. among training examples examples used validation. cifar- also object recognition dataset categories. training examples test examples. among training examples examples used validation. svhn digit recognition dataset obtained house numbers google street view images. training examples test examples categories. among training examples examples used validation. additional dataset training svhn dataset. facial expression recognition dataset training validation test examples categories. note test every case never used model selection used ﬁnal model evaluation. experiments used momentum optimizer minimize cross-entropy loss minibatch size initial learning rate decreased every epochs factor momentum maximum number epochs used early stopping validation accuracy stops improving within trials. used weight decay parameter equal training common image preprocessing techniques including pixel mean subtraction random cropping random horizontal ﬂipping experiments selected top- models based validation accuracy report test accuracy best model. also construct ensemble top- mdoels averaging responses. used open source project tensorﬂow carry experiments. randomly searched operations combination types create building block. created block stacked repeatedly assemble deep learning architecture trained. report results architectures searched. figure shows best validation accuracies search trial cifar- dataset. observed iterations performance longer improved. trials enough reasonable performance datasets analysed work. table shows comparison several automatic architecture search methods different datasets along best performance found corresponding literature. results observe approach achieved competitive performance respect number searched architectures model complexity measure numthe actual number architectures searched provided real authors provided wall-clock time. mentioned population size also figure- paper estimated number architectures searched less parameters. cifar- achieved better results methods except zoph zoph number searches number model parameters much larger approach. cifar- dataset method achieved better results architecture search methods. dataset achieves state-of-the classiﬁcation accuracy. svhn dataset approach achieves competitive performance compared methods fewer model parameters. figure visualizes results different architecture search strategies cifar- dataset. approaches start basic building blocks hard learn effective block replicate deep network short amount time. size data point ﬁgure represents model complexity measured number weight parameters. model discovered approach smaller still maintaining good prediction performance. design search strategy limits number branching options number blocks enforces repeatable blocks assemble architecture. effect constrain search small networks repeatable blocks instead arbitrarily deep networks unconstrained layer sizes. relatively small size models also computational beneﬁts. models require fewer resources trained faster architecture discovered extensive search. although smaller model representative complex ones still able achieve comparable performance. finally models smaller also generalizable exhibit transferability. designed deep using best building block cifar- experiment trained network cifar- dataset achieving error rate gains reducing training time model complexity enough offset reduction performance effective improve results construct ensembles searched models. even though models share similar performance experiments ensemble prediction improved accuracy. building blocks used models searched cifar dataset. analysis allows understand different block components contribute result provides hints future design. instance block concat operations present models form ensemble absent potentially showing good choice part architecture building block. also conv ﬁlter size slightly favored problem settings. however analysis preliminary number searches limited draw conclusions. addition search iterations optimization algorithms leverage relations hyperparameters outperform random search method used paper. important advance improves deep learning performance building blocks small branching/spanning convolution blocks pooling batch normalization layers repeated construct deep architectures. however ﬁnding effective building block task essentially adds another parameters already rich hyperparameter space. ever increasing search space hyperparameters means effective architecture design often result extensive search combined deep expertise allows experienced modelers restrict search certain promising combinations parameters. much stretch effective architecture design partly rather completely science. even playing ﬁeld architecture design allow architectures learned rather explicitly designed. paper shown posing design building block search problem limited building block components effective generating cnns image recognition. takeaway approach simple choices often effective comfigure histogram building blocks searched deep architectures. blue bars searched architectures orange bars models form ensemble. black lines expected occurrences models randomly selected. plicated approaches architecture design added advantage generating smaller models better generalizability. limit search space small components chosen existing designs limit number building blocks four random search explore space. choices simplify search space greatly reduce search time. remarkable technique discovered architectures cases better performance existing architecture search approaches perform close state-of-the-art always trade-off terms performance cost believe cases cost important factor approach provide solutions comparable performance. remain many avenues future work. interested expanding technique non-vision datasets model architectures including recurrent networks even reinforcement learning methodologies. addition also interested better understanding much transfer possible blocks learned random search. finally still work understand exactly simpliﬁed approach captures architecture design allows effective. understanding could", "year": 2018}