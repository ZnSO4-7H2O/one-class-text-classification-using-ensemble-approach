{"title": "Multi-Task Feature Learning Via Efficient l2,1-Norm Minimization", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "The problem of joint feature selection across a group of related tasks has applications in many areas including biomedical informatics and computer vision. We consider the l2,1-norm regularized regression model for joint feature selection from multiple tasks, which can be derived in the probabilistic framework by assuming a suitable prior from the exponential family. One appealing feature of the l2,1-norm regularization is that it encourages multiple predictors to share similar sparsity patterns. However, the resulting optimization problem is challenging to solve due to the non-smoothness of the l2,1-norm regularization. In this paper, we propose to accelerate the computation by reformulating it as two equivalent smooth convex optimization problems which are then solved via the Nesterov's method-an optimal first-order black-box method for smooth convex optimization. A key building block in solving the reformulations is the Euclidean projection. We show that the Euclidean projection for the first reformulation can be analytically computed, while the Euclidean projection for the second one can be computed in linear time. Empirical evaluations on several data sets verify the efficiency of the proposed algorithms.", "text": "problem joint feature selection across group related tasks applications many areas including biomedical informatics computer vision. consider -norm regularized regression model joint feature selection multiple tasks derived probabilistic framework assuming suitable prior exponential family. appealing feature -norm regularization encourages multiple predictors share similar sparsity patterns. however resulting optimization problem challenging solve non-smoothness -norm regularization. paper propose accelerate computation reformulating equivalent smooth convex optimization problems solved nesterov’s method—an optimal ﬁrst-order black-box method smooth convex optimization. building block solving reformulations euclidean projection. show euclidean projection ﬁrst reformulation analytically computed euclidean projection second computed linear time. empirical evaluations several data sets verify efﬁciency proposed algorithms. multi-task learning recently received increasing attention machine learning artiﬁcial intelligence computer vision. aims learn shared information among related tasks improved performance successfully employed applications including medical diagnosis handwritten character recognition conjoint analysis text classiﬁcation underlying assumption behind many multi-task learning algorithms tasks related other. thus issue capture task relatedness take account learning formulation existing multi-task learning formulations employ various strategies capture task relatedness. evgeniou pontil assumed tasks related true models close given common model. proposed solve multi-task models -norm regularization analogous svms obozinski proposed penalize -norms blocks coefﬁcients associated feature across tasks leading -norm regularized nonsmooth convex optimization problem. obozinski proposed solve problem blockwise boosting scheme follows regularization path. however known convergence rate blockwise boosting scheme. argyriou assumed tasks share small subset features formulated problem squared -norm regularized non-convex optimization problem. feature matrix learned problem reduces squared -norm regularized convex optimization problem similar proposed xiong proposed probabilistic framework imposing automatic relevance determination prior parameters across different tasks. zhang proposed uniﬁed probabilistic framework task parameters share common structure latent variables. multi-task feature learning -norm regularization studied appealing property -norm regularization encourages multiple predictors different tasks share similar parameter sparsity patterns. -norm regularization also successfully employed group lasso logistic group-lasso generalized linear models loss convex -norm regularized problem convex admits globally optimal solution. previous approaches solving paper propose solve -norm regularized problem using ﬁrst-order black-box methods evaluate iteration function value gradient only. major challenge lies nonsmoothness -norm regularization term. shown lower complexity bound smooth convex optimization signiﬁcantly better nonsmooth convex optimization. moreover nesterov’s method optimal ﬁrst-order black-box method smooth convex optimization. superior convergence rate smooth convex optimization nonsmooth propose reformulate non-smooth -norm regularized problem equivalent smooth convex optimization problem. speciﬁcally consider smooth reformulations paper reformulation introduction additional variables; -ball constrained optimization. applying nesterov’s method solving reformulations building block euclidean projection onto constraint. show euclidean projection ﬁrst reformulation computed analytically projection second computed linear time. equivalent reformulations solve -norm regularization problem smooth loss function time complexity denote total number training samples sample dimensionality number tasks desired accuracy respectively. empirical evaluations several data sets verify efﬁciency proposed algorithms. organization section introduce -norm regularized multi-task feature learning probabilistic framework; section present proposed method solving equivalent smooth reformulations; section detail efﬁciently solve euclidean projection problems; section report empirical results; conclude paper section lation amtfl following theorem theorem loss smooth convex loss function. problem equivalent following constrained smooth convex optimization problem number tasks equals prior reduces laplace prior distribution easy show case problem reduces -norm regularized optimization problem. particular problem reduces lasso multiple tasks weights corresponding i-th feature grouped together -norm thus -norm regularization tends select features based strength input variables tasks jointly rather strength individual input variables case single task learning objective function nonsmooth since known subgradient method solving problem requires iterations achieving accuracy signiﬁcantly larger required nesterov’s method solving thus propose solve equivalent smooth convex reformulations second reformulation also derive equivalent smooth reformulation moving nonsmooth -norm term constraint. results following equivalent reformulation amtfl theorem loss smooth convex loss function. problem equivalent following -ball constrained smooth convex optimization problem remark reformulation amtfl difﬁcult give analytical relationship however relationship critical cases optimal values unknown usually tuned using cross-validation. follows theorems solve equivalent reformulations eqs. simplicity presentation focus solving general problem constrained smooth convex optimization problem. propose employ nesterov’s method solving recall nesterov’s method much faster convergence rate traditional methods subgradient descent gradient descent. speciﬁcally nesterov’s method convergence rate gradient descent subgradient descent convergence rates respectively denotes number iterations. logistic loss costs ﬂoating point operations evaluating function value gradient objective function iteration. moreover shall shown section euclidean projections onto analytically computed time complexity therefore equivalent reformulation solve -norm regularized problem time complexity denote total number training samples sample dimensionality number tasks desired accuracy respectively. following similar analysis time complexity solving reformulation also sequence {xi} {si} illustrated figure computed recursively according eqs. optimal solution obtained. algorithm solving nesterov’s method given algorithm deﬁnition show following theorem dual optimal point known primal optimal point computed analytically. theorem dual optimal point known primal optimal point given figure illustration euclidean projection onto domain one-dimensional case. convenience illustration assume onedimensional notations respectively. domain corresponds golden region i.e. point projected following three different regions euclidean projection interpreted geometrical perspective shown figure denote point projected lies region case euclidean projection onto observed domain four illustrating points figure lies region case euclidean projection onto domain still already resides domain lies region case nearest point origin figure illustration proposed algorithms solving -norm regularized least squares problem plots ﬁrst second correspond results school letter data sets respectively; amtfl amtfl correspond reformulations respectively. slightly different optimization problem discussed paper. comparison note difﬁcult possible carry fair comparison among different methods issue implementation choice algorithm parameters different stopping criterion thus results interpreted caution. conduct relatively fair comparison given ﬁrst mtl-feat obtain solution apply algorithms proposed section solving mtl-feat following settings epsilon init= iterations= method=. proposed algorithms terminate program achieve objective function value smaller equal mtl-feat term school data test efﬁciency under different values regularization parameter report results table letter data test efﬁciency varying number training samples given report results table observe tables proposed algorithms much efﬁcient mtl-feat theorems suggest following procedure solving first compute dual optimal either zero unique root analytically obtain employ improved bisection computing root analyze time complexity projection folcosts ﬂops compute lows. ﬂops compute root improved bisection ﬂops obtaining therefore overall time complexity solving scales linearly size rn×k projected. section demonstrate efﬁciency proposed algorithms using following data sets school letter school data consists scores students secondary schools london years sample containing attributes. school data tasks predicting student performance school. letter data collected kassel spoken language systems group. data consists default tasks two-class classiﬁcation problems handwritten letters h/n. writings collected different writers convergence analysis solve -norm regularized least squares regression problem proposed algorithms section report results data sets figure horizontal axis denotes number iterations vertical axis denotes objective function value. reformulation amtfl report results school data set; letter data set. reformulation amtfl solution obtained amtfl. observe form ﬁgures objective function values reformulations decrease rapidly ﬁrst iterations; function values reformulations become stable iterations speciﬁc data sets. conﬁrms fast convergence rate prosposed algorithms nesterov’s method. comparison competing algorithms demonstrate efﬁciency proposed algorithms section compare gradient descent solves reformulation also compare proposed algorithms mtl-feat solving relatively large solution sparse. thus value amtfl comparable amtfl. this together efﬁcient computation euclidean projection amtfl explains amtfl competitive amtfl relatively large. relatively small solution less sparse. case value amtfl much larger amtfl. explains amtfl less efﬁcient amtfl case. computing sequence solutions applications optimal values unknown. common approach estimating valies compute solutions corresponding sequence values parameter optimal chosen evaluating certain criteria. done simply applying proposed algorithms solving independent problems however efﬁcient approach so-called warm-start uses solution previous problem warm-start latter. indeed amtl amtl beneﬁt warm-start technique since solution corresponding lies feasible domain conduct experiments school data report results figure horizontal axis denotes sequence values parameter vertical axis denotes number iterations computing solution given cold-start warmstart. cold-start problems solved independently; warm-start solution corresponding employed initialization solution corresponding results ﬁgure observe warm-start signiﬁcantly outperform code-start. paper study problem joint feature selection across group related tasks. consider multi-task feature learning formulation based norm regularization encourages multiple predictors share similar sparsity patterns. resulting optimization problem however challenging solve amtfl versus amtfl compare reformulations amtfl amtfl solving -norm regularized least squares regression. given regularization parameter ﬁrst amtfl obtain solution program terminated relative adjacent approximate solutions less amtfl program terminated achieves objective function value smaller equal amtfl terms conduct experiments school data different values regularization parameter report results table observe table that relatively large amtfl comparable amtfl; relatively small amtfl efﬁcient amtfl. similar phenomenon observed table next analyze underlying difference reformulations based parameters involved theorem lipschitz gradient loss identical loss thus reforρ mulations lipschitz gradient additional variable amtfl usually larger amtfl; iii) discussion section k.-a. sohn xing. multivariate regression approach association analysis quantitative trait network. technical report cmu-ml-- carnegie mellon university roth fischer. group-lasso generalized linear models uniqueness solutions efﬁcient algorithms. international conference machine learning pages figure solving sequence solutions amtfl amtfl cold-start warm-start cold-start problems solved independently; warm-start solution corresponding employed initialization solution corresponding non-smoothness -norm. accelerate computation propose ﬁrstly reformulate equivalent smooth convex optimization problems solve reformulations nesterov’s method. solving reformulations building block euclidean projection. show euclidean projection ﬁrst reformulation analytically computed projection second computed linear time. empirical evaluations several data sets demonstarte efﬁciency proposed algorithms. plan improve practical performance proposed algorithms using adaptive line search proposed apply proposed algorithms solving -norm regularized problems including group lasso logistic group-lasso group-lasso generalized linear models also plan compare proposed algorithms coordinate gradient descent method locally linear convergence rate certain conditions finally plan apply proposed algorithms real-world applications including association analysis quantitative trait network image classiﬁcation brain-computer interfacing biomarker identiﬁcation", "year": 2012}