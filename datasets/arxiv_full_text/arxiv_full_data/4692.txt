{"title": "Structure and Parameter Learning for Causal Independence and Causal  Interaction Models", "tag": ["cs.AI", "cs.LG"], "abstract": "This paper discusses causal independence models and a generalization of these models called causal interaction models. Causal interaction models are models that have independent mechanisms where a mechanism can have several causes. In addition to introducing several particular types of causal interaction models, we show how we can apply the Bayesian approach to learning causal interaction models obtaining approximate posterior distributions for the models and obtain MAP and ML estimates for the parameters. We illustrate the approach with a simulation study of learning model posteriors.", "text": "begin discussing causal independence models generalize models causal interaction models causal interaction models independent mech anisms mechanisms several causes addition introducing several particular types causal interaction show apply bayesian approach learning causal interaction obtaining approximate posterior distribu tions models obtain estimates parameters illus trate approach simulation study learning model posteriors models causal independence\u0001 noisyor good pearl noisymax henrion proved useful proba bilistic assessment pearl henrion heck erman breese addition easier sessment techniques performing infer ence eciently models causal independence heckerman breese zhang poole techniques eciently calculate upper lower bounds likelihoods exact inference intractable jaakkola jordan essen tial idea causal independence models causes lead eect independent mech anisms type model assumed needs separately assess probability distri butions describes mechanism give rule combining results mechanisms hand using full probability tables repre sent conditional distribution eect given part paper introduces causal interac tion models like causal independence model causal interaction model mechanisms causes eect unlike causal indepen dence model cause need associated single mechanism multiple causes associ ated single mechanism allowing several causes associated single mechanism allows partial causal interaction causes thus causal interaction models generalize causal independence model complete causal interac tion model section show represent causal interaction models directed acyclic graphi models bayesian networks belief networks causal networks hidden variables addition introduce special type causal interac tion model exponential causal interaction model examples exponential causal interaction models given section second part paper turn attention representation learning structure rameters exponential causal interaction models much initial work learning discrete models focus learning structure network assuming full conditional probabil tables variable network ditional probability table variable represented conditional probability variable given possible combination values parents model structure representation number parameters associated variable exponential number parents variable exponential explosion restrict work structures learned methods methods boukaert part limitations interest learning models parsimonious representations conditional probability variables given parents instance friedman goldszmidt chickering authors sider using decision trees generalization cision trees represent conditional probability variable given parents representations local structure allows dramatic reductions dimension parameter space causal interaction models provide alternative representation local structure model illustrate fact noisymaxinteraction models parsimoniously represented decision trees decision trees types local struc tures embedded causal interaction models thus causal interaction models rich models parsimoniously representing local structure since causal interaction models models hidden variables hidden variables treme case missing data discuss learning models missing data section also discuss algorithm obtain estimates hidden variable models finally section illustrate fact learn structure causal interaction models small simulation study addition section illustrates importance correctly calculating dimension hidden variable models learning structure constructing parameterized models must specify conditional probability vari able given possible conguration parents figure shows variable several parents causes often feasible specify complete probability table represent required probabili ties number probabilities grows expo nentially number parents addition several authors argued model inaccurate cause fails represent independence causal interactions overcome inadequacies researchers used models shown represent causal independence good pearl henrion srinivas shall call causes fect noisy mechanism variables noisy mechanism variable represents contri bution mechanism eect value deterministic function indicated double circle graph values mechanism variables independence causal mechanisms captured conditional inde pendence mechanism variables given causes causal interaction model relaxes restrictions cause unique mechanism variable mechanism variable unique cause figure shows example causal interaction model causal interaction model possible model rela tionships causes interact cause eect causes independently example interactions often found medicine instance studies smoking estrogen level found synergistic eect rate stroke females reason stop modeling causal process level mechanism described conditional distri bution given parent could modeled decision tree model additional hidden variables roughly mechanism describes path causes lead eect mechanism causes eect nodes observed hidden distinguished variable called noisy mechanism variable simply mechanism variable members mechanism causes point members nodes form directed acyclic graph variable points nonmember mechanism variable points figure illustrates exam mechanism note cause point multiple nodes mechanism causal interaction model roughly model mechanisms describes conditional distri bution eect given causes precisely causal interaction model causes eect variable mecha nisms eect denote value eect variable deterministic function mechanism variables call combination function domain eect variable ordered nary relation likelihood mechanism variable given values parents ponential family combination function maxx\u0001 note eect mech anism variables need discrete follows combination function model cause mechanism variable generalization noisy noisymax models models noisymax models without distinguished state absent normal course create noisymaxinteraction model distinguished states simply distinguishing parent conguration mechanism variable forcing associated parameters clearly param eters reducing number free parameters model benet models without distin guished states easier learn case know distinguished states mechanism variables additional learning problem namely need iden tify parent congurations distinguished states course know parent congu ration distinguished state force parameter restrictions algorithm calculate estimate parameters approximate posteriors models special case consider discrete model discrete random variable necessarily nite mechanism contains mechanism variable kjpaxi paxi parents thus pkxi instantiation causes mechanism variable discussed section algorithm need calculate paxi jipxi indicator function variables mechanisms causes eect case causal independence models independence causal mechanisms captured conditional independence variables mechanism given causes independent given independence tween mechanism variables variables model given causes eect common leak term noisyor noisymax models leak term added model mechanisms associated variables model leak term corresponds mechanism vari able thus mechanism causes model finally exponential causal interaction model causal interaction model conditional like lihood variable mechanism exponential family section discuss vari specic exponential causal interaction models focus exponential causal interaction models cause models often tractable algorithms inference tractable models inference apply algorithm xjpaxi expij mechanism contains mechanism variable called rate parameter poisson case conditional rate parameter parameter mechanism variable parents mechanism variable state instantiation parents mechanism variable using theorem independent random variables poisson tributions parameters distributed poisson random variable parameter characterize poisson model poisson random variables useful analyzing rates number page hits week number headaches week thus poisson model potential modeling conditional rates even cases causes rate interact noisymaxinteraction models given mech anism variable need unique parameter instantiation parents mechanism variable rather decision trees deci sion graphs reduce number parameters needed specifying conditional distribution mech anism variable interesting feature poisson model possible inference using clique tree type inference algorithm despite fact clique potentials innite trick form clique potentials value known value known bound values thus bound size clique tential instantiation causes mech anism variable discussed section algorithm need calculate paxi jipxi indicator function equation mechanism variables kjpaxi inferences mechanism variables analogous apply algorithm conditional tribution pxij distributed according poisson multinomial distribution addition need unique conditional distribution instantiation parents mechanism variable rather decision tree cision graph reduce number conditional tributions thus reduce number parameters needed specifying conditional distribution mechanism variable even done conditional distribution function poisson tribution since conditional distribution mech anism variable represented decision tree model least representationally rich decision trees noisyor model special case model binary causes binary eect param eters however almost values param eters lebesgue measure zero full probability table complete decision tree must used represent distribution exactly thus causal interaction models provide rich representa tion modeling conditional distributions causal teraction models viewed alternative decision trees decision graphs parsimonious representations however since decision trees graphs embedded causal interaction models strictly richer representation caveat shall section must iter ative methods approximating several quantities interest using causal interaction models suitable assumptions case decision trees decision graphs finally noisyor noisymax models leak term model mechanisms sociated variables model discussed section leaks however extra degrees freedom model compared noisymax somewhat like leak term noisymax model noisyadditiveinteraction model causal interaction model which mechanism sists single mechanism variable main subset domain eect vari able domain eect variable closed addition likelihood mechanism variable given values parents ponential family combination function used inference generally independence mechanisms causal interac tion model lead computational eciencies infer ence because clique tree conditional nodes dierent mechanism connected paths mechanism variables point illustrated conditional clique tree figure addition allowing nested structure causal teraction models also allow types combi nation functions instance combination function combination function binary eect variable equal nary mechanism variables equal clearly generalized handle continuous variables using additive threshold combination function capture threshholding eects causal teraction model another combination function parity combination function nary eect variable equal even number binary mechanism variables equal causal interaction models combination function causes jointly independent lead parameterized version pseudoindependence model xiang section investigate learn rameters structure exponential causal teraction models section show algorithm dempster compute estimate parameters tion investigate asymptotic approximations marginal likelihood particular cheeseman stutz approximation write causal interaction model model particular means assume true physical joint probability distribution variables model encoded model section variables model mechanism variables causal inter action model write models gaussian model causal interaction model conditional distribution mechanism variables gaussian including discrete nite state hidden variable inside mecha nism possible conditional distribution mechanism variables mixtures gaus sians types models conditional distribution gaussian poisson inference dicult models fairly simple structure figure illustrates causal interaction model complicated nested structure causal interaction model layer mechanism nodes followed deterministic combi nation function expanded version model figure illustrates conditional distribu tion mechanism nodes given parent causes nested structure case mecha nisms associated mechanism variables nested causal interaction models mechanism associated mechanism variable nested hidden variable important note values observed variables interaction model dseparated variables model variables interaction model thus inference localized interaction model might think inference thus using would computationally hopeless expanded version model figure complicated causal interaction models always case expanded version model interaction structure conditional forms polytree thus polynomialtime algorithm pearl approximation based observation that sample size increases eect prior psjs diminishes thus approximate maximum maximum likelihood conguration class techniques nding gradientbased optimization example gradient ascent follow derivatives likelihood pdjs local maxi russell thiesson show compute derivatives likelihood bayesian network unrestricted multinomial tributions buntine discusses eral case likelihood function comes exponential family course gradientbased methods local maxima another technique nding local expectationmaximization algorithm dempster local begin assigning conguration somehow random next compute expected cient statistics complete data expec tation taken respect joint distribution conditioned assigned conguration known data discrete example compute possibly incomplete case variables observed case term case requires trivial compu tation either zero otherwise bayesian network inference algorithm evaluate term computation called expectation step algorithm next expected sucient statistics actual sucient statistics complete sample calculation determine conguration maxi mize pdcjs discrete example probability distribution refer element case finally prior probabil density function psjs parameters model problem learning probabilities bayesian network stated simply given random sample compute posterior distribution psjd refer conditional distribution pxijpai local conditional distribution function section illustrate algorithm case local distribution function lection multinomial distributions distribution conguration namely assume conguration also maximizes psjd known maximum posteriori congu ration also dene conguration maximizes pdjs conguration known maximum likelihood conguration case causal interaction models need compute posterior given incomplete data unlike completedata case need approximation techniques details instance heck erman techniques include monte carlo approaches gibbs sampling importance sampling neal madigan raftery asymptotic approximations kass sequential updating methods spiegelhalter ritzen cowell asymptotic approximations based servation that number cases increases posterior parameters distributed accord multivariategaussian distribution continue cases gaussian peak come sharper tending delta function conguration limit conguration approximate distribution equation desirable properties computes marginal likelihood punishes model complexity complete albeit imaginary data computation criterion ecient dimension model given data region around sthat number parameters increases dierence tween pd\u0000j increase also discussed either case equation asymptotically correct simple modication equation addresses problems equation without correction dimension proposed cheeseman stutz scoring criterion autoclass algorithm data clustering shall refer equation cheesemanstutz scoring criterion note scoring crireria given equation equation applied compute marginal like lihood complete data given model obtain estimate buntine shows pute marginal likelihood complete data given model local likelihoods exponential family algorithm obtain estimate section describe small simulation study highlights important features approach described section struc ture models used simulation study given figure variables assignment called maximization step algorithm dempster showed that certain regularity conditions iteration pectation maximization steps converge local maximum algorithm typically plied sucient statistics exist local distribution functions exponential family although generalizations used complicated local distributions saul step bayesian approach learning graph ical models computation marginal likeli hood data given model pdjs given plete data setthat data sample contains observations every variable model marginal likelihood computed exactly eciently certain assumptions cooper skovits contrast observations missing including situations variables hidden never observed exact determination marginal likelihood typically intractable conse quently approximation techniques puting marginal likelihood exponential causal interaction models section focus attentions asymptotic approximation called cheesemanstutz approxi mation simulation study described section chosen simulation study cause computational performance features chickering heckerman discussion approximations experimental results computing asymptotic approximations must determine dimension model dimension model interpreted equivalent ways first number free param eters needed represent parameter space near maximum likelihood value second rank jacobian matrix transformation parameters network parameters observable nonhidden variables either case dimension depends value space simulation study mathematical software pack calculate rank jacobian matrix transformation parameters network parameters observable variables details motivation geiger binary conditional distributions given complete probability tables model could also represented without deterministic combination function complete probability table given model chose parameter values rameters used parameterized model generating model generate dataset \u0006\u0004\u0000\u0000 cases parameter values chosen hand ever similar results would expected parameters chosen random approximated model teriors using adjusted cheesemanstutz score dierent sized initial segments \u0006\u0004\u0000\u0000 cases dimension models calculated using mathe matica techniques described geiger although done study easy automatically generate equations mathemat calculate dimension thus automate calculation dimension results simula tion study summarized figure model poste riors presented initial segments size \u0001\u0006\u0000\u0000 cases surprisingly mass continues accumulate generating model sample size increases exception model generating model reason behavior posterior generating model distributions rameterized strict subset distributions parameterized surprisingly dimension models identical unusual relationship occurs binary finally would like draw attention impor tance using correct dimension calculating bayesian approximation posterior adjusted dimension model number parameters model including parameters hidden variables table describes dimension models used simulation study consider models clearly every distribu tion represented represented asymptotic proximation used unadjusted dimension then least asymptotically would impossible choose little work done parameter learning causal interaction models notable exception work neal neal showed could learn parameters noisyor network using local learning rule however particular gradientascent procedure must constrained avoid entering invalid region parameter space since using guaranteed stay within valid region parameter space guaranteed local maximum plan investigating representational power causal interaction models compared local structures decision graphs compare ease assessment various models addition consider automating learning causal interaction models dening search space search oper ators compare result algorithm approaches learning local structure also interest best combine search local structure search global structure", "year": 2013}