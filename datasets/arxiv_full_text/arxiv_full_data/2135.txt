{"title": "Safe Exploration in Finite Markov Decision Processes with Gaussian  Processes", "tag": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "abstract": "In classical reinforcement learning, when exploring an environment, agents accept arbitrary short term loss for long term gain. This is infeasible for safety critical applications, such as robotics, where even a single unsafe action may cause system failure. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an, a priori unknown, safety constraint that depends on states and actions. We aim to explore the MDP under this constraint, assuming that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm for this task and prove that it is able to completely explore the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover.", "text": "classical reinforcement learning agents accept arbitrary short term loss long term gain exploring environment. infeasible safety critical applications robotics even single unsafe action cause system failure harm environment. paper address problem safely exploring ﬁnite markov decision processes deﬁne safety terms priori unknown safety constraint depends states actions satisﬁes certain regularity conditions expressed gaussian process prior. develop novel algorithm safemdp task prove completely explores safely reachable part without violating safety constraint. achieve this cautiously explores safe states actions order gain statistical conﬁdence safety unvisited state-action pairs noisy observations collected navigating environment. moreover algorithm explicitly considers reachability exploring ensuring stuck state safe out. demonstrate method digital terrain models task exploring unknown rover. today’s robots required operate variable often unknown environments. traditional solution specify potential scenarios robot encounter operation priori. time consuming even infeasible. consequence robots need able learn adapt unknown environments autonomously exploration algorithms known safety still open problem development systems fact learning algorithms allow robots make unsafe decisions exploration. damage platform environment. paper provide solution problem develop algorithm enables agents safely autonomously explore unknown environments. speciﬁcally consider problem exploring markov decision process priori unknown state-action pairs safe. algorithm cautiously explores environment without taking actions unsafe render exploring agent stuck. related work. safe exploration open problem reinforcement learning community several deﬁnitions safety proposed risk-sensitive reinforcement learning goal maximize expected return worst case scenario however approaches minimize risk treat safety hard constraint. example geibel wysotzki deﬁne risk probability driving system previously known undesirable states. main difference approach assume undesirable states known priori. garcia fernández propose ensure safety means backup policy; policy known safe advance. approach different since require backup policy initially safe states agent starts explore. another approach makes backup policy shown hans safety deﬁned terms minimum reward learned data. conference neural information processing systems barcelona spain. moldovan abbeel provide probabilistic safety guarantees every time step optimizing ergodic policies; policies agent recover visited state. approach needs solve large linear program every time step computationally demanding even small state spaces. nevertheless idea ergodicity also plays important role method. control community safety mostly considered terms stability constraint satisfaction controlled systems. akametalu reachability analysis ensure stability assumption bounded disturbances. work uses robust control techniques order ensure robust stability model uncertainties uncertain model improved. another ﬁeld recently considered safety bayesian optimization there order global optimum priori unknown function regularity assumptions form gaussian process prior made. corresponding posterior distribution unknown function used guide evaluations informative locations. setting safety centered approaches include work schreiter goal safely reachable optimum without violating priori unknown safety constraint evaluation. achieve this function cautiously explored starting points known safe initially. method applied ﬁeld robotics safely optimize controller parameters quadrotor vehicle however considered bandit setting iteration played. contrast consider exploring introduces restrictions terms reachability considered bayesian optimization before. contribution. introduce safemdp novel algorithm safe exploration mdps. model safety priori unknown constraint depends state-action pairs. starting initial states actions known satisfy safety constraint algorithm exploits regularity assumptions constraint function order determine nearby unvisited states safe. leads safe exploration state-actions pairs known fulﬁl safety constraint evaluated. main contribution consists extending work safe bayesian optimization bandit setting deterministic ﬁnite mdps. order achieve this explicitly consider safety constraint also reachability properties induced dynamics. provide full theoretical analysis algorithm. provably enjoys similar safety guarantees terms ergodicity discussed reduced computational cost. reason method separates safety reachability properties mdp. beyond this prove safemdp able fully explore safely reachable region without getting stuck violating safety constraint high probability. best knokwledge ﬁrst full exploration result mdps subject safety constraint. validate method exploration task rover explore priori unknown map. section deﬁne problem assumptions. unknown environment modeled ﬁnite deterministic tuple ﬁnite states state-dependent actions known deterministic transition model reward function typical reinforcement learning framework goal maximize cumulative reward. paper consider problem safely exploring mdp. thus instead aiming maximize cumulative rewards deﬁne priori unknown safety feature. although unknown make regularity assumptions make problem tractable. traversing discrete time step agent decide action thereby state visit next. assume underlying system safety-critical visited state-action pair unknown associated safety feature must safety threshold assumption deterministic dynamics hold general mdps framework uncertainty environment captured safety feature. requested agent obtain noisy measurements safety feature taking action state index used index measurements denotes movement steps. typically hopeless achieve goal safe exploration unless agent starts safe location. hence assume agent stays initial state action pairs known safe priori. goal identify maximum safely reachable region starting without visiting unsafe states. clarity exposition assume safety depends states only; provide extension safety features also depend actions fig. assumptions reward function ensuring visited states safe without prior knowledge safety feature impossible task however many practical safety features exhibit regularity similar states lead similar values following assume endowed positive deﬁnite kernel function function bounded norm associated reproducing kernel hilbert space norm induced inner product rkhs indicates smoothness functions respect kernel. assumption allows model k)). probability distribution functions fully speciﬁed mean function covariance function randomness expressed distribution captures uncertainty environment. assume without loss generality. posterior distribution computed analytically based measurements states measurements ωt]t corrupted zero-mean gaussian noise posterior distribution mean ktt−yt variance covariance ktt−kt) positive deﬁnite kernel matrix )]ss∈dt. identity matrix denoted rt×t. also assume l-lipschitz continuity safety function respect metric guaranteed many commonly used kernels high probability goal section deﬁne goal safe exploration. particular best algorithm hope achieve since observe noisy measurements impossible know underlying safety function exactly ﬁnite number measurements. instead consider algorithms knowledge statistical conﬁdence based conﬁdence within safe states small distance classiﬁed satisfy safety constraint using lipschitz continuity resulting safe states contains states classiﬁed safe given information states considers safety constraint consider restrictions place structure mdp. particular able visit every state rsafe without visiting unsafe state ﬁrst. result agent restricted rreach states reached starting safe step. states called one-step safely reachable states. however even restricted agent still stuck state without safe actions. deﬁne states able return states step. particular care ability return certain safe states therefore called one-step safely returnable states. general return routes require taking action fig. n-step returnability operator rret rret considers longer return routes repeatedly applying return rret operator rret times. limit contains states reach arbitrarily long path safe exploration mdps requirements; state want visit needs safe reachable must able return safe states state. thus algorithm aims safely explore allowed visit states intersection three safety-relevant sets. given safe fulﬁlls safety requirements states return visiting states classiﬁed safety threshold. including deﬁnition avoid agent getting stuck state without action leads another safe state take. given knowledge safety feature accuracy thus allows expand safe ergodic states algorithm goal exploring state space consequently explore newly available safe states gain knowledge safety feature potentially enlargen safe set. safe expansions found repeatedly applying operator ultimately size safe bounded surrounding unsafe states number states result biggest algorithm classify safe without visiting unsafe states given taking limit limn→∞ thus given tolerance level initial safe seed states algorithm hope classify safe. denote states algorithm determines safe iteration following refer complete safe exploration whenever algorithm fulﬁlls limt→∞ algorithm classiﬁes every safely reachable state accuracy safe without misclassiﬁcation visiting unsafe states. start giving high level overview method. safemdp algorithm relies model make predictions safety feature uses predictive uncertainty guide safe exploration. order guarantee safety maintains sets. ﬁrst contains states classiﬁed satisfying safety constraint using posterior second additionally considers ability reach points ability safely return previous safe ˆst−. algorithm ensures safety ergodicity visiting states ˆst. order expand safe region algorithm visits states candidate states that visited could expand safe set. speciﬁcally algorithm selects uncertain state safe state gain information about. move state shortest safe path guaranteed exist algorithm summarized algorithm initialization. algorithm relies initial safe starting point explore mdp. states must safe; must also fulﬁll reachability returnability requirements sec. consequently states must exist path connects them seem restrictive requirement example fulﬁlled single state action leads back state. classiﬁcation. order safely explore algorithm must determine states safe without visiting them. regularity assumptions introduced sec. allow model safety feature uncertainty estimate model order determine conﬁdence interval within true safety function lies high probability. positive scalar determines amplitude interval. discuss select sec. rather deﬁning high probability bounds values directly terms consider intersection sets iteration safe states otherwise. choice ensures states classify safe shrink iterations justiﬁed selection sec. based conﬁdence intervals deﬁne lower bound upper bound values safety features likely take based data obtained iteration based lower bounds deﬁne states fulﬁll safety constraint high probability using lipschitz constant generalize beyond current safe set. based classiﬁcation ergodic safe states states achieve safety threshold additionally fulﬁll reachability returnability properties discussed sec. expanders. safe states deﬁned task algorithm identify explore states might expand states classiﬁed safe. uncertainty estimate order deﬁne optimistic expanders optimistic measurement equal upper conﬁdence bound would allow determine previously unsafe state indeed value safety threshold. intuitively sampling might lead expansion thereby ˆst. explicitly considers expansion safe exploration goal fig. graphical illustration set. sampling shortest safe path. remaining part algorithm concerned selecting safe states evaluate ﬁnding safe path leads towards them. goal visit states allow safe expand quickly possible waste resources exploring mdp. posterior uncertainty states order make choice. iteration select next target sample state highest variance argmaxs∈gt choice justiﬁed points safe potentially enlarge safe based noisy sample gain information state uncertain about. design choice maximizes knowledge acquired every sample lead long paths measurements within safe region. given dijkstra’s algorithm within order shortest safe path target current state st−. since require reachability returnability safe states path guaranteed exist. terminate algorithm reach desired accuracy; argmaxs∈gt action-dependent safety. considered safety features depend states general safety also depend actions section introduce modiﬁed captures dependencies without modifying algorithm. modiﬁed equivalent original terms dynamics however introduce additional action-states action original mdp. start state take action ﬁrst transition corresponding action-state transition deterministically. model illustrated fig. safety features depend action-states equivalent action-dependent safety features. safemdp algorithm used modiﬁed without modiﬁcation. experiments sec. example. safety exploration aspects algorithm presented previous section rely correctness conﬁdence intervals particular require true value safety feature lies within high probability iterations furthermore conﬁdence intervals shrink sufﬁciently fast time. probability taking values within conﬁdence intervals depends scaling factor scaling factor trades conservativeness exploration probability unsafe states visited. appropriate selection studied srinivas multi-armed bandit setting. even though framework different setting applied case. choose bound rkhs norm function probability visiting unsafe states maximum mutual information gained noisy observations; max|a|≤t information capacity sublinear dependence many commonly used kernels choice justiﬁed following lemma follows noise zero-mean conditioned history lemma assume well uniformly bounded chosen then holds probability least lemma states that safety function takes values within conﬁdence intervals high probability. show safe shortest path problem always solution lemma assume states then using algorithm assumptions theorem states lemma states that given initial safe fulﬁlls initialization requirements always policy drives state state without leaving safe states lemmas role ensuring safety exploration thus main theoretical result theorem assume l-lipschitz continuous assumptions lemma hold. also assume states choose then probability least along state trajectory induced algorithm transition function moreover smallest integer log. exists that probability least theorem states algorithm performs safe complete exploration state space; explores maximum reachable safe without visiting unsafe states. moreover desired accuracy probability failure safely reachable region found within ﬁnite number observations. bound depends information capacity turn depends kernel. safety feature allowed change rapidly across states information capacity larger safety feature smooth. intuitively less prior knowledge kernel encodes careful exploring requires measurements. section demonstrate algorithm exploration task. consider setting exploration surface mars rover. code experiments available http//github.com/befelix/safemdp. space exploration communication delays rover operator earth prohibitive. thus important robot autonomously explore environment without risking unsafe behavior. experiment consider mars science laboratory rover deployed mars. communication delays travel meters obtain instructions operator. climb maximum slope experiments digital terrain models surface mars high resolution imaging science experiment resolution meter opposed experiments considered subsample smoothen data order achieve good exploration results. ﬂexibility framework considers noisy measurements. therefore every state represents square area opposed every state agent take four actions down left right. rover attempts climb slope steeper fails damaged. otherwise moves deterministically desired neighboring state. setting deﬁne safety state transitions using extension introduced fig. safety feature transition deﬁned terms height difference states given maximum slope rover climb safety threshold conservative tan. encodes unsafe robot climb hills steep. particular dynamics assume mars every state reached safety constraint depends priori unknown heights. therefore prior belief unknown transitions safe. model height distribution matérn kernel limitation grid resolution tuning hyperparameters necessary achieve safety satisfactory exploration results. ﬁner resolution cautious hyperparameters would also able generalize neighbouring states. lengthscales prior standard deviation heights assume noise standard deviation since safety feature state transition linear combination heights model heights induces model differences heights classify whether state transitions fulﬁll safety constraint. particular safety depends direction travel going downhill possible going uphill might unsafe. following recommendations experiments conﬁdence intervals directly determine safe result lipschitz constant used determine expanders guaranteeing safe exploration high probability multiple steps leads conservative behavior every step beyond known safe decreases ‘probability budget’ failure. order demonstrate safety achieved empirically using less conservative parameters suggested theorem constant value choice aims guarantee safety iteration rather jointly iterations. assumption used compare algorithm several baselines. ﬁrst considers safety threshold ergodicity requirements neglects expanders. setting agent samples uncertain safe state transaction corresponds safe bayesian optimization framework expect exploration safe less efﬁcient approach. second baseline considers safety threshold consider ergodicity requirements. setting expect rover’s behavior fulﬁll safety constraint never attempt climb steep slopes stuck states without safe actions. third method uses unconstrained bayesian optimization framework order explore states without safety requirements. setting agent tries obtain measurements uncertain state transition entire space rather restricting safe set. case rover easily stuck also incur failures attempting climb steep slopes. last consider random exploration strategy similar \u0001-greedy exploration strategies widely used reinforcement learning. figure comparison different exploration schemes. background color shows real altitude terrain. algorithms iterations ﬁrst unsafe action attempted. saturated color indicates region strategy able explore. baselines stuck crater bottom-right corner fail explore algorithm manages safely explore unknown environment. statistics fig. compare baselines meters area latitude longitude. accuracy σnβ. resulting exploration behaviors seen fig. rover starts center-top part plot relatively planar area. top-right corner hill rover cannot climb bottom-right corner crater that entered rover cannot leave. safe behavior expect explore planar area without moving crater attempting climb hill. algorithms iterations ﬁrst unsafe action attempted. seen fig. method explores safe area surrounds crater without attempting move inside. state-action pairs closer crater also safe model would require data classify safe necessary conﬁdence. contrast baselines perform signiﬁcantly worse. baseline ensure ability return safe seen fig. explore area quickly reaches state without safe path next target sample. approach avoids situations explicitly. unsafe exploration baseline fig. considers ergodicity concludes every state reachable according model. consequently follows path crosses boundary crater eventually evaluates unsafe action. overall enough consider ergodicity safety order solve safe exploration problem. random exploration fig. attempts unsafe action exploration. contrast algorithm manages safely explore large part unknown environment. running algorithm without considering expanders leads behavior fig. safe manages explore small subset safely reachable area within number iterations algorithm explores results summarized table presented safemdp algorithm safely explore priori unknown environments. used gaussian process model safety constraints allows algorithm reason safety state-action pairs visiting them. important aspect algorithm considers transition dynamics order ensure safe return route visiting states. proved algorithm capable exploring full safely reachable region measurements demonstrated practicality performance experiments. acknowledgement. research partially supported planck center learning systems snsf grant felix berkenkamp angela schoellig andreas krause. safe controller optimization quadrotors gaussian processes. proc. ieee international conference robotics automation alexander hans daniel schneegaß anton maximilian schäfer steffen udluft. safe exploration reinforcement learning. proc. european symposium artiﬁcial neural networks pages alfred mcewen eric eliason james bergstrom nathan bridges candice hansen alan delamere john grant virginia gulick kenneth herkenhoff laszlo keszthelyi randolph kirk michael mellon steven squyres nicolas thomas catherine weitz. mars reconnaissance orbiter’s high resolution imaging science experiment journal geophysical research planets jens schreiter nguyen-tuong mona eberts bastian bischoff heiner markert marc toussaint. safe exploration active learning gaussian processes. proc. european conference machine learning volume pages niranjan srinivas andreas krause sham kakade matthias seeger. gaussian process optimization bandit setting regret experimental design. proc. international conference machine learning proof. rret rret taking action. repeating procedure times system reaches state rret actions. particular choose prove agent reaches actions. therefore sequence actions length inducing state trajectory that rret every consider means rret case therefore rret rret know rret. similarly rret rret apply reasoning times prove rret lemma rret proof. direct consequence lemma fact lemma states belongs rret path length starting contained drives system state since dealing ﬁnite different states. therefore path exists cannot longer |s|. lemma given holds last inequality follows lemma implies equivalently furthermore know initialization rreach. moreover since conclude induction step assume ˆst− ˆst. then proof. result follows repeatedly applying lemma lemma assume noise zero-mean conditioned history well uniformly bounded chosen then holds probability least µt−| safety lemma proof. recursive argument prove lemma. since know lemmas know inducing ˆst−. similarly build another sequence actions drives system state ˆst− passing starting ˆst−. applying repeatedly procedure build ﬁnite sequence actions drives system state passing starting lemmas equivalent lemma proof. proof analogous gave lemma difference need reachability property instead recovery property ˆst. lemma assume states then using algorithm assumptions theorem states proof. lemma direct consequence properties listed lemmas lemma following holds probability least proof. let’s prove result induction. initialization know induction step assume holds deﬁnition exists ˆst− proof. since changing always computing enlargement function points. therefore need prove enlargement function increasing. known lemma increasing function furthermore know lemma hence enlargement function increasing proof complete. proof. sake contradiction assume implies ˆst. hand since included sets whose intersection deﬁnes know that implies apply repeatedly step reachability operator sides equality obtain ˆst. lemmas know implies st+tt contradiction. thus st+tt. want focus recovery reachability properties order reach contradiction ˆst+tt. since ˆst+tt know that equation lemma lemma st+tt. hence fact st+tt allow conclude ˆst+tt. contradiction proves theorem. lemma probability least proof. proof induction. know deﬁnition. induction step assume holds ˆst− goal show order this show know that means rsafe hence ˆst−). result lemma allow together leads conclusion assumed induction step ˆst− applying sides operator conclude proves induction step complete. lemma smallest integer |r|tt∗ exists that probability least holds ˆst+tt ˆst. proof. sake contradiction assume opposite holds true ˆst+tt. implies ˆst. furthermore know increasing therefore ˆstt∗ know that theorem smallest integer then probability least proof. lemma know ˆst+tt probability least implies probability least lemma therefore ˆst. furthermore know probability least lemma completes proof. main result theorem assume l-lipschitz continuous assumptions lemma hold. also assume states choose then probability least along state trajectory induced algorithm transition function moreover smallest integer log. exists that probability least", "year": 2016}