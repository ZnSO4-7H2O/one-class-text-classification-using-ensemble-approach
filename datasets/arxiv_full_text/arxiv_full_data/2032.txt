{"title": "Bayesian and L1 Approaches to Sparse Unsupervised Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The use of L1 regularisation for sparse learning has generated immense research interest, with successful application in such diverse areas as signal acquisition, image coding, genomics and collaborative filtering. While existing work highlights the many advantages of L1 methods, in this paper we find that L1 regularisation often dramatically underperforms in terms of predictive performance when compared with other methods for inferring sparsity. We focus on unsupervised latent variable models, and develop L1 minimising factor models, Bayesian variants of \"L1\", and Bayesian models with a stronger L0-like sparsity induced through spike-and-slab distributions. These spike-and-slab Bayesian factor models encourage sparsity while accounting for uncertainty in a principled manner and avoiding unnecessary shrinkage of non-zero values. We demonstrate on a number of data sets that in practice spike-and-slab Bayesian methods outperform L1 minimisation, even on a computational budget. We thus highlight the need to re-assess the wide use of L1 methods in sparsity-reliant applications, particularly when we care about generalising to previously unseen data, and provide an alternative that, over many varying conditions, provides improved generalisation performance.", "text": "regularisation sparse learning generated immense research interest many successful applications diverse areas signal acquisition image coding genomics collaborative ﬁltering. existing work highlights many advantages methods paper regularisation often dramatically under-performs terms predictive performance compared methods inferring sparsity. focus unsupervised latent variable models develop minimising factor models bayesian variants bayesian models stronger l-like sparsity induced spike-and-slab distributions. spikeand-slab bayesian factor models encourage sparsity accounting uncertainty principled manner avoid unnecessary shrinkage non-zero values. demonstrate number data sets practice spike-and-slab bayesian methods outperform minimisation even computational budget. thus highlight need re-assess wide methods sparsity-reliant applications particularly care generalising previously unseen data provide alternative that many varying conditions provides improved generalisation performance. last decade tremendous excitement learning parsimonious models using sparsity. sparse learning signiﬁcant research topic signiﬁcance tied theoretical practical advancement sparse learning methods using norm. norm penalised regression problems lasso natural scene understanding image coding problems recently compressed sensing served cement importance eﬃcacy norm means inducing sparsity. among important properties norm closest convex norm norm number provable properties relating optimality solutions oracle properties allows wide array tools convex optimisation used computing sparse solutions. sparse methods increasingly diverse application domains timely contextualise norm critically evaluate behaviour relation competing methods. achieve sparsity idealised intractable sparsity criterion uses norm penalise number non-zero parameters. closely match objective function develop discrete mixture priors sparse learning commonly referred spike-and-slab priors spike-and-slab discrete mixture point mass zero continuous distribution similar norm imposes penalty number non-zero parameters model. show spike-and-slab distributions provide improvements learning bayesian methods analysis focuses unsupervised linear latent variable models class models amongst core tools machine learning practitioner’s toolbox. factor analysis inspiration class models describes real-valued data underlying factors linearly combined explain observed data. base model allows many adaptations generalisations non-gaussian data learning sparse underlying factors unsupervised learning sparse representation desirable situations where many underlying factors could explain data subset explain data subset diﬀerent observation. framework unsupervised models develop approaches sparse bayesian learning culminating thorough comparative analysis. contributions include introduce generalised latent variable models strong sparsity providing important class sparse models readily handle nongaussian heterogeneous data sets develop spike-and-slab model sparse unsupervised learning derive full mcmc algorithm mcmc method applicable models based discrete-continuous mixtures eﬃcient naive samplers present ﬁrst comparison approaches sparse unsupervised learning based optimisation methods bayesian methods using continuous sparsity-favouring priors bayesian methods using spike-and-slab. bring methods together compare performance controlled manner benchmark real world data sets across breadth model types interestingly results show strong sparsity spike-and-slab models outperform commonly used methods unsupervised modelling tasks. weights combined explain observed data often consider gaussian latent variables gaussian noise diagonal isotropic covariances case model recovers familiar factor analysis principal components analysis models respectively. sparse subsets underlying factors explain data diﬀerent subsets explain observed data point. increasingly deal real-data well described gaussian distribution data binary categorical non-negative heterogeneous these. interesting consider generalisations basic model conditional probability observed data deﬁned using exponential family distributions family distributions sent exponential natural parameters vnθ. model natural parameters parameters weighted points latent subspace corresponding data point exponential family distributions conditional probability given parameter vector takes suﬃcient statistics vector natural parameters log-partition function. probability distributions belong exponential family also natural conjugate prior distributions model distribution parameters notation conj shorthand conjugate distribution figure graphical representation general unsupervised models; shaded node represents observed data item plate notation represents replication variables dashed node represents appropriate prior distribution latent variables observed data forms discrete-continuous mixture point mass zero referred ‘spike’ distribution known ‘slab’. slab distribution often uniform gaussian distribution appropriate distribution. since positive mass zero samples produced include exact zeroes thereby enforcing strong sparsity. spike-and-slab also seen placing penalty number non-zero parameters thus enforces sparsity manner similar norm penalisation. mcmc allows stochastically suitable solutions setting possible otherwise combinatorial nature optimisation. construct spike-and-slab prior using binary indicator matrix indicate whether latent dimension contributes explaining observed data not. observed data point corresponding vector bernoulli indicator variables spike components combined gaussian distribution forms slab component represents gaussian density mean place beta prior variance bernoulli parameters equation becomes δ-function zero indicating spike chosen instead slab. complete model speciﬁcation using gaussian-gamma prior unknown mean variance denote unknown variables inferred hyperparameters mcmc sampling scheme since spike-and-slab diﬀerentiable many popular mcmc techniques hybrid monte carlo applicable. proceed context metropolis-within-gibbs sampling sequentially sample unknown variables using metropolis-hastings. sampling procedure iterates following steps sample jointly; sample slice sampling sample gibbs sampling. variable hyperparameters exponential family gaussian gaussian latent variables recover factor analysis; general exponential families corresponds well known exponential considering non-gaussian latent variables instantiates models relevance vector machine unsupervised models sparsity obtained employing sparsity-favouring distributions. sparsity-favouring distribution distribution high excess kurtosis indicating highly peaked heavy tails distribution delta-mass zero. sparsity-favouring distributions includes normal-gamma normal inverse-gaussian laplace exponential generally class scale-mixtures gaussian distributions distributions encourage sparsity fall classes continuous sparsity-favouring spike-andslab distributions give rise notions weak strong sparsity respectively weak sparsity. parameter vector considered ‘weakly sparse’ none elements exactly zero elements close zero large entries. implies weakly sparse vector small norm small entries decay absolute value according power strong sparsity. parameter vector considered ‘strongly sparse’ elements exactly zero. spike-and-slab prior places mass explicitly zero thus prior suited achieving notion sparsity learning. bayesian approach learning averages model parameters variables according posterior probability distribution given data rather searching single best parameter setting optimisation approach. obtain bayesian models strong sparsity spike-and-slab prior latent variable sampled spike slab component. variables associated slab components sampled using slice sampling. evaluating probabilities involves computing following integrals computing equation easy integral equation tractable general. case gaussian family marginalised exactly this. families integral must approximated. number approximation methods exist monte carlo integration importance sampling pseudo-marginal approaches laplace approximation here. laplace’s method introduces bias approximation target distribution. problem studied guihenneuc-jouyaux rousseau laplace approximation used mcmc schemes latent variables case show approach behave well. guihenneuc-jouyaux rousseau show number observations increases approximate distribution becomes close true distribution describe number assumptions hold requiring diﬀerentiability positive deﬁnite information matrix conditions behaviour prior boundaries parameter space. least three approaches sampling latent variables considered naive sampling alternating without integrating slab. sampling integrating found collapsed scheme describe quickly informs state slab overall resulted faster mixing. reversible jump mcmc also feasible requires diﬀerent prior speciﬁcation also using binary indicator vector prior number non-zero latent variables sample steps slice sampling thought general version gibbs sampler. sampling proceeds alternately sampling auxiliary variable slice level randomly drawing norm become established mechanism encode sparsity many problems strong connection continuous densities promote sparsity. norm number appealing properties gives closest convex optimisation problem problem; broad theoretical basis provable properties negative likelihood obtained using equation equation provides unifying framework sparse models regularisation. regularisation parameters control sparsity latent variables degree parameters penalised learning. function regulariser model parameters model speciﬁed generally applicable wide choice including norm. loss function described previously focus unsupervised settings specify loss generally allowing sparse activations well basis functions. conﬁguration consider body related work broad work described exhaustive attempts capture many papers relevance contextualising approaches applications sparse learning. wide body literature sparse learning problems feature selection compressed sensing regression using norm tibshirani d’aspremont candes bayesian methods sparse regression problems using continuous distributions also discussed seeger. carvalho o’hara sillanp¨aa wipf nagarajan derive relationship automatic relevance determination maximum likelihood iterative optimization. archambeau bach provide nice exploration ard-related priors variational sparse sparse cca. realvalued data sparse variants wide body literature matrix factorisation also indirectly related methods deal exponential family generalisation yield sparse factors by-product rather construction. also many papers relevance bioinformatics computer vision blind deconvolution methods develop also strong bearing basis pursuit problem widely used geophysics engineering ﬁelds allow solution basis pursuit also obtaining useful estimates uncertainty. problems which rely extensive literature regarding norm minimisation. number methods exist solve problems recast equivalent inequality constrained optimisation problems solved using modiﬁed lars algorithm recast second order cone program solved using number smooth approximations regularisation term amongst others. sparse bayesian learning continuous densities high excess kurtosis zero-mean laplace distribution student’s-t distribution often used bayesian models sparsity desired. model priors prefer sparsity bayesian averaging process often results non-sparse posteriors give solutions nearly zero resulting weakly sparse models. consider models sparsity latent variables model described equation speciﬁed. equivalence model model seen comparing log-joint probability using laplace distribution loss equation refer bayesian inference laplace model lxpca contrast model optimisation-based method. exponential model. parameters latent variables positively constrained natural choice would exponential distribution peaked distributions popular sparse regression problems natural candidates unsupervised models explored here. hierarchical model speciﬁcation completed placing gamma prior unknown rate parameters shared shape scale parameters respectively. denote unknown variables inferred inference model accomplished using markov chain monte carlo methods joint probability central sampling. sampling approach based hybrid monte carlo implemented easily defer algorithmic details mackay sparsity variable selection established statistics mitchell beauchamp recently ishwaran describes majorisation–minimisation algorithm estimation l¨ucke sheikh describe gaussian sparse coding. carvalho spike-and-slab-type priors introduce sparsity bayesian factor regression models. consider hierarchical sparsity prior reduce uncertainty whether parameter non-zero. comes increased computation necessarily improve performance. courville describe spike-and-slab deep belief networks. inference. create test sets randomly selecting elements data matrix. test elements missing values training data learning algorithms designed cases handle missing data. calculate predictive probability root mean squared error using testing data. created data sets diﬀerent missing data provide mean standard deviation error bars evaluation metrics. fairness regularisation parameters section chosen cross-validation using validation data chosen data elements. independent data aside training testing data. block images data synthetic benchmark data set. data consists binary images image represented -dimensional vector. images generated four latent features type block. observed data combination number latent features. noise added ﬂipping bits images probability data consists number latent factors subset contributes explaining single data point. data synthetic generated models tested. tested performance signiﬁcantly better this. optimisation-based bayesian learning approaches well spike-and-slab model shows best performance smaller error bars. patches extracted larger images. gaussian instantiation sparse generalised model evaluate performance optimisation; laplace-prior factor model; bayesian spike-and-slab model. results shown underdetermined overcomplete bases ﬁgure methods perform similarly low-rank approximation cases model becomes overcomplete bayesian methods perform better spike-and-slab method much better methods particularly reconstructing held-out/missing data. animal attributes data kemp tenenbaum consists animal species ecological biological properties features. binary unsupervised model show results various latent dimensions rmse ﬁgure data random classiﬁer bits models values much lower this. also subset popular newsgroups data consisting documents counts words used document data sparsity figure shows performance poisson unsupervised model using spike-and-slab. apart application model count data results show spike-and-slab model able deal eﬀectively sparse data provides eﬀective reconstructions good predictive performance held data. also able show improved behaviour spike-and-slab model using hapmap data set. comparative performance shown ﬁgure showing spike-and-slab performance similar terms rmse much better performance large figure comparison predictive probabilities ‘s&s ﬁxed’ time-matched spike-and-slab performance num. non-zeros newsgroups reconstruction true number optimisation methods cross-validation procedure needed regularisation parameters computationally demanding need execute optimisation many combinations parameters. approach also wasteful data since separate validation data needed make sensible choices values avoid model overﬁtting. individual optimisations quick overall procedure take extended time depends granularity grid regularisation values searched for. parameters learnt bayesian setting advantage obtain information distribution latent variables rather point estimates signiﬁcantly better performance. figure demonstrates tradeoﬀ running time performance optimisation bayesian approaches. allowed convergence spike-and-slab iterations. instance bayesian method seemingly slower produced signiﬁcantly better reconstructions human judgements newsgroups data. considered setting ﬁxed time budget ﬁxed running time spike-and-slab used model result shown ﬁgure shows even ﬁxed time budget mcmc performs better setting. table ﬁgure shows number non-zeroes reconstructions various results showed spike-and-slab approach better performance methods compared model class. models based norm bayesian models continuous sparsity favouring priors enforce global shrinkage parameters model. property induces sparsity property also results shrinkage parameters relevance data. problematic certain cases newsgroups data resulted overly sparse data reconstructions. spike-and-slab ability give global local shrinkage thus allowing sparsity model parameters restricting parameters contribute explaining data. current approaches sparse learning difﬁculty scaling large data sets regime. might think potential solution used hern´andez lobato restricted regression problems. standard gaussian model rattray discuss issue propose hybrid vb-ep approach achieving fast inference approach ideal leaving scope future work. proaches. particular bayesian learning spikeand-slab priors consistently showed best performance held data produced accurate reconstructions even ‘large paradigm restricted running times. considering broad family unsupervised latent variable models developed sparse generalised model provided sampling methods sparse bayesian learning using spike-and-slab distribution. importantly provided ﬁrst comparison sparse unsupervised learning using three approaches optimisation using norm bayesian learning using continuous sparsity favouring priors bayesian learning using spike-and-slab prior. also demonstrated methods diverse applications including text modelling image coding psychology showing ﬂexibility sparse models developed. results show bayesian sparsity spike-and-slab methods warrant prominent role wider sparse modelling applications.", "year": 2011}