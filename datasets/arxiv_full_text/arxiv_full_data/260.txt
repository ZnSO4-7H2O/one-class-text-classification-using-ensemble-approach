{"title": "Variable Computation in Recurrent Neural Networks", "tag": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "abstract": "Recurrent neural networks (RNNs) have been used extensively and with increasing success to model various types of sequential data. Much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data, such as long range dependency or localized attention phenomena. However, while many sequential data (such as video, speech or language) can have highly variable information flow, most recurrent models still consume input features at a constant rate and perform a constant number of computations per time step, which can be detrimental to both speed and model capacity. In this paper, we explore a modification to existing recurrent units which allows them to learn to vary the amount of computation they perform at each step, without prior knowledge of the sequence's time structure. We show experimentally that not only do our models require fewer operations, they also lead to better performance overall on evaluation tasks.", "text": "recurrent neural networks used extensively increasing success model various types sequential data. much progress achieved devising recurrent units architectures ﬂexibility capture complex statistics data long range dependency localized attention phenomena. however many sequential data highly variable information recurrent models still consume input features constant rate perform constant number computations time step detrimental speed model capacity. paper explore modiﬁcation existing recurrent units allows learn vary amount computation perform step without prior knowledge sequence’s time structure. show experimentally models require fewer operations also lead better performance overall evaluation tasks. class recurrent neural network models particularly well suited dealing sequential data successfully applied diverse array tasks language modeling speech recognition machine translation acoustic modeling among others. factors instrumental allowing paradigm widely adopted give rise aforementioned successes. hand recent advances hardware software signiﬁcant role bringing training recurrent models tractable time periods. hand novel units architectures allowed recurrent networks model certain features sequential data better elman’s simple architecture include developments lstm units easily learn model long range interactions attention mechanisms allow model focus speciﬁc part history making prediction work focus another feature recurrent networks ability efﬁciently model processes happening different possibly varying time scales. existing recurrent models take approaches regarding amount computation require. either computational load constant time follows ﬁxed schedule latter approach proven especially useful dealing sequences reﬂect processes taking place different levconsider sequential data video feeds audio signal language. video data time periods frames differ slightly underlying model probably much less computation scene completely changes. modeling speech audio signal also reasonable expect model able little computation silences. finally case character level language modeling computational power word boundaries certainly help reading left context prime. model able higher likelihood sequence characters make word minister. however take idea step further reading prime min. next characters almost deterministic model require little computation predict sequence i-s-t-e-r. work show modify commonly used recurrent unit architectures namely elman gated recurrent unit obtain variable computation counterparts. gives rise architecture variable computation variable computation take advantage phenomena deciding time step much computation required based current hidden state input. show models learn time patterns interest perform fewer operations even take advantage time structures produce better predictions constant computation versions. start giving overview related work section provide background class recurrent neural networks section describe model learning procedure section present experimental results music well character level language modeling section finally section concludes lays possible directions future work. properly handle sequences reﬂect processes happening different time scales widely explored question. among proposed approaches variety notable systems based hidden markov models forward last decades. factorial model parallel interacting hidden states model concurrent processes. explicit handling different time scales model achieves good held-out likelihood bach chorales exhibit multi-scale behaviors. hierarchical model takes direct approach representing multiple scales processes. works higher level recursively call sub-hmms generate short sequences withchanging state authors show successful application modeling cursive writing. finally switching state-space model combines hmms linear dynamical systems model used switch parameters experiments show learns higher-level slower dynamics lds. side recurrent neural networks idea models mechanisms allow handle processes happening different time scales either. hand early works already presented level architecture automatizer acting every time step chunker called automatizer fails predict next item author hypothesizes learns model slower scale processes. hand model proposed slow-moving units well regular ones slowness deﬁned parameter deciding fast representation changes taking convex combination previous predicted hidden state. notions along different approaches multi-scale sequence modeling developed recent work. expand upon idea slow moving units proposing extension elman unit forces parts transition matrix close identity. idea recurrent layers called different time steps also recently regained popularity. clockwork example layers called every etc. time steps. conditional takes another approach using known temporal structure data character level level language modeling application ﬁrst layer called every character second called word. also noted state-of-the results language models obtained using multi-layer rnns higher layers theory model slower processes. however introspection models challenging difﬁcult determine whether actually exhibiting signiﬁcant temporal behaviors. finally even recent efforts considered using dynamic time schedules. presents multi-layer lstm layer decides whether activate next every time step. show model able learn sensible time behaviors achieve good perplexity chosen tasks. another implementation general concept adaptive timedependent computation presented work amount computation performed time step varied calling units several layers rather unique perform update hidden state single time step. model shown learn intuitive time schedule. paper present alternative view adaptive computation single variable computation unit decides dynamically much hidden state needs change leading savings number operations time step possibility higher dimensions hidden state keep longer term memory. start formally deﬁning class recurrent neural networks tasks language modeling interested deﬁning probability distribution sequences using chain rule negative likelihood sequence written ﬁltration function summarizes relevant information past. rnns class models read sequences arbitrary length provide summary form hidden state applying operation time step. speciﬁcally recurrent unit deﬁned recurrence function takes input previous hidden state time step well representation input outputs hidden state elman unit. unit described often considered standard unit. parametrized square d-dimensional transition matrices uses sigmoid non-linearity obtain hidden state elman unit bulk computation comes matrix multiplications cost time step following section show simple modiﬁcation unit allows reduce cost signiﬁcantly. gated recurrent unit. gated recurrent unit introduced main difference elman unit consists model’s ability interpolate proposed hidden state current makes easier model longer range dependencies. speciﬁcally time step model computes reset gate update gate proposed hidden state ﬁnal hidden state follows figure time steps vcu. step scheduler takes current hidden vector input vector decides number dimensions unit uses ﬁrst dimensions compute ﬁrst elements hidden state carries remaining dimensions ht−. noted previous section bulk computation aforementioned settings comes linear layers; natural option reduce number operations would apply linear transformations sub-set hidden dimensions. could theory correspond sub-set indices however want setting computational cost choice much less cost computing hidden state. thus consider sets ﬁrst dimensions single parameter compute. variable computation units implement idea using modules scheduler decides many dimensions need updated current time step performs partial update hidden state accordingly illustrated figure section formally describes scheduler partial update operations section outlines procedure jointly learn modules. scheduler. model ﬁrst needs decide much computation required current time step. make decision recurrent unit access current hidden state input; model learn ignore uninformative input decide computation unexpected given current hidden state. scheduler deﬁned function decides portion hidden state change based current hidden input vectors. work decide implement simple log-linear function parameter vectors bias time step have partial update. scheduler decided computation budget needs perform partial update ﬁrst dimensions hidden state. recall hidden state d-dimensional vector. given smaller dimension partial update hidden state would take following form. d-dimensional version model’s recurrence function deﬁned equation uses upper left square sub-matrices denote ﬁrst elements linear transformations apply carry dimensions previous hidden state hidden state deﬁned soft mask. practice transition function deﬁned would require making hard choice time step number dimensions updated makes model non-differentiable signiﬁcantly complicate optimization. instead approximate hard choice using gate function apply soft mask. given sharpness parameter gating vector deﬁned thres\u0001 maps values greater smaller respectively. model performs update using ﬁrst dimensions hidden state goes increases leaves last dimensions unchanged. thus recurrence function deﬁned equation have computational cost model step deﬁned number multiplications involvd). construction vcrnn vcgru possibly non-zero elements penalty values greater target simply diverge cost function deﬁned equation becomes secondly model able explore effect using fewer dimensions need start training smooth mask since small values model actually uses whole hidden state. gradually increase sharpness parameter model truly partial update. experiments variable computation variants elman gated recurrent units several sequence modeling tasks. experiments using symmetrical penalty scheduler penalizing greater smaller target taking various values range experiments start sharpness parameter increase epoch maximum value experiments interested investigating speciﬁc aspects model. hand time patterns emerge agree intuition time dynamics expressed data? hand variable computation unit yield good predictive model? speciﬁcally lead lower perplexity constant computation counterpart performs many operations? order able properly assess efﬁciency model since know priori much computation uses always report equivalent dimension along performance test data i.e. dimension elman would performed amount computation. note computational complexity gains refer exclusively terms lowering number operations necessarily correlate speed training using general purpose kernels; however prerequisite achieving speed proper implementation motivating effort. answer questions tasks music modeling character level language modeling penn treebank text character level language modeling text data well languages europarl corpus. downloaded corpus irish traditional tunes https//thesession.org split training validation test melodies respectively. sub-set includes variations melodies melody variations across subsets. consider pair different symbol; rests symbols comes total vocabulary symbols. table compares perplexity test elman rnns equivalent computational costs vcrnn hidden dimension achieves better perplexity fewer operations dimension looking output scheduler validation also reveals interesting patterns. first symbols mostly ignored average value symbols opposed others. surprising pre-processing handle polyphony time signatures bars different lengths. best thing model ignore focus melody. similarly model spends lest computation rests pays less attention repeated notes also notice model needs computation fast passages often richer ornamentation illustrated table difﬁcult think priori sorts behaviors could interest initial results certainly show sensible behavior scheduler music modeling task. also chose apply model tasks level character level language modeling. appeared good applications since know priori kind temporal structure look ascii encoding means expect signiﬁcant change every bits level modeling believe structure word units useful modeling text character level. ﬁrst experiments english language modeling tasks using penn treebank text data sets. chose former well studied corpus corpora people reported bit-level language modeling results. however quite small purposes characters motivated apply models larger text data table shows character results character level language modeling. compare results obtained standard elman lstm networks well conditional table left bits character character level language modeling penn treebank. crnn refers conditional middle bits level language modeling penn treebank. right bits character character level language modeling text. ∗from quantitative results. ﬁrst compare vcrnn regular elman well conditional combines layers running character level level modeling character word level character level modeling. level language modeling vcrnn performs fewer operations standard unit also achieves better performance. character level modeling elman model using hidden dimension achieved bits character best performing vcrnn slightly better requiring much computation dimension elman unit. slightly computation conditional noted model explicitly given word-level information learns summarize character-level input. comparison constant computation variable computation follows pattern text corpora. vcgru best validation perplexity performs well dimension less half number operations. text vcgru models various values target always achieve better perplexity models performing similar greater numbers operations. noted none models text overﬁts signiﬁcantly would indicate gain solely matter regularization. level scheduler. scheduler level language model manages learn structure ascii encoding figure shows higher dimensions modiﬁed roughly every bits. also created artiﬁcial data taking text adding bits character. figure shows model learns mostly ignore buffers computation actual characters. character level scheduler. character level language modeling scheduler learns make word boundaries language structures. figure shows higher dimensions used words cases even observe spike morpheme provide results vcrnn speciﬁcally section vcgru scheduler follows patterns. also model languages form europarl corpus. chose czech larger alphabet languages corpus german language features long composite words without white spaces indicate unit. made characters. tried settings. guide setting penalty encourage model dimensions white spaces. learn setting fully unsupervised encourages lower values across board. figure bits character different computational loads europarl czech german datasets. vcrnn whether guided boundaries fully unsupervised achieves better held-out log-likelihood efﬁciently standard rnn. figure shows perform similarly czech dataset achieving better performance efﬁciently standard rnn. german guided settings remains slightly efﬁcient fully learned efﬁcient achieve performance using dimensions. learn dimensions word boundaries shown figure german model also appears learning interesting morphology grammar work presented kinds variable computation recurrent units vcrnn vcgru modify elman gated recurrent unit respectively allow models achieve better performance fewer operations shown time patterns interest sequential data. hope encouraging results open paths exploration adaptive computation paradigms neural networks general could lead computation-efﬁcient models better able deal varying information multi-scale processes. also immediate possibilities extensions speciﬁc model. example idea adaptive computation similarly applied commonly used recurrent units lstms work within different layers stacked architecture working adapting implementation settings. also hope investigate beneﬁts using stronger supervision signals train scheduler entropy prediction hopefully push current results even further. kyunghyun bart merrienboer dzmitry bahdanau yoshua bengio. properties neural machine translation encoder-decoder approaches. proceedings ssstemnlp eighth workshop syntax semantics structure statistical translation kyunghyun bart merrienboer aglar g¨ulc¸ehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. proceedings conference empirical methods natural language processing emnlp jurgen gael whye zoubin ghahramani. inﬁnite factorial hidden markov model. advances neural information processing systems vancouver british columbia canada december alex graves navdeep jaitly. towards end-to-end speech recognition recurrent neural proceedings international conference machine learning icml tony robinson lu´ıs almeida jean-marc boite herv´e bourlard frank fallside mike hochberg kershaw phil kohn yochai konig nelson morgan jo˜ao paulo neto steve renals marco saerens chuck wooters. neural network based speaker independent large vocabulary continuous speech recognition system wernicke project. third european conference speech communication technology eurospeech berlin germany september saizheng zhang yuhuai tong zhouhan roland memisevic ruslan salakhutdinov adyoshua bengio. architectural complexity measures recurrent neural networks. vances neural information processing systems annual conference neural information processing systemsn apply method outlined previous paragraph commonly used architecture. recall that given proportion dimensions sharpness parameter gating vector deﬁned", "year": 2016}