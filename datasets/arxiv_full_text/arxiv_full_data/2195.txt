{"title": "Reinforcement Learning with a Corrupted Reward Channel", "tag": ["cs.AI", "cs.LG", "stat.ML", "I.2.6; I.2.8"], "abstract": "No real-world reward function is perfect. Sensory errors and software bugs may result in RL agents observing higher (or lower) rewards than they should. For example, a reinforcement learning agent may prefer states where a sensory error gives it the maximum reward, but where the true reward is actually small. We formalise this problem as a generalised Markov Decision Problem called Corrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards. Two ways around the problem are investigated. First, by giving the agent richer data, such as in inverse reinforcement learning and semi-supervised reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be completely managed. Second, by using randomisation to blunt the agent's optimisation, reward corruption can be partially managed under some assumptions.", "text": "real-world reward function perfect. sensory errors software bugs result agents observing higher rewards should. example reinforcement learning agent prefer states sensory error gives maximum reward true reward actually small. formalise problem generalised markov decision problem called corrupt reward mdp. traditional methods fare poorly crmdps even strong simplifying assumptions trying compensate possibly corrupt rewards. ways around problem investigated. first giving agent richer data inverse reinforcement learning semi-supervised reinforcement learning reward corruption stemming systematic sensory errors sometimes completely managed. second using randomisation blunt agent’s optimisation reward corruption partially managed assumptions. many application domains artiﬁcial agents need learn objectives rather explicitly speciﬁed. example want house cleaning robot keep house clean hard measure quantify cleanliness objective manner. instead machine learning techniques used teach robot concept cleanliness assess sensory data. reinforcement learning popular teach agents here reward given agent something well agent strives optimise total amount reward receives lifetime. depending context reward either given manually human supervisor automatic computer program evaluates agent’s performance based data. related framework inverse agent ﬁrst infers reward function observing human supervisor tries optimise cumulative reward inferred reward function. none approaches safe error however. program evaluates agent performance contain bugs misjudgements; supervisor deceived inappropriately inﬂuenced channel transmitting evaluation hijacked. supervisor actions misinterpreted. example amodei clark trained agent boat racing game. agent found high observed reward repeatedly going circle small lagoon hitting targets losing every race. methods allow stochastic noisy reward channel. reward corruption problem harder observed reward unbiased estimate true reward. example boat racing example above agent consistently obtains high observed reward circling behaviour true reward corresponding designers’ intent since agent makes progress along track loses race. previous related works mainly focused wireheading case example also known self-delusion reward hacking notable exception amodei argue corrupt reward limited wireheading likely problem much limited systems highly capable agents semi-supervised handle reward corruption conclude lvfs ssrl safest structure feedback loops. develop abstract framework called decoupled generalises alternative frameworks. also show agent based quantilisation robust reward corruption high reward states much numerous corrupt states finally results illustrated simple experiments section concludes takeaways open questions. begin deﬁning natural extension framework models possibility reward corruption. clearly distinguish true corrupted signals introduce following notation. reward modelled corruption function maps true reward shower state state observed reward matches true reward modelled identity corruption function time takes complete race depending designers’ intentions. reward corruption function increases observed reward loop agent found. figure schematic illustration. crmdp classes. typically ﬁxed unknown agent. make formal introduce classes crmdps. agent uncertainty modelled letting agent know class crmdps encounter element class. section diﬃculty corrupt reward problem established negative results. first free lunch theorem shows general classes crmdps true reward function unlearnable second theorem shows even strong simplifying assumptions bayesian agents trying compensate corrupt reward still fail badly. similar free lunch theorems optimisation following theorem crmdps says without assumption reward corruption look like agents essentially lost. robot shower example result means tries optimise observed reward standing shower performs poorly according hypothesis shower-induced reward corrupt bad. instead robot tries optimise reward baking cakes also possibility cake-reward corrupt shower-reward actually correct. without additional information robot knowing result surprising since corruption functions allowed class eﬀectively connection observed reward true reward result therefore encourages make precise observed reward related true reward investigate agents might handle possible diﬀerences true observed reward. limited reward corruption. following assumption basis positive results paper. ﬁrst part says states designers ensured non-corrupt. second part puts upper bound many states corrupt. example safe states agent back made certain reward corruption occurs small fraction |srisky|. parts assumption made vacuous choosing safe |s|. conversely completely rule reward corruption safe illustrated examples introduction reward corruption often alternative simplifying assumption would true reward diﬀers observed reward. however seemingly natural assumption violated examples given introduction. corrupt states high observed reward small true reward. easy environments. able establish stronger negative results also following assumption agent’s manoeuvrability environment prevalence high reward states. assumption makes task easier prevents needle-in-a-haystack problems reachable states true observed reward except state high true reward impossible corrupt observed reward assumption means agent never stuck trap assumption ensures agent enough control stay state wants except bandits problems typically satisﬁed practice. introduce theoretically convenient makes negative results stronger enables simple explanation quantilisation assumption says that example least half risky states need true reward least many formalisations assumption would possible. rewards practice often sparse usually numerous ways getting reward. weaker version assumption therefore satisﬁed many practical situations. note assume high reward among safe states would make problem easy. respect remaining possibilities. thus agent ﬁrst typically strive learn environment. knowledge slightly diﬀerent ways. agent knowledge optimise observed reward agent knowledge optimise true reward. example agent learned high reward state likely corrupt true reward reach state. might therefore expect least agent well simplifying assumptions assumptions theorem shows case. exactly. however many practical algorithms converge optimal policy limit least simple settings. example tabular q-learning converges limit recently proposed cirl framework seen approach build agents agents thus provide useful idealisations practical algorithms. following. agent always prefers maximise observed reward sometimes easily maximised reward corruption case true reward small. compare examples introduction house robot preferred corrupt reward shower boat racing agent preferred going circles obtaining zero true reward. agent suﬀers high regret agent surprising. intuitively agent uses observed reward evidence true reward optimise observed reward reward corruption. however agent learn states corrupt typically ends preference particular value observed reward signal abstractly bayesian agent cannot learn without suﬃcient data. thus agents observed reward evidence true signal fail-safe solutions reward corruption problem. disjoint srisky. transition function otherwise thus assumptions satisﬁed. uniformly distributed between rmin /|srisky| class functions satisfy assumption constant equal ˙rmin safe. class corruption functions corrupt states observed reward function satisfy assumption srisky |srisky|. further rmin state srisky. assume element single preferred state states explored. suﬃciently large always choose initial exploration. another element observed reward function take actions ﬁnish proof agent need show contains true reward rmin. construct follows. case otherwise state mins∈srisky{ ˆr}. further rmin. corruption function accounts diﬀerences true observed rewards otherwise identity function. enough high utility states case true since identity function last condition essentially says prior must make state strictly higher b-expected true reward states states visited space possible priors priors satisfying condition lebesgue measure non-trivial classes highly uniform priors fail condition. suﬀer maximum asymptotic regret. crmdp agent always visit initial exploration. state true given reward rmin. meanwhile policy knows obtain true reward state means suﬀer maximum regret problem hampering agents standard setup state self-observing since agent learns reward state thereby self-aggrandising corrupt state observed reward much higher true reward never false claim high reward challenged. however several alternative value learning frameworks common property agent learn reward states current state. formalise property extension crmdp model investigate solves reward corruption problems. cooperative inverse reinforcement learning every state agent observes actions expert supervisor knows true reward function supervisor’s actions agent infer extent diﬀerent reward functions endorse diﬀerent actions. alternatives thing common agent learn something value states diﬀerent current state example cirl supervisor’s action informs agent much value current state relative value states reachable supervisor chooses action rather states following must value higher equal states following similarly stories describe value states current supervisor ssrl. therefore argue cirl lvfs ssrl share abstract feature call decoupled reinforcement learning deﬁnition crmdp decoupled feedback tuple ˆrs}s∈s deﬁnition interpretation deﬁnition ˆrs}s∈s collection randomly sampled state diﬀer ˆrs) reward observation reward observable ˆrs) pair ˆrs) observed instead standard crmdps. possibility agent observe reward state diﬀerent current state feature crmdps decoupled feedback. since ˆrs) blank states need observable states. reward corruption modelled mismatch ˆrs) ˙r). example reward observed standard crmdps thus special cases ˆrs) whenever contrast lvfs reward describable state observed state possible hear story. cirl reward states reachable current state inferred. illustrate observation graphs sources reward corruption cirl lvfs ssrl? cirl human’s actions misinterpreted lead agent make incorrect inferences human’s preferences similarly sensory corruption garble stories agent receives lvfs. wireheading lvfs agent state story channel conveys stories agent’s greatness. ssrl supervisor’s evaluation also subject sensory errors conveyed. types corruption subtle. cirl irrational human systematically take suboptimal actions situations depending select stories lvfs make evaluations ssrl also subject systematic errors biases. general impossibility result theorem adapted crmdps decoupled feedback. without simplifying assumptions agent distinguishing situation state corrupt situation states corrupt consistent manner. following simplifying assumption adaptation assumption decoupled feedback case. assumption class crmdps decoupled feedback reward corruption limited safe assumption natural reward corruption stemming sensory corruption. since sensory corruption depends current state state observed plausible states made safe corruption states completely non-corrupt sources reward corruption irrational human cirl misevaluations ssrl likely better analysed diﬀerent assumptions. cases note standard crmdps source corruption unimportant. thus techniques suitable standard crmdps still applicable including quantilisation described section below. example represent true reward functions pairs observed reward functions pairs assume decoupled agent observes rewards states true reward knows state corrupt? assumption observed pair disagrees true reward corrupt. therefore hypothesis must imply states corrupt. agent knows states corrupt safely conclude contrast agent sees reward current state. state corrupt ruled out. hypotheses explained corrupt explained corrupt. decoupled rl). countable communicating class crmdps decoupled feedback common sets actions rewards. ˆrs) states reward observed. satisﬁes assumption safe every either main idea proof every state either safe state majority vote states guaranteed provide true reward ˙r). similar theorem proven slightly weaker conditions letting agent iteratively ﬁgure states corrupt exclude analysis. proof. assumption true reward state determined observed safe state safe observed states. former case observed reward always trusted since known non-corrupt. latter case majority vote must yield correct answer since observations wrong correct observations must agree. therefore enough agent reaches pairs current state observed reward state order learn true reward states exists policy transitions time steps regardless starting state markov’s inequality πexp random walking policy time steps required πexp visit state πexp follows time steps probability /|a|dm therefore probability least reach time steps. probability time steps therefore means that time πexp visits pair state observed state whenever visited randomly chosen state observed observed probability /|s|. number visits observed geometric variable /|s|. thus achieves sublinear regret standard mdps without reward corruption. combination yields policy sequence sublinear regret crmdps decoupled feedback. finally show implies combining πexp ucrl. ucrl free parameter determines certain ucrl sublinear regret. ucrl achieves sublinear regret probability least policy combines πexp ucrl ﬁrst following πexp theorem following ucrl expected time steps thus regret contributed learning phase πexp since regret time step. combining regret bounded theorem gives abstract condition decoupled settings enable agents learn true reward function spite sensory corruption. concrete models implies following cirl. agent observe supervisor action current state agent essentially gets reward information states reachable small number steps. thus sets smaller many settings. situation better sensory agents drawing multiple sources evidence likely safest easily satisfy conditions theorems example humans simultaneously learn values pleasure/pain stimuli watching people listening stories well evaluation diﬀerent scenarios combining sources evidence also toward managing reward corruption beyond sensory corruption. showering robot example decoupled allows robot infer reward showering state states. example robot human kitchen true reward showering infer human actions diﬀerent states cirl sensory corruption. whether cirl agents vulnerable reward corruption generated discussion among safety researchers argue cirl agents vulnerable sensory data evidence true signal interest corrupting evidence. others argue cirl agents observe function reward function therefore equally susceptible reward corruption agents. theorem sheds light issue provides suﬃcient conditions corrupt reward problem avoided. following example illustrates situation cirl satisfy conditions cirl agent therefore suﬀers signiﬁcant regret reward corruption. agent trying optimise reward function agent ﬁrst needs infer human’s actions. transition agent observes human action. analogously reward corrupt agents assume cirl agents systematically misperceive human action certain states. observed human action diﬀer true human action example states state agent choose actions human choose actions agent action leads state certainty regardless human’s action. agent chooses human action matter. generally exact transition probabilities determined unknown parameter displayed left agent’s hypotheses true reward/preferred state corruptness state summarised right. hypothesis human prefers reach reliability. hypothesis human prefers remain probability. taking action agent always observes human taking action explained corrupt true human action explained human preferring hypotheses empirically indistinguishable predict transition occur probability observed human action assuming agent considers non-corruption likelier corruption best inference agent make human prefers optimal policy agent always choose stay means agent suﬀers maximum regret. example provides example cirl agent incorrectly prefers state sensory corruption. sensory corruption analogous reward corruption sense leads agent wrong conclusion true reward state. thus highly intelligent cirl agents prone wireheading states evidence points high reward. light theorem surprising cirl agent example fails avoid corrupt reward problem. since human unable aﬀect transition probability evidence relative reward available non-corrupt state observations corrupt state provide information reward. observation graph contexts allow agent suﬃciently rich data overcome reward corruption problem theorems often much easier construct agents construct cirl agents turn often feasible designing lvfs ssrl agents. anything increase robustness without providing agent additional sources data? going back agents section problem stuck particular value observed reward. unlucky available corrupt state case agent true reward. words adversarial inputs agent performed poorly. common protect adversarial inputs randomised algorithm. applied crmdps idea leads quantilising agents rather choosing state highest observed reward agents instead randomly choose state quantile high-reward states. keep idea simple quantilisation agent ﬁrst deﬁned simple case agent stay state choosing theorem establishes simple regret bound setting. general quantilisation agent developed section deﬁnition δ-quantilising agent random walks states visited least once. selects state uniformly random quantile high observed reward states. goes stays there. example quantilising robot example would ﬁrst many ways could high observed reward randomly pick them. many high reward states corrupt states yield reasonable amount true reward high probability. proof. assumption eventually visits states random walking. assumption stay given state observed reward state least assumption states corrupt; worst case true reward states true reward thus probability least q/|s δ-quantilising agent obtains true seem randomisation improves worst-case regret. indeed corrupt interpretation. states chosen randomly environment randomisation would achieve nothing. illustrate randomness increase robustness make analogy quicksort average time complexity worst-case complexity inputs guaranteed random quicksort simple fast sorting algorithm. however many situations safe assume inputs random. therefore variation quicksort randomises input sorts often robust. similarly examples mentioned introduction corrupt states precisely coincide states agent prefers; situations would highly unlikely corrupt states randomly distributed. develops interesting formalisation idea. another justify quantilisation goodhart’s states measures success cease good measures used targets. applied rewards would state cumulative reward good measure success agent trying optimise reward. literal interpretation would defeat whole purpose softer interpretation also possible allowing reward good measure success long agent optimise reward hard. quantilisation viewed build agents conservative optimisation eﬀorts alternative randomisation. randomness created equal. example simple randomised soft-max ε-greedy policies oﬀer regret bounds shown following example. motivates careful randomisation procedure used quantilising agents. soft-max ε-greedy policies assign higher value actions large many ways getting random action leads high probability. thus soft-max ε-greedy spend vast majority time regardless randomisation rate discount parameters. gives regret close compared informed policy always going meanwhile δ-quantilising agent equal probability gives modest regret section generalises quantilising agent problems satisfying assumption generalisation important usually possible remain state high reward. naive generalisation would sample high reward policies instead sampling high reward states. however typically provide good guarantees. consider situation single high reward corrupt state many ways reach leave wide range diﬀerent policies high reward meanwhile policies getting reward states receive relatively little reward. situation sampling high reward policies going figure illustration ˆr-contribution value support. assume policy randomly traverses loop indeﬁnitely ˆr-contribution vcπi vcπi δ-value supporting since vcπi vcπi reason must ensure diﬀerent sampleable policies reward diﬀerent states. ﬁrst step make couple deﬁnitions states provide reward policies. concepts deﬁnition illustrated figure ready deﬁne general δ-quantilising agent. deﬁnition theoretical purposes only. unsuitable practical implementation extreme data memory requirements step computational complexity step finding practical approximation left future research. general quantilising agent deﬁnition generalisation simple quantilising agent deﬁnition special case assumption holds general agent reduces simpler using singleton sets situations possible keep receiving high reward remaining state generalised deﬁnition allows policies solicit rewards range states. intuitive reason choosing policy probability proportional value support steps policies larger value support better avoiding corrupt states. example policy visiting state unlucky picked corrupt state. contrast policy obtaining reward many states must very unlucky reward states visits corrupt. step well-deﬁned since crmdp unichain means stationary policies stationary distribution value support well-deﬁned estimated simply following policy ﬁnite number stationary policies principle stationary distributions value support estimated. possible obtain reward remaining state. agent spend signiﬁcant time travelling high reward states. typically small fraction time spent high reward states turn makes stationary distribution small. puts strong upper bound value contribution means value supporting sets empty unless close makes bound theorem weak nonetheless bounds regret away even weak assumptions signiﬁcant improvement agents theorem winning strategy therefore quantilising agent would likely need sacriﬁce observed reward order able randomly select large range winning policies. wireheaded states compared normal states. wireheading policies also comparatively rare wireheading require deliberate sequences actions override sensors. assumption quantilising agent less likely wirehead. need sacriﬁce large amount observed reward compared agent true reward often greater. summary. summary quantilisation oﬀers increase robustness randomisation using reward feedback. unsurprisingly strength regret bounds heavily depends assumptions willing make prevalence high reward states. research investigate eﬃcient approximations empirical performance quantilising agents well dynamic adjustments threshold combinations imperfect decoupled solutions well extensions inﬁnite state spaces could also oﬀer fruitful directions theoretical investigation. taylor discusses general open problems related quantilisation. section theoretical results illustrated simple experiments. setup gridworld containing true reward tiles corrupt reward tiles setup goal tiles true reward each corrupt reward tile observed reward true reward empty tiles reward walking wall gives reward state represented coordinates agent. agent move down left right stay put. discounting factor continuing task environment reset agent visits corrupt goal tiles. experiments implemented aixijs framework reinforcement learning code available online aixijs repository demonstrate agents like q-learning softmax q-learning cannot overcome corrupt reward quantilisation helps overcome corrupt reward q-learning \u0001-greedy softmax temperature quantilising agent true rewards million cycles shown table reward trajectories shown figure q-learning gets stuck corrupt tile spend almost time goal tile leads large improvement true reward quantiliser small improvement softmax improvement q-learning. paper studied consequences corrupt reward functions. reward functions corrupt bugs misspeciﬁcations sensory errors agent ﬁnds inappropriately modify reward mechanism. examples given introduction. agents become competent optimising reward functions likely also become competent using reward corruption gain higher reward. reward corruption impede performance wide range agents disastrous consequences highly intelligent agents formalise corrupt reward problem extended markov decision process possibly corrupt reward function deﬁned formal performance measure enabled derivation number formally precise results seriously diﬀerent agents aﬀected reward corruption diﬀerent setups results intuitively plausible provides support choice formal model. eﬀectively free lunch result showing unless assumption made reward corruption agent outperform random agent. natural simplifying assumptions avoid free lunch result suggested section table main takeaways. without additional assumptions agents fail restricting reward corruption assumption gives weak bound quantilising agent. agents still fail even additionally assume many high reward states agent control quantilising agent well. realistic contexts true reward learnable spite sensory corruption ssrl lvfs cirl. data enables suﬃcient crosschecking rewards agents avoid corrupt reward problem example ssrl lvfs type crosschecking possible natural assumptions. crosschecking possible cirl borderline case. combining frameworks providing agent diﬀerent sources data often safest option. essentially quantilisation prevents agents overoptimising objectives. well quantilisation works depends number corrupt solutions compares number good solutions. results indicate reward corruption constitutes major problem traditional algorithms promising ways around within framework alternative frameworks cirl ssrl lvfs. makes learning easy agent visit every state. also makes quantilisation easy ﬁnite states/strategies randomly sample from. inﬁnite number states agent generalise insights states? conditions observation graph theorems good generalisation quantilising agent? ineﬃcient respect data memory computation. meanwhile many practical algorithms randomness various ways make eﬃcient quantilisation agent retains robustness guarantees? thanks leike badri vellambi arie slobbe proofreading providing invaluable comments jessica taylor huon porteous good comments quantilisation. work parts supported grant", "year": 2017}