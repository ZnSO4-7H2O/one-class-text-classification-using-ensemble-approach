{"title": "Sparse Principal Component Analysis via Rotation and Truncation", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Sparse principal component analysis (sparse PCA) aims at finding a sparse basis to improve the interpretability over the dense basis of PCA, meanwhile the sparse basis should cover the data subspace as much as possible. In contrast to most of existing work which deal with the problem by adding some sparsity penalties on various objectives of PCA, in this paper, we propose a new method SPCArt, whose motivation is to find a rotation matrix and a sparse basis such that the sparse basis approximates the basis of PCA after the rotation. The algorithm of SPCArt consists of three alternating steps: rotate PCA basis, truncate small entries, and update the rotation matrix. Its performance bounds are also given. SPCArt is efficient, with each iteration scaling linearly with the data dimension. It is easy to choose parameters in SPCArt, due to its explicit physical explanations. Besides, we give a unified view to several existing sparse PCA methods and discuss the connection with SPCArt. Some ideas in SPCArt are extended to GPower, a popular sparse PCA algorithm, to overcome its drawback. Experimental results demonstrate that SPCArt achieves the state-of-the-art performance. It also achieves a good tradeoff among various criteria, including sparsity, explained variance, orthogonality, balance of sparsity among loadings, and computational speed.", "text": "sparse principal component analysis aims ﬁnding sparse basis improve interpretability dense basis meanwhile sparse basis cover data subspace much possible. contrast existing work deal problem adding sparsity penalties various objectives paper propose method spcart whose motivation rotation matrix sparse basis sparse basis approximates basis rotation. algorithm spcart consists three alternating steps rotate basis truncate small entries update rotation matrix. performance bounds also given. spcart efﬁcient iteration scaling linearly data dimension. easy choose parameters spcart explicit physical explanations. besides give uniﬁed view several existing sparse methods discuss connection spcart. ideas spcart extended gpower popular sparse algorithm overcome drawback. experimental results demonstrate spcart achieves state-of-the-art performance. also achieves good tradeoff among various criteria including sparsity explained variance orthogonality balance sparsity among loadings computational speed. many research areas data encountered usually high dimensions examples signal processing machine learning computer vision document processing computer network genetics etc. however almost data areas much lower intrinsic dimensions. thus handle data traditional problem. principal component analysis popular analysis tools deal situation. given data whose mean removed approximates data representing another orthonormal basis called loading vectors. coefﬁcients data represented using loadings called principal components. obtained projecting data onto loadings i.e. inner products loading vectors data vector. usually loadings deemed ordered vectors variances data explained decreasing order e.g. leading loading points maximal-variance direction. data dimensional subspace i.e. distribution mainly varies directions loadings enough obtain good approximation; original high-dimensional data represented low-dimensional principal components dimensionality reduction achieved. commonly dimensions original data physical explanations. example ﬁnancial biological applications dimension correspond speciﬁc asset gene however loadings obtained usually dense principal component inner product mixture dimensions makes difﬁcult interpret. entries loadings zeros principal component becomes linear combination non-zero entries. facilitates understanding physical meaning loadings well principal components further physical interpretation would clearer different loadings different non-zero entries i.e. corresponding different dimensions. sparse aims ﬁnding sparse basis make result interpretable time basis required represent data distribution faithfully. thus tradeoff statistical ﬁdelity interpretability. past decade variety methods sparse proposed. considered tradeoff sparsity explained variance. however three points received enough attentions orthogonality loadings balance sparsity among loadings pitfall deﬂation algorithms. orthogonality. loadings orthogonal. pursuing sparse loadings property easy lose. orthogonality desirable indicates independence physical meaning loadings. loadings sufﬁciently sparse orthogonality usually implies non-overlapping supports. background improving interpretation loading associated distinctive physical variables principal components. makes interpretation much easier. besides loadings orthogonal basis inner products data loadings used compute components constitute exact projection. extreme example loadings close components would similar too. meaningless. balance sparsity. member loadings highly dense particularly leading ones take account variance otherwise meaningless. emphasize point quite existing algorithms yield loadings leading ones highly dense minor ones highly sparse; sparsity achieved minor ones variance explained dense ones. unreasonable. pitfall deﬂation. existing work categorized groups deﬂation group block group. obtain sparse loadings deﬂation group computes loading time; removing components computed follows traditional pca. block group ﬁnds loadings together. generally optimal loadings found restrict subspace dimension overlap optimal loadings dimension increases problem occur whose loadings successively maximize variance loadings found deﬂation always globally optimal case sparse deﬂation method greedy cannot optimal sparse loadings. however block group potential. paper propose approach called spcart contrast traditional work based adding sparsity penalty objective motivation spcart distinctive. spcart aims rotation matrix sparse basis sparse basis approximates loadings rotation. resulting algorithm consists three alternative steps rotate loadings truncate small entries them update rotation matrix. spcart turns resolve alleviate previous three points. following merits. able explain much variance loadings since sparse basis spans almost subspace loadings. basis close orthogonal since approximates rotated loadings. truncation tends produce balanced sparsity since vectors rotated loadings equal length. greedy compared deﬂation group belongs block group. contributions paper four-fold propose efﬁcient algorithm spcart achieving good performance series criteria overlooked previous work; devise various truncation operations different situations provide performance analysis; give uniﬁed view series previous sparse approaches together ours; uniﬁed view relation table time complexities computing loadings samples dimension number iterations. cardinality loading. preprocessing initialization overheads omitted. spcart additional cost pca. complexities spcart listed truncation types t-ℓ. t-sp t-en gpower rsvd method extend gpower rsvd implementation called rsvd-gp overcome drawbacks–parameter tuning problem imbalance sparsity among loadings. rest paper organized follows section introduces representative work sparse pca. section presents method spcart four types truncation operations analyzes performance. section gives uniﬁed view series previous work. section shows relation gpower rsvd method extends gpower rsvd implementation called rsvd-gp. experimental results provided section finally conclude paper section post-processing pca. early days interpretability gained post-processing loadings. loading rotation applies various criteria rotate loadings ’simple structure’ emerges e.g. varimax criterion drives entries either small large close sparse structure. simple thresholding instead obtains sparse loadings directly setting entries loadings small threshold zero. covariance matrix maximization. recently systematic approaches based solving explicit objectives proposed starting scotlass optimizes classical objective i.e. maximizing quadratic form covariance matrix additionally imposing sparsity constraint loading. matrix approximation. spca formulates problem regression-type optimization facilitate lasso elastic-net techniques solve problem. rsvd obtain sparse loadings solving sequence rank- matrix approximations sparsity penalty constraint imposed. semideﬁnite convex relaxation. methods proposed local ones suffer getting trapped local minima. dspca transforms problem semideﬁnite convex relaxation problem thus global optimality solution guaranteed. distinguishes local methods. unfortunately computational complexity high expensive applications. later variable elimination method complexity developed order make application large scale problem feasible. greedy methods. greedy search branch-and-bound methods used solve small instances problem exactly. step algorithm complexity leading total complexity full solutions later bound improved classiﬁcation setting another greedy algorithm pathspca presented approximate solution process resulting complexity full solutions. review dspca pathspca applications power methods. gpower method formulates problem maximization convex objective function solution obtained generalizing power method used compute loadings. recently power method tpower somewhat different related power method itspca aims recovering sparse principal subspace proposed. augmented lagrangian optimization. alspca solves problem based augmented lagrangian optimization. special feature alspca simultaneously considers explained variance orthogonality correlation among principal components. interpretation data matrix samples variables loadings arranged column-wise. denotes column. denotes ﬁrst columns rotation matrix rotated loadings i.e. spare loadings arranged column-wise similar matrix rn×p thin rp×p olar vector entry-wise soft thresholding sign+ otherwise vector entry-wise hard thresholding xi]+ i.e. |xi| otherwise ···}. vector sets smallest entries zero vector sets smallest entries whose energy take zero. found following sort |x||x| ascending order maxi s.t. idea spcart simple. since rotation loadings rp×r constitutes orthogonal basis spanning subspace want rotation matrix transformed sparsest basis difﬁcult solve problem directly instead would rotation matrix sparse basis sparse basis approximates loadings rotation note vectors centered mean zero loadings obtained pca. clearly rr×r urςrr also solution implies rotation orthonormal leading eigenvectors rp×r solution best rank approximation orthonormal basis corresponding eigen-subspace capable representing original data distribution well original basis. thus natural idea sparse rotation matrix becomes sparse i.e. denotes norm columns matrix i.e. counts non-zeros matrix. objective optimization unfortunately problem hard solve. approximate instead. since want rotation matrix sparse basis approximates original loadings. without confusion denote hereafter. simplicity version postponed next section consider version ﬁrst norm vector i.e. absolute values. well-known norm sparsity inducing convex surrogate norm objective solution orthogonal deviate eigen-subspace spanned however approximation accurate enough i.e. would nearly orthogonal explain similar variance note objective turns matrix approximation problem eckart-young theorem. difference sparsity penalty added. solutions still share common features. λkxik s.t.kxik independent subproblems column minxi /kzi− λkxik s.t.kxik solution sλ/ksλk equivalent maxxi entry-wise soft thresholding deﬁned table truncation type soft thresholding. following physical explanations. rotated loadings orthonormal. obtained truncating small entries hand unit length column single threshold feasible make sparsity distribute evenly among columns otherwise apply different thresholds different columns hard determine. hand orthogonality small truncations still possible nearly orthogonal. distinctive features spcart. enable easy analysis parameter setting. algorithm spcart input data matrix rn×p number loadings truncation type truncation parameter output sparse loadings rp×r. compute rank-r rp×r. initialize repeat convergence section given rotated loadings introduce truncation operation following four types soft thresholding hard thresholding t-sp truncation sparsity t-en truncation energy introduced last section resulted penalty. optimization similar case. fixing olar. fixing problem becomes λkxk. decomposed entry-wise subproblems minx solution apparent |zji| x∗ji otherwise x∗ji zji. hence solution expressed normalization compared case. unit length constraint kxik added closed form solution. however practice still hλ/khλk consistency since empirically signiﬁcant difference observed. note penalties result thresholding operation nothing else /kpλk proof appendix t-en truncation energy. truncate smallest entries whose energy take percentage eλ/keλk. described table however aware objective associated type. algorithm describes complete algorithm spcart truncation type. spcart promotes seminal ideas simple thresholding loading rotation using ﬁrst iteration spcart i.e. corresponds ad-hoc simple thresholding frequently used practice sometimes produced good results another motivation spcart i.e. similar loading rotation whereas spcart explicitly seeks sparse loadings pseudo-norm loading rotation seeks ’simple structure’ various criteria e.g. varimax criterion maximizes variances squared loadings ki)] drives entries distribute unevenly either small large much sparsity guaranteed? much deviates orthogonality much variance explained performance bounds derived functions thus directly indirectly control sparsity orthogodeﬁnition sparsity proportion zero entries kxk/p. deﬁnition tλ/ktλk deviation sin) included angle deﬁned deﬁnition nonorthogonality cos)| included angle deﬁnition given data matrix rn×p containing samples dimension basis rp×r explained variance orthonormal basis subspace spanned cumulative percentage explained variance tr/tr intuitively larger leads higher sparsity larger deviation. truncated vectors deviate originally orthogonal vectors worst case nonorthogonality degenerates ‘sum’ deviations. another deviations sparse basis rotated loadings small expect sparse basis still represents data well explained variance cumulative percentage explained variance maintains similar level pca. nonorthogonality explained variance depend deviations deviation sparsity turn controlled details. proofs following results included appendix bounds obtained considering conical surfaces generated axes rotational angles proposition implies nonorthogonality determined deviated angles. deviations small orthogonality good. deviation depends analyzed below. following results concern single vector basis. denote simplicity derive bounds sparsity deviation sin) depend value entry value uniform vector. theorem speciﬁc spcart concerns important explained variance. results apply general situations proposition apply orthonormal theorem applies matrix obtain results speciﬁc spcart make assumption data distribution. nevertheless still absolute performance bounds spcart guide performance guarantee. t-en direct control deviation. recall deviation direct inﬂuence explained variance. thus desirable gain speciﬁc explained variance t-en preferable. besides moderately large t-en also gives nice control sparsity. finally derive bounds explained variance results provided. ﬁrst general applicable basis limited sparse ones. second tailored spcart. theorem rank-r rn×p rr×r. given rp×r assume rr×r dmin mini dii. theorem interpreted follows. basis approximates rotated loadings well dmin close variance explained close explained pca. note variance explained loadings largest value possible achieved orthonormal basis. conversely deviates much rotated loadings dmin tends zero variance explained guaranteed much. less sparse loadings deviates rotated loadings variance explain. since sparse loadings obtained truncating small entries rotated loadings deviation angle sparse loadings rotated loadings theorem implies deviation small variance explained sparse loadings close example truncated energy k¯zk roughly satisﬁed t-en using small uniform means projected length less projected onto must satisﬁed exactly orthogonal whereas likely satisﬁed nearly orthogonal achieved setting small according proposition case guaranteed. practice prefer cpev cpev measures variance explained subspace rather basis. since also projected length onto subspace spanned higher cpev better represents data. orthogonal basis overestimates underestimates variance. however nearly orthogonal difference small nearly proportional cpev. series methods scotlass spca gpower rsvd tpower spcart though proposed independently formulated various forms derived common source theorem eckart-young theorem. seen problems matrix approximation different sparsity penalties. matrix variables solutions usually obtained alternating scheme solve other. similar spcart subproblems sparsity penalized/constrained regression problem procrustes problem. treated target sparse loadings weights. ﬁxed problem equivalent λikzik. ﬁxed procrustes problem elastic-net problems minzi kaxi azik minx gpower except artiﬁcial factors original gpower solves following versions objectives objectives seen derived mirror version theorem exists minyx s.t. rn×p still seen data matrix containing samples dimension solution vrςrr olar urr. adding sparsity penalties following alternating optimization scheme. ﬁxed cases olar. case iterative steps combined together produce essentially solution processes original ones matrix approximation formulation makes relation gpower spcart others apparent. three methods rsvd tpower seen special cases gpower. rsvd rsvd seen special case gpower i.e. single component case olar reduces unit length normalization. loadings deﬂation e.g. update procedure again. since subsequent loadings obtained nearly orthogonal closed form solutions ax/kaxk pp−λ. sets smallest entries zero. iteration pp−λ) indicates equivalence original tpower. s.t. serves length variables ﬁxed ﬁxed problem maxyx s.t. small modiﬁcation leads note similar forms respectively. important points differences. first spcart deals orthonormal loadings rather original data. second spcart takes rotation matrix rather merely orthonormal matrix variable. differences points success spcart. compared spcart gpower drawbacks. gpower work deﬂation mode block mode block mode mechanism ensure orthogonality loadings. orthogonal thresholding also tend orthogonal. besides easy determine weights since lengths zi’s usually vary great range. e.g. initialize vrςr scaled loadings whose lengths usually decay exponentially. thus simply thresholds λi’s uniformly easy lead unbalanced sparsity among loadings leading loadings highly denser. deviates goal sparse pca. deﬂation mode though produces nearly orthogonal loadings greedy scheme makes solution optimal. still exists problem weights appropriately. besides modes performance analysis difﬁcult obtain. major drawback rsvd gpower cannot uniform thresholds applying thresholding problem exist spcart since inputs unit length. extend similar idea gpower rsvd equivalent truncating according length using adaptive thresholds tλkzk truncation types t-en t-sp introduced gpower too. t-sp insensitive length trouble parameter setting; deﬂation version happens tpower. deﬂation version improved algorithm rsvd-gp shown algorithm block version rsvdgpb shown algorithm rsvd-gpb follows optimization described section rsvd-gp since olar reduces normalization vector extended truncation insensitive length input combine olar step step ignore length iterations. besides efﬁcient work covariance matrix algorithm rsvd-gpb input data matrix rn×p number loadings truncation type parameter output sparse loadings rp×r. compute rank-r repeat convergence normalize xi/kxik. data sets used include synthetic data underlying sparse loadings classical pitprops data natural image data moderate dimension relatively large sample size comprehensive evaluations conducted; gene data high dimension small sample size random data increasing dimensions purpose speed test. compare methods methods spca pathspca alspca gpower tpower spca toolbox implements constraint versions. gpowerb denote block version gpower rsvd-gpb. spcart denote spcart using t-ℓ; methods similar abbreviations. note that rsvd-gp equivalent tpower except spcart rsvd-gp codes others downloaded authors’ websites. time cost including initialization. sometimes worst sparsity among loadings mini instead appropriate show imbalance sparsity. methods involved comparison parameter induces sparsity. methods direct control sparsity view belonging t-sp denote number zeros vector. gpowerb initialized parameters uniform loadings. alspca since consider correlation among principal components spcart default since minimal threshold ensure sparsity maximal threshold avoid truncating zero vector. termination conditions spcart spca relative change loadings iterations exceed rsvd-gp uses test whether spcart rsvd-gp recover underlying sparse loadings. synthetic data introduced became classical sparse problem. considers three hidden gaussian factors +.h+ǫ variables generated words independent correlations them particularly ﬁrst variables generated variables generated sets variables independent. last variables generated correlations variables particularly latter. covariance matrix determined ai’s algorithms. algorithms accept data matrix artiﬁcial data σ−/v made reasonable since share loadings. algorithms required sparse loadings. besides cpev nonzero supports loadings recorded consistent generating model. results reported table except spca others including spcart rsvd-gp successfully recovered acceptable loading patterns seen cpev. pitprops data classical data test sparse covariance matrix variables available algorithms accept data matrix input artiﬁcial data matrix σ−/v made algorithms tested sparse loadings. fairness tuned algorithm yields total cardinality loadings denoted mainly t-sp algorithms tested. criteria cpev reported. results shown table spcart best overall although cpev best. others especially gpower suffer unbalanced cardinality seen loading patterns std; cpev high mainly contributed dense leading vector aligns direction maximal variance i.e. leading loading. improvements rsvd-gp gpower point signiﬁcant seen tradeoff cpev. t-sp focusing cpev performance rsvd-gp good spcart somewhat cpev worst although ranks two. figure performance bounds spcart levels image data. legends similar normalized version tr/tr compared cpev. evdmin minev evcos cosev /tr. evcos better evdmin estimation evcos meets empirical performance well. algorithm performs optimistic upper bounds. owning good orthogonality comparable cpev. iteration goes improves cpev sacriﬁces little. investigation distribution natural image patches important computer vision pattern recognition communities. data evaluate convergence spcart performance bounds make comprehensive comparisons different algorithms. randomly select gray-scale patches bsds patch reshaped vector dimension component patch removed ﬁrst mean data removed. show stability convergence improvement spcart simple thresholding take example. cpev results shown figure gradually spcart found local optimal rotation less truncated energy rotated loadings needed sparser variance explained orthogonal basis. note that results ﬁrst iteration equal simple thresholding ﬁnal solution spcart signiﬁcantly improves simple thresholding. compare theoretical bounds provided section empirical performance. t-en taken example guaranteed. achieve systematic evaluation three levels subspace dimension tested corresponding cpev results shown figure note theoretical bounds absolute bounds without assuming speciﬁc distribution data different empirical performance. finally explained variance found figure large discrepancy between cpev owning near orthogonality sparse basis indicated figure hand speciﬁc bound evcos better universal bound evdmin. contrast sparsity nonorthogonality evcos meets empirical performance well analyzed section figure rsvd-gp v.s. gpower image data. gpower uniform parameter setting leads unbalanced sparsity worst case rather dense. rsvd-gp signiﬁcantly improves gpower balance sparsity well criteria. figure rsvd-gp v.s. rsvd-gpb image data. initialized pca. block version gets much worse orthogonality deﬂation version. criteria comparable except time cost. figure spcart v.s. spca pathspca alspca image data. make ﬁgures less messy taken representative spcart. spcart performs best overall pathspca performs best cpev. alspca spca unstable. pathspca spca time consuming. rsvd-gp v.s. gpower figure gpower uniform parameter setting leads unbalanced sparsity. fact worst case usually achieved leading loadings. rsvd-gp signiﬁcantly improves gpower criterion well others. spcart v.s. rsvd-gp figure methods obtain comparable results criteria. spcart v.s. spca pathspca alspca figure spcart performs best overall. generally pathspca performs best cpev time cost increases cardinality. alspca unstable sensitive parameter spca. besides spca time consuming. figure evolution solution increases image data spcart v.s. pathspca alspca spca rsvdgp. spcart rsvd-gp shown. spcart insensitive parameter setting. compared deﬂation algorithms loadings spcart adaptive whose properties gradually improve. becomes full dimension perfectly recovers natural basis globally optimal seen worst sparsity nor. sparsity criteria reach touches bottom t-en achieves similar results. figure images ﬁrst last loadings among total loadings image data. line pca; line rsvd-gp; line spcart. ⌊.p⌋. rsvd-gp greedy results conﬁned spcart ﬂexible. figure spcart rsvd-gp alspca spca gene data less messy truncation types shown. alspca much costly shown spcart rsvd-gp perform best ﬁnish within second high dimensional data. using threshold always sparse orthogonal explaining less variance. spcart insensitive parameter. constant setting produces satisfactory results across r’s. contrast deﬂation algorithms spcart block algorithm. solution evolves sparsity explained variance orthogonality balance sparsity improve increases potential optimal solution. evident t-en becomes full dimension perfectly recovers natural basis globally optimal; t-en obtains similar results. visualized images loadings deﬂation block algorithm shown figure greedy nature results obtained deﬂation algorithm conﬁned pca; ﬁrst loadings differ signiﬁcantly last loadings. gene data algorithms leukemia dataset contains genes samples i.e. data. classical application motivates development sparse pca. thousands genes sparse basis help locate determines distribution data. results shown figure type data spca mode efﬁciency. pathspca slow except involved comparison. spcart rsvd-gp perform best finally test computational efﬁciency random data increasing dimensions following zero-mean unit-variance gaussian data used test. make computational cost depends clear fair comparison t-sp ⌊.p⌋ tested. results shown figure rsvd-gp pathspca increase nonlinearly spcart grows much slowly. remember figure already showed time complexity pathspca increases nonlinearly cardinality figure spcart increases nonlinearly consistent table dealing high dimensional data pursuing loadings spcart advantageous. according experiments spcart signiﬁcantly improves simple thresholding. rsvd-gp improves gpower. rsvd-gp obtains loadings orthogonal rsvd-gpb. spcart rsvd-gp pathspca generally perform well. pathspca consistently explains variance time-consuming among three. rsvd-gp spcart perform similarly sparsity explained variance orthogonality balance sparsity. however rsvdgp sensitive parameter setting i.e. tpower) greedy deﬂation algorithm. spcart belongs block group solution improves target dimension potential obtain globally optimal solution. sample size larger dimension time cost pathspca rsvd-gp nonlinearly dimension spcart increases much slowly. deal high dimensional data different situations spcart number loadings small; rsvd-gp sample size small; pathspca target cardinality small. four truncation types spcart work well different aspects hard thresholding performs well overall; soft thresholding gets best sparsity orthogonality; t-sp hard sparsity constraint directly controls sparsity zero sparsity variance; t-en truncation energy guarantees explained variance performance bound tight. open questions unresolved. conditions spcart truncation type recover underlying sparse basis? efforts made recently problem explicit objective formulation t-en? proposition pλ/kpλk solution s.t. kxik λkxik ﬁrst prove nonproof. problem equivalent maxxi zeros normalized entries support prove kx∗i support divide parts corresponds largest entries assume support support remaining support. problem reduced maxxi zi/k ˜zik ˜zik achieve s.t. support kxik solution ˜zi/k ˜zik. next since minima ˜zik large possible. largest entries combining kˆzk cos) kˆzk kyk. note upper bound achieved direction case kˆzk k˜zk. kˆzk cos) k˜zk. k˜zk upper bound approached becomes orthogonal λkxk. besides case kˆzk k˜zk deduced matrix approximation formulations split s.t.kwik diagonal matrix whose diagonal element fact models length corresponding column become λikwik. assume sufﬁciently small satisﬁed. substituting s.t. kwik solve previous assumption equivalent substituting sλi/ksλik back obtain finally solutions literally combine solution steps", "year": 2014}