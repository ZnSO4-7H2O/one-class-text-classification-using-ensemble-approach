{"title": "Multi-column Deep Neural Networks for Image Classification", "tag": ["cs.CV", "cs.AI"], "abstract": "Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.", "text": "idsia joint institute university lugano university applied sciences southern switzerland founded dalle molle foundation promoted quality life. traditional methods computer vision machine learning cannot match human performance tasks recognition handwritten digits trafﬁc signs. biologically plausible deep artiﬁcial neural network architectures can. small receptive ﬁelds convolutional winnertake-all neurons yield large network depth resulting roughly many sparsely connected neural layers found mammals retina visual cortex. winner neurons trained. several deep neural columns become experts inputs preprocessed different ways; predictions averaged. graphics cards allow fast training. competitive mnist handwriting benchmark method ﬁrst achieve near-human performance. trafﬁc sign recognition benchmark outperforms humans factor two. also improve state-of-the-art plethora common image classiﬁcation benchmarks. recent publications suggest unsupervised pre-training deep hierarchical neural networks improves supervised pattern classiﬁcation train nets simple online back-propagation setting greatly improved records mnist latin letters chinese characters trafﬁc signs norb cifar benchmarks. focus deep convolutional neural networks introduced improved reﬁned simpliﬁed lately proved mettle data sets ranging handwritten digits handwritten characters toys faces dnns fully unfold potential deep training requires weeks months even years cpus. high data transfer latency prevents multi-threading multi-cpu code saving situation. recent years however fast parallel neural code graphics cards overcome problem. carefully designed code image classiﬁcation orders magnitude faster counterpart hence train huge hours days implement building upon work training algorithm fully online i.e. weight updates occur error back-propagation step. show properly trained deep dnns outperform previous methods demonstrate unsupervised initialization/pretraining necessary also show combining several columns multi-column decreases error rate initially random weights iteratively trained minimize classiﬁcation error labeled training images; generalization performance tested separate test images. architecture combining several techniques novel unlike shallow used many applications deep inspired neocognitron many layers non-linear neurons stacked other comparable number layers found retina visual cortex macaque monkeys shown multi-layered hard train standard gradient descent method choice mathematical/algorithmic point view. today’s computers however fast enough this times faster early carefully designed code massively parallel graphics processing units allows gaining additional speedup factor serial code standard computers. given enough labeled data networks need additional heuristics unsupervised pre-training carefully prewired synapses paper -dimensional layers winner-take-all neurons overlapping receptive ﬁelds whose weights shared given input pattern simple pooling technique determines winning neurons partitioning layers quadratic regions local inhibition selecting active neuron region. winners layer represent smaller down-sampled layer lower resolution feeding next layer hierarchy. approach inspired hubel wiesel’s seminal work cat’s primary visual cortex identiﬁed orientation-selective simple cells overlapping local receptive ﬁelds complex cells performing down-sampling-like operations note point down-sampling automatically leads ﬁrst -dimensional layer. trivial -dimensional winner-take-all regions possible part hierarchy becomes standard multi-layer perceptron receptive ﬁelds winner-take-all regions often minimal e.g. neurons. results maximal depth layers non-trivial winner-take-all regions. fact insisting minimal ﬁelds automatically deﬁnes entire deep architecture apart number different convolutional kernels layer depth plain top. winner neurons trained neurons cannot forget learnt although affected weight changes peripheral layers. resulting decrease synaptic changes time interval corresponds biologically plausible reduction energy consumption. training algorithm fully online i.e. weight updates occur gradient computation step. inspired microcolumns neurons cerebral cortex combine several columns form multi-column given input pattern predictions columns democratically averaged. training weights columns randomly initialized. various columns trained inputs inputs preprocessed different ways. latter helps reduce error rate number columns required reach given accuracy. mcdnn architecture training testing procedures illustrated figure following give detailed description experiments performed. evaluate architecture various commonly used object recognition benchmarks improve state-of-the-art them. description architecture used various experiments given following xx-c-mp-c-mp-c-mp-n-n-n represents input images size convolutional layer maps ﬁlters max-pooling layer figure architecture. mcdnn architecture. input image preprocessed blocks. arbitrary number columns trained inputs preprocessed different ways. ﬁnal predictions obtained averaging individual predictions dnn. training dnn. dataset preprocessed training then beginning every epoch images distorted text explanations. overlapping regions size convolutional layer maps ﬁlters max-pooling layer overlapping regions size fully connected layer hidden units fully connected layer hidden units fully connected output layer neurons scaled hyperbolic tangent activation function convolutional fully connected layers linear activation function max-pooling layers softmax activation function output layer. trained using on-line gradient descent annealed learning rate. training images continually translated scaled rotated whereas original images used validation. training ends validation error zero learning rate reaches predetermined minimum. initial weights drawn uniform random distribution range original mnist digits normalized width height bounding equals pixels. aspect ratios various digits vary strongly therefore create additional datasets normalizing digit width pixels. like seeing data different angles. train columns normalization resulting total columns entire mcdnn. xx-c-mp-c-mp-n-n trained around epochs annealed learning rate training takes almost hours training epochs little additional improvement observed. training digits randomly distorted epoch internal state single depicted figure particular digit forward propagated trained network activations together network weights plotted. figure handwritten digits training distorted versions epoch errors mcdnn correct label ﬁrst second best predictions architecture mnist. output layer drawn scale; weights fully connected layers displayed. results individual nets various mcdnn summarized table mcdnn nets trained preprocessor achieve better results constituent dnns except original images mcdnn error rate improving state least ﬁrst time artiﬁcial method comes close error rate humans task many wrongly classiﬁed digits either contain broken strange strokes wrong labels. errors associated correct second guesses. also trained single datasets simultaneously yielded worse result mcdnn individual dnn. shows improvements come mcdnn using preprocessed data. mcdnn errors affected number preprocessors? train dnns datasets. mcdnn out-of-’ averages nets trained datasets. table shows preprocessing results lower mcdnn error. published result total characters test many easy classify digits hard classify letters explains lower overall error rate -class problem compared -class letters problem. errors -class problem digits misclassiﬁed letters misclassiﬁed. letters general difﬁcult classify also higher amount confusion similar lowerupper-case letters example. indeed error rates case insensitive task drop confused upperlower-case classes merged resulting different classes error rate slightly bigger upper-case letters easier classify lowercase letters smaller writer dependent in-class variability. detailed analysis errors confusions different classes confusion matrix informative compared latin character recognition isolated chinese character recognition much harder problem mainly much larger category also wide variability writing styles confusion similar characters. dataset institute automation chinese academy sciences contains samples characters resulted data million characters posed major computational challenge even system. without fast implementation nets task would train year. forward propagation training takes normal training single epoch would consequently lasted several days. fast implementation hand training single epoch takes makes feasible train within days instead many months. pixels place center image. contrast image normalized independently. suggested organizers ﬁrst writers database casia-hwdb. used training remaining writers used testing. total numbers training test characters respectively. online dataset draw character list coordinates resize resulting images pixels place center image. additionally smooth-out resulting images gaussian blur ﬁlter pixel neighborhood uniform standard deviation suggested organizers characters writers database casia-olhwdb. used training classiﬁer characters remaining writers used testing. resulting numbers training test characters respectively. methods previously applied dataset perform feature extraction followed dimensionality reduction whereas method directly works pixel intensities learns feature extraction dimensionality reduction supervised way. ofﬂine task obtain error rate compared best method even though much information lost drawing character it’s coordinate sequence obtain recognition rate online task compared best method conclude hard classiﬁcation problem many classes relatively samples class fully supervised beats current state-of-the-art methods large margin. recognizing trafﬁc signs essential automotive industry’s efforts ﬁeld driver’s assistance many trafﬁc-related applications. gtsrb trafﬁc sign dataset original color images contain trafﬁc sign each border around sign. vary size pixels necessarily square. actual trafﬁc sign always centered within image; bounding part annotations. training consists images; test images. crop images process within bounding box. implementation requires training images equal size. visual inspection image size distribution resize images pixels. consequence scaling factors along axes different trafﬁc signs rectangular bounding boxes. resizing forces square bounding boxes. mcdnn artiﬁcial method outperform humans produced twice many errors. since trafﬁc signs greatly vary illumination contrast standard image preprocessing methods used enhance/normalize dataset trained resulting mcdnn columns achieving error rate test set. figure depicts errors plus ground truth ﬁrst second predictions. errors associated correct second predictions. erroneously predicted class probabilities tend low—here mcdnn quite unsure classiﬁcations. general however conﬁdent—most predicted class probabilities close zero. rejecting percent images results even lower error rate reach error rate images rejected method outperforms second best algorithm factor takes hours train mcdnn columns four gpus. trained mcdnn check images second table error rates averages standard deviations runs layer cifar test set. nets ﬁrst trained preprocessed images whereas second trained original images. centered contain parts object show different backgrounds. subjects vary size order magnitude colors textures objects/animals also vary greatly. input layers three maps color channel -layer architecture small kernels xx-c-mp-c-mp-c-mp-c-mp-n-nn. like mnist initial learning rate decays factor every epoch. transforming cifar color images gray scale reduces input layer complexity increases error rates. hence stick original color images. mnist augmenting training randomly translated images greatly decreases error additional scaling rotation translation individual errors decrease another small maximal bounds prevent loss much information leaked beyond pixels rectangle. figure confusion matrix cifar mcdnn correct labels vertical axis; detected labels horizontal axis. square areas proportional error numbers shown relative percentages total error number absolute value. left images birds classiﬁed planes. right images planes classiﬁed birds. confusion sub-matrix animal classes gray backround. repeat experiment different random initializations compute mean standard deviation error rather small original images showing robust. mcdnn obtains error rate greatly rising benchmark. confusion matrix shows mcdnn almost perfectly separates animals artifacts except planes birds seems natural although humans easily distinguish almost incorrectly classiﬁed images even many cluttered contain parts objects/animals many confusions different animals; frog class collects false positives animal classes false negatives. expected cats hard tell dogs collectively causing errors. test mcdnn four columns norb collection stereo images models objects centrally placed randomly chosen backgrounds also cluttering peripherally placed second object. database designed experimenting object recognition shape. contains images toys belonging generic categories four-legged animals human ﬁgures airplanes trucks cars. objects imaged cameras lighting conditions elevations azimuths training folds images total images; testing consists folds totalizing images. preprocessing used dataset. scale images original pixels. size enough preserve details present images small enough allow fast training. perform rounds experiments using ﬁrst folds using training data. tested several distortion parameters small nets found maximum rotation maximum translation maximum scaling good choices hence norb experiments. compare previous results ﬁrst train ﬁrst -folds data. architecture deep maps layer xx-c-mp-c-mp-c-mp-n-n-n. learning rate setup start factor stop small size training fast s/epoch epochs. testing sample requires .ms. even less data train mcdnn greatly improves state method fast enough process entire training though. architecture double number maps training folds xx-c-mp-c-mp-cmp-n-n-n. learning rate setup remains same. training time increases min/epoch bigger times data. testing sample takes .ms. pays resulting error rate improving state art. although norb classes training test instances sometimes differ greatly making classiﬁcation hard. errors confusions cars trucks. considering second predictions error rate drops showing errors associated correct second prediction. ﬁrst time human-competitive results reported widely used computer vision benchmarks. many image classiﬁcation datasets mcdnn improves state-of-the-art drastically improve recognition rates mnist nist chinese characters trafﬁc signs cifar norb. method fully supervised additional unlabeled data source. single already sufﬁcient obtain state-of-the-art results; combining mcdnns yields dramatic performance boosts. work partially supported fp-ict-- grant project code neurodynamic framework cognitive robotics scene representations behavioral sequences learning.", "year": 2012}