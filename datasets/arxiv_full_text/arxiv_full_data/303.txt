{"title": "CoDraw: Visual Dialog for Collaborative Drawing", "tag": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "abstract": "In this work, we propose a goal-driven collaborative task that contains vision, language, and action in a virtual environment as its core components. Specifically, we develop a collaborative `Image Drawing' game between two agents, called CoDraw. Our game is grounded in a virtual world that contains movable clip art objects. Two players, Teller and Drawer, are involved. The Teller sees an abstract scene containing multiple clip arts in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip arts. The two players communicate via two-way communication using natural language. We collect the CoDraw dataset of ~10K dialogs consisting of 138K messages exchanged between a Teller and a Drawer from Amazon Mechanical Turk (AMT). We analyze our dataset and present three models to model the players' behaviors, including an attention model to describe and draw multiple clip arts at each round. The attention models are quantitatively compared to the other models to show how the conventional approaches work for this new task. We also present qualitative visualizations.", "text": "figure overview proposed collaborative drawing task. game consists players teller drawer. teller sees abstract scene drawer sees empty canvas. players need collaborate communicate drawer reconstruct image teller dragging dropping clip objects. iors e.g. natural language understanding generation grounding language perceptual input grounding execution statements user action spaces environment ﬁrst step towards goal propose collaborative drawing game incorporates requirements simple intuitively uniﬁed task. task involves perception communication actions partially observable virtual environment. shown figure game grounded virtual world constructed clip objects players teller drawer play game. teller sees abstract scene made clip objects semantically meaningful conﬁguration drawer sees drawing canvas initialized empty canvas. players need collaborate natural language communication drawer reconstruct image teller. work propose goal-driven collaborative task contains vision language action virtual environment core components. speciﬁcally develop collaborative ‘image drawing’ game agents called codraw. game grounded virtual world contains movable clip objects. players teller drawer involved. teller sees abstract scene containing multiple clip arts semantically meaningful conﬁguration drawer tries reconstruct scene empty canvas using available clip arts. players communicate two-way communication using natural language. collect codraw dataset dialogs consisting messages exchanged between teller drawer amazon mechanical turk analyze dataset present three models model players’ behaviors including attention model describe draw multiple clip arts round. attention models quantitatively compared models show conventional approaches work task. also present qualitative visualizations. training goal-driven agents interact humans natural language take actions grounded environments fundamental goals artiﬁcial intelligence. this need agent perceive environment understand language communicate proactively comprehend goal precisely ﬁnally decide actions take. consider following simple interaction human ‘can recommend shirt similar wearing now?’ ‘okay {shirt} recommend. would like purchase it?’ human ‘yes please’ ‘placed order {shoppingapp}’. despite simplicity short dialog requires various kinds intelligent behavnotice reconstruction challenging. teller drawer must ground words describe draw accordingly drawer must infer perceptual properties clip teller’s message information asymmetry neither agents other’s canvas makes communication necessary component build mental models canvas interlocutor. quality reconstruction quantitatively measured similarity reconstructed original image using metric described section proposed game contains several known vision language tasks sub-problems though subtle differences. instance compared image captioning visual question answering proposed task involves multiple rounds interaction. agents hold partially observable states need build mental model collaborate. compared visual dialog tasks agents cooperate change environment actions thus agents possess ability adapt hold dialog novel scenes constructed consequence dialog. note proposed task well-deﬁned goal facilitates objective measurement success enables end-to-end goaldriven learning. table summarizes comparison. task presents challenges around vision-language grounding coupled learning sequentially environment. figure shows example codraw dataset. contributions. paper propose collaborative drawing task contains vision language action core components. collect codraw dataset variable-length dialogs consisting messages drawing history. also propose novel attentional mechanism describe draw multiple clip arts round mimic human drawings. attention model compared baselines illustrate capability limitation help investigations task along qualitative visualizations. structure dialog pre-speciﬁed individual slots replaced relevant information. recently end-to-end neural models also proposed goaldriven dialog well goal-free dialog ‘chitchat’ unlike codraw approaches symbols dialog grounded visual objects. visually grounded dialog. generalized visual question answering visually-grounded dialog image referential games proposed. tasks agents must understand image answer sequence relevant questions refer right image based conversation. this agent must deal visual grounding memory dialog history modeling long-term consistency regarding response etc. however general agents change state environment language grounded environments. learning language games change environment studied agent change environment using grounded natural language. however agents cooperate. language grounding also studied robot navigation manipulation environment mapping however works manually pair command robot actions lack end-to-end training dialog communications. seminal works propose cooperative games agents sender receiver share common interests different roles. speciﬁcally sender observes world state sends signals receiver cannot directly perceive state. receiver takes actions accordingly. setting social interaction naturally emerges solution coordination. commnet learns continuous multiagent communication cooperative tasks end-to-end supervised reinforcement learning setting. show communication among agents improves performance downstream task. similarly reinforced differentiable inter-agent learning proposed multiagent cooperative learning. end-to-end approaches learned ‘language’ immediately interpretable humans. moreover visual grounding language solve common goal sufﬁciently explored. synthetic minimize potential biases present real datasets enable in-depth investigation speciﬁc problem interest another example babi tasks investigate reasoning language understanding. abstract scenes made clip arts used focus high-level semantics image descriptions visual grounding language visual question answering zero-shot learning work abstract scenes study dynamic visual collaborative task. figure example collaborative drawing dataset. images depict drawer’s drawing canvas right round conversation. left right rounds last round ground truth scene shown teller drawer reconstruct. corresponding dialog teller drawer shown below. overall scores automatically quantify close drawer’s ﬁnal scene ground truth teller’s image also shown section describe dataset collection procedure present analysis codraw dataset. note collecting dataset nature involves sophisticated infrastructure including live chat workers amazon mechanical turk well live drawing interface drawer. dataset well infrastructure made publicly available. abstract scenes. enable workers draw semantically rich scenes canvas easily leverage abstract scenes dataset zitnick parikh dataset consists semantically consistent scenes created human subjects amazon mechanical turk. scenes contain objects scenes depict children playing park made library clip arts including girl poses expressions various objects including trees toys hats animals food etc. abstract scene created dragging dropping multiple clip objects position canvas. also clip different spatial transformations applied including three levels scales three levels depths orientations total transformations. teller shown scenes dataset task drawer recreate scene communicating teller using natural language. workers receive rewards reconstruction accurate. collection procedure. built drag-and-drop interface based visual dialog chat interface shown figure workers involved data collections. conversation teller describes either teller drawer start conversation. side allowed send message wait hand reply. prevent excessively lengthy descriptions scene teller maximal length single message characters. drawer updates canvas based teller’s messages updating clip arts. drawer also questions clariﬁcation conversation. appendix details user interface instructions workers. workers asked submit task conﬁdent drawer accurately reconstructed scene teller. workers accomplish task good quality given bonus payment. average took workers minutes complete task chat history workers well move made drawer. details similarity metric well qualitative inspection found section figure shows example dataset. teller starts conversation describing target scene. drawer asks position trees size sun. teller answers questions misses others. additional interaction. language-based communication effective exchanging high-level ideas details right direct input visual information could constructive. this give chance teller peek drawer’s canvas using ‘peek’ button interface. communication allowed peek window closed. please appendix details. note although additional signal could lead interesting behaviors leave analysis future work. scene similarity metric. deﬁne scene similarity metric quantitatively assess similarity abstract scenes ground-truth abstract scene distribution number tokens teller’s drawer’s messages. notice number single-token messages drawer medians teller drawer respectively. distribution numbers conversational rounds. median rounds. distribution duration dialog sessions. median minutes. denotes intersection union clip i.e. number clip objects scenes common divided total number objects scenes combined. second term measures aspects perceptual differences direction person pose depth absolute position relative position clip objects. pose non-person clip arts ignored metric. overall score bounded weights balance terms respectively. please appendix details. important detail score visible workers collecting dataset. scheme ensures workers accomplish task communication optimize actions maximize metric. collect dialogs consisting total utterances. split dataset train test messages. shown figure distribution number tokens drawer’s message skewed toward passive replies like done etc. exist heavy tail shows drawers clarifying questions scene like where trunk second tree high. feedbacks drawer help improve estimation game status teller. hand distribution teller’s relatively smooth long tails. size vocabulary since subject conversations abstract scenes limited number clip arts vocabulary relatively small compared real images. rounds. figure shows distribution numbers conversational rounds dialog sessions. interactions shorter rounds median durations. figure median session duration minutes. placed -minute maximum limit session. scores. figure shows distribution score. notice scores peek chance. score effect peek shown figure challenges. note dataset presents multiple challenges beyond image language understanding tellers tend describe multiple aspects message efﬁciency drawers make change scene round moreover drawer update drawing right away. often wait clear picture needs created making signiﬁcant change scene. agents succeed codraw task would deal challenges. setup model teller drawer evaluate performance team jointly quality reconstructed scene utterance teller utterance drawer utterance drawer generated using utterance teller talk content-speciﬁc e.g. teller pick salient clip art. call method seqatt. train model dataset conversation rounds drawer changes single clip art. overall percents conversational rounds used training. details refer appendix note even rounds associated messages might describe relationships clip dynamic multi-attention model conversation history make attention content-aware propose dynamic multi-attention model gives memory teller drawer models. loss function written following simple baseline pick frequent clip train split place using modes categorical properties median positions. figure shows mode drawing. contains jenny mike cloud bushy tree dog. notice bushy tree occluding cloud common individual realistic instances scenes. extract features target image rc×p using following feature extraction procedure. feature matrix total number clip arts total feature dimension clip art. i-th feature concatenation pieces information extracted clip whose ﬁrst pieces ec-dimensional embeddings categorical values clip pose-expression combination depth direction note pose used person clip arts. last pieces positions encoded er-dimensional vectors. ﬁnal feature obtained concatenating pieces together yielding total dimension note clip corresponding rows zeros. ﬁrst approach model conversation follows following pre-deﬁned content-independent random order teller sequentially selects single clip time generate utterance drawer receives message updates canvas generates reply denote attention clip arts since attention dependent memory attention distribution changes conversation moves forwards. notice attention mechanism inspired word embedding rn×v parameters convolutional ﬁlters convolution learnable. bias terms omitted readability. training. utterances learned supervised manner cross-entropy mimic human utterances dataset ﬁrst epochs that output utterances teller drawer used ﬁne-tune validation score drawing result. testing model output utterances teller drawer. receives supervision indicator vector updated clip arts dataset t-th round ground-truth. then generate actions clip ground truth {i|α instead estimate evaluation used predicted attention rather ground-truth ones. {k|ˆα pick clip arts draw. notice learnable vector dedictated index clip generate actor aggregation linear models generate properties clip art. notice function called times update multiple clip arts described utterance teller learnable parameters. parameters shared teller model. memory states last hidden state vectors separate lstms. conversation starts initialized zero. error backpropagated rounds. figure show schematic diagram dynatt model. section evaluate multiple baselines proposed models. evaluation metric proposed equation includes scores measuring different quality aspects overall score. overall score iou. baseline mode drawing model gives overall score ﬁxed clip arts relatively higher random samplings reﬂects bias distribution abstract images seqatt achieves better performance overall compared mode despite trained subset dataset. hand dynatt signiﬁcantly improves mode seqatt result dynatt achieves better performance overall seqatt att-nojoint model without joint attention loss inferior dynatt joint loss. table ﬁnal scores penalties test split codraw dataset. dynatt-nj dynatt joint attention training. averaged scores randomly-initialized models reported. score overall quality scene rest unweighted error measures better). directional error pose pose expression error person clip arts depth depth error positional error relx rely relative positional error. please section appendix details. performance conversations proceeds. dynatt models advantage seqatt models early stage shown figure attention mechanism describe draw multiple clip arts convey information early rounds. figure shows attention values rounds. round teller’s message ’the facing girl’ drawer updates jenny owl. round slide updated along person clip arts. teller’s attention slowly changes rainy cloud bushy tree drawn round incrementally decreased visualization shows contributors drawing reproduction along limitations model figure appendix visualization shows dynatt biased toward mode x-position persists even version result indicates actor function equation needs improved decode corresponding actions. model limitations. seqatt shows stronger overall performance despite simplicity. however limitations. perceptual similarity involves various visual aspects. pose expressions person clip arts play role conveying semantics sequential nauture natural language quite relevant note clip crisply delineated representation scene. applying techniques real image would need additional extract semantics information feed system focus paper. leave future work. figure overall score test varies conversation advances different methods. error bars indicate standard deviation across randomly initalized models. seqatt although simple shows strong performance. dynatt works decently well aspects e.g. depth relx work well. particular performance relx rely worse seqatt. dynatt allows attention multiple clip arts described teller multiple actions decoded drawer. complex interaction often fails results prediction average coordinate values. sentence representations. similar previous works also using words encoders decoders agents yields comparable performance compared lstms. interestingly seqatt achieves outperforms models however dynatt not. since teller seqatt generates concise utterances specify name clip arts position drawer seqatt generates action single clip art. dynatt sufﬁcient model change human performance. human outperforms baseline models whose showing human drawers capture majority clip arts described tellers overall score humans signiﬁcantly outperform models capturing pose expressions mike jenny. seqatt generates utterances using concise expression strong performance limitations simple language model dynatt hand tries mimic attention shift humans across rounds dialog well drawing behaviors found collected dataset. although cannot effectively generate detailed realistic drawings involving rich interactions multiple clip arts hope work spur future efforts direction. note could hand-craft rule-based system task rule-based teller seem natural human drawer importantly drawer work rule-based teller would work human teller. hence work train neural drawers tellers mimic human interaction. pairing trained agents humans part future work. propose collaborative drawing task involves vision-grounded action-grounded communication agents asymmetric information working towards common goal. collect codraw figure attention values teller drawer dynatt model shown. values correspond rounds figure line indicates threshold clip drawn please text section dataset containing dialogs consisting messages human shown semantically rich abstract scene made clip objects shown blank canvas clip objects dragged must communicate drawer re-create scene teller sees. present analysis dataset propose three neural drawer teller models. avenues future work include modeling theory mind reinforcement learning help agents discover novel strategies explored humans dataset.", "year": 2017}