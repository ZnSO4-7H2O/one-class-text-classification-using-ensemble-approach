{"title": "Unsupervised Learning for Lexicon-Based Classification", "tag": ["cs.LG", "cs.CL", "stat.ML", "I.2.6; I.2.7"], "abstract": "In lexicon-based classification, documents are assigned labels by comparing the number of words that appear from two opposed lexicons, such as positive and negative sentiment. Creating such words lists is often easier than labeling instances, and they can be debugged by non-experts if classification performance is unsatisfactory. However, there is little analysis or justification of this classification heuristic. This paper describes a set of assumptions that can be used to derive a probabilistic justification for lexicon-based classification, as well as an analysis of its expected accuracy. One key assumption behind lexicon-based classification is that all words in each lexicon are equally predictive. This is rarely true in practice, which is why lexicon-based approaches are usually outperformed by supervised classifiers that learn distinct weights on each word from labeled instances. This paper shows that it is possible to learn such weights without labeled data, by leveraging co-occurrence statistics across the lexicons. This offers the best of both worlds: light supervision in the form of lexicons, and data-driven classification with higher accuracy than traditional word-counting heuristics.", "text": "lexicon-based classiﬁcation documents assigned labels comparing number words appear opposed lexicons positive negative sentiment. creating words lists often easier labeling instances debugged non-experts classiﬁcation performance unsatisfactory. however little analysis justiﬁcation classiﬁcation heuristic. paper describes assumptions used derive probabilistic justiﬁcation lexicon-based classiﬁcation well analysis expected accuracy. assumption behind lexicon-based classiﬁcation words lexicon equally predictive. rarely true practice lexicon-based approaches usually outperformed supervised classiﬁers learn distinct weights word labeled instances. paper shows possible learn weights without labeled data leveraging co-occurrence statistics across lexicons. offers best worlds light supervision form lexicons data-driven classiﬁcation higher accuracy traditional word-counting heuristics. lexicon-based classiﬁcation refers classiﬁcation rule documents assigned labels based count words lexicons associated label example suppose opposed labels associated lexicons document vector word counts lexicon-based decision rule lexicon-based classiﬁcation widely used industry academia applications ranging sentiment classiﬁcation opinion mining psychological ideological analysis texts popularity approach explained relative simplicity ease domain experts creating lexicons intuitive comparison labeling instances offer faster path towards reasonably however machine learning perspective number drawbacks lexicon-based classiﬁcation. first intuitively reasonable lexicon-based classiﬁcation lacks theoretical justiﬁcation clear conditions necessary work. second lexicons incomplete even designers strong substantive intuitions. third lexicon-based classiﬁcation assigns equal weight word words strongly predictive others. fourth lexiconbased classiﬁcation ignores multi-word phenomena negation discourse supervised classiﬁcation systems trained labeled examples tend outperform lexicon-based classiﬁers even without accounting multi-word phenomena several researchers proposed methods lexicon expansion automatically growing lexicons initial seed also work handling multi-word phenomena negation discourse however theoretical foundations lexiconbased classiﬁcation remain poorly understood lack principled means automatically assigning weights lexicon items without resorting labeled instances. paper elaborates assumptions lexicon-based classiﬁcation equivalent na¨ıve bayes classiﬁcation. derive expected error rates assumptions. expected error rates matched observations real data suggesting underlying assumptions invalid. importance assumption lexicon item equally predictive. relax assumption derive principled method estimating word probabilities label using method-of-moments estimator cross-lexical co-occurrence counts. some lexicons attach coarse-grained predeﬁned weights word. example opinionfinder subjectivity lexicon labels words strongly weakly subjective poses additional burden lexicon creator. opposite label parameter controls predictiveness lexicon example sentiment classiﬁcation problem would indicate words positive sentiment lexicon three times likely appear documents positive sentiment documents negative sentiment vice versa. word atrocious might less likely overall good still three times likely negative class positive class. limit implies lexicons distinguish classes implies lexicons distinguish classes perfectly observation single in-lexicon word would completely determine document label. begin showing lexicon-based classiﬁcation rule shown derived special case na¨ıve bayes classiﬁcation. suppose prior probability label likelihood function px|y random variable corresponding vector word counts. conditional label probability computed bayesian inversion introduce assumptions likelihood function px|y random variable deﬁned vectors counts natural choice form likelihood multinomial distribution corresponding multinomial na¨ıve bayes classiﬁer. speciﬁc vector counts write pmultinomial probability vector associated label total count tokens count word multinomial likelihood proportional product likelihoods categorical variables corresponding individual words ﬁnite positive constant. therefore identical counting-based classiﬁcation rule words lexicon-based classiﬁcation minimum bayes risk classiﬁcation multinomial probability model assumptions equal prior likelihood lexicon completeness equal predictiveness words equal coverage. advantage deriving formal foundation lexiconbased classiﬁcation possible analyze expected performance. label write count count in-lexicon words pi∈wy lexiconopposite-lexicon words pi∈w¬y based classiﬁcation makes correct prediction whenever correct label assess likelihood sufﬁcient compute expectation variance difference m¬y; central limit theorem treat difference approximately normally distributed compute probability difference positive using gaussian cumulative distribution function recall already taken assumption sums baseline word probabilities lexicons equal. multinomial probability model given document tokens expected counts bounds fairly tight products probθiθj nearly always small abilities words rare. covariance fact negative bound variance margin obtaining upper bound central limit theorem margin approximately normally distributed mean variance upper-bounded probability making correct prediction equal cumulative density standard normal distribution z-score equal ratio expectation standard deviation according approximation accuracy expected increase predictiveness document length lexicon coverage helps explain dilemma lexicon design words added coverage increases average predictiveness word decreases thus increasing size lexicon adding marginal words improve performance. analysis also predicts longer documents easier classify. expected size grows standard deviation grows prediction tested empirically four datasets considered paper false longer documents harder classify accurately. clue underlying assumptions valid. decreased accuracy especially long reviews reviews complex perhaps requiring modeling discourse structure apply equality prior becomes increasingly diffuse. repeated counts word better explained document-speciﬁc variation prior properties label. situation shown figure plots effective counts implied classiﬁcation rule range values concentration parameter holding parameters constant high values effective counts track observed counts linearly multinomial model; values effective counts barely increase beyond minka presents number estimators concentration parameter corpus text. label unknown cannot apply estimators directly. however described above out-of-lexicon words assumed identical probability labels. assumption exploited estimate exclusively ﬁrst second moments out-of-lexicon words. analysis expected accuracy model left future work. crucial simpliﬁcation made lexicon-based classiﬁcation words lexicon equally predictive. reality words less predictive class labels reasons sense ambiguity degree introducing per-word predictiveness factor arrive model restricted form na¨ıve bayes. labeled data available model could estimated maximum likelihood. section shows estimate model without labeled data using method moments. first note baseline probabilities estimated directly counts unlabeled corpus; challenge estimate parameters words lexicons. intuition makes possible highly predictive words rarely appear words opposite lexicon. idea formalized delta function returns boolean condition true zero otherwise. context supervised classiﬁcation pang vaithyanathan word presence predictive feature word frequency. ignoring repeated mentions word heuristic emphasizes diversity ways document covers lexicon robust document-speciﬁc idiosyncrasies review luck club might include positive words luck many times even review negative. word-appearance heuristic also explained framework deﬁned above. multinomial likelihood px|y replaced dirichlet-compound multinomial distribution also known multivariate polya distribution distribution written pdcm vector parameters associated label compound distribution treats parameter multinomial latent variable marginalized intuitively think distribution encoding model document multinomial distribution words; document-speciﬁc distribution drawn prior depends class label suppose parameter deﬁned constant concentration distribution grows probability distribution closely concentrated around prior expecθi likelihood function penalty parameter. augmented lagrangian biconvex suggests iterative solution speciﬁcally hold ﬁxed solve subject solve conditions. finally update dual variable representing extent equality constraint violated. updates iterated convergence. unconstrained local updates computed solving system linear equations result projected back onto feasible region. penalty parameter initialized dynamically updated based primal dual residuals details available appendix online source code. empirical evaluation performed four datasets languages. datasets involve binary classiﬁcation problems performance quantiﬁed area-under-thecurve measure classiﬁcation performance robust unbalanced class distributions. perfect classiﬁer achieves expectation random decision rule gives datasets proposed method relies co-occurrence counts therefore best suited documents containing least sentences each. mind following datasets used evaluation amazon english-language product reviews across four domains; reviews labeled another unlabeled cornell english-language reviews corpuscine spanish-language movie reviews rated scale ratings four considered positive; ratings considered negative. reviews rating three excluded. write indicate vector parameters expectation linear function linear function vector analogously linear function goal choose expectations closely match observed counts viewed form method moments estimation following objective lexicons preliminary several english-language sentiment lexicons. lexicon consistently obtained best performance three english-language datasets made focus subsequent experiments. ribeiro also found lexicon strongest lexicons review analysis. spanish data isol lexicon used modiﬁed translation lexicon. classiﬁers evaluation compares following unsupervised classiﬁcation strategies lexicon basic word counting decision rule lex-presence counting word presence rather freproblex-dcm probabilistic lexicon-based classiﬁcation using dirichlet compound multinomial likelihood reduce effective counts repeated words; alternative approach discussed related work impute document labels seed words compute sentiment scores individual words pointwise mutual information words imputed labels implementation method based description kiritchenko mohammad using lexicons seed word sets. upper bound supervised logistic regression classiﬁer also considered. classiﬁer trained using ﬁvefold cross validation. classiﬁer access training data. problex-mult problexdcm methods lexicon words co-occur opposite lexicon greater chance frequency eliminated lexicon preprocessing step. results results shown table superior performance logistic regression classiﬁer conﬁrms principle supervised classiﬁcation accurate lexicon-based classiﬁcation. therefore supervised classiﬁcation preferred labeled data available. nonetheless probabilistic lexicon-based classiﬁers developed paper considerable towards closing improvements ranging less corpuscine data nearly imdb data. approach performs poorly improving simpler lexicon-based classiﬁers four datasets. word presence heuristic offers consistent improvements bayesian adjustment classiﬁcation rule offers modest improvements four datasets. turney uses pointwise mutual information estimate semantic orientation vocabulary words co-occurrence small seed set. approach later extended social media domain using emoticons seed like approach proposed here basic intuition leverage co-occurrence statistics learn weights individual words; however heuristic score justiﬁed probabilistic model text classiﬁcation problem. pmi-based classiﬁcation underperforms problex-mult problex-dcm four datasets evaluation. method-of-moments become increasingly popular estimator unsupervised machine learning applications topic models sequence models elaborate linguistic structures particular relevance anchor word techniques learning latent topic models methods topic deﬁned ﬁrst keywords assumed generated single topic. anchor words co-occurrence statistics topic-word probabilities recovered. difference strong anchor word assumption required work none words assumed perfectly predictive either label. require much weaker assumption words lexicon tend co-occur less frequently words opposite lexicon. lexicon-based classiﬁcation popular heuristic previously analyzed machine learning perspective. analysis yields techniques improving unsupervised binary classiﬁcation method-of-moments estimator word predictiveness bayesian adjustment repeated counts word. method-ofmoments estimator yields substantially better performance conventional lexicon-based classiﬁcation without requiring additional annotation effort. future work consider generalization multi-class classiﬁcation ambitiously extension multiword units. acknowledgment research supported national institutes health award number rgm- force ofﬁce scientiﬁc research. problem biconvex parameters optimize using alternating direction method multipliers remainder document used indicate product used indicate elementwise product. constrained solution update must within constraint sets ensure apply boundary-constrained l-bfgs augmented lagrangian equation solution requires gradient simply slightly faster solution apply admm again using following iterative updates", "year": 2016}