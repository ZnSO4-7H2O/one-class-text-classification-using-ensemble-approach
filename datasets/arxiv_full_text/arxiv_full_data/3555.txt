{"title": "Guided Labeling using Convolutional Neural Networks", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Over the last couple of years, deep learning and especially convolutional neural networks have become one of the work horses of computer vision. One limiting factor for the applicability of supervised deep learning to more areas is the need for large, manually labeled datasets. In this paper we propose an easy to implement method we call guided labeling, which automatically determines which samples from an unlabeled dataset should be labeled. We show that using this procedure, the amount of samples that need to be labeled is reduced considerably in comparison to labeling images arbitrarily.", "text": "last couple years deep learning especially convolutional neural networks become work horses computer vision. limiting factor applicability supervised deep learning areas need large manually labeled datasets. paper propose easy implement method call guided labeling automatically determines samples unlabeled dataset labeled. show using procedure amount samples need labeled reduced considerably comparison labeling images arbitrarily. deep learning gained interest last years methods perform well wide range machine learning tasks. class especially successful deep learning methods convolutional neural networks image classiﬁcation. unfortunately cnns need large amount labeled training data perform well. many cases labeling performed humans. common approach form crowd based labeling. example amazon mechanical turk used labeling imagenet dataset data also obtained side effect human interaction online system. example captcha challenges prevent bots using online services produce labeled data side effect veriﬁcation procedure alas simply labeling available samples inefﬁcient human labor since samples equal value. hand adding sample similar samples already dataset usefull. hand cases classes difﬁculty might make sense samples difﬁcult classes dataset. therefore would advantageous label sample would maximize classiﬁcation accuracy system. unfortunately much quality dataset would increase adding speciﬁc labeled sample determined labeling training resulting dataset. obviously useful want decide samples labeled ﬁrst place. propose classiﬁcation conﬁdence trying predict class unlabeled images decide label next. proposed procedure together extensive data augmentation evaluated small neural networks mnist cifar dataset. practice propose following workﬂow small labeled dataset used train neural network used select batch confusing images unlabeled data. batch given human workers label images added training dataset process repeats. call procedure guided labeling. hypothesis that using procedure able trade human computational resources. idea system decide data wants learn generally called active learning. survey ﬁeld published settles. concept active learning already used angluin although case samples labeled chosen preexisting unlabeled dataset synthetically generated learner itself. method currently feasible image classiﬁcation. method shown baum lang work poorly handwritten character recognition. active learning small amount labeled data larger ﬁxed unlabeled data available generally called pool–based sampling ﬁrst presented lewis learning text classiﬁers. pool–based sampling used task image classiﬁcation tong using support vector machines. pooling size dropout layer dropout probability fully connected layer output neurons dropout layer dropout probability fully connected layer output neurons output layer. training dataset randomly augmented training epoch. thus network never sees identical images training. mnist dataset following augmentations performed rotation range applied image scaled range image sheared axis range pixels random elastic distortion presented simard applied image cropped back original size pixels necessary. cifar dataset images randomly mirrored along vertical axis rotated range scaled range cropped original size pixels. measuring confusion network following work park machine learning methods response distribution entropy classiﬁcation conﬁdence measure. feeding sample classiﬁcation network gives probability distribution possible classes called response distribution. entropy distribution serves measure overall certainty network regarding classiﬁcation. given categorical probability distribution possible classes probability class given entropy distribution calculated system predicts single class probability gives bits. completely uniform probability distribution classes gives log|c| bits number classes. thus conﬁdence network classiﬁcation decreases increasing response distribution entropy. problem might arise generating dataset selecting confusing samples might become unbalanced since difﬁcult classes become overrepresented. shown experimental section happens practice. assume certain point beneﬁcial side effect guided labeling procedure since want samples confusing figure classiﬁcation accuracy mnist dataset depending size training set. either randomly sampled dataset generated using proposed guided labeling approach. note training size logarithmic scale. idea using uncertainty system guide labeling called uncertainty sampling ﬁeld active learning also already introduced lewis although entropy resulting probability distribution propose method. entropy confusion measure used rebecca statistical parsing. following network architectures used experiments. note networks selected give best possible results speciﬁc dataset. important aspect difference performance randomly selected images images selected guided labeling. layers except last relu activation function. last layer uses softmax activation function. mnist dataset employed network consisting following seven layers. starting input layer convolutional layer kernels convolutional layer kernels pooling layer pooling size dropout layer dropout probability fully connected layer output neurons dropout layer dropout probability fully connected layer output neurons last output layer. cifar dataset employ network consisting following eleven layers. convolutional layer kernels input layer convolutional layer kernels maxpooling layer pooling size dropout layer dropout probability convolutional layer kernels convolutional layer kernels maxpooling layer classes dataset. unfortunately extremely unbalanced datasets problematic training neural network. lessen effects imbalance weigh misclassiﬁcations underrepresented classes higher loss function during training. weights calculated following fashion following fashion network trained small subset datasets remaining images treaded unlabeled. guided labeling procedure selects images labeled added training using labels provided dataset. total number samples training dataset number samples speciﬁc class dataset scaling factor determined empirically. provided best results experimental evaluation. training starts small labeled training large unlabeled images. depending used dataset convolutional neural networks section trained training set. images unlabeled dataset augmented procedure presented section augmented images trained network returns class probabilities entropy distribution calculated according section giving response distribution entropy augmentation image. procedure repeated multiple times different augmentations average image recorded. average indication confusing certain unlabeled image trained network also accounting employed data augmentation. predetermined amount confusing images selected labeling. images removed unlabeled dataset labeled human added training set. retrained dataset. procedure repeated satisfactory performance reached data left labeled. hypothesis that using procedure instead randomly labeling images satisfactory performance reached dataset containing fewer labeled samples. algorithm presents pseudo code guided labeling procedure. problems evaluating proposed guided labeling method actually using humans labeling. first procedure time consuming needs human resources different modalities tested. second hard decide whether guided labeling performs better randomly labeling images. this decided experiments experiments used adam optimizer learning rate categorical cross–entropy loss function. weights initialized using normalized initialization glorot training utilized early stopping using accuracy validation dataset images stopping criterion. patience epochs used. means improvement accuracy validation dataset epochs training stopped network epochs used testing purposes. using guided labeling approach decide many images selected labeling step. principle would expect method work better fewer images selected iteration since newly labeled images used inform selection images next iteration. hand selecting fewer images results iterations. since iteration neural network trained procedure takes longer. addition fewer images also means labeling workﬂow including human labor less efﬁcient. determined exponential selection scheme offers good compromise. exponential selection scheme many images selected labeling already training set. thus every iteration size training doubles. number images label iteration starting labeled training size calculated follows comparison reasons identical network architectures also trained equally sized randomly selected subset given dataset. prediction accuracy network trained images selected using guided labeling network trained randomly selected images compared testing sets provided mnist cifar dataset respectively. prediction accuracy number correctly classiﬁed images relation number tested images. comparison guided labeling using exponential selection scheme randomly selected images seen figure mnist dataset. note training size logarithmic scale. obvious guided labeling helpful mnist. able achieve accuracy times smaller training selected images even outperfroms whole dataset. important question whether response distribution entropy images actually correlate samples would considered less difﬁcult human observer. figure shows twenty confusing least confusing images second guided labeling iteration. confusing images clearly difﬁcult images. many either unconventional writing styles numbers poor quality contain noise. hand twenty images lowest show slightly different variations number would probably much training considering data augmentation used training. figure twenty least confusing images mnist dataset selected response distribution entropy guided labeling steps started randomly selected images. could assume number general distinctive confusing cnn. check look class distributions training guided labeling iteration. visualized figure addition average class distribution figure classiﬁcation accuracy cifar dataset depending size training set. either randomly sampled dataset generated using proposed guided labeling approach. note training size logarithmic scale. generalize well testing set. point guided labeling performs strictly better random selection training reaches images point less whole cifar dataset used training. guided labeling almost performance reached images using whole dataset cutting amount images labeled half. analogously mnist dataset also look twenty least confusing images figure twenty confusing images harder interpred case compared twenty least confusing images clear difference seen. least confusing images almost exclusively contain bright cars. presumably color appear images classes often. compared instances cars confusing images obvious response distribution entropy correlate difﬁculty images. looking class distributions figure clear cars least confusing class. presumably overall color image already indicative class. interesting note technical classes general less confusing animal classes looking dataset images technical classes distinct overall appearence thus training become imbalanced cifar. class weights presented section mitigate imbalance certain degree. distribution images training among possible classes different training sizes. training size increased using guided labeling. doubling training size corresponds iteration guided labeling procedure. iterations shows number seems less confusing numbers system. still overall mnist dataset seems relatively balanced regarding confusing different classes are. cifar seems overall difﬁcult dataset gains guided labeling smaller seen figure give possible reasons case discussion. images guided labeling even performs worse purely random selection. presumably happens point training consists confusing special cases distribution images training among possible classes different training sizes. training size increased using guided labeling. doubling training size corresponds iteration guided labeling procedure. achievable accuracy. case mnist going images images improved accuracy presumably mnist dataset using augmentation less exhaustive additional training data would improve achievable accuracy signiﬁcantly. presumably presented active learning scheme works best unlabeled data achievable accuracy would increase size unlabeled dataset without increasing number samples actually labeled. figure twenty least confusing images cifar dataset selected response distribution entropy guided labeling steps started randomly selected images. first could show response distribution entropy able identify difﬁcult examples unlabeled dataset case mnist well cifar dataset. even trained images response distribution entropy unlabeled data seems good metric detect redundant samples need labeled. second demonstrated presented guided labeling scheme potentially reduce number samples labeled large amount. case mnist well selected dataset size achieves classiﬁcation accuracy. cifar dataset size could still reduced half. still procedure perform worse cifar mnist? hypothesize main reasons. hand dataset much difﬁcult general since classes much higher variation also replicated easily augmentation images. example image bird always variation image number variation background lighting different bird species etc. hand cifar dataset also much smaller respect difﬁculty therefore less exhaustive. contains amount training data mnist despite much difﬁcult. also seen looking achieved accuracy depending size randomly selected training set. comparing figure mnist figure cifar apparent bigger dataset would likely improve accuracy cifar would mnist. going images images improved accuracy improvement going images. suggests increasing dataset size would likely improve future research evaluate presented method additional datasets pascal imagenet also interesting well dataset selected using network architecture generalize another architecture. hypothesized earlier guided labeling likely would perform better tasks large number unlabeled data available. experiments synthetically generated dataset amount unlabeled data unlimited labels easily generated sample might able clarify this. might also interesting bring additional concepts ﬁeld active learning area deep learning. presented active learning method named guided labeling allows automatic selection confusing/difﬁcult samples pool unlabeled samples. method easy implement could show using reduces number samples needed achieve desired accuracy considerably. mnist size dataset could reduced fold cifar half. might move manual labeling region feasibility tasks especially since hypothesize results would improve even pool unlabeled data big. case mnist reduced dataset even outperformed training full dataset presumably removing unnecessary variations hinder generalizability.", "year": 2017}