{"title": "An Exponential Lower Bound on the Complexity of Regularization Paths", "tag": ["cs.LG", "cs.CG", "cs.CV", "math.OC", "stat.ML", "90C20", "F.2.2; I.5.1"], "abstract": "For a variety of regularized optimization problems in machine learning, algorithms computing the entire solution path have been developed recently. Most of these methods are quadratic programs that are parameterized by a single parameter, as for example the Support Vector Machine (SVM). Solution path algorithms do not only compute the solution for one particular value of the regularization parameter but the entire path of solutions, making the selection of an optimal parameter much easier.  It has been assumed that these piecewise linear solution paths have only linear complexity, i.e. linearly many bends. We prove that for the support vector machine this complexity can be exponential in the number of training points in the worst case. More strongly, we construct a single instance of n input points in d dimensions for an SVM such that at least \\Theta(2^{n/2}) = \\Theta(2^d) many distinct subsets of support vectors occur as the regularization parameter changes.", "text": "abstract. variety regularized optimization problems machine learning algorithms computing entire solution path developed recently. methods quadratic programs parameterized single parameter example support vector machine solution path algorithms compute solution particular value regularization parameter entire path solutions making selection optimal parameter much easier. assumed piecewise linear solution paths linear complexity i.e. linearly many bends. prove support vector machine complexity exponential number training points worst case. strongly construct single instance input points dimensions least many distinct subsets support vectors occur regularization parameter changes. regularization methods support vector machines related kernel methods become successful standard tools many optimization classiﬁcation regression tasks variety areas example signal processing statistics biology computer vision computer graphics well data mining. optimization problems containing special parameter objective function called regularization parameter representing tradeoﬀ optimization objectives. machine learning terms usually model complexity accuracy training data words tradeoﬀ good generalization performance over-ﬁtting. parameterized quadratic programming problems studied extensively optimization machine learning resulting many algorithms able compute solutions single value parameter along whole solution path parameter varies. many variants known solution paths piecewise linear functions parameter however complexity paths remained unknown. ∗institute theoretical computer science zurich switzerland gaertnerinf.ethz.ch †cmap ´ecole polytechnique palaiseau france jaggicmap.polytechnique.fr ‡inria sophia antipolis-m´editerran´ee france clement.mariainria.fr prove complexity solution path svms simple instances parameterized quadratic programs indeed exponential worst case. furthermore example shows exponentially many distinct subsets support vectors optimal solution occur regularization parameter changes. exponentially many valid terms number input points also dimension space containing points. functions describe objective function constraints vary real parameter assume always symmetric positive semi-deﬁnite matrix example gram matrix. methods exactly form include cν-svm versions -loss support vector regression lasso regression classiﬁcation one-class multiple kernel learning kernels -regularized least squares least angle regression also basis pursuit denoising problem compressed sensing parametric quadratic programs limited machine learning also important control theory also occur geometry example polytope distance smallest enclosing ball moving points also many ﬁnance applications mean-variance portfolio selection well instances multi-variate optimization. observe majority mentioned applications indeed special form depend linearly therefore result piecewise linear solution paths. particular holds prominent application machine learning -loss e.g. hand -loss probably easiest example matrix parameterized ﬁxed interesting measures complexity solution paths parameter deﬁned above first consider number pieces bends solution path. bend parameter value solution path turns i.e. diﬀerentiable. alternatively interested number distinct subsets support vectors appear parameter changes. support vector corresponds strictly non-zero coordinate solution dual quadratic program based empirical observations conjectured complexity solution path two-class i.e. number bends number distinct support vectors linear number training points. empirical conjecture repeatedly stated related methods disprove conjecture showing complexity case indeed exponential number training points. natural construction many input points program d-dimensional space main interesting properties first subsets size support vectors indeed occur parameter changes. furthermore number bends solution path o-notation hides constant construction therefore proves exponential complexity solution paths parameterized quadratic programs even simple case linear part objective quadratic program depends linearly parameter. avoid confusion construction show particular algorithm needs exponentially many steps compute solution path indeed shows algorithm reporting solution path need exponential time path example unique exponentially many bends. brief overview existing solution path algorithms following section conceptually construction motivated fact standard equivalent geometric problem ﬁnding closest distance polytopes. geometric framework employ goldfarb cube originally served prove simplex algorithm linear programming needs exponential number steps pivot rule formally algebraically deﬁne instance program formally prove optimality constructed solutions means standard conditions. also implies construction could probably modiﬁed give lower bound complexity instances parameterized quadratic programs restricted svms. continuing line research recently constructed example exponential path complexity lasso regression using diﬀerent proof technique. recently methods independent revival machine learning particular computing exact solution paths context support vector machines related problems also regression techniques -regularized least squares similar methods also applied special cases quadratic programs particular cases solution path piecewise linear. machine learning solution path algorithm special case c-svm proposed gave algorithm lasso later proposed solution path algorithms ν-svm one-class respectively. multi-class svms laplacian svm. also case cost asymmetric svms computed solution path methods. support vector regression interesting underlying quadratic program depends parameters regularization parameter tube-width parameter obtained solution path algorithm). however mentioned specialized methods disadvantages speciﬁc individual problem usually require principal minors matrix invertible always realistic dealing large numerical data later pointed path problem indeed speciﬁc instance general parametric quadratic programming problem generic path optimization algorithms already exist e.g. also methods valid arbitrary positive semi-deﬁnite matrices issue non-invertible sub-matrices also addressed fact theory linear programming gass-saaty shadow vertex pivot rule simplex method needs exponentially many steps goldfarb cube. rule originally conceived gass saaty solve parametric linear programming problem objective function depends linearly real parameter goal compute optimal solutions possible parameter values contribution adapt result support vector machines obstacles overcome. first nature parameterization standard two-class quite diﬀerent goldfarb’s parametric linear programs. secondly goldfarb’s solution path discontinuous need provide continuous path unique solution every parameter value. approach dualize goldfarb’s contruction carefully transform standard regularized two-class instance goldfarb’s linear objective function turns quadratic similar geometry. support vector machine well studied standard tool classiﬁcation problems among widely applied methods machine learning. paper discuss svms standard -loss term. primal ν-svm problem given following parameterized quadratic program given solution problem vectors appearing non-zero coeﬃcient called support vectors. formulation equivalent polytope distance problem reduced convex hulls classes data-points formally note also slightly commonly used c-svm variant equivalent exactly geometric distance problem shown monotone correspondence regularization parameters geometric parameter explained detail therefore following lower bound constructions solution path complexity hold ν-svm c-svm case. literature topic reduced convex hulls also role optimization refer point classes. also observed solution path piecewise linear function regularization parameter empirically suggested number bends solution path roughly number range left optimal distance vector multiple optimal walks nearly boundary faces blue class. precisely number bends path optimal least twice number inner blue vertices claimed above. argument formal proof gives main idea guide high-dimensional construction. going higher dimensions surprisingly allow prove path complexity lower bound linear number input points even exponential also dimension space containing points. figure dimensional example path least many bends. green lines indicate optimal solutions polytope distance problem equivalently formulations indicated parameter value idea spice two-dimensional example construct classes points respectively. point sets construction ensures relevant values parameter points optimal distance close two-dimensional plane crucial feature construction convex hull points intersects convex polygon vertices edges. moreover walk through constant fraction changing parameter thus mimic process depicted figure except number relevant bends exponential main technical tool well-known goldfarb cube slightly deformed ddimensional cube facets vertices distinctive property vertices visible projection cube taking geometric dual goldfarb cube obtain d-dimensional polytope vertices facets intersect twodimensional plane vertices dual goldfarb cube form ﬁrst point class applying linear stretching transform keeps walk close called face deﬁned inequality. origin interior suﬃces consider inequalities form faces dimension vertices faces dimension called facets. full-dimensional every vertex intersection facets. goldfarb cube originally constructed linear program simplex algorithm shadow vertex pivot rule needs exponential number steps optimal solution deﬁne disjoint opposite facets. vertex therefore intersection exactly facets pair opposite facets. fact every choice facets yields distinct vertex means vertices indexed index vector tells pair inequalities whether left tight vertex right shadow gold projection theorem tells goldfarb cube vertices appear boundary shadow. usually shadow polytope much smaller complexity since many vertices project interior. simple examples consider three-dimensional platonic solids. geometric dual tetrahedron tetrahedron. cube dual octahedron dodecahedron dual icosahedron. geometric dual d-dimensional unit cube cross-polytope vertices facets. dual goldfarb cube therefore perturbed version cross-polytope figure able follow initial idea outlined beginning section vertices facets. moreover proposition dual goldfarb cube easily facets intersect two-dimensional plane deﬁned fact already know points facets. corollary point constructed figure dual goldfarb cube dimensions perturbed cross-polytope imagine vertices lying slightly behind intersection plane vertices slightly front plane intersects triangular facets. ﬁrst class ideally would like vertices dual goldfarb cube points make sure solution path walks along exponentially many facets intersect two-dimensional plane according corollary that need walk stay close achieve this still need stretch facets almost orthogonal stretching transform scales second part ﬁrst observe problem written quadratic program problem minimizing convex quadratic function subject linear equality constraints. indeed squaring objective function obtain following equivalent program easily yields indeed optimal pair. according negative multiple hence choose observe indeed since negative multiple vσd) choice corollary last complementary slackness conditions satisﬁed vσ)t relaxed problem require strictly negative multiple vσ). complementary slackness turn implies vσ)t according already deﬁnition projection rule optimal solution determines outlined introductory section standard solution svm-like optimization problem expressed ways either explicit vector solving primal problem distance version secondly convex combination input points consider corresponding dual problem case input points appearing non-zero coeﬃcient convex combination called support vectors. polytope distance problems representations even easier convert other point polytope convex combination vertices polytope also polytope basics section point class polytope distance problem support vectors point constructed section precisely vertices means every chosen diﬀerent support vectors must coeﬃcient deﬁnition inequalities deﬁne goldfarb cube know section vertex exactly facets deﬁned inequalities follows. means convex combination actually desired form using point classes prove regularization parame) indeed occur changes exponentially many constructed pairs therefore also solution path corresponding dual sponds case words convex hulls reduced. small section prove constructed solutions corresponding support vectors ﬁrst point class actually valid suﬃciently close formally boundedness follows also minimum/maximum exactly many ﬁnite values recall deﬁnition fact finally points distinct explained corollary know qmin qmax. {uleft uright} easy reduced proof. arbitrary points convex hull reduction factor given line segment uright µuright uleft]. case uleft uright interested d-th coordinate calculation slightly simpliﬁed write regularization value state ﬁnal theorem parameter value optimal solutions optimality theorem also optimal distance problem meaning realize shortest distance therefore established exponentially many subsets exactly support vectors many input points occur regularization parameter changes between exact number distinct sets dimension space implemented goldfarb cube construction using exact arithmetic could conﬁrm theoretical ﬁndings. constructed stretched dual goldfarb cube gold using polymake figure shows dimensional intersection dual goldfarb cube goldfarb gfarb.poly center gcenter.poly gfarb.poly; polarize gpolar.poly gcenter.poly; intersection gint.poly gpolar.poly plane.poly; polymake gint.poly. considered exponentially many original facets point class conv none additional reduced facets reduced convex hull convµ occur coordinates attain upper bounds equality parameter shown worst case complexity solution path svms representing type parameterized quadratic programs exponential number points dimension example also shows exponentially many distinct subsets support vectors optimal solution occur regularization parameter changes. want point construction also interpreted general result theory parameterized quadratic programs. ignoring fact constructed instance shown idea solving parameterized quadratic programs tracking solution path leads exponential-time algorithm worst case. result also implies complexity exact solution paths quite diﬀerent complexity path approximate solutions -loss shown complexity approximate path constant depending approximation quality. thus independent inputs strong contrast worst-case complexity exact path proved here. acknowledgements. project supported swiss national science foundation work done jaggi zurich maria visiting zurich. would like thank anonymous reviewers helpful comments suggestions. thank joachim giesen madhusudan manjunath stimulating discussions.", "year": 2009}