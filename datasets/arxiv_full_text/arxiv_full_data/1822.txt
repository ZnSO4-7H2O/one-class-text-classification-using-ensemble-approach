{"title": "Context encoders as a simple but powerful extension of word2vec", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "With a simple architecture and the ability to learn meaningful word embeddings efficiently from texts containing billions of words, word2vec remains one of the most popular neural language models used today. However, as only a single embedding is learned for every word in the vocabulary, the model fails to optimally represent words with multiple meanings. Additionally, it is not possible to create embeddings for new (out-of-vocabulary) words on the spot. Based on an intuitive interpretation of the continuous bag-of-words (CBOW) word2vec model's negative sampling training objective in terms of predicting context based similarities, we motivate an extension of the model we call context encoders (ConEc). By multiplying the matrix of trained word2vec embeddings with a word's average context vector, out-of-vocabulary (OOV) embeddings and representations for a word with multiple meanings can be created based on the word's local contexts. The benefits of this approach are illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition (NER) task.", "text": "simple architecture ability learn meaningful word embeddings efﬁciently texts containing billions words wordvec remains popular neural language models used today. however single embedding learned every word vocabulary model fails optimally represent words multiple meanings. additionally possible create embeddings words spot. based intuitive interpretation continuous bag-of-words wordvec model’s negative sampling training objective terms predicting context based similarities motivate extension model call context encoders multiplying matrix trained wordvec embeddings word’s average context vector out-ofvocabulary embeddings representations word multiple meanings created based word’s local contexts. beneﬁts approach illustrated using word embeddings features conll named entity recognition task. representation learning prominent ﬁeld natural language processing example word embeddings learned neural language models shown improve performance used features supervised learning tasks named entity recognition popular wordvec model learns meaningful word embeddings considering words’ local contexts. thanks shallow architecture trained efﬁciently large corpora. model however learns single representation words ﬁxed vocabulary. consequently task encounter word present texts used training cannot create embedding word without repeating time consuming training procedure model. furthermore single embedding optimally represent word multiple meanings. example washington name state well former president taking account word’s local context identify proper sense. based intuitive interpretation continuous bag-of-words wordvec model’s negative sampling training objective propose extension model call context encoders allows easy creation embeddings well better representation words multiple meanings simply multiplying trained wordvec embeddings words’ average context vectors. demonstrated conll challenge classiﬁcation performance signiﬁcantly improved using features word embeddings created conec instead wordvec. related work past addressed issue polysemy various ways. example sensevec extension wordvec preprocessing step words training corpus annotated part-of-speech practice model trained large vocabulary rare encounter word embedding. still scenarios case example unlikely term encountered regular training corpus might still want embedding represent search query like whirlpool maker part. embeddings learned tokens consisting words tags. different representations generated e.g. words used noun verb methods ﬁrst cluster contexts words appear additional resources wordnet identify multiple meanings words possibility create embeddings learn representations character n-grams texts compute embedding word combining embeddings n-grams occurring however none designed solve polysemy problem time. furthermore compared wordvec require parameters resources additional steps training procedure. conec hand generate embeddings well improved representations words multiple meanings simply multiplying matrix trained wordvec embeddings words’ average context vectors. scanning huge amounts text sentence sentence. based context words algorithm tries predict target word them. mathematically realized ﬁrst computing embeddings context words selecting appropriate rows vector multiplied several rows selected rows corresponds target word others correspond ‘noise’ words selected random after applying non-linear activation function backpropagation error computed comparing output label vector position target word noise words. training model complete word embedding target word corresponding similar words appear similar contexts example words synonymous could exchanged another alcontexts without reader noticing. based context word co-occurrences pairwise similarities words vocabulary computed resulting similarity matrix rn×n similarity scores similarities preserved word embeddings e.g. cosine similarity between embedding vectors words used similar contexts close generally scalar product matrix word embeddings rn×d approximate obviously straightforward obtaining word embeddings satisfying would compute singular value decomposition similarity matrix eigenvectors corresponding largest eigenvalues vocabulary typically comprises tens thousands words performing corresponding similarity matrix computationally expensive. similarity matrix would huge would also quite sparse many words course synonymous other. picked small number random words chances similarities target word would close therefore product single word’s embedding matrix embeddings result vector close true similarities word consider small subset corresponding word random words sufﬁcient approximates binary vector word elsewhere. cbow wordvec model trained negative sampling therefore interpreted neural network predicts word’s similarities words training occurrence word texts binary vector positions context words elsewhere used input network multiplied weights arrive embedding embedding multiplied another weights corresponds full matrix word embeddings produce choice determines much emphasis placed word’s local context helps distinguish multiple meanings word out-of-vocabulary word global embedding computed solely based local context i.e. setting perspective model optimization procedure another advancement feasible. since context words merely sparse feature vector used input reason input vector contain features target word well. example feature vector could extended contain information word’s case part-ofspeech relevant details. would increase dimensionality ﬁrst weight matrix include additional features mapping input word’s embedding training objective therefore also would remain unchanged. additional features could especially helpful details words would otherwise lost preprocessing retain information word’s position sentence ignored approach. extended conecs expected create embeddings even better distinguish words’ different senses taking account example word used noun verb current context similar sensevec algorithm instead explicitly learning multiple embeddings term like sensevec dimensionality input vector increased include current word feature expected improve generalization training examples available. output network vector ˆswi containing approximated similarities word words. training error computed comparing subset output binary target vector serves approximation true similarities considering small number random words. refer interpretation model context encoders closely related similarity encoders dimensionality reduction method used learning similarity preserving representations data points training procedure conec identical wordvec difference computation word’s embedding training complete. case wordvec word embedding simply tuned matrix. considering idea behind optimization procedure instead propose create representation target word multiplying word’s average context vector better resembles word embeddings computed training. figure results task based three random initializations wordvec model. left panel overall results mean performance using wordvec embeddings considered baseline embeddings computed conecs using various combinations words’ global local cvs. right panel increased performance test fold using conec multiplying wordvec embeddings global yields performance gain percentage points additionally using local create word embeddings gain another points using combination global local distinguish different meanings words f-score increases another points yielding f-score marks signiﬁcant improvement compared reached wordvec features. embedding dimensionality context window words. word embeddings created conec built directly wordvec model multiplying original embeddings respective context vectors. code replicate experiments available online. additionally performance word analogy task reported appendix. named entity recognition main advantage context encoders ability local context create embeddings distinguish different senses words. effects prominent task local context word make difference e.g. distinguish chicago bears city chicago tested conll task using word embeddings features together logistic regression classiﬁer. reported f-scores computed using ofﬁcial evaluation script. results achieved various word embeddings training development test part conll task reported fig. noted using task extrinsic evaluation illustrate advantages conec embeddings regular wordvec embeddings. isolate effects performance using word embeddings features typically performance challenge much higher features word’s case included well. wordvec embeddings trained documents used training part task. words development test parts represented zero vectors. three parameter settings illustrate advantages conec multiplying wordvec embeddings words’ average context vectors generally improves embeddings. show this conec word embeddings computed using global means words zero representation. embeddings performance improves test folds task. useful embeddings created local context word. show this conec embeddings words training vocabulary computed ‘oov’ ﬁgure). training performance obviously stays same words embedding based global contexts. however jump conec performance test folds words representation based local contexts. better embeddings word multiple meanings created using combination word’s average global local input conec. show this embeddings computed words occurring training vocabulary local context taken account well setting best performances folds achieved averaging global local around multiplying wordvec embeddings. clearly shows conec embeddings created incorporating local context help distinguish multiple meanings words. context encoders simple powerful extension cbow wordvec model trained negative sampling. multiplying matrix trained wordvec embeddings words’ average context vectors conecs easily able create embeddings spot well distinguish multiple meanings words based local contexts. beneﬁts demonstrated conll challenge. would like thank antje relitz ivana balaževi´c christoph hartmann andreas nowag klausrobert müller anonymous reviewers helpful comments earlier versions manuscript. franziska horn acknowledges funding elsa-neumann scholarship berlin.", "year": 2017}