{"title": "A Brief Study of In-Domain Transfer and Learning from Fewer Samples  using A Few Simple Priors", "tag": ["cs.AI", "cs.LG"], "abstract": "Domain knowledge can often be encoded in the structure of a network, such as convolutional layers for vision, which has been shown to increase generalization and decrease sample complexity, or the number of samples required for successful learning. In this study, we ask whether sample complexity can be reduced for systems where the structure of the domain is unknown beforehand, and the structure and parameters must both be learned from the data. We show that sample complexity reduction through learning structure is possible for at least two simple cases. In studying these cases, we also gain insight into how this might be done for more complex domains.", "text": "domain knowledge often encoded structure network convolutional layers vision shown increase generalization decrease sample complexity number samples required successful learning. study whether sample complexity reduced systems structure domain unknown beforehand structure parameters must learned data. show sample complexity reduction learning structure possible least simple cases. studying cases also gain insight might done complex domains. many domains constrained data availability. includes domains youtube recommendations ostensibly large amount data long tail instances handful data points. also includes domains humans require several orders magnitude less training data state-of-the-art approaches motor manipulation playing atari understanding low-frequency words. free lunch theorem suggests domain prior needed offset sample complexity cases. domains images audio convolutions commonly used prior. view convolutions allow in-domain transfer sharing weights among otherwise weakly-connected areas domain effectively multiplying number samples. example convolutional layer takes mnist image google research work done part google brain google residency program brain mountain view usa. correspondence marc pickett <pickettgoogle.com>. uses ﬁlters size stride gives windows image means every image training points ﬁlter effectively decreases sample complexity times ﬁlter. convolutions allow internally transfer information top-left window bottom-right window however domains structure might known beforehand robot joint angle trajectories trafﬁc speed sensor data. question address paper domains aren’t given structure priori weaker prior allow system learn structure leverage learned structure still decrease sample complexity baseline without prior? present simple probability density estimation problem examine three priors affect sample complexity. show least simple domain structure recovered merely given prior repeated structure number ﬁlters size windows. using prior show system automatically transfer parts domain samples relatively plentiful parts samples rare. help illustrate simple prior might help reduce sample complexity consider setup given four urns ﬁlled balls eight different colors given samples drawn urns replacement assume urns independent other. setup given samples urns time. choose sample told sampled. example might sequence samples aren’t uniform among four urns sample probability sampled probability goal model urns’ distributions minimizing divergence estimated distribution unseen true distribution assume uniform prior urn’s distribution can’t better tallying outcomes taking expected values dirichlet distribution. case takes thousands samples strong estimate distribution it’s sampled infrequently shown figure given prior knowledge actually distributions instead four sample complexity signiﬁcantly. draws four urns still independent urns told ﬁlled balls sampled much larger urns though aren’t told distributions balls larger urns larger urns four urns ﬁlled. formally assume larger urns ∀i∈{}pi pb}. case alternatively update estimates classiﬁcation distributions classiﬁcation probabilities update estimates underlying distributions. speciﬁcally estimate probability distribution drawn ∀i∈{}j∈{ab}p data seen thus estimates compute estimates values update classiﬁcation probabilities convergence. show results process figures plotting function number samples estimates single run. figure signiﬁcantly faster convergence case make priors uniform prior. break error estimates four urns figure sources difference estimates. ﬁrst source model quickly concludes urns identical distributions. thus uses samples inform probability distribution vice versa. effect doubles samples urns halving number samples needed create probability estimates them. source difference model’s estimate probabilities rarely seen ﬁrst samples different ball colors uninformed estimate nowhere near convergence seen average samples color. conversely samples system correctly concludes urns drawn distribution transfers knowledge note knowledge transfer goes ways green line dips slightly line. system concludes urns identical distributions adds paltry samples tallies distribution shared urns. aninterpretation system primarily creating model probability distribution samples whereas uses samples classify type analogy might made scenario knowing donald duck tells much donald also informs little means duck. finally note averaged error ours figure brieﬂy increases decreasing. insight might gained explain looking breakdowns figure suspect model initially erroneously assigns distribution urns thus negatively transferring tallies samples. section give simple example might generalize priors domains must simultaneously learn structure probability distributions. present vastly simpliﬁed version searching convolutional structure images. conceptually would like discover convolutions images withprior knowledge pixels next which even we’re dealing grid given prior knowledge repeated structure. real images might given real-valued vectors size grossly simplify bit-vectors size discover repeated structure case. given bit-vector time longer assume independence among elements vectors. previous task task model -way joint distribution. assume uniform distribution outcome without priors best model outcomes dirichlet distribution possible outcomes. given variables form groups variables’ order within group. equivalent told colors ball urn. knowledge vector length equivalent sample four urns. example sample consists boolean variables {v··· prior break ordered triples equivalent color figure error samples seen. total error estimates four urns single averaged runs breakdown kl-error estimates four urns using tallies using prior types distributions. individual samples shown markers sample. given grouping problem equivalent four urns problem. e.g. using grouping above color respectively would equivalent drawing ball color given grouping explore different assignments. since latent variables distribution gets assigned exactly latent variables e.g. assigned coming either distribution technique searching possible groupings clearly intractable taking exponential time length vectors feasible tiny vectors. explicitly given data d··· search permutation perm ordered possible group assignment function maximize using equations priors following cases plots shown figure case assume elements bit-vector independent other. here model variable using beta distribution uniform prior. model converges quickly plateaus vectors expressive enough represent true distribution. case interesting case purposes. assume four distributions four distributions really distinct types though we’re told variables grouped together. case case exhaustive search possible groupings variables compute likely ordering using similar techniques previous section. example grouping indices might indicated figure search takes vector samples converge correct grouping follows patterns curves figure shows that least case sample complexity reduced using ﬁrst priors. figure shows averages runs. latent classes document mixture exactly latent class. knowledge special case isn’t directly addressed topic analysis literature latent classes allowed mix. difﬁcult phrase -bit vector problem instance topic analysis given documents/urns parts documents must deduce together. also overlap setup contextual bandits systems motivated make estimates processes true distributions efﬁciently terms number samples. system differs utility directly tied accuracy estimate choice observation next. main motivation work came transfer learning continual learning lifelong learning learning learn multitask learning share idea knowledge learned area leveraged learn another area fewer samples. paper contributes areas investigating simple case offering insight system transfer knowledge areas estimate respective certainty areas. shown example assumptions allow decrease sample complexity. thought simple example in-domain transfer hypothesize transfer knowledge needs measure certainty parameters something bayesian approaches handle naturally. measure also implicit freezing weights trained convergence work preliminary exploration done. main directions we’d like explore future research. ﬁrst generalizing priors conjecture wrapped description length prior cheaper inherit existing models distributions create distribution scratch. would allow model search number distributions distribution types. second direction heuristics make search structure tractable. references ammar haitham eaton eric luna jos´e marcio ruvolo paul. autonomous cross-domain knowledge transfer lifelong policy gradient reinforcement learning. proc. ijcai andrychowicz marcin denil misha gomez sergio hoffman matthew pfau david schaul freitas nando. learning learn gradient descent gradient descent. advances neural information processing systems dudik miroslav daniel kale satyen karampatziakis nikos langford john reyzin zhang tong. efﬁcient optimal learning contextual bandits. arxiv preprint arxiv. kirkpatrick james pascanu razvan rabinowitz neil veness joel desjardins guillaume rusu andrei milan kieran quan john ramalho tiago grabskabarwinska agnieszka overcoming catastrophic forgetting neural networks. proceedings national academy sciences rusu andrei rabinowitz neil desjardins guillaume soyer hubert kirkpatrick james kavukcuoglu koray pascanu razvan hadsell raia. progressive neural networks. arxiv preprint arxiv. taylor matthew eaton eric ruvolo paul. lifelong transfer learning heterogeneous teams agents sequential decision processes. technical report washington state university pullman united states", "year": 2017}