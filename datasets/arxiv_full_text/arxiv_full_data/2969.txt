{"title": "Visual Learning of Arithmetic Operations", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "A simple Neural Network model is presented for end-to-end visual learning of arithmetic operations from pictures of numbers. The input consists of two pictures, each showing a 7-digit number. The output, also a picture, displays the number showing the result of an arithmetic operation (e.g., addition or subtraction) on the two input numbers. The concepts of a number, or of an operator, are not explicitly introduced. This indicates that addition is a simple cognitive task, which can be learned visually using a very small number of neurons.  Other operations, e.g., multiplication, were not learnable using this architecture. Some tasks were not learnable end-to-end (e.g., addition with Roman numerals), but were easily learnable once broken into two separate sub-tasks: a perceptual \\textit{Character Recognition} and cognitive \\textit{Arithmetic} sub-tasks. This indicates that while some tasks may be easily learnable end-to-end, other may need to be broken into sub-tasks.", "text": "simple neural network model presented end-to-end visual learning arithmetic operations pictures numbers. input consists pictures showing -digit number. output also picture displays number showing result arithmetic operation input numbers. concepts number operator explicitly introduced. indicates addition simple cognitive task learned visually using small number neurons. operations e.g. multiplication learnable using architecture. tasks learnable end-to-end easily learnable broken separate sub-tasks perceptual character recognition cognitive arithmetic sub-tasks. indicates tasks easily learnable end-to-end need broken sub-tasks. visual learning arithmetic operations naturally broken sub-tasks perceptual sub-task optical character recognition cognitive sub-task learning arithmetic. common approach cases learn sub-task separately. examples popular perceptual sub-tasks domains include object recognition segmentation. cognitive sub-tasks include language modeling translation. progress deep neural networks become possible learn complete tasks end-to-end. systems exist end-to-end training image sentence generation speech sentence generation end-toend learning introduce extra difﬁculty sub-tasks examine end-to-end learning neural network perspective model perception cognition performing arithmetic operations visual input visual output. input output examples network pictures training example give student input pictures showing digit integer number written standard font. target output also picture displaying input numbers. order succeed task network required implicitly able learn arithmetic operation without taught meaning numbers. seen similar teaching arithmetic person possess common language. model learning process feed-forward artiﬁcial neural network input network pictures numbers output also picture network trained sufﬁcient number examples tiny fraction possible inputs. training given pictures previously unseen numbers network generates picture displaying sum. therefore learned concept numbers without direct supervision also learned addition operation. although initially surprising result present analysis visual learning addition demonstrate realizable using simple neural network architectures. arithmetic operations subtraction also shown learnable similar networks. multiplication however learned successfully setting. shown multiplication sub-task difﬁcult realize addition architecture. interestingly addition roman numerals arithmetic sub-tasks shown realizable endto-end training task fails. demonstrates extra difﬁcultly end-to-end training. figure input output examples neural network trained addition. ﬁrst examples show typical correct response. last example shows rare failure case. tion arithmetic concepts taught visually across different cultures. also shown endto-end learning fails tasks even though subtasks learned easily. work deals arithmetic tasks future research required characterize non-visual sub-tasks learned visually e.g. video frame prediction. protocol visual learning arithmetic based image prediction. given input pictures target picture correct prediction. learner required predict output picture predicted picture denoted prediction loss evaluated square differences pixel intensities predicted picture target picture integers randomly selected prespeciﬁed range written input pictures. result arithmetic operation input numbers written target output picture numbers written pictures using standard font always placed image position. fig. examples. sec. simple powerful learner feed-forward fully-connected artiﬁcial neural network shown fig. network consists input layer dimensions fx×fy× dimensions input pictures. used fx×fy unless speciﬁed otherwise. network three hidden layers nodes relu activation functions output layer sigmoid activation. nodes adjacent layers fully connected. loss function used score difference predicted picture expected picture. network trained minibatch stochastic gradient descent using backpropogation algorithm. objective paper examine arithmetic operations learned end-to-end using purely visual information. several experiments carried experimental procedure using protocol sec. generated input pictures example showing single integer number. numbers randomly generated pre-speciﬁed range detailed below. output pictures created similarly displaying result arithmetic operation input. subtraction trained neural network identical architecture network used addition. subtraction small number larger found comparable difﬁculty addition. predicted digit error rate around comparable addition. multiplication task found much challenging operation feed-forward neural network. data experiment consisted input pictures -digit integers resulting output picture digit number network used similar used addition. theoretical work shown multiplication binary numbers require layers addition experimented adding hidden layers. network even hidden layers perform well task giving large train test errors. example input/output pair seen fig.. seen least signiﬁcant digit signiﬁcant digits predicted correctly enumeration different possibilities feasible network uncertain central digits. predicted digit error rate high engine often unable read numbers several blurry digits. addition roman numerals hypothesized marr others arithmetic using roman numerals challenging using arabic numerals. repeated addition experiment numbers written roman numerals digits long. demonstrated quantitatively tab. network able predict output frame roman numeral basis. suggests end-to-end visual learning addition roman numeral basis challenging agreement marr’s hypothesis. analyze result sec. experiment added strong gaussian noise input output pictures seen fig.. network achieved good performance task giving output pictures display correct result also clean noise. failures occur input digits almost illegible. cases network generated probabilistic output digit displaying mixture digits. mixture digits caused problems veriﬁcation using reporting digit error rate whereas human inspection obtained error rate. fig. details. theoretical characterization operations learnable neural networks established area research. pioneering paper presented used threshold circuits figure diagram showing construction neural network hidden layers able preform addition using visual data. pictures used input picture output. network fully connected uses relu units hidden layers sigmoid output layer. hidden layers units each. experiment input/output pairs randomly generated training pairs randomly created testing. proportion numbers used training small fraction possible combinations. feed-forward artiﬁcial network trained architecture described fig. network trained using mini-batch stochastic gradient descent learning rate momentum mini-batch size epochs training carried out. network implemented using caffe package correctness test measured using software applied output pictures. results compared desired output percentage incorrect digits computed. effectiveness neural network approach tested following operations. addition three results test shown fig. input output numbers included training set. examples qualitatively demonstrate effectiveness network learning addition figure examples performance network subtraction multiplication addition noisy pictures. network performs well subtraction insensitive additive noise. performs poorly multiplication. note bottom right image ground truth image example type training output images used noisy addition scenario. table digit prediction error rates end-to-end training pictures stripped -hot representation described sec. purpose error computation digits output predicted images found using ocr. addition subtraction always accurate. network able learn multiplication. although roman numeral addition failed using picture prediction network learned successfully -hot vectors. model neural network capacity. line papers established feasibility implementation several arithmetic operations binary numbers. recently addressed implementing universal turing machines using neural networks. theoretical work ﬁeld used binary representation numbers address arithmetic operations decimal form. notably general result shows operations implementable turing machine time implemented neural network layers nodes. sample complexity guarantees training time. research also dealt visual learning. hypotheses difference difﬁculty learning arithmetic using decimal roman representations made marr others. review algorithms roman numeral addition multiplication. learning execute python code textual data shown possible using lstms zaremba sutskever adding mnist digits randomly located blank picture performed recurrent neural networks used algebraic expression simpliﬁcation. works however required non-visual representation number either input output. paper show ﬁrst time end-to-end visual learning arithmetic possible. end-to-end learning image-to-sentence speech-to-sentence described multiple researchers. recent related work vondrick successfully learned predict objects appear future video frame several previous frames. work also seen frame prediction requiring network implicitly understand concepts driving change between input output frames. visual arithmetic easier task easier interpret analyze. greater simplicity task allows frames rather intermediate representation used paper shown feed-forward deep neural networks able learn certain arithmetic operations end-to-end purely visual cues. several operations learned architecture. section give intuition method network employs learn addition subtraction reasons multiplication roman numerals challenging. figure examples bottom layer weights ﬁrst input picture. recognizes leftmost position recognized center position. examples layer weights. outputs second position outputs leftmost position. looking network weights addition subtraction bottom hidden layer node sensitive particular digit given position. example bottom layer weights observed fig. .a-b. bottom hidden layer nodes therefore represent m-digit numbers vector length element representing presence digit position representation converting variable possible values binary variables apart single position known -hot. hidden layer contains similar representation output number representing presence digit position total size task central hidden layers mapping -hot representations input numbers -hot representation output number order evaluate sub-tasks separately repeated experiments data transformed -hot representation thereby bypassing visual sub-task. used architecture end-to-end case except removed ﬁrst last hidden layers results test sets measured percentage wrong digits output number presented tab. addition subtraction performed accurately visual case. network able learn multiplication difﬁculty arithmetic sub-task line results visual case. also justiﬁed theoretically binary multiplication shown previous papers require deeper networks binary addition. turing machine complexity basic multiplication algorithm opposed decimal addition means operation realizable deeper layers) larger network nodes). interesting relative accuracy roman numeral addition performed opposed failure visual case. believe high number digits large numbers roman numerals causes input output images high dimensional. hypothesize convergence improved preliminary unsupervised learning tasks conclude roman arithmetic learned dnns visual end-to-end learning challenging difﬁculty joint optimization sub-task. visual learning data corrupted strong noise quite successful. fact concepts learned well enough output pictures denoised network. performance illegible digits particularly interesting. found corrupted digits could possibly read multiple possibilities output digit also reﬂected uncertainly resulting mixture possible outputs respective probabilities. experiments found visual learning works unary operations signiﬁcant difference model cognitive system invariance ﬁxed permutation pixels. human would struggle learn images artiﬁcial neural networks manages well. invariance broken slight random displacement training data introduction convolutional architecture. although recurrent neural networks generally better learning algorithms chosen fully connected architecture ease analysis. hypothesize better performance multiplication obtained using lstm-rnn leave investigation future work. section provide feasibility proof construction neural network architecture learn addition visual data end-to-end. construction network illustrated fig. rely logic gates simplicity. logic gate implemented arbitrary accuracy single sigmoid linear combination relu units relu )/δ. although reported results obtained using network utilizing relu units also tested network relu units replaced sigmoid units obtaining similar results much slower convergence. logic gates therefore sufﬁciently good model network. input example shown fig. ﬁrst layer network dimensionality reduction layer. choose weights correspond ﬁlters containing digit position experimental network fact chooses complex ﬁlters usually concentrated similar digits increase accuracy digit detection construct nodes layer indicating templates triggered. ﬁrst hidden layer node responds speciﬁc template example corresponds template detecting digit present position picture value template appears not. similarly output layer represented templates corresponding digit given position worth noticing given digits position numbers respectively )mod digit output +)mod. pair digits arithmetic result equation therefore end-to-end training network sufﬁcient number examples network arrive weights practice good performance achieved. end-to-end training visual data therefore theoretically shown experimentally demonstrated able learn addition little guidance. powerful paradigm generalize visual learning non-visual concepts easily directly communicated learner. examined capacity neural networks learning arithmetic operations pictures using visual end-to-end learning protocol. neural network able learn addition subtraction robust strong image noise. concept numbers explicitly used. shown network able learn operations multiplication visual addition using roman numerals. latter shown although sub-tasks easily learned end-to-end task not. order better understand capabilities network theoretical analysis presented showing network capable performing visual addition constructed. theoretical framework help determine arithmetic operation learnable using feed-forward architecture. note analysis quite restrictive hypothesize experimental conﬁrmation end-to-end learnability complex tasks often result surprising ﬁndings. although work dealt primarily arithmetic operations approach used general cognitive sub-task learning using frame prediction. sub-tasks need restricted ﬁeld arithmetic include general concepts association. generating data cognitive sub-task trivial generating visual examples easy e.g. predicting future frames video. figure illustration operation hidden layer neural network able perform addition using visual training. example network handles digit numbers larger numbers handled similarly linear increase number nodes. pictures ﬁrst projected onto binary vector indicating digit present position numbers. digit threshold variable summation result compute indicator variables acknowledgments. research supported intelicrc israel science foundation. authors thank poggio shalev-shwartz weiss wolf fruitful discussions.", "year": 2015}