{"title": "Rapid Adaptation with Conditionally Shifted Neurons", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We describe a mechanism by which artificial neural networks can learn rapid adaptation - the ability to adapt on the fly, with little data, to new tasks - that we call conditionally shifted neurons. We apply this mechanism in the framework of metalearning, where the aim is to replicate some of the flexibility of human learning in machines. Conditionally shifted neurons modify their activation values with task-specific shifts retrieved from a memory module, which is populated rapidly based on limited task experience. On metalearning benchmarks from the vision and language domains, models augmented with conditionally shifted neurons achieve state-of-the-art results.", "text": "growing interest progress building ﬂexible adaptive models particularly within framework metalearning goal metalearning algorithms ability learn tasks efﬁciently given little training data individual task. metalearning models learn ability training distribution related tasks. work develop neural mechanism metalearning rapid adaptation call conditionally shifted neurons. conditionally shifted neurons like standard artiﬁcial neurons produce activation values based input connected neurons modulated connection weights. additionally capacity shift activation values based auxiliary conditioning information. conditional shifts adapt model behavior task hand. model csns operates phases description phase prediction phase. assume task access description simplest case example datapoints corresponding labels description phase model processes extracts conditioning information function performance based information generates activation shifts adapt task stores key-value memory. prediction phase model acts unseen datapoints task predict labels improve predictions model retrieves shifts memory applies activations individual neurons. training model learns meta procedure extract conditioning information description phase generate useful conditional shifts prediction phase. test time uses procedure adapt tasks describe mechanism artiﬁcial neural networks learn rapid adaptation ability adapt little data tasks call conditionally shifted neurons. apply mechanism framework metalearning replicate ﬂexibility human learning machines. conditionally shifted neurons modify activation values task-speciﬁc shifts retrieved memory module populated rapidly based limited task experience. metalearning benchmarks vision language domains models augmented conditionally shifted neurons achieve state-of-the-art results. ability adapt behavior rapidly response external internal feedback primary ingredient human intelligence. cognitive ﬂexibility commonly ascribed prefrontal cortex working memory brain. neuroscientiﬁc evidence suggests areas incoming information support task-speciﬁc temporal adaptation planning occurs within hundred milliseconds supports wide variety task-speciﬁc behaviors hand existing machine learning systems designed single task. trained optimization phase learning ceases. systems built train-and-then-test manner scale complex realistic environments require gluts single-task data prone issues related distributional shifts catastrophic forgetting adversarial data points proposed neuron-level adaptation several advantages previous methods metalearning adapt connections neurons instance fast weights optimizer first efﬁcient computationally since number neurons generally much less number weight parameters second conditionally shifted neurons incorporated various neural architectures including convolutional recurrent networks without special modiﬁcations suit structure models. describing details framework demonstrate experimentally resnet deep lstm models equipped csns achieve accuracy standard mini-imagenet -shot benchmarks accuracy penn treebank -shot language modeling tasks. results mark signiﬁcant improvement previous state art. primary contributions paper follows propose generic neural mechanism conditionally shifted neurons learning systems adapt introduce direct feedback computationally inexpensive metalearning signal; implement evaluate conditionally shifted neurons several widelyused neural network architectures. core idea conditionally shifted neurons modify network’s activation values shifting function auxiliary conditioning information. layer csns takes following form hidden layer output layer pre-activation vector layer neurons take various forms depending network architecture nonlinear function computes element-wise activation. layer-wise conditional shift vector determined layer-wise conditioning information implement model csns must deﬁne functions extract transform conditioning information shifts build metanet architecture munkhdalai metanet consists base learner plus shared meta learner working memory. task metanet processes task description stores relevant meta information key-value memory. classify unseen examples described task model queries working memory attention mechanism generate fast weights; modify base learner turn predicts labels begin describe model details fully connected feed-forward network csns. architecture depicted figure shown model factors base learner makes predictions inputs meta learner. meta learner extracts conditioning information base learner uses key-value memory store retrieve activation shifts. walking model details deﬁne error gradient direct feedback variants conditioning information describe csns added resnet lstm architectures respectively. model operates phases description phase wherein processes task description prediction phase wherein acts unseen datapoints predict labels episode training test sample task model ingests task description uses learns therefrom conditioning information make predictions unseen task data. base learner maps input datapoint label prediction layers described equation case pre-activation vector given wtht− weight matrix bias vector learned parameters. base learner operates similarly phases. description phase base learner’s input datapoint softmax output estimate label conditional shifts phase. prediction phase base learner operates inputs receives conditional shifts meta learner applies layer-wise according conditioned shifts base learner computes estimate label figure schematic illustration model conditionally shifted neurons. description phase meta learner populates working memory keys values based base learner’s performance task description; prediction phase meta learner retrieves task-speciﬁc shifts memory key-based attention feeds base learner adapt task. description phase base learner processes meta learner extracts layer-wise conditionx information example according meta learner uses conditioning information generate memory values. template conditional shifts task computed memory function irlt encodes shift template layer input these; arrange matrix irn×lt full task description. parsimony desire single memory function layers base learner different sizes therefore parameterize multi-layer perceptron operates independently vector conditioning information neuron sophisticated lt-agnostic parameterizations possible recurrent networks. parallel description phase meta learner constructs embedded representation input uses memory. objective function parameterize linear output layer. function generates description input d-dimensional vector train test model episodes. episode sample training test task process description feed unseen data forward obtain label predictions. training test tasks drawn distribution crucially partition data classes seen training time overlap seen test time. collection training episodes optimize model parameters end-to-end stochastic gradient descent gradients taken respect scheme model’s computational graph contains operations processing description like transformation condierror gradient information inspired success metanets ﬁrst consider gradients base learner’s loss task description conditioning information. compute error gradients apply chain rule standard backpropagation algorithm base learner. given true label task description model’s corresponding label prediction obtain loss gradients base-learner neurons layer lt-dimensional vector pre-activations layer size denote loss function note target optimization sgd. obtain conditioning information neuron using gradient preprocessing formula andrychowicz signum function preprocessing smooth variation since gradients respect different base-learner activations different magnitudes. neuron obtains -dimensional vector conditioning information. case interpret one-step transformed gradient update neuron activations gradients transformed preprocessing memory read write operations nonlinearity backpropagation inherently sequential information expensive compute. becomes increasingly costly deeper networks rnns processing long sequences. direct feedback information direct feedback information inspired feedback alignment methods biologically plausible deep learning obtain information base-learner neurons layer represents derivative nonlinear activation function derivative cross entropy loss respect softmax input. thus conditioning information neuron derivative loss function scaled derivative activation function. case neuron obtains c-dimensional vector information number output classes. compute conditioning information neurons network simultaneously single multiplication. efﬁcient sequentially locked backpropagation-based error gradients. furthermore obtain information sufﬁcient loss neuron activation functions differentiable. relaxed backpropagation methods. demonstrate effectiveness conditioning variants inputs block output pre-activations respectively. function conv denotes convolutional layer optionally followed batch normalization layer. activations csns resblock computed task-speciﬁc shift retrieved memory constructed based activation values analogously case; i.e. conditioning information computed neurons output residual block. stack several residual blocks csns construct deep adaptive resnet model. relu function nonlinearity model. given current input previous hidden state previous memory cell state lstm model csns computes gates memory cell states hidden states time step following update rules represents element-wise multiplication concatenation task-speciﬁc shift memory. lstm case memory constructed processing conditioning information extracted memory cell stacking layers together build deep lstm model adapts across depth time. tanh function nonlinearity model. among many problems supervised reinforcement unsupervised learning framed metalearning few-shot learning emerged natural popular test bed. few-shot supervised learning refers scenario learner introduced sequence tasks task entails multi-class classiﬁcation given single labeled examples class. challenge setting classes concepts vary across tasks; thus models require capacity rapid adaptation order recognize concepts few-shot learning problems previously addressed using metric learning methods recently shift towards building ﬂexible models problems within learning-to-learn paradigm vinyals uniﬁed training testing one-shot learner procedure developed end-to-end differentiable nearest-neighbor method one-shot learning. recently one-shot optimizers proposed ravi larochelle finn maml framework learns parameter initialization model adapted rapidly given task using steps gradient updates. learn initialization makes sophisticated second-order gradient information. harness ﬁrst-order gradient information simpler direct feedback information. highlighted architecture model conditionally shifted neurons closely related meta networks metanet modiﬁes synaptic connections neurons using fast weights implement rapid adaptation. metanet’s fast weights enable ﬂexibility expensive modify weights connections dense. neuron-level adaptation proposed work signiﬁcantly efﬁcient lending range network architectures including resnet lstm. previous work metalearning also formulated problem two-level learning speciﬁcally slow learning meta model across several tasks fast learning base model acts within task schmidhuber discussed netviewed form feature-wise transformation csns closely related conditional normalization techniques film similar approach modulates feature maps using global scale shift operations conditioned auxiliary input modality.in contrast csns apply shifts individual neurons’ activations locally modiﬁcation based model’s behavior task description rather input itself. case gradient-based conditioning information approach viewed synthesis conditional normalization model learned optimizer speciﬁcally learned memory functions transform error gradients conditioning shifts applied like one-step update activation values. model uses learned optimizer evaluate proposed csns tasks vision language domains. describe datasets evaluate according preprocessing steps followed test results ablation study. vision domain used widely adopted fewshot classiﬁcation benchmarks omniglot miniimagenet datasets. omniglot consists images classes different alphabets images class previous studies randomly selected classes training testing augmented training degree rotations. resized images pixels computational efﬁciency. omniglot benchmark performed -way classiﬁcation tests labeled examples class description convolutional network ﬁlters base learner. network convolutional layers uses convolutions followed relu nonlinearity max-pooling layer. convolutional layers followed fully connected layer softmax output. another architecture used function csns last four layers components referring model adacnn. full implementation details found appendix though state-of-the-art results omniglot tasks. obvious ceiling effect among best performing models accuracy saturates near mini-imagenet features -pixel color images classes class exemplar images. experiments class subset released ravi larochelle compared omniglot mini-imagenet fewer classes labeled examples provided class given larger number examples evaluated similar adacnn model ﬁlters well model sophisticated resnet components mini-imagenet -way classiﬁcation tasks. resnet architecture follows tcml exceptions memory constraints. instead convolutional layers ﬁlters single layer ﬁlters relu nonlinearity instead leaky variant. incorporate csns last residual blocks well fully connected output layers. full implementation details found appendix every training tasks tested model another tasks sampled validation set. model performance exceeded previous best validation result applied test set. following previous approaches unlike omniglot remains signiﬁcant room improvement mini-imagenet. shown table challenging task cnn-based models conditionally shifted neurons achieve performance best cnn-based approaches like maml metanet sophisticated adaresnet model hand achieves state-of-the-art results. best-performing adaresnet yields almost improvement corresponding adacnn model improves previous best result tcml shot -way classiﬁcation tasks respectively. note tcml likewise uses resnet architecture. best accuracy among different test runs -layer adalstm conditioning ﬁve-shot task. evaluate effectiveness recurrent models conditionally shifted neurons experiments few-shot penn treebank language modeling task introduced vinyals task model given query sentence missing word support onehot-labeled sentences also missing word each. missing words description identical missing query sentence. model must select label corresponding sentence. following vinyals split sentences training test that test target words prediction sentences appear unseen training. concretely removed test target words well sentences containing words training data. process necessarily reduces training data increases out-of-vocabulary test words. used target words testing provided vinyals evaluated models conditionally shifted neurons -shot language modelling tasks. cases represent words randomly initialized dense embeddings. ﬁrst model stacked -layer feed-forward csns lstm network prediction timestep. model adaffn adapt task processes hidden state underlying lstm. lstm encoder builds context word provides generic representation adaffn. components trained jointly. second model propose task ﬂexible lstm conditionally shifted neurons recurrence entire model adapted task-speciﬁc shifts every time step. few-shot classiﬁcation output softmax layer csns stacked adalstm. comparing lstm+adaffn adalstm former much faster since adapt activations three feedforward layers lacks full ﬂexibility since lstm unaware current task information. also evaluated deep versions lstm+adaffn adalstm models. full implementation details found appendix ation. first randomly sampled tasks test data report average accuracy. second make sure include test words task formulation. randomly partition target words groups solve group task. random approach chance word could missed included multiple times different tasks. however random approach also enables formulation exponential number test tasks. table summarizes results. approximate upper bound achieved oracle lstm-lm vinyals best accuracy around -shot task comes using -layer adalstm improves matching nets results -shot tasks respectively. comparing model variants adalstm consistently outperforms standard lstm augmented conditionally shifted output deeper models yield higher accuracy. providing sentences target word increases performance expected. results indicate model’s few-shot language modelling capabilities exceed matching networks improvement surely arises adalstm’s recurrent structure known apply well sequence-based tasks language domain. however strengths conditionally shifted neurons alternative approaches like matching networks ported easily various neural architectures. comparing direct feedback information gradientbased variant across full suite experiments observe overall information performs competitively well. even positively information speeds model runtime considerably. example -layer adalstm processed test episodes seconds using gradient information seconds information representing speedup tioning information remove preprocessing eqn. replace learned simple learned scalar multiplies gradient vector ∇ti. case clearly interpret conditional shift one-step gradient update activation values shown figure adacnn model learned scaling loses percentage points accuracy image classiﬁcation task. however figure adalstm performance plummets learned scaling dropping test accuracy direct feedback conditioning information cannot scalar parameter c-dimensional information vector neuron therefore parameterize one-layer perceptron case rather deep mlp. using simple perceptron process direct feedback information decreased test accuracy signiﬁcantly mini-imagenet tasks highlights deeper mapping function crucial processing conditioning information. ﬁnally attempted ﬁxed word-embedding layer task. -layer adalstm model ﬁxed embeddings performed quite well obtaining ./.% ./.% accuracy -shot problems; competitive results given table introduced conditionally shifted neurons mechanism rapid adaptation neural networks. conditionally shifted neurons generic easily incorporated various neural architectures. also computationally efﬁcient compared alternative metalearning methods adapt synaptic connections neurons. proposed variants conditioning information csns based error gradients another based feedback alignment methods. latter efﬁcient require sequential backpropagation procedure achieves competitive performance former. demonstrated empirically models conditionally shifted neurons improve state metalearning benchmarks vision language domains. figure model ablation adacnn tested mini-imagenet one-shot task. blue scalar multiplier perceptron green without normalization; baseline model. figure model ablation one-shot language modeling task single layer adalstm. report average accuracy random test tasks. yellow scalar multiplier perceptron violet without normalization; grey baseline model. ﬁrst ablation determine effect normalizing task shifts nonlinear activation function ablating activation function simply adding resulted slight performance drop miniimagenet task signiﬁcant decrease one-shot task variants conditioning information. conclude squashing conditional shift range neuron’s standard activation value beneﬁcial. references andrychowicz marcin denil misha gomez sergio hoffman matthew pfau david schaul freitas nando. learning learn gradient descent advances neural information gradient descent. processing systems bachman philip sordoni alessandro trischler adam. learning algorithms active learning. precup doina whye proceedings international conference machine learning volume proceedings machine learning research international convention centre sydney australia pmlr. http//proceedings. mlr.press/v/bachmana.html. bengio yoshua bengio samy cloutier jocelyn. learning synaptic learning rule. universit´e montr´eal d´epartement d’informatique recherche op´erationnelle vries harm strub florian mary j´er´emie larochelle hugo pietquin olivier courville aaron modulating early visual processing language. advances neural information processing systems finn chelsea abbeel pieter levine sergey. modelagnostic meta-learning fast adaptation deep networks. precup doina whye proceedings international conference machine learning volume proceedings machine learning research international convention centre sydney australia pmlr. http//proceedings.mlr. press/v/finna.html. goodfellow mirza mehdi xiao courville aaron bengio yoshua. empirical investigation catastrophic forgetting gradient-based neural networks. iclr kaiming zhang xiangyu shaoqing jian. delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation. proceedings ieee international conference computer vision kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition hochreiter sepp younger steven conwell peter learning learn using gradient descent. international conference artiﬁcial neural networks springer ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing proceedings internal covariate shift. international conference international conference machine learning volume icml’ jmlr.org http//dl.acm.org/ citation.cfm?id=.. kirkpatrick james pascanu razvan rabinowitz neil veness joel desjardins guillaume rusu andrei milan kieran quan john ramalho tiago grabskabarwinska agnieszka overcoming catastrophic forgetting neural networks. arxiv preprint arxiv. jimmy swersky kevin fidler sanja predicting deep zero-shot convolutional neural networks using textual descriptions. proceedings ieee international conference computer vision lillicrap timothy cownden daniel tweed douglas akerman colin random synaptic feedback weights support error backpropagation deep learning. nature communications szegedy christian zaremba wojciech sutskever ilya bruna joan erhan dumitru goodfellow fergus rob. intriguing properties neural networks. arxiv preprint arxiv. munkhdalai tsendsuren hong. meta networks. precup doina whye proceedings international conference machine learning volume proceedings machine learning research international convention centre sydney australia pmlr. http//proceedings.mlr. press/v/munkhdalaia.html. tokui seiya oono kenta hido shohei clayton next-generation open proceedsource framework deep learning. ings workshop machine learning systems twenty-ninth annual conference neural information processing systems http//learningsys.org/papers/ learningsys__paper_.pdf. perez ethan strub florian vries harm dumoulin vincent courville aaron. film visual reasoning general conditioning layer. arxiv preprint arxiv. santoro adam bartunov sergey botvinick matthew wierstra daan lillicrap timothy. meta-learning memory-augmented neural networks. proceedings international conference machine learning srivastava rupesh masci jonathan kazerounian sohrob gomez faustino schmidhuber j¨urgen. compete compute. advances neural information processing systems hyperparameters models listed tables size throughout experiments. dropout rate applied layer adaffn. adaptive models input dropout rate dropout last layers varied shown table memory constraints used adalstm smaller number hidden units deep models applying -shot tasks. neural network weights initialized using method. hard gradient clipping threshold adacnn model gradient clipping performed models. listed setup optimizers table adam optimizer rest hyperparameters default values although different parameterizations meta learner function improve performance simplicity used -layer relu activation units layer. acts coordinate-wise processes conditioning information neuron independently. occasionally observed difﬁculty optimizing lstm+adaffn models often seeing improvement training loss certain initializations. decreasing learning rate case information applying dropouts adaffn layers helped training model.", "year": 2017}