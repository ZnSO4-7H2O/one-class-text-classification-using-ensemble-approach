{"title": "Domain Generalization for Object Recognition with Multi-task  Autoencoders", "tag": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "abstract": "The problem of domain generalization is to take knowledge acquired from a number of related domains where training data is available, and to then successfully apply it to previously unseen domains. We propose a new feature learning algorithm, Multi-Task Autoencoder (MTAE), that provides good generalization performance for cross-domain object recognition.  Our algorithm extends the standard denoising autoencoder framework by substituting artificially induced corruption with naturally occurring inter-domain variability in the appearance of objects. Instead of reconstructing images from noisy versions, MTAE learns to transform the original image into analogs in multiple related domains. It thereby learns features that are robust to variations across domains. The learnt features are then used as inputs to a classifier.  We evaluated the performance of the algorithm on benchmark image recognition datasets, where the task is to learn features from multiple datasets and to then predict the image label from unseen datasets. We found that (denoising) MTAE outperforms alternative autoencoder-based models as well as the current state-of-the-art algorithms for domain generalization.", "text": "mains. example frontal-views rotated-views correspond different domains. alternatively associate views domains standard image datasets pascal ofﬁce problem learning multiple source domains testing unseen target domains referred domain generalization domain probability distribution samples yi}nk drawn. source domains provide training samples whereas distinct target domains used testing. standard supervised learning framework assumed source target domains coincide. dataset bias becomes signiﬁcant problem training test domains differ applying classiﬁer trained dataset images sampled antypically results poor performance goal paper learn features improve generalization performance across domains. contribution. challenge build system recognizes objects previously unseen datasets given multiple training datasets. introduce multi-task autoencoder feature learning algorithm uses multi-task strategy learn unbiased object features task data reconstruction. autoencoders introduced address problem ‘backpropagation without teacher’ using inputs labels learning reconstruct minimal distortion denoising autoencoders particular powerful basic circuit unsupervised representation learning intuitively corrupting inputs forces autoencoders learn representations robust noise. paper proposes broader view autoencoders generic circuits learning invariant features. main contribution training strategy based naturally occurring transformations rotations viewing angle dilations apparent object size shifts lighting conditions. resulting multi-task autoencoder learns features robust real-world image variability therefore generalize well across domains. extensive experiments show mtae denoising criterion outperforms prior state-of-the-art domain generalization various cross-dataset recognition tasks. problem domain generalization take knowledge acquired number related domains training data available successfully apply previously unseen domains. propose feature learning algorithm multi-task autoencoder provides good generalization performance crossdomain object recognition. algorithm extends standard denoising autoencoder framework substituting artiﬁcially induced corruption naturally occurring inter-domain variability appearance objects. instead reconstructing images noisy versions mtae learns transform original image analogs multiple related domains. thereby learns features robust variations across domains. learnt features used inputs classiﬁer. algorithm benchmark image recognition datasets task learn features multiple datasets predict image label unseen datasets. found mtae outperforms alternative autoencoder-based models well current state-of-the-art algorithms domain generalization. recent years seen dramatic advances object recognition deep learning algorithms much increased performance derives applying large networks massive labeled datasets pascal imagenet unfortunately dataset bias include factors backgrounds camera viewpoints illumination often causes algorithms generalize poorly across datasets signiﬁcantly limusefulness practical applications. developing algorithms invariant dataset bias therefore compelling problem. autoencoders become established pretraining model deep learning autoencoder training consists stages encoding decoding. given unlabeled input single hidden layer autoencoder formulated rdx×dy rdy×dx inputto-hidden hidden-to-output connection weights respectively hidden node vector σenc senc] σdec sdec] element-wise non-linear activation functions senc sdec necessarily identical. popular choices activation function e.g. sigmoid rectiﬁed linear max. autoencoder parameters {xi}n input data. learning corresponds minimizing following objective loss function usually form least square cross-entropy loss regularization term used avoid overﬁtting. objective optimized backpropagation algorithm apply autoencoders pixels visual object images weights usually form visually meaningful ﬁlters interpreted qualitatively. create discriminative model using learnt autoencoder model either following options considered feature σencx) extracted used input supervised learning algorithms keeping weight matrix ﬁxed; learnt weight matrix used initialize neural network model updated supervised neural network training denoising autoencoders contractive autoencoders proposed extract features robust small changes input. daes objective reconstruct clean input given corrupted counterpart commonly used types corruption zero-masking gaussian salt-and-pepper noise. features extracted proven discriminative ones extracted domain generalization recently attracted attention classiﬁcation tasks including automatic gating cytometry data object recognition khosla proposed multi-task max-margin classiﬁer refer undo-bias explicitly encodes dataset-speciﬁc biases feature space. biases used push dataset-speciﬁc weights similar global weights. fang developed unbiased metric learning based learning rank framework. validated weakly-labeled images produces less biased distance metric provides good object recognition performance. validated weakly-labeled images. recently extended exemplar-svm domain generalization adding nuclear norm-based regularizer captures likelihoods positive samples. proposed model denoted lre-svm. works object recognition exist address similar problem sense unknown targets unseen dataset contains noisy images training however designed noise-speciﬁc suffer dataset bias observing objects different types noise. closely related task domain generalization domain adaptation unlabeled samples target dataset available training. many domain adaptation algorithms proposed object recognition domain adaptation algorithms readily applicable domain generalization since information available target domain. proposed algorithm based feature learning approach. feature learning great interest machine learning community since emergence deep learning references therein). feature learning methods successfully applied domain adaptation transfer learning applications best knowledge prior work along lines difﬁcult problem domain generalization i.e. create useful representations without observing target domain. goal learn features provide good domain generalization. extend autoencoder model jointly learns multiple data-reconstruction tasks taken related domains. strategy motivated prior work demonstrating learning multiple related tasks improve performance novel related task relative methods trained singletask words matrix data points taken domains matrix replicated data sets taken domain. replication imposed constructs input-output pairs autoencoder learning algorithm. practice algorithm implemented efﬁciently without replicating matrix memory. regularization term. work standard l-norm weight penalty stochastic gradient descent applied reconstruction task achieve objective training completed optimal shared weights obtained. stopping criterion empirically determined monitoring average loss reconstruction tasks training process stopped average loss stabilizes. detailed steps mtae training summarized algorithm training protocol supplemented denoising criterion induce robust-to-noise features. simply replace corrupted counterpart ˜¯xi name mtae model applying denoising criterion denoising multi-task autoencoder refer proposed domain generalization algorithm multi-task autoencoder architectural viewpoint mtae autoencoder multiple output layers fig. input-hidden weights represent shared parameters hidden-output weights represent domain-speciﬁc parameters. architecture similar supervised multi-task neural networks proposed caruana main difference output layers mtae correspond different domains instead different class labels. important component mtae training strategy constructs generalized denoising autoencoder learns invariances naturally occurring transformations. denoising autoencoders focus special case transformation simply noise. contrast mtae training treats speciﬁc perspective object corrupted counterpart another perspective autoencoder objective reformulated along lines multi-task learning model aims jointly achieve good reconstruction source views given particular view. example applying strategy handwritten digit images several views mtae learns representations invariant across source views section types reconstruction tasks performed during mtae training self-domain reconstruction between-domain reconstruction. given source domains reconstruction tasks task self-domain reconstructions remaining tasks between-domain reconstructions. note self-domain reconstruction identical standard autoencoder reconstruction mtae requires every instance particular domain category-level corresponding pair every domain. mtae’s apparent applicability therefore limited situations number source samples category every domain. however unbalanced samples category occur frequently applications. overcome issue propose simple random selection procedure applied between-domain reconstructions denoted rand-sel simply balancing samples category keeping category-level correspondence. detail rand-sel strategy follows. number subsamples c-th category number samples c-th category domain category domain select samples randomly procedure executed every iteration mtae algorithm line algorithm conducted experiments several real world object datasets evaluate domain generalization ability proposed system. section investigate behaviour mtae comparison standard single-task autoencoder models pixels proof-ofprinciple. section evaluate performance mtae several state-of-the-art algorithms modpart understand mtae’s behavior learning multiple domains form physically reasonable object transformations roll pitch rotation dilation. task categorize objects views presented training. evaluate mtae several autoencoder models. perform evaluation variety object views constructed mnist handwritten digit object datasets. data setup. created four datasets mnist eth- images mnist-r mnist-s ethp eth-y. sets contain multiple domains every instance domain pair another domain. detailed setting dataset follows. mnist-r contains domains corresponding degree roll rotation. randomly chose digit images classes original mnist training represent basic view i.e. degree rotation; class images. image subsampled representation simplify computation. subset images denoted created rotated views difference counterclockwise direction denoted mnist-s counterpart mnist-r domain corresponds dilation factor. views denoted subscripts represent dilation factors respect eth-p consists eight object classes subcategories class. subcategory different views respect pose angles. took views class denoted represent horizontal poses i.e. pitch-rotated views starting view side view. makes number instances view. greyscaled subsampled images eth-y contains views eth- representing vertical poses i.e. yaw-rotated views starting right-side view left-side view denoted e+y◦ e+y◦ e−y◦ e−y◦. settings image dimensionality preprocessing stage similar eth-p. examples resulting views depicted fig. baselines. compared classiﬁcation performance models several single-task autoencoder modnote rotation angle basic view perfectly since standard autoencoder model trained stochastic gradient descent object views concatenated inputs. number hidden nodes ﬁxed mnist dataset eth- dataset. learning rate weight decay penalty number iterations empirically determined respectively. mtae proposed multi-task autoencoder model identical hyper-parameter settings except learning rate also chosen empirically. value provides lower reconstruction error task visually clearer ﬁrst layer weights. also evaluated unsupervised domain-invariant component analysis datasets completness. hyper-parameters tuned using fold cross-validation source domains. also experiments using supervised variant dica tuning strategy. surprisingly peak performance udica consistently higher dica. possible explanation dirac kernel function measuring label similarity less appropriate application. normalized pixels range autoencoder-based models l-unit ball udica. evaluated classiﬁcation accuracies learnt features using multi-class linear kernel using linear kernel keeps classiﬁer simple since main focus feature extraction process. liblinear package used l-svm. cross-domain recognition results. evaluated object classiﬁcation accuracies algorithm leaveone-domain-out test i.e. taking domain test remaining domains training set. autoencoder-based algorithms repeated experiments leave-one-domain-out case times reported average accuracies. standard deviations reported since small detailed results mnist-r mnist-s seen table average mtae second best classiﬁcation accuracies particular outperforms single-task autoencoder models. indicates multi-task feature learning strategy provide better discriminative features single-task feature learning w.r.t. unseen object views. algorithm best performance datasets d-mtae. speciﬁcally d-mtae performs best average also individual cross-domain cases mnist-r mnist-s. closest single-task feature learning competitor d-mtae cae. suggests denoising criterion strongly beneﬁts domain generalization. denoising criterion also useful single-task feature learning although yield competitive accuracies performance. also obtain consistent trend eth-p eth-y datasets i.e. d-mtae mtae best second best models. detail d-mtae mtae produce average accuracies eth-p eth-y. observe anomaly mnist-r dataset performance worse neighbors anomaly appears related geometry mnist-r digits. found frequently misclassiﬁed digits rarely occurs mnist-r’s domains typically phenomenon applies l-svm. weight visualization. useful insight obtained considering qualitative outcome mtae training visualizing ﬁrst layer weights. figure depicts weights autoencoder models including ours mnist-r dataset. mtae d-mtae’s weights form ﬁlters tend capture underlying transformation across mnist-r views rotation. effect unseen ﬁlters explain contents handwritten digits form fourier component-like descriptors local blob detectors stroke detectors might reason mtae d-mtae features provide better domain generalization since implicitly capture relationship among source domains. next discuss difference mtae dmtae ﬁlters. d-mtae ﬁlters capture object transformation also produce features describe object contents distinctively. ﬁlters basically combine properties mtae ﬁlters might beneﬁt domain generalization. invariance analysis. possible explanation effectiveness mtae relates dimensionality manifold feature space samples concentrate. test hypothesis examine singular value spectrum jacobian matrix input feature vectors respectively spectrum describes local dimensionality manifold around samples concentrate. spectrum decays rapidly manifold locally dimension. figure depicts average singular value spectrum test samples mnist-r mnist-s. spectrum d-mtae decays rapidly followed mtae decaying slowest. ranking decay rates four algorithms matches ranking terms empirical performance table figure thus provides partial conﬁrmation hypothesis. however detailed analysis necessary drawing strong conclusions. second experiments evaluated crossrecognition performance proposed algorithms modern object datasets. show mtae d-mtae applicable competitive general setting. used ofﬁce caltech pascal labelme datasets formed cross-domain datasets. general strategy extend generalization features extracted current best deep convolutional neural network data setup. ﬁrst cross-domain dataset consists images pascal labelme caltech- datasets represents domain. object-centric dataset scene-centric. dataset abbrefigure visualization randomly chosen weights pretraining mnist-r dataset. patch corresponds learnt weight matrix represents ﬁlter. weight value depicted white depicted black otherwise gray. viate vlcs shares object categories ‘bird’ ‘car’ ‘chair’ ‘dog’ ‘person’. domain vlcs dataset divided training test random selection overall dataset. detailed training-test conﬁguration domain summarized table instead using features directly employed decaf features inputs algorithms. features dimensionality publicly available. second cross-domain dataset referred ofﬁce+caltech dataset contains four domains amazon webcam dslr caltech- share common categories. dataset instances category domain instances total. also used decaf features extracted dataset also publicly available. training protocol. datasets utilized mtae d-mtae learning pretraining fullyconnected neural network hidden layer number hidden nodes less input dimensionality. pretraining stage number output layers number source domains –each corresponds particular source domain. sigmoid activation linear activation functions used σenc σdec. mtae pretraining learning rate number epochs batch size empirically determined w.r.t. smallest average reconstruction loss. d-mtae hyper-parameter setting mtae except additional zero-masking corruption level pretraining completed performed back-propagation ﬁne-tuning using softmax output ﬁrst layer weights initialized either mtae d-mtae learnt weights. supervised learning hyper-parameters tuned using -fold cross validation source domains. denote overall models mtae+hnn d-mtae+hnn. baselines. compared proposed models baselines structural metric learning-based algorithm aims learn less biased distance metric classiﬁcation tasks. initial tuning proposal method using weakly-labeled data retrieved querying class labels search engine. however tuned hyperparameters using strategy others fair comparison. lre-svm non-linear exemplar-svms model nuclear norm regularization impose low-rank likelihood matrix. four hyper-parameters tuned using fcv. report performance terms classiﬁcation accuracy following algorithms optimized stochastically independent training processes using best performing hyper-parameters times reported average accuracies. similar previous experiment report standard deviations small values results vlcs dataset. ﬁrst conducted standard training-test evaluation using l-svm i.e. learning model training domain testing test another domain check groundtruth performance also identify existence dataset bias. performance summarized table bias indeed exists every domain despite decaf sixth layer features state-of-the-art deep convolutional neural network. performance best cross-domain performance groundtruth large difference. evaluated domain generalization performance algorithm. conducted leave-one-domainevaluation induces four cross-domain cases. complete recognition results shown table general dataset bias reduced algorithms learning multiple source domains furthermore caltech- object-centric appears easiest dataset recognize consistent investigation scene-centric datasets tend generalize well object-centric datasets. surprisingly performance already achieved competitive accuracy compared complicated state-of-the-art algorithms undo-bias lre-svm. furthermore d-mtae outperforms algorithms three four crossdomain cases average mtae second best performance average. results ofﬁce+caltech dataset. report experiment results ofﬁce+caltech dataset. table summarizes recognition accuracies algorithm four cross-domain cases. d-mtae+hnn best performance four cross-domain cases ranks second remaining cases. average dmtae+hnn better performance prior stateof-the-art dataset lre-svm proposed approach multi-task feature learning reduces dataset bias object recognition. main idea extract features shared across domains training protocol that given image domain learns reconstruct analogs image domains. strategy yields variants multi-task autoencoder denoising mtae incorporates denoising criterion. comprehensive suite crossdomain object recognition evaluations shows algorithms successfully learn domain-invariant features yielding state-of-the-art performance predicting labels objects unseen target domains. results suggest several directions study. firstly worth investigating whether stacking mtaes improves performance. secondly effective procedures handling unbalanced samples required since occur frequently practice. finally natural application mtaes streaming data video appearance objects transforms real-time. problem dataset bias remains solved best model vlcs dataset achieved accuracies less average. partial explanation poor performance compared supervised learning insufﬁcient training data class-overlap across datasets quite small progress domain generalization requires larger datasets.", "year": 2015}