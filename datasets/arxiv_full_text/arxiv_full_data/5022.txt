{"title": "Continuous Adaptation via Meta-Learning in Nonstationary and Competitive  Environments", "tag": ["cs.LG", "cs.AI"], "abstract": "Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation strategies. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.", "text": "ability continuously learn adapt limited experience nonstationary environments important milestone path towards general intelligence. paper cast problem continuous adaptation learning-to-learn framework. develop simple gradient-based meta-learning algorithm suitable adaptation dynamically changing adversarial scenarios. additionally design multi-agent competitive environment robosumo deﬁne iterated adaptation games testing various aspects continuous adaptation. demonstrate meta-learning enables signiﬁcantly efﬁcient adaptation reactive baselines few-shot regime. experiments population agents learn compete suggest meta-learners ﬁttest. recent progress reinforcement learning achieved impressive results ranging playing games applications dialogue systems robotics despite progress learning algorithms solving many tasks designed deal stationary environments. hand real-world often nonstationary either complexity changes dynamics objectives environment life-time system presence multiple learning actors nonstationarity breaks standard assumptions requires agents continuously adapt training execution time order succeed. learning nonstationary conditions challenging. classical approaches dealing nonstationarity usually based context detection tracking i.e. reacting already happened changes environment continuously ﬁne-tuning policy. unfortunately modern deep algorithms able achieve super-human performance certain tasks known sample inefﬁcient. nevertheless nonstationarity allows limited interaction properties environment change. thus immediately puts learning few-shot regime often renders simple ﬁne-tuning methods impractical. nonstationary environment seen sequence stationary tasks hence propose tackle multi-task learning problem learning-to-learn approaches particularly appealing few-shot regime produce ﬂexible learning rules generalize handful examples. meta-learning shown promising results supervised domain gained attention research community recently paper develop gradient-based meta-learning algorithm similar suitable continuous adaptation agents nonstationary environments. concretely agents meta-learn anticipate changes environment update policies accordingly. complexity emergent behavior practical interest applications ranging multiplayer games coordinating self-driving ﬂeets multi-agent environments nonstationary perspective individual agent since actors learning changing concurrently paper consider problem continuous adaptation learning opponent competitive multi-agent setting. design robosumo—a environment simulated physics allows pairs agents compete other. test continuous adaptation introduce iterated adaptation games—a setting trained agent competes opponent multiple rounds repeated game allowed update policies change behaviors rounds. iterated games agent’s perspective environment changes round round agent ought adapt order game. additionally competitive component environment makes nonstationary also adversarial provides natural training curriculum encourages learning robust strategies evaluate meta-learning agents along number baselines locomotion task handcrafted nonstationarity iterated adaptation games robosumo. results demonstrate meta-learned strategies clearly dominate adaptation methods few-shot regime singlemulti-agent settings. finally carry large-scale experiment train diverse population agents different morphologies policy architectures adaptation methods make interact competing iterated games. evaluate agents based trueskills games well evolve population whole generations—the agents lose disappear winners duplicated. results suggest agents meta-learned adaptation strategies ﬁttest. videos demonstrate adaptation behaviors available https//goo.gl/tboqan. problem continuous adaptation considered work variant continual learning related lifelong neverending learning. life-long learning systems solving multiple tasks sequentially efﬁciently transferring utilizing knowledge already learned tasks tasks minimizing effect catastrophic forgetting neverending learning concerned mastering ﬁxed tasks iterations keeps growing performance tasks keeps improving iteration iteration. scope continuous adaptation narrower precise. life-long never-ending learning settings deﬁned general multi-task problems continuous adaptation targets solve single nonstationary task environment. nonstationarity former problems exists dictated selected sequence tasks. latter case assume nonstationarity caused underlying dynamics properties given task ﬁrst place finally life-long never-ending scenarios boundary training execution blurred systems constantly operate training regime. continuous adaptation hand expects agent adapt changes environment execution time pressure limited data interaction experience changes. nonstationarity multi-agent environments well known issue extensively studied context learning simple multi-player iterated games episode one-shot interaction games discovering converging nash equilibrium strategy success learning agents. modeling exploiting opponents even learning processes advantageous improves convergence helps discover equilibria certain properties contrast episode robosumo consists multiple steps happens continuous time requires learning good intra-episodic controller. finding nash equilibria setting hard. thus fast adaptation becomes viable strategies changing opponents. fig. probabilistic model maml multi-task setting. task policies trajectories random variables dependencies encoded edges given graph. extended model suitable continuous adaptation task changing dynamically non-stationarity environment. policy trajectories previous step used construct policy current step. computation graph meta-update φi+. boxes represent replicas policy graphs speciﬁed parameters. model optimized truncated backpropagation time starting lti+. proposed method continuous adaptation follows general meta-learning paradigm i.e. learns high-level procedure used generate good policy time environment changes. wealth work meta-learning including methods learning update rules neural models explored past recent approaches focused learning optimizers deep networks generating model parameters learning task embeddings including memory-based approaches learning learn implicitly simply learning good initialization problem continuous adaptation nonstationary environments immediately puts learning few-shot regime agent must learn limited amount experience collect environment changes. therefore build method upon previous work gradient-based model-agnostic meta-learning shown successful fewshot settings section re-derive maml multi-task reinforcement learning probabilistic perspective extend dynamically changing tasks. task-speciﬁc loss function maps trajectory loss value i.e. deﬁne markovian dynamics environment task denotes horizon; observations actions elements observation space action space respectively. loss trajectory negative cumulative reward goal meta-learning procedure which given access limited experience task sampled produce good policy solving formally querying would like construct trajectories task policy denoted task-speciﬁc policy would minimize expected subsequent loss task particular maml constructs parameters task-speciﬁc policy using gradient w.r.t. algorithm adaptation execution time. input stream tasks initialize incoming tasks task stream. solve using policy. solving collect trajectories trajectories obtained respectively. general think task trajectories policies random variables generated conditional distribution meta-update equivalent assuming delta distribution policy gradient method gradient follows classical multi-task setting make assumptions distribution tasks environment nonstationary sequence stationary tasks certain timescale tasks correspond different dynamics environment. then deﬁned environment changes tasks become sequentially dependent. hence would like exploit dependence consecutive tasks meta-learn rule keeps updating policy minimizes total expected loss encountered interaction changing environment. instance multi-agent setting playing opponent changes strategy incrementally agent ideally meta-learn anticipate changes update policy accordingly. probabilistic language nonstationary environment equivalent distribution tasks represented markov chain goal minimize expected loss chain tasks length here denote initial transition probabilities markov chain tasks. note deal markovian dynamics levels hierarchy upper level dynamics tasks lower level mdps represent particular tasks objectives ltiti+ depend meta-learning process deﬁned. since interested adaptation updates optimal respect markovian transitions tasks deﬁne meta-loss pair consecutive tasks follows come principal difference loss trajectories current task used construct policy good upcoming task ti+. note even though policy parameters sequentially dependent always start initial parameters hence optimizing ltiti+ equivalent truncated backpropagation time unit chain tasks. construct parameters policy task start multiple meta-gradient steps adaptive step sizes follows meta-gradient step sizes optimized jointly computation {αm}m graph meta-update given fig. expression policy gradient expectation taken w.r.t. details analog policy gradient theorem setting given appendix note computing adaptation updates requires interacting environment computing meta-loss ltiti+ requires using hence interacting task sequence twice. often impossible execution time hence slightly different algorithms training execution times. meta-learning training time. access distribution pairs consecutive tasks meta-learn adaptation updates optimizing jointly gradient method given algorithm collect trajectories interacting ti+. intuitively algorithm searching adaptation update computed trajectories brings policy good solving ti+. main assumption trajectories contain information ti+. note treat adaptation steps part computation graph optimize backpropagation entire graph requires computing second order derivatives. adaptation execution time. note compute unbiased adaptation gradients training time collect experience using test time environment nonstationarity usually luxury access task multiple times. thus keep acting according re-use past experience compute updates incoming task adjust fact past experience collected policy different importance weight correction. case single step meta-update have this stability considerations. empirically optimization sequential updates unstable often tends diverge starting initialization leads better behavior. empirically turns constructing multiple meta-gradient steps adaptive fig. three types agents used experiments. robots differ anatomy number legs positions constraints thigh knee joints. nonstationary locomotion environment. torques applied red-colored legs scaled dynamically changing factor. robosumo environment. πφi− used rollout respectively. extending importance weight correction multi-step updates straightforward requires simply adding importance weights intermediate steps designed environments testing different aspects continuous adaptation methods scenarios simple environments change episode episode according underlying dynamics competitive multi-agent environment robosumo allows different agents play sequences games keep adapting incremental changes other’s policies. environments based mujoco physics simulator agents simple multi-leg robots shown fig. first consider problem robotic locomotion changing environment. six-leg agent observes absolute position velocity body angles velocities legs acts applying torques joints. agent rewarded proportionally moving speed ﬁxed direction. induce nonstationarity select pair legs agent scale torques applied corresponding joints factor linearly changes course episodes. words ﬁrst episode legs fully functional last episode agent legs fully paralyzed goal agent learn adapt episode episode changing gait able move maximal speed given direction despite changes environment also ways select pair legs six-leg creature gives different nonstationary environments. allows subset environments training separate held testing. training testing procedures described next section. multi-agent environment robosumo allows agents compete -vs- regime following standard sumo rules. introduce three types agents spider different anatomies game agent observes positions opponent joint angles corresponding velocities forces exerted body action spaces continuous. iterated adaptation games. test adaptation deﬁne iterated adaptation game game pair agents consists rounds consists ﬁxed length episodes outcome round either loss draw. agent wins majority rounds declared winner game. distinguishing aspects setup first agents trained either pure self-play versus opponents ﬁxed training collection. test time face opponent testing collection. second agents allowed learn test time. particular fig. agent competes opponent iterated adaptation games consist multi-episode rounds. agent wins round wins majority episodes agent opponent update policies round round agent exploit fact plays opponent multiple consecutive rounds adjust behavior accordingly. since opponent also adapting setup allows test different continuous adaptation strategies versus other. reward shaping. robosumo rewards naturally sparse winner gets loser penalized case draw opponents receive points. encourage fast learning early stages training shape rewards given agents following agent gets reward staying closer center ring moving towards opponent exerting forces opponent’s body gets penalty inversely proportional opponent’s distance center ring. test time agents continue access shaped reward well update policies. throughout experiments discounted rewards discount factor details appendix calibration. study adaptation need well-calibrated environment none agents initial advantage. ensure balance increased mass weaker agents rates games agent type versus type non-adaptation regime became almost equal goal test different adaptation strategies proposed nonstationary settings. however known test-time behavior agent highly depend variety factors besides chosen adaptation method including training curriculum training algorithm policy class etc. hence ﬁrst describe precise setup experiments eliminate irrelevant factors focus effects adaptation. low-level details deferred appendices. video highlights experiments available https//goo.gl/tboqan. policies. consider types policy networks -layer embedding followed -layer lstm architecture additionally takes previous reward done signals inputs step keeps recurrent state throughout entire interaction given environment resets state latter changes. advantage functions networks structure corresponding policies parameter sharing two. meta-learning agents policy advantage function structures baselines learn -step meta-update adaptive step sizes given illustrations details architectures given appendix meta-learning. compute meta-updates gradients negative discounted rewards received number previous interactions environment. training time meta-learners interact environment twice ﬁrst using initial policy meta-updated policy test time agents limited interacting environment once hence always according compute meta-updates using importance-weight correction additionally reduce variance meta-updates test time agents store experience collected interaction test environment experience buffer keep re-using experience fig. episodic rewards consecutive episodes held nonstationary locomotion environments. evaluate adaptation strategies environment episodes followed full reset environment policy meta-updates shaded regions conﬁdence intervals. best viewed color. update size experience buffer ﬁxed episodes nonstationary locomotion episodes robosumo. details given appendix adaptation baselines. consider following three baseline strategies naive implicit adaptation adaptation tracking keeps updates execution time. training nonstationary locomotion. train methods collection nonstationary locomotion environments constructed choosing possible pairs legs whose joint torques scaled except pairs held testing agents trained environments concurrently i.e. compute policy update rollout environments parallel compute aggregate average gradients lstm policies retain state course episodes environment. meta-learning agents compute meta-updates nonstationary environment separately. training robosumo. ensure consistency training curriculum agents ﬁrst pre-train number policies type every agent type pure self-play algorithm snapshot save versions pretrained policies iteration. lets train agents play versions pre-trained opponents various stages mastery. next train baselines meta-learning agents pool pre-trained opponents concurrently. iteration randomly select opponent training pool sample version opponent’s policy rollout opponent. baseline policies trained ppo; meta-learners also used outer loop optimizing parameters. retain states lstm policies course interaction version opponent reset time opponent version updated. similarly locomotion setup meta-learners compute meta-updates opponent training pool separately. detailed description distributed training given appendix experimental design. design experiments answer following questions interaction environment changes strictly limited episodes behavior different adaptation methods nonstationary locomotion competitive multi-agent environments? sample complexity different methods i.e. many episodes required method successfully adapt changes? test controlling amount experience agent allowed form environment changes. competitive multi-agent environments besides self-play plenty ways train agents e.g. train pairs concurrently randomly match switch opponents iterations. found concurrent training often leads unbalanced population agents trained vastly different curricula introduces spurious effects interfere analysis adaptation. hence leave study adaptation naturally emerging curricula multi-agent settings future work. fig. rates different adaptation strategies iterated games versus different pre-trained opponents. test time agents opponents started versions opponents’ versions increasing consecutive round learning self-play agents allowed adapt limited experience given opponent. round consisted episodes. iterated game repeated times; shaded regions denote bootstrapped conﬁdence intervals; smoothing. best viewed color. given diverse population agents trained curriculum population agents evolved several generations—such agents interact iterated adaptation games lose disappear winners duplicated—what happens proportions different agents population? few-shot adaptation nonstationary locomotion environments. trained baselines meta-learning policies described sec. selected testing environments corresponded disabling different pairs legs six-leg agent back middle front legs. results presented fig. three observations first ﬁrst episode meta-learned initial policy turns suboptimal task however episodes starts performing policies. second episodes meta-updated policies perform much better rest. note gradient meta-updates adaptation meta-learners; meta-updates computed based experience collected previous episodes. finally tracking able improve upon baseline without adaptation sometimes leads even worse results. adaptation robosumo few-shot constraint. evaluate different adaptation methods competitive multi-agent setting consistently consider variation iterated adaptation game changes opponent’s policies test time pre-determined unknown agents. particular pre-train opponents lstm policies self-play snapshot policies iteration. next iterated games trained agents different adaptation algorithms versus policy snapshots pre-trained opponents. crucially policy version opponent keeps increasing round round training self-play. agents keep adapting increasingly competent versions opponent setup allows test different adaptation strategies consistently learning opponents. results given fig. note meta-learned adaptation strategies cases able adapt improve win-rates within episodes interaction constantly improving opponents. hand performance baselines often deteriorates rounds iterated games. note pre-trained opponents observing episodes self-play iteration agents access episodes round. sample complexity adaptation robosumo. meta-learning helps update suitable fast few-shot adaptation. however different adaptation methods behave experience available? answer question employ setup previously vary number episodes round iterated game iterated game repeated times measure win-rates last rounds game. results presented fig. number episodes round goes adaptation tracking technically turns learning test time able learn compete self-trained opponents never seen training time. meta-learned adaptation strategy performed near constantly few-shot standard regimes. suggests meta-learned strategy acquires particular bias training time allows perform better limited experience also limits capacity utilizing data. note that design meta-updates ﬁxed gradient steps step-sizes tracking keeps updating policy throughout iterated game. allowing meta-updates become ﬂexible availability data help overcome limitation. leave future work. combining different adaptation strategies different policies agents different morphologies puts situation diverse population agents would like rank according level mastery adaptation employ trueskill metric similar rating popular -vs- competitive video-games. experiment consider population trained agents agent types different policy adaptation combinations different stages training first assume initial distribution agent’s skill default distance guarantees winning next randomly generate matches pairs opponents adapt competing -round iterated adaptation games game record outcome updated belief skill corresponding agents using trueskill algorithm. distributions skill agents type iterated adaptation games randomly selected players pool visualized fig. fig. trueskill top-performing mlplstm-based agents. trueskill computed based outcomes iterated adaptation games randomly selected pairs opponents population pre-trained agents. observations make first recurrent policies dominant. second adaptation tended perform equally little worse plain lstm without tracking setup. finally agents meta-learned adaptation rules training time consistently demonstrated higher skill scores categories corresponding different policies agent types. finally enlarge population agents duplicating times evolve several generations follows. initially start balanced population different creatures. next randomly match pairs agents make play iterated adaptation games remove agents lost population duplicate winners. process repeated times. result presented many agents quickly disappear form initially uniform population meta-learners dominating. work proposed simple gradient-based meta-learning approach suitable continuous adaptation nonstationary environments. idea method regard nonstationarity sequence stationary tasks train agents exploit dependencies consecutive tasks handle similar nonstationarities execution time. applied method nonstationary locomotion within competitive multi-agent setting. latter designed robosumo environment deﬁned iterated adaptation games allowed test various aspects adaptation strategies. cases meta-learned adaptation rules efﬁcient baselines few-shot regime. additionally agents meta-learned adapt demonstrated highest level skill competing iterated games other. problem continuous adaptation nonstationary competitive environments solved work ﬁrst attempt meta-learning setup. indeed meta-learning algorithm limiting assumptions design choices made mainly computational considerations. first meta-learning rule one-step-ahead update policy computationally similar backpropagation time unit time lag. could potentially extended fully recurrent meta-updates take account full history interaction changing environment. additionally meta-updates based gradients surrogate loss function. updates explicitly optimized loss required computing second order derivatives training time slowing training process order magnitude compared baselines. utilizing information provided loss avoiding explicit backpropagation gradients would appealing scalable. finally approach unlikely work sparse rewards meta-updates policy gradients heavily rely reward signal. introducing auxiliary dense rewards designed enable meta-learning potential overcome issue would like explore future work. would like thank harri edwards jakob foerster aditya grover aravind rajeswaran vikash kumar yuhuai many others openai helpful comments fruitful discussions. marcin andrychowicz misha denil sergio gomez matthew hoffman david pfau schaul nando freitas. learning learn gradient descent gradient descent. advances neural information processing systems trapit bansal jakub pachocki szymon sidor ilya sutskever igor mordatch. emergent complexity multi-agent competition. international conference learning representations https//openreview.net/forum?id=sygnuxcb. samy bengio yoshua bengio jocelyn cloutier gecsei. optimization synaptic learning rule. preprints conf. optimality artiﬁcial biological neural networks univ. texas vincent conitzer tuomas sandholm. awesome general multiagent learning algorithm converges self-play learns best response stationary opponents. machine learning bruno silva eduardo basso bazzan paulo engel. dealing nonstationary environments using context detection. proceedings international conference machine learning duan john schulman chen peter bartlett ilya sutskever pieter abbeel. fast reinforcement learning slow reinforcement learning. arxiv preprint arxiv. jakob foerster richard chen maruan al-shedivat shimon whiteson pieter abbeel igor mordatch. learning opponent-learning awareness. arxiv preprint arxiv. erin grant chelsea finn sergey levine trevor darrell thomas grifﬁths. recasting gradientbased meta-learning hierarchical bayes. international conference learning representations https//openreview.net/forum?id=bj_ul-kb. richard mealing jonathan shapiro. opponent modelling sequence prediction lookahead two-player games. international conference artiﬁcial intelligence soft computing springer mitchell william cohen estevam hruschka partha pratim talukdar justin betteridge andrew carlson bhavana dalvi mishra matthew gardner bryan kisiel jayant krishnamurthy never ending learning. aaai volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature peng peng quan yuan ying yaodong yang zhenkun tang haitao long wang. multiagent bidirectionally-coordinated nets learning play starcraft combat games. arxiv preprint arxiv. john schulman sergey levine pieter abbeel michael jordan philipp moritz. trust region policy optimization. proceedings international conference machine learning john schulman philipp moritz sergey levine michael jordan pieter abbeel. high-dimensional continuous control using generalized advantage estimation. arxiv preprint arxiv. david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature satinder singh michael kearns yishay mansour. nash convergence gradient dynamics general-sum games. proceedings sixteenth conference uncertainty artiﬁcial intelligence morgan kaufmann publishers inc. richard sutton david mcallester satinder singh yishay mansour. policy gradient methods reinforcement learning function approximation. advances neural information processing systems emanuel todorov erez yuval tassa. mujoco physics engine model-based control. intelligent robots systems ieee/rsj international conference ieee jane wang kurth-nelson dhruva tirumala hubert soyer joel leibo remi munos charles blundell dharshan kumaran matt botvinick. learning reinforcement learn. arxiv preprint arxiv. derivation bound particular form adaptation update. general interested meta-learning procedure parametrized which given access limited experience task produce good policy solving note responsible collecting initial experience constructing ﬁnal policy given task. example case maml represented initial policy adaptation update rule produces α∇θlt procedure collects produce task-speciﬁc policy assuming loss search close optimal optimizing empirical distribution trajectories parameters policy thought random variables marginalize construct objective depends only. adaptation update rule assumes following dτθi given note expression consists terms ﬁrst term standard policy gradient w.r.t. updated policy second policy gradient w.r.t. original policy used collect omit marginalization update given unbiased estimate gradient long loss simply discounted rewards meta-learning). similarly deﬁne uses value advantage function extend policy gradient theorem sutton make suitable meta-learning. theorem gradient value function w.r.t. takes following form ﬁrst term similar expression used original policy gradient theorem second comes differentiating trajectories depend following sutton unroll derivative q-function ﬁrst term arrive following ﬁnal expression policy gradient remark theorem applicable continuous setting changes distributions used compute expectations particular outer expectation taken w.r.t. inner expectation w.r.t. pti+. derivations assumed single step gradient-based adaptation update. experimentally found multi-step version update often leads stable training better test time performance. particular construct intermediate gradient steps intermediate policy parameters. note intermediate step requires interacting environment sampling intermediate trajectories compute policy gradient need marginalize intermediate random variables objective function takes following form since intermediate steps delta functions ﬁnal expression multi-step maml objective form integration taken w.r.t. intermediate trajectories. similarly unbiased estimate gradient objective gets additional terms expectation taken w.r.t. trajectories again note training time constrain number interactions particular environment rollout using intermediate policy compute updates. testing time interact environment rely importance weight correction described sec. neural architectures used policies value functions illustrated fig. architectures memory-less reactive. lstm architectures used fully connected embedding layer followed recurrent layer state lstm-based architectures kept throughout episode reset zeros beginning episode. architecture additionally took reward done signals previous time step kept state throughout whole interactions given environment recurrent architectures unrolled time steps optimized backprop time. used different surrogate loss functions meta-updates outer optimization. meta-updates used vanilla policy gradients computed negative discounted rewards outer optimization loop used objective. mentioned main text similar large batch sizes used ensure enough exploration throughout policy optimization critical learning competitive setting robosumo. experiments epoch size episodes batch size clipping hyperparameter penalty experiments learning rate generalized advantage function estimator optimized jointly policy train agents reasonable time used distributed implementation algorithm. versioned agent’s parameters used versioned queue rollouts. multiple worker machines generating rollouts parallel recent available version agent parameters pushing versioned rollout queue. optimizer machine collected rollouts queue made optimization steps details) soon enough rollouts available. trained agents multiple environments simultaneously. nonstationary locomotion environment corresponded different pair legs creature becoming dysfunctional. robosumo environment corresponded different opponent training pool. simultaneous training achieved assigning environments rollout workers uniformly random rollouts mini-batch guaranteed come training environments. observation action spaces robosumo continuous. observations agent consist position body position opponent’s body joint angles velocities forces exerted part body forces exerted opponent’s torso forces squared clipped additionally normalized observations using running mean clipped values action spaces dimensions joint. table summarizes observation action spaces agent type. note agents observe neither opponents velocities positions opponent’s limbs. allows keep observation spaces consistent regardless type opponent. however even though agents blind opponent’s limbs sense forces applied agents’ bodies contact opponent. robosumo winner gets reward loser penalized case draw agents addition sparse win/lose rewards used following dense rewards encourage fast learning early training stages quickly push opponent outside. agent penalty time step proportional moving towards opponent. reward time step proportional magnitude opponent. reward proportional square total forces exerted control penalty. penalty actions prevent jittery/unnatural movements. calibrate robosumo environment used following procedure. first trained agent pure self-play lstm policy using number iterations tested recorded rates ensure balance kept increasing mass weaker agents repeated calibration procedure rates equilibrated. table average win-rates last rounds -round iterated adaptation games different agents different opponents. base policy value function lstms hidden units. table fig. top- agents lstm policies population ranked trueskill. heatmap shows priori win-rates iterated games based trueskill agents other. ranking top- agents lstm policies according trueskill given tab. priori rates fig. note within lstm categories best meta-learners likely best agents adaptation strategies. continuous adaptation meta-learning assumes consistency changes environment opponent. happens changes drastic? unfortunately training process meta-learning procedure turns sensitive shifts diverge distributional shifts iteration iteration large. fig. shows training curves meta-learning agent policy trained versions opponent pre-trained self-play. iteration kept updating opponent policy steps. meta-learned policy able achieve non-negative rewards training opponent changing steps iteration. fig. reward curves meta-learning agent trained learning opponent. agents ants policies. iteration opponent updating policy given number steps using self-play meta-learning agent attempted learn adapt distributional shifts. setting training process repeated times; shaded regions denote conﬁdence intervals.", "year": 2017}