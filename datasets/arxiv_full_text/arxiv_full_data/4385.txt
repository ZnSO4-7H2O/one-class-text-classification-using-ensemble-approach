{"title": "Lossy Image Compression with Compressive Autoencoders", "tag": ["stat.ML", "cs.CV"], "abstract": "We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.", "text": "propose approach problem optimizing autoencoders lossy image compression. media formats changing hardware technology well diverse requirements content types create need compression algorithms ﬂexible existing codecs. autoencoders potential address need difﬁcult optimize directly inherent non-differentiabilty compression loss. show minimal changes loss sufﬁcient train deep autoencoders competitive jpeg outperforming recently proposed approaches based rnns. network furthermore computationally efﬁcient thanks sub-pixel architecture makes suitable high-resolution images. contrast previous work autoencoders compression using coarser approximations shallower architectures computationally expensive methods focusing small images. advances training neural networks helped improve performance number domains neural networks surpass existing codecs lossy image compression. promising ﬁrst results recently achieved using autoencoders particular small images neural networks already achieving state-of-the-art results lossless image compression autoencoders potential address increasing need ﬂexible lossy compression algorithms. depending situation encoders decoders different computational complexity required. sending data server mobile device desirable pair powerful encoder less complex decoder requirements reversed sending data direction. amount computational power bandwidth available also changes time technologies become available. purpose archiving encoding decoding times matter less streaming applications. finally existing compression algorithms optimal media formats lightﬁeld images video content. development codec take years general compression framework based neural networks able adapt much quicker changing tasks environments. unfortunately lossy compression inherently non-differentiable problem. particular quantization integral part compression pipeline differentiable. makes difﬁcult train neural networks task. existing transformations typically manually chosen optimized task different lossy compression contrast previous work line ball´e directly optimizing rate-distortion tradeoff produced autoencoder. propose simple effective approach dealing non-differentiability rounding-based quantization approximating non-differentiable cost coding generated coefﬁcients. using approach achieve performance similar better jpeg evaluated perceptual quality. unlike jpeg however framework optimized speciﬁc content arbitrary metrics readily generalizable figure effects rounding differentiable alternatives used replacements jpeg compression. crop image compression blocking artefacts jpeg caused rounding coefﬁcients nearest integer. since rounding used test time good approximation produce similar artefacts. stochastic rounding nearest integer similar binarization toderici uniform additive noise forms media. notably achieve performance using efﬁcient neural network architectures would allow near real-time decoding large images even low-powered consumer devices. discrete probability distribution deﬁned used assign number bits representations based frequencies entropy coding. three components parameters goal optimize tradeoff using small number bits small distortion here controls tradeoff square brackets indicate quantization rounding nearest integer measures distortion introduced coding decoding. quantized output encoder code used represent image stored losslessly. main source information loss quantization additional information discarded encoder decoder perfectly decode available information increasing distortion. unfortunately cannot optimize equation directly using gradient-based techniques non-differentiable. following sections propose solution deal problem. derivative rounding function zero everywhere except integers undeﬁned. propose replace derivative backward pass backpropagation derivative smooth approximation effectively deﬁning derivative importantly fully replace rounding function smooth approximation derivative means quantization still performed usual forward pass. replaced rounding smooth approximation completely decoder might learn invert smooth approximation thereby removing information bottle neck forces network compress information. empirically found identity work well sophisticated choices. makes operation easy implement simply pass gradients without modiﬁcation decoder encoder. note gradient respect decoder’s parameters computed without resorting approximations assuming differentiable. contrast related approaches approach advantage change gradients decoder since forward pass kept same. following discuss alternative approaches proposed authors. motivated theoretical links dithering ball´e proposed replace quantization additive uniform noise figure shows effect using alternatives part jpeg whose encoder decoder based block-wise transformation note output visibly different output produced regular quantization rounding error signal sent autoencoder depends images. whereas fig. error signal received decoder would remove blocking artefacts signal fig. remove high-frequency noise. expect difference less problem simple metrics mean-squared error bigger impact using perceptually meaningful measures distortion. alternative would latter approximations gradient encoder gradients decoder. possible comes cost increased computational implementational complexity since would perform forward backward pass decoder twice using rounding using approximation. approach gradient decoder correct even single forward backward pass. since discrete function cannot differentiate respect argument prevents computing gradient encoder. solve problem continuous differentiable approximation. upper-bound non-differentiable number bits ﬁrst expressing model’s distribution terms probability density second step follows jensen’s inequality unbiased estimate upper bound obtained sampling unit cube main differences deal quantization entropy rate estimation. transformations used ball´e consist single linear layer combined form contrast gain control framework relies standard deep convolutional neural networks. toderici proposed recurrent neural networks compression. instead entropy coding work network tries minimize distortion given number bits. image encoded iterative manner decoding performed step able take account residuals next iteration. advantage design allows progressive coding images. disadvantage compression much time consuming approach efﬁcient convolutional neural networks necessarily require decoding encoding stage. gregor explored using variational autoencoders recurrent encoders decoders compression small images. type autoencoder trained maximize lower bound log-likelihood equivalently minimize plays role encoder plays role decoder. gregor used gaussian distribution encoder link approach work ball´e assuming uniform also assume gaussian likelihood ﬁxed variance objective function written here constant encompasses negative entropy encoder normalization constant gaussian likelihood. note equation identical rate-distortion trade-off quantization replaced additive uniform noise. however distortions equivalent formulation variational autoencoder works normalizable normalization constant depend otherwise constant. direct empirical comparison approach variational autoencoders provided appendix figure illustration compressive autoencoder architecture used paper. inspired work convolutions performed downsampled space speed computation upsampling performed using sub-pixel convolutions reduce clutter residual blocks encoder decoder shown. convolutions followed leaky rectiﬁcations indicated solid arrows transparent arrows indicate absence additional nonlinearities. model distributions quantized coefﬁcients gaussian scale mixtures. notation refers convolutions ﬁlters. number following slash indicates stride case convolutions upsampling factors case sub-pixel convolutions. common convolutional neural networks encoder decoder compressive autoencoder. architecture inspired work demonstrated super-resolution achieved much efﬁciently operating lowresolution space convolving images upsampling instead upsampling ﬁrst convolving image. ﬁrst layers encoder perform preprocessing namely mirror padding ﬁxed pixelwise normalization. mirror-padding chosen output encoder spatial extent times downsampled image. normalization centers distribution channel’s values ensures approximately unit variance. afterwards image convolved spatially downsampled time increasing number channels followed three residual blocks block consists additional convolutional layers ﬁlters each. ﬁnal convolutional layer applied coefﬁcients downsampled quantization rounding nearest integer. decoder mirrors architecture encoder instead mirror-padding valid convolutions zero-padded convolutions. upsampling achieved convolution followed reorganization coefﬁcients. reorganization turns tensor many channels tensor dimensionality fewer channels larger spatial extent convolution reorganization coefﬁcients together form sub-pixel convolution layer. following three residual blocks sub-pixel convolution layers upsample image resolution input. finally denormalization pixel values clipped figure scale parameters obtained ﬁnetuning compressive autoencoder ﬁnegrained control rates achieved interpolating scales corresponds scale parameter coefﬁcient particular rate-distortion trade-off. coefﬁcients ordered incremental training procedure. comparison incremental training versus non-incremental training. learning rate decreased iterations non-incremental training initially less stable shows worse performance later iterations. using small learning rate beginning stabilizes non-incremental training considerably slower range similar deal gradients rounding function redeﬁne gradient clipping function outside clipped range. ensures training signal non-zero even decoded pixels outside range iterate spatial positions iterates channels coefﬁcients single image gsms well established useful building blocks modelling ﬁlter responses natural images used scales gsm. rather using common parametrization above parametrized easily used gradient based methods optimizing log-weights log-precisions rather weights variances. note leptokurtic nature gsms means rate term encourages sparsity coefﬁcients. networks implemented python using theano lasagne entropy encoding quantized coefﬁcients ﬁrst created laplace-smoothed histogram estimates coefﬁcient distributions across training set. estimated probabilities used publicly available licensed implementation range coder. models trained using adam applied batches images pixels size. found beneﬁcial optimize coefﬁcients incremental manner done introducing additional binary mask initially entries mask zero. networks trained performance improvements reach threshold another coefﬁcient enabled setting entry binary mask coefﬁcients enabled learning rate reduced initial value training performed updates usually reached good performance much earlier. model trained ﬁxed rate-distortion trade-off introduce ﬁne-tune scale parameters values keeping parameters ﬁxed. figure comparison different compression algorithms respect psnr ssim msssim kodak photocd image dataset. note blue line refers results toderici achieved without entropy encoding. used initial learning rate continuously decreased factor current number updates performed scales optimized iterations. even ﬁne-grained control rates interpolated scales optimized nearby rate-distortion tradeoffs. trained compressive autoencoders high quality images licensed creative commons obtained ﬂickr.com. images downsampled pixels stored lossless pngs avoid compression artefacts. images extracted crops train network. mean squared error used measure distortion training. hyperparameters affecting network architecture training evaluated small held-out flickr images. testing commonly used kodak photocd dataset uncompressed pixel images. compared method jpeg jpeg rnnbased method bits header information counted towards rate jpeg jpeg among different variants jpeg found optimized jpeg chroma sub-sampling generally worked best ﬁne-tuning single compressive autoencoder wide range rates worked well optimizing parameters network particular rate distortion trade-off still worked better. chose compromise combining autoencoders trained medium high rates image rate choose autoencoder producing smallest distortion. increases time needed compress image since image encoded decoded multiple times. however decoding image still fast since requires choosing running decoder network. efﬁcient potentially less performant solution would always choose autoencoder given rate-distortion tradeoff. added byte coding cost encode autoencoder ensemble used. rate-distortion curves averaged test images shown figure evaluated different methods terms psnr ssim multiscale ssim used implementation walt ssim implementation toderici ms-ssim. terms psnr method performs http//rk.us/graphics/kodak/ used code made available https//github.com/tensorflow/models/ tree/a/compression. note time writing implementation include entropy coding paper toderici figure closeups images produced different compression algorithms relatively rates. second shows example method performs well producing sharper lines fewer artefacts methods. fourth shows example method struggles producing noticeable artefacts hair discolouring skin. higher rates problems disappear reconstructions appear sharper jpeg complete images provided appendix similar jpeg although slightly worse medium rates slightly better high rates. terms ssim method outperforms tested methods. ms-ssim produces similar scores methods except rates. however also results highly image dependent. results individual images provided supplementary material. figure show crops images compressed rates. line quantitative results jpeg reconstructions appear visually similar reconstructions methods. however artefacts produced jpeg seem noisy cae’s smoother sometimes appear g´abor-ﬁlter-like. quantify subjective quality compressed images mean opinion score test. tests limitations widely used standard evaluating perceptual quality test included full-resolution uncompressed originals kodak dataset well images compressed using four algorithms near three different rates bits pixel. low-bit-rate included test. image chose setting produced highest rate exceed target rate. average rates compressed images respectively. chose smallest quality factor jpeg jpeg rate exceeded cae. average rates jpeg jpeg images rate lowest setting still higher target rate. images excluded ﬁnal results leaving images respectively. perceptual quality resulting images rated non-expert evaluators. evaluator ﬁnish experiment data discarded. images presented individual random order. evaluators gave discrete opinion score image scale rating began subjects presented uncompressed calibration image dimensions test images shown four versions calibration image using worst quality setting four compression methods given instruction these examples compressed images. worst quality examples. figure shows average results algorithm rate. conﬁdence intervals computed bootstrapping. found jpeg achieved higher jpeg method toderici rates tested. also found signiﬁcantly outperformed jpeg introduced simple effective dealing non-differentiability training autoencoders lossy compression. together incremental training strategy enabled achieve better performance jpeg terms ssim scores. notably performance achieved using efﬁcient convolutional architecture combined simple roundingbased quantization simple entropy coding scheme. existing codecs often beneﬁt hardware support allowing energy costs. however hardware chips optimized convolutional neural networks likely widely available soon given networks good performance many applications. trained algorithms shown provide similar results jpeg knowledge ﬁrst time end-to-end trained architecture demonstrated achieve level performance high-resolution images. end-to-end trained autoencoder advantage optimized arbitrary metrics. unfortunately research perceptually relevant metrics suitable optimization still infancy perceptual metrics exist correlate well human perception certain types distortions developing perceptual metric optimized challenging task since requires metric behave well much larger variety distortions image pairs. future work would like explore optimization compressive autoencoders different metrics. promising direction presented bruna achieved interesting superresolution results using metrics based neural networks trained image classiﬁcation. gatys used similar representations achieve breakthrough perceptually meaningful style transfer. alternative perceptual metrics generative adversarial networks building work bruna dosovitskiy brox ledig recently demonstrated impressive super-resolution results combining gans feature-based metrics. dieleman schluter raffel olson sonderby nouri maturana thoma battenberg kelly fauw heilman moitinho almeida mcfee weideman takacs rivaz crall sanders rasul french degrave. lasagne first release http//dx.doi.org/./zenodo.. goodfellow pouget-abadie mirza warde-farley ozair courville bengio. generative adversarial nets. advances neural information processing systems laparra ball´e simoncelli. perceptual image quality assessment using normalized laplacian pyramid. spie conf. human vision electronic imaging ledig theis huszar caballero aitken tejani totz wang shi. photo-realistic single image super-resolution using generative adversarial network arxiv.. caballero huszar totz aitken bishop rueckert wang. real-time single image video super-resolution using efﬁcient sub-pixel convolutional neural network. ieee conf. computer vision pattern recognition toderici o’malley hwang vincent minnen baluja covell sukthankar. variable rate image compression recurrent neural networks. international conference learning representations wang simoncelli bovik. multiscale structural similarity image quality assessment. conference record thirty-seventh asilomar conference signals systems computers volume assume larger without redeﬁnition derivative error signal helpful. without clipping error signal depend value even though value effect loss test time. hand using clipping different signal backward pass intuitive yields error signal proportional error would also incurred test time. compared optimized non-optimized jpeg without chroma subsampling. optimized jpeg computes huffman table speciﬁc given image unoptimized jpeg uses predeﬁned huffman table. count bits allocated header format optimized jpeg counted bits required store huffman table. found average chroma-subsampled optimized jpeg performed better kodak dataset since single real number carry much information high-dimensional entity dimensionality reduction alone amount compression. however constrain architecture encoder forced discard certain information. better understand much information lost dimensionality reduction much information lost quantization figure shows reconstructions produced compressive autoencoder without quantization. effect dimensionality reduction minimal compared effect quantization. figure disentangle effects quantization dimensionality reduction reconstructed images quantization disabled. original uncompressed image. reconstruction generated compressive autoencoder rounding operation removed. dimensionality encoder’s output smaller input. reconstruction generated compressive autoencoder. effects dimensionality reduction almost imperceptible quantization introduces visible artefacts. here number coefﬁcients produced encoder dimensionality high-bit-rate trained output channels medium-bit-rate trained output channels low-bit-rate trained output channels. figure alternative approach replace rounding function additive uniform noise training using mean-squared error measuring distortion optimizing rate-distortion equivalent training variational autoencoder gaussian likelihood uniform encoder using training proecedure autoencoder architecture approaches additive noise performs worse redeﬁning derivatives approach. rounding-based quantization used test time approaches. show complete images corresponding crops figure image show original image reconstructions using reconstructions using jpeg reconstructions using method images best viewed monitor screen.", "year": 2017}