{"title": "Are Tensor Decomposition Solutions Unique? On the global convergence of  HOSVD and ParaFac algorithms", "tag": ["cs.CV", "cs.AI"], "abstract": "For tensor decompositions such as HOSVD and ParaFac, the objective functions are nonconvex. This implies, theoretically, there exists a large number of local optimas: starting from different starting point, the iteratively improved solution will converge to different local solutions. This non-uniqueness present a stability and reliability problem for image compression and retrieval. In this paper, we present the results of a comprehensive investigation of this problem. We found that although all tensor decomposition algorithms fail to reach a unique global solution on random data and severely scrambled data; surprisingly however, on all real life several data sets (even with substantial scramble and occlusions), HOSVD always produce the unique global solution in the parameter region suitable to practical applications, while ParaFac produce non-unique solutions. We provide an eigenvalue based rule for the assessing the solution uniqueness.", "text": "hosvd parafac objective functions nonconvex. implies theoretically exists large number local optimas starting different starting point iteratively improved solution converge different local solutions. non-uniqueness present stability reliability problem image compression retrieval. paper present results comprehensive investigation problem. found although tensor decomposition algorithms fail reach unique global solution random data severely scrambled data; surprisingly however real life several data sets hosvd always produce unique global solution parameter region suitable practical applications parafac produce non-unique solutions. provide eigenvalue based rule assessing solution uniqueness. tensor based dimension reduction recently extensively studied computer vision pattern recognition machine learning applications. typically approaches seek subspaces information retained discared subspaces contains noises. tensor decomposition methods unsupervised enable researchers apply machine learning applications including unsupervised learning semisupervised learning. perhaps high order singular value decomposition parallel factors widely used tensor decompositions. could viewed extensions matrix. hosvd used computer vision vasilescu terzopoulos parafac used computer vision shashua levine recently yang proposed dimensional proposed method called generalized rank approximation matrices glram dpca viewed framework dsvd solved non-iterative algorithm error bounds hosvd derived equivalence tensor k-means clustering hosvd also established although tensor decompositions widely used many properties well characterized. example tensor rank problem remains research issue. counter examples exist argue optimal low-dimension approximations tensor. paper address solution uniqueness issues. problem arises tensor decomposition objective functions non-convex respect variables constraints optimization also nonconvex. standard algorithms compute decompositions iterative improvement. non-convexity optimization implies iterated solutions converge different solutions start different initial points. note fundamental uniqueness issue differs representation redundancy issues equivalence transformations change individual factors leaves reconstructed image untouched. representation redundancy issues avoided compare different solutions level reconstructed images rather level individual factors. main ﬁndings investigation surprising comforting. real life datasets tested hosvd solutions unique parafac solution almost always unique. furthermore even substantial randomizations real datasets hosvd converge unique solution too. also found whether hosvd solution unique reasonably predicted inspecting eigenvalue distributions correlation matrices involved. thus eigenvalue distributions provide clue solution uniqueness global convergence. looking theoretical explanation rather robust uniquenss hosvd. hosvd nonconvex optimization problem nonconvex jective function w.r.t. orthonormality constraints nonconvex well. well-known nonconvex optimization problems many local optimal solutions starting different initial guess converged solutions different. therefore theoretically solutions hosvd unique. clearly parafac objective function nonconvex linearly independent constraints also nonconvex. therefore parafac optimization nonconvex optimization. many different computational algorithms developed computing parafac. type algorithm uses sequence rank- approximations however solution heuristic approach differ optimal solutions. standard algorithm compute factor time alternating fashion. objective decrease monotonically step iteration converges optimal solution. however nonconvexity parafac optimization converged solution depends heavily initial starting point. reason parafac often unique. randomly generate rank deﬁcient matrices proper size. ﬁrst initialization randomly pick column column zero. rest columns randomly generated second third initializations randomly pick three columns zero typically thus rank-deﬁciency strong. optimization problem unique solution typically starts nonzero value gradually decrease zero. indeed occurs often figure sooner decreases zero faster algorithm converges. example figure parameter setting algorithm converges faster setting. optimization unique solution typically remains nonzero times solution hosvd unique. figure show results hosvd parafac random tensor. tests shown lines ﬁgure none ever decrease zero. convex optimization problem local optimal solution also global optimal solution. non-convex optimization problem many local optimal solutions converged solutions hosvd/parafac iterations depend initial starting point. paper take experimental approach. tensor decomposition many runs dramatically different starting points. solutions runs agree consider decomposition unique solution. following explain dramatically different starting point experiments three different real life data sets. eigenvalue distributions predict uniques hosvd. solution also solution. reason following. collection vectors. corresponding covariance matrix gram matrix eigenvectors principal components. coming back decomposition gram matrix consider image vector. solution principal eigenvectors principal components. also experimented occulsions sizes upto half images. found occulsion consistently produce smaller randomization affects hosvd results converge unique solution. reason space limitation show results here. example at&t dataset hosvd converges parameter settings except block scramble ignored eigenmodes similar eigenvalues ﬁrst eigenvalues. ambiguous hosvd select signiﬁcant eigenmodes. thus hosvd fails converge unique solution. second image dataset wang contains categories images category. original size image either select buildings buses food categories resize images size. also transform images level gray images. selected images form tensor. third dataset caltech contains categories. images category. categories images. collected september andreetto ranzato. size image roughly pixels. randomly pickup images resize transform level gray images form tensor. three types randomization considered block scramble pixel scramble occlusion. block scramble image divided blocks; blocks scrambled form images inoue urahama. equivalence non-iterative algorithms simultaneous rank approximations matrices. proc. ieee conf. computer vision pattern recognition perona fergus learning generative visual models training examples incremental bayesian approach tested object categories. workshop generative model based vision page pixel scramble dataset wang hosvd ambiguous select eigenmodes large number nearly identical eigenvalues around cutoff. however reduce dimensions ambiguity gone hosvd clearly selects eigenmodes. converges observation also applies caltech dataset pixel scramble hosvd solution unique parafac solution almost always unique. ﬁnding also surprising comforting. assured applications using hosvd solutions unique results reliable repeatable. rare cases data highly irregular severely distored/randomized results indicate predict whether hosvd solution unique inspecting eigenvalue distributions.", "year": 2009}