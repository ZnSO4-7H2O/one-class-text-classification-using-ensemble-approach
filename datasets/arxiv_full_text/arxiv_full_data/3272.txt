{"title": "Random Forests Can Hash", "tag": ["cs.CV", "cs.IR", "cs.LG", "stat.ML"], "abstract": "Hash codes are a very efficient data representation needed to be able to cope with the ever growing amounts of data. We introduce a random forest semantic hashing scheme with information-theoretic code aggregation, showing for the first time how random forest, a technique that together with deep learning have shown spectacular results in classification, can also be extended to large-scale retrieval. Traditional random forest fails to enforce the consistency of hashes generated from each tree for the same class data, i.e., to preserve the underlying similarity, and it also lacks a principled way for code aggregation across trees. We start with a simple hashing scheme, where independently trained random trees in a forest are acting as hashing functions. We the propose a subspace model as the splitting function, and show that it enforces the hash consistency in a tree for data from the same class. We also introduce an information-theoretic approach for aggregating codes of individual trees into a single hash code, producing a near-optimal unique hash for each class. Experiments on large-scale public datasets are presented, showing that the proposed approach significantly outperforms state-of-the-art hashing methods for retrieval tasks.", "text": "hash codes efﬁcient data representation needed able cope ever growing amounts data. introduce random forest semantic hashing scheme information-theoretic code aggregation showing ﬁrst time random forest technique together deep learning shown spectacular results classiﬁcation also extended large-scale retrieval. traditional random forest fails enforce consistency hashes generated tree class data i.e. preserve underlying similarity also lacks principled code aggregation across trees. start simple hashing scheme independently trained random trees forest acting hashing functions. propose subspace model splitting function show enforces hash consistency tree data class. also introduce information-theoretic approach aggregating codes individual trees single hash code producing near-optimal unique hash class. experiments large-scale public datasets presented showing proposed approach signiﬁcantly outperforms state-of-the-art hashing methods retrieval tasks. view recent huge interest image classiﬁcation object recognition problems spectacular success deep learning random forests tasks seems astonishing much less efforts invested related often difﬁcult problems image contentbased retrieval generally similarity assessment large-scale databases. problems arising primitives many computer vision tasks becoming increasingly important exponentially increasing information. semantic similarity-preserving hashing methods recently received considerable attentions address need part memory computational advantage representations. methods learn embed data points space binary strings; thus producing compact representations constant sub-linear search time. embedding considered hashing function data translates underlying similarity collision probability hash generally similarity codes hamming metric. examples recent similarity-preserving hashing methods include gionis kulis grauman weiss masci zhang conceptual similarity problems semantic hashing binary classiﬁcation numerous classiﬁcation techniques adapted former task. example state-of-the-art supervised hashing techniques like masci masci norouzi based deep learning methodologies. random forest another popular classiﬁcation techniques. random forests used construct semantic hashing schemes. mainly acting hashing function random forest fails preserve underlying similarity inconsistency hash codes generated tree class data; also lacks principled aggregating hash codes produced individual trees single longer code. work ﬁrst construction semantic hashing scheme based random forest. ﬁrst introduce transformation learner model random forest enforcing hash consistency tree thereby preserving similarity. then propose information-theoretic approach aggregating hash codes forest encouraging unique code class. using challenging large-scale examples demonstrate signiﬁcantly consistent unique hashes data semantic class compared state-of-the-art hashing schemes. random forest ensemble binary decision trees. following random forest literature paper specify maximum tree depth also avoid post-training operations tree pruning. thus tree depth consists tree nodes excluding root node indexed breadth-ﬁrst order. training introduce randomness forest combination random sampling randomized node optimization thereby avoiding duplicate trees. discussed breiman criminisi shotton training tree different randomly selected decreases risk overﬁtting improves generalization classiﬁcation forests signiﬁcantly reduces training time. given classes randomly partition classes arriving binary split node categories node randomness. pedagogic hashing scheme constructed follows data point pushed tree reaching corresponding leaf node. simply nodes visited rest. ordering bits predeﬁned node order e.g. breadth-ﬁrst order obtain sparse hash code always containing exactly ones. random forest consisting trees depth point simultaneously pushed trees obtain -bit hash codes. training hashing processes done parallel achieve high computational efﬁciency modern parallel hardware. classiﬁcation forest originally designed ensemble posterior obtained averaging large number trees thus boosting classiﬁcation accuracy conﬁdent class posteriors required individual trees. however lack conﬁdent class posteriors individual trees obtain highly inconsistent hashes individual tree class data. also obvious combine hashes different trees given target code length. inconsistency hash codes prevents standard random forest directly adopted hashing codes critical large-scale retrieval. address problems ﬁrst propose transformation learner model random forest tree enforces consistent codes similar points. though class assigned unique code tree limited leaf availability class shares code different classes different trees underlying node randomness models. propose information-theoretic approach aggregate hashes across trees unique code data class. consider random forest consisting trees depth hash codes obtained training samples denoted {bi}m codes generated i-th tree henceforth denoted code blocks. given target hash code length objective select code blocks maximizing mutual information selected remaining codes maxb|b|=k class labels available subset training samples semi-supervised aggregation performed maxb|b|=k terms evaluated using different samples exploit labeled unlabeled data. note code aggregation step learned training cost testing. present experimental evaluation foresthash retrieval tasks using standard public benchmarks. hashing methods compared include supervised methods fasthash hdml ldahash unsupervised methods klsh software provided authors. adopt setup norouzi image retrieval experiments mnist. trained forest trees depth table summarizes retrieval performance various methods hamming radius hdml deep learning based hashing method fasthash booted trees based method. denote proposed method foresthash. subspacebased leaner models known robust small training samples bengio semi-supervised code aggregation exploits labeled unlabeled data foresthash signiﬁcantly outperforms state-of-the-art methods reduced training cases. adopt challenging setup masci image retrieval experiments cifar table summarizes retrieval performance various methods hamming radius foresthash-base pedagogic random forest hashing scheme section decision stump learner model used random subset trained trees selected. surprising simple pedagogic scheme outperforms compared supervised methods radius orders magnitude speedup recall significantly improved proposed code aggregation foresthash using transformation learner dramatically improves precision pedagogic scheme signiﬁcantly outperforms compared methods radius reports comparable precision signiﬁcantly higher recall radius figure presents image query examples cifar- dataset. results reported table refer experiment pubﬁg face dataset construct hashing forest using training faces subject search among unseen faces. subspace methods robust small training samples problems extraordinarily effective representing faces foresthash shows signiﬁcantly higher precision recall compared state-of-the-art methods. figure presents several examples face queries. considering importance compact computationally efﬁcient codes introduced random forest semantic hashing scheme which best knowledge ﬁrst instance using random forests hashing extending beyond classiﬁcation large-scale retrieval. proposed scheme consists forest transformation learners information-theoretic code aggregation scheme. proposed framework combines fundamental fashion feature learning random forests similarity-preserving hashing straightforwardly extended retrieval incommensurable multi-modal data. method shows exceptional effectiveness preserving similarity hashes outperforms state-of-the-art hashing methods large-scale retrieval tasks. shen hengel suter fast supervised hashing decision trees high-dimensional data. proc. ieee computer society conf. computer vision patt. recn. masci bronstein bronstein sprechmann sapiro sparse similaritypreserving hashing. international conference learning representations banff canada", "year": 2014}