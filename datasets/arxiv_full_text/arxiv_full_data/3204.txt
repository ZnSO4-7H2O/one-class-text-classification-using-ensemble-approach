{"title": "Using n-grams models for visual semantic place recognition", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "The aim of this paper is to present a new method for visual place recognition. Our system combines global image characterization and visual words, which allows to use efficient Bayesian filtering methods to integrate several images. More precisely, we extend the classical HMM model with techniques inspired by the field of Natural Language Processing. This paper presents our system and the Bayesian filtering algorithm. The performance of our system and the influence of the main parameters are evaluated on a standard database. The discussion highlights the interest of using such models and proposes improvements.", "text": "paper present method visual place recognition. system combines global image characterization visual words allows efﬁcient bayesian ﬁltering methods integrate several images. precisely extend classical model techniques inspired ﬁeld natural language processing. paper presents system bayesian ﬁltering algorithm. performance system inﬂuence main parameters evaluated standard database. discussion highlights interest using models proposes improvements. semantic mapping relatively ﬁeld robotics aims give robot high-level human-compatible understanding environment order ease integration robots daily environments notably homes workplaces. environments usually composed discrete places correspond different functions. instance house usually made different rooms corridors used move them. places called semantic places deﬁned high-level human concepts opposed traditional low-level landmarks used robot mapping. context it’s important robot able recognize place category places lies. tasks called respectively instance recognition categorization. semantic place recognition important component semantic mapping. moreover semantic category place used foster object detection recognition provide qualitative localization. different types sensors employed semantic place recognition. ﬁrst works domain used range sensors discriminate places based geometrical information. however spatial conﬁguration places category different. therefore geometrical information useful categorization. vision modality choice semantic place recognition gives access rich allothetic information. although multimodal approaches work focuses visual place recognition. article develop analogy semantic place recognition language modelling. analogy allows design efﬁcient temporal integration methods i.e. take several images account order reduce ambiguity. precisely extend hidden markov model formalism n-grams models. models extensively used natural language processing efﬁcient estimation techniques proposed. paper aims assess models semantic place recognition. goal compare temporal integration method previously proposed models. particular study inﬂuence length n-gram model estimation procedure performance. article structured follows. section presents related work. model links language modelling described section section presents experiments results. finally conclude section standard algorithm object localization recognition. places described frequency objects found combined constraints position. however object categorization still difﬁcult task position objects greatly vary environment another. therefore approaches used large databases. vast majority research place recognition techniques developed visual scene classiﬁcation. distinguish methods using global features methods using descriptors computed around interest points uses bag-ofwords model local features ﬁrst clustered so-called dictionary visual words learned mean vector quantization algorithm. image represented distribution visual words found major advantage learning space discretized geometrical information lost. generally speaking using single image single type information enough place recognition tasks. therefore research conducted disambiguate perception. conﬁdence criterion iteratively compute several cues image conﬁdence classiﬁcation sufﬁciently high another method reduce ambiguity several images mutually disambiguate perception. authors simple spatio-temporal accumulation process ﬁlter decision discriminative conﬁdence-based place recognition system problem method system needs wait time giving response. also special care must taken detect places boundaries adjust size bins. place hidden state feature vector stands observation. drawback input space continuous high-dimensional. learning procedure computationally expensive. uses technique called bayesian online change-point detection. main idea detect abrupt changes parameters input’s statistics caused moving place another. main advantage robot able learn unsupervised relies hypothesis shape distribution every place. case image described single visual word. sequence images translated sequence words. techniques allow draw parallel place recognition language modelling. propose discretized signatures. temporal integration performed bayesian ﬁltering propose extended model called auto-regressive take account dependence images. model similar described image described unique feature vector mapped given visual word thanks vector quantization algorithm main novelty lies high-order hidden markov model techniques visual word selection hmms relationship robot’s knowledge world time perception represented ﬁgure case place recognition state discrete random variable represents place robot model place modelled continuous probability distribution formalism allows efﬁciently estimate posteriori probability recursive equation given discrete place transition probability distribution encodes topology environment. assumed current observation depends current hidden state i.e. state complete. however huge semantic between human notion place extracted image. several authors proposed extensions classic take account long-term dependencies observations paper call model high-order hidden markov model case current knowledge depends last states xt−t−. similarly current observation depends order simplify learning place model proposed global image characterization combination vector quantization algorithms discretize them. case variable reduced discrete random variable ﬁnite number values number words dictionary. given discrete probability distribution known -gram model uses words. shown estimation model empirical data important factor. problem even large training sequences words observed training data given class therefore assigned null probability class’ model. sequence observed testing posteriori probability class clamped equation avoid problem necessary take probability mass observed sequences distribute unobserved sequences. techniques called smoothing discounting. refer reader uniﬁed presentation smoothing techniques. srilm toolkit learn n-grams models. characterize images gist descriptors efﬁcient global image characterization. image divided subwindows ﬁltered using bank gabor ﬁlters energy ﬁlter averaged subwindow scale orientation. finally output projected vector quantization algorithm used paper self-organizing current set-up training performed off-line randomly chosen images made cold number neurons sets number words visual dictionary important parameter system. square maps parametrized length paper values selected shown small maps good performance categorization tasks larger maps perform well instance recognition training algorithm stochastic results vary another. therefore size results averaged soms. sampling rate databases several hertz. case image time different image time high probability described close vectors therefore visual word. desirable feature image description vector quantization problem method probability seeing visual word high. therefore might interesting subset images learning. order evaluate phenomenon computed average number consecutive timesteps characterized visual word training sequence used section results given table test three different strategies selecting visual words. ﬁrst simply sub-sample input image i.e. select image strategy called subsample. second strategy replace every sequence identical prototypes unique instance word call strategy compress. last strategy word time different word time call strategy unique. strategies simple implemented online real robot limited computational power. cold database standard database evaluate vision-based place recognition systems. consists sequences acquired human-driven robot different laboratories across europe different illumination conditions laboratory paths explored path followed least times illumination condition. experiments carried perspective images. protocols proposed uses hundreds images place enough robustly estimate transition probabilities. therefore designed experiment evaluate interest method. images acquired saarbruecken part parts database known contain errors complete classes training performed sequences number three illumination conditions. similarly testing performed sequence illumination conditions. following deﬁne transition matrix xt−; rest probability mass shared uniformly among transitions. tested lidstone-laplace smoothing parameter witten-bell smoothing. training small knesser-nay smoothing. experiments interpolated models tested several values sub-sampling rate compress strategy. unique strategy don’t need parameter. setting lidstone-laplace smoothing integration method gives temporal setting lidstone-laplace smoothing without interpolation gives system similar results presented ﬁgure must noted instance recognition task larger gives better results. expected literature second observation could made word selection methods generally increase results several percent. seen difference bars group. subsample strategy rather efﬁcient sometimes increasing performance setting generally gives less important increase. performance decreases smoothing. however strategy leads best results task smoothing. compress strategy usually efﬁcient except smoothing. unique strategy always among best choices it’s results less sensitive n-gram order. generally speaking effect clear ﬁgure using i.e. take account dependence last image clear improvement i.e. classical hmm. however using n-grams little impact performance. noted performance drops word selection performance high large seems conﬁrm intuition behind word selection techniques. presented model temporal integration using hohmm semantic place recognition models dependence observations. shown taking dependence account lead interesting gains performance. however contrary expected using larger don’t improve performance. smoothing technique seems minor effect. caused fact relatively small training sets compared ﬁeld techniques developed. results must take account fact recognition rates already quite high task studied here. shown simple methods select important words could improve results. results suggest large could interesting combined good word selection techniques. future works focus vector quantization process learn better words. sophisticated word selection techniques also useful. finally could also look discriminative descriptors. dubois guillaume tarroux frenoux visual place recognition using bayesian ﬁlproceedings tering markov chains. european symposium artiﬁcial neural networks guillaume dubois tarroux frenoux temporal bag-of-words generative model visual place recognition using temporal integration. proceedings international conference computer vision theory applications l.-m. j.-c. study high-order hidden markov models applications speech recognition. dapoigny editors advances applied artiﬁcial intelligence volume lecture notes computer science. pronobis caputo conﬁdence-based integration visual place recognition. proccedings ieee/rsj international conference intelligent robots systems. torralba murphy freeman rubin. context-based vision system place object recognition. proceedings nineth ieee international conference computer vision volume pages ullah pronobis caputo jensfelt christensen towards robust place recognition robot localization. proceedings ieee international conference robotics automation pasadena usa. figure results instance recognition task. vertical axis correct recognition rate horizontal axis value upper-row results lower results left column results lidstone-laplace smoothing. right column results witten-bell smoothing.", "year": 2014}