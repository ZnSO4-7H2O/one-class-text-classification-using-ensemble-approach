{"title": "Robust Bayesian Optimization with Student-t Likelihood", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Bayesian optimization has recently attracted the attention of the automatic machine learning community for its excellent results in hyperparameter tuning. BO is characterized by the sample efficiency with which it can optimize expensive black-box functions. The efficiency is achieved in a similar fashion to the learning to learn methods: surrogate models (typically in the form of Gaussian processes) learn the target function and perform intelligent sampling. This surrogate model can be applied even in the presence of noise; however, as with most regression methods, it is very sensitive to outlier data. This can result in erroneous predictions and, in the case of BO, biased and inefficient exploration. In this work, we present a GP model that is robust to outliers which uses a Student-t likelihood to segregate outliers and robustly conduct Bayesian optimization. We present numerical results evaluating the proposed method in both artificial functions and real problems.", "text": "bayesian optimization recently attracted attention automatic machine learning community excellent results hyperparameter tuning. characterized sample eﬃciency optimize expensive black-box functions. eﬃciency achieved similar fashion learning learn methods surrogate models learn target function perform intelligent sampling. surrogate model applied even presence noise; however regression methods sensitive outlier data. result erroneous predictions case biased ineﬃcient exploration. work present model robust outliers uses student-t likelihood segregate outliers robustly conduct bayesian optimization. present numerical results evaluating proposed method artiﬁcial functions real problems. keywords bayesian optimization robust regression hyperparameter tuning gaussian process student-t likelihood bayesian optimization become state-of-the-art hyperparameter tuning expensiveto-train systems. sample eﬃciency black-box approach appealing features bayesian optimization used many environments systems. plethora studies dealing performance machine learning systems tuning hyperparameters using bayesian optimization methods snoek hutter bergstra following black-box approach little knowledge required system optimized allowing even stochastic outcomes. however methods assume system well behaved sense produce outliers adversarial outcomes. recently work dealing certain robustness bayesian optimization either input noise combining sources diﬀerent ﬁdelities using mcmc regression model author’s knowledge ﬁrst work robust bayesian optimization statistical testing sense. context hyperparameter tuning many situations produce unexpected outcomes outliers might appear result random occurrences like code failure system database problem network issue. also appear alternate sources multi-ﬁdelity environments security issues exploited vulernability. able identify remove outliers crucial building robust system. outliers devastating regression single outlier result large estimation error therefore outliers might also problematic bayesian optimization sampling process relies surrogate regression model. example outlier near optimum might result predictions outcomes neighborhood resulting area undersampled sampled reducing possibility improving model future iterations. furthermore fact outliers distributed independently true values result numerical issues stability problems estimating parameters surrogate model. context optimization distinguish kind outliers false positives false negatives false positive outliers could occur example given particular hyperparameter conﬁguration user mistakenly train small fraction data. contrary false negative outlier could appear result gradient descent accidentally terminating prior convergence. false negatives aﬀect bayesian optimization indirectly surrogate model false positives might also complicate identiﬁcation optimal point. thus many scenarios false positives might require perfect detection mechanism guarantee correct optimum returned. present method hyperparameter tuning using bayesian optimization simultaneously identifying removing outliers gaussian process studentlikelihood. method shows improvement across several benchmarks applications. although method theoretically deal false positives negatives present work limited results false negatives. future research needed guarantee identiﬁcation correct optima. bayesian optimization refers class primarily black-box optimization strategies relies probabilistic surrogate models decision making improve sample eﬃciency. article follows common bayesian optimization based gaussian process deﬁne surrogate model function interest. given previous observations points assume gaussian observation model homoscedastic noise model gives predictions query point step normal distribution with positive deﬁnite matrix automatic relevance determination kernel restricts diagonal. hyperparameters estimated maximum likelihood although mcmc methods could also used snoek based bayesian optimization observation model usually gaussian likelihood allowing closed form inference. however shown figure sensitive outliers. approach consists using surrogate probabilistic model based large tail distribution observation model like laplace hyperbolic secant student-t likelihood. observation models robust presence outliers student-t likelihood usually providing best results o’hagan proved student-t distribution reject outliers tending inﬁnity provided least observations all. article also showed gaussian distribution outlier-resistant meaning outlier ever rejected. degrees freedom scale parameter. however student-t likelihood well alternative distributions mentioned allow closed form inference posterior. vanhatalo ﬁrst suggested laplace approximation compute posterior inference. authors later compared diﬀerent strategies jyl¨anki mcmc neal variational approaches expectation propagation showed modiﬁcation robust estimation method although increased computational cost. found that context bayesian optimization observations arrive sequentially laplace approximation works ﬁne. figure regression outliers using gaussian likelihood yield biased estimates high variance student-t likelihood process allows better regression estimate respect non-corrupted values biased numerically instable student-t likelihood remove outliers gaussian likelihood remaining points built regression model student-t likelihood able identify outliers rest data. purpose compute upper lower quantile classiﬁcation threshold. theory assuming single observation arrives iteration last observation questioned model sequentially improved found reclassifying points worked better information allows better classiﬁcation past observations. sometimes points initially considered outliers found part model while frequently points initially misclassiﬁed acceptable properly detected better model. although student-t likelihood able identify outliers points found practices reasonable wait certain number iterations starting classifying data. found waiting budged works many scenarios. also found that sequential nature bayesian optimization last point misclassiﬁed outlier large probability selected next iteration wasting resources. furthermore computational cost student-t likelihood much expensive gaussian likelihood. therefore propose student-t likelihood iterations. finally outliers classiﬁed removed optimization performed standard computed remaining points produces stable faster solutions non-gaussian observation models also used past bayesian optimization although context preference learning student-t distribution also used past bayesian optimization context student-t process evaluated method artiﬁcial functions realistic applications. cases outliers artiﬁcially generated distribution equivalent methods. results compared optimization described section using outliers well removing outliers described section baseline conduct optimization without outliers present data. numerical benchmarks used methodology hennig schuler generated random functions types gaussian process. within model comparison generated samples gaussian process kernel used optimization figure optimization robot walking policy. number outliers large student-t likelihood allows recover performance baseline. small outlier rate student-t likelihood able prune out-of-model points allows better reﬁnement. active policy search reinforcement learning method control robot autonomous agent reﬁning policy using bayesian optimization reward function. successfully applied robot walking controlled environments case objective policy stable policy even presence external perturbations. however trials robot might obstacles perturbations physically impossible overcome. thus robot returns poor reward even policy good conditions. experiments simulated failures robot reaching crash state random time trajectory. therefore resulting reward similar reward obtained policy also results crash. figure plots average conﬁdence bounds trials. shown robot policy search complex problem bayesian optimization non-stationary behavior many reward functions large number outliers yield underperforming model near optimum good results results cannot agree single stationary function. thus student-t likelihood also classiﬁes outliers points conﬂict good values resulting subtle improvement. however research needed.", "year": 2017}