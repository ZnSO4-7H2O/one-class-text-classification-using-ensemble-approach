{"title": "Boundary Optimizing Network (BON)", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Despite all the success that deep neural networks have seen in classifying certain datasets, the challenge of finding optimal solutions that generalize still remains. In this paper, we propose the Boundary Optimizing Network (BON), a new approach to generalization for deep neural networks when used for supervised learning. Given a classification network, we propose to use a collaborative generative network that produces new synthetic data points in the form of perturbations of original data points. In this way, we create a data support around each original data point which prevents decision boundaries from passing too close to the original data points, i.e. prevents overfitting. We show that BON improves convergence on CIFAR-10 using the state-of-the-art Densenet. We do however observe that the generative network suffers from catastrophic forgetting during training, and we therefore propose to use a variation of Memory Aware Synapses to optimize the generative network (called BON++). On the Iris dataset, we visualize the effect of BON++ when the generator does not suffer from catastrophic forgetting and conclude that the approach has the potential to create better boundaries in a higher dimensional space.", "text": "increase ability network generalize unseen data. explicit regularization techniques restrict expressiveness network implicit regularization methods noise corruption need careful choices noise forms. paper propose boundary optimizing networks regularize neural networks introducing synthetic data points derived original data. particular employ generative network works collaborative fashion generates noisy data points used train neural network. create data support around original data point prevents decision boundaries passing close original data points i.e. prevents overﬁtting. conjectured rozsa generalization diminished diminishing gradients correctly classiﬁed samples. contrast works opposite direction gradients diminish misclassiﬁed samples relative correctly classiﬁed samples creates signiﬁcant data support around correctly classiﬁed data points. prevent catastrophic forgetting training propose variation memory aware synapses optimize generative networks variation referred bon++ rest paper. iris dataset show algorithm successfully creates data support densely populated areas hence prevents classiﬁer overﬁtting outliers. similarities drawn famous versions generative networks generative adversarial networks generative adversarial perturbations differences bons follows unlike gaps perturbations necessarily imperceptible. addition perturbations designed keep noisy datapoint around original datapoint order help optimization. despite success deep neural networks seen classifying certain datasets challenge ﬁnding optimal solutions generalize still remains. paper propose boundary optimizing network approach generalization deep neural networks used supervised learning. given classiﬁcation network propose collaborative generative network produces synthetic data points form perturbations original data points. create data support around original data point prevents decision boundaries passing close original data points i.e. prevents overﬁtting. show improves convergence cifar- using state-of-the-art densenet. however observe generative network suffers catastrophic forgetting training therefore propose variation memory aware synapses optimize generative network iris dataset visualize effect bon++ generator suffer catastrophic forgetting conclude approach potential create better boundaries higher dimensional space. despite signiﬁcant success deep neural networks seen recent times generalization abilities remain questionable lack generalization overparameterization network subsequent overﬁtting optimization. regularization techniques restricting magnitude parameter values e.g. l-norm injecting noise training e.g. dropout employed corti denmark university copenhagen department computer science denmark biomediq denmark. correspondence marco singh <mscortilabs.com> akshay <akshaybiomediq.com>. proposed work overlap areas data augmentation regularization noise. relate sense areas address issue generalization dnn. data augmentation involves implicitly coupling synthetic point lying vicinity original data point derived synthetic data point typically generated varying attributes data point example rotations. advanced approached using dnns approximate data distributions subsequently ﬁnding data points sampling distribution hand regularization noise works injecting noise particular form weights network optimization. popular noise-based regularization dropout. here certain hidden units neural network randomly turned multiplying noise sampled bernoulli distribution. approaches directly modify weights neural networks injecting controlled noise noise form ensemble networks another form noise injection training work changes injection noise perturbing data points controlled noise. controlled noise perturbed speciﬁc obtain better decision boundaries classiﬁer. paper organized follows next section introduce formalisms around following this apply cifar- using state-of-the-art densenet continue constructed dataset visually show happening furthermore develop bon++ ﬁght catastrophic forgetting. bon++ tested iris dataset show proposed algorithm indeed prevent overﬁtting happening idea behind bons couple synthetic data point real data points given dataset. synthetic data point created generative network takes input real data point outputs synthetic data point neural network takes input real data point creates distortions input i.e. output dimensionality equals input. fact restriction network enough parameters able produce distortions data points data set. suited given dataset hand. approach helps deﬁning data points order make create boundaries generalize better hence invariant actual structure algorithm starts feeding real data point create synthetic data point real synthetic data point labels. parameters updated using accumulated gradients real synthetic data point. assuming cross-entropy loss function minimize updating evaluate performance synthetic data point; misclassiﬁed likely altered real data point much hence minimize mean squared error synthetic data point real data point created update parameters evaluation shows correctly classiﬁed update parameters correctly classify added enough noise original data point hope created supporting data point better generalizing decision boundary. mentioned single generative network create data support population different generator networks task mentioned i.e. creating synthetic data point data points training data. create several synthetic data points real data point hence create cloud data points supporting decision boundary. figure illustration algorithm compute synthetic data point update using cross-entropy loss real synthetic data point update using loss real synthetic data point synthetic data point misclassiﬁed experiment densenet depth growth rate cifar-+ i.e. cifar- data augmentation scheme presented original densenet paper single generator i.e. layers convolutions increasing number channels except last layer collapses number channels original input image. kernel size stride furthermore batch normalization relu activations convolution layer except last one. figure shows accuracy training progresses. clear approach helps convergence happen faster. exact learning rate schedule densenet paper i.e. decrease learning rate epoch although faster convergence desirable feature main objective bon. interested making better boundaries hence would expected higher accuracy training. happen instead approach produces equivalent accuracies densenet without bon. understanding happens plotted mean squared error synthetic data points real equivalent training progresses. figure shows this. would expect decrease slowly since longer pushing synthetic data points closer real equivalent soon correctly classiﬁed since densenet classiﬁes samples correctly would expect marginal change samples small. however case instead collapses almost samples including ones correctly classiﬁed early training. happens network sees misclassiﬁed samples many epochs hence parameter update using information misclassiﬁed samples propagated closer closer real equivalent. happening collapses indicator function hence learns replicate real data point. catastrophic forgetting preventing working intended current state. experiment feedforward neural network hidden layers output layer ﬁrst hidden layers neurons third layer neurons output neurons match number classes. relu activations. figure shows simple example. epochs synthetic data points created exactly support would wish for. synthetic data points surrounding outlier hence making difﬁcult overﬁt. give reference trained exact model without using networks. seen figure case feedforward neural network alone decision boundary class attempting stretch towards outlier exactly avoided case. later analysis approach real life example complex dataset also benchmark using dropout. figure algorithm constructed data set. real data points synthetic data points plotted together decision boundary left epoch center epoch right epoch bon. many synthetic data points correctly classiﬁed epoch suddenly collapsing real data point generated from. reason generator network trains misclassiﬁed samples stage. epoch almost samples correctly classiﬁed hence sees outlier. keeps pushing synthetic data point corresponding outlier close outlier however process neural network updates weights fulﬁll objective single data point hence forgetting generate remaining data points. catastrophic forgetting something neural networks notoriously known switching task another. difference necessarily well-deﬁned switch tasks normally case academia tackle issue. however making per-epoch version memory aware synapses manage successfully overcome issue show complex dataset. order make algorithm work need make sure generator networks forget create synthetic data points data points correctly classiﬁed problem updates weights network alter misclassiﬁed samples hence removing memory achieved correct samples. tackle making approximations importance weights particular weight used given time make harder network update particular weight. instead force other less used weights learn things. number epochs don’t want place much emphasis parameter importance early training. training progresses want memorize important parameters since data points classiﬁed correctly. established algorithm demonstrate real life scenario. concretely choose iris dataset sepal length width input features. space class easily separable classes tough separate. ﬁrst show baseline using dropout using dropout establish benchmark since right wrong deciding decision boundary better other. however able visually show differences conclude whether bon++ expect. want estimate parameter importance. using parameter importance restrict network change least important parameters learn task preserve current important parameters. prevent catastrophic forgetting already learned tasks. assume single data point output change parameters small amount output estimate difference network output gradients output w.r.t. parameters hence have gij) value estimate parameter importance network particular data point fact continuously update measure importance whenever network sees data point hence giving following estimate parameter importance data points calculated parameter importance above need incorporate loss function original article parameters θ∗after task. setting speciﬁc tasks instead network parameters epoch denote given showcasing small datasets paper beneﬁcial parameters often larger datasets. ﬁxed parameters loss calculated distance current parameters ﬁxed parameters multiplied important parameters total loss hence initial positive effect training cifar- using densenet. however training progressed generator network suffered catastrophic forgetting leading collapse synthetic data points real equivalent generated from. furthermore showed method works simple constructed data algorithm able prevent overﬁtting single outlier. developed bon++ algorithm using per-epoch version memory aware synapses tested iris dataset decision boundary much harder classes constructed example. model prevent overﬁtting keeping expressiveness difﬁcult areas. late discovery memory aware synapses completed full study bon++ larger datasets hence future work adopt approach cifar domains higher-dimensional input space. also make code used experiments public people different domains test bon++ approach increase training time signiﬁcantly however worth mentioning number generators parallel since each independently creating synthetic data points. added training time training approach hence order training time single although training time increased impact speed inference since classiﬁer equal classiﬁer paradigm. leads next point; bon++ approach works architecture requirement able create generator effectively create synthetic data points taking inputs real data points. areas test different measure similarity data points. currently mean squared error synthetic data point real equivalent however could different measures. measure push synthetic data point closer intermediate layer evaluated real data point. could potentially push data points closer internal representation real data points hence creating real life augmentations current approach. expected bon++ creating support existing data points hence blocking potential overﬁtting areas. particularly large green area topright ﬁgure prevented large mass supporting synthetic data points ﬁgure furthermore bon++ preserves expressiveness w.r.t. decision boundary green purple class i.e. data points mixed complex manner approach still capable creating complex decision boundary. hand using dropout creates clear trade-off simplicity decision boundary overﬁtting hence cannot prevent overﬁtting areas preserving complexity areas. per-epoch version memory aware synapses working intended although model trains many epochs supporting data points areas correctly classiﬁes synthetic data points early training suffering catastrophic forgetting anymore. memory aware synapses played role robustness algorithm. figure iris dataset feedforward neural network different levels dropout epochs. colors represent hard decision boundaries i.e. class prediction confused actual probabilities. andrew jordan michael discriminative generative classiﬁers comparison logistic regression naive bayes. dietterich becker ghahramani advances neural information processing systems press zeiler matthew zhang sixin yann fergus rob. regularization neural networks using dropconnect. dasgupta sanjoy mcallester david proceedings international conference machine learning volume proceedings machine learning research atlanta georgia pmlr. http// proceedings.mlr.press/v/wan.html. hinton srivastava krizhevsky sutskever salakhutdinov improving neural networks preventing co-adaptation feature detectors. arxiv e-prints july", "year": 2018}