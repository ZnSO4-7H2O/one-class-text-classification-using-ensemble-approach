{"title": "Denoising Adversarial Autoencoders", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Unsupervised learning is of growing interest because it unlocks the potential held in vast amounts of unlabelled data to learn useful representations for inference. Autoencoders, a form of generative model, may be trained by learning to reconstruct unlabelled input data from a latent representation space. More robust representations may be produced by an autoencoder if it learns to recover clean input samples from corrupted ones. Representations may be further improved by introducing regularisation during training to shape the distribution of the encoded data in latent space. We suggest denoising adversarial autoencoders, which combine denoising and regularisation, shaping the distribution of latent space using adversarial training. We introduce a novel analysis that shows how denoising may be incorporated into the training and sampling of adversarial autoencoders. Experiments are performed to assess the contributions that denoising makes to the learning of representations for classification and sample synthesis. Our results suggest that autoencoders trained using a denoising criterion achieve higher classification performance, and can synthesise samples that are more consistent with the input data than those trained without a corruption process.", "text": "figure comparison autoencoding models previous works include denoising autoencoders variational autoencoders adversarial autoencoders denoising variational autoencoders contributions daae idaae models. arrows diagram represent mappings implemented using trained neural networks. introduction denoising criterion model learns reconstruct clean samples corrupted ones; regularisation latent space match prior latter priors take simple form multivariate normal distributions. denoising variational autoencoder combines denoising regularisation single generative model. however introducing denoising criterion makes variational cost function used match latent distribution prior analytically intractable reformulation cost function makes tractable certain families prior posterior distributions. propose using adversarial training match posterior distribution prior. taking approach expands possible choices families prior posterior distributions. denoising criterion introduced adversarial autoencoder choice either shape conditional distribution latent variables given corrupted samples match prior abstract—unsupervised learning growing interest unlocks potential held vast amounts unlabelled data learn useful representations inference. autoencoders form generative model trained learning reconstruct unlabelled input data latent representation space. robust representations produced autoencoder learns recover clean input samples corrupted ones. representations improved introducing regularisation training shape distribution encoded data latent space. suggest denoising adversarial autoencoders combine denoising regularisation shaping distribution latent space using adversarial training. introduce novel analysis shows denoising incorporated training sampling adversarial autoencoders. experiments performed assess contributions denoising makes learning representations classiﬁcation sample synthesis. results suggest autoencoders trained using denoising criterion achieve higher classiﬁcation performance synthesise samples consistent input data trained without corruption process. modelling drawing data samples complex highdimensional distributions challenging. generative models used capture underlying statistical structure real-world data. good generative model able draw samples distribution data modelled also useful inference. modelling complicated distributions made easier learning parameters conditional probability distributions intermediate latent variables simpler distributions complex ones often intermediate representations learned used tasks retrieval classiﬁcation typically train model classiﬁcation deep convolutional neural network constructed demanding large labelled datasets achieve high accuracy large labelled datasets expensive difﬁcult obtain tasks. however many state-of-the-art generative models trained without labelled datasets example autoencoders learn generative model referred decoder recovering inputs corrupted encoded versions themselves. shape full posterior conditional original data samples match prior. shaping posterior distribution corrupted samples require additional sampling training trying shape full conditional distribution respect original data samples does. explore approaches using adversarial training avoid difﬁculties posed analytically intractable cost functions. additionally model trained using posterior conditioned corrupted data requires iterative process synthesising samples whereas using full posterior conditioned original data not. similar challenges exist denoising addressed al.. analyse address challenges adversarial autoencoders introducing novel sampling approach synthesising samples trained models. summary contributions include types denoising adversarial autoencoders efﬁcient train efﬁcient draw samples from; methods draw synthetic data samples denoising adversarial autoencoders markov chain sampling; analysis quality features learned denoising adversarial autoencoders application discriminative tasks. supervised learning setting given training data wish learn model maximises likelihood epfψ true label given observation supervised setting many ways calculate approximate likelihood ground truth label every training data sample. trying learn generative model absence ground truth calculating likelihood model observed data distribution ex∼ppθ challenging. autoencoders introduce step learning process allows estimation auxiliary variable variable take many forms shall explore several section. step process involves ﬁrst learning probabilistic encoder conditioned observed samples second probabilistic decoder conditioned auxiliary variables. using probabilistic encoder form training dataset ground truth output input probabilistic decoder trained dataset supervised fashion. sampling conditioning suitable obtain joint distribution marginalised integrating obtain encoding process local corruption process need learned. corruption process deﬁned corrupted auxiliary variable decoder therefore trained data pairs using local corruption process number dimensions close each-other. makes easy learn bengio shows learned model sampled using iterative process explore representations learned model transfer applications classiﬁcation. hinton show auxiliary variables autoencoder lower dimension observed data encoding model learns representations useful tasks classiﬁcation retrieval. rather treating corruption process encoding process missing potential beneﬁts using lower dimensional auxiliary variable vincent learn encoding distribution conditional corrupted samples. decoding distribution learns reconstruct images encoded corrupted images daes figure vincent show compared regular autoencoders denoising autoencoders learn representations useful robust tasks classiﬁcation. parameters learned simultaneously minimisng reconstruction error training include ground truth given unknown. form distribution samples mapped also unknown making difﬁcult draw novel data samples decoder model variational autoencoders given ez∼qφ]−kl||p] term corresponds likelihood reconstructed given encoding data sample formulation variational lower bound involve corruption process. term kl||p] kullback-libeller divergence samples drawn re-parametrisation trick figure chosen parametrised multivariate gaussian prior chosen gaussian distribution kl||p] computed analytically. divergence computed analytically certain choices prior posterior distributions. ‘fake’. generative model input samples drawn chosen prior distribution trained generate output samples indistinguishable target samples order ‘fool’ discriminative model making incorrect predictions. achieved following mini-max objective shown optimal discriminative model optimising generative model equivalent minimising jensen-shannon divergence generated target distributions general reasonable assume that training discriminative model quickly achieves near optimal performance property useful learning distributions jensen-shannon divergence easily calculated. optimal distribution generated samples matches target distribution. conditions discriminator maximally confused cannot distinguish ‘real’ samples ‘fake’ ones. consequence this adversarial training used capture complicated data distributions shown able synthesise images handwritten digits human faces almost indistinguishable real data makhzani introduce adversarial autoencoder probabilistic encoding model autoencoder framework generative model adversarial framework. discriminative model introduced. discriminative model trained distinguish latent samples drawn cost function used train discriminator zi=...n− size training batch. lprior ldis optimised indistinguishable makhzani al.’s adversarial autoencoder speciﬁed neural network whose input whose output allows arbitrary complexity unlike complexity usually limited gaussian. adversarial autoencoder chosen gaussian many cases mixture gaussians. case analytical solution kl||p] denoising variational lower bound becomes analytically intractable. however still analytical solution kl||p]. denoising variational autoencoder therefore maximises refer model trained maximise objective dvae dvae figure show dvae achieves lower negative variational lower bounds regular variational autoencoder test dataset. however note matched prior rather ˜qφ. means generating novel samples using simple process generating samples variational autoencoder. generate novel samples sample difﬁcult need evaluate ˜qφ. address problem. dvaes vaes limited choice prior posterior distributions exists analytic solution divergence. alternatively adversarial training used learn model matches samples arbitrarily complicated target distribution provided samples drawn target model distributions. adversarial training model trained produce output samples match target probability distribution achieved iteratively training competing models generative model discriminative model discriminative model samples either generator samples target distribution trained correctly predict whether samples ‘real’ zi=...nj=...m ˆzi=...n− ˜xi=...nj=...m xi=...n− daae matching prior since drawing samples trivial matched adversarial training. efﬁcient matching since monte-carlo integration step needed figure using adversarial training place divergence restriction must able draw samples chosen prior. matching achieved minimising following loss function though computationally efﬁcient train drawbacks trying synthesise novel samples rather matched prior. effects using daae rather idaae visualized plotting empirical distribution encodings data samples corrupted data samples desired prior shown figure section review several techniques used draw samples trained autoencoders identify problem sampling dvaes also applies daaes propose novel approach sampling daaes; draw strongly previous work bengio samples generated sampling learned conditioning drawn suitable distribution. case variational adversarial autoencoders choice distribution simple training distribution auxiliary variables matched chosen prior distribution therefore easy efﬁcient sample variational adversarial autoencoders following process process sampling denoising autoencoders complicated. case auxiliary variable corrupted image sampling process follows case auxiliary variable encoding sampling process same encompassing encoding decoding process. however since denoising autoencoder trained reconstruct corrupted versions inputs likely similar bengio propose method iteratively sampling denoising autoencoders deﬁning markov chain whose stationary distribution certain conditions exists equivalent certain assumption training data distribution. approach generalised extended bengio introduce latent distribution prior assumptions posterior analytically deﬁned adversary used match prior avoiding need analytically compute divergence. makhzani demonstrate adversarial autoencoders able match several different priors including mixture d-gaussian distributions. explore another direction adversarial autoencoders extending incorporate denoising criterion. propose denoising adversarial autoencoders denoising autoencoders adversarial training match distribution auxiliary variables prior distribution formulate versions denoising adversarial autoencoder trained approximately maximise denoising variational lower bound ﬁrst version directly match posterior prior using adversarial training. refer integrating denoising adversarial autoencoder idaae. second match intermediate conditional probability distribution prior refer daae. idaae adversarial training used bypass analytically intractable divergences daae using adversarial training broadens choice prior posterior distributions beyond divergence analytically computed. distribution encoded data samples given coded data samples given trained maximise likelihood reconstructed sample minimising reconstruction cost obfunction lrec tained following sampling process xi=...n− distribution training data. also want match distribution auxiliary variables prior choice match either choice trade-offs either training sampling. idaae matching prior dvaes often analytical solution divergence making difﬁcult match rather propose using adversarial training match requiring samples drawn training. challenging draw samples directly samples approximated ˜xi=...m samples training data figure matching achieved minimising following cost function show certain conditions transition operator deﬁnes ergodic markov chain converges show exists stationary distribution drawn speciﬁc choice initial distribution markov chain homogeneous transition operator deﬁned distributions whose parameters ﬁxed sampling. show markov chain also ergodic since chain homogeneous ergodic exists unique stationary distribution markov chain converge step shows stationary distribution know unique stationary distribution. markov chain converges section only change notation training data probability distribution previously represented represented help make distinctions natural system probability distributions learned distributions. further note prior distribution required sampling that lemma markov chain deﬁned transition operator ergodic provided corruption process additive gaussian noise adversarial pair optimal within adversarial framework. theorem assuming approximately equal adversarial pair optimal transition operator tθφ|z) deﬁnes markov chain whose unique stationary distribution figure compare idaae daae match encodings prior trained celeba dataset. encoding refers prior refers normal prior encoded corrupted data refers daae encoded corrupted samples match prior idaae encoded data samples match prior. consider implication drawing samples denoising adversarial autoencoders introduced section iii-a. using idaae formulation matched prior samples drawn conditioning however daae matching prior sampling becomes non-trivial. matched training implication that attempting synthesize novel samples drawing samples prior unlikely yield samples consistent become clear section iv-b. ensure draw novel data samples want draw samples training data point sample synthesis. means cannot data samples training data approximately draw samples completeness would like acknowledge several methods markov chains training autoencoders improve performance. approach synthesising samples using daae focused sampling trained models; markov chain sampling used update model parameters. analyses sections deliberately general rely speciﬁc implementation choice capture model distributions. section consider speciﬁc implementation denoising adversarial autoencoders apply task learning models image distributions. deﬁne encoding model maps corrupted data samples latent space maps samples latent space image space. respectively draw samples according conditional probabilities also deﬁne corruption process draws samples according parameters models learned autoencoder framework; parameters also updated adversarial framework. models trained using large datasets unlabelled images. autoencoder framework encoder decoder. used fully connected neural networks encoder decoder. rectifying linear units used intermediate layers encourage networks learn representations capture multi-modal distributions. ﬁnal layer decoder network sigmoid activation function used output represents pixels image. ﬁnal layer encoder network left linear layer distribution encoded samples restricted. described section iii-a autoencoder trained maximise log-likelihood reconstructed image given corrupted image. although several ways evaluate log-likelihood chose measure pixel wise binary cross-entropy reconstructed sample original samples corruption training learn parameters minimise binary cross-entropy training process summarised lines algorithm appendix. vectors output encoder take real values therefore minimising reconstruction error sufﬁcient match either prior this parameters must also updated adversarial framework. network denote output probability ﬁnal layer neural network sigmoid activation function constraining range intermediate layers network relu activation functions encourage network capture highly non-linear relations labels {‘real’ ‘fake’}. adversarial training applied depends whether prior refers samples drawn distribution wish zreal samples drawn prior discriminator trained predict whether ‘real’ ‘fake’. achieved learning parameters maximise probability correct labels assigned zreal. training procedure shown algorithm lines drawing samples zreal involves sampling prior distribution often gaussian. consider draw fake samples ake. samples drawn depends whether prior prior. drawing samples easy matched prior simply obtained mapping corrupted samples though encoder however matched prior must monte carlo sampling approximate samples process calculating given algorithm appendix detailed section iii-a. finally order match distribution samples prior adversarial training used update parameters holding parameters ﬁxed. parameters updated minimise likelihood correctly classiﬁes ‘fake’. training procedure laid lines algorithm although training process matching less computationally efﬁcient matching easy draw samples matched prior simply draw random value calculate sample. drawing samples parameters ﬁxed. matched prior iterative sampling process needed order draw samples sampling process described section iv-b. implement sampling process trivial. random sample drawn distribution; distribution chosen prior samples obtained iteratively decoding corrupting encoding given following section evaluate performance denoising adversarial autoencoders three image datasets handwritten digit dataset synthetic colour image dataset tiny images complex dataset hand-written characters denoising non-denoising adversarial autoencoders compared tasks including reconstruction generation classiﬁcation. evaluate denoising adversarial autoencoder three image datasets varying complexity. here describe datasets complexity terms variation within dataset number training examples size images. datasets omniglot omniglot dataset handwritten character dataset consisting categories character different writing systems examples character. example dataset -by- pixels taking values dataset split examples categories make training dataset example categories makes testing dataset. characters remaining categories make evaluation dataset. means experiments performed reconstruct classify samples categories seen training autoencoders. datasets sprites sprites dataset made unique human-like characters. character attributes including hair body armour trousers weapon type well gender. character animations consisting frames each. examples character however every example different pose. sample -by- pixels samples colour. training validation test datasets split character unique characters each sets character. datasets celeba celeba dataset consists images faces colour. though version dataset tightly cropped faces exists un-cropped dataset. samples testing rest training. example dimensions -by- labelled facial attributes example beard’ ‘blond hair’ ‘wavy hair’ etc. face dataset complex toronto face dataset used makhzani training aae. older version code theano available https//github.com/ tonicreswell/daae results presented ipython notebooks. since revised version paper theano longer supported experiments celeba datasets performed using pytorch. dataset detail architecture training parameters networks used train denoising adversarial autoencoders. dataset several daaes idaaes aaes trained. order compare models trained datasets network architectures batch size learning rate annealing rate size latent code used each. models trained using optimization algorithm. trained models benchmark allowing compare proposed daaes idaaes. architecture training omniglot decoder encoder discriminator networks consisted fully connected layers respectively layer neurons. found deeper networks proposed makhazni better convergence. networks trained epochs using learning rate batch size adam optimization algorithm. used gaussian prior additive gaussian noise standard deviation corruption process. training idaae steps monte carlo integration architecture training sprites encoder discriminator -layer fully connected neural networks neurons layer. decoder used -layer fully connected network neurons ﬁrst layer last layers conﬁguration allowed capture complexity data without ﬁtting. networks trained epochs using batch size learning rate adam optimization algorithm. used encoding units gaussian prior additive gaussian noise standard deviation corruption process. idaae trained steps monte carlo integration. architecture training celeba encoder decoder constructed convolutional layers rather fully connected layers since celeba dataset complex toronto face dataset makhzani encoder decoder consisted convolutional layers similar structure dcgan proposed radford used -layer fully connected network discriminator. networks trained epochs batch size using rmsprop learning rate momentum training discriminator. found using smaller momentum values lead blurred images however larger momentum values prevented network converging made training unstable. using adam instead rmsprop found values encodings became large consistent prior. encoding made units used gaussian prior. used additive gaussian noise corruption process. experimented different noise level found several values range suitable. classiﬁcation experiments ﬁxed synthesis daae demonstrate effect sampling used idaae experimented found sufﬁcient train idaae. comparing histograms encoded data samples histograms prior idaae trained particular value able whether sufﬁciently larger not. found sufﬁciently large experiments. samples synthesized using decoder trained idaae passing latent samples drawn prior decoder. hand pass samples prior decoder trained daae samples likely inconsistent training data. synthesize consistent samples using daae draw initial random distribution gaussian distribution simplicity decode corrupt encode sample several times synthesized sample. process equivalent sampling markov chain iteration markov chain includes decoding corrupting encoding iterations. used synthesize novel sample call sample generated passed decoder. evaluate quality synthesized samples calculated log-likelihood real samples model achieved ﬁtting parzen window number synthesised samples. details log-likelihood calculated dataset given appendix expect initial samples drawn daae lower log-likelihood drawn however expect markov chain sampling improve synthesized samples larger log-likelihood initial samples. clear whether drawn using daae better samples drawn form idaae. purpose experiments demonstrate challenges associated drawing samples denoising adversarial autoencoders show proposed methods sampling daae training idaaes allows address challenges. also hope show idaae daae samples competitive drawn aae. sampling omniglot here explore omniglot dataset look log-likelihood score testing evaluation dataset. recall testing dataset samples classes training dataset evaluation dataset samples different classes. figure omniglot log-likelihood compared testing evaluation datasets. training evaluation datasets samples different handwritten character classes. models trained using gaussian prior. training testing datasets samples handwritten character classes. less negative log-likelihood. initial samples drawn using daae negative log-likelihood values samples drawn using aae. however iteration sampling synthesized samples less negative log-likelihood values aae. additional iterations sampling worse results possibly synthesized samples tending towards multiple modes data generating distribution appearing like samples classes represented training data. omniglot testing dataset consists example every category training dataset. means multiple iterations sampling cause synthesized samples tend towards modes training data likelihood score testing dataset likely increase. results shown figure conﬁrm expectation; log-likelihood sample less negative sample. apparently conﬂicting results whether sampling improves worsens synthesized samples highlights challenges involved evaluating generative models using log-likelihood discussed depth theis reason also show qualitative results. figure shows initial samples drawn daae samples synthesised iterations sampling figure samples appear well vaired capturing multiple modes data generating distribution. sampling sprites alignment expectation idaae model synthesizes samples higher loglikelihood initial image samples drawn daae model under-perform compared model however iteration sampling synthesized samples higher log-likelihood samples aae. results also show step sampling applied log-likelihood decreases similar results omniglot evalutation dataset related training test data split dataset unique characters combinations seen training present testing dataset. results suggest sampling pushes synthesized samples towards modes training data. figure compare samples synthesized using synthesized using idaae trained using intergration steps. figure show samples drawn idaae improve upon drawn model. figure show samples synthesized daae using iterative approach described section iv-b initial samples blurry artifacts ﬁnal samples sharp free blurry artifacts. drawing samples idaae daae models trained celeba critical difference models emerges samples synthesized using daae good structure appear quite similar other idaae samples less good structure appear lots variation. lack variation daae samples related sampling procedure accoriding theory presented alain would similar taking steps toward highest density regions distribution explaining samples appear quite similar. comparing daae idaae samples samples generative models gans notice samples less sharp. however gans often suffer ‘mode collapse’ synthesised samples similar idaae suffer mode collapse require additional procedures prevent mode collapse further gans offer encoding model. variants bi-gan offer encoding models however ﬁdelity reconstruction poor. daae idaae models able reconstruct samples faithfully. explore ﬁdelity reconstruction next section compare state-of-art modiﬁed improved reconstruction ﬁdelity alice conclude section sampling making following observations; samples synthesised using idaaes out-performed aaes datasets convenient relatively small yields improvement time needed train idaae increase linearly also observed initial samples synthesized using daae poor cases even iteration sampling improves image synthesis. finally evaluating generated samples challenging loglikelihood always reliable qualitative analysis subjective. reason provided quantitative qualitative results communicate beneﬁts introducing sampling trained daae advantages idaaes aaes. reconstruction task involves passing samples test dataset trained encoder decoder recover sample similar original sample. reconstruction evaluated computing mean squared error reconstruction original sample. interested reconstruction several reasons. ﬁrst wish encodings stream tasks example classiﬁcation good indication whether encoding modeling sample well check reconstructions. example reconstructed image missing certain features present original images table reconstruction shows mean squared error reconstructions corrupted test data samples. table server purposes demonstrate cases daae idaae better able reconstruct images compared aae. motivate interested aaes opposed related approaches comparing reconstruction error mnist state variant alice designed improve reconstruction ﬁdelity gan-like models. motivated understand properties representations learned daae idaae trained unlabeled data. particular property interest separability latent space objects different classes. evaluate separability rather training semisupervised fashion obtain class predictions training representations similar fashion kumar classiﬁcation omniglot classifying samples omniglot dataset challenging training testing datasets consists classes examples class training dataset. classes make writing systems symbols writing systems visually indistinguishable. previous work focused classifying classes within single writing system however attempt perform classiﬁcation across classes. omniglot training dataset used train svms encodings extracted encoding models trained daae idaae models. classiﬁcation scores reported omniglot evaluation dataset results show daae idaae out-perform classiﬁcation task. daae idaae also out-perform classiﬁer trained encodings obtained applying image samples showing beneﬁts using denoising. perform separate classiﬁcation task using classes omniglot evaluation dataset second test performed reasons study well autoencoders trained subset classes generalise feature extractors classiﬁers classes seen autoencoder training; facilitate performance comparisons previous work linear classiﬁer trained samples classes evaluation dataset tested remaining sample class. perform classiﬁcation times leaving different sample class experiment. results shown table comparison information preserved encoding. second reason checking sample reconstructions also method evaluate whether model overﬁt test samples. ability reconstruct samples seen training suggests model overﬁt. ﬁnal reason motivate daae idaae models alternatives based models augmented encoders stream tasks require good sample reconstruction. expect adding noise training would prevent ﬁtting encourage model learn robust representations therefore expect daae idaae would outperform aae. reconstruction omniglot table compares reconstruction errors daae idaae trained omniglot dataset. reconstruction errors idaae daae less aae. results suggest using denoising criterion training helps network learn robust features compared non-denoising variant. smallest reconstruction error achieved daae rather idaae; qualitatively reconstructions using daae captured small details idaae lost some. likely related multimodal nature daae compared unimodal nature idaae. reconstruction sprites table shows reconstruction error samples sprite test dataset models trained sprite training data. case idaae model out-performed daae performed well aae. reconstruction celeba table shows reconstruction error celeba dataset. compare daae idaae models trained momentum daae idaae corruption idaae trained integration steps. also experimented however better results obtained using daae performs similarly well idaae outperforms both. figure shows examples reconstructions obtained using idaae. although reconstructions slightly blurred reconstructions highly faithful suggesting facial attributes correctly encoded idaae model. results show daae model out-performs model idaae performs less well suggesting features learned daae transfer better tasks learned idaae. idaae daae models also out-perform pca. classiﬁcation celeba perform extensive experiments evaluate linear separability encodings learned celeba dataset compare state methods including β-vae. train linear encodings daae predict labels facial attributes example ‘blond hair’ beard’ etc. experiments compare classiﬁcation accuracy attributes obtained using daae idaae compared previously reported results obtained β-vae shown figure results β-vae obtained using similar approach reported kumar used hyper parameters train models ﬁxed noise level idaae trained figure shows idaae daae models outperform β-vae models facial attribute categories. compare models easily question many facial attributes model perform another?’ context facial attribute classiﬁcation. question various combinations model pairs results shown figure figures comparing daae idaae respectively demonstrate attributes denoising models outperform non-denoising models. over particular attributes daae idaae outperform consistent daae idaae outperform attributes ‘attractive’ ‘blond hair’ ‘wearing hat’ ‘wearing lipstick’. idaae outperforms additional attribute ‘arched eyebrows’. various hyper parameters chosen train models; daae idaae choose level corruption idaae additionally choose number integration steps used training. compare attribute classiﬁcation results vastly different choices parameter settings. results figure facial attribute classiﬁcation comparison classiﬁcation scores daae idaae compared β-vae linear classiﬁer trained encodings demonstrate linear separability representation learned model. attribute classiﬁcation values β-vae obtained kumar presented chart figure daae. additional results idaae shown appendix ﬁgures show models perform well various different parameter settings. figure suggests model performs better smaller amount noise rather however important note large amount noise ‘break’ model. results demonstrate model works well various hyper parameters tuning necessary achieve reasonable results possible tuning done achieve better results however full parameter sweep highly computationally expensive. training sampling. terms log-likelihood synthesized samples idaae using even small number integration steps training idaae leads better quality images generated similarly using even step sampling daae leads better generations. conﬂicting log-likelihood values generated samples between testing evaluation datasets means measurements clear indication number sampling iterations affects visual quality samples synthesized using daae. cases necessary visually inspect samples order assess effects multiple sampling iterations propose types denoising autoencoders posterior shaped match prior using adversarial training. ﬁrst match posterior conditional corrupted data samples prior; call model daae. second match posterior conditional original data samples prior. call second model integrating daae idaae approach involves using monte carlo integration training. ﬁrst contribution extension adversarial autoencoders denoising adversarial autoencoders second contribution includes identifying addressing challenges related synthesizing data samples using daae models. propose synthesizing data samples iteratively sampling daae according transition operator deﬁned learned encoder decoder daae model corruption process used training. finally present results three datasets three tasks compare daae idaae models. datasets include handwritten characters collection human-like sprite characters dataset faces tasks reconstruction classiﬁcation sample synthesis. acknowledge engineering physical sciences research council funding doctoral training studentship. would also like thank arulkumaran interesting discussions managing cluster many experiments performed. also acknowledge nick pawlowski martin rajchl additional help clusters. classiﬁcation. suggests aaes variants interesting models study setting learning linearly separable encodings. also show speciﬁc several facial attribute categories idaae daae performs better aae. consistency suggests speciﬁc attributes denoising variants learn better non-denoising aae. daae idaae out-perform models generation reconstruction tasks suggest sometimes beneﬁcial incorporate denoising training adversarial autoencoders. however less clear models daae idaae better classiﬁcation. evaluating must consider practicalities training generative purposes practicalities primarily computational load model. integrating steps required training idaae means take longer train daae. hand possible perform integration process parallel provided sufﬁcient computational resource available. further model trained time taken compute encodings classiﬁcation models. finally results suggest using integrating steps training leads improvement classiﬁcation score. means classiﬁcation tasks worthwhile train idaae rather daae. generative tasks neither daae idaae model consistently out-perform terms loglikelihood synthesized samples. choice model strongly affected computational effort required vincent larochelle bengio p.-a. manzagol. extracting composing robust features denoising autoencoders. proceedings international conference machine learning pages vincent larochelle lajoie bengio p.-a. manzagol. stacked denoising autoencoders learning useful representations journal machine deep network local denoising criterion. learning research goodfellow pouget-abadie mirza warde-farley ozair courville bengio. generative adversarial nets. advances neural information processing systems pages nguyen yosinski bengio dosovitskiy clune. plug play generative networks conditional iterative generation images latent space. arxiv preprint arxiv. radford metz chintala. unsupervised representation learning deep convolutional generative adversarial networks. international conference learning representations case proof. consider equation following sampling process also equation similar proof bengio therefore stationary distribution markov chain deﬁned lemma markov chain deﬁned transition operator ergodic provided corruption process additive gaussian noise adversarial pair optimal within adversarial framework. proof. consider assuming good approximation underlying probability distribution s.t. training shapped distribution match prior s.t. holds points could visited would matched prior. suggests every point reached point suggests every point reached point assumption additive gaussian corruption process likely within spherical region around corruption process sufﬁciently large spheres nearby samples overlap xi+m {xi+ xi+m−} that sup) sup) support. then possible reach therefore chain irreducible positive recurrent. ergodic chain must also aperiodic points boundary values boundary mapped points boundary mapped applying corruption process followed reconstruction process always least possible outcomes assume spheres induced corruption process overlap least sphere either pushed boundary remains same pushed boundary moves state. probability either outcome positive always route points thus avoiding periodicity provided consider case would possible recover using corrupt samples encode corrupted samples reconstruct minimise reconstruction cost lrec log).mean α∇φlrec α∇φlrec match using adversarial training =approx draw samples zreal draw samples prior train discriminator ldis a∇χldis train decoder match prior lprior table models trained mnist models trained mnist dataset. corruption indicates standard deviation gaussian noise added corruption process prior indicated prior distribution imposed latent space. number monte carlo integration steps used training applies idaae. mnist dataset consists grey-scale images handwritten digits training samples validation samples testing samples. training validation testing dataset equal number samples category. samples -by- pixels. mnist dataset simple dataset classes many training examples making good dataset proof-ofconcept. however dataset simple necessarily reveal effects subtle potentially important changes algorithms training sampling. reason consider datasets greater complexity. table mnist reconstruction recon. shows mean squared error reconstructions corrupted test data samples accompanied standard error. corruption standard deviation additive gaussian noise used training testing. mnist dataset train total models detailed table iii. encoder decoder discriminator networks fully connected layers neurons each. models size encoding units prior distribution encoding matched gaussian. networks trained epochs training dataset learning rate batch size standard deviation additive gaussian noise used training idaae daae models. trained using training parameters networks described mixture gaussians prior. gaussian standard deviation equally spaced mean around circle radius units. results prior modes separated large regions probability. model prior unrealistic assumes mnist digits occupy distinct regions image probability space. reality expect numbers similar exist side side image probability space exist smooth transition handwritten digits. consequence this model intended speciﬁcally evaluating well posterior table shows reconstruction error samples testing dataset models trained mnist training dataset. reconstruction error daae idaae trained gaussian prior outperform aae. however reconstruction task model less challenging daae idaae models samples corrupted encoding step. best model reconstruction idaae increasing worse reconstruction error reconstructions tended appear like mean samples. reconstruction error daae trained using mixture gaussians under-performed compared rest models. reconstructions looked like mean images expected given nature prior. calculate log-likelihood samples drawn idaae models trained mnist daae dataset parzen window ﬁtted generated samples single trained model. bandwidth constructing parzen window selected choosing values evenly space along axis table shows log-likelihood test dataset samples drawn models trained mnist training dataset. first discuss models trained gaussian prior. best log-likelihood achieved idaae synthesised samples generated sampling daae model caused log-likelihood decrease. samples tended towards mean samples dropping modes causing log-likelihood decrease. initial samples obtained iterations sampling shown figure samples clearer although mode-dropping immediately apparent note digit present iterations. consider daae model trained prior. samples drawn prior shown figure purpose experiment show effects sampling distribution initial samples drawn signiﬁcantly different prior. samples drawn normal distribution passed decoder daae model produce initial image samples figure steps sampling applied synthesise samples shown figure expected initial image samples look like mnist digits sampling improves samples dramatically. unfortunately however many samples appear correspond samples modes data distribution. addition several modes appear missing model distribution. attributed nature prior since encounter problem extent using gaussian prior synthesising mnist samples fairly trivial since many training examples classes. reason difﬁcult beneﬁts using daae idaae models compared models. focus complex datasets omniglot sprites. idaae models correctly sampled daae models used synthesise samples higher log-likelihood samples synthesised using aae. classiﬁcation mnist mnist dataset consists classes classiﬁcation task involves correctly predicting label interval. mnist dataset classiﬁer trained encoded samples mnist training dataset evaluated encoded samples mnist testing dataset results shown table first consider results daae idaae models trained gaussian prior. classiﬁers trained encodings extracted encoders trained daae idaae models out-performed classiﬁers trained image samples. classiﬁers trained encodings extracted encoders learned daae idaae models out-performed trained encodings extracted encoders model. figure mnist markov chain sampling channel gaussian prior channel gaussian prior used train daae. samples drawn projected space colour coded digit labels. initial samples generated normal distribution. corresponding samples iterations sampling. notice highlighted changes iterations sampling. figure mnist markov chain sampling gaussian mixture prior used train daae. samples drawn projected space using colour coded digit labels. initial sample generated normal distribution. corresponding samples generated iterations sampling. table mnist log-likelihood calculate log-likelihood parzen window generated samples mean likelihood reported testing data set. bandwidth used parzen window determined using validation set. training test validation datasets different samples. table mnist classiﬁcation classiﬁers kernels trained encoded mnist training data samples. samples encoded using encoder trained daae idaae models. classiﬁcation scores given mnist test dataset. differences classiﬁcation score model mnist dataset small; might relatively easy classify mnist digits high accuracy turn complicated dataset omniglot dataset better show beneﬁts using denoising training adversarial autoencoders. calculate log-likelihood samples parzen window synthesised samples bandwidth determined testing dataset similar section e-d. log-likelihood evaluated evaluation dataset testing dataset. compute loglikelihood testing dataset parzen window synthesized samples different used calculate bandwidth. results shown figure", "year": 2017}