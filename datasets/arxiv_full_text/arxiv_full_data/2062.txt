{"title": "Continuous Inverse Optimal Control with Locally Optimal Examples", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Inverse optimal control, also known as inverse reinforcement learning, is the problem of recovering an unknown reward function in a Markov decision process from expert demonstrations of the optimal policy. We introduce a probabilistic inverse optimal control algorithm that scales gracefully with task dimensionality, and is suitable for large, continuous domains where even computing a full policy is impractical. By using a local approximation of the reward function, our method can also drop the assumption that the demonstrations are globally optimal, requiring only local optimality. This allows it to learn from examples that are unsuitable for prior methods.", "text": "inverse optimal control also known inverse reinforcement learning problem recovering unknown reward function markov decision process expert demonstrations optimal policy. introduce probabilistic inverse optimal control algorithm scales gracefully task dimensionality suitable large continuous domains even computing full policy impractical. using local approximation reward function method also drop assumption demonstrations globally optimal requiring local optimality. allows learn examples unsuitable prior methods. algorithms inverse optimal control also known inverse reinforcement learning recover unknown reward function markov decision process expert demonstrations corresponding policy. reward function used perform apprenticeship learning generalize expert’s behavior situations infer expert’s goals performing continuous high-dimensional domains challenging because algorithms usually much computationally demanding corresponding forward control methods. paper present algorithm eﬃciently handles deterministic mdps large continuous state action spaces considering shape learned reward function neighborhood expert’s demonstrations. integrate global information reward along alternative paths. analogous trajectory optimization methods solve forward control problem ﬁnding local optimum. however lack global optimality disadvantage solving forward problem actually advantageous ioc. removes assumption expert demonstrations globally optimal thus allowing algorithm examples exhibit local optimality. complex tasks human experts might easier provide locally optimal examples. instance skilled driver might execute every turn perfectly still take globally suboptimal route destination. algorithm optimizes approximate likelihood expert trajectories parameterized reward. approximation assumes expert’s trajectory lies near peak likelihood resulting optimization ﬁnds reward function peak prominent. since approach considers shape reward around examples require examples globally optimal remains eﬃcient even high dimensions. present variants algorithm learn reward either linear combination provided features common prior work nonlinear function features number recent methods prior methods solve entire forward control problem inner loop iterative procedure methods often arbitrary possibly approximate forward solver solver must used numerous times learning process making reward learning signiﬁcantly costly forward problem. dvijotham todorov avoid repeated calls forward solver directly learning value function however requires value function bases impose figure trajectory locally optimal globally suboptimal globally optimal trajectory prior methods usually require globally optimal examples approach examples locally optimal. warmer colors indicate higher reward. known reward unfortunately real demonstrations rarely perfectly optimal require model expert’s behavior explain suboptimality noise. employ maximum entropy model closely related linearly-solvable mdps model probability actions proportional exponential rewards encountered along trajectory partition function. model expert follows stochastic policy becomes deterministic stakes high random choices similar value. prior work likelihood derived equation maximized directly. however computing partition function requires ﬁnding complete policy current reward using variant value iteration high dimensional spaces becomes intractable since computation scales exponentially dimensionality state space. following sections present approximation equation admits eﬃcient learning high dimensional continuous domains. addition breaking exponential dependence dimensionality approximation removes requirement example trajectories globally optimal requires approximate local optimality. example locally optimal globally suboptimal trajectory shown figure although another path higher total reward local perturbation trajectory decreases reward total. structure solution instead common reward bases. good value function bases diﬃcult construct portable across domains. considering reward around expert’s trajectories method removes need repeatedly solve diﬃcult forward problem without losing ability utilize informative reward features. eﬃcient algorithms proposed special case linear dynamics quadratic rewards however unlike forward case approaches diﬃcult generalize arbitrary inverse problems learning quadratic reward matrix around example path readily generalize states non-lqr task. this methods applied tasks conform assumptions method also uses quadratic expansion reward function instead learning values quadratic reward matrix directly learns general parameterized reward using hessian gradient. show section particularly eﬃcient variant algorithm derived dynamics linearized derivation fact follow standard assumptions. however approximation required general form algorithm assume linearized dynamics. previous methods also assume expert demonstrations globally optimal near-optimal. although makes examples informative insofar learning algorithm extract relevant global information also makes methods unsuitable learning examples locally optimal. shown evaluation method learn rewards even locally optimal examples. path length action dimensionality. describe approximate algorithms evaluate likelihood time linear linearizing dynamics. greatly speeds method longer examples though noted modern linear solvers well optimized symmetric matrices making quite feasible evaluate likelihood without linearization moderate length paths. evaluate equation without computing partition function apply laplace approximation locally models distribution gaussian note equivalent modeling reward function gaussian since equation uses rewards along path. context corresponds assuming expert performs local optimization choosing actions rather global planning. assumption strictly less restrictive assumption global optimality. write almost entirely terms matrices block diagonal block triangular. unfortunately ﬁnal second order term exhibit convenient structure. particular hessian last state respect actions arbitrarily dense. therefore disregard term. since zero dynamics linear corresponds linearizing dynamics. ﬁrst describe approach directly evaluating likelihood assumption zero. ﬁrst exploit structure evaluate time linear essential computing requires simple recursion intuitively likelihood indicates reward functions example paths small gradients large negative hessians likely. magnitude gradient corresponds close example local peak reward landscape hessian describes steep peak given parameterization reward learn likely parameters maximizing equation next section discuss objective gradients computed eﬃciently. approximate likelihood equation makes assumptions dynamics algorithm section linearizes dynamics around examples. matches assumptions commonly studied linear-quadratic regulator setting suggests alternative derivation algorithm linear-quadratic system linearized dynamics given quadratic reward matrices given diagonal blocks linear reward vectors given complete derivation resulting algorithm presented appendix supplement similar maxent algorithm described ziebart addition recursion compute derivatives parameterized reward hessians. since gradients computed recursively method lacks convenient form provided equation easier implement. objective equation learn reward functions variety representations. present variant learns reward linear combination features second variant uses gaussian process learn nonlinear reward functions. linear variant algorithm provided features depend state action reward given weights learned. letting denote gradients hessians feature respect actions states full gradients hessians sums quantities weighted ˜gθk. gradient respect simply gradients matrices given analogously. likelihood gradient obtained equation evaluating equation determinant negative hessian undeﬁned determinant positive. corresponds example path lying valley rather peak energy landscape. high-probability reward function avoid cases nontrivial initial point objective evaluated. therefore dummy regularizer feature ensures negative hessian positive determinant. feature gradient uniformly zero hessian equal negative identity. pseudoinverse potentially nonsquare matrix linear system solved passes upward pass solve downward pass solve pass block generalization forward back substitution like recursion equation exploit structure time linear however upward pass must also handle oﬀ-diagonal entries potentially nonsquare blocks invertible. therefore construct partial solution upward pass expressed terms ¯ht−. ﬁnal values reconstructed downward pass together solution complete algorithm computes determinant |−h| time linear included given parameterization reward determine likely parameters maximizing likelihood gradient-based optimization requires gradient equation reward parameter gradient ttij denotes ijth entry block last vanishes zero quantities computed time linear diagonal blocks jth−j computed solving shown appendix supplement. gradients reward parameterization compute gradients apply equation. initial weight feature must negative hessians example paths positive deﬁnite. suitable weight simply doubling requirement met. optimization must drive zero order solve original problem. role relaxation allowing algorithm explore parameter space without requiring hessian always negative deﬁnite. unfortunately driving zero quickly create numerical instability hessians become ill-conditioned singular. rather simply penalizing regularizing weight found maintain numerical stability still obtain solution using augmented lagrangian method method solves sequence maximization problems augmented penalty term form vector corresponding covariance inducing point given exact likelihood proposed approximation obtained using likelihood prior likelihood equation feature derivatives deﬁned previous section given analogously. using quantities likelihood computed described section likelihood gradients derived appendix supplement. algorithm learn expressive rewards domains linear reward basis known usual bias variance tradeoﬀ comes increased model complexity. shown evaluation linear method requires fewer examples linear basis available nonlinear variant work much less expressive features. nonlinear variant algorithm represent reward function gaussian process maps feature values rewards proposed levine inputs gaussian process inducing feature points noiseless outputs points learned. location inducing points chosen variety ways follow levine choose points example paths concentrates learning regions examples informative. addition outputs also learn hyperparameters describe kernel function given evaluate method simulated robot control planar navigation simulated driving. robot task expert sets continuous torques joint n-link planar robot arm. reward depends position end-eﬀector. link angle velocity producing state space dimensions. changing number links vary dimensionality task. example -link shown figure complexity task makes diﬃcult compare prior work also include simple planar navigation task expert takes continuous steps plane shown figure finally humancreated examples simulated driving task shows method learn complex policies human demonstrations realistic domain. reward function robot navigation tasks gaussian peak center surrounded four pits. reward also penalizes action square magnitude. algorithms provided grid evenly spaced gaussian features squared action magnitude. nonlinear test section cartesian coordinates end-eﬀector provided instead grid. compare linear nonlinear variants method maxent optv algorithms present results linear time algorithm section though found variant direct non-linearized approach produced similar results. maxent used grid discretization states actions optv used discretized actions adapted value function features described dvijotham todorov. since optv cannot learn action-dependent rewards provided true weight action penalty term. evaluate learned reward ﬁrst compute optimal paths respect reward random initial states part training set. also paths begin initial states optimal respect true reward. cases compute evaluation paths figure planar navigation rewards learned locally optimal examples. black lines show optimal paths reward originating example initial states. rewards learned algorithms better resemble true reward learned prior methods. globally optimal ﬁrst solving discretization task value iteration ﬁnetuning paths continuous optimization. evaluation paths computed reward functions obtain reward loss subtracting true reward along learned reward’s path true reward along true optimal path. loss learned reward induces policy true high learned reward causes costly mistakes. since reward loss measured entirely globally optimal paths captures well algorithm learns true global reward regardless whether examples locally globally optimal. test well method handles locally optimal examples navigation task increasing numbers examples either globally locally optimal. discussed above globally optimal examples obtained discretization locally optimal examples computed optimizing actions random initialization. test repeated eight times random initial states example. results figure show variants algorithm converge correct policy. linear variant requires fewer examples since features form good linear basis true reward. maxent assumes global optimality converge correct policy examples locally optimal. also suﬀers discretization error. optv figure reward loss algorithm either globally locally optimal planar navigation examples. prior methods converge expert’s policy examples globally optimal. figure reward loss algorithm gaussian grid end-eﬀector position features -link robot task. nonlinear variant method could learn reward using position features. table statistics sample paths learned driving rewards corresponding human demonstrations starting initial states. statistics learned paths closely resemble holdout demonstrations. evaluate method handles human demonstrations simulated driving task. although driving policies learned prior methods discrete formulation required discrete simulator agent makes simple decisions choosing lane switch constrast driving simulator fully continuous second order dynamical system. actions correspond directly breaks steering simulated state space includes position orientation linear angular velocities. this prior methods rely discretization cannot tractably handle domain. used nonlinear method learn sixteen -second examples aggressive driver cuts cars evasive driver drives fast keeps plenty clearance tailgater follows closely behind cars. features speed deviation lane centers gaussians covering front back sides cars road. since ground truth reward tasks cannot reward loss metric. follow prior work quantify much learned policy resembles demonstration using task-relevant statistics measure average speed sample paths learned reward amount time spend within car-lengths behind front cars compare statistics unobserved holdout user demonstrations start initial states. results table show statistics learned policies similar holdout demonstrations. robot task evaluated method gaussian grid features simple features provide position eﬀector therefore form linear basis true reward. examples globally optimal. number links resulting -dimensional state space. nonlinear variant algorithm could successfully learn reward simple features shown figure even grid features form linear basis reward maxent suﬀered greater discretization error complex dynamics task optv could meaningfully generalize reward increased dimensionality task. evaluate eﬀect dimensionality increased number robot links. shown figure processing time methods scaled gracefully dimensionality task quality reward deteriorate appreciably. processing time optv increased exponentially action space discretization. maxent discretization intractable links therefore shown. figure reward loss processing time increasing numbers robot links corresponding state spaces dimensions. methods eﬃciently learn good rewards even dimensionality increased. plots learned rewards shown figure videos optimal paths learned rewards downloaded project website along supplementary appendices source code http//graphics.stanford.edu/projects/cioc.", "year": 2012}