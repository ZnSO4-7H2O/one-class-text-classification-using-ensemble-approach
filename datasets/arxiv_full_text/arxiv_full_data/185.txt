{"title": "Origami: A 803 GOp/s/W Convolutional Network Accelerator", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "B.7.1; I.2.6"], "abstract": "An ever increasing number of computer vision and image/video processing challenges are being approached using deep convolutional neural networks, obtaining state-of-the-art results in object recognition and detection, semantic segmentation, action recognition, optical flow and superresolution. Hardware acceleration of these algorithms is essential to adopt these improvements in embedded and mobile computer vision systems. We present a new architecture, design and implementation as well as the first reported silicon measurements of such an accelerator, outperforming previous work in terms of power-, area- and I/O-efficiency. The manufactured device provides up to 196 GOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power efficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it the first architecture scalable to TOp/s performance.", "text": "increasing number imaging devices importance digital signal processing imaging continues grow. amount onnear-sensor computation rising thousands operations pixel requiring powerful energy-efﬁcient digital signal processing solutions often cointegrated imaging circuitry reduce overall system cost size embedded vision systems extract meaning imaging data enabled energy-efﬁcient low-cost integrated parallel processing engines permits generation distributed computer vision systems bring huge value vast range applications reducing costly data transmission forwarding desired information many opportunities challenging research innovative applications evolution advanced embedded video processing future situational awareness systems. opposed conventional visual monitoring systems send video data data center stored processed embedded smart cameras process image data directly board. signiﬁcantly reduce amount data transmitted required human intervention sources expensive aspects video surveillance embedding convolutional network classiﬁers distributed computer vision systems seems natural direction evolution however deep neural networks commonly known demand computing power making challenging bring computational load within power envelope embedded systems fact state-of-the-art neural networks currently trained also evaluated workstations powerful gpus achieve reasonable performance. strong demand mobile vision solutions ranging object recognition advanced humanmachine interfaces augmented reality. market size estimated grow many billions dollars next years annual growth rate prompted many commercial solutions become available recently speciﬁcally targeting mobile sector paper present architecture novel convolutional network accelerator scalable top/s performance remaining areaenergy-efﬁcient keeping throughput within limits economical packages power budgets. extends work abstract—an ever increasing number computer vision image/video processing challenges approached using deep convolutional neural networks obtaining state-of-the-art results object recognition detection semantic segmentation action recognition optical superresolution. hardware acceleration improvements embedded mobile computer vision systems. present architecture design implementation well ﬁrst reported silicon measurements accelerator outperforming previous work terms power- areai/oefﬁciency. manufactured device provides gop/s silicon technology achieve power efﬁciency gop/s/w. massively reduced bandwidth requirements make ﬁrst architecture scalable top/s performance. success many application areas solving real-world problems entertainment systems robotics surveillance researchers engineers tackling action object recognition problems help brain-inspired algorithms featuring many stages feature detectors classiﬁers lots parameters optimized using wealth data recently become available. deep learning techniques achieving recordbreaking results challenging problems datasets outperforming either mature concepts trying model speciﬁc problem hand joining forces traditional approaches improving intermediate steps convolutional networks prime example powerful conceptually simple paradigm applied various data sources perform best information spatially temporally well-localized still seen global context images. testimony success deep learning approaches several research programs launched even major global industrial players pushing towards deploying services based brain-inspired machine learning customers within cavigelli benini department electrical engineering information technology zurich zurich switzerland. e-mail {cavigelli benini}iis.ee.ethz.ch. implementation architecture optimized precision using ﬁxed-point evaluations constrained accelerator-sized asic. silicon measurements taped-out asic providing experimental characterization silicon. thorough comparison discussion previous organization paper section shortly introduces convolutional networks highlights need acceleration. previous work investigated section discussing available software fpga asic implementations explaining selection design objectives. section present architecture properties. implementation aspects shown section present results section discuss compare section vii. conclude paper section viii. convolutional networks built basic building blocks convolution layers activation layers pooling layers. sequence convolution activation pooling considered stage modern deep networks often consist multiple stages. convolutional network used feature extractor transforming data higher-dimensional meaningful representation. convnets particularly preserve locality limited ﬁlter size makes suitable visual data feature extraction followed classiﬁer normal neural network support vector machine. indexes output channels indexes input channels pixel identiﬁed tuple denotes support ﬁlters. recently published networks pooling operation determines maximum small neighborhood channel often areas stride poolmax× linear unit designates function max. activation function introduces nonlinearity neural networks giving potential powerful linear methods. typical ﬁlter sizes range sometimes even feature extractor convolutional layers usually followed classiﬁcation step fully-connected neural network layers interspersed activation functions reducing dimensionality several hundred even thousands number classes. case scene labeling fully-connected layers applied per-pixel basis inputs values channels given pixel pixel convolutional networks deep neural networks general advancing domains computer vision becoming increasingly accurate traditional application area object recognition detection. convnets able compute highly accurate optical super-resolution more. newer networks usually deeper require computational effort newly tapped topics already deep beginning. research done various platforms computing devices evolving rapidly making time measurements meaningless. deep learning community thus started measure complexity deep learning networks independent underlying computing platform counting additions multiplications synapses networks. convolutional layer input feature maps size ﬁlter kernel size nout output feature maps number amounts nout number output channels |cout| number |cin| size image hk×wk size ﬁlter spatial domain. factor multiplications additions counted separate operations measure common neural network literature however measuring complexity still allow perfectly determine network performs different platforms. accelerators might need initialized suspend computation load ﬁlter values often performing better artiﬁcially large small problems. reason distinguish throughput obtained real network measurements obtained synthetic benchmark optimized squeeze largest possible value maximum throughput computation units without caring bandwidth limits often stated device speciﬁcations non-specialized processors software hardware implementations alike often come throughput dependent actual size convolutional layer. make sure chip large range convnets efﬁciently presented reference performance evaluation. three stages assume input images size resulting sizes complexities individual layers summarized table ﬁlter size them. total number operations required gop/frame. give idea complexity well-known convnets listed table take existing system like neuflow able operate gop/s/w high quality dense optical video computed frame/s power around could scale architecture. also optimized implementation high-end around frame/s. convolutional networks evaluated signiﬁcantly faster traditional approaches comparable accuracy approaching area real-time applications become feasible workstations often several gpus. however application areas require complete solution within power envelope embedded systems even mobile device. taking aforementioned scene labeling convnet examples usage real-time setting frame/s amounts gop/s scope even recent commercially available mobile processors subject area changing rapidly deep learning long-term usability important objective thinking hardware acceleration building blocks systems. structure networks changing application application year year better activation pooling operations continuously published commonality convnets convolutional layer. around since early changed since fortunately element also computation-intensive part welloptimized software implementations shown figure time activation pooling negligible well computation time pixel-wise classiﬁcation fully-connected layers. stanford backgr. stanford backgr. stanford backgr. imagenet/ilsvrc imagenet/ilsvrc imagenet/ilsvrc imagenet/ilsvrc imagenet/ilsvrc synthetic kitti sintel convolutional networks achieving amazing results lately even outperforming humans image recognition large complex datasets imagenet. performers achieved top- error rate ilsvrc competition best performance single human dataset exceeded since last large image recognition competition also subjects face recognition convnets exceeding human performance. listed required number operations evaluate networks table remainder section focus existing implementations evaluate convnets. compare software implementations running desktop workstations cpus gpus also dsp-based works existing fpga asic implementations. section iii-d discuss many accelerators suitable evaluate networks size conclude investigation previous work discussing limitation existing hardware architectures section iii-e. acceleration convolutional neural networks discussed many papers. fast user-friendly frameworks publicly available torch caffe nvidia’s cudnn nervana systems’ neon gpu-accelerated training evaluation commonly working convnets. optimized implementations used obtain performance power efﬁciency baseline desktop workstations cuda-compatible embedded processors tegra desktop performance reach gop/s special problems gop/s meaningful convnets. tegra gop/s achieved gop/s achieved actual convnet. platforms energy-efﬁciency gop/s/w considering power entire platform gop/s/w differential power measurements obtained except evaluation focus usually training speed multiple images processed together batches attain higher performance batch processing suitable real-time applications since introduces delay many frames. comparison throughput many optimized software implementations gpus based several well-known convnets provided list lead implementation nervana systems details works known publicly. conﬁrm based maxdnn started optimized matrix-matrix multiplication adapted convolutional layers ﬁne-tuned assembly code. implementation tightly followed nvidia’s cudnn library edge implementations others originates using half-precision ﬂoating point representations instead singleprecision storage memory thus reducing required memory bandwidth currently limiting factor. gpu-based platforms nvidia tegra supporting half-precision computation used save power provide speedup thorough investigations published this. computer vision silicon presented recently movidius myriad device used google tango mobileye eyeq platform benchmarking results regarding convnets available yet. different approach increase throughput fourier transform diagonalizing convolution operation. positive effect kernels larger bandwidth problem generally becomes much worse already considerable memory requirements boosted further since ﬁlters padded input image size however optimized software running platforms always constrained underlying architecture arithmetic precision cannot adapted needs computation caches used instead optimized onchip buffers instructions loaded decoded. pushes need specialized architectures achieve high powerarea-efﬁciency. embeddability energy-efﬁciency major concern regarding commercialization convnet-based computer vision systems hence prompted many researchers approach issue using fpga implementations. arguably popular architecture started improved renamed neuflow later nn-x spartan fpga using ﬁxed-point arithmetic multiplications. architecture designed self-contained allowing execute operations common convnet layers coming soft control overall program ﬂow. also features compiler converting network implementations torch directly instructions. cnps architecture allow easy scaling performance prompting follow-up work neuflow uses multiple convolution engines interconnect smart controller. data processing tiles rerouted runtime. work published features virtex vlxt achieve gop/s using ﬁxed-point arithmetic. make newly available platform neuflow ported zynq improved making hard-wired cores renamed nnx. increases throughput gop/s uses mb/s fullduplex memory interfaces. alternatives cnp/neuflow/nn-x exist. relevant convnet accelerator based microsoft’s catapult platform little known details hls-based implementation performance energy efﬁciency inferior nn-x. neuflow architecture implemented asic silicon process. results based post-layout simulations published featuring performance gop/s operating external memory bandwidth gb/s full-duplex. explore possibilities terms energy efﬁciency convolution accelerator suitable small convnets implemented fdsoi technology achieve gop/s gop/s/w gop/s gop/s/w simulation implementation using aggressive voltage scaling combined reverse body biasing available fdsoi technology. interesting aspects highlighted shidiannao evolved diannao original diannao tailored fully-connected layers also able evaluate convolutional layers. however buffering strategy making structure computational problem hand. improved shidiannao. nevertheless performance strongly depends size convolutional layer computed unfolding performance tiny feature maps networks. achieve peak performance gop/s core-only area tsmc post-layout evaluation. another approach problem hand look general convolution accelerators convengine particularly targets convolutions common computer vision applications. comes array alus input output buffers optimized task last months seen wave vision cores socs becoming commercially available cevaxm synopsys designware cadence tensilica vision targeted general vision applications speciﬁcally tailored convnets. processor-based vector engines many small specialized processing units. many mentioned blocks never implemented silicon architecture kept conﬁdential peer reviewed making quantitative comparison impossible. however instruction-based processing energy efﬁciency respect specialized asics expected. besides aforementioned efforts many accelerators targeted accelerating non-convolutional neural networks. accelerator k-brain evaluated achieve outstanding power efﬁciency top/s/w technology. comes sram store weights dataset. applications insufﬁcient vgg-oxfordnet parameters) presented architectures scale larger networks requiring excessive amounts on-chip memory neural network accelerators targeted experimental concepts like spiking neural networks thorough performance evaluations still missing recent work hardware accelerators convnets shows highly energy-efﬁcient implementations feasible signiﬁcantly improving software implementations. however existing architectures scalable higher performance applications consequence need wide memory interface. manifests pins required achieve gop/s using neuflow many interesting applications much higher throughput needed e.g. scene labeling full-hd frames requires gop/s process frame/s trend clearly points towards even complex convnets. underline need better options want emphasize linearly scaling neuflow would require almost pins gb/s full-duplex memory bandwidth. issue currently common related work long target application limited tiny networks allow caching entire data processed. work particularly focuses issue reducing memory bandwidth required achieve high computational throughput without using large on-chip memories store ﬁlters intermediate results. state-of-the-art networks storing learned parameters on-chip feasible googlenet requiring oxfordnet parameters. aforementioned scene labeling convnet required parameters parameters ﬁlter weights convolutional layers. experiments section ﬁrst present concept operation architecture simple conﬁguration. explain changes make suitable areaefﬁcient implementation. proceed looking possible inefﬁciencies processing convnet data. conclude section presenting system architecture suitable embed origami fpga-based system. top-level diagram architecture shown figure shows different clock areas explained later concept operation architecture ﬁrst assumes single clock entire circuit simplicity. figure show timeline input output utilization. note utilization internal blocks corresponds utilizations direct short delay. input data stripes conﬁgurable height circuit stored sram keeps spatial window input image data. data loaded image bank smaller window size ﬁlter kernel kept registers moved image stripe jumping next column. register-based memory provides input sum-of-product units inner products individual ﬁlter kernels computed. unit image channel different ﬁlters computes partial different output channel. circuit iterates channels input image partial sums accumulated channel summer size want keep image bank small possible requiring excessive data rate sram. size image bank chosen nchhkwk. every cycle current input channel elements loaded image window sram shifted image bank. situation illustrated figure individual channel. order sram able provide minimum amount data needs store element wide window channels selectable height large image circuit stripes maximum height hinmax overlap pixels. overlap evaluation kernel need surrounding pixel height pixel width. image bank reaches bottom image window stored sram jumps back shifted pixel right. introduces delay cycles rest circuit idling. delay loading values image bank also receive pixels image window sram external i/o. choosing thus mostly tradethroughput area. performance penalty overall circuit factor behavior observed beginning horizontal stripe. ﬁrst nchhin cycles processing units idling. filter bank ﬁlter bank stores weights ﬁlters nchnchhkwk values. conﬁguration mode ﬁlter values shifted registers clocked lower frequency normal operation entire ﬁlter bank read-only. cycle ﬁlter values supplied changed means nchhkwk ﬁlter values read cycle. many ﬁlters read parallel change frequently possible keep sram. instead implemented registers multiplexer capable multiplexing selecting sets nchhkwk weights. architecture tile convolutional layer blocks ﬁxed number input output channels nch. chhkwk operations every clock cycles perform transmitting receiving values instead different previous work improves throughput bandwidth factor nch. architecture also formulated non-equal block size input output channels advantage thus keep constraint simplicity notation. proceed presenting individual blocks architecture detail. image window sram image bank image window sram image bank charge storing received image data providing units image patch required every computation cycle. minimize size ﬁlter bank depends quadratically number channels processed results trade-off area bandwidth efﬁciency. doubling efﬁciency storage requirements ﬁlter bank quadrupled. global memory structures provide lots data high speed often problematic back design. thus important highlight ﬁlter bank seen global memory structure actually local unit needs access ﬁlters output channel processes unit accesses ﬁlters. sum-of-products units unit calculates inner product image patch ﬁlter kernel. built hkwk multipliers hkwk adders arranged tree. mathematically output unit described previous steps loaded stored data perform arithmetic operations raises question numerical precision. ﬁxed-point analysis select word-width shown section v-b. terms architecture word width doubled multiplier adder tree adds bits. truncate result original word width underlying ﬁxed-point representation. truncation also reduces accuracy adder tree multipliers implemented. idea using ﬁxed-point representation input output motivated fact multiple convolutional layers output also serve input. channel summer units chsum unit sums inner products receives unit connected reducing amount data transmitted circuit factor /nch naive transmitting individual convolution results. units built able perform accumulation still storing total results one-by-one transmitted circuit next computations already running. chsum units also perform calculations full precision results truncated original ﬁxed-point format. achieve high area efﬁciency essential large logic blocks operated high frequency. pipeline multipliers adder tree inside units achieve desired clock frequency. streaming nature overall architecture makes simple vary withdrawbacks encountered closed-loop architectures. limiting factor overall clock frequency sram keeping image window comes ﬁxed delay minimum clock period speed cmos pads. sram’s maximum frequency much lower computation-densitypoweroptimized units chosen running twice frequency. unit calculates inner products image bank changes channel input image takes step forward. makes unit responsible output channels. little change image bank values taken ﬁlter bank switched faster frequency well. additionally chsum units adapted alternatingly accumulate inner products different output channels. changes induced ﬁlter bank reduce number ﬁlter values read nchhkwk/ cycle however twice clock rate. adapter ﬁlter bank able read sets nchhkwk/ weights operations second. looking units calculate multiplications additions cycle. mentioned before clock running twice speed maximize area efﬁciency using nsop nch/ units. blocks circuit designed able sustain maximum throughput. nevertheless several aspects cause core operation units stall. discuss aspects following paragraphs. border effects borders image valid convolution results calculated core wait necessary data transferred device. waiting periods occur beginning image columns preloaded beginning column pixels loaded cycles. effective throughput thus depends size image maximum limited hinmax depending size image window sram. feasible choose large enough process reasonably sized image otherwise image tiled multiple smaller horizontal assuming hinmax large enough considering reference network factor stages respectively case pixel input image. larger images signiﬁcantly improved e.g. image stage efﬁciency factor however height input image limited pixel memory size image bank. filter loading image transmission start ﬁlters loaded used transmit image data. causes loss cycles. instead nchhinwin input data values additional chhkwk words ﬁlter weights transferred. results additional efﬁciency loss factor channel idling number output input channels usually correspond number output input channels processed parallel core. output input channels partitionned blocks ﬁlling all-zero ﬁlters unused cases. outputs blocks summed pixel-wise off-chip. processing blocks strong additional impact efﬁciency fully utilizing core. reasonable choice stages reference convnet perfectly split blocks thus performance lost stage input channels load core ηblocks however stages small number input and/or output channels generally perform much less operations efﬁciency cases thus important. total throughput reference convnet running device conﬁguration used implementation summarized table alongside details efﬁciency individual stages. designing architecture important keep mind used larger system. system able take video stream camera analyze content images using convnets display result transmit alerts data analysis network. general architecture elaborate conﬁguration based show advantages design. besides necessary peripherals four origami chips lpddr memory. fpga could xilinx zynq device. fpga conﬁgured include preprocessing core rescaling color space conversion local contrastive normalization. store data memory preprocessing also load store data applying convolutional layer using origami chips applying fully-connected layers controller memory controller. remaining steps convnet like summing partial results returned origami chips adding bias applying relu non-linearity max-pooling done fpga requires little resources since multipliers required. multipliers required apply fully-connected layers following convnet feature extraction fast since share operations layer less scene labeling convnet every stage convnet tile data blocks height hinmax input channels output channels. blocks input channels reassemble ﬁnal image terms output channels horizontal stripes. bandwidth considerations naive setup means need able provide memory accesses full bandwidth every connected origami chip together. however also need load previous value output pixel results partial sums need added ﬁnal result. case relu operation max-pooling done scan-line procedure right computing ﬁnal values convolutional layers requiring buffer )hinmax/ values max-pooling since operation applied vertical horizontal direction independently however optimal improve using concept origami chip itself. arrange origami chips calculate result larger tile input output channels making chips share input data chips generate output data immediately summed writing memory. analogous principle applied on-chip saves factor read write access memory. course limitations explained pixel-wise fully-connected layers computed single pass requiring entire image loaded once. scene labeling convnet require parameters readily stored within fpga alongside intermediate values computations. system also integrated reduced system size lower cost well improved energy efﬁciency. makes memory bandwidth requirement important aspect system able narrow moderate-bandwidth memory interface translates lower cost terms packaging signiﬁcantly higher energy efﬁciency ﬁrst present general implementation circuit. thereafter present results ﬁxed-point analyses determine number format. ﬁnalize section summarizing implementation ﬁgures taking look implementation aspects entire system. discussed section operate design clocks originating source running twice frequency other. slower clock driving sram also elements need fast image ﬁlter bank. units channel summers computation achieve high area efﬁciency. achieve frequency multipliers subsequent adder tree pipelined. added pipeline stages each multipliers adder tree. taped-out chip ﬁlter size since found common ﬁlter sizes device capable computing larger ﬁlter sizes also capable calculate smaller ones. maximum height horizontal image stripe chose hinmax requiring image window sram size words. size latency split four blocks words word width seen ﬂoorplan alignment shown resulted best performance various conﬁgurations tried. macro cells placed besides left right boundary chip cells ﬂipped ports face towards center device. silicon testing included built-in self-test memory blocks. pads input data placed chip around image bank memory stored pipeline stage. output data located bottom-left together in-phase clock output test interface bottom-right die. control clock pads found around center right side. core pads placed center left right side each core placed fig. classiﬁcation accuracy ﬁlter coefﬁcients stored precision. single precision implementation achieves accuracy choosing input length results accuracy loss less core clock capabilities standard cmos pads on-chip clock generation unsuitable small chip also complicating testing. overcome this phase-shifted clocks circuit. clock directly drives clock tree slower clock domain inside chip. clock also xor-ed second input clock signal generate faster clock. previous work conclusive required precision convnets common values determine optimal data width design performed ﬁxed-point analysis based reference convnet. replaced convolution operations software model ﬁxed-point versions thereof evaluated resulting precision depending input output weight data width. quality analyzed based per-pixel classiﬁcation accuracy test images omitted training. used images stanford backgrounds dataset train network. results shown output length sufﬁcient keep implementation loss drop accuracy. since convolution layers applied repeatedly little processing them chose signal width input although could reduced further. ﬁlter weights signal width selected well. implementation ﬁxed ﬁlter size chose smaller ﬁlters zero-padded larger ﬁlters decomposed multiple ﬁlters added keep cycles lost column changes also larger image chose sram technology libraries used provide fast enough module accommodate words mhz. therefore sram split modules words each. used synopsys design compiler synthesis cadence encounter back-end design. synthesis back-end design performed clock frequency typical case corners functional setup view best case corners functional hold view. clock trees synthesized fast slow clock maximum delay maximum skew scan testing different view looser constraints used. reset scan enable signals also inserted clock trees relaxed constraints. clock gating used. performed post-layout power estimation based switching activity information simulation running parts scene labeling convnet. total core power estimated used used lower frequency clock tree. unit uses ﬁlter bank image bank image window sram remaining power used power buffers connecting blocks buffers control logic. entire core power domain nominal voltage frame uses power used pads line termination silicon measurements asic named cmos origami taped-out technology. measurement results asic compiled table high-speed conﬁguration apply clock core achieving peak throughput gop/s. running scene labeling convnet achieve actual throughput gop/s core consumes amounts power efﬁciency gop/s/w measured respect peak throughput comparability related work. data interface consist input output data running half core frequency providing peak bandwidth mb/s full-duplex. achieve high throughput density gop/s/mm despite generously chosen core area logic on-chip memory occupy total area would correspond throughput density gop/s/mm. operating chip high-efﬁciency conﬁguration maximum clock speed without inducing errors mhz. throughput scaled accordingly gop/s peak performance gop/s running reference convnet. core’s power consumption reduced dramatically yielding power-efﬁciency gop/s/w. required bandwidth shrinks mb/s full-duplex mb/gop. throughput density amount gop/s/mm conﬁguration. chip originally targeted operating point hold violations operating room temperature. thus four origami chips require mb/s given implementation using input output feature sharing discussed section iv-d save factor inputs outputs read directly written directly memory. since chips output partial sums read memory added ﬁnal convolution result. combined activation pooling adding slightly less mb/s read mb/s write memory bandwidth. third stage scene labeling convnet pooling layer instead subsequent pixelwise classiﬁcation applied directly reduces feature maps yielding even lower memory write throughput requirement. require gb/s memory bandwidth processing. achieve maximum performance load ﬁlters full speed four chip independently beginning processing burst requiring memory bandwidth gb/s less computation. leaves enough bandwidth available prepost-processing memory access overhead. given conﬁguration frames processed frame/s accordingly higher resolution. analyzed measured multiple metrics architecture bandwidth throughput area power efﬁciency. chip operating points highspeed conﬁguration high-efﬁciency conﬁguration taped chip results actual silicon measurement results opposed post-layout results known quite inaccurate. proceed presenting implementation results silicon measurements. implementation show resulting ﬁnal area breakfigure ﬁlter bank accounts third area consists registers storing ﬁlter weights multiplexers switching units take almost another third circuit consist mostly logic pipeline registers rest space none previous work convnet accelerators silicon measurement results. thus compare postlayout post-synthesis results state-of-the-art related works although simulation results known optimistic. listed ﬁgures works table discuss various results sections below. chip area-efﬁcient convnet accelerator reported literature. measure area terms input nand gate equivalents compensate technology differences extent. gop/s/mge implementation area-efﬁcient even high power-efﬁciency conﬁguration outperform previous stateof-the-art results. next best implementation neuflow design gop/s/mge requiring factor space performance. shidiannao comparable areaefﬁciency gop/s/mge. also note chip size limited pad-frame area occupied standard cells on-chip sram would thus achieve throughput density enormous gop/s/mge optimistic scenario. would require complex expensive pad-frame architecture e.g. ﬂip-chip multiple rows pads decided implement. reason good results approach compute multiple input output channels parallel. buffer window input channels compute convolutions instead buffering input images signiﬁcant saving storage particularly also window size input images buffered larger size single convolution kernel. another attributed instead words expresses mostly size sram ﬁlter kernel buffer. max. clock frequency powerb peak throughput effective throughput core power-efﬁciency including sram blocks. power usage measured running real data measurement obtained forced ambient temperature resulting shmoo plot shown figure besides mentioned operating points many more allowing continuous trade-off throughput energy efﬁciency changing core supply voltage evaluated empirically figure expected ﬁgures slightly worse measurements higher temperature. static power dissipation takes share around across entire voltage range share different types power measurements abbreviate abbreviations values obtained convnet described static power makes around total power entire range feasible vcore increased energy efﬁciency titan signiﬁcant neither attributed solely technology software implementation number pins often congested resources designing chip ﬁght bandwidth even present valuable memory bandwidth shared accelerators. achieve bandwidth efﬁciency gop/gb providing improvement factor best previous work neuflow comes memory bandwidth gb/s provide gop/s i.e. perform gop/gb. shidiannao provide information external bandwidth hwce gop/gb. large differences particularly work previous results attributed focused reducing required bandwidth maximizing throughput small piece silicon. architecture designed maximize reuse input data calculating pixels multiple output channels parallel bringing signiﬁcant improvement caching accelerating individual convolutions however technology strong impact energy efﬁciency. design done neuflow using hwce even resorted fdsoi. order analyze architecture independently particular implementation technology used take look effect. take simple model projected results shown table obtain operating voltage technology scale operating voltage linearly respect common operating voltage used technology. projection although based accurate model gives idea various implementations perform recent technology. clearly competitive results terms core power efﬁciency shidiannao work. previous work always excluded power although major contributor overall power usage. estimate power based energy usage pj/bit reported lpddr memory model chip assuming reasonable output load high page rate. chip amounts additional work newer technologies programmable logic conﬁgurability build entire high-performance low-power system planned alongside investigations convnet learning-phase adapt networks very-low precision accelerators training. flow convolutional networks arxiv. revaud weinzaepfel harchaoui epicflow edgepreserving interpolation correspondences optical proc. ieee conf. comput. vis. pattern recognit. zbontar lecun computing stereo matching cost convolutional neural network proc. ieee conf. comput. vis. pattern recognit. high-performance high-throughput conﬁguration respectively. neuflow much higher bandwidth looks worse additional gop/s implementation. assume power efﬁciency devices original technology reduces power efﬁciency including gop/s/w gop/s/w gop/s/w chip high-throughput highefﬁciency conﬁguration well neuflow. look decreased gop/s/w gop/s/w gop/s/w. clearly shows importance reduced bandwidth design relevance power general making share total power consumption three devices. presented ﬁrst silicon measurement results convolutional network accelerator. developed architecture also ﬁrst scale multi-top/s performance signiﬁcantly improving external memory bottleneck previ yang vision chip based dynamically reconﬁgurable hybrid architecture comprising array self-organizing neural network proc. ieee int. conf. solidstate circuits farabet couprie najman learning hierarchical features scene labeling ieee trans. pattern anal. mach. intell. long shelhamer darrell fully convolutional networks semantic segmentation proc. ieee conf. comput. vis. pattern recognit. chen diannao small-footprint highthroughput accelerator ubiquitous machine-learning proc. int. conf. archit. support program. lang. oper. syst. park bong shin .tops/w scalable deep learning inference processor tetra-parallel mimd architecture big-data applications proc. ieee int. conf. solid-state circuits akopyan sawada cassidy truenorth design tool flow million neuron programmable neurosynaptic chip ieee trans. comput. des. integr. circuits syst. vol. gupta agrawal gopalakrishnan deep learning limited numerical precision proc. int. conf. mach. learn. vanhoucke senior improving speed neural networks cpus adv. neural inf. process. syst. work. lukas cavigelli received m.sc. degree electrical engineering information technology zurich zurich switzerland since integrated systems laboratory zurich pursuing ph.d. degree. current research interests include deep learning computer vision digital signal processing low-power integrated circuit design. cavigelli received best paper award ieee vlsi-soc conference. luca benini chair digital circuits systems zurich full professor university bologna. served chief architect platform/sthorm project stmicroelectronics grenoble. held visiting consulting researcher positions epfl imec hewlett-packard laboratories stanford university. benini’s research interests energy-efﬁcient system multi-core design. also active area energy-efﬁcient smart sensors sensor networks biomedical ambient intelligence applications. published papers peer-reviewed international journals conferences four books several book chapters. fellow ieee member academia europaea.", "year": 2015}