{"title": "Optimizing and Contrasting Recurrent Neural Network Architectures", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Recurrent Neural Networks (RNNs) have long been recognized for their potential to model complex time series. However, it remains to be determined what optimization techniques and recurrent architectures can be used to best realize this potential. The experiments presented take a deep look into Hessian free optimization, a powerful second order optimization method that has shown promising results, but still does not enjoy widespread use. This algorithm was used to train to a number of RNN architectures including standard RNNs, long short-term memory, multiplicative RNNs, and stacked RNNs on the task of character prediction. The insights from these experiments led to the creation of a new multiplicative LSTM hybrid architecture that outperformed both LSTM and multiplicative RNNs. When tested on a larger scale, multiplicative LSTM achieved character level modelling results competitive with the state of the art for RNNs using very different methodology.", "text": "recurrent neural networks long recognized potential model complex time series. however remains determined optimization techniques recurrent architectures used best realize potential. experiments presented take deep look hessian free optimization powerful second order optimization method shown promising results still enjoy widespread use. algorithm used train number architectures including standard rnns long short-term memory multiplicative rnns stacked rnns task character prediction. insights experiments creation multiplicative lstm hybrid architecture outperformed lstm multiplicative rnns. tested larger scale multiplicative lstm achieved character level modelling results competitive state rnns using different methodology. declare thesis composed myself work contained herein except explicitly stated otherwise text work submitted degree professional qualiﬁcation except speciﬁed. feedforward neural networks powerful method modelling static functions enough hidden units theory model function arbitrary accuracy however modelling sequences simple feedforward neural network must ﬁxed context past inputs make predictions. order large contexts make predictions feed forward neural networks must many inputs therefore must much larger. much efﬁcient powerful using long time contexts give feedforward neural networks recursive connections making recurrent neural networks rnns shown great potential complex non-linear sequences require long time range dependencies. however many difﬁculties associated training rnns make hard fully realize potential. additionally neural networks datasets become larger highly parallelizable hardware gpus improve becoming increasingly important training become efﬁcient parallelization currently used methods. reasons study takes deep look training recurrent neural networks hessian free optimization shown potential effective much efﬁcient parallelization commonly used training methods. many possible architectures appear expressive others. often architecture outperforms another difﬁcult know higher performing architecture truly expressive architecture simply underperformed ﬁtting algorithm powerful enough. reason study compares several different architectural ideas using hessian free optimization training thought much less prone under-ﬁtting commonly used methods architectures never trained hessian free optimization others novel architectures never used likely would practical without powerful optimizer. benchmarking models character level sequence modelling used task requires using highly non-linear transitions well long range dependencies successful. goals study compare different ways implementing hessian free optimization well expressiveness several architectures compare best combinations state results character modelling. recurrent neural networks powerful class sequence modellers recursive connections store information past. recursive nature hidden states rnns allows potentially unlimited context make predictions. theoretical potential recurrent neural networks solve sequence modelling tasks long known recently improvements optimization well computer hardware allowed rnns achieve successful results many supervised learning tasks. achievements include state results speech recognition language modelling handwriting recognition among many others. standard architecture consists input vector hidden vector output vector contains weights matrices connections input hidden hidden output recurrent hidden hidden contains vector hidden state biases additionally output units typically softmax function applied model normalized probability distribution output units hidden units non-linear squashing function hyperbolic tangent applied. operates sequence timesteps predicts using well passed context shown equations below. partial derivatives cross entropy error respect parameters network also known gradient direction steepest descent computed using chain rule back propagation time algorithm given standard following equations basic learning algorithm would iteratively update weights small negative factor gradient training algorithm known gradient descent. done single training cases sequences segmented ﬁxed lengths case stochastic gradient decent. alternatively training become much computationally efﬁcient using mini batches gradient computed several training cases simultaneously parallel. also possible full batch gradient descent gradients computed full dataset once. computationally efﬁcient generally impractical change error surfaces examples usually cannot accurately approximated straight line gradient descent attempts furthermore stochasticity training orders updates provides training advantages becomes longer possible. general training gradient descent result training difﬁculty known vanishing/exploding gradient problem gradient tends decay explode exponentially back-propagated time. seen problem arises considering matrix derivatives large matrix derivatives tend either explode decay exponentially product many matrices. result makes difﬁcult rnns learn long-term dependencies predictions. gradient vanishes updates weights help learning long time lags contribution gradient exponentially small gradient explodes learning becomes unstable. reason advanced architectures and/or learning algorithms usually needed train rnns difﬁcult problems. long short-term memory architecture designed address gradient based learning problems standard architecture addresses memory cells soft multiplicative gates control information ﬂow. original formulation lstm input gates control much memory cell’s total input written internal state output gates control much memory cell’s internal state output. memory cell self-recurrent weight preserving internal state. like standard recurrent neural network hidden state receives inputs input layer previous hidden state sometimes non-linear squashing function applied hin. input output gates also receive inputs typically sigmoid function squash input allowing soft differentiable gates. output gate controls much memory cells internal state output squashed non-linear function hyperbolic tangent. output gate allows lstm cell keep information relevant current output relevant later. linear connections successive internal states allow hidden states partially linear functions passed states whereas standard become highly non-linear functions past states quickly. additionally ∂hstate internal state self recurrent weight ∂hstate contains term protecting vanishing gradient problem. lstm’s original learning algorithm exploding gradients dealt type derivative truncation ∂hout approximated ∂hout thereby allowing error back-propagate though lstm’s internal state. including truncation assured ∂hstate approximated gradients would never vanish explode. ∂hstatetrunc also allowed learning occur completely online real-time making possible store ∂hstate trunc parameters network. would memory intensive standard recurrent neural network approximations made gradient force ∂hstate forget gates later added lstm control much memory cell’s previous internal state remembered place using self-recurrent weight forget gates receive input squashed non-linearly ∂hstate includes term equal instead gradients vanish usually slowly usually close forget gates proved helpful learning made lstm similar standard rnns. diagram information lstm memory cell including forget gates presented figure note diagram passing square represents going multiplicative gate edges simply represent direction information ﬂow. lstm ﬁrst invented tested number synthetic tasks previously unsolvable rnns designed benchmark time-lag capabilities. example tasks marked addition task receives boolean linear input every time step gives linear output sequence. boolean input twice every time step. must learn ignore linear inputs boolean ﬁnal time-step must output marked linear inputs boolean lstm able solve task even sequence time steps long meaning able learn store relevant inputs hundreds time-steps ignoring irrelevant inputs. signiﬁcant demonstrated architecture able information long time context. lstm also successful natural tasks achieving state results many supervising learning problems including handwriting recognition speech recognition language modelling character prediction commonly used strategy increase expressiveness architecture stack rnns sequentially forming sort deep recurrent neural network hybrid allows greater deal non-linear processing occur seeing input outputting output could also potentially allow store different time scales information layer. basic architecture stacked given figure drawback architecture gradients back-propagated feed forward layers well time magnifying exploding vanishing gradient problems. layer layer training similar commonly seen deep neural networks helpful training stacked rnns another commonly used strategy circumvent give stacked layers direct input output connections gives ﬂexibility using varying degrees non-linear processing predicting output intending make training easier. concept stacked rnns used architecture often found improve results multiplicative architecture invented allow varying transitions hidden states depending input accomplished matrix factorization allows weights hidden transition vary great degree depending input. hidden hidden weight matrix standard replaced factorization intermediate state effective hidden hidden weight matrix results multiplying different input instance signs effective hidden hidden weights positive inputs negative others. full equations architecture given below. multiplicative rnns especially difﬁcult train gradient descent high degree curvature error surface reason usually necessary order methods train architecture. challenging sequence modelling tasks character prediction entails generating probability distribution series text characters. start ﬁrst character string predict followed effectively generating probability distribution possible sequences characters. typically receive input predict time step. performance generally evaluated cross entropy unseen text units bits character computed representative theoretical minimum number bits character sequence text could compressed using probabilistic model. modelling language word level certainly easier model starts knowledge possible words modelling language character level presents potential sub-word segments determine word meaning well correctly handle punctuation. character level models challenging optimize push limits ﬁtting algorithms making good option benchmarking advanced optimization methods. several experiments performed using character level models. widely benchmarked corpora penn treebank corpus several variations wikipedia corpus. number different architectures tested corpora main results presented tables table experiments penn treebank corpus test error given bits/char mrnn stacked lstm using weight noise regularization dynamic evaluation stacked table experiments wikipedia corpus test error given bits/char mrnn using split million character subset training testing dataset mrnn using full billion character training million character subset testing dataset stacked using full billion character training million character subset testing dataset stacked lstm using split million character subset training testing included dataset stacked lstm using split million character subset training testing included dataset using stated previous chapter certain difﬁculties optimizing rnns exponentially exploding decaying gradients. sensible solution problem either directly indirectly second order information optimization process. increase step size directions curvature decrease step size directions high curvature allowing much ﬁner convergence. reason sensible deal vanishing exploding gradient problems second derivatives likely decay explode similar rate ﬁrst derivatives step sizes taken second order algorithm naturally amplify decaying gradients reduce exploding gradients appropriately. simpler methods indirectly second order information involve slight modiﬁcations ﬁrst order algorithms better control step size direction gradient. straightforward momentum designed accelerate convergence along valleys error surface using factor previous updates part current update. number ways momentum classical given following equations momentum amplify updates account this. another indirectly second order information found work especially well rnns gradient norm clipping. method norm gradient exceeds predeﬁned threshold gradient normalized norm equals threshold idea behind method gradient high likely result exploding gradient high second derivatives. therefore cases makes sense reduce updates account high curvature. first order methods indirectly account curvature work well curvature extreme. however certain cases modifying step size direction gradient enough good solution reasonable amount time. illustrate this consider multiplicative architecture previous chapter hidden hidden transitions given hidden hidden transition many weights multiplied together small change weight could greatly affect gradients many weights varying ways. phenomenon known pathological curvature problem neural networks especially severe mrnns. first order algorithms rely gradient direction constant enough possible make reasonably signiﬁcant step sizes direction gradient without gradient completely changing. however mrnn even taking small step size direction gradient cause direction steepest descent completely change making original gradient longer viable search direction. gradient descent could theory train mrnn small step sizes could take impractically large amount time. order deal this advisable second order methods account full curvature matrix computing updates. allows local error surface approximated quadratic bowl rather line allowing approximation valid much larger radius curvature high. vector parameters network error function parameters gradient hessian matrix. solving minimum taking derivative respect setting equal yields equation simple straightforward newton’s method would computationally feasible train large neural networks. network weights storing hessian would memory inverting would computation. large neural networks often millions weights making training method impractical. solution problem iterative truncated newton’s methods compute −a−b matrix vector using matrix vector products technique well studied optimization community recently introduced machine learning. case neural networks methods developed efﬁciently compute hessian-vector products exactly ever store methods generally would require iterations compute −a−b exactly dense matrix vector. however sparse rank generally case hessian neural networks −a−b approximated well fewer steps. however diagonal elements update directions partially oppose previous updates resulting slower convergence. prevent this conjugate gradient ensures search direction conjugate respect previous search direction adding additional term conjugacy would achieved setting search directions satisfy condition shown combining updates step sizes directions continually produce updates conjugate previous updates full conjugate gradient algorithm given below. note derivation conjugate gradient somedifferent from mathematically equivalent form typically presented. conjugate gradient method several advantages iterative methods solving linear systems including able make good initial guess requiring memory storage previous iterations. another important detail applying conjugate gradient neural networks deciding terminate. conjugate gradient algorithm theory could take iterations fully converge network parameters however curvature matrix almost always sparse and/or rank conjugate gradient methods applied neural networks limited iterations. additionally various heuristics used terminate conjugate gradient early progress optimizing quadratic approximation neural network error objective nearly stopped. commonly used heuristic terminate progress made optimizing quadratic iterations less tolerance iteration potential limitation conjugate gradient method curvature matrix must positive semi-deﬁnite else algorithm converge maximum saddle point rather minimum. reason approximation hessian matrix includes positive curvature known gauss-newton matrix used place hessian. note point refer hessian loss function references refer hidden state neural network. jacobian matrix derivatives loss function positive semi-deﬁnite long convex loss function used. efﬁcient method computing gauss-newton matrix vector products exactly neural network also derived achieved steps. multiply gauss newton matrix vector ﬁrst step compute equivalent directional derivative differential operator called r-operator derived compute directional derivatives neural networks follows rules differential operators. derived recurrent network compute using simple rules. notation shorthand notation refers directional rate change direction propagating weight matrices layer hout layer using product rule matrices algorithm computing approximately computational cost forward passes. next step multiply result hessian loss function. case loss function hessian simply identity matrix. case cross-entropy error softmax output units hessian time step either case hessian-vector product computed quickly compared steps computing long computed element wise multiplication instead actually computing matrix vector products would inefﬁcient matrix expressed vector. forced gauss-newton matrix place hessian could viewed drawback conjugate gradient even iterative solvers capable using full hessian found perform better gaussnewton matrix approximation negative contributions curvature could tend higher third derivatives lead less stable approximation. additionally obtaining gauss-newton matrix vector products neural networks requires half computation obtaining hessian-vector products order second order method work well training neural networks additional modiﬁcation needs made. second order taylor series approximation local approximation however conjugate gradient converge global minimum approximation usually outside region approximation accurate. technique called damping used prevent happening. simple perform damping known tikhonov damping works reasonably well feedforward neural networks constant diagonal elements gauss-newton matrix conservatively overestimating curvature larger updates prevented. method damping turns insufﬁcient training recurrent neural networks likely small changes weights still sometimes lead large changes network dynamics. another type damping called structural damping created cope problem structural damping creates penalty updates based changes make hidden state dynamics rather magnitude update. uses second quadratic approximation error change hidden states added initial quadratic optimization objective form objective. hidden state values given network parameters meta parameter controls weighting loss function. note previous experiments used tikhonov damping structural damping loss function structural damping used weight however thesis contains experiments structural damping only refer weight structural damping regardless also used. change hidden state also approximated second order taylor series. ﬁrst order term taylor series irrelevant case error objective second order term curvature change hidden states accounted gauss-newton matrix change hidden states computing curvature-vector products done applying r-forward algorithm back-propagating resulting values since steps quite similar steps computing original gauss-newton matrix-vector products weighted computed slight modiﬁcation backward pass step rnns parameter vector cause quadratic inaccurate kept control. using hessian free optimization structural damping standard rnns ﬁrst became capable solving synthetic time-lag problems previously solvable lstm another perspective damping ideal damping parameters vary great deal different points training even different updates conjugate gradient techniques exist adjusting parameters based heuristic information unlikely damping parameters adjusted quickly enough become optimal search direction. conjugate gradient method conjugacy search directions ensures direction optimizes quadratic independently. justiﬁes stepping search directions degree minimize loss neural network. alternative method damping separate updates made quadratic neural network parameters. total quadratic update optimized exactly step size computed minimize quadratic approximation along search direction iterations conjugate gradient line-search makes sense start backtrack since updating full direction along optimal search direction approximated quadratic almost always overestimate. line-search done randomly select training examples compute gradient compute forward pass compute using back propagation time select subset compute curvature matrix-vector products conjugate gradient algorithm using computing products using steps described compute dampened gauss newton matrix-vector products update conjugate gradient algorithm advantage using hessian free optimization ﬁrst order methods make much larger batch sizes training. ﬁrst order methods must rely many small updates converge accounting curvature allows hessian free optimization make much larger updates making suitable larger batch sizes. larger batch sizes allows much greater efﬁciency gpus multi-threaded cpus parallelization. levels parallelization used training neural networks. parallelization matrix multiplication level occur propagating state vectors forward backward passes neural network. instance forward pass units training examples +whhh step multiplied matrix multiplied matrix. larger greater amount local parallelization performed especially gpu. also possible divide computations completely separate threads. case computing gradients products results thread computing subset examples summed yield total values. attempting train long sequences large batch sizes memory requirements become prohibitive especially gpus often less manage memory efﬁciently. compute backward passes rnns previous hidden states values needed. computing hidden states reverse direction inferring results exponential loss precision therefore completely impractical. storing hidden state values memory requirement hidden state dimensionality number training examples sequence length. fortunately results doubling useful work around reduces seem inefﬁcient effectively compute forward pass twice time lost typically much less time gained able multiply much larger matrices opposed compute training examples sequentially smaller matrices. choosing batch size computing gradient batch size computing curvature matrix vector products important decisions balancing efﬁciency successful training hessian free optimization. batches small parallelization less efﬁcient optimizer over-ﬁt mini-batches slow training. mini-batches large neural network could lose advantages stochasticity examples used help prevent settling local optimum. general atleast several times larger curvature batch subset gradient batch. reasoning larger gradient needs computed every conjugate gradient conjugate generally many iterations require curvature matrix vector products computed. sensible accurate approximation gradient must computed less frequently. terms general batch sizes optimal number task speciﬁc. character level modelling past work shown computing gradient million characters curvature hundred thousand characters works well would entail splitting characters sequences length ranges. general experimentation needed determine optimal batch sizes given task previous work give good indications approximately start with. initial experiments carried intended benchmarked comparison different methodologies character level modelling. experiments subset penn treebank corpus containing ﬁrst million characters training approximately validation testing. experiments used penn treebank corpus form preprocessing numbers rare words included. smaller subset used allow models trained reasonable amount time cpus. architectures designed around thousand trainable parameters including weights biases. runs training stopped validation error stopped improving. experiments carried using custom-written matlab code. experiments machine quad core opteron processors using threads paralellization possible multithreading performed using matlab’s parallel computing toolbox. ﬁrst experiments compared original structural damping method rnns mrnns line-search damping. second experiments compared lstm trained hessian free optimization original order online learning algorithm lstm. next experiments compared stacked mrnn trained hessian free optimization earlier results. last experiment chapter tested novel multiplicative lstm hybrid architecture trained hessian free optimization terminate conjugate gradient quadratic approximation longer accurate termination occurred line-search damping only conjugate gradient also terminated line-search failed make progress times. line-search make progress decays search direction predeﬁned maximum number iterations fails improve upon error network update. lastly partially efﬁciency purposes conjugate gradient limited maximum iterations. conjugate gradient backtracking line-search used step size minimized error training set. capital letters well commonly used symbols dollar sign left mistake mapped single character. ﬁxed full-scale experiments signiﬁcant impact comparisons chapter characters missing. structural damping penalizes updates much change hidden state expected result proven viable option second order training rnns. however alternative method named augmented hessian free optimization suggested robust however experiment provided demonstrate this controller trained method resulted slower initial convergence slightly better ﬁnal outcome structural damping. limited research method experiments comparing similar approach carried here. exact implementation details method spared original paper difﬁcult know implementation identical least inspired insights. experiments carried comparing structural damping line-search damping standard rnns multiplicative rnns. standard hidden units mrnn hidden units. sparse initialization hidden hidden matrix standard used designed similar used past experiments hessian free optimization parameters initialized probability initialized independently normal distribution mean standard deviation probability input output weights initialized normal distribution mean standard deviation multiplicative weights initialized mean standard deviation iteration training gradient computed full dataset split sequences length random selection sequences totalling fourth dataset used compute gauss-newton matrix-vector products conjugate gradient runs. another result demonstrated multiplicative performs significantly better standard regardless damping. consistent past results thought multiplicatives rnn’s ability alter transition matrix function input gives advantage ﬁtting character level models. overall results showed line-search damping viable alternative structural damping training rnns hessian free optimization. also provided baseline values experiments. applied rnns questioned whether lstm architecture actually expressive natural tasks achieved success easier train without advanced optimization methods authors study compared lstm trained stochastic gradient descent classical momentum standard similar number parameters trained hessian free optimization found standard achieved superior performance variety natural tasks including bouncing balls video prediction speech prediction music prediction however learning algorithm lstm training problems exploding gradients lstms still vulnerable successful lstm learning algorithms typically derivative clipping protect authors paper known gradient clipping methods time provided fairer comparison also hessian free optimization train lstms. knowledge published result ever used hessian free optimization train lstm. would allow fairer comparison standard rnns also trained algorithm also could provide alternative train lstms large datasets increased efﬁciency paralellization. step needs change train lstm hessian free optimization algorithm multiplying gauss-newton matrix-vector products. recalling previous chapter gauss newton matrix-vector product expressed computing requires applying r-forward algorithm lstm derived easily using basic rules given chapter equations r-forward algorithm lstm given section appendix. multiplying result r-forward algorithm exactly described previous chapter nature lstm output units same. multiplying simply back-propagation time algorithm lstm applied result ﬁrst steps. described equations section appendix also used compute gradient lstm. experiments carried identical lstm architectures used hessian free optimization another used original lstm online learning algorithm described section lstm. original lstm online learning algorithm gives updates similar stochastic gradient descent prevents exploding gradients error truncation. still used online dynamic adaptation character level models weights experiments initialized randomly mean standard deviation results experiments given table along standard baseline previous section. note total training time lstm trained hessian free optimization order magnitude less utilization multithreading possible online lstm learning algorithm. interestingly lstm trained hessian free optimization achieved superior performance original lstm learning algorithm well standard trained hessian free optimization. hessian free optimization able deal exploding gradients still full gradient likely gives advantage online lstm learning algorithm error truncation. lstm’s superior performance standard suggests atleast speciﬁc task character prediction lstm likely expressive architecture standard rnn. couple possible reasons this. obvious gated internal states lstm help longer time context making predictions important modelling language character level. another possibility similarly mrnn lstm able non-linearity transitions depending input character input gates forget gates. acter prediction results however previous papers mrnns never attempted stack them. mrnns able highly non-linear transitions sense already make deep making deep completely different could certainly increase expressiveness. reason stacked mrnn trained data comparison shallow mrnn. make trainer easier layers stacked mrnn given direct input output connections. stacked mrnn total layers units distributed ﬁrst layer last layer notation refers layer wmli input connection matrix multiplicative intermediate state layer wmlh hidden connection matrix multiplicative intermediate state layer whli input hidden connection matrix hidden layer wolh output connection matrix hidden layer whlm multiplicative hidden connection matrix layer whlh input hidden layer exists bias terms hidden layer total number stacked hidden layers. stacked trained hessian free optimization weights initialized randomly mean standard deviation results stacked mrnn given table original mrnn section also given comparison. interestingly performance stacked mrnn seem differ much original mrnn. stacking architectures often leads improvement past experiment used stacked rnns character prediction corpus experienced almost improvement standard rnns despite stacked signiﬁcantly greater number parameters possible penn treebank corpus long enough passages enough variation words stacked mrnns provide signiﬁcant advantage. also case stacked mrnns would easier regularize however tested here. using multiplicative hidden hidden weights using lstm cells individually resulted improvements performance. course ideas mutually exclusive help rnns perform better different reasons. factorized intermediate states mrnns allows ﬂexible hidden hidden transition depending input. input gates forget gates give lstms ﬂexibility transition nowhere near ﬂexibility mrnns. lstms hand advantage storing information undisturbed longer periods time. could useful ability mrnns have seeing information could difﬁcult store long periods time highly complex transitions. reasons novel multiplicative lstm hybrid architecture created tested data set. mlstm factorized intermediate state dimensionality hidden state. intermediate state regular mrnn. mlstm able improve upon results lstms mrnns alone suggesting advantages combining architectures. seems combination able highly input dependant transitions also able store protected hidden states persist transitions works reasonably well task. several general conclusions made results chapter. linesearch damping provides alternative structural damping leads slower initial converge potentially slightly better generalization. lstm cells seemed provide advantage standard units advantage realized fully lstm also trained hessian free optimization. additionally multiplicative hidden weights provided advantage standard hidden weights. combining lstm cells multiplicative hidden weights novel multiplicative lstm architecture best overall results. overview experiments chapter presented table train error error test error number parameters architecture rnn/structural rnn/line-search mrnn/structural mrnn/line-search lstm online lstm hessian free stacked mrnn mlstm moving forward mlstm successful model earlier experiments tested full penn treebank dataset. penn treebank corpus divided subsections labelled standard procedure sections used training used validation used testing. total training consists approximately million characters. like previous experiments unprocessed corpus used. models trained different numbers parameters. cases models trained hessian free optimization structural damping weights initialized randomly mean standard deviation random sets million characters used compute gradients random subsets thousand characters used conjugate gradient runs. training sets split sequences length experiments machine containing geforce nvidia tesla gpus utilized using matlab’s parallel computing toolbox. speciﬁc details training procedure essentially previous chapter. results experiment presented table experiments originally intended comparable results literature turned signiﬁcant amount preprocessing applied penn treebank corpus used past character level models made task much easier preprocessing limited vocabulary unique words mapping rare words names token mapped numbers capital letter lower-cased words removed punctuation. preprocessing steps make text considerably predictable especially names numbers occur frequently corpus among hardest segments predict. surprisingly results literature indicate lower cross entropy obtained here. instance mrnn trained hessian free optimization test cross entropy error bits/char preprocessed corpus certainly seems plausible model tested would able atleast well preprocessed corpus follow experiments using preprocessed corpus would needed directly compare model models literature. general results indicate over-ﬁtting signiﬁcant problem. strong evidence larger model performed worse despite much greater ﬁtting capability. part reason corpus relatively small model over-ﬁt speciﬁc numbers names rare words training data. since hessian free optimization powerful ﬁtting method mlstm expressive model regularization methods need developed make combined system. another benchmark character level models text generated sampling model. samples taken starting short string text give context predict probability distribution next character. next character chosen probabilistically according output chosen character used next input rnn. repeatedly generate samples text. penn treebank corpus passages start string start. samples string start. given generated samples initial context. character samples taken model parameters model million parameters. results given below. text sometimes makes sense words rarely meaning beyond that. since character level model also sometimes invents words plausible others. probabilistic sample output sometimes character fairly unlikely model happened chosen chance. makes difﬁcult words real words text meaning beyond short context. overall though clear model learn fair amount words sentence structure punctuation capitalization considering built knowledge this. also clear model learned much memorization. instance model produced perfectly valid name michael miller even though michael miller never occurred together training set. additionally produced name connie hifts also sounds relatively plausible. token hifts training happens real surname. addition names words model produced never occurred training set. instance word corporators valid english word. model produced word even though examples corporators corporator training set. furthermore model properly used corporators like noun ability model plausible rarely never seen words potential advantage another experiment carried subset wikipedia corpus sometimes used test larger wikipedia corpus originally used compression competition known hutter prize compared penn treebank corpus wikipedia corpus unique words punctuation. also includes sections mostly must learn model english text mark language. experiments designed similar past study used -layer stacked lstm million parameters past study used ﬁrst million characters training ﬁnal million characters validation. present experiment used mlstm million trainable parameters using approximately million characters training million validation million testing. slight difference dataset used dataset past study present study around characters lost dataset recognized matlab. however even unrecognised characters encoded full byte each cross entropy error would increased maximum bits character. certain combinations rare characters often clumped together likely model could much better this. experiments carried using structural damping another using line-search damping. structural damping increased tikhonov damping also used. reason limit unstable updates. training broken million character subsets. training iteration gradient computed full million character subset conjugate gradient computed random tenth subset’s training examples. batches split sequences length structural damping experiment geforce line-search damping experiment geforce nvidia tesla gpus utilized using matlab’s parallel computing toolbox. save memory gpus essential hidden state recomputing strategy presented results experiments given table results indicate similar performance -layer stacked lstm previously trained dataset achieved validation error bits/char previous result considered state rnns static weights although author able achieve much better result online weight adaptation occurred test time attempted experiment. several major differences methodology past study present study. first past study used minibatch stochastic gradient descent derivative clipping learning algorithm present study used hessian free optimization. hessian free optimization potential computationally efﬁcient encouraging able obtain result competitive state ﬁrst order training method. secondly past study used layers stacking improve expressiveness lstm present study used stacking multiplicative connections. assume author past study believed stacking quite important performing well dataset layers quite deep. interesting similar performance achieved using stacking multiplicative connections gain advantage expressiveness. possible utilization stacking multiplicative connections could lead improvements dataset. clear samples models learned many structural regularities english. impressive feats line-search damping model able nest realistic looking address inside contributor correct syntax spanning characters. text generated model similar still distinguishable text real corpus. model’s ability close parentheses braces brackets long time context. particular corpus common occurrence training set. given context relatively quickly mean capable using context inﬂuence predictions longer. experiment measure time-lag capabilities model model given initial context model predicts high since training data opening bracket. ratio measured time steps generates alphabetic text long context used. avoid make assumptions baseline value control experiments also carried beginning context ’th’. ratios according model measured time steps. applied ratio data smoothed taking means sets time steps leaving data points. data experimental context control context averaged across trials. results figure indicate ratio much higher experiments demonstrated expansion hessian free optimization several architectures larger rnns. results indicate methods able produce models competitive state models character prediction. future work needed develop regularization methods improve generalization rnns trained methodology. graves schmidhuber ofﬂine handwriting recognition multidimensional recurrent neural networks. advances neural information processing systems pages hochreiter bengio frasconi schmidhuber gradient recurrent nets difﬁculty learning long-term dependencies. kremer kolen editors field guide dynamical recurrent neural networks. ieee press.", "year": 2015}