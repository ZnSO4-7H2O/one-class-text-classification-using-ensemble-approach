{"title": "Policy Improvement for POMDPs Using Normalized Importance Sampling", "tag": ["cs.AI", "cs.LG"], "abstract": "We present a new method for estimating the expected return of a POMDP from experience. The method does not assume any knowledge of the POMDP and allows the experience to be gathered from an arbitrary sequence of policies. The return is estimated for any new policy of the POMDP. We motivate the estimator from function-approximation and importance sampling points-of-view and derive its theoretical properties. Although the estimator is biased, it has low variance and the bias is often irrelevant when the estimator is used for pair-wise comparisons. We conclude by extending the estimator to policies with memory and compare its performance in a greedy search algorithm to REINFORCE algorithms showing an order of magnitude reduction in the number of trials required.", "text": "present method estimating expected return pomdp experi­ ence. estimator assume knowledge pomdp estimate returns finite state controllers perience gathered arbitrary quences policies policy. motivate estima­ function-approximation impor­ tance sampling points-of-view derive bias variance. although estimator biased variance bias irrelevant pair-wise tending estimator policies compare performance search algorithm reinforce algo­ rithm showing order magnitude reduc­ tion number trials required. assume standard reinforcement agent interacts partially-obs sider situation agent accumulated data would like data select next. par­ ticular actions mapping observations interaction goal agent find policy mapping maximizes agent's return rewards experienced. presents method estimating return every policy simultaneously gathered executing fixed policy. consider case policies importance sampling studied junction reinforcement portance sampling estimate q-values mdps function data collected using single pol­ icy. uses importance sam­ pling pomdps modify reinforce algorithm ignores past tri­ als. considers estima­ tors similar ones developed prove theoretical them. paper differs previous work allows multiple sampling policies lems derives exact malized unnormalized portance sampling reactive controllers. next section develop estimators malized normalized). normalized lower unnormalized sulting better estimator comparisons. sec­ results simulated envi­ tion demonstrates ronments. conclude discussion improve estimator further. consider reactive policies repre­ sents history therefore states bser­ tuple offour sequences rewards state sequence algorithm theoretical available con­ sideration only. lastly return policies ratios necessary importance sampling actly ratios computable without knowing state sequence. particular history induced drawn according would like unbiased estimate return r��w';? although neither numerator denominator importance sampling ratio computed term ratio cancels leaving calculated. different statement fact shown fact exploited paper. different potentially drawn according different distribution data drawn independently identically importance data construct average them. quite high. par­ problem variance ticular sampled policies close target policy elements variance. variances high overwhelm total estimate. might estimate policy motivate estimator next section demonstrate importance sampling viewed terms function approximation. seeks estimate portance sampling general valuating mations derived data. particular foresight nearest-neighbor estimates. data point nearest must define size \"basin\" near sample particular sampling ampling space discrete number points closer sampled sampled point. continuous volume space closest definition cannot computed thus need proximate distribution pled. average points inversely proportional volume nearest point. instance sampled uniformly unit volume average sity points average volume nearest given point �-extending inversely take estimate sampling density importance normalized estimates normalize importance sampling mate obtain lower variance estimate cost adding bias. previous work used uni­ variety names including weighted weighted form sampling require knowledge sampling density. approximations timating need estimate den­ sity points estimate therefore average bution points results estimator action). return policies memory style finite-state time step agent reads makes choice along observation setting action take memory. policy expands form pick­ action memory state given obser­ splitting world dynamics agent namics. term involves pos­ sible memory sequences. noting probability tion sequence given observation sequence memory sequence unobserved. ariation hidden markov model input-output hmm. difference observation policy action policy respectively) value time step. making possible compute probability derivative figure empirical estimates means stan­ dard deviations estimates number data points. data collected point ecuting policy corresponding figure estimators poli­ means standard deviations eraged experiments. plots mean represent returns. well. unnormalized trials enough good estimate bias variance. unnormalized mean constant true return difference deviation unnormalized estimator relies heavily weighted unlikely offset common events) correspond dicative unnormalized estimates. such estimators esti­ policies memory. particular mator explicit knowledge working memory. direct contrast method adding memory action observation spaces running standard reinforcement algorithm agent must learn dynamics memory. explicit memory model learning duce correct action sequence uses memory state coordinating time steps. kloek dijk additionally normalized estimators sampling distribution distribution interested sizes. estimator chosen policies expectation experiences true expected return policy similarly estimator specific goal constructing choose good policy. involves compar­ estimates instead considering difference points words estimators expected returns policies. full version paper bias normalized return difference variance return differences decrease useful note same thus -rs. case estimator value good true return difference depend difference fro�n expected return sitjk depend difference zero. without knowledge underlying pomdp expect return arbitrary history closer arbitrarily cho­ sign true differ­ value ence returns overlined values less counterparts estimator estimator. figure compared estimates problem described note quantities number samples provided relative frequen­ remains fixed. cies sampling policies histo­ measures average distribution ries. measures second moments turn either estimators greedy learning algorithm. find policy agent maximizes value estimator hill­ climbing using previous reaches maximum. agent uses policy next trial. trial adds policy-history-return data repeats. hill-climbing many estimates varies greatly magnitude there­ fore found best direction gradient rection climb. particular conjugate gradient descent algorithm ratio line search figure diagram \"load-unload\" world nine states. horizontal positioning whether cart loaded. agent observes position cart cart loaded reaches left­ state reaches right-most loaded unloaded agent receives single unit reward. agent actions point move left moving left right leaves cart unmoved. trial begins left-most figure shows simple world policy described numbers true pected return function policy. figure pares normalized unnormal­ ized estimators greedy pol­ improvement algorithm random policy choices. feel example illustrative sons normalized problems tried. bias observed returns works well smooth space. estimator willing extrapolate unseen regions unnormalized causes greedy algorithm explore policy space whereas unnormalized gets trapped visited area greedy explo­ ration successfully function. although left-right problem nice simple. estimator load-unload performance actions must achieve reasonable depend history. give agent memory section results twenty independent policy parameters. reinforce also used attack similar problem trial begins fourth state left lasts time steps. right true expected return function policy world. figure comparison normalized shown plotted iterations left column estimator greedy policy improvement algorithm right column uniformly sampled policies. first shows returns function trial number. second shows path taken policy space estimators given sequence data random case. random sampling policies algorithm quickly maximizes return provides better estimate surface near maximum. force figure reinforce algorithm fre­ quently gets stuck local minima. graph shown best settings step size schedule bias term reinforce. best case rein­ force converges near optimal policy trials greedy algorithm norm ized estimate makes much better data. reuse experience need model memory therefore learn \"dynamics\" memory. runs converge optimal policy trials. trial took twice suboptimal move reinforce's memory model disadvantage requiring importance sampling algorithm's grades approximately think normalized estimator makes good data combined greedy algorithm produces quick learning. would like extend immediate ways. first bounds return provide error estimates timate. although formula variance estimator still need good estimate variance samples estimate would allow exploration gorithm. second fied.\" trials computing estimate given policy takes work. makes entire algorithm quadratic number trails. however similar estimate could achieved fewer points. remembering important trials would produce simpler estimate. finally must remem­ policies used trail. turn really depend policy agent wants execute; depends agent tually act. theory able forget policies data icy. policies unobserved state sequences. work remove dependence. at&t central research institute dustry eastman kodak company daimler-chrysler ital equipment corporation fund nippon telegraph telephone porate research report describes research done within cbcl department brain cognitive sciences mit. research sponsored grants contracts n--l- dms-. additional", "year": 2013}