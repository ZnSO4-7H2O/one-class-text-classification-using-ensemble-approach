{"title": "Transductive Rademacher Complexity and its Applications", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We develop a technique for deriving data-dependent error bounds for transductive learning algorithms based on transductive Rademacher complexity. Our technique is based on a novel general error bound for transduction in terms of transductive Rademacher complexity, together with a novel bounding technique for Rademacher averages for particular algorithms, in terms of their \"unlabeled-labeled\" representation. This technique is relevant to many advanced graph-based transductive algorithms and we demonstrate its effectiveness by deriving error bounds to three well known algorithms. Finally, we present a new PAC-Bayesian bound for mixtures of transductive algorithms based on our Rademacher bounds.", "text": "develop technique deriving data-dependent error bounds transductive learning algorithms based transductive rademacher complexity. technique based novel general error bound transduction terms transductive rademacher complexity together novel bounding technique rademacher averages particular algorithms terms unlabeled-labeled representation. technique relevant many advanced graph-based transductive algorithms demonstrate eﬀectiveness deriving error bounds three well known algorithms. finally present pac-bayesian bound mixtures transductive algorithms based rademacher bounds. alternative learning models utilize unlabeled data received considerable attention past years. prominent models semi-supervised transductive learning. main attraction models theoretical empirical evidence indicating often allow eﬃcient signiﬁcantly faster learning terms sample complexity. paper support theoretical evidence providing risk bounds number state-of-the-art transductive algorithms. bounds utilize labeled unlabeled examples much tighter bounds relying labeled examples alone. focus distribution-free transductive learning. setting given labeled training sample well unlabeled test sample. goal guess labels given test points accurately possible. rather generating general hypothesis capable predicting label point inductive learning advocated vapnik transduction solve easier problem transferring knowledge directly labeled points unlabeled ones. transductive learning already proposed brieﬂy studied thirty years vapnik chervonenkis lately empirically recognized transduction often facilitate eﬃcient accurate learning traditional supervised learning approach recognition motivated ﬂurry recent activity focusing transductive learning many algorithms many papers refer model semi-supervised learning. however setting semi-supervised learning diﬀerent transduction. semi-supervised learning learner given randomly drawn training consisting labeled unlabeled examples. goal learner generate hypothesis providing accurate predictions unseen examples. heuristics proposed. nevertheless issues identiﬁcation universally eﬀective learning principles transduction remain unresolved. statistical learning theory provides principled approach attacking questions study error bounds. example inductive learning bounds proven instrumental characterizing learning principles deriving practical algorithms paper consider classiﬁcation setting transductive learning. several general error bounds transductive classiﬁcation developed vapnik blum langford derbeko el-yaniv meir el-yaniv pechyony continue fruitful line research develop technique deriving explicit data-dependent error bounds. bounds less tight implicit ones developed vapnik blum langford. however explicit bounds potentially used model selection guide development learning algorithms. technique consists parts. ﬁrst part develop novel general error bound transduction terms transductive rademacher complexity. bound syntactically similar known inductive rademacher bounds fundamentally diﬀerent sense transductive rademacher complexity computed respect hypothesis space chosen observing unlabeled training test examples. opportunity unavailable inductive setting hypothesis space must ﬁxed example observed. second part bounding technique generic method bounding rademacher complexity transductive algorithms based unlabeled-labeled representation representation soft-classiﬁcation vector generated algorithm product matrix depends unlabeled data vector depend given information including labeled training set. transductive algorithm inﬁnite number ulrs including trivial identity matrix. show many state-of-the-art algorithms non-trivial leading non-trivial error bounds. based representation bound rademacher complexity transductive algorithms terms spectrum matrix ulr. bound justiﬁes spectral transformations developed chapelle weston sch¨olkopf joachims johnson zhang commonly done improve performance transductive algorithms. instantiate rademacher complexity bound consistency method zhou spectral graph transducer algorithm joachims tikhonov regularization algorithm belkin matveeva niyogi bounds obtained algorithms explicit easily computed. also show simple monte-carlo scheme bounding rademacher complexity transductive algorithm using ulr. demonstrate eﬃcacy scheme consistency method zhou ﬁnal contribution pac-bayesian bound transductive mixture algorithms. result stated theorem obtained consequence theorem using techniques meir zhang result motivates ensemble methods transduction explored setting. rademacher complexity. section develop novel concentration inequality functions partitions ﬁnite points. inequality transductive rademacher complexity used section derive uniform risk bound depends transductive rademacher complexity. section introduce generic method bounding rademacher complexity transductive algorithm using unlabeled-labeled representation. section exemplify technique obtain explicit risk bounds several known transductive algorithms. finally section instantiate risk bound transductive mixture algorithms. discuss directions future research section technical proofs results presented appendices a-i. vapnik presented ﬁrst general loss bounds transductive classiﬁcation. bounds implicit sense tail probabilities speciﬁed bound outcome computational routine. vapnik’s bounds reﬁned include prior beliefs noted derbeko similar implicit somewhat tighter bounds developed blum langford loss case. explicit pac-bayesian transductive bounds bounded loss function presented derbeko catoni audibert developed pac-bayesian dimensionbased risk bounds special case size test multiple size training set. unlike pac-bayesian bound published transductive pacbayesian bounds hold deterministic hypotheses gibbs classiﬁers. bounds balcan blum semi-supervised learning also hold transductive setting making conceptually similar transductive pac-bayesian bounds. general error bounds based stability developed el-yaniv pechyony eﬀective applications general bounds mentioned particular algorithms learning principles automatic. case pac-bayesian bounds several successful applications presented terms appropriate priors promote various structural properties data ad-hoc bounds particular algorithms developed belkin johnson zhang unlike bounds bound johnson zhang depend empirical error properties hypothesis space. size training test increases bound converges zero. thus bound johnson zhang eﬀectively proves consistency transductive algorithms consider. however bound holds hyperparameters algorithms chosen w.r.t. unknown test labels. hence bound johnson zhang cannot computed explicitly. error bounds based rademacher complexity introduced koltchinskii well-established topic induction ﬁrst rademacher transductive risk bound presented lanckriet bound straightforward extension inductive paper distribution-free transductive model deﬁned vapnik consider ﬁxed sm+u points space together labels learner provided full-sample xm+u consisting points selected xm+u uniformly random among subsets size points together labels given learner training set. re-numbering points denote unlabeled training points {xm+ xm+u} xm+u called test set. unlabeled points learner’s goal predict labels test points based paper focuses binary learning problems labels {±}. learning algorithms consider generate soft classiﬁcation vectors rm+u soft conﬁdence-rated label example given hypothesis actual classiﬁcation algorithm outputs sgn). denote hout rm+u possible soft classiﬁcation vectors generated algorithm. based full-sample xm+u algorithm selects hypothesis space rm+u soft classiﬁcation hypotheses. note hout then given labels training points algorithm outputs hypothesis hout classiﬁcation. goal transductive learner hypothesis minimizing test error yi). work also margin loss function positive real min{ yy/γ} otherwise. empirical error yi). denote margin error test natural numbers throughout paper assume vectors column ones. mark vectors boldface. adapt inductive rademacher complexity transductive setting generalize also include neutral rademacher values. deﬁnition rm+u vector i.i.d. random variables i=m+ yi). notice inductive risk bounds standard deﬁnition rademacher complexity binary values used bound generalization error inductive analogue full sample error lm+u deﬁnition probability distribution suppose examples {xi}n sampled independently according class functions mapping {σi}n independent uniform {±}-valued random variables probability probability. empirical rademacher complexnotice transductive complexity empirical quantity depend underlying distribution including choices training set. since distribution-free transductive model unlabeled full sample training test points ﬁxed transductive rademacher complexity don’t need outer expectation appears inductive deﬁnition. also transductive complexity depends training test points whereas inductive complexity depends training points. following lemma whose proof appears appendix states rm+u monotone increasing proof based technique used proof lemma paper meir zhang lemma rm+u rm+u rm+u. forthcoming results utilize transductive rademacher complexity rm+u. lemma bounds also apply rm+u since rademacher complexity involved results strictly smaller standard inductive rademacher complexity deﬁned xm+u. also transduction approaches induction namely ﬁxed section develop novel concentration inequality functions partitions compare several known ones. concentration inequality utilized derivation forthcoming risk bound. random permutation vector variable chosen uniformly random. perturbed permutation vector obtained exchanging values function permutations called -permutation symmetric section present novel concentration inequality -permutation symmetric functions. note -permutation symmetric function essentially function partition items sets sizes thus forthcoming inequalities lemmas stated -permutation symmetric functions also hold exactly form functions partitions. conceptually convenient view results concentration inequalities functions partitions. however technical point view convenient consider -permutation symmetric functions. right hand side approximately similar less tight inequality obtained reduction draw random permutation draw independent random variables application bounded diﬀerence inequality mcdiarmid right hand side approximately inequality explicit version vapnik’s absolute bound note using unable obtain explicit version vapnik’s relative bound derivation uniform concentration inequality vectors inequality depends rademacher complexity set. substituting vectors values loss functions obtain error bound depending rademacher complexity values loss function. step done section order bound rademacher complexity terms properties hypothesis space rademacher complexity ‘translated’ using contraction property domain loss function values domain soft hypotheses hypothesis space. step done section show sections adaptation steps transductive setting immediate involves several novel ideas. section combine results steps obtain transductive rademacher risk bound. also provide thorough comparison risk bound corresponding inductive bound. follow three steps induction establishment steps achieved using inductive techniques. throughout section performing derivation step transductive context discuss diﬀerences inductive counterpart. vector permuted according following abbreviations averages subsets components hk{v} i=k+ special case h{v} tm{v}. uniform concentration inequality develop shortly states probability least random permutation remark derivation ghost sample permutation elements drawn distribution inductive rademacher-based risk bounds ghost sample training size independently drawn original one. note transductive setting ghost sample corresponds independent draw training/test partition equivalent independent draw random permutation remark principle could avoid introduction ghost sample consider elements h{v} ghosts elements t{v}. approach would lead deﬁnition rademacher averages probability u/). deﬁnition obtain corollary however since distribution alternative rademacher averages symmetric around zero technically know prove lemma remark induction step performed using application mcdiarmid’s bounded diﬀerence inequality cannot apply inequality setting since function supremum function independent variables rather permutations. lemma replaces bounded diﬀerence inequality step. proof proof based ideas proof lemma bartlett mendelson technical convenience following deﬁnition pairwise rademacher variables. deﬁnition rm+u. vectors rm+u. {˜σi}m+u vector i.i.d. random variables deﬁned split rademacher variable split randomly ﬁrst component indicates component ﬁrst elements last elements former case value second component meaning ﬁrst replaced exactly coeﬃcients appearing inside t{v} t{v} h{v} h{v} coeﬃcients random distribution induced uniform distribution permutations. course proof establish precise relation distribution coeﬃcients distribution pairwise rademacher variables. remark technique bound expectation supremum complicated technique koltchinskii panchenko commonly used induction. caused structure function supremum conceptual point view step utilizes novel deﬁnition transductive rademacher complexity. section instantiate inequality obtain ﬁrst risk bound. idea apply theorem appropriate instantiation t{v} correspond test error h{v} empirical error. true labeling full-sample hout deﬁne hout}. thus vector values loss full sample examples transductive algorithm operated training/test partition. possible vectors possible training/test partitions. apply theorem bmax obtain following corollary defer analysis slack terms bmaxcq section bound obtained straightforward application concentration inequality convenient deal with. that’s clear bound rademacher complexity rm+u loss values terms properties transductive algorithm. next sections eliminate deﬁciency utilizing margin loss function. following lemma version well-known ‘contraction principle’ theory rademacher averages lemma adaptation accommodates transductive rademacher variables lemma meir zhang proof provided appendix risk bound comparison related results applying theorem obtain theorem hout full-sample soft labelings algorithm generated operating possible training/test partitions. choice hout depend full-sample xm+u. ﬁxed probability least choice training xm+u hout rm+u large enough values value close therefore slack term convergence rate slow small slow rate small surprising latter case somewhat surprising. however note mean elements drawn elements large variance. hence case high-conﬁdence interval estimation large. conﬁdence interval reﬂected slack term compare bound rademacher-based inductive risk bounds. following variant rademacher-based inductive risk bound meir zhang theorem probability distribution suppose examples sampled i.i.d. according class functions empirical rademacher complexity respectively generalization error empirical margin error probability least random draw slack term bound order bounds quantitatively comparable. inductive bound holds high probability random selection examples distribution bound average error examples transductive bound holds high probability random selection training/test partition. bound test error hypothesis particular points. kind meaningful comparison obtained follows. using given full sample xm+u deﬁne corresponding inductive distribution dtrans uniform distribution xm+u; training size generated sampling xm+u times replacements. given inductive hypothesis space function deﬁne transductive hypothesis space projection full sample xm+u rm+u deﬁnition lm+u. alternatively compare could express bound bound error bound holds setting random draws replacement. setting number unique training examples smaller thus number remaining test examples larger hence draw training examples replacement imply draw subset test examples transductive setting. thus cannot express bound bound randomly drawn note inductive bound sampling training done replacement transductive bound done without replacement. thus inductive case actual number distinct training examples smaller bounds consist three terms empirical error term term depending rademacher complexity slack term third fourth summands empirical error terms bounds. hard compare analytically rademacher complexity terms. inductive bound derived setting sampling replacement transductive bound derived setting sampling without replacement. thus transductive rademacher complexity example xm+u appears rm+u multiplied contrast sampling replacement inductive rademacher multiplied diﬀerent nevertheless transduction full control rademacher complexity choose hypothesis space hout arbitrarily small rademacher complexity. induction choose large. thus provisions argue transductive rademacher term larger inductive counterpart. refer unlabeled-labeled representation section develop bounds rademacher complexity algorithms based ulrs. note transductive algorithm trivial example taking setting regime occurs following class applications. given large library tagged vanilla matrix simply speciﬁes given labels point view vanilla trivial encode ﬁnal classiﬁcation algorithm. example algorithm zhou straightforwardly admits vanilla ulr. hand natural algorithms belkin niyogi vanilla type. algorithms necessarily obvious non-trivial ulrs. sections consider cases particular algorithms joachims belkin section present generic bound rademacher complexity transductive algorithm based ulr. section consider case matrix kernel matrix. case develop another bound transductive rademacher complexity. finally section present method computing high-conﬁdence estimate transductive rademacher complexity. present bound transductive rademacher complexity transductive algorithm based ulr. {λi}r singular values well-known fact kukfro frobenius norm suppose hout hout possible outputs algorithm operated possible training/test partitions using abbreviation full-sample xm+u. following proof idea lemma bartlett mendelson obtained using respectively cauchy-schwarz jensen inequalities. using bound conjunction theorem immediately data-dependent error bound algorithm computed derive upper bound maximal length possible values vector appearing ulr. notice vanilla section derive tight bound non-trivial ulrs joachims consistency method zhou bound syntactically similar form corresponding inductive rademacher bound kernel machines however noted above fundamental diﬀerence induction choice kernel must data-independent sense must selected training examples observed. transductive setting hout selected unlabeled full-sample observed. rademacher bound well forthcoming rademacher bound depend spectrum matrix section non-trivial ulrs transductive algorithms spectrum depends spectrum laplacian graph used algorithm. thus transforming spectrum laplacian control rademacher complexity hypothesis class. exists strong empirical evidence spectral transformations improve performance transductive algorithms. lemma rm+u vector depending transductive algorithm generating soft-classiﬁcation vector {λi}r singular values upper bound kαk. algorithm bound conjunction bound vacuous; namely generated holds matrix kernel matrix decomposition kernel-ulr. rm+u reproducing kernel hilbert space corresponding denote h··ig inner product since kernel matrix reproducing property uig. means transductive algorithm kernel-ulr bound rademacher complexity. kernel bound tighter non-kernel counterpart kernel matrix eigenvalues larger and/or section derive tight bound non-trivial ulrs consistency method zhou tikhonov regularization method belkin show compute monte-carlo rademacher bounds high conﬁdence transductive algorithm using ulr. empirical examination bounds shows tighter analytical bounds technique based simple application hoeﬀding’s inequality made particularly simple vanilla ulrs. rm+u rademacher vector supv∈v deﬁnition rm+u eσ{g}. i.i.d. sample rademacher vectors. estimate rm+u high conﬁdence applying hoeﬀding inequality apply hoeﬀding inequality need bound supσ derived case hout. namely assume possible outputs algorithm speciﬁcally suppose output algorithm assume bound value supαkαk≤µ computed randomly drawn computation algorithm-dependent section show compute algorithm zhou cases compute supremum exactly also lower bound using symmetric hoeﬀding inequality. section exemplify rademacher bounds particular transductive algorithms. section instantiate generic bound algorithm joachims section instantiate kernel-ulr bound algorithm belkin finally section instantiate three bounds algorithm zhou compare resulting bounds numerically. vectors found singular value decomposition application approach induction seems hard impossible. example case kernel machines need optimize inﬁnite-dimensional vectors feature space. xm+u. entry represents similarity matrix constructed various ways example k-nearest neighbors graph. graph vertex represents example full sample xm+u. edge pair vertices corresponding examples among similar examples other. weights edges proportional similarity adjacent vertices examples commonly used measures similarity cosine similarity kernel. diagonal matrix whose entry unnormalized laplacian eigenvectors eigenvalues ivvt vector speciﬁes given labels labeled points otherwise. ﬁxed constant vector whose entries diagonal matrix example training soft classiﬁcation produced algorithm solution following optimization problem shown joachims matrix whose columns vi’s vector. depends training test sets matrix depends unlabeled full-sample. substituting second constraint using orthonormality columns hence take conclude following bound transductive rademacher complexity number non-zero eigenvalues notice bound oblivious magnitude eigenvalues. small value bound small shown joachims test error bad. increases bound increases test error improves. joachims shows empirically smallest value achieving nearly optimal test error deﬁning rkhs induced graph laplacian done herbster pontil wainer applying generalized representer theorem sch¨olkopf herbrich smola show algorithm belkin kernel-ulr. based kernel-ulr derive explicit risk bound this. also derive explicit risk bound based generic ulr. show former bound tighter latter one. finally compare kernel bound risk bound belkin proofs lemmas section appear appendix diﬀerence constraint change resulting hard classiﬁcation. belkin developed stability-based error bound algorithm based connected graph. analysis follows also assume underlying graph connected shown section argument also extended unconnected graphs. represent full-sample labeling vector reproducing kernel hilbert space associated graph laplacian derive transductive version generalized representer theorem sch¨olkopf considering rm+u}. soft classiﬁcation vectors. deﬁne inner product denote along inner product λm+u eigenvalues increasing order. since laplacian connected graph eigenvector corresponding since symmetric vectors {ui}m+u orthogonal. assume also w.l.o.g. vectors {ui}m+u consequence lemma algorithm performs regularization rkhs regularization term following transductive variant generalized representer theorem sch¨olkopf concludes derivation kernel-ulr algorithm belkin lemma solution optimization problem deﬁned above. then exists rm+u remark consider case unconnected graph. number connected components underlying graph. zero eigenvalue laplacian multiplicity eigenvectors corresponding zero eigenvalue um+u eigenvectors corresponding non-zero eigenvalues brieﬂy compare bound risk bound algorithm given belkin belkin provide following bound algorithm. probability least random draw training examples xm+u start brief description consistency method algorithm zhou algorithm natural vanilla matrix computed follows. matrices d−/w parameter then output speciﬁes given following lemma proven appendix provides follows lemma positive deﬁnite matrix hence also kernel matrix. therefore decomposition kernel-ulr. apply kernel rayleigh-ritz theorem compute bound johnson obtain obtained turns exact value supremum analytically derived. recall vectors induce hypothesis space particular exactly components values {±}; rest components zeros. possible α’s. largest elements |t|. derivation holds vanilla ulr. demonstrate rademacher bounds discussed paper present empirical comparison bounds datasets repository. dataset took size dataset took full-sample size. matrix -nearest neighbors graph computed cosine similarity metric. applied algorithm monte-carlo bounds computed compared upper lower mote-carlo bounds generic bound kernel-ulr bound graphs figure compare four bounds datasets function number non-zero eigenvalues speciﬁcally point x-axis corresponds bounds computed matrix approximates using smallest eigenvalues examples lower upper monte-carlo bounds tightly sandwich true rademacher complexity. striking generic-ulr bound close true rademacher complexity. principle simple monte-carlo method approximate true rademacher complexity desired accuracy cost drawing suﬃciently many rademacher vectors. ﬁnite base-hypotheses. class formed observing full-sample xm+u obtaining training/test partition labels. r|b| probability vector i.e. |b|. vector computed observing training/test partition training labels. goal posterior vector mixture section derive uniform risk bound q’s. bound depends kl-divergence prior probability vector r|b| vector deﬁned based unlabeled full-sample. thus forthcoming bound belongs family pac-bayesian bounds depend prior posterior information. notice bound diﬀerent pac-bayesian bounds gibbs classiﬁers bound eh∼blu j=m+ eh∼b‘ random draw base hypothesis according distribution hence risk bound transductive gibbs classiﬁer holds true also transductive mixture classiﬁer. currently known risk bound transductive gibbs classiﬁers diverges forthcoming risk bound deﬁciency. instantiate corollary kl-divergence derive pac-bayesian bound. kl-divergence adopting lemma meir zhang transductive rademacher variables deﬁned obtain following bound. theorem prior posterior distribution respectively. then probability theorem pac-bayesian result prior depend xm+u posterior optimized adaptively based also general bound bound convergence rate bound syntactically similar inductive pac-bayesian bound mixture hypothesis similar convergence rate however conceptual diﬀerence inductive transductive bounds transduction deﬁne prior vector observing unlabeled full-sample induction deﬁne observing data. studied rademacher complexity analysis transductive setting. results include ﬁrst general rademacher bound soft classiﬁcation algorithms unlabeled-labeled representation technique bounding rademacher complexity transductive algorithm bound bayesian mixtures. demonstrated usefulness results particular eﬀectiveness framework deriving error bounds several advanced transductive algorithms. would nice improve bounds using example local rademacher approach bartlett bousquet mendelson however believe main advantage transductive bounds possibility selecting hypothesis space based full-sample. clever data-dependent choice space provide suﬃcient ﬂexibility achieve training error rademacher complexity. opinion opportunity explored exploited much further. particular would interesting develop eﬃcient procedure choice hypothesis space learner knows properties underlying distribution work opens avenues future research. example would interesting optimize matrix explicitly constraint rademacher complexity. also would nice low-rademacher approximations particular matrices. pac-bayesian bound mixture algorithms motivates development transductive mixtures area investigated. finally would interesting utilize bounds model selection process. grateful anonymous reviewers insightful comments. also thank yair wiener nati srebro fruitful discussions. dmitry pechyony supported part programme european community pascal network excellence ist--. proof based technique used proof lemma paper meir zhang rademacher random variables rm+u rademacher random variables rm+u. real-valued function arbitrary function random variables. elemen thus obtain tary fact martingale function random variables. routine obtaining martingale arbitrary function called doob’s martingale process. deﬁnition consequently bound mean suﬃcient bound diﬀerence deviation algorithm randperm abridged version procedure drawing random permutation elements drawing non-identically distributed independent random variables presented section paper talagrand proof proof induction single random variable uniformly drawn among im+u therefore uniform distribution suppose claim holds possible values πππm consider -permutation symmetric function random permutations using algorithm randperm represent random permutation function independent random variables. value function output algorithm randperm operated values random draws given next lemma relates lipschitz constant function lipschitz constant another algorithm generating random permutation independent draws presented appendix lanckriet algorithm draws random permutation means drawing independent random variables. since deal -permutation symmetric functions interested ﬁrst elements random permutation. algorithm lanckriet needs draws independent random variables deﬁne elements. algorithm randperm presented section needs draws. algorithm lanckriet instead randperm forthcoming bound would term instead change turn would result non-convergent risk bound derived using techniques. induce rem} since -permutation symmetric value prove change results combined property refer respectively ‘old’ ‘new’ draws. consider operation randperm draws values respectively permutation iteration permutation iteration iteration randperm value remains intact. however values change. particular values among randperm. four cases values distribution respect take expectation induce distribution assignments coeﬃcients components realizations component assigned exactly coeﬃcients permutations pair coeﬃcients. pair takes values coeﬃcients ﬁrst component induced realization second component realization distribution vectors induced distribution uniform distribution partitions elements subsets elements respectively. clearly uniform distribution elements. distribution random vector ﬁrst elements pairs equivalent par. vector obtained taking ﬁrst indices realization assigning corresponding components. components assigned similarly distribution random vector equivalent par. therefore distribution entire vector equivalent product distribution uniform distribution elements element pair independent permutations. since independent ni’s distribution uniform possible rademacher assignments satisfying constraints easy support size support size moreover support sets distributions identical; hence distributions identical. therefore follows d−/w normalized laplacian eigenvalues i}m+u zero eigenvalues matrix since eigenvalues strictly positive. hence matrix invertible eigenvalues finally proof theorem require several deﬁnitions facts convex analysis function conjugate function deﬁned supx∈rn domain consists values value supremum ﬁnite. consequence deﬁnition so-called fenchel inequality inequality follows inequality obtained applying deﬁnition inequality obtained application jensen inequality inequality obtained applying times minimizing w.r.t. obtain", "year": 2014}