{"title": "A Kernel Method for the Two-Sample Problem", "tag": ["cs.LG", "cs.AI", "G.3; I.2.6"], "abstract": "We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.", "text": "propose framework analyzing comparing distributions allowing design statistical tests determine samples drawn diﬀerent distributions. test statistic largest diﬀerence expectations functions unit ball reproducing kernel hilbert space present tests based large deviation bounds test statistic third based asymptotic distribution statistic. test statistic computed quadratic time although eﬃcient linear time approximations available. several classical metrics distributions recovered function space used compute diﬀerence expectations allowed general apply two-sample tests variety problems including attribute matching databases using hungarian marriage method perform strongly. excellent performance also obtained comparing distributions graphs ﬁrst tests. address problem comparing samples probability distributions proposing statistical tests hypothesis distributions diﬀerent tests application variety areas. bioinformatics interest compare microarray data identical tissue types measured diﬀerent laboratories detect whether data analysed jointly whether diﬀerences experimental procedure caused systematic diﬀerences data distributions. equally interest comparisons microarray data diﬀerent tissue types either determine whether subtypes cancer treated statistically indistinguishable diagnosis perspective detect diﬀerences healthy cancerous tissue. database attribute matching desirable merge databases containing multiple ﬁelds known advance ﬁelds correspond ﬁelds matched maximising similarity distributions entries. test whether distributions diﬀerent basis samples drawn them ﬁnding well behaved function large points drawn small points test statistic diﬀerence mean function values samples; large samples likely diﬀerent distributions. call statistic maximum mean discrepancy clearly quality statistic depends class smooth functions deﬁne hand must rich enough population vanishes hand test consistent needs restrictive enough empirical estimate converge quickly expectation sample size increases. shall unit balls universal reproducing kernel hilbert spaces function classes since shown satisfy foregoing properties practical note reasonable computational cost compared two-sample tests given points sampled cost time. also propose less statistically eﬃcient algorithm computational cost yield superior performance given computational cost looking larger volume data. deﬁne three non-parametric statistical tests based mmd. ﬁrst distribution-independent uniform convergence bounds provide ﬁnite sample guarantees test performance expense conservative detecting diﬀerences third test based asymptotic distribution practice sensitive diﬀerences distribution small sample sizes. present work synthesizes expands results gretton smola song turn build earlier work borgwardt note latter addresses third kind test approach gretton employs accurate approximation asymptotic distribution test statistic. begin presentation section formal deﬁnition proof population zero unit ball universal rkhs. also review alternative function classes deﬁnes metric probability distributions. section give overview hypothesis testing applies two-sample problem review approaches problem. present ﬁrst hypothesis tests section based diﬀerent bounds deviation population empirical mmd. take diﬀerent approach section asymptotic distribution empirical estimate basis third test. large volumes data available cost computing excessive therefore propose section modiﬁed version statistic linear cost number samples associated asymptotic test. section provide overview methods related statistics machine learning literature. finally section demonstrate performance mmd-based two-sample tests problems neuroscience bioinformatics attribute matching using hungarian marriage method. approach performs well high dimensional data sample size; addition able successfully distinguish distributions graph data ﬁrst proposed test. section present maximum mean discrepancy describe conditions metric space probability distributions. deﬁned terms particular function spaces witness diﬀerence distributions therefore begin section introducing arbitrary function space. section compute population empirical estimates associated function space reproducing kernel hilbert space derive rkhs function witnesses given pair distributions section finally describe general function classes section empirical deﬁned upward bias must identify function class rich enough uniquely identify whether restrictive enough provide useful ﬁnite sample estimates unit ball reproducing kernel hilbert space empirical computed eﬃciently. main approach pursue present study. possible function classes discussed section. refer universal whenever deﬁned compact metric space associated kernel figure illustration function maximizing mean discrepancy case gaussian compared laplace distribution. distributions zero mean unit variance. function witnesses scaled plotting purposes computed empirically basis empirical statistic unbiased estimate although minimum variance since ignoring cross-terms minimum variance estimate almost identical though biased statistic also easily computed following reasoning. finally note harchaoui recently proposed modiﬁcation kernel statistic lemma scaling feature space mean distance using inverse within-sample covariance operator thus employing kernel fisher discriminant statistic testing homogeneity. statistic shown related divergence. illustrate behavior figure using one-dimensional example. data generated distributions equal means variances gaussian laplacian. chose unit ball rkhs using gaussian kernel. observe function witnesses words function maximizing mean discrepancy smooth positive laplace density exceeds gaussian density negative gaussian density larger. moreover magnitude direct reﬂection amount density exceeds other insofar smoothness constraint permits deﬁnition maximum mean discrepancy means limited rkhs. fact function class comes uniform convergence guarantees suﬃciently powerful enjoy properties. used obtain similarity measure distributions simply assign classiﬁer able separate sets. case maximization achieved ensuring many possible correspond whereas many possible consequently neural networks decision trees boosted classiﬁers objects uniform convergence bounds obtained used purpose distribution comparison. instance ben-david error hyperplane classiﬁer approximate a-distance distributions kifer function spaces inspired statistics literature also considered deﬁning mmd. indeed lemma deﬁnes space bounded continuous realvalued functions banach space supremum norm describe metrics space probability distributions kolmogorov-smirnov earth mover’s distances associated function classes. allows eﬃcient characterization distribution null hypothesis eﬃcient numerical approximations found numerical analysis handbooks distribution alternative however unknown. interpret cost transferring mass distributed according distribution accordance movement schedule. general large variety costs moving mass used psychooptical similarity measures image retrieval following theorem holds present three background results. first introduce terminology used statistical hypothesis testing. second demonstrate example even tests asymptotically error cannot guarantee performance ﬁxed sample size without making assumptions distributions. finally brieﬂy review earlier approaches two-sample problem. described metric probability distributions based distances between hilbert space embeddings empirical estimates metric address problem determining whether empirical shows statistically signiﬁcant diﬀerence distributions. brieﬂy describe framework statistical hypothesis testing applies present context following casella berger given i.i.d. samples size size statistical test used distinguish null hypothesis alternative hypothesis achieved comparing test statistic particular threshold threshold exceeded test rejects null hypothesis acceptance region test thus deﬁned real numbers threshold. since test based ﬁnite samples possible incorrect answer returned deﬁne type error probability rejecting based observed sample despite null hypothesis generated data. conversely type error probability accepting despite underlying distributions diﬀerent. level test upper bound type error design parameter test used threshold compare test statistic consistent test achieves level type error zero large sample limit. tests proposed paper consistent. even test consistent possible distinguish distributions high probability given ﬁxed sample size without prior assumptions nature diﬀerence true regardless two-sample test used. several ways illustrate this give diﬀerent insight kinds diﬀerences might undetectable given number samples. following example illustration. example assume distribution draw observations. moreover construct distribution drawing observations subsequently deﬁning discrete distribution instances probability each. easy check draw observations least next give brief overview earlier approaches sample problem multivariate data. since later experimental comparison respect certain methods give abbreviated algorithm names italics appropriate used tables section generalisation wald-wolfowitz runs test multivariate domain proposed analysed friedman rafsky henze penrose involves counting number edges minimum spanning tree aggregated data connect points points resulting test relies asymptotic normality test statistic quantity distribution-free null hypothesis ﬁnite samples computational cost method using kruskal’s algorithm log) although modern methods improve term. chazelle details. friedman rafsky claim calculating matrix distances costs dominates computing time; return point experiments possible generalisations kolmogorov-smirnov test multivariate case studied approach friedman rafsky case requires minimal spanning tree similar cost multivariate runs test. recent multivariate test introduced rosenbaum entails computing minimum distance non-bipartite matching aggregate data using number pairs containing sample test statistic. resulting statistic distribution-free null hypothesis ﬁnite sample sizes respect superior friedman-rafsky test; hand costs compute. another distribution-free test proposed hall tajvidi point requires computing closest points aggregated data counting many shall experimental comparisons test statistic costly compute; hall tajvidi consider tens points experiments. another approach distance parzen window estimates densities test statistic based asymptotic distribution distance given norm used test statistic related present here although arrived diﬀerent perspective. brieﬂy test anderson obtained restricted setting rkhs kernel inner product parzen windows. since density estimation however need decrease kernel width sample grows. fact decreasing kernel width reduces convergence rate associated two-sample test compared rate ﬁxed kernels. provide detail section approach biau gyorﬁ requires space partitioned grid bins becomes diﬃcult impossible high dimensional problems. hence test low-dimensional problems experiments. section introduce statistical tests independence exact performance guarantees ﬁnite sample sizes based uniform convergence bounds. ﬁrst section uses mcdiarmid bound biased statistic second section uses hoeﬀding bound unbiased statistic. establish properties derive hypothesis test. first show regardless whether empirical converges probability rate population value. shows consistency statistical tests based mmd. second give probabilistic bounds large deviations empirical case bounds lead directly threshold ﬁrst hypothesis test. begin discussion convergence mmdb mmd. theorem illustrate possible bounds bias empirical estimate ﬁrst inequality interesting inasmuch provides link bias bound kernel size would likely close bias small). context testing however would need provide additional bound show convergence empirical estimate population equivalent. thus following test based theorem bound bias. emphasise theorem guarantees consistency test type error probability decreases zero rate assuming convergence rate perspective consider test whether normal distributions equal worth noting bounds obtained deviation expectations empirical means completely analogous fashion. proof requires symmetrization means ghost sample i.e. second observations drawn distribution. focus present paper bounds used design inference principles based moment matching previous bounds interest since proof strategy used general function classes well behaved rademacher averages much easier approach used directly unbiased statistic lemma base test following theorem straightforward application large deviation bound u-statistics hoeﬀding compare thresholds tests. note ﬁrst threshold biased statistic applies estimate whereas unbiased statistic estimate mmd. squaring former threshold make quantities comparable squared threshold corollary decreases whereas threshold corollary decreases m−/. thus suﬃciently large mcdiarmid-based threshold lower type error better given type bound. conﬁrmed section experiments. note however rate convergence squared biased finally note bounds obtained rather conservative number fact reasons ﬁrst take actual distributions account. ﬁnite sample size distribution free bounds hold even worst case scenario. bounds could tightened using localization moments distribution etc. improvements could plugged straight theorem tighter bound. e.g. bousquet detailed discussion recent uniform convergence bounding methods. second computing bounds rather trying characterize distribution explicitly force test conservative design. following exact characterization asymptotic distribution instead bound. satisfy uniform convergence requirements leads superior tests practice. asymptotic distribution test statistic given serﬂing distribution follows serﬂing anderson appendix details. illustrate density null alternative hypotheses approximating empirically large outside quantile null distribution estimate quantile using bootstrap aggregated data following arcones gin´e alternatively approximate null distribution ﬁtting pearson curves ﬁrst four moments taking advantage degeneracy u-statistic obtain figure left empirical distribution gaussians unit standard deviation using samples each. right empirical distribution laplace distribution unit negative since unbiased estimator however terms missing ensure nonnegativity terms removed remove spurious correlations observations. consequently bound tests already eﬃcient tests described earlier still desirable obtain tests sacriﬁce much statistical power. moreover would like obtain tests storage requirements computing test statistic order apply data streams. describe achieve computing test statistic based subsampling terms sum. empirical estimate case obtained drawing pairs respectively without replacement. average random variables hoeﬀding’s bound central limit theorem readily allow provide uniform convergence asymptotic statements little eﬀort. ﬁrst follows directly hoeﬀding factor arises since averaging ⌊m/⌋ observations. note case interested average conditional variance ezvarz′|z] whereas latter case compute full variance varzz′]. noting another potential approach reducing computational cost computing rank approximation gram matrix incremental computation based rank approximation would require storage computation rather storage operations. said remains determined eﬀect approximation would distribution test statistic hence test threshold. section demonstrate connection test statistic parzen window-based statistic anderson show two-sample test based parzen windows converges slowly rkhs-based test also following anderson proceeding motivate discussion short overview parzen window estimate properties assume distribution associated density function also written minimise notation. parzen window estimate density i.i.d. sample size show distance parzen windows density estimates special case biased equation denote distance. distance known levy distance encounter distance measures derived renyi entropy disadvantage parzen window interpretation parzen window estimates consistent resulting two-sample test converges slowly using ﬁxed kernels. according anderson type error two-sample test converges m−/h−d/ thus given schedule parzen window size decrease convergence rate open interval upper limit approached decreases slowly lower limit corresponds decreasing near upper bound words avoiding density estimation obtain better convergence rate using parzen window estimate permissible bandwidth decrease schedule. addition parzen window interpretation cannot explain excellent performance based tests experimental settings dimensionality greatly exceeds sample size finally tests able employ universal kernels cannot written inner products parzen windows normalized otherwise several examples given steinwart micchelli generalize kernels structured objects strings graphs also experiments section g¨artner propose kernels deal sets observations. used context multi-instance classiﬁcation problem attempts solve estimators able infer fact elements satisfy certain property observations property too. instance dish mushrooms poisonous contains poisonous mushrooms. likewise keyring open door contains suitable key. given ensemble however rather information instance satisﬁes property. note however property testing distributions probably optimal using mean respectively) interested determining whether instances domain desired property rather making statement regarding distribution instances. taking account leads improved algorithm jointly drawn distribution prxy. wish determine whether distribution factorizes i.e. whether application independence measure independent component analysis goal linear mapping observations obtain mutually independent outputs. kernel methods employed solve problem bach jordan gretton following re-derive kernel independence measures using mean operators instead. latter however exactly gretton show hilbert-schmidt norm cross-covariance operator rkhss zero independent universal kernels. following theorem theorem denote covariance operator random variables drawn jointly prxy functions reproducing kernel hilbert spaces respectively. hilbert-schmidt norm kcxykhs equals gretton provide explicit constants. certain circumstances including case rkhss gaussian kernels empirical also interpreted terms smoothed diﬀerence joint empirical characteristic function product marginal ecfs interpretation hold cases however e.g. kernels strings graphs structured remark hypothesis test based kernel statistic complicated two-sample problem since product marginal distributions eﬀect simulated permuting variables original sample. details provided gretton figure illustration function maximizing mean discrepancy used measure independence. sample dependent random variables shown black associated function witnesses plotted contour. latter computed empirically basis samples using gaussian kernel integral independent this note unit vector turned into ﬁrst canonical basis vector rotation leaves integral invariant bearing mind rotation invariant. application related sample problem outlier detection question whether novel point generated distribution particular i.i.d. sample. special case sample test second sample contains observation. several methods essentially rely distance novel point sample mean feature space detect outliers. instance davy related method deal nonstationary time series. likewise shawe-taylor cristianini discuss detect novel observations using following reasoning probability outlier bounded function spread points feature space uncertainty empirical feature space mean instead using sample mean variance duin estimate center radius minimal enclosing sphere data advantage bounds potentially lead reliable tests single observations. sch¨olkopf show minimal enclosing sphere problem equivalent novelty detection means ﬁnding hyperplane separating data origin least case radial basis function kernels. conducted distribution comparisons using mmd-based tests datasets three real-world domains database applications bioinformatics neurobiology. investigated uniform convergence approaches asymptotic approaches bootstrap moment matching pearson curves described section asymptotic approach using linear time statistic section also compared several alternatives literature multivariate t-test friedman-rafsky kolmogorov-smirnov generalisation friedman-rafsky wald-wolfowitz generalisation biau-gy¨orﬁ test hall-tajvidi test section details regarding tests. note apply biau-gy¨orﬁ test high-dimensional problems method applicable structured data graphs. important issue practical application mmd-based tests selection kernel parameters. illustrate gaussian kernel must choose kernel width empirical zero kernel size median distance points aggregate sample compromise extremes remains heuristic similar described takeuchi sch¨olkopf optimum choice kernel size ongoing area research. considered values performance mmd-based tests cannot therefore explained context density estimation since associated density estimates necessarily meaningless here. levels tests samples used results averaged repetitions. ﬁrst case distributions diﬀerent means unit variance. percentage times null hypothesis correctly rejected euclidean distances distribution means computed function dimensionality normal distributions. case t-test ridge added covariance estimate avoid singularity second case samples drawn distributions diﬀerent variance. percentage null rejections averaged values logarithmically spaced t-test compared case since output would irrelevant. results plotted figure case gaussians diﬀering means observe t-test performs best dimensions however performance severely weakened number samples exceeds number dimensions. performance comparable t-test sample sizes outperforms methods larger sample sizes. worst performance obtained though also relatively poorly unsurprising given tests derive distribution-free large deviation bounds whereas sample size relatively small. remarkably performs quite well compared classical tests high dimensions. case gaussians diﬀering variance hall test performs best followed closely wolf smirnov diﬃculties high dimensions failing completely dimensionality becomes great. linear test performance highest dimensionality. perform poorly former failing completely several illustrations encounter much greater tightness corollary threshold corollary next application performed distribution testing data integration objective aggregate datasets single sample understanding original samples generated distribution. clearly important check last condition proceeding analysis could detect patterns dataset caused combining diﬀerent source distributions realworld phenomena. chose several real-world settings perform task compared microarray data normal tumor tissues microarray data diﬀerent subtypes cancer local ﬁeld potential electrode recordings macaque primary visual cortex without spike events cases data sets diﬀerent statistical properties detection diﬀerences made diﬃcult applied tests datasets following fashion. given datasets either chose sample samples either repeated process times. results reported table asymptotic tests perform better competitors besides wolf latter case greater type error neural dataset lower type error health status data identical performance remaining examples. note type error bootstrap test subtype dataset design value indicating pearson curves provide better threshold estimate sample sizes. remaining datasets type errors pearson bootstrap approximations close. thus larger datasets bootstrap preferred since costs compared cost pearson finally uniform convergence-based tests conservative mmdb ﬁnding diﬀerences distribution data largest sample size table distribution testing data integration multivariate data. numbers indicate percentage repetitions null hypothesis accepted given sample size neural neural health status subtype next investigate tradeoﬀ computational cost performance various tests particular attention quadratic time tests sections compare linear time mmd-based asymptotic test section consider datasets higher-dimensional datasets results plotted figure cost factor shows best overall performance function sample size type error dropping zero fast faster competing approaches three four cases narrowly trailing wolf fourth said datasets cnum forest forestd linear time achieves results comparable smaller computational cost albeit looking great deal data. cnum case however linear test able achieve zero error even largest data size. neuroii data attaining zero type error cost approaches. diﬀerence cost mmdb bootstrapping required former produces constant oﬀset cost t-test also performs well three four problems fact represents best cost-performance tradeoﬀ three datasets t-test assumes diﬀerence means important distinguishing distributions requires accurate estimate within-sample covariance; test fails completely neuroii data. emphasise kolmogorov-smirnov results obtained using classical statistic friedman-rafsky statistic hence computational cost. cost friedman-rafsky statistics therefore given wolf cost case. latter scales similarly sample size quadratic time tests conﬁrming friedman rafsky’s observation obtaining pairwise distances sample points dominant cost tests. also remark unusual behaviour type error wolf test forest dataset worsens increasing sample size. conclude approach recommended testing homogeneity depend data available small amounts data best results obtained using every observation maximum eﬀect employing quadratic time test. large volumes data available better option look point once yield greater accuracy given computational cost. also worth t-test ﬁrst case running sophisticated non-parametric tests t-test accepts null hypothesis verify distributions identical mean. ﬁnal series experiments addresses automatic attribute matching. given databases want detect corresponding attributes schemas databases based data-content two-sample test pairs attributes databases corresponding pairs. procedure also called table matching tables diﬀerent databases. performed attribute matching follows ﬁrst dataset split halves attributes represented instances tested pairs attributes note corresponding attributes diﬀerent distributions real-world databases. hence schema matching cannot solely rely distribution testing. advanced approaches schema matching using statistical test topic current research. naive approach could assume possible pair attributes might correspond thus every attribute needs tested attributes optimal match. report results naive approach aggregated pairs possible attribute matches table used three datasets census income dataset archive protein homology dataset forest dataset archive ﬁnal dataset performed univariate matching attributes multivariate matching tables diﬀerent databases table represents type forest. asymptotic u-based tests perform well better alternatives notably cnum advantage large. unlike table next best alternatives consistently across data e.g. wolf hall whereas forest smir biau t-test. thus appears perform consistently across multiple datasets. friedman-rafsky tests always return type error close design parameter instance wolf type error dataset finally mmdb performs much better table although surprisingly fails reliably detect diﬀerences forestd. results appear crude heuristic nonetheless deﬁnes semi-metric sample spaces corresponding distributions follows fact matching distances proper metrics matching cost functions metrics. formalize follows table naive attribute matching univariate multivariate data numbers indicate percentage accepted null hypothesis pooled attributes. sample size forest cnum forestd three univariate datasets table matching fourth study structured data obtained datasets protein graphs used graph kernel proteins borgwardt table matching hungarian method challenge match tables representing functional class proteins dataset corresponding tables results shown table besides cnum datasets made errors. maximum mean discrepancy deﬁned maximum deviation expectation function evaluated random variables taken suﬃciently rich function class case universal reproducing kernel hilbert space equivalently statistic written norm diﬀerence distribution feature means rkhs. require density estimates intermediate step. tests provide type error bounds exact distribution-free ﬁnite sample sizes. also give third test based quantiles asymptotic distribution associated test statistic. three tests computed time however suﬃcient data available linear time statistic used employs data better results smaller computational cost. addition number metrics distributions well certain kernel similarity measures distributions included within framework. result establishes statistical tests based consistent universal kernels compact domains draw attention recent introduction characteristic kernels fukumizu kernels mean injective. fukumizu establish gaussian laplace kernels characteristic thus consistent test domain. sriperumbudur explore properties characteristic kernels providing simple condition determine whether convolution kernels characteristic describing characteristic kernels universal compact domains. also note rkhss associated particular kernel probability distributions. hein describe several kernels induce corresponding distances feature space distribution mappings turn lead powerful two-sample tests. recent studies shown additional divergence measures distributions obtained empirically optimization reproducing kernel hilbert space. harchaoui build work gretton considering homogeneity statistic arising kernel fisher discriminant rather diﬀerence rkhs means; nguyen obtain divergence estimate approximating ratio densities function rkhs. design kernel-based statistics prioritise diﬀerent features measuring divergence them resulting eﬀects distinguishability distributions therefore interest. finally seen section several classical metrics probability distributions written maximum mean discrepancies function classes hilbert spaces rather banach metric semi-metric spaces. would particular interest establish conditions could write discrepancies terms norms diﬀerences mean elements. particular consider banach spaces endowed semi-inner product general riesz representation exists elements dual. rkhs compact domain kernel bounded according i.i.d. sample size drawn according probability measure i.i.d take values equal probability. deﬁne rademacher average want show absolute diﬀerence mmdb close expected value independent distributions prove three intermediate results combine. ﬁrst result need upper bound absolute diﬀerence mmdb. second provide upper bound diﬀerence expectation. changing either results changes magnitude respectively. apply mcdiarmid’s theorem given denominator exponent section derive theorem result namely large deviation bound note also consider positive deviations mmdb since negative deviations irrelevant hypothesis test. proof follows three steps previous section. ﬁrst step becomes derive results needed asymptotic test section appendix describes distribution empirical appendix contains derivations second third moments empirical also result adequate purposes make clear dependence null distribution kernel choice. reason provide alternative expression based reasoning anderson bearing mind following changes require greater generality since deal distributions compact metric spaces densities correspondingly kernels necessarily inner products probability density functions acknowledgements thank philipp berens olivier bousquet john langford omri guttman matthias hein novi quadrianto song vishy vishwanathan constructive discussions; patrick warnat providing microarray datasets; nikos logothetis providing neural datasets. national australia funded australian government’s backing australia’s ability initiative part australian research council. work supported part programme european community pascal network excellence ist-- austrian science fund project s-n.", "year": 2008}