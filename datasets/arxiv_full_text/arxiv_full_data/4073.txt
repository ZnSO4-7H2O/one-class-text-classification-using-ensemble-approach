{"title": "Commonly Uncommon: Semantic Sparsity in Situation Recognition", "tag": ["cs.CV", "cs.AI"], "abstract": "Semantic sparsity is a common challenge in structured visual classification problems; when the output space is complex, the vast majority of the possible predictions are rarely, if ever, seen in the training set. This paper studies semantic sparsity in situation recognition, the task of producing structured summaries of what is happening in images, including activities, objects and the roles objects play within the activity. For this problem, we find empirically that most object-role combinations are rare, and current state-of-the-art models significantly underperform in this sparse data regime. We avoid many such errors by (1) introducing a novel tensor composition function that learns to share examples across role-noun combinations and (2) semantically augmenting our training data with automatically gathered examples of rarely observed outputs using web data. When integrated within a complete CRF-based structured prediction model, the tensor-based approach outperforms existing state of the art by a relative improvement of 2.11% and 4.40% on top-5 verb and noun-role accuracy, respectively. Adding 5 million images with our semantic augmentation techniques gives further relative improvements of 6.23% and 9.57% on top-5 verb and noun-role accuracy.", "text": "semantic sparsity common challenge structured visual classiﬁcation problems; output space complex vast majority possible predictions rarely ever seen training set. paper studies semantic sparsity situation recognition task producing structured summaries happening images including activities objects roles objects play within activity. problem empirically object-role combinations rare current state-of-the-art models signiﬁcantly underperform sparse data regime. avoid many errors introducing novel tensor composition function learns share examples across role-noun combinations semantically augmenting training data automatically gathered examples rarely observed outputs using data. integrated within complete crf-based structured prediction model tensor-based approach outperforms existing state relative improvement top- verb noun-role accuracy respectively. adding million images semantic augmentation techniques gives relative improvements top- verb noun-role accuracy. many visual classiﬁcation problems image captioning visual question answering referring expressions situation recognition structured semantically interpretable output spaces. contrast classiﬁcation tasks imagenet problems typically suffer semantic sparsity; combinatorial number possible outputs dataset cover performance existing models degrades signiﬁcantly evaluated rare unseen inputs paper consider situation figure three situations involving carrying semantic roles agent carrier item carried agentpart part agent carrying place situation happening. carrying many possible carry-able objects example semantic sparsity holds many roles situation recognition. recognition prototypical structured classiﬁcation problem signiﬁcant semantic sparsity develop models semantic data augmentation techniques signiﬁcantly improve performance better modeling underlying semantic structure task. situation recognition task producing structured summaries happening images including activities objects roles objects play within activity. problem challenging many activities carrying open ended semantic roles item thing carried nearly object carried training data never contain possibilities. prototypical instance semantic sparsity rare outputs constitute large portion required predictions figure current state-of-the-art perfigure percentage images imsitu development function total number training examples least frequent role-noun pair situation. uncommon target outputs observed fewer times training common constituting required predictions. semantic sparsity central challenge situation recognition. figure verb role-noun prediction accuracy baseline imsitu function frequency least observed role-noun pair training set. solid horizontal lines represent average performance across whole imsitu irrespective frequency. even target output becomes uncommon accuracy decreases. formance situation recognition drops signiﬁcantly even participating object samples it’s role propose address challenge ways building models effectively share examples objects different roles semantically augmenting training rarely represented noun-role combinations. introduce compositional conditional random field formulation reduce effects semantic sparsity encouraging sharing nouns different roles. like previous work deep neural network directly predict factors crf. models required factors predicted using global image representation linear regression unique factor. contrast propose novel tensor composition function uses dimensional representations nouns roles shares weights across roles nouns score combinations. model compositional independent representations nouns roles combined predict factors allows globally shared representation nouns across entire crf. model trained form semantic data augmentation provide extra training samples rarely observed noun-role combinations. show possible generate short search queries correspond partial situations used image retrieval. noisy data incorporated pre-training optimizing marginal likelihood effectively performing soft clustering values unlabeled aspects situations. data also supports show self training model predictions used prune images training ﬁnal predictor. experiments imsitu dataset demonstrate compositional semantic augmentation techniques reduce effects semantic sparsity strong gains relatively rare conﬁgurations. show contribution helps signiﬁcantly combined approach improves performance relative strong baseline top- verb nounrole accuracy respectively. uncommon predictions methods provide relative improvement average across measures. together experiments demonstrate beneﬁts effectively targeting semantic sparsity structured classiﬁcation tasks. background situation recognition situation recognition recently proposed model events within images order answer questions beyond what activity happening? what what with?. general formulations build semantic role labelling problem natural language processing verbs automatically paired arguments sentence semantic role corresponds question event study situation recognition imsitu largescale dataset human annotated situations containing activities roles nouns images. imsitu images collected cover diverse situations. example seen figure situations annotated imsitu development contain least rare role-noun pair. situation recognition imsitu strong test evaluating methods addressing semantic sparsity large scale structured easy evaluate clearly measurable range semantic sparsity across different verbs roles. furthermore seen figure semantic sparsity signiﬁcant challenge current situation recognition models. formal deﬁnition situation recognition assume discrete sets verbs nouns frames frame paired semantic roles every element mapped exactly verb frame derived framenet lexicon semantic role labeling noun drawn wordnet semantic role paired noun value ∪{∅} indicates value either known apply. pairs semantic roles values called realized frame ef}. realized frames valid assigned exactly noun given image task predict situation speciﬁed verb valid realized frame refers frame mapped example ﬁrst image figure predicted situations conditional random field predicting situation given image decomposes verb semantic role-value pairs realized frame similarly previous work full distribution potentials verbs semantic roles takes form encode scores computed neural network. learn model assume image dataset general possible ground truth situations optimize log-likelihood observing least situation compositional tensor potential previous work potentials computed using global image representation p-dimensional image vector derived convolutional neural network potential value computed linear regression parameters unique possible decision verb verb-role-noun example verb-role-noun potential equation model directly represent fact nouns reused different roles although underlying neural network could hypothetically learn encode reuse tuning. instead introduce compositional potentials make reuse explicit. formulate compositional potential introduce m-dimensional vectors rm|n vector noun nouns. create matrices rp×o| matrix verb semantic role pair occurring frames image representations o-dimensional verbrole representations. finally introduce tensor global composition weights rm×o×p. deﬁne tensor weighting function takes input verb semantic role noun image representation tensor weighting function constructs image speciﬁc verb-role representation multiplying global image vector verb-role matrix then combines global noun representation image speciﬁc role representation global image representation outer products. finally weights dimension outer product weight weights indicate features -way outer product important. ﬁnal potential produced summing elements tensor produced figure overview compositional conditional random field predicting situations. deep neural network used compute potentials crf. verb-role-noun potential built global bank noun representations image speciﬁc role representations global image representation combined weighted tensor product. model allows sharing among nouns different roles leading signiﬁcant gains seen section tensor produced general high dimensional expressive. allows small dimensionality representations making function robust small numbers samples noun. matrix parameters ﬂattened layout noun role dimensions together. aligning terms equation tensor potential offers alternative parametrized linear regression uses many general purpose parameters furthermore eliminates parameter ever uniquely associated regression instead compositionally using noun verb-role representations build parameters regression. situation recognition strongly connected language. situation thought simple declarative sentence activity happening image. example ﬁrst situation figure could expressed carrying baby chest outside knowing prototypical ordering semantic roles around verbs inserting prepositions. relationship used reduce seconvert annotated situations phrases semantic augmentation exhaustively enumerating possible sub-pieces realized situations occur imsitu training ﬁrst situation fig}) pieces ect. substructures converted deterministically phrase using template speciﬁc every verb. example template carrying {agent} carrying {item} {with agentpart} place}. partial situations realized phrases taking ﬁrst gloss wordnet synset associated every noun substructure inserting corresponding slots template discarding unused slots. example phrases sub-pieces realized carrying carrying baby. phrases used retrieve images google image search construct images annotated verb partially complete realized frames assigning retrieved images sub-piece generated retrieval query. while templates generate completely ﬂuent phrases preliminary experiments found sufﬁciently accurate image search often phrase could retrieve correct images. longer phrases tended much lower precision. pre-training images retrieved incorporated pre-training phase. images retrieved partially speciﬁed realized situations labels. account this instead compute marginal likelihood partially observed situations self training images retrieved contain signiﬁcant noise. especially true role-noun combinations occur infrequently limiting utility pretraining. therefore also consider ﬁltering images model already trained fully supervised data imsitu. rank images according computed trained model ﬁlter top-k every unique pretrain subset train imsitu increase repeat process model longer improves. experimental setup models models implemented caffe pretrained network base image representation ﬁnal fully connected layers replaced fully connected layers dimensionality ﬁnetune layers models. tensor potential noun embedding size role embedding size ﬁnal layer network global image representation larger values seem improve results slow pretrain omit them. experiments image regression conjunction compositional potential remove regression parameters associated combinations seen fewer times imsitu training reduce overﬁtting. baseline compare models alternative methods introducing effective sharing nouns. ﬁrst baseline adds potential baseline nouns independent roles. modify probability equation situation given image decompose pairs roles nouns realized frame also nouns second baseline consider compositional tensor based composition method. model instead constructs many verb-role representations combines noun representations using inner-products model tensor model section global image representation noun vectors every noun also assume verb-role matrices htve ro×p every verb-role compute corresponding potential equation model motivated compositional models used semantic role labeling allows trade-off need reduce parameters associated nouns expressivity. grid search values largest size network could afford requirement inner product. found best setting decoding experimented decoding methods ﬁnding best scoring situation models. systems used compositional potentials performed better ﬁrst predicting verb using max-marginal semantic roles optimization models trained stochastic gradient descent momentum weight decay pretraining semantic augmentation conducted initial learning rate gradient clipping batch size training imsitu data initial learning rate models learning rate reduced factor model improve imsitu set. semantic augmentation experiments semantic augmentation images retrieved using google image search. retrieved medium sized full-color safe search ﬁltered images query phrase. produced million possible query phrases imsitu training majority extremely rare. limited phrases occur times imsitu phrases occur times accepted containing noun. roughly phrases table situation recognition results full imsitu development set. results divided models trained imsitu data rows models data semantic data augmentation marked rows models marked +reg also include image regression potentials used baseline. tensor composition model signiﬁcantly outperforms existing state addition noun potential compositional baseline tensor composition model able make better semantic data augmentation baseline table situation prediction results rare portion imsitu development set. results divided models trained imsitu data rows models data semantic data augmentation marked rows models marked +reg also include image regression potentials used baseline. semantic data augmentation baseline hurts rare cases. semantic augmentation yields larger relative improvement rare cases composition-based model required realize gains. used retrieve million images web. duplicate images occurring imsitu removed. pretraining experiments updates self training self train rare realized frames self training yielded diminishing gains iterations ﬁrst iteration second evaluation standard data split imsitu train development test images. follow evaluation setup deﬁned imsitu evaluating verb predictions semantic role-value pair predictions full structure correctness report accuracy top- top- given ground truth verb average across measures also report performance examples requiring rare predictions. results compositional tensor potential results full imsitu presented table rows overresults demonstrate adding noun potential baseline composition model ineffective perform worse baseline hypothesize systematic variation object appearance roles challenging models. tensor composition model able better capture variation effectively share information among nouns reﬂected improvements value value-all accuracy given ground truth verbs maintaining high top- top- verb accuracy. however expected many situations cannot predicted compositionally based nouns combination image regression potential tensor composition potential yields best performance indicating modeling complementary aspects problem. ﬁnal model trained imsitu data outperforms baseline every measure improving points overall. results rare portion imsitu dataset presented table rows ﬁnal model provides best overall performance rare cases among models trained imsitu data improving points average. models struggle correctly entire structures indicating rare predictions extremely hard completely correct baseline model uses image regression potentials performs best. hypothesize image regression potentials allow model easily coordinate predictions across roles simultaneously semantic data augmentation results full imsitu development presented table rows overall results indicate semantic data augmentation helps models tensor model beneﬁts baseline self training improves tensor model slightly making perform better top- top- predictions hurting performance given gold verbs. average ﬁnal model outperforms baseline trained identical data points. results rare portion imsitu dataset presented table rows surprisingly rare cases semantic augmentation hurts baseline rare instance image search results extremely noisy. close inspection many returned results contain target activity instead contain target nouns. hypothesize without effective global noun representation baseline cannot extract meaningful information extra data. hand tensor model improves rare cases overall self training improves overall results experiments show tensor model able perform better comparable data settings semantic augmentation techniques largely beneﬁt models tensor model beneﬁts semantic augmentation. also present full performance top- verb across numbers samples figure compositional semantic augmentation outperforms baseline models continue struggle uncommon cases. techniques seem give beneﬁt examples requiring predictions structures seen between times providing somewhat less beneﬁt even rarer ones. challenging future work make figure top- verb accuracy imsitu development set. ﬁnal compositional semantic data augmentation outperforms baseline rare cases models continue struggle semantic sparsity. ﬁnal model largest improvement relative baseline cases examples training set. also evaluated models imsitu test exactly once. results summarized table full imsitu test table rare portion. general trends established imsitu supported. provide examples figure predictions ﬁnal system made rare examples development set. learning cope semantic sparsity closely related zero-shot k-shot learning. attribute-based learning cross-modal transfer using text priors proposed study classiﬁcation simpliﬁed settings. structured case image captioning models observed suffer lack diversity figure output ﬁnal model development examples containing rare role-noun pairs. ﬁrst contains examples model correctly predicts entire structures top- highlight particular role-noun pairs make examples rare yellow number occurances imsitu training set. second contains examples verb correctly predicted top- values predicted correctly. highlight incorrect predictions red. many predictions occurr zero times training systems struggle cases. generalization recent efforts gain insight issues extract subject-verb-object triplets captions count prediction failures rare tuples imsitu study semantic sparsity circumvents need intermediate processing captions generalizes verbs arguments. compositional models explored number applications natural language processing sentiment analysis dependency parsing text similarity visual question answering effective tools combining natural language elements prediction. recently bilinear pooling compact bilinear pooling proposed second-order feature representations tasks grained recognition visual question answer. build methods using dimensional embeddings semantic units expressive outer product computations. queries. supervision also explored pretraining convolutional neural networks ﬁnegrained bird classiﬁcation common sense reasoning ﬁrst explore connection semantic sparsity language automatically generating queries semantic augmentation able show improvement large scale fully supervised structured prediction task. studied situation recognition prototypical instance structured classiﬁcation problem signiﬁcant semantic sparsity. despite fact vast majority possible output conﬁgurations rarely observed training data showed possible introduce compositional models effectively share examples among required outputs semantic data augmentation techniques signiﬁcantly improved performance. future important introduce similar techniques related problems semantic sparsity generalize ideas zero-shot learning.", "year": 2016}