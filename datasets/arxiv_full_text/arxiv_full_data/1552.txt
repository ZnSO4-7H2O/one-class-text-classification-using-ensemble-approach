{"title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/~glai1/data/race/ and the code is available at https://github.com/qizhex/RACE_AR_baselines.", "text": "present race dataset benchmark evaluation methods reading comprehension task. collected english exams middle high school chinese students range race consists near passages near questions generated human experts covers variety topics carefully designed evaluating students’ ability understanding reasoning. particular proportion questions requires reasoning much larger race benchmark datasets reading comprehension signiﬁcant performance state-of-the-art models ceiling human performance hope dataset serve valuable resource research evaluation machine comprehension. dataset freely available http//www.cs.cmu.edu/ ˜glai/data/race/ code available https//github.com/ qizhex/race_ar_baselines constructing intelligence agent capable understanding text people major challenge research. recent advances deep learning techniques seems possible achieve human-level performance certain language understanding tasks surge effort devoted machine comprehension task people construct system ability towards goal several large-scale datasets proposed allow researchers train deep learning systems obtain results comparable human performance. suitable dataset crucial evaluating system’s true ability reading comprehension existing datasets suffer several critical limitations. firstly datasets candidate options directly extracted context leads fact lots questions solved trivially word-based search context-matching withdeeper reasoning; constrains types questions well. secondly answers questions datasets either crowd-sourced automatically-generated bringing signiﬁcant amount noises datasets limits ceiling performance domain experts childrens book test who-didwhat. another issue existing datasets topic coverages often biased speciﬁc ways data initially collected making hard evaluate ability systems text comprehension broader range topics. address aforementioned limitations constructed dataset collecting large questions answers associated passages english exams middle-school high-school chinese students within range. exams designed domain experts evaluating reading comprehension ability students ensured quality broad topic coverage. furthermore answers machines humans objectively graded evaluation comparison using evaluation metrics. although efforts made similar motivation including mctest dataset created several others usefulness datasets signiﬁcantly restricted small sizes especially suitable training powerful deep neural networks whose success relies availability relatively large training sets. dataset namely race consists passages questions. reading passage student asked answer several questions question provided four candidate answers correct unlike existing datasets questions candidate answers race restricted text spans original passage; instead described words. sample dataset presented table latter analysis shows correctly answering large portion questions race requires ability reasoning important feature machine comprehension dataset race also offers important subdivisions reasoning types questions namely passage summarization attitude analysis introduced existing large-scale datasets knowledge. addition compared existing datasets passages either domain-speciﬁc single ﬁxed style passages race almost cover types human articles news stories biography philosophy etc. variety styles. comprehensiveness topic/style coverage makes race desirable resource evaluating reading comprehension ability machine learning systems general. questions candidate options generated human experts intentionally designed test human agent’s ability reading comprehension. makes race relatively accurate indicator reﬂecting questions substantially difﬁcult existing datasets terms large portion questions involving reasoning. meantime also sufﬁciently large support training deep learning models. unlike existing large-scale datasets candidate options race human generated sentences appear original passage. makes task challenging allows rich type questions passage summarization attitude analysis. mctest mctest popular dataset question answering format race question associated four candidate answers single correct answer. although questions mctest high-quality ensured careful examinations crowdsourcing contains stores questions substantially restricts usage training advanced machine comprehension models. moreover mctest designed years children race constructed middle high school students years hence complicated requires stronger reasoning skills. words race viewed larger difﬁcult version mctest dataset. cloze-style datasets past years witnessed several largescale cloze-style datasets whose questions formulated obliterating word entity sentence. passage small village england years mail coach standing street. didn’t come village often. people letter. person sent letter didn’t postage receiver here’s letter miss alice brown said mailman. alice brown girl said voice. alice looked envelope minute handed back mailman. sorry can’t take don’t enough money said. gentleman standing around sorry her. came paid postage her. gentleman gave letter said smile thank much letter tom. going marry him. went london look work. i’ve waited long time letter don’t need nothing really? know that? gentleman said surprise. told would signs envelope. look cross corner means well circle means found work. that’s good news. gentleman rowland hill. didn’t forgot alice letter. postage paid receiver changed said good plan. postage much lower penny? person sends letter pays postage. stamp envelope. said government accepted plan. ﬁrst stamp called penny black. picture queen questions idea using stamps thought government rowland hill alice brown passage know high postage made people never send letters lovers almost lose every touch people best avoid paying receivers refuse coming letters answer adabc money postage received letter didn’t want open already known written letter know alice’s words told signs meant leaving alice clever could guess meaning signs alice signs envelope signs alice told cnn/daily mail largest machine comprehension datasets questions. however require limited reasoning ability fact best machine performance obtained researchers close human’s performance cnn/daily mail. childrens book test book test constructed similar manner. passage consist contiguous sentences extracted children’s books next sentence used make question. main difference datasets size times larger. machine comprehension models also matched human performance another cloze-style dataset constructed english gigaword newswire corpus. authors generate passages questions picking news articles describing event datasets squad newsqa marco recently proposed triviaqa answer question form text span article. articles squad newsqa marco come wikipedia news bing search engine respectively. answer certain question unique could multiple spans. instead evaluating accuracy researchers need score bleu rouge metrics measure overlap prediction ground truth answers since datasets span-based answers challenging space possible spans usually large. however restricting answers text spans context passage unrealistic importantly intuitive even humans indicated suffered human performance squad newsqa. words format span-based answers necessarily good examination reading comprehension machines whose approach comprehension ability humans. datasets examinations several datasets extracted examinations aiming evaluating systems under conditions humans evaluated schools. e.g. elementary school science questions dataset contains questions students elementary schools; ntcir evaluates systems task solving real-world university entrance exam questions; entrance exams task clef track evaluates system’s reading comprehension ability. however data provided existing tasks sufﬁcient training advanced data-driven machine reading models partially expensive data generation process human experts. section study nature questions covered race detailed level. speciﬁcally present dataset statistics section analyze different reasoning/question types race remaining subsections. dataset statistics mentioned section race collected english examinations designed year-old middle school students yearold high school students china. distinguish subgroups drastic difﬁculty race-m denotes middle school examinations race-h denotes high school examinations. split data development test race-m race-h respectively. number samples shown table statistics race-m race-h summarized table length passages vocabulary size race-h much larger race-m evidence higher difﬁculty high school examinations. however notice since articles questions selected designed test chinese students learning english foreign language vocabulary size complexity language constructs simpler news articles wikipedia articles datasets. reasoning types questions comprehensive picture reasoning difﬁculty requirement race conduct human annotations questions types. following chen trischler stratify questions classes follows ascending order difﬁculty chanical turk generated passage questions. question labeled crowdworkers. require turkers answer questions label reasoning type. passage race-m race-h respectively restrict access master turkers only. finally labels questions. statistics reasoning type summarized table higher difﬁculty level race justiﬁed higher ratio reasoning questions comparison squad newsqa. speciﬁcally questions race either category single-sentence reasoning category multi-sentence reasoning ratio squad newsqa respectively. also notice ratio word matching questions race lowest among several categories. addition questions race-h complex questions race-m since race-m word matching questions fewer reasoning questions. subdividing reasoning types better understand dataset facilitate future research list subdivisions questions reasoning category. frequent reasoning subdivisions include detail reasoning whole-picture understanding passage summarization attitude analysis world knowledge. question fall multiple divisions. deﬁnition subdivisions associated examples follows detail reasoning answer question agent clear details passage. answer appears passage canfound simply matching question passage. example question sample passage falls category. whole-picture reasoning agent needs understand whole picture story obtain correct answer. example answer question sample passage agent required comprehend entire story. passage summarization question requires agent select best summarization passage among four candidate summarizations. typical question type main idea passage example question found appendix many people optimistically thought industry awards better equipment would stimulate production quieter appliances. even suggested noise building sites could alleviated section compare performance several state-of-the-art reading comprehension models human performance. accuracy metric evaluate different models. methods comparison sliding window algorithm firstly build rule-based baseline introduced richardson chooses answer highest matching score. speciﬁcally ﬁrst concatenates question answer calculates tf-idf style matching score concatenated sentence every window article. window size decided model performance training sets. stanford attentive reader stanford attentive reader strong model achieves state-of-the-art results cnn/daily mail. moreover authors claim model nearly reached ceiling performance datasets. suppose triple passage question options denoted ﬁrst employ bidirectional grus encode respectively summarize relevant part passage attention model. following chen adopt bilinear attention form. specifically collected data three large free public websites china reading comprehension problems extracted english examinations designed teachers china. data cleaning contains passages questions total passages questions middle school group passages questions high school group. following ﬁltering steps conducted clean data. firstly remove problems questions format problem setting e.g. question would removed number options four. secondly ﬁlter articles questions self-contained based text information i.e. remove articles questions containing images tables. also remove questions containing keywords underlined paragraph since difﬁcult reproduce effect underlines paragraph segment information. thirdly remove duplicated articles. websites answers stored images. used standard programs tesseract abbyy finereader process images. remove answers software disagree. task easy since need recognize printed alphabet standard font. finally cleaned dataset race passages questions. figure test accuracy different baselines question type category introduced section word-match single-reason multi-reason ambiguous abbreviations word matching single-sentence reasoning multi-sentence reasoning insufﬁcient/ambiguous respectively. gated-attention reader gated state-of-the-art model multiple datasets. build query-speciﬁc representations tokens document employs attention mechanism model multiplicative interactions query embedding document representation. multi-hop architecture also enables model scan document question iteratively multiple passes. words multi-hop structure makes possible reader reﬁne token representations iteratively attention mechanism relevant part document. refer readers details. implementation details follow chen experiment settings. vocabulary size choose word embedding size -dimensional glove word embedding embedding initialization. weights initialized gaussian distribution parameters initialized uniform distribution hidden dimensionality number layers stanford vanilla stochastic gradient descent train models. apply dropout word embeddings gradient clipped norm gradient larger grid search validation choose learning rate within dropout rate within highest accuracy validation obtained setting learning rate stanford dropout rate data race-m race-h used together train model testing performed separately. human evaluation described section randomly sampled subset test labeled amazon turkers contains questions half race-h half race-m. turkers’ performance race-m race-h. however hard guarantee every turker performs survey carefully given difﬁcult long passages high school problems. therefore obtain ceiling human performance race manually labeled proportion valid questions. question valid unambiguous correct answer. found data valid sets ceiling human performance. similarly ceiling performance race-m race-h respectively. main results compare models’ human ceiling performance datasets evaluation metric race. compared datasets include race mctest cnn/daily mail wdw. report performance subsets missing token either common noun name entity since language models already reached human-level performance types comparison shown table performance sliding window ﬁrst compare mctest race using sliding window unable train stanford gated mctest’s limited training data. sliding window achieves accuracy mctest race meaning answer questions race requires reasoning mctest. since candidate answers question average three. instead evaluate performance improvement sliding window random baseline. larger improvement indicates questions solvable simple matching. race sliding window better random baseline improvement cbtn cbt-c wdw. performance neural models compare difﬁculty different datasets state-of-the-art neural models’ performance. lower performance means problems unsolvable machines. stanford gated achieve accuracy race accuracy much higher cnn/daily mail childrens book test who-did-what. justiﬁes fact that among current large-scale machine comprehension datasets race challenging one. human ceiling performance human performance shows data quite clean compared large-scale machine comprehension datasets. since cannot enforce every turker test cautiously result shows turkers’ performance human performance. reasonably problems high school group longer passages complex questions lead signiﬁcant divergence. nevertheless start-of-the-art models still large room improved reach turkers’ performance. performance middle school problems high school problems. what’s more performance stanford less half ceiling human performance indicates match humans’ reading comprehension ability still long evaluate human models different types questions shown figure turkers best word matching problems worst reasoning problems. sliding window performs better word matching problems needing reasoning paraphrasing. surprisingly stanford stronger performance word matching category reasoning categories. possible reason proportion data reasoning categories larger data. also candidate answers simple matching questions share similar word embeddings. example question color difﬁcult distinguish candidate answers green blue yellow embedding vector space. similar performance different categories also explains reason performance neural models close middle high school groups table introduce large high-quality dataset reading comprehension carefully designed examine human ability task. desirable properties race include broad coverage domains/styles richness question format. importantly requires substantially reasoning well race datasets signiﬁcant between performance state-of-the-art machine comprehension models human. hope dataset stimulate development advanced machine comprehension models. felix hill antoine bordes sumit chopra jason weston. goldilocks principle reading children’s books explicit memory representations. arxiv preprint arxiv. daniel khashabi tushar khot ashish sabharwal peter clark oren etzioni roth. question answering integer programming arxiv preprint semi-structured knowledge. arxiv. automatic evaluation summaries using n-gram coproceedings occurrence statistics. conference north american chapter association computational linguistics human language technology-volume association computational linguistics pages nguyen rosenberg song jianfeng saurabh tiwary rangan majumder deng. marco human generated machine arxiv preprint reading comprehension dataset. arxiv. kishore papineni salim roukos todd ward weijing zhu. bleu method automatic evalproceedings uation machine translation. annual meeting association computational linguistics. association computational linguistics pages hideyuki shibuki kotaro sakamoto yoshinobu kano teruko mitamura madoka ishioroshi kelly itakura wang tatsunori mori noriko kando. overview ntcir- qa-lab task. ntcir. adam trischler tong wang xingdi yuan justin harris alessandro sordoni philip bachman kaheer suleman. newsqa machine comprehension dataset. arxiv preprint arxiv. passage love holidays hate gaining weight? alone. holidays times celebrating. many people worried weight. proper planning though possible keep normal weight holidays. idea enjoy holidays much. don’t turn away foods enjoy. avoid high-fat foods. dishes look oily creamy large amount fat. choose lean meat fill plate salad green vegetables. lemon juice instead creamy food. questions best title passage? options avoid holiday feasting do’s don’ts keeping slim avoid weight gain holidays. wonderful holidays boring experiences.", "year": 2017}