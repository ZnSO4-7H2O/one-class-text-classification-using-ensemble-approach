{"title": "Kernel diff-hash", "tag": ["cs.CV", "cs.AI"], "abstract": "This paper presents a kernel formulation of the recently introduced diff-hash algorithm for the construction of similarity-sensitive hash functions. Our kernel diff-hash algorithm that shows superior performance on the problem of image feature descriptor matching.", "text": "paper presents kernel formulation recently introduced diﬀ-hash algorithm construction similarity-sensitive hash functions. kernel diﬀ-hash algorithm shows superior performance problem image feature descriptor matching. eﬃcient representation data compact convenient similaritysensitive hashing methods ﬁrst considered later similarity-sensitive hashing methods regarded particular instance supervised metric learning tries construct hashing function data space preserves known similarity training set. typically similarity binary related hash collision probability methods enjoying increasing popularity computer vision pattern recognition community image analysis retrieval video copy detection shape retrieval followed sign function. posed problem similarity-sensitive hash construction boosted classiﬁcation dimension hash acts weak binary classiﬁer. parameters hashing function learned using adaboost. used setting problem proposed much simpler algorithm wherein projections selected eigenvectors ratio diﬀerence covariance matrices similar dissimilar pairs data points; former method dubbed lda-hash latter diﬀ-hash. applying methods sift local features images compact accurate binary descriptors produced. inspiration paper diﬀ-hash method remarkably simple eﬃcient method suﬀers major limitations. first length hash limited descriptor dimensionality. situations clear disadvantage longer hashes allow produce accurate matching. secondly aﬃne hashing functions many cases simple fail represent correctly structure data. paper propose kernel formulation diﬀ-hash algorithm eﬃciently resolved problems. show performance algorithm problem image descriptor matching using patches dataset show outperforms original diﬀ-hash. denote data space. denote pairs similar data points pairs dissimilar data points problem similarity-sensitive hashing represent data common space m-dimensional binary vectors hamming metric aibi means alternatively expressed e{dhm e{dhm former interpreted false negative rate latter false positive rate jection matrix threshold vector. similarity-sensitive hashing algorithm considers hash construction boosted binary classiﬁcation hash dimension acts weak binary classiﬁer. dimension adaboost used maximize following loss function sign adaboost weigh pair iteration. shakhnarovich selected axis projection onto minimizes objective. minimization problem relaxed following first removing non-linearity setting projection vector then ﬁxing projection threshold disadvantages boosting-based ﬁrst high computational complexity second tendency unnecessary long hashes. w.r.t. projection matrix threshold vector ﬁrst second terms thought respectively. parameter controls tradeoﬀ fnr. limit case eﬀectively considers positive pairs ignoring negative set. denote covariance matrices positive negative data. solution given explicitly compared adaboost-based methods must dimensionality-reducing since compute projection eigenvectors covariance matrix size dimensionality embedding space must restriction limiting many cases ﬁrst depends data dimensionality second dimensionality longer hash would achieve better performance. furthermore aﬃne parametric form embedding many cases oversimpliﬁcation generic required. paper cope problems using kernel formulation transforms data feature space never dealt explicitly required). order simplify following discussion since problem separable consider one-dimensional projections. whole method summarized algorithm idea kernelization replace original data corresponding feature vectors replacing linear projection βihφ kx]. here vector unknown linear combination coeﬃcients denote representative points data space. kernel selected account correctly structure data space formulation dimensionality hash bounded number basis vectors limited training size computational complexity. order test approach applied problem image feature matching. problem core many modern internet-scale computer vision applications including city scale reconstruction basic underlying task problems repeated millions billions times comparison local image features similar methods typically features represented means multidimensional descriptors vectors compared using figure example positive negative pair image patches corresponding descriptors. first patches second sift descriptors third binary descriptors length produced using kdif. euclidean distance. large datasets severe scalability issues encountered including problems storage similarity query feature descriptors. eﬃcient representation comparison feature descriptors addressed many recent works computer vision community proposed using similarity-sensitive hashing methods produce compact binary descriptors descriptors several appealing properties make especially suitable large-scale applications. first compact easy store standard databases. second comparison binary descriptors done using hamming metric amounts count operation carried extremely eﬃciently modern architectures signiﬁcantly faster computation euclidean distances. finally construction binarization transformations involves metric learning thus modeling correctly distance descriptors usually noneuclidean. particular allows compensate imperfect invariance descriptor cope descriptor variability pairs images wide baseline. result last property similarity-sensitive hashing reduces descriptor size actually improving performance unlike methods typically come price decreased performance. experiments used data datasets contained rectiﬁed normalized patches extracted multiple images depicting three diﬀerent scenes ﬁrst scenes similar representing architectural landmarks; last scene diﬀerent representing natural mountain environment. scene total nearly patches corresponding around diﬀerent feature points available; feature appeared multiple times. training used pairs patches corresponding diﬀerent views points positives pairs patches diﬀerent points negatives testing diﬀerent subset dataset containing positive negative pairs used. patch -dimensional sift descriptor computed using toolbox vedaldi compared performance binary descriptor obtained means diﬀ-hash method strecha kernel version diﬀ-hash appeared best performing algorithm extensive evaluations done since kdif extended version choose commethods used value pare method. experimentally found produce best results. kdif used gaussian kernel mahalanobis distance form exp{−tς−/ training testing data used methods. reference show euclidean distance original sift descriptors. figures show performance diﬀerent hashing algorithms function diﬀerent datasets. several conclusions drawn ﬁgure. first kdif appears consistently outperform three scenes hash length second suﬃciently large method outperforms sift still compact. third learned hashing functions generalize gracefully scenes though slight perfigure compares performance diﬀerent descriptors terms points binary descriptors outperform sift compact second kdif consistently outperforms dif. third using longer hash increases performance. figure shows examples ﬁrst matches patch descriptors obtained using euclidean distance hamming distance hashed descriptors using method. method provides superior performance. presented kernel formulation diﬀ-hash similarity-sensitive hashing algorithm showed method used produce eﬃcient compact binary feature descriptors. though showed results sift method generic applied local feature descriptor. method showed superior results compared original diﬀ-hash proposed generic allows obtain hashes length also incorporate nonlinearity choice kernel.", "year": 2011}