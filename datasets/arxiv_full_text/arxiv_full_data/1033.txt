{"title": "Combating Reinforcement Learning's Sisyphean Curse with Intrinsic Fear", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Many practical environments contain catastrophic states that an optimal agent would visit infrequently or never. Even on toy problems, Deep Reinforcement Learning (DRL) agents tend to periodically revisit these states upon forgetting their existence under a new policy. We introduce intrinsic fear (IF), a learned reward shaping that guards DRL agents against periodic catastrophes. IF agents possess a fear model trained to predict the probability of imminent catastrophe. This score is then used to penalize the Q-learning objective. Our theoretical analysis bounds the reduction in average return due to learning on the perturbed objective. We also prove robustness to classification errors. As a bonus, IF models tend to learn faster, owing to reward shaping. Experiments demonstrate that intrinsic-fear DQNs solve otherwise pathological environments and improve on several Atari games.", "text": "many practical environments contain catastrophic states optimal agent would visit infrequently never. even problems deep reinforcement learning agents tend periodically revisit states upon forgetting existence policy. introduce intrinsic fear learned reward shaping guards agents periodic catastrophes. agents possess fear model trained predict probability imminent catastrophe. score used penalize qlearning objective. theoretical analysis bounds reduction average return learning perturbed objective. also prove robustness classication errors. bonus models tend learn faster owing reward shaping. experiments demonstrate intrinsic-fear dqns solve otherwise pathological environments improve several atari games. following success deep reinforcement learning atari games board game researchers increasingly exploring practical applications. investigated applications include robotics dialogue systems energy management self-driving cars amid push apply might trust agents wild? agents acting society cause harm. self-driving might pedestrians domestic robot might injure child. agents might also cause self-injury atari lives lost inconsequential robots expensive. unfortunately feasible prevent catastrophes without requiring extensive prior knowledge moreover typical dqns providing large negative rewards solve problem soon catastrophic trajectories ushed replay buer updated q-function ceases discourage revisiting states. paper dene avoidable catastrophes states prior knowledge dictates optimal policy visit rarely never. additionally dene danger states—those catastrophic state reached small number steps assume optimal policy visit danger states rarely never. notion danger state might seem absent assumptions transition function. fully-connected transition matrix states danger states. however physical environments fully connected. cannot parked second underwater second later. work primarily addresses might prevent agents perpetually making mistakes. bonus show prior knowledge knowledge catastrophic states avoided accelerates learning. experiments show even simple problems classic deep q-network algorithm fails badly repeatedly visiting catastrophic states long continue learn. poses formidable obstacle using dqns real world. trust drl-based agent doomed periodically experience catastrophes remember exist? imagine self-driving periodically pedestrians remember undesirable. tabular setting agent never forgets learned dynamics environment even policy evolves. moreover markovian assumption holds convergence globally optimal policy guaranteed. however tabular approach becomes infeasible high-dimensional continuous state spaces. trouble dqns owes function approximation training successively update neural network based experiences. experiences might sampled online fashion trailing window uniformly past experiences. regardless mode train network eventually states learned policy never encounters come form innitesimally small region training distribution. times networks suer well-known problem catastrophic forgetting nothing prevents dqn’s policy drifting back towards revisits forgotten catastrophic mistakes. illustrate brittleness modern algorithms simple pathological problem called adventure seeker. problem consists one-dimensional continuous state actions simple dynamics admits analytic solution. nevertheless fails. show similar dynamics exist classic environment cart-pole. combat problems propose intrinsic fear algorithm. approach train supervised fear model predicts states likely lead catastrophe within steps. output fear model scaled fear factor penalizes q-learning target. crucially fear model maintains buers safe danger states. model never forgets danger states possible infrequency catastrophes. validate approach empirically theoretically. experiments address adventure seeker cartpole several atari games. environments label every lost life catastrophe. environments agents learns avoid catastrophe indenitely. seaquest experiments agent achieves higher reward asteroids agent achieves higher reward fewer catastrophes. improvement freeway dramatic. also make following theoretical contributions first prove reward bounded optimal policy rarely visits danger states optimal policy learned perturbed reward function approximately return optimal policy learned original value function. second prove method robust noise danger model. agent interacts environment markov decision process step agent observes state chooses action according policy environment transitions state according transition dynamics generates reward expectation cycle continues episode terminates. trt. temporal-dierences methods like q-learning model q-function gives optimal discounted total reward state-action pair. problems practical interest tend large state spaces thus q-function typically approximated parametric models neural networks. q-learning function approximation agent collects experiences acting greedily respect updates parameters updates proceed follows. given experience minimize squared bellman error q-learning methods also require exploration strategy action selection. simplicity consider \u0003-greedy heuristic. tricks help stabilize q-learning function approximation. notably experience replay agent maintains buer experiences experience update q-function. propose formulation suppose exists subset known catastrophe states/ assume given environment optimal policy rarely enters catastrophe states reachable short number steps. dene distance length smallest sequence transitions denition suppose priori knowledge acting according optimal policy agent rarely encounters states within distance catastrophe state state s.t. danger state. algorithm agent maintains separate supervised fear model provides auxiliary source reward penalizing q-learner entering likely danger states. case neural network architecture could sharing weights networks tricks relevant paper’s contribution. train fear model predict probability state lead catastrophe within moves. course training agent adds experience experience replay buer. whenever catastrophe reached turn episode preceding states danger buer. states episode safe buer. states episode added list danger states. turn addition updating q-network update fear model sampling states danger buer assigning label remaining safe buer assigning label input fear factor fear phase-in length fear radius output learned parameters initialize parameters randomly initialize replay buer danger state buer safe state buer start per-episode turn counter probability select random action otherwise select greedy action maxa execute action environment observing reward successor state store transition catastrophe state states st−kr states st−ne st−kr note perturbs objective function. thus might concerned perturbed reward might lead sub-optimal policy. fortunately show formally labeled catastrophe states danger zone violate assumptions fear model reaches arbitrarily high accuracy happen. average reward return follows optimal policy model policy maximizes average reward return maxπ stationary polices. theorem given catastrophe detector denote optimal policy denote optimal policy equipped fear model environment probability visits states danger zone words λ\u0003-optimal original mdp. proof. policy visits fear zone probability therefore applying environment intrinsic fear provides expected return least since exists policy expected return therefore optimal policy must result expected return least i.e. expected return decomposes parts expected return original environment expected return fear model. visits fear zone probability therefore applying promises expected return least lower bounded worth noting theorem holds optimal policy visit fear zone fear signal boost process learning optimal policy. since empirically learn fear model using collected data nite sample size agent access imperfect fear model therefore computes optimal policy based case agent trains intrinsic fear generated learning dierent value function agent perfect show robustness errors interested average deviation value functions agents. second main theoretical result given theorem allows agent smaller discount factor denoted γplan actual reduce planning horizon computation cost. moreover estimated model environment used jiang shows using smaller discount factor planning prevent over-tting estimated model. result demonstrates using smaller discount factor planning reduce reduction expected return estimated fear model used. denote specically given environment fear model discount factor state value function optimal policy environment fear model discount factor environment denote visitation distribution states policy interested average reduction expected return caused imperfect classier; respectably denote solved using iterative updates dynamic programing. i’th iteration dynamic programmings corresponding second equalities therefore state theorem holds nite continuous state-action mdps. course experiments discovered following pattern intrinsic fear models eective fear radius large enough model experience danger states safe distance correct policy without experiencing many catastrophes. fear radius small danger probability nonzero states catastrophes inevitable anyway intrinsic fear seems help. also found wider fear factors train stably phased course many episodes. experiments gradually phase fear factor reaching full strength predetermined time step demonstrate algorithms following environments adventure seeker pathological environment designed demonstrate catastrophic forgetting; cartpole classic environment; atari games seaquest asteroids freeway adventure seeker imagine player placed hill sloping upward right turn player move right left environment adjusts player’s position accordingly adding random noise. left right edges hill player gets reward spending time higher hill. player goes right fall terminating episode formally state single continuous variable denoting player’s position. starting position episode chosen uniformly random interval available actions consist given action state successor state produced .·at reward turn player falls hill entering catastrophic terminating state whenever game easy solve. exists threshold agent always choose left always right. agent periodically die. initially quickly learns good policy avoids catastrophe course continued training agent owing shape reward function collapses policy always moves right regardless state. might critically real-world scenario could depend upon system cannot solve adventure seeker. classic environment agent balances pole atop cart qualitatively cart-pole game exhibits four distinct catastrophe modes. pole could fall right fall left. additionally cart could right boundary screen left. formally time agent observes four-dimensional state vector consisting respectively cart position cart velocity pole angle pole’s angular velocity. time step agent chooses action applying force either every time step pole remains upright cart remains screen agent receives reward pole falls episode terminates giving return penultimate state. experiments implementation cartpole-v contained openai like adventure seeker problem admits analytic solution. perfect policy never drop pole. adventure seeker converges constant rate catastrophes turn. addition pathological cases address freeway asteroids seaquest games atari games atari learning environment. freeway agent controls chicken goal crossing road dodging trac. chicken loses life starts original location car. points rewarded successfully crossing road. asteroids agent pilots ship gains points shooting asteroids. must avoid colliding asteroids cost lives. seaquest player swims water. periodically oxygen gets must rise surface oxygen. additionally shes swim across screen. player gains points time shoots colliding first examples evaluate standard dqns intrinsic fear dqns using multilayer perceptrons single hidden layer hidden nodes. train mlps stochastic gradient descent using adam optimizer adventure seeker agent escape danger time steps notice fear radius phase fear factor quickly reaching full strength steps. figure catastrophes row) reward/episode dqns intrinsic fear. adventure seeker intrinsic fear models cease within runs giving unbounded reward thereafter. seaquest model achieves similar catastrophe rate signicantly higher total reward. asteroids model outperforms dqn. freeway randomly exploring never gets reward model learns successfully. problem fear factor cart-pole wider fear radius initially tried training model short fear radius made following observation runs if-dqn would surviving millions experiences runs might experience many catastrophes. manually examining fear model output successful unsuccessful runs noticed runs fear model outputs non-zero probability danger precisely moves catastrophe. cart-pole time correct course. successful runs fear model often outputs predictions range suspect gradation mildly dangerous states certain danger provides richer reward signal dqn. adventure seeker cart-pole environments dqns augmented intrinsic fear outperform otherwise identical counterparts. also compared traditional approaches mitigating catastrophic forgetting. example tried memory-based method preferentially sample catastrophic states updating model improve dqn. seems notion danger zone necessary here. seaquest asteroids freeway fear radius fear factor atari games models outperform counterparts. interestingly games models achieve higher reward seaquest if-dqns similar catastrophe rates perhaps if-dqn enters region policy space strong incentives exchange catastrophes higher reward. result suggests interplay various reward signals warrants exploration. asteroids freeway improvements dramatic. thousand episodes freeway randomly exploring achieves zero reward. however reward shaping intrinsic fear leads rapid improvement. paper studies safety intrinsically motivated stability q-learning function approximation distributional shift. work also connection reward shaping. attempt highlight relevant papers here. several papers address safety garcıa fernández provide thorough review topic identifying main classes methods perturb objective function external knowledge improve safety exploration. typical reinforcement learner optimizes expected return papers suggest safely acting agent also minimize risk. hans denes fatality return threshold propose solution comprised safety function identies unsafe states backup model navigates away states. work addresses tabular setting suggests agent minimize probability fatality instead maximizing expected return. heger suggests alternative q-learning objective concerned minimum return. papers suggest modifying objective penalize policies high-variance returns maximizing expected returns minimizing variance classic problem nance common objective ratio expected return standard deviation moreover azizzadenesheli suggests learn variance returns order make safe decisions decision step. moldovan abbeel give denition safety based ergodicity. consider fatality state cannot return start state. shalev-shwartz theoretically analyzes strong penalty discourage accidents. also consider hard constraints ensure safety. none works address case distributional shift dooms agent perpetually revisit known catastrophic failure modes. papers incorporate external knowledge exploration process. typically requires access oracle extensive prior knowledge environment. extreme case papers suggest conning policy search known subset safe policies. reasonably complex environments classes policies seems infeasible. potential oscillatory divergent behavior q-learners function approximation previously identied outside problem covariate shift extensively studied murata ozawa addresses problem catastrophic forgetting owing distributional shift function approximation proposing memory-based solution. many papers address intrinsic rewards internally assigned standard reward. typically intrinsic rewards used encourage exploration acquire modular skills papers refer intrinsic reward discovery curiosity. like classic work intrinsic motivation methods perturb reward function. instead assigning bonuses encourage discovery novel transitions assign penalties discourage catastrophic transitions. paper undertake novel treatment safe reinforcement learning dierences literature oers several notions safety reinforcement learning following problem existing safety research perturbs reward function requires little foreknowledge fundamentally changes objective globally. hand processes relying expert knowledge presume unreasonable level foreknowledge. moreover little prior work safe reinforcement learning best knowledge specically addresses problem catastrophic forgetting. paper proposes class algorithms avoiding catastrophic states theoretical analysis supporting robustness. experiments demonstrate dqns susceptible periodically repeating mistakes however raising questions real-world utility harm come actions. easy visualize problems examples similar dynamics embedded complex domains. consider domestic robot acting barber. robot might receive positive feedback giving closer shave. reward encourages closer contact steeper angle. course shape reward function belies catastrophe lurking past optimal shave. similar dynamics might imagines vehicle rewarded traveling faster could risk accident excessive speed. results intrinsic fear model suggest small amount prior knowledge simultaneously accelerate learning avoid catastrophic states. work step towards combating drl’s tendency revisit catastrophic states catastrophic forgetting.", "year": 2016}