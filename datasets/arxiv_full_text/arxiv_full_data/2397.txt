{"title": "Sample complexity of learning Mahalanobis distance metrics", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Metric learning seeks a transformation of the feature space that enhances prediction quality for the given task at hand. In this work we provide PAC-style sample complexity rates for supervised metric learning. We give matching lower- and upper-bounds showing that the sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution. However, by leveraging the structure of the data distribution, we show that one can achieve rates that are fine-tuned to a specific notion of intrinsic complexity for a given dataset. Our analysis reveals that augmenting the metric learning optimization criterion with a simple norm-based regularization can help adapt to a dataset's intrinsic complexity, yielding better generalization. Experiments on benchmark datasets validate our analysis and show that regularizing the metric can help discern the signal even when the data contains high amounts of noise.", "text": "metric learning seeks transformation feature space enhances prediction quality given task hand. work provide pac-style sample complexity rates supervised metric learning. give matching lowerupper-bounds showing sample complexity scales representation dimension assumptions made underlying data distribution. however leveraging structure data distribution show achieve rates ﬁne-tuned speciﬁc notion intrinsic complexity given dataset. analysis reveals augmenting metric learning optimization criterion simple norm-based regularization help adapt dataset’s intrinsic complexity yielding better generalization. experiments benchmark datasets validate analysis show regularizing metric help discern signal even data contains high amounts noise. many machine learning tasks data represented high-dimensional euclidean space dimension corresponds interesting measurement observation. often practitioners include variety measurements hopes combination features capture relevant information. natural represent data real space measurements reason expect using euclidean distances compare observations necessarily useful task hand. indeed presence uninformative mutually correlated measurements simply inﬂates l-distances pairs observations rendering distance-based comparisons ineffective. metric learning emerged powerful technique learn good notion distance metric representation space emphasize feature combinations help predication task suppressing contribution spurious measurements. past decade seen variety successful metric learning algorithms leverage various attributes problem domain. notable examples include exploiting class labels mahalanobis distance metric maximizes distance dissimilar observations minimizing distances between similar ones improve classiﬁcation quality despite popularity metric learning methods studies focused studying problem complexity scales attributes given dataset. instance expect generalization error scale—both theoretically practically—as varies number informative uninformative measurements changes noise levels? study supervised metric learning formally gain better understanding different modalities data affect metric learning problem. develop general frameworks pac-style analysis supervised metric learning. categorize popular metric learning algorithms empirical error minimization problem frameworks. ﬁrst generic framework distance-based metric learning framework uses class label information derive distance constraints. objective learn metric average yields smaller distances examples class different classes. popular algorithms optimize distance-based objectives include mahalanobis metric clustering xing information theoretic metric learning davis instead using distance comparisons proxy however also optimize speciﬁc prediction task directly. second generic framework classiﬁer-based metric learning framework explicitly incorporates hypothesis associated prediction task interest learn effective distance metrics. interesting examples regime include work mcfee lanckriet ﬁnds metrics improve ranking quality information retrieval tasks work shaw learns metrics help predict connectivity structure networked data. analysis shows frameworks sample complexity scales representation dimension given dataset dependence necessary absence speciﬁc assumptions underlying data distribution considering lipschitz loss results generalize previous sample complexity results ﬁrst time literature provide matching lower bounds. light observation made earlier data measurements often include uninformative weakly informative features expect metric yields good generalization performance deemphasize features accentuate relevant ones. thus formalize metric learning complexity given dataset terms intrinsic complexity metric reweights features yields best generalization performance. reﬁne sample complexity result show dataset-dependent bound frameworks scales dataset’s intrinsic metric learning complexity taking guidance dataset-dependent result propose simple variation empirical risk minimizing algorithm that given i.i.d. sample returns metric jointly minimizes observed sample bias expected intra-class variance metrics ﬁxed complexity bias-variance balancing algorithm viewed structural risk minimizing algorithm provides better generalization performance algorithm justiﬁes norm-regularization weighting metrics optimization criteria metric learning. finally evaluate practical efﬁcacy proposed norm-regularization criteria popular metric learning algorithms benchmark datasets experiments highlight norm-regularization indeed helps learning weighting metrics better adapt signal data high-noise regimes. preliminaries given representation space real-valued measurements observations interest goal metric learning learn metric minimizes notion error data drawn unknown underlying distribution speciﬁcally want metric class metrics consideration rd×d σmax supervised metric learning error typically label-based deﬁned multiple reasonable ways. discussed earlier explore intuitive regimes deﬁning error. distance-based error. popular criterion quantifying error metric learning comparing distances amongst points drawn underlying data distribution. ideally want weighting metric brings data class closer together opposite classes. distance-based framework natural accomplish weighting yields shorter distances pairs observations class different classes. penalizing often much distances violate constraints gives rise particular form error. variable denote random draw observation associated label denote severely wants penalize distance violations natural deﬁnition distance-based error becomes example instantiation popular literature encourages metrics yield distances upper limit observations class distances less lower limit different classes thus xing optimize efﬁciently computable variant criterion look metric keeps total pairwise distance amongst observations class less constant maximizing total pairwise distance amongst observations opposite classes. variant proposed davis explicitly includes upper lower limits added regularization learned close pre-speciﬁed metric interest pair opposite classes must obey triplet-based comparison typically focuses relative distances three observations time. natural instantiation case becomes weinberger saul discuss interesting variant this instead looking triplets given training sample focus triplets observations local neighborhoods learn metric maintains margin among distances observations class opposite class. improving quality distance comparisons local neighborhoods directly affects nearest neighbor performance making popular technique. classiﬁer-based error. distance comparisons typically surrogate speciﬁc downstream prediction task. want metric directly optimizes task need explicitly incorporate hypothesis class used task ﬁnding good weighting metric. simple effective insight used recently mcfee lanckriet improving ranking results information retrieval problems explicitly incorporating ranking losses learning effective weighting metric. shaw also follow principle explicitly include network topology constraints learn weighting metric better predict connectivity structure social networks. formalize classiﬁer-based metric learning framework considering ﬁxed hypothesis class interest measurement domain. keep discussion general shall assume hypotheses real-valued regarded measure conﬁdence classiﬁcation form then error induced particular weighting metric measurement space deﬁned best possible error obtained hypotheses denote sample size denote empirical error sample discussed later). deﬁne empirical risk minimizing metric based samples argminm err. practical algorithms course return approximation thus important compare generalization ability distance-based error analysis given i.i.d. sequence observations pair observations together form paired sample size deﬁne sample based distance error errλ lemma sample size i.i.d. paired sample size unknown bounded distribution distance-based loss function λ-lipschitz ﬁrst argument probability least draw sufﬁcient conclude conﬁdence least empirical risk minimizing metric estimation error shows never needs number proportional representation dimension examples achieve desired level accuracy. since typical applications large representation dimension instructive study strong dependency necessary. turns even simple distance-based loss functions like data distributions cannot away fewer linear samples ensure good estimation errors. particular following. lemma algorithm that given i.i.d. sample ﬁxed unknown bounded support distribution returns weighting metric minimizes empirical seem discouraging large-scale applications metric learning note made assumptions underlying structure data distribution making worst-case analysis. individual features real-world datasets contain varying amounts information good classiﬁcation performance hopes relaxed dependence metric learning settings. explored section classiﬁer-based error analysis setting i.i.d. sequence observations obtain sample {zi}m size directly. analyze generalization ability weighting metrics optimized respect underlying hypothesis class need effectively analyze classiﬁcation complexity scale sensitive version vc-dimension also known fat-shattering dimension real-valued hypothesis class encodes right notion classiﬁcation complexity provides intuitive relate generalization error empirical error margin excellent discussion). bounded support distribution following convergence result relates estimation error rate weighting metrics fat-shattering dimension underlying base hypothesis class. lemma λ-lipschitz base hypothesis class. pick fatγ/ probability least i.i.d. draw sample bounded unknown distribution interesting note task ﬁnding optimal metric additively increases sample complexity complexity ﬁnding optimal hypothesis underlying hypothesis class. contrast sample complexity distance-based framework quadratic dependence representation dimension. following lemma shows strong dependence representation dimension necessary absence speciﬁc assumptions underlying data distribution base hypothesis class. lemma pick base hypothesis class λ-lipschitz functions mapping interval closed addition constants. different measurements varying degrees information content particular supervised classiﬁcation task interest. algorithm analysis studies design effective comparisons observations must account variability. solid footing study introduce concept metric learning complexity given dataset. observation metric yields good generalization performance emphasize relevant features suppressing contribution spurious features. thus good metric reﬂects quality individual feature measurements data relative value learning task. leverage deﬁne metric learning complexity given dataset intrinsic complexity weighting metric yields best generalization performance dataset natural characterize intrinsic complexity weighting metric norm matrix representation using metric learning complexity gauge richness feature given dataset reﬁne analysis canonical metric learning frameworks. lemma class weighting metrics feature space sample size i.i.d. paired sample size unknown bounded distribution distance-based loss function λ-lipschitz ﬁrst argument probability least draw uniform upperbound frobenius norm quadratic form weighting metrics supm∈m observe dataset metric learning complexity considering appropriate class norm-bounded weighting metrics help sharpen sample complexity result yielding dataset-dependent bound. discuss automatically adapt right complexity class section below. effective data-dependent analysis classiﬁer-based metric learning requires accounting potentially complex interactions arbitrary base hypothesis class distortion induced weighting metric unknown underlying data distribution. make analysis tractable still keeping base hypothesis class general shall assume class layer feed-forward neural networks. recall smooth target function layer feed-forward neural network approximate arbitrarily well class ﬂexible enough incorporate reasonable target hypotheses. smooth strictly monotonic γ-lipschitz activation function generalization error weighting metric deﬁned respect classiﬁer-based λ-lipschitz loss function following. lemma class weighting metrics feature space h-net layer feed-forward neural network base hypothesis class classiﬁer-based loss function λ-lipschitz ﬁrst argument. sample size i.i.d. sample size unknown bounded distribution probability least note lemmas provide sample complexity bound tuned metric learning complexity given dataset results useful directly since cannot select correct norm bounded class priori fortunately considering appropriate sequence norm-bounded classes weighting metrics provide uniform bound automatically adapts intrinsic complexity unknown underlying data distribution particular following. corollary i.i.d. sample size unknown bounded distribution deﬁne consider nested sequence weighting metric class non-negative measure across sequence probability least observe measure encodes prior belief complexity class target metric selected metric learning algorithm given training sample absence prior beliefs simply unit spectral-norm weighting metrics. thus unknown underlying data distribution metric learning complexity result also highlights generalization error weighting metric returned algorithm proportional norm-bounded class belongs metrics similar empirical errors given sample different intrinsic complexities expected risk metrics considerably different. expect metric lower intrinsic complexity yield better generalization error. partly explains observed empirical success various types norm-regularized optimization criteria ﬁnding optimal weighting metric using guiding principle design improved optimization criteria metric learning problems jointly minimizes sample error frobenius norm regularization penalty. particular figure nearest-neighbor classiﬁcation performance lmnn itml metric learning algorithms withregularization regularization benchmark datasets. horizontal dotted line classiﬁcation error random label assignment drawn according class proportions solid gray line shows classiﬁcation error k-nn performance respect identity metric baseline reference. error criteria ‘err’ used downstream prediction task interest regularization hyper-parameter proportional m−/. explore practical efﬁcacy augmented optimization representative applications below. analysis shows generalization error metric learning scale representation dimension regularization help mitigate adapting intrinsic metric learning complexity given dataset. want explore degree effects manifest practice. select popular metric learning algorithms lmnn weinberger saul itml davis designed metrics improve nearest-neighbor classiﬁcation quality. algorithms varying degrees regularization built optimization criteria lmnn implicitly regularizes metric large margin criterion itml allows explicit regularization letting practitioners specify prior weighting metric. modiﬁed lmnn optimization criteria also allow explicit norm-regularization controlled trade-off parameter datasets. benchmark datasets experiments iris wine ionosphere datasets dataset ﬁxed intrinsic dimension; vary representation dimension augmenting dataset synthetic correlated noise varying dimensions simulating regimes datasets contain large numbers uninformative features. dataset augmented synthetic d-dimensional correlated noise follows. ﬁrst sample covariance matrix unit-scale wishart distribution drawn i.i.d. ata). sample dataset appended independently drawing noise vector added datasets creating noise-augmented datasets. noise-augmented dataset randomly split training validation test samples. used default settings algorithm. regularized lmnn picked best performing trade-off parameter validation set. regularized itml seeded rank-one discriminating metric i.e. prior matrix zeros except diagonal entry corresponding discriminating coordinate one. results. figure shows nearest-neighbor performance lmnn itml noise-augmented datasets. notice unregularized versions algorithms scale poorly noisy features introduced. number uninformative features grows performance algorithms quickly degrades classiﬁcation performance original unweighted space metric learning showing poor adaptability signal data. interestingly neither unregularized algorithms performs consistently better datasets high noise itml yields better results wine whereas lmnn seems better ionosphere algorithms yield similar performance iris. regularized versions algorithms signiﬁcantly improve classiﬁcation performance. remarkably regularized itml shows almost degradation classiﬁcation performance even high noise regimes demonstrating strong robustness noise. previous theoretical work metric learning focused almost exclusively analyzing generalization error variants optimization criteria distance-based metric learning framework. instance analyzed generalization ability regularized convex-loss optimization criteria pairwise distances algorithmic stability analysis. derive datasets representation dimeninteresting sample complexity result sublinear sion discuss sample complexity potentially independent characterize speciﬁc instances classes problems possible. likewise recent work bellet habrard uses algorithmic robustness analyze generalization ability pairwisetriplet-based distance metric learning. analysis relies existence partition input space cell partition training loss test loss deviate much note sample complexity bound scales partition size general exponential representation dimension. perhaps works similar approach sample complexity analyses bian bian analyze consistency criterion metric learning. show rate convergence samples expected risk thresholds bounded convex losses distance-based metric learning. upper-bound lemma generalizes result considering arbitrary distance-based lipschitz losses explicitly shows dependence representation dimension provide alternate analysis based norm regularization weighting metric distance-based metric learning. result parallels norm-regularized criterion worth emphasizing none related works discuss importance leverage intrinsic structure data metric learning problem. results section formalize intuitive notion dataset’s intrinsic complexity metric learning show sample complexity rates ﬁnely tuned metric learning complexity. classiﬁer-based framework discuss parallels kernel learning literature. typical focus kernel learning analyze generalization ability hypothesis class linear separators general hilbert spaces work provides complementary analysis learning explicit linear transformations given representation space arbitrary hypotheses classes. theoretical analysis partly justiﬁes empirical success norm-based regularization well. empirical results show regularization helps designing metric learning algorithms even beneﬁt existing metric learning algorithms high-noise regimes.", "year": 2015}