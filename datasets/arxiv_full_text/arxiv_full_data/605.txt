{"title": "Training Language Models Using Target-Propagation", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "While Truncated Back-Propagation through Time (BPTT) is the most popular approach to training Recurrent Neural Networks (RNNs), it suffers from being inherently sequential (making parallelization difficult) and from truncating gradient flow between distant time-steps. We investigate whether Target Propagation (TPROP) style approaches can address these shortcomings. Unfortunately, extensive experiments suggest that TPROP generally underperforms BPTT, and we end with an analysis of this phenomenon, and suggestions for future work.", "text": "concretely treat hidden states free variables optimized independently encouraged predictable previous hidden states. formulating model offers approach avoiding sequential nature bptt training allowing optimization parameters hidden states simultaneously entire data easily parallelized. extensively evaluate applying tprop train language models batch tprop effective batch bptt minimizing training loss fail generalize well; mini-batch tprop achieves comparable generalization performance bptt limit tprop reduces mere bptt; relatively unconstrained nature hidden state optimization appears responsible performance degradation. non-linear parametric functions inputs. instance elman language modeling might speciﬁed deﬁning softmax denotes logistic sigmoid function. typical back-propagation time popular approach training recurrent neural networks suffers inherently sequential truncating gradient distant time-steps. investigate whether target propagation style approaches address shortcomings. unfortunately extensive experiments suggest tprop generally underperforms bptt analysis phenomenon suggestions future work. modern rnns trained almost exclusively using truncated back-propagation time despite popularity bptt training major drawbacks namely inherently sequential truncates number time-steps gradient information propagate. inspired recent reports success target-propagation training nonrecurrent deep networks explore training rnns tprop idea suggested repeatedly literature tprop understood generalization backpropagation neural networks trained providing local targets hidden unit. approaches motivated appealing biological plausibility numerical stability computational parallelizability conduciveness constrained training time-steps effect instantiating implicitly instead treat explicit variables optimized over. order maintain recurrent property model however additional constraint terms loss encourage adjacent hidden states approximately obey parametric recurrence. penalty constraint function introduced force predicted hidden state time step close free hidden state coefﬁcient whose value governs strictly wish enforce constraint. approach referred targetpropagation serve targets optimization. figure reference. given well dataset consisting input sequence desired output sequence possible obtain loss entire dataset terms parameters unrolling time depicted figure particular typically loss equation optimized batch. rather sequence unrolled window steps gradient step taken minimize loss steps. updating parameters minimization continues unrolling next window steps initializing value k’th hidden state previous window minimizing loss window continuing dataset way. crucially window considered independent previous ones gradients calculated respect time-steps previous windows. going forward refer loss induced windowing truncation truncated bptt loss. issues truncated bptt truncated bptt loss associated minimization procedure suffers problems. first training algorithm inherently sequential cannot process consecutive spans size parallel since processing k-window depends processed previous k-window makes parallelization impossible. hard truncation gradients beyond steps note many choices architecture impossible perform either θminimization h-minimization analytically. such simply gradient-based algorithms ﬁxed number steps necessary minimize convergence. also consider alternatives algorithm. ﬁrst refer penalty method identical admm except avoids dual variables entirely skips step second augmented langrangian method minimizes jointly updating dual variables effectively merging steps found admm outperforms minimally outperforms well report admm results follows. exceptions include various forms multiplicative rnns non-linearity. case alternating minimizations carried least squares. experimented along lines found methods underperform gradient based approaches also much ﬂexible. also consider generalization tprop loss instead making free variable free variable every time steps. remaining hidden states deﬁned implicitly recurrence constrains model training. refer sub-sequence length consisting timesteps beginning free variable ending before next free variable block. note block size recover tprop formulation previous subsection. beneﬁts btprop emphasize btprop loss offers approach addressing issues truncated bptt loss identiﬁed section first independence nonboundary time steps different blocks suggests training efﬁciently parallelized distributing large number blocks machine cluster would necessitate inter-machine communication small number border blocks different machine. note however since parameters shared across time-steps scheme offer speed-up must also case optimized independently h-optimization results faster convergence θ-optimization. address second issue bptt identiﬁed section note penalty terms encourage ﬁnal hidden state block close initial hidden state subsequent block allow capturing dependencies multiple blocks training. also note restricting block size restrict temporal dependence loss given thereby mitigating vanishing exploding gradient problem. finally note expect btprop offer advantage tprop since leads constrained optimization problem allows intra-block bptt become relatively mature technology. report perplexity obtained validation dataset freezing ﬁnal parameters obtained alternating optimization process; optimization done test time. results report main results table compare btprop validation perplexity performance various block-sizes various numbers steps bptt. report validation numbers training performance between btprop bptt generally comparable. starting left portion table btprop roughly comparable bptt bigger importantly however btprop performance tend improve additional h-steps. disappointing shown btprop single h-step essentially equivalent bptt; supplemental material rigorous formulation proof. furthermore clear table batch btprop batch bptt significantly outperformed minibatch counterparts presumably regularization effect updating parameters seeing subset data discussion phenomenon). suggests even batch btprop easily parallelized would worth decreased generalization. consider right portions table involve minibatch btprop. note applying btprop minibatch setting straightforward since penalty term involve hidden states updated since previous epoch therefore provide reasonable targets. address initializing hidden states current minibatch given current parameters starting h-optimization. also emphasize minibatch setting little hope parallelization gain; possible however btprop loss still allow model beneﬁt training longer-range dependencies. unfortunately results suggest btprop gains little bptt moreover additional h-steps general beneﬁcial. additional experiments summarized table suggest small btprop performance gains minibatch setting attributable regularization effect penalty rather training longer dependencies. there compare validation obtained minibatch h-steps h-optimization perplexity h-optimization also setting perplexity). stephen boyd neal parikh eric borja peleato jonathan eckstein. distributed optimization statistical learning alternating direction method multipliers. foundations trends machine learning kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. daniel gabay bertrand mercier. dual algorithm solution nonlinear variational problems ﬁnite element approximation. computers mathematics applications l’approximation ´el´ements ﬁnis d’ordre r´esolution p´enalisation-dualit´e d’une classe revue probl`emes dirichlet lin´eaires. franc¸aise d’automatique informatique recherche op´erationnelle. analyse num´erique nitish shirish keskar dheevatsa mudigere jorge nocedal mikhail smelyanskiy ping peter tang. large-batch training deep learning generalization sharp minima. arxiv. possible explanation future work explanation h-optimization hurting performance relatively unconstrained nature allow ﬁnding hidden states decrease training loss without leading parameters generalize. indeed many reported successes tprop-style training involved constrained problems binary hidden-state constraints sparsity constraints test hypothesis figure plot perplexity obtained btprop well perplexity obtained bptt dimensionality hidden state increases. bptt actually underperforms btprop small hidden states improves relatively hidden state size increases lending support hypothesis. similar pattern emerges comparing btprop bptt suggests getting btprop work larger hidden states involve ﬁnding additional approaches constraining hoptimization instance penalizing divergence distributions softmax softmax leave future work. connection bptt tprop show using penalty method loss btprop h-steps θ-steps results gradients respect equal gradients respect obtained bptt initialized updated vanilla gradient descent. dong-hyun saizheng zhang asja fischer yoshua bengio. difference target propagajoint european conference machine tion. learning knowledge discovery databases. springer pages piotr mirowski yann lecun. dynamic facmachine graphs time series modeling. learning knowledge discovery databases springer isbn--- volume pages nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research gavin taylor ryan burmeister zheng bharat singh ankit patel goldstein. training neural networks without gradients scalable admm approach. proceedings icml. pages obvious approach speed training consists increasing mini-batch size sgd. unfortunately results table suggest performance deteriorates training test sets whenever minibatch size enough leverage parallel computation ﬁndings line ﬁnding section batch bptt batch btprop signiﬁcantly inferior mini-batch counterparts. table varying mini-batch size training bptt using wiki-large dataset word level text dataset character level. wikilarge used layer lstm hidden units unrolled steps. text dataset used layer lstm hidden units unrolled time steps. cases increasing mini-batch size increases perplexity training test sets.", "year": 2017}