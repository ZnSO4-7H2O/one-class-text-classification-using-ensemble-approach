{"title": "Feature Learning in Deep Neural Networks - Studies on Speech Recognition  Tasks", "tag": ["cs.LG", "cs.CL", "cs.NE"], "abstract": "Recent studies have shown that deep neural networks (DNNs) perform significantly better than shallow networks and Gaussian mixture models (GMMs) on large vocabulary speech recognition tasks. In this paper, we argue that the improved accuracy achieved by the DNNs is the result of their ability to extract discriminative internal representations that are robust to the many sources of variability in speech signals. We show that these representations become increasingly insensitive to small perturbations in the input with increasing network depth, which leads to better speech recognition performance with deeper networks. We also show that DNNs cannot extrapolate to test samples that are substantially different from the training examples. If the training data are sufficiently representative, however, internal features learned by the DNN are relatively stable with respect to speaker differences, bandwidth differences, and environment distortion. This enables DNN-based recognizers to perform as well or better than state-of-the-art systems based on GMMs or shallow networks without the need for explicit model adaptation or feature normalization.", "text": "recent studies shown deep neural networks perform signiﬁcantly better shallow networks gaussian mixture models large vocabulary speech recognition tasks. paper argue improved accuracy achieved dnns result ability extract discriminative internal representations robust many sources variability speech signals. show representations become increasingly insensitive small perturbations input increasing network depth leads better speech recognition performance deeper networks. also show dnns cannot extrapolate test samples substantially different training examples. training data sufﬁciently representative however internal features learned relatively stable respect speaker differences bandwidth differences environment distortion. enables dnn-based recognizers perform well better state-of-the-art systems based gmms shallow networks without need explicit model adaptation feature normalization. automatic speech recognition active research area decades. however performance systems still satisfactory human speech recognition still large tasks. primary reasons speech recognition challenging high variability speech signals. example speakers different accents dialects pronunciations speak different styles different rates different emotional states. presence environmental noise reverberation different microphones recording devices results additional variability. complicate matters sources variability often nonstationary interact speech signal nonlinear way. result virtually impossible avoid degree mismatch training testing conditions. conventional speech recognizers hidden markov model acoustic state modeled gaussian mixture model model parameters discriminatively trained using objective function maximum mutual information minimum phone error rate systems known susceptible performance degradation even mild mismatch training testing conditions encountered. combat this variety techniques developed. example mismatch speaker differences reduced vocal tract length normalization nonlinearly warps input feature vectors better match acoustic model maximum likelihood linear regression adapt parameters representative test data. techniques vector taylor series adaptation designed address mismatch caused environmental noise channel distortion methods successful degree complexity latency decoding process. require multiple iterations decoding perform well ample adaptation data making unsuitable systems process short utterances voice search. recently alternative acoustic model based deep neural networks proposed. model collection gaussian mixture models replaced single context-dependent deep neural network number research groups obtained strong results variety large scale speech tasks using approach temporal structure maintained refer models cd-dnn-hmm acoustic models. paper analyze performance dnns speech recognition particular examine ability learn representations robust variability acoustic signal. interpret joint model combining nonlinear feature transformation loglinear classiﬁer. using view show many layers nonlinear transforms convert features highly invariant discriminative representation effectively classiﬁed using log-linear model. internal representations become increasingly insensitive small perturbations input increasing network depth. addition classiﬁcation accuracy improves deeper networks although gain layer diminishes. however also dnns unable extrapolate test samples substantially different training samples. series experiments demonstrates training data sufﬁciently representative learns internal features relatively invariant sources variability common speech recognition speaker differences environmental distortions. enables dnn-based speech recognizers perform well better state-of-the-art gmm-based systems without need explicit model adaptation feature normalization algorithms. rest paper organized follows. section brieﬂy describe dnns illustrate feature learning interpretation dnns. section show dnns learn invariant discriminative features demonstrate empirically higher layer features less sensitive perturbations input. section point feature generalization ability effective test samples small perturbations training samples. otherwise dnns perform poorly indicated mixed-bandwidth experiments. apply analysis speaker adaptation section deep networks learn speaker-invariant representations aurora noise robustness task section show achieve performance equivalent current state without requiring explicit adaptation environment. conclude paper section deep neural network conventional multi-layer perceptron many hidden layers input output denoted respectively interpreted directed graphical model approximates posterior probability py|x class given observation vector stack layers log-linear models. ﬁrst layers model posterior probabilities hidden binary vectors given input vectors consists hidden units denoted posterior probability expressed represent weight matrix bias vector layer respectively. observation propagated forward network starting lowest layer output variables layer become input variables next i.e. ﬁnal layer class posterior probabilities computed multinomial distribution estimation posterior probability py|x also considered twostep deterministic process. ﬁrst step observation vector transformed another feature vector layers non-linear transforms.in second step posterior probability py|x estimated using log-linear model given transformed feature vector consider ﬁrst layers ﬁxed learning parameters softmax layer equivalent training conditional maximum-entropy model features conventional maxent model features manually designed dnns however feature representations jointly learned maxent model data. eliminates tedious potentially erroneous process manual feature extraction also potential automatically extract invariant discriminative features difﬁcult construct manually. following discussions dnns framework cd-dnn-hmm speech recognition classiﬁcation task. detailed training procedure decoding technique cd-dnn-hmms found using dnns instead shallow mlps component success cd-dnn-hmms. table extracted summarizes word error rates switchboard hub’-swb test set. switchboard corpus conversational telephone speech. system trained using -hour training labels generated viterbi alignment maximum likelihood trained gmm-hmm system. labels correspond tied-parameter context-dependent acoustic states called senones. baseline corresponding discriminatively trained traditional gmm-hmm system best cddnn-hmm achives .%—a relative error reduction observe deeper networks outperform shallow ones. decreases number hidden layers increases using ﬁxed layer size hidden units. words deeper models stronger discriminative ability shallow models. also reﬂected improvement training criterion interestingly architectures equivalent number parameters compared deep models consistently outperform shallow models deep model sufﬁciently wide layer. reﬂected right column table shows performance shallow networks number parameters deep networks left column. even increase size single hidden layer hidden units achieve signiﬁcantly worse obtained using conditions. note number hidden layers increases limited additional gains obtained performance saturates hidden layers. performs equally well parameters. practice tradeoff needs made width layer additional reduction increased cost training decoding number hidden layers increased. noticed biggest beneﬁt using dnns shallow models dnns learn invariant discriminative features. many layers simple nonlinear processing generate complicated nonlinear transform. show nonlinear transform robust small variations input features let’s assume output layer equivalently input layer changed small change. change cause output layer equivalently input layer change refers element-wise product. note magnitude majority weights typically small size hidden layer large. example trained using hours data weights layers except input layer magnitudes less element less equal actual value typically much smaller. means large percentage hidden neurons active shown figure result average norm diag+ +)))t across development smaller layers indicated figure since hidden layer values bounded range indicates small perturbation input perturbation shrinks higher hidden layer. words features generated higher hidden layers invariant variations represented lower layers. note maximum norm development larger seen figure necessary since differences need enlarged around class boundaries discrimination ability. result applicable small perturbations around training samples. test samples deviate signiﬁcantly training samples dnns cannot accurately classify them. words dnns must examples representative variations data training order generalize similar variations test data. demonstrate point using mixed-bandwidth study. typical speech recognizers trained either narrowband speech signals recorded wideband speech signals recorded khz. would advantageous single system could recognize narrowband wideband speech i.e. mixed-bandwidth asr. system recently proposed using cd-dnn-hmm work following architecture used experiments. input features mel-scale ﬁlter-bank outputs together dynamic features. -frame context window used generating input layer nodes. hidden layers nodes. output layer nodes corresponding number senones determined system. -dimensional ﬁlter bank parts ﬁrst ﬁlters span last ﬁlters span center frequency ﬁrst ﬁlter higher ﬁlter bank khz. speech wideband ﬁlters observed values. however speech narrowband high-frequency information captured ﬁnal ﬁlters figure illustrates architecture mixed-bandwidth system. experiments conducted mobile voice search corpus. task consists internet search queries made voice smartphone.there training sets consisting hours wideband audio data respectively. sets collected table summarizes wideband narrowband test sets trained without narrowband speech. table clear training data wideband performs well wideband test poorly narrowband test however convert narrowband speech train using mixed-bandwidth data performs well wideband narrowband speech. understand difference scenarios take output vectors layer wideband narrowband input feature pairs measure euclidean distance. layer whose output senone posterior probability calculate kl-divergence nats py|x py|x. table shows statistics frames randomly sampled test trained using wideband speech trained using mixed-bandwidth speech. table euclidean distance output vectors hidden layer divergence posteriors layer narrowband wideband input features measured using wideband mixed-bandwidth dnn. table observe dnns distance hidden layer vectors generated wideband narrowband input feature pair signiﬁcantly reduced layers close output layer compared ﬁrst hidden layer. perhaps interesting average distances variances data-mixed consistently smaller trained wideband speech only. indicates using mixed-bandwidth training data learns consider differences wideband narrowband input features irrelevant variations. variations suppressed many layers nonlinear transformation. ﬁnal representation thus invariant variation still ability distinguish different class labels. behavior even obvious output layer since kl-divergence paired outputs mixed-bandwidth much smaller observed wideband dnn. major source variability variation across speakers. techniques adapting gmm-hmm speaker investigated decades. important techniques vtln featurespace mllr vtln fmllr operate features directly making application context straightforward. vtln warps frequency axis ﬁlterbank analysis account fact precise locations vocal-tract resonances vary roughly monotonically physical size speaker. done training testing. hand fmllr applies afﬁne transform feature frames adaptation data better matches model. cases including work ‘self-adaptation’ used generate labels using unsupervised transcription re-recognize adapted model. process iterated four times. gmm-hmms fmllr transforms estimated maximize likelihood adaptation data given model. dnns instead maximize cross entropy discriminative criterion prefer call transform feature-space discriminative linear regression note transform applied individual frames prior concatenation. typically applying vtln fmllr jointly gmm-hmm system reduce errors initially similar gains expected dnns well. however gains realized shown table table compares vtln fmllr/fdlr gmm-hmms context-dependent ann-hmm single hidden layer deep network hidden layers switchboard task described section task test data consistent training thus small amount adaptation factors recording conditions environmental factors occurs. conﬁguration table speaker independent using single-pass decoding. gmm-hmm vtln achieves strong relative gain vtln also effective shallow neural-network system gaining slightly smaller however improvement vtln deep network hidden layers much smaller gain. combining vtln fdlr reduces relative gmm-hmm shallow network respectively. reduction also tried transplanting vtln fmllr transforms estimated system achieved similar results vtln fdlr implementations shallow deep networks identical. thus conclude signiﬁcant degree deep neural network able learn internal representations invariant respect sources variability vtln fdlr address. many speech recognition tasks often cases despite presence variability training data signiﬁcant mismatch training test data persists. environmental factors common sources mismatch e.g. ambient noise reverberation microphone type capture device. analysis previous sections suggests dnns ability generate internal representations robust respect variability seen training data. section evaluate extent invariance obtained respect distortions caused environment. performed series experiments aurora corpus -word vocabulary task based wall street journal corpus. experiments performed multi-condition training consisting utterances speakers. half utterances recorded high-quality close-talking microphone half recorded using different secondary microphones. halves include combination clean speech speech corrupted different types noise range signal-to-noise ratios evaluation consists utterances speakers. test recorded primary microphone number secondary microphones. sets corrupted noises used training snrs creating total test sets. test sets grouped subsets based type distortion none additive noise only channel distortion only noise channel. notice types noise common across training test sets snrs data not. trained using -dimensional ﬁlterbank features utterance-level mean normalization. ﬁrstsecond-order derivative features appended static feature vectors. input layer formed context window frames creating input layer input units. hidden layers hidden units layer ﬁnal softmax output layer units corresponding senones baseline system. network initialized using layer-by-layer generative pre-training discriminatively trained using back propagation. table performance obtained acoustic model compared several systems. ﬁrst system baseline gmm-hmm system remaining systems representative state acoustic modeling noise speaker adaptation. used training set. authors’ knowledge best published results task. second system combines minimum phone error discriminative training noise adaptive training using adaptation compensate noise channel mismatch third system uses hybrid generative/discriminative classiﬁer follows first adaptively trained adaptation used generate features based state likelihoods derivatives. then features input discriminative log-linear model obtain ﬁnal hypothesis. fourth system uses trained combines adaptation environment compensation mllr speaker adaptation finally last table shows performance system. noteworthy obtain good performance gmm-based systems required complicated adaptive training procedures multiple iterations recognition order perform explicit environment and/or speaker adaptation. systems required classiﬁers. contrast system required standard training single forward pass classiﬁcation. outperforms systems perform environment adaptation matches performance system adapts environment speaker. finally recall results section trained wideband data could accurately classify narrowband speech. similarly trained clean speech ability learn internal features robust environmental noise. aurora trained using clean speech examples performance noisechannel-distorted speech degrades substantially resulting average conﬁrms earlier observation dnns robust modest variations training test data perform poorly mismatch severe. dnns less sensitive small perturbations input features. property enables dnns generalize better shallow networks enables cd-dnn-hmms perform speech recognition manner robust mismatches speaker environment bandwidth. hand dnns cannot learn something nothing. require seeing representative samples perform well. using multi-style training strategy letting dnns generalize similar patterns equaled best result ever reported aurora noise robustness benchmark task without need multiple recognition passes model adaptation. deng dahl roles pretraining ﬁne-tuning context-dependent dbn-hmms real-world speech recognition proc. nips workshop deep learning unsupervised feature learning g.e. dahl deng acero context-dependent pretrained deep neural networks large vocabulary speech recognition ieee trans. audio speech lang. proc. vol. jan. jaitly nguyen senior vanhoucke application pretrained deep neural networks large vocabulary conversational speech recognition tech. rep. tech. rep. department computer science university toronto sainath kingsbury ramabhadran fousek novak mohamed making deep belief networks effective large vocabulary continuous speech recognition proc. asru flego gales discriminative adaptive training proc. asru ragni gales derivative kernels noise robust proc. asru y.-q. wang gales speaker noise factorisation robust speech recognition ieee", "year": 2013}