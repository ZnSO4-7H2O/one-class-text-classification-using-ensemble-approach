{"title": "VOI-aware MCTS", "tag": ["cs.AI", "cs.LG"], "abstract": "UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS) in games and Markov decision processes, is based on UCB1, a sampling policy for the Multi-armed Bandit problem (MAB) that minimizes the cumulative regret. However, search differs from MAB in that in MCTS it is usually only the final \"arm pull\" (the actual move selection) that collects a reward, rather than all \"arm pulls\". In this paper, an MCTS sampling policy based on Value of Information (VOI) estimates of rollouts is suggested. Empirical evaluation of the policy and comparison to UCB1 and UCT is performed on random MAB instances as well as on Computer Go.", "text": "random reward unknown stationary distribution encountered. cumulative setting encountered rewards collected. shown near-optimal respect. extension mcts described shown outperform many state search algorithms adversarial search simple regret setting agent gets collect reward last pull. different scheme control sampling principles bounded rationality rational metareasoning search maintains current best action ﬁnds expected gain ﬁnding another action better current best. upper bounds value information intrinsic pulling expected decrease regret compared selecting best without pulling all. cases possible highest sample mean pulled another pulled becomes higher myopic estimate limited applicability monte-carlo sampling since effect single sample small myopic estimate often zero. however common case ﬁxed budget samples node estimated intrinsic pulling rest budget. denote current number samples remaining number samples abstract. state-of-the algorithm monte carlo tree search games markov decision processes based sampling policy multi-armed bandit problem minimizes cumulative regret. however search differs mcts usually ﬁnal pull collects reward rather pulls. paper mcts sampling policy based value information estimates rollouts suggested. empirical evaluation policy comparison performed random instances well computer mcts especially appears numerous search applications although methods shown successful empirically authors appear using because shown successful past because good trading exploration exploitation. latter statement correct multi-armed bandit problem algorithm argue simple reconsideration basic principles result schemes outperform uct. core issue mcts adversarial search search games nature goal typically best ﬁrst action good policy closer minimizing simple regret rather cumulative regret minimized ucb. however simple cumulative regret cannot minimized simultaneously; moreover shows many cases smaller cumulative regret greater simple regret. begin background deﬁnitions related work. estimates pulls presented voi-aware sampling policy suggested simple regret mcts. finally performance proposed sampling policy evaluated sets bernoulli arms computer showing improved performance. monte-carlo tree search initially suggested scheme ﬁnding approximately optimal policies markov decision processes mcts explores performing rollouts— trajectories current state state termination condition satisﬁed powerful engine pachi still strong player; hand additional features advanced engines would obstruct mcts phenomena subject experiment.) engines compared board summary future work work suggested monte-carlo sampling policy sample selection based upper bounds value information. empirical evaluation showed policy outperforms heuristic algorithms pure exploration well mcts. mcts still remains largely unexplored ﬁeld application voi-aware algorithms. elaborate estimates taking consideration re-use samples future search states considered. policy introduced paper differs algorithm ﬁrst step voi-aware decisions made. consistent application principles rational metareasoning steps rollout improve sampling. voi-based sample allocation following principles rational metareasoning pure exploration multi-armed bandits highest pulled step. upper bounds established corollary used estimates. mcts pure exploration takes place ﬁrst step rollout action highest utility must chosen. mcts differs pure exploration multiarmed bandits distributions rewards stationary. however estimates computed stationary distributions work well practice. illustrated empirical evaluation estimates based upper bounds result rational sampling policy exceeding performance stateof-the-art heuristic algorithms. sampling policies ﬁrst compared random multi-armed bandit problem instances. figure shows results randomlygenerated multi-armed bandits bernoulli arms mean rewards arms distributed uniformly range range sample budgets multiplicative step experiment number samples repeated times. always considerably worse voi-aware sampling policy. playing policies also compared computer search domain uct-based mcts particularly successful modiﬁed version pachi state program used experiments. engine extended voiaware sampling policy time allocation mode ensuring original policy voi-aware policy average number samples node added. (while engine", "year": 2012}