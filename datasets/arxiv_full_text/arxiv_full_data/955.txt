{"title": "How to Construct Deep Recurrent Neural Networks", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "In this paper, we explore different ways to extend a recurrent neural network (RNN) to a \\textit{deep} RNN. We start by arguing that the concept of depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, however, we find three points of an RNN which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. Based on this observation, we propose two novel architectures of a deep RNN which are orthogonal to an earlier attempt of stacking multiple recurrent layers to build a deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an alternative interpretation of these deep RNNs using a novel framework based on neural operators. The proposed deep RNNs are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNNs benefit from the depth and outperform the conventional, shallow RNNs.", "text": "paper explore different ways extend recurrent neural network deep rnn. start arguing concept depth clear feedforward neural networks. carefully analyzing understanding architecture however three points made deeper; input-to-hidden function hidden-tohidden transition hidden-to-output function. based observation propose novel architectures deep orthogonal earlier attempt stacking multiple recurrent layers build deep provide alternative interpretation deep rnns using novel framework based neural operators. proposed deep rnns empirically evaluated tasks polyphonic music prediction language modeling. experimental result supports claim proposed deep rnns beneﬁt depth outperform conventional shallow rnns. recurrent neural networks recently become popular choice modeling variable-length sequences. rnns successfully used various task language modeling learning word embeddings online handwritten recognition speech recognition work explore deep extensions basic rnn. depth feedforward models lead expressive models believe hold recurrent models. claim that unlike case feedforward neural networks depth ambiguous. sense consider existence composition several nonlinear computational layers neural network deep rnns already deep since expressed composition multiple nonlinear layers unfolded time. schmidhuber hihi bengio earlier proposed another building deep stacking multiple recurrent hidden states other. approach potentially allows hidden state level operate different timescale nonetheless notice aspects model still considered shallow. instance transition consecutive hidden states single level shallow viewed separately.this implications kind transitions model represent discussed section shallow part propose alternative deeper design leads number deeper variants rnn. proposed deeper variants empirically evaluated sequence modeling tasks. layout paper follows. section brieﬂy introduce concept rnn. section explore different concepts depth rnns. particular section ..–.. propose novel variants deep rnns evaluate empirically section tasks polyphonic music prediction language modeling. finally discuss shortcomings advantages proposed models section recurrent neural network neural network simulates discrete-time dynamical system input output hidden state notation subscript represents time. dynamical system deﬁned respectively transition input output matrices element-wise nonlinear functions. usual saturating nonlinear function logistic sigmoid function hyperbolic tangent function illustration fig. deep learning built around hypothesis deep hierarchical model exponentially efﬁcient representing functions shallow number recent theoretical results support hypothesis instance shown delalleau bengio deep sum-product network require exponentially less units represent function compared shallow sum-product network. furthermore wealth empirical evidences supporting hypothesis ﬁndings make suspect argument apply recurrent neural networks. depth deﬁned case feedforward neural networks multiple nonlinear layers input output. unfortunately deﬁnition apply trivially recurrent neural network temporal structure. instance unfolded time fig. deep computational path input time output time crosses several nonlinear layers. close analysis computation carried time step individually however shows certain transitions deep results linear projection followed element-wise nonlinearity. clear hidden-to-hidden hiddento-output input-to-hidden functions shallow sense exists intermediate nonlinear hidden layer. consider different types depth considering transitions separately. make hidden-to-hidden transition deeper intermediate nonlinear layers consecutive hidden states time hidden-tooutput function made deeper described previously plugging multiple intermediate nonlinear layers hidden state output choices different implication. model exploit non-temporal structure input making input-to-hidden function deep. previous work shown higher-level representations deep networks tend better disentangle underlying factors variation original input ﬂatten manifolds near data concentrate hypothesize higher-level representations make easier learn temporal structure successive time steps relationship abstract features generally expressed easily. been instance illustrated recent work showing word embeddings neural language models tend related temporal neighbors simple algebraic relationships type relationship holding different regions space allowing form analogical reasoning. approach making input-to-hidden function deeper line standard practice replacing input extracted features order improve performance machine learning model recently chen deng reported better speech recognition performance could achieved employing strategy although jointly train deep input-to-hidden function together parameters rnn. deep hidden-to-output function useful disentangle factors variations hidden state making easier predict output. allows hidden state model compact result model able summarize history previous inputs efﬁciently. denote deep hidden-to-output function deep output figure illustrations four different recurrent neural networks conventional rnn. deep transition rnn. dt-rnn shortcut connections deep transition deep output rnn. stacked third knob play depth hidden-to-hidden transition. state transition consecutive hidden states effectively adds input summary previous inputs represented ﬁxed-length hidden state. previous work rnns generally limited architecture shallow operation; afﬁne transformation followed element-wise nonlinearity. instead argue procedure constructing summary hidden state combination previous input highly nonlinear. nonlinear transition could allow instance hidden state rapidly adapt quickly changing modes input still preserving useful summary past. impossible modeled function family generalized linear models. however highly nonlinear transition modeled hidden layers universal approximator property approach deep transition however introduces potential problem. introduction deep transition increases number nonlinear steps gradient traverse propagated back time might become difﬁcult train model capture longterm dependencies possible address difﬁculty introduce shortcut connections deep transition added shortcut connections provide shorter paths skipping intermediate layers gradient propagated back time. refer deep transition shortcut connections dt-rnn furthermore call deep hidden-to-output function deep transition deep output deep transition fig. illustration dot-rnn. consider shortcut connections well hidden hidden transition call resulting model dot-rnn. approach similar deep hidden-to-hidden transition proposed recently pinheiro collobert context parsing static scene. introduced recurrent convolutional neural network understood recurrent network whose transition consecutive hidden states modeled convolutional neural network. rcnn shown speed scene parsing obtained state-of-the-art result stanford background sift flow datasets. dieter proposed deep transitions gaussian process models. earlier valpola karhunen used deep neural network model state transition nonlinear dynamical state-space model. noticed dt-rnn srnn extend conventional shallow different aspects. look recurrent level srnn separately easy transition consecutive hidden states still shallow. argued above limits family functions represent. example structure data sufﬁciently complex incorporating input frame summary seen might arbitrarily complex function. case would like model function something universal approximator properties mlp. model rely higher layers higher layers feed back lower layer. hand srnn deal multiple time scales input sequence obvious feature dt-rnn. dt-rnn srnn however orthogonal sense possible features dt-rnn srnn stacking multiple levels dt-rnns build stacked dt-rnn explore paper. noticed state transition equation dynamical system simulated rnns restriction form hence propose multilayer perceptron approximate instead. case implement intermediate layers illustration building deep state transition function shown fig. illustration state transition function implemented neural network single intermediate layer. element-wise nonlinear function weight matrix l-th layer. implementing kind multilayered output function deep output recurrent neural network top-level hidden state computed output obtained using usual formulation alternatively hidden states compute output hidden state level also made depend input well considered approaches using shortcut connections discussed earlier. section brieﬂy introduce novel approach already discussed deep transition and/or deep output recurrent neural networks built. call approach based building predeﬁned neural operators operatorbased framework. operator-based framework ﬁrst deﬁnes operators implemented multilayer perceptron instance plus operator deﬁned function receiving vectors returning summary them clear plus operator predict operator correspond transition function output function eqs. thus step thought performing plus operator update hidden state given input predict operator compute output fig. illustration understood operator-based framework. operator parameterized hidden layers hence neural operator since cannot simply expect operation linear respect input vector. using implement operators proposed deep transition deep output naturally arises. framework provides insight constructed regularized. instance regularize model plus operator commutative. however paper explore approach. note different learned embeddings words happened suitable algebraic operators. operator-based framework proposed rather geared toward learning operators directly. term right-hand side replaced single timestep rnn. setting predicts probability next symbol sequence given previous symbols xt−. then train maximizing log-likelihood. task modeling joint distribution three different tasks; polyphonic music prediction character-level word-level language modeling. test rnns task polyphonic music prediction using three datasets nottingham chorales musedata task characterlevel word-level language modeling penn treebank corpus compare conventional recurrent neural network deep transition shortcut connections transition -rnn) deep output/transition shortcut connections hidden hidden transition -rnn) stacked fig. illustrations models. table sizes trained models. provide number hidden units well total number parameters. dt-rnn numbers provided number units mean size hidden state intermediate layer respectively. dot-rnn three numbers size hidden state intermediate layer consecutive hidden states intermediate layer hidden state output layer. srnn number corresponds size hidden state level size model chosen limited minimize validation error polyphonic music task case language modeling tasks chose size models word-level character-level tasks respectively. cases logistic sigmoid function element-wise nonlinearity hidden unit. character-level language modeling used rectiﬁed linear units intermediate layers output function gave lower validation error. stochastic gradient descent employ strategy clipping gradient proposed pascanu training stops validation cost stops decreasing. polyphonic music prediction nottingham musedata datasets compute gradient step subsequences steps subsequences steps chorales. reset hidden state subsequence unless subsequence belongs different song previous subsequence. cutoff threshold gradients hyperparameter learning rate schedule tuned manually dataset. hyperparameter nottingham musedata chroales. correspond epochs single epoch third epoch respectively. weights connections pair hidden layers sparse nonzero incoming connections unit weight matrix rescaled unit largest singular value weights connections input layer hidden state well hidden state output layer initialized randomly white gaussian distribution standard deviation ﬁxed respectively. case deep output functions -rnn) weights connections hidden state intermediate layer sampled initially white gaussian distribution standard deviation cases biases initialized regularize models white gaussian noise standard deviation weight parameter every time gradient computed language modeling used strategy initializing parameters case language modeling. character-level modeling standard deviations white gaussian distributions input-to-hidden weights hidden-to-output weights used respectively hyperparameters word-level modeling. case dot-rnn sample weights hidden state rectiﬁer intermediate layer output function white gaussian distribution standard deviation using rectiﬁer units biases language modeling learning rate starts initial value halved time validation cost decrease signiﬁcantly regularization character-level modeling word-level modeling strategy adding weight noise polyphonic music prediction. tasks stacked dot-rnn initialized weights conventional dt-rnn similar layer-wise pretraining feedforward neural network times smaller learning rate parameter pretrained either dt-rnn. table performances four types rnns polyphonic music prediction. numbers represent negative log-probabilities test sequences. obtained results using dot-rnn units deep transition maxout units deep output function dropout log-probabilities test data presented ﬁrst four columns tab. able observe cases proposed deep rnns outperformed conventional shallow rnn. though suitability deep depended data trained best results obtained dt-rnns notthingam chorales close tively learning rate starts decreasing quickly learning rate decreases. experiment coincide time validation error starts increasing ﬁrst time. order quickly investigate whether proposed deeper variants rnns also beneﬁt recent advances feedforward neural networks non-saturating activation functions method dropout. built another dot-rnns recently proposed units deep transition maxout units deep output function. furthermore used method dropout instead weight noise training. similarly previously trained models searched size models well learning hyperparameters minimize validation performance. however pretrain models. results obtained dot-rnns maxout units trained dropout shown last column tab. every music dataset performance model signiﬁcantly better achieved models well best results reported recurrent neural networks suggests proposed variants deep rnns also beneﬁt non-saturating activations using dropout like feedforward neural networks. reported results details experiment however acknowledge model-free state-of-the-art results datasets obtained using combined conditional generative model restricted boltzmann machines neural autoregressive distribution estimator output table performances four types rnns tasks language modeling. numbers represent bit-per-character perplexity computed test sequence respectively character-level word-level modeling tasks. previous/current state-of-the-art results obtained shallow rnns. previous/current state-of-the-art results obtained rnns long-short term memory units. tab. perplexities test achieved four models. clearly deep rnns -rnn dot-rnn srnn) outperform conventional shallow signiﬁcantly. tasks dot-rnn outperformed models suggests important highly nonlinear mapping hidden state output case language modeling. results dot-rnn srnn word-level modeling surpassed previous best performance achieved long short-term memory units well shallow larger hidden state even used dynamic evaluation. results report without dynamic evaluation. character-level modeling state-of-the-art results obtained using optimization method hessian-free speciﬁc type architecture called mrnn regularization technique called adaptive weight noise result however better performance achieved conventional shallow rnns without advanced note trivial non-saturating activation functions conventional rnns cause explosion activations hidden states. however perfectly safe non-saturating activation functions intermediate layers deep deep transition. reported mikolov using mrnn hessian-free optimization technique. reported mikolov using dynamic evaluation. reported graves using dynamic evaluation weight noise. dynamic evaluation refers approach parameters model updated validapaper explored novel approach building deep recurrent neural network considered structure timestep revealed relationship between consecutive hidden states hidden state output shallow. based observation proposed alternative designs deep make shallow relationships modeled deep neural networks. furthermore proposed make shortcut connections deep rnns alleviate problem difﬁcult learning potentially introduced increasing depth. empirically evaluated proposed designs conventional single hidden layer another approach building deep task polyphonic music prediction language modeling. experiments revealed proposed deep transition deep output rnn) outperformed conventional stacked task language modeling achieving state-of-the-art result task word-level language modeling. polyphonic music prediction different deeper variant achieved best performance dataset. importantly however cases conventional shallow able outperform deeper variants. results strongly support claim beneﬁts deeper architecture like feedforward neural networks. observation clear winner task polyphonic music prediction suggests proposed deep rnns distinct characteristic makes more less suitable certain types datasets. suspect future possible design train another deeper variant combines proposed models together robust characteristics datasets. instance stacked dt-rnn constructed combining dt-rnn srnn. quick additional experiment trained dot-rnn constructed using nonsaturating nonlinear activation functions trained method dropout able improve performance deep recurrent neural networks polyphonic music prediction tasks signiﬁcantly. suggests important investigate possibility applying recent advances feedforward neural networks novel non-saturating activation functions method dropout recurrent neural networks well. however leave future research. practical issue experiments difﬁculty training deep rnns. able train conventional well dt-rnn easily trivial train dot-rnn stacked rnn. paper proposed shortcut connections well pretrain either conventional dt-rnn. however believe learning become even problematic size depth model increase. future important investigate root causes difﬁculty explore potential solutions. recently introduced approaches advanced regularization methods advanced optimization algorithms promising candidates. would like thank developers theano also thank justin bayer insightful comments paper. would like thank nserc compute canada calcul qu´ebec providing computational resources. razvan pascanu supported deepmind fellowship. kyunghyun supported fics academy finland references bastien lamblin pascanu bergstra goodfellow bergeron bouchard bengio theano features speed improvements. deep learning unsupervised feature learning nips workshop. boulanger-lewandowski bengio vincent modeling temporal dependencies high-dimensional sequences application polyphonic music generation transcription. icml’. graves practical variational inference neural networks. shawe-taylor zemel bartlett pereira weinberger editors advances neural information processing systems pages graves liwicki fernandez bertolami bunke schmidhuber novel connectionist system improved unconstrained handwriting recognition. ieee transactions pattern analysis machine intelligence. hinton deng dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition. ieee signal processing magazine hinton srivastava krizhevsky sutskever salakhutdinov improving neural networks preventing co-adaptation feature detectors. technical report arxiv.. larochelle murray neural autoregressive distribution estimator. proceedings fourteenth international conference artiﬁcial intelligence statistics volume jmlr w&cp. martens deep learning hessian-free optimization. bottou littman editors proceedings twenty-seventh international conference machine learning pages acm. mikolov karaﬁ´at burget cernocky khudanpur recurrent neural network based language model. proceedings annual conference international speech communication association volume pages international speech communication association. mikolov kombrink burget cernocky khudanpur extensions recurrent neural network language model. proc. ieee international conference acoustics speech signal processing mikolov sutskever chen corrado dean distributed representations words phrases compositionality. advances neural information processing systems pages mikolov chen corrado dean efﬁcient estimation word representations vector space. international conference learning representations workshops track. raiko valpola lecun deep learning made easier linear transformations perceptrons. proceedings fifteenth internation conference artiﬁcial intelligence statistics volume jmlr workshop conference proceedings pages jmlr w&cp. sutskever martens hinton generating text recurrent neural networks. getoor scheffer editors proceedings international conference machine learning pages york usa. acm.", "year": 2013}