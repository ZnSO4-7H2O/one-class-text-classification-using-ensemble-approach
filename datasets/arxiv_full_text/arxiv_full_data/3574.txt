{"title": "Optimal Bayesian Transfer Learning", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Transfer learning has recently attracted significant research attention, as it simultaneously learns from different source domains, which have plenty of labeled data, and transfers the relevant knowledge to the target domain with limited labeled data to improve the prediction performance. We propose a Bayesian transfer learning framework where the source and target domains are related through the joint prior density of the model parameters. The modeling of joint prior densities enables better understanding of the \"transferability\" between domains. We define a joint Wishart density for the precision matrices of the Gaussian feature-label distributions in the source and target domains to act like a bridge that transfers the useful information of the source domain to help classification in the target domain by improving the target posteriors. Using several theorems in multivariate statistics, the posteriors and posterior predictive densities are derived in closed forms with hypergeometric functions of matrix argument, leading to our novel closed-form and fast Optimal Bayesian Transfer Learning (OBTL) classifier. Experimental results on both synthetic and real-world benchmark data confirm the superb performance of the OBTL compared to the other state-of-the-art transfer learning and domain adaptation methods.", "text": "abstract—transfer learning recently attracted signiﬁcant research attention simultaneously learns different source domains plenty labeled data transfers relevant knowledge target domain limited labeled data improve prediction performance. propose bayesian transfer learning framework source target domains related joint prior density model parameters. modeling joint prior densities enables better understanding transferability domains. deﬁne joint wishart density precision matrices gaussian feature-label distributions source target domains like bridge transfers useful information source domain help classiﬁcation target domain improving target posteriors. using several theorems multivariate statistics posteriors posterior predictive densities derived closed forms hypergeometric functions matrix argument leading novel closed-form fast optimal bayesian transfer learning classiﬁer. experimental results synthetic real-world benchmark data conﬁrm superb performance obtl compared state-of-the-art transfer learning domain adaptation methods. basic assumption traditional machine learning data training test sets independently sampled domain identical underlying distribution. however growing amount heterogeneity modern data assumption domain reasonable. transfer learning learning strategy enables learn source domain plenty labeled data well target domain labeled data order design better classiﬁer target domain ones trained target-only data generalization performance. reduce effort collecting labeled data target domain might costly impossible. importance ongoing research topic transfer learning many surveys recent years covering transfer learning domain adaptation methods different perspectives train model domain directly apply another trained model generalize well domains related appropriate transfer learning domain adaptation methods borrow information data across domains develop better generalizable models target domain. transfer learning medical genomics desirable since number labeled data samples often limited difﬁculty disease samples prohibitive costs human clinical trials. however relatively easier obtain gene-expression data cell lines model species like mice dogs. different life systems share underlying disease cellular mechanisms utilize data cell lines model species source domain develop transfer learning methods accurate human disease prognosis target domain domain adaptation speciﬁc case transfer learning source target domains classes categories methods either adapt model learned source domain applied target domain adapt source data distribution close target data. depending availability labeled target data methods categorized unsupervised semi-supervised algorithms. unsupervised problems applies cases labeled target data algorithm uses unlabeled data target domain along source labeled data semisupervised methods unlabeled labeled target data learn classiﬁer target domain help source labeled data depending whether source target domains feature space feature dimension homogeneous heterogeneous methods. ﬁrst direction homogeneous instance re-weighting popular measure re-weight data maximum mean discrepancy domains. transfer adaptive boosting another method adaptively sets weights source target samples iteration based relevance source target data help train target classiﬁer. another direction model parameter adaptation. several efforts adapt classiﬁer designed source domain target domain example based residual error feature augmentation methods geodesic flow sampling geodesic flow kernel derive intermediate subspaces using geodesic ﬂows interpolate source target domains. finding invariant latent domain distance empirical distributions source target data minimized another direction tackle problem domain adaptation invariant latent space authors proposed learn invariant latent hilbert space address unsupervised semisupervised problems notion domain variance simultaneously minimized maximizing measure discriminatory power using riemannian optimization techniques. max-margin domain transform semisupervised feature transformation method uses cost function based misclassiﬁcation loss jointly optimizes transformation classiﬁer parameters. another domain-invariant representation method matches distributions source target domains regularized optimal transportation model. heterogeneous feature augmentation heterogeneous method typically embeds source target data common latent space prior data augmentation. domain adaption recently studied deep learning frameworks like deep adaptation network residual transfer networks models based generative adversarial networks domain adversarial neural network coupled although deep methods shown promising results require fairly large amount labeled data. paper treats homogeneous transfer learning domain adaptation bayesian perspectives better theoretical understanding data source domain transferrable help learning target domain. learning complex systems limited data bayesian learning integrate prior knowledge compensate generalization performance loss lack data required sample complexity based predictive models choice. rooted optimal bayesian classiﬁers minimize bayesian error estimates classiﬁers uncertainty classes feature-label distributions propose bayesian transfer learning framework corresponding optimal bayesian transfer learning classiﬁer formulate target domain taking advantages available data joint prior knowledge source target domains. bayesian learning framework transfer learning source target domain joint prior probability density function model parameters feature-label distributions domains. explicitly modeling dependency model parameters feature-label distribution posterior target model parameters updated joint prior probability distribution function conjunction source target data. based that derive effective class-conditional densities target domain obtl classiﬁer constructed. problem deﬁnition aforementioned domain adaptation methods plenty labeled source data labeled target data. source target data follow different multivariate gaussian distributions arbitrary mean vectors precision matrices. obtl deﬁne joint normal-wishart prior distribution precision matrices domains jointly connected. joint prior distribution precision matrices domains acts like bridge useful knowledge source domain transferred target domain making posterior target parameters tighter less uncertainty. bayesian transfer learning framework several theorems multivariate statistics deﬁne appropriate joint prior precision matrices using hypergeometric functions matrix argument whose marginal distributions wishart well. corresponding closedform posterior distributions target model parameters derived integrating source model parameters. closed-form posteriors facilitates closed-form effective class-conditional densities. hence obtl classiﬁer derived based corresponding hypergeometric functions need iterative costly techniques like mcmc sampling. although obtl classiﬁer closed form computing hypergeometric functions involves computation series zonal polynomials timeconsuming scalable high dimension. resolve issue laplace approximations functions preserves good prediction performance obtl making efﬁcient scalable. performance obtl tested synthetic data real-world benchmark image datasets show superior performance state-of-the-art domain adaption methods. paper organized follows. section introduces bayesian transfer learning framework. section derives closed-form posteriors target parameters section obtains effective class-conditional densities target domain. section derives obtl classiﬁer section presents target domain shows obtl classiﬁer converts target-only interaction domains. section introduces laplace approximation hypergeometric functions matrix argument. section viii presents experimental results using synthetic real-world benchmark data. finally section concludes paper. common classes domain. denote labeled datasets source target =nxl data target domain label obviously since consider homogeneous transfer learning scenario feature spaces source target domains features source target domains respectively. gaussian model feature-label distribution domain using corollary ensure using wishart distribution precision matrix joint model lead wishart marginal distributions source target domains separately desired property. introduce theorem proposed gives form joint distribution submatrices partitioned wishart matrix. domains label respectively precision matrices source target domains label respectively. bayesian framework normal-wishart distribution employed prior mean precision matrices gaussian models. require joint prior distribution parameters source target domains account dependency relatedness domains. deﬁne following joint prior distribution mean vector precision matrix. account interactions features within source target domains respectively accounts interactions features across source target domains class {··· point transfer learning joint sampling source target domains possible always assume datasets separately sampled source target domains. instead sampling model. would sampling model would wishart distribution prior precision matrix idea deﬁning joint prior distribution wishart distribution marginalizing term could derive joint distribution target source domains. dependence domains dependence prior distributions precision matrices shown within domain source target likelihoods different classes also conditionally independent given parameters classes. second equality follows theorem gauss hypergeometric function matrix argument such derived closed-form posterior distribution target parameters given optimal bayesian classiﬁer using posterior predictive densities classes called effective class-conditional densities leads optimal choices classiﬁers order minimize bayesian error estimates classiﬁers. similarly derive effective classconditional densities deﬁning obtl classiﬁer target domain albeit posterior target parameters derived target source datasets. concentration parameters {··· dirichlet distribution conjugate prior categorical distribution upon observing data class target domain posterior dirichlet distribution binary classiﬁcation deﬁnition equivalent deﬁnition deﬁned binary classiﬁer possessing minimum bayesian mean square error estimate relative posterior distribution. derived effective class-conditional densities closed forms however deriving obtl classiﬁer requires computing gauss hypergeometric function matrix argument. computing exact values hypergeometirc functions matrix argument using series zonal polynomial time-consuming scalable high dimension. facilitate computation propose laplace approximation function computationally efﬁcient scalable. errors return average classiﬁcation error. note ﬁgures hyperparameters used obtl classiﬁer ones used simulating data except ﬁgures showing sensitivity performance respect different hyperparameters case assume true values hyperparameters used simulating data unknown. examine source data improves classiﬁer target domain compare performance obtl classiﬁer designed target domain alone. average classiﬁcation error versus depicted fig. obtl different values close performance obtl classiﬁer much better greater relatedness domains appropriate source data. performance improvement especially noticeable small reﬂects real-world scenario. fig. also observe errors obtl classiﬁer converging similar value gets large meaning source data redundant large amount target data. larger error curves converge faster optimal error average bayes error target classiﬁer. recall obtl classiﬁer reduces obc. particular example sign matter performance obtl figure depicts average classiﬁcation error versus obtl different values error constant employ source data. error obtl classiﬁer equals starts decrease increases. fig. larger rate improvement greater since domains related. know true value αtrue amount relatedness source target domains. figs. plot error curves αtrue respectively. observe several important trends performance gain obtl ﬁgures. first considered simulation setup evaluated obtl classiﬁers average classiﬁcation error different joint prior densities modeling relatedness source target domains. setup follows. unless mentioned feature dimension number classes domain number source training data class number target training data class classes all-zero all-one vectors respectively. scale matrices choose ksid ktsid classes identity matrix. note choosing identity matrix makes sense order features domains same. constraint scale matrix positive deﬁnite class |kts| √ktks. deﬁne α√ktks particular example value shows amount relatedness source target domains. domains related close greater relatedness. plot average classiﬁcation error curves different values |α|. simulations assume evaluate prediction performance according common evaluation procedure bayesian learning average classiﬁcation errors. sample prior ﬁrst sample wishart distribution pick given sample samples generate different training test sets training sets contain samples target source domains test contains samples target domain. numbers source target training data class source target training data total respectively. assume size test class simulations total. training test obtl targetversion calculate error. average errors different training test sets. repeat whole process times different realizations ﬁnally average sensitivity results figs. reveal simulation setup performance improvement obtl depends value true relatedness domains affected much choices hyperparameters like could reasonable range improved performance correct estimates relatedness transferability critical important future research direction test obtl classiﬁer ofﬁce caltech image datasets adopted help benchmark different transfer learning algorithms literature. used exactly evaluation setup data splits mmdt αmax obtl performance αtrue. second performance gain higher domains highly related third domains related example αtrue fig. αmax meaning obtl leads performance loss compared obc. means exaggeration amount relatedness domains hurt transfer learning classiﬁer domains actually related refers concept negative transfer. figure shows errors versus assuming unknown true value νtrue different values νtrue salient point performance obtl classiﬁer sensitive chosen figure depicts average classiﬁcation error versus different values true value κtrue similar greater value performance change much. according better choose proportional respectively since values updated means weighted averages prior knowledge means assuming βtnt βsns classes including ofﬁce stuff like backpack chair keyboard etc. three domains amazon webcam dslr contain images amazon’s website webcam digital single-lens reﬂex camera respectively different lighting backgrounds. surf image features used domains dimension common classes ofﬁce caltech datasets feature dimension accordingt data splits numbers training data class source domain amazon three domains target domain four domains. four-domain dataset random train-test splits created obtl classiﬁer provided train-test splits report average accuracy. note test data solely target domains. authors mmdt reduce dimension using pca. follow procedure obtl classiﬁer. following comparison framework used evaluation setup compare obtl’s performance terms accuracy table target-only classiﬁers four state-of-the-art semisupervised transfer learning algorithms itself). evaluation setup exactly obtl methods. result results ﬁrst methods table compare obtl classiﬁer. methods follows. learns invariant latent space reduce discrepancy source target domains uses riemannian optimization techniques match statistical properties samples projected latent space different domains. marked best accuracy column second best accuracy blue. cases obtl best accuracy. cases obtl second best accuracy accuracy close best accuracy. written mean accuracy method last column averaged different experiments. obtl classiﬁer best mean accuracy second best accuracy among methods. values hyperparameters classes domain drop superscript denoting class label. experiments. choose separately experiment since relatedness distinct pairs domains different. pool target source data classes respectively sample means datasets. reasonable values using cross-validation method. show values hyperparameters used experiment table assumed equal prior probabilities classes used obtl classiﬁer. constructed bayesian transfer learning framework tackle semi-supervised supervised transfer learning problems. proposed optimal bayesian transfer learning classiﬁer deal lack labeled data target domain optimal bayesian framework since minimizes expected classiﬁcation error. obtained closed-form posterior distribution target parameters accordingly closed-form effective classconditional densities target domain deﬁne obtl classiﬁer. obtl’s objective function consists hypergeometric functions matrix argument laplace approximations functions derive computationally efﬁcient scalable obtl classiﬁer preserving superior performance. compared performance obtl target-only version transferring source target domain help. tested obtl classiﬁer real-world benchmark image datasets demonstrated excellent performance compared state-of-the-art domain adaption methods. paper considers gaussian model derive closed-form solutions case obc. since many practical problems cannot approximated gaussian model important aspect development utilization mcmc methods naturally extended obtl setting albeit greater computational burden. considered domains paper assuming source domain. seen good performance obtl classiﬁer domains future work going apply multi-source transfer learning problems beneﬁt knowledge different related sources order improve target classiﬁer. case basic engineering aspect obtl prior construction. studied different conditions context using data unused features infer prior distribution deriving prior distribution models datagenerating technology deriving prior uncertainties mathematical model characterizing relevant physical system applying constraints based prior knowledge prior knowledge prior distribution optimization latter methods general placed formal mathematical structure prior results optimization involving kullback-leibler divergence constrained conditional probability statements characterizing physical knowledge genetic pathways genomic medicine. focus future work extend general framework obtl require formulation incorporates knowledge relating source target domains. emphasized optimal bayesian classiﬁcation well optimal bayesian ﬁltering prior distribution operator designed underlying scientiﬁc model venkateswara chakraborty panchanathan deep-learning systems domain adaptation computer vision learning transferable feature representations ieee signal processing magazine vol. ganchev malehorn bigbee gopalakrishnan transfer learning classiﬁcation rules biomarker discovery veriﬁcation molecular proﬁling studies journal biomedical informatics vol. hoffman rodner donahue kulis saenko asymmetric category invariant feature transformations domain adaptation international journal computer vision vol. y.-h. hubert tsai y.-r. y.-c. frank wang learning crossdomain landmarks heterogeneous domain adaptation proceedings ieee conference computer vision pattern recognition borgwardt gretton rasch h.-p. kriegel sch¨olkopf smola integrating structured biological data kernel maximum mean discrepancy bioinformatics vol. bruzzone marconcini domain adaptation problems dasvm classiﬁcation technique circular validation strategy ieee transactions pattern analysis machine intelligence vol. dehghannasiri esfahani dougherty intrinsically bayesian robust kalman ﬁlter innovation process approach ieee transactions signal processing vol. ganin ustinova ajakan germain larochelle laviolette marchand lempitsky domain-adversarial training neural networks journal machine learning research vol. m.-y. tuzel coupled generative adversarial networks advances neural information processing systems dalton dougherty optimal classiﬁers minimum expected error within bayesian framework—part discrete gaussian models pattern recognition vol. dalton dougherty bayesian minimum mean-square error estimation classiﬁcation error-part deﬁnition bayesian mmse error estimator discrete classiﬁcation ieee transactions signal processing vol. saenko kulis fritz darrell adapting visual category models domains proceedings european conference computer vision part ser. eccv’. berlin heidelberg springer-verlag knight ivanov dougherty mcmc implementation optimal bayesian classiﬁer non-gaussian models model-based rna-seq classiﬁcation bioinformatics vol. knight ivanov triff chapkin dougherty detecting multivariate gene interactions rna-seq data using optimal bayesian classiﬁcation ieee/acm transactions computational biology bioinformatics zollanvari dougherty incorporating prior knowledge induced stochastic differential equations classiﬁcation stochastic observations eurasip journal bioinformatics systems biology vol. esfahani dougherty incorporation biological pathway knowledge construction priors optimal bayesian classiﬁcation ieee/acm transactions computational biology bioinformatics vol. optimization-based framework transformation incomplete biological knowledge probabilistic structure application utilization gene/protein signaling pathways discrete phenotype classiﬁcation ieee/acm transactions computational biology bioinformatics vol. boluki esfahani qian dougherty incorporating biological prior knowledge bayesian learning maximal knowledge-driven information priors bioinformatics vol.", "year": 2018}