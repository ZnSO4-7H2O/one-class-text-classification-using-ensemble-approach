{"title": "Tensor Completion Algorithms in Big Data Analytics", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Tensor completion is a problem of filling the missing or unobserved entries of partially observed tensors. Due to the multidimensional character of tensors in describing complex datasets, tensor completion algorithms and their applications have received wide attention and achievement in data mining, computer vision, signal processing, and neuroscience, etc. In this survey, we provide a modern overview of recent advances in tensor completion algorithms from the perspective of big data analytics characterized by diverse variety, large volume, and high velocity. Towards a better comprehension and comparison of vast existing advances, we summarize and categorize them into four groups including general tensor completion algorithms, tensor completion with auxiliary information (variety), scalable tensor completion algorithms (volume) and dynamic tensor completion algorithms (velocity). Besides, we introduce their applications on real-world data-driven problems and present an open-source package covering several widely used tensor decomposition and completion algorithms. Our goal is to summarize these popular methods and introduce them to researchers for promoting the research process in this field and give an available repository for practitioners. In the end, we also discuss some challenges and promising research directions in this community for future explorations.", "text": "tensor completion problem lling missing unobserved entries partially observed tensors. multidimensional character tensors describing complex datasets tensor completion algorithms applications received wide aention achievement data mining computer vision signal processing neuroscience etc. survey provide modern overview recent advances tensor completion algorithms perspective data analytics characterized diverse variety large volume high velocity. towards beer comprehension comparison vast existing advances summarize categorize four groups including general tensor completion algorithms tensor completion auxiliary information scalable tensor completion algorithms dynamic tensor completion algorithms besides introduce applications real-world data-driven problems present open-source package covering several widely used tensor decomposition completion algorithms. goal summarize popular methods introduce researchers promoting research process give available repository practitioners. also discuss challenges promising research directions community future explorations. additional words phrases tensor tensor completion tensor decomposition tensor factorization multilinear data analysis dynamic data analysis data analytics introduction tensor analysis aroused increasing aention recent years. related topics tensor decomposition widely studied theoretical well applications practice multidimensional datasets incomplete owing various unpredictable unavoidable reasons mal-operations limited permissions missing random practical demand raises issue handling missing data multi-way analysis evolving problem tensor completion. tensor completion imputation considered byproduct dealing missing data early work diusely presents various applications link prediction recommender system urban computing information diusion image completion video completion compressed sensing healthcare chemometrics climate data analytics etc. rapid growth data velocity volume real world eective tensor completion algorithms need developed capable handling real large-scale data sets well dynamical changes time. past decades matrix completion problem special case well-dened studied. mature algorithms theoretical foundations varies applications pave solving completion problem high-order tensors core subject focus review. ough various researches conducted tensor completion problem recent years best knowledge still lacks survey give comparatively consummate overview fast evolving advances. existing surveys either resting matrix level focusing related topics e.g. tensor decompositions moreover incremental interest data analytics motivates provide view data perspective present survey diers existing reviews. goal survey give overview general advanced high-order tensor completion methods applications dierent elds. summarizing state-of-the-art tensor completion methods promoting research process area providing available python repository handy tool researchers practitioners. tightly coupling challenges data analytics including variety volume velocity organize structure article shown figure notations primary multilinear algebra operations introduced help formally formulate tensor completion problem variations. concise summary general tensor completion algorithms provided along several statistical assumptions theoretical analysis. subsequently coping challenges data analytics three categories advanced completion methods introduced including tensor completion auxiliary information scalable tensor completion algorithms dynamic completion methods real-world applications dierent areas outlined section several extensive experimental simulations metrics presented section finally describe brief future research outlook covering open challenges promising directions. tensor completion problem section provide formal denition tensor completion problem. mathematically dene tensor completion problem introduce preliminaries containing needful tensor operations basic denitions notations. preliminaries denition tensor could dened dierent approaches various levels abstraction follow general dene multidimensional array. dimensionality described order. th-order tensor n-way array also known n-dimensional n-mode tensor denoted xxx. denition outer product tensor product vectors denoted arbitrary column vectors hadamard product element-wise product tensors size denoted bbb. inverse division operation denoted bbb. n-mode product multiplication given tensor ri×i×...×in nth-mode matrix rin×j denoted ...in−×j×in+ ...in elementwise result described general tensor product general tensor product tensors. given tensors ri×...×in rj×...×jn tensor product dened ri×...×in ×j×...×jn elementwise result described denition tensor matricization unfold tensor matrix format predened sequence mode order. commonly used tensor matricization nmode matricization unfold tensor ri×...×in along ik). order modes except mode arranged randomly construct column commonly used arrangement forward backward comprehensive comparison visualization could found denition also called simple tensor. th-order tensor could wrien outer product vectors i.e. denition general rank tensor dened minimum number summations rank-one tensors generate i.e. denition calculating tensor rank np-hard problem tensor n-rank introduced kruskal special case multiplex rank introduced hitchcock tensor n-rank dened rank i.e. rankn rank). another similar denition called tucker rank multilinear rank introduced earlier tucker dened ranktc rank)). denition inner product tensors size dened unless otherwise specied treat product dened follows tensor completion problem tensor completion dened problem lling missing elements partially observed tensors. particular matrix case avoid underdetermined problem rank necessary hypothesis restrict degree freedoms missing entries intractable. hence focus low-rank tensor completion problem paper. lrtc problem sometimes considered byproduct tensor decomposition problem missing values could formulated general imputation problem given rank follows represents completed low-rank tensor denotes real tensor practical observations. index denoting indices observations. error measure xxxωωω tttωωω dened frobenius norm dierence assumption gaussian noise. rank tensor. variations constraint also considered researches rank ranktc ranktc element-wise notation. practice predening rank always unrealistic. avoid misestimations tensor rank suitable assumption rank target tensor unknown. generalized matrix completion lrtc problem equivalently dened variant rank-minimization problem noise term usually assumed gaussian real-world applications. researchers also gaussian measurement operator denote constraint sampling ensemble constraint entries longer based random sampling approach we’ll discuss section researchers linear operator pωωω describe pωωω pωωω denotes sampling set. noiseless case also considered researchers e.g. tensor rank hard handle researchers always replace tensor rank tensor n-rank tucker rank predened function linear operator determining constraints. case tensor completion equal pωωω represents orthogonal projector onto span tensors vanishing outside corresponding observations. general tensor completion algorithms start summarizing fundamental tensor completion approaches categorizing decomposition based approaches trace-norm based approaches variants. methods means necessarily straightforward non-scalable compare advance tensor completion methods. reason either general primary eect laying foundation building advanced tools original focus perspective data analytics. decomposition based approaches tensor decompositions factorizations powerful tools extracting meaningful latent structures heterogeneous multi-aspect data since real-world datasets incomplete many early researches conducted tensor decomposition missing values could regarded pioneer problem lrtc. focus widely used decomposition methods tucker decomposition proceed overview. readers interested tensor decompositions applications could comprehensive introduction. surprisingly decomposition approaches could also applied solving completion problem hierarchical tensor representations parafac models candecomp/parafac based methods. decomposition proposed hitchcock discussed carroll harshman formaly dene given th-order tensor decomposition approximation loading matrices rin×r that borrowing idea weighted least square method applied matrix case demonstrates ways handling missing data parafac model might earliest approaches targeting missing data imputation problem multi-way data analysis. alternatively estimate model parameters imputing missing data could regarded expectation maximization like approach assumption gaussian residuals. hence call em-like approach. approach usually used alternating projection optimizations also mentioned early works handling missing entries parafac model another straightforward approach skip missing value build model based observed part. approach found gradient based optimizations probabilistic methods. also realized masking missing entries optimization scheme. hence call missing-skipping approach. ideas illustrated subsequent paper collaborated tomasi. proposed parafac models als-si indafac based ideas respectively. model relies standard alternating least squares optimization popular algorithm calculating decomposition proposed carroll chang harshman handle missing values main idea conduct imputation based following equation iteratively approach easy conducted needs follow standard optimization scheme performing imputation every iteration based however increasing percentage missing entries convergence rate reduced risk converging local minimum would increased solve problems second model indafac proposed based second idea ignoring missing entries model construction. model optimized utilizing levenberg-marquardt version gauss-newton algorithm proved computationally ecient missing ratio high. general approach gauss-newton algorithm solving weighted least square parafac decomposition found earlier paper peni acar also second idea develop algorithm named cp-wopt dierent second-order approach employed indafac cp-wopt optimized based rst-order gradient shown enjoy eciency scalability. besides probabilistic methods bayesian inference also proposed solving missing-value prediction problem methods could taken probabilistic format decomposition method delete terms missing observations likelihood functions handle missing values conduct imputation. tucker based methods. tucker decomposition proposed tucker developed kroonenberg lathauwer given th-order tensor tucker decomposition dened approximation core tensor n=rn multiplied factor matrices rin×rn along mode that tucker decomposition also widely used tool tensor completion. shown usually eective based approaches core tensor general capturing complex interactions among components strictly trilinear. similar based methods em-based approach approach still traditional ways handling missing values well data imputation. early works walczak andersson mentioned using em-based approach handling tucker decomposition missing values. method combined higher-order orthogonal iteration algorithm deal missing values paern-recognition problems researchers utilized approach karatzoglou former applies high-order algorithm track low-rank tucker subspace complete multidimensional arrays algorithm optimization observed values. later considers probabilistic approach called ptucker missing part naturally deleted likelihood functions. recently several methods based hierarchical tensor representations provide exible generalization classical tucker model deal high order tensors. approaches optimized using projected gradient methods iterative hard thresholding algorithms idea riemaniann gradient iteration method contains step iteration perform gradient step ambient space result back low-rank tensor manifold hierarchical singular value thresholding procedure. another variant projected gradient method called riemannian optimization approach also considered based hierarchical representations manifold construction. particularity leave later section detailed introduction. trace norm based approaches matrix trace norm popularized convex surrogate non-convex rank function solving matrix completion problem. matrices bounded operator norm shown tightest lower bound matrix rank function among possible convex approximation generalized matrix relaxation dened tensor trace norm combination trace norms unfoldings reformulated tensor rank minimization problem convex optimization problem. signoreo generalized tensor trace norm particular case shaen-pq norm. fact direct generalization matrix trace norm tensor trace norm utilize denition tensor n-rank. substituting tensor rank linear combination tensor n-rank could could relaxed trace norms dening low-n-rank tensor pursuit problem usually dened maintain consistency matrix trace norm call model model. ough unfolding matrices cannot optimized independently multi-linear correlations convex relaxation allows solve completion problem without predening tensor rank tractable practice. equivalent problem case gaussian noise could formulated trade-o constant. solve problem propose simple algorithm optimized block coordinate decent. algorithm trace-norm based tensor completion algorithm best knowledge tensor completion algorithm without xing rank tensor advance decomposition-based approaches simplied version ough formulation solved kind dierent essence. advanced algorithms proposed subsequent journal paper popular approach solving problem apply spliing method e.g. alternating direction method multipliers special case douglas-rachford spliing method. slight changes methods consider noise exist case introduce integrated approach grandy tomioka come back noiseless case discussed reformulated inducing auxiliary matrices singular value thresholding operator shrinkage operator dened svtδ represents singular value decomposition matrix matrix max{x max{· denotes element-wise operator. update could several choices em-like approach describe exact inexact approaches exact updating approach describe algorithm deal noiseless case signoreo gandy propose decrease value equation directly delete term prove algorithm ecient. douglas-rachford spliing method also shown special case point proximal methods useful accommodating various non-smooth constraints. details discussed model neither tightest relaxation tensor rank optimal solution considering sample size requirement. reduce sample size required completion type methods mainly focusing proposing beer convex relaxation based trace norm structure reduce completion requirement sample size. suggest single powerful regularizer rather combination structure model i.e. apply trace norm balanced unfolding matrix rather using summation trace norm. besides numerical experiments theoretical analysis sampling bound also conducted exploiting sharp properties gaussian measurement. dierent matrix trace-norm structure trace-norm based structures also proposed tensor-nuclear-norm incoherent trace norm either target rank structure consider incoherence condition. focusing analyzing sampling size requirement based dierent sampling approaches stress theoretical ndings statistical analysis section. second type approaches trace norm constraints factorized matrices rather unfolding matrices could treated mixed approaches trace-norm based methods decomposition based methods. methods mainly focusing reducing computation complexity since trace norm minimization methods costly require decomposition potentially large matrices also exible incorporating dierent structures decomposition matrices alleviate requirement predened ranks decomposition based methods ying focus exponential signal completion problem. signal model degree freedoms explore exploit exponential structure factor vectors reduce number samples exact recovery. factor transformed hankel matrix promote exponential structure nuclear norms applied force hankel matrices low-rank. address computation complexity problem researchers utilize decomposition researchers trace-norm constraints tucker factor matrices approaches apply trace norm onto factorized matrices rather original unfolding matrices acceleration laer also alleviates rank pre-denition problem faced decomposition-based approaches extent. another interesting straightforward variation trace norm approach transform matrix trace norm frobenius norm. matrix rank decomposition equivalent expression trace norm could dened variants provide comprehensive informative introduction include several important variants section supplementary general tensor completion methods. non-negative constrained approaches. real-world applications sometimes require nonnegative constraints image completion medical data analysis impute missing entries non-negative tensors popular non-negative constraints latent factor matrices based decomposition models dierent optimization method could applied block coordinate descent admm framework iteratively non-negative threshold. non-negative tensors integer entries takeuchi ueda generalized kullback-leibler divergence analogy several matrix-based approaches robust tensor completion. robust data analysis plays instrumental role dealing outliers gross corruptions completion problem drawing upon advances robust analysis robust tensor completion complete tensor separating low-rank part plus sparse part capture dierent noise paerns. objective function could formulated follows similar trace-norm strategy could added low-rank part relaxation rank minimization norm used convex surrogate relax norm. completion problem could well addressed joint minimization trace norm norm. utilize block coordinate decent method similar shows promising results separating corrupted noise image restoration. goldfarb study problem robust low-rank tensor completion convex admm optimization framework provide theoretical practical analysis convex non-convex models. recent work jain also adopts admm framework complete noisy tensors factors sparse. besides admm framework zhao consider probabilistic framework fully bayesian treatment optimized variational bayesian inference approach. javed provide online stochastic optimization method robust low-rank sparse error separation addressing background subtraction problem. riemannian optimization. riemannian optimization technique gained increasing popularity recent years idea lies alternative treatment focusing xed-rank tensor completion problem embedding rank constraint search space unconstrained problem smooth manifold {xxx ri×i×...×in |ranktc dened follows riemannian optimization iteratively gradient projection approach based two-step procedure projected gradient step retraction step. smooth manifold tangent space proper denition riemannian metric gradient projection retraction three essential seings riemannian optimization. tucker decomposition gives ecient representation tensors factorized orthogonal matrices belonging stiefel manifold matrices usually used parametrization manifold tangent space besides tucker manifold choices smooth manifold also used hierarchical tucker space handling high-dimensional applications dening manifold tangent space important ingredients riemannian approach riemannian metric gradient. kressner focused search space exploited dierential geometry rank constraint based tucker decomposition. kasai laid emphasis cost function pωωω inducing block diagonal approximation hessian dened novel metric tangent space quotient manifold. generalized preconditioned conjugate gradient algorithm proposed total computational cost three-order tensor. finally retraction used updated tensor back low-rank tensor manifold. maps hosvd method hierarchical singular value thresholding satisfy necessary properties retraction mentioned could used. still various interesting methods solving completion problem alternative convex relaxation approach tucker ranks adaptive sampling methods briey introduced next section. unpractical cover bases readers interested approaches encouraged explore original paper. finally list several representative algorithms mentioned table summing comparison. statistical assumption theoretical analysis section introduce primary statistical assumptions theoretical analysis completion feasibility. goal give theoretical foundations previously mentioned methods delving advanced tensor completion techniques data analytics. sampling assumption. tensor completion usually based random sampling assumption assume partially observed entries uniformly sampled original tensor. matrix case bernoulli sampling independent sampling replacement always assumed simply random sampling approach. generalizing assumptions bernoulli sampling tensor case straightforward several sampling assumptions also used either convenience theoretical demonstration practicability real-world applications e.g. gaussian measurements fourier measurements adaptive sampling gaussian random measurement gaussian linear ri×...×in dened tensors gggi ri×...×in gggi tensor gggi i.i.d. standard normal entries. enjoys sharp stiefel manifold matrices dened rn×p theoretical tools well-studied signal processing especially compressed sensing area another widely studied sampling measurement random fourier measurement constructed fourier transformation comparing subgaussian measurements usually serve benchmark guarantees required observations exact low-rank recovery fourier measurement oers advantages structured sampling property available fast multiplication routines real-world applications sampling populations sparse clustered. circumstances non-adaptive sampling methods able achieve exact recovery using similar size samples usual. instead varies adaptive sampling approaches derived relax incoherence assumptions another important assumption introduced next. incoherence assumption. tensor incoherence assumption generalized conception matrix incoherence arisen compressed sensing. closely correlated passive uniformly samplings entry comparable amount information guarantee recovery tractable. opposite situation coherent means mass tensor concentrate small number elements informative elements completely sampled. lead missing mass sampling methods uniformly random independent underlying distribution tensor. mathematically coherence r-dimensional linear subspace dened linear subspace spanned mode-i bers. besides n-mode incoherence tensor incoherence conditions extended mutual incoherence based singular value decompositions matricizations x’s. based dierent sampling strategies incoherence assumptions discuss theoretical bounds observed entries need recover tensors using dierent algorithms. number observed entries. guarantee unique reconstruction original tensor theoretic lower bound number observed entries required. matrix case given incoherent matrix could recovered high probability sample size larger polylo❕ constant matrix rank generalizing result high-order situation straightforward tomioka conducted statistical analysis low-rank tensor recovery. provided reliable recovery bound model assumption gaussian measurement. analysis given th-order tensor ri×...×i tucker rank could completed fact freedom tensor ri×...×in tucker rank only\u0001n number entries core tensor randomly valued demonstrate freedom is\u0001n vectors freedom total freedom is\u0001n in\u0001n balanced matrix dened reshape\u0001j however necessary bound model satisfactory needs parameters describe tensor dened based tucker decomposition equation furthermore prove thatn measurements sucient complete almost surely based non-convex model formulated equation result calculated based easy two-step construction construct rest part iteratively unfold core tensor along mode mode-n unfolding matrix size matrix represents uncorrelated vectors could used construct rest analysis motivates researchers lower bounds well advanced methods tensor completion. authors improve model beer convexication. rather using combination mode-n trace norms suggest using individual trace norm n=j+ entires sucient recovery rankcp observations needed recover tensor tucker rank jain stress seing random sampling consider square deal approach requires observation dense random projection rather single observed entry. generate matrix alternating minimization technique tensor level decomposition approach prove symmetric third order tensor ri×i×i rank-r could correctly reconstructed randomly sampled entries. similar spirit barak moitra give algorithm based sum-of-squares relaxation prediction incoherent tensors. instead exact recovery main result shows observations could guarantee approximation explicit upper bound error. recently yuan zhang explore correlations tensor norms dierent coherence conditions. prove th-order tensor ri×...×i rank-r could completed entirely uniformly sampled entries proper incoherent nuclear norm minimization. recently theme adaptive sensing emerges ecient alternative random sampling approach large-scale data obtaining processing. exploiting adaptivity identify highly informative entries required number observed entries could substantially reduced. krishnamurthy propose adaptive sampling method estimation algorithm could provably complete th-order rank-r tensor entries. subsequently bhojanapalli sanghavi give sampling approach based parallel weighted alternating least square algorithm. show symmetric rank-r third order tensor ri×r orthonormal matrix could proved exactly κlo❕) rows restricted condition number. although algorithm crucially relies adaptive sampling technique generalize random samples achieve completion purpose lile incoherence assumptions underlying tensor. summarize sucient tensor completion auxiliary information tensor completion methods usually require low-rank assumption uniformly sampling. however increasing ratio missing entries prediction accuracy tends signicantly decreased. real-world data-driven applications besides target tensor object variety additional side information spatial temporal similarities among objects auxiliary coupled matrices/tensors also exist potential help improving completion quality. section provide overview related approaches mainly introduce dierent incorporating variety auxiliary information i.e. similarity based approaches coupled matrices/tensors factorization. similarity based approaches inspired relation regularized matrix factorization proposed narita regularization methods called within-mode regularization cross-mode regularization respectively incorporate auxiliary similarity among modes tensor factorization. methods based em-like approach combined tucker decomposition. idea construct within-mode cross-mode similarity matrices incorporate following regularization terms mode-n latent matrix tucker decomposition mode-n auxiliary similarity matrix size laplacian matrix dened laplacian matrix also called kronecker product similarity early work kashima combining standard decomposition approaches applying tensor completion problem auxiliary information shown improve completion accuracy especially observations sparse. fact early work taken advantage within-mode regularization combine coupled matrix method mobile recommendation problem. similar idea approaches also used respectively. perspective probabilistic approaches similarity matrices among mode could treated inverse kernel matrices dened prior distributions factor matrices. coupled matrices/tensors factorization another popular incorporating auxiliary information couple tensors matrices together jointly factorization imputation. deriving coupled matrix factorization coupled side information also widely developed multi-way data analysis popular methods applied tensor completion coupled matrix tensor factorization proposed acar based idea sharing factorization matrices among matrices tensors constructed heterogeneous datasets propose all-at-once algorithm impute incomplete tensor coupled matrices. mathematically translated besides decomposition factor matrices tucker decomposition could also coupled auxiliary matrices tensors either dealing completion tasks imputing missing entries factorization byproduct scalable tensor completion rapid growth volume datasets high-order tensor structure challenging tensor completion approaches handle large scale datasets billions elements mode high computational costs space. faces several challenges intermediate data explosion problem amount intermediate data operation exceeds capacity single machine even cluster large regularization problem various types regularization term aect scalability parallelism tensor factorization. notably prior research studies main focus intermediate data explosion problem occurred operations updating factor matrices alternating least squares algorithm increase size input data size intermediate data operations could become huge intermediate data explosion problem tensor decomposition alternating least squares popular algorithm decompose either fully observed tensor data partially observed tensor data conditionally updates factor matrix given others. however previous approaches usually scalability issues. inevitable materialize matrices calculated khatri-rao product large sizes cannot memory. instance review denition khatri-rao product ri×r rj×r number columns denoted size apparently million million khatri-rao product size trillion normally large dense obviously cannot memory. erefore solve intermediate data explosion problem plays important role scaling tensor decomposition algorithms. since sparsity common property tensor data meaning elements tensor zeros naive solution intermediate data explosion problem adopt coordinate format store nonzero elements tensor completion avoid materialization huge unnecessary intermediate khatri-rao products denotes identity matrix size indices observed entries whose mode’s index erefore updating latent matrix parallelized distributing rows latent matrix across machines performing update simultaneously without aecting correctness als. however since approach requires machine exchange latent matrix others communication memory costs iteration. hence machine load similarly zinkevich extend stochastic gradient descent distributed version parallelized stochastic gradient descent concretely psgd randomly splits observed tensor data machines runs machine independently. updated parameters averaged iteration. element latent matrix updated following rule another optional avoid intermediate data explosion tensor completion reduce operations large memory like tensor-matrix multiplication. instance kolda work tucker decomposition method sparse data solve intermediate data explosion problem target yyyn multiplication tensor-matrix multiplication dimension i.e. computing product sparse tensor times series dense matrices large memory process. order maximize computational speed optimally utilizing limited memory computation handled piecemeal fashion adaptively selecting order operations. concretely tensor data stored coordinate format proposed tensor-matrix multiplication calculated slice time instead directly multiplying matrix since tensor-vector multiplication produce results much smaller size easily limited memory. ough methods stated capable eciently computing tensor decomposition tensor data large sparse methods still operate limited memory single machine therefore cannot handle gigabytes terabytes tensor data. hence many researchers utilize parallel computing system like distributed computing system mapreduce eectively eciently perform tensor decomposition large-scale datasets. order address intermediate data explosion problem distributed/parallel fashion main ideas include carefully selecting order computations update latent matrices minimizing oating point operations) exploiting sparsity tensor data avoid intermediate data explosion utilizing distributed cache multiplication minimize intermediate data exploiting skewness matrix multiplications distributed/parallel computing system. popular scalable distributed algorithm parafac tensor decomposition gigatensor proposed kang solves intermediate data explosion problem reordering computation exploiting sparsity tensor data mapreduce. specically gigatensor targets intermediate data explosion problem occurred updating latent matrices based upon follows unfolded matrix tensor st-order latent facotrized matrices three dimensions respectively pseudo inverse. inspired concept divide-conquer main idea take advantage sparsity tensor data decouple terms khatri-rao product sequentially performing algebraic operations unfolded tensor instead directly computing khatri-rao product. concretely computing equally treated calculating bin)∗ function converts non-zero value preserving sparsity all- vector size size intermediate data reduce number non-zero elements. operations implemented mapreduce designing mapper reducer. similar fashion jeon propose haten improves gigatensor unifying tucker parafac decompositions general framework. gigatensor haten easily extended solve completion problem adding indication tensor objective function. recently dfacto ecient scalable distributed algorithm proposed choi also addresses intermediate data explosion problem particularly focusing scaling concretely dfacto computes column-wise manner follows unvec operator reshaping dimensional vector matrix. operations dfacto ecient assuming sparse tensor represented compressed sparse format. dfacto implemented applying master-slave architecture communication cost dfacto relatively high since master transmits latent matrices slaves every iteration. decentralization tensor data problem dierent perspective fact focuses decentralizing observed tensor data based fashion divide-conquer. splits observed tensor data blocks dis-joint blocks treated stratum without sharing common bers. fact processes stratum time decentralizing dis-joint blocks machines processes independently simultaneously. machine fact applies stochastic gradient descent solve block. updated parameters machine disjoint ones updated machines. hence memory requirements fact distributed among machines. nevertheless communication cost high since processing stratum fact needs aggregate parameters updated machine master machine updates decentralizes next iteration. overcome problem high communication cost algorithms like fact scalable tensor factorization cdtf developed shin extends ccd++ higher orders based coordinate descent. concretely parameter updated following rule update sequence parameters cdtf adopts column-wise order updating column latent matrix moving column next latent matrix. updating columns latent matrices columns latent matrices started updated. extensively shin propose another scalable tensor factorization algorithm based subset alternating least square updates columns latent matrices row. contrary cdtf updates column entry entry; updates columns row. cdtf sals implemented mapreduce table compare several state-of-the-art scalable tensor completion algorithms aspect scalability platform complexity. dynamic tensor completion beyond traditional static/batch seing increasing amount high-velocity streaming data concerns addressed dynamic tensor analysis existing surveys lack detailed introduction dynamic tensor analysis give brief investigation existing works dynamic tensor analysis especially focusing dynamic tensor factorization introduce several dynamic tensor completion algorithms. decomposition nion sidiropoulos introduce adaptive parafac algorithms adopting recursive least square simultaneous diagonalization tracking approaches address online third-order tensor factorization problem. recent work zhou develops accelerated online algorithm incrementally track decompositions one-mode-change tensors. phan partition large-scale tensor small grids develop grid-based scalable tensor factorization method could also easily applied address streaming tensor factorization problem. besides decomposition tucker decomposition methods also proposed. earliest work conducted propose dynamic tucker factorization methods based incremental update covariance matrices dynamic tensor analysis. subsequently raised accelerated online tensor learning algorithm conduct streaming tucker factorization. several online tucker decomposition methods conducted recently targeting one-mode-increase paern also deriving possible solutions multi-aspect streaming paerns drawing upon matrix-based online-learning methods incremental furthermore histogram-based approach conducted multi-aspect streaming tensor analysis could regarded pioneering research multi-aspect streaming analysis. although various researches conducted dynamic tensor analysis hasn’t much work focusing tensor completion problem dynamic paerns. early work meng parafac models in-lling missing future data shown overcome challenge signal monitoring. matsubara topic modeling gibbs sampling method posterior latent factors mode solve future trac forecasting problem e.g. many clicks user generate tomorrow. however approach close pure future prediction problem rather completion problem. proper dynamic completion problem addressed recent approach proposed mardani focused streaming tensor factorization missing entries proposed online stochastic gradient decent algorithm tensor decomposition imputation. performing rank-minimization leveraging separable nuclear-norm regularization describe equation main objective function dened exponentially-weighted least-squares loss follows three-order tensor example. transforming nuclear norm related tractable format online optimization could performed generalization matrix online algorithm. outline algorithm provided algorithm similar online latent assumption online methods rarely smoothly changed streaming tensor subspace surveillance video streaming allows pursue real subspace iteratively. real-world situation processing speed could faster data acquiring speed. kasai proposes another online decomposition algorithm olstec conducting data imputation based recursive least squares algorithm. consider situations data processing speed much faster data acquiring speed propose rapid convergence algorithm tracking dramatically changed subspace. storing auxiliary matrices non-increasing modes size modes could eciently updated considering parallel smaller least square problems. incremental subspace time mode obtained one-time pursuit timestamp. although sacricing time complexity extent third-order tensor ri×i×i) olstec able converge faster tracking dramatically changed low-rank subspace comparing based method. recently song propose general multi-aspect streaming tensor completion method could handle all-mode-change tensor data imputing missing data. also provides another possible vision dynamic tensor decomposition/completion problem future research. applications tensor completion problem diusely presents various applications. section cover widespread domain applications including social sciences computer vision signal processing healthcare bioinformatics others. application three questions mainly discussed concrete problem formulated tensor completion problem; kind methods applied. social computing social computing concerns applying computation methods explore individual activities relationships within societies. contains various interdisciplinary branches includes multiple problems could modeled tensor completion problems. link prediction. link prediction aims predicting missing edges graph probabilities future node connections. commonly used approaches extend traditional graph topology matrix tensors. adding temporal mode bipartite graph natural derives binary completion problem three-order tensor. acar exploit decomposition heuristic denition similarity score future link prediction. subsequent work gives another denition score using holt-winters forecasting method proves eectiveness tensor-based methods forecasting varying periodic paerns among links. dierent type interactions among nodes could also dened third mode based construction kashima utilize auxiliary similarity information nodes tensor link prediction. ermis¸ address link prediction problem jointly analyzing multiple sources coupled factorization. apply proposed generalized framework dierent scale real-world datasets uclaf digg compare aect dierent loss function factorization models practice. also ermis¸ recommender systems. recommender systems predicting users’ preferences partial knowledge personalized item recommendation. naturally derives completion problem treat unobserved ratings users items missing entries data matrices tensors. early works rendle illustrate power tensor-based methods personalized recommendation. proposed three dierent methods recommend items user would like tag. approach focuses learning best ranking minimizing area curve score rather element-wise error. method shown outperform tensor-based method hosvd ranking methods pagerank quality runtime. second approach special case decomposition. approach separates trinity interaction among user-item-tag summation pairwise interactions proved achieve higher prediction accuracy faster. finally propose mixture approach called factorization machines combining factorization models support vector machines beneted factorization models model could nest interactions among variables overcome sparsity scenarios svms fail. furthermore factorization machines general predictors could mimic matrix tensor-based factorization models svd++ second approach pitf. aempts hosvd-based dimensionality reduction also considered personalized recommendations tag-involved recommender systems. another transforming user-item matrix tensor object incorporate temporal information. early approaches bayesian probabilistic tensor factorization model proposed xiong extend matrix-based probabilistic approach well mcmc optimization method proposed demonstrates advantage highorder temporal model matrix-based static models. general case model considered karatzoglou take advantage dierent types context information considering additional dimensions besides traditional user-item matrix. exploit tucker factorization optimized method missing entries skipped decomposition process reconstructed end. multiverse recommendation approach outperforms traditional matrix-based systems wider margin demonstrates superiority multidimensional analysis contextual information available. situations side information users items well features available motivates combine auxiliary information collective learning. borrowing idea matrix case zheng target problem location-based activity recommendation system. couple four kinds side information including location features visiting information well similarity relationships users activities respectively. auxiliary regularizers eectively alleviate sparsity problem mining knowledge trajectory data mobile recommendations locations activities. recently propose model called taper contextual tensor-based approach tailored towards personalized expert recommendation. work recommending personalized topics produced experts social media dataset constructed third-order tensor rough modeling homogeneous similarities homogeneous entities heterogeneous similarities pair-wise entities proposed framework achieve high-quality recommendation measured precision recall. besides methods mentioned above models optimization techniques also applied recommender systems. general applications recommender systems focused decomposition-based approaches. because factorization models easy implement eective learning latent factors among users items incorporating interactions among dierent types modes. advantages dealing sparsity cold start problem usually encountered recommender systems described urban computing. urban computing deals human behaviors mobilities help computing technology benet living quality urban areas. trac mobility signicant topics also correlated tensor completion problem. example intelligent transportation systems outlier data generated malfunctions collection procedure record systems. researchers connect problem tensor completion problem problem seing dierent traditional completion problem assume positions missing parts known beforehand. another word partial existed entries might entirely noise data actual values still unknown corrupted data recovery problem addressed robust tensor recovery framework proved outperform traditional trace-norm approach proposed gandy wang focus problem estimating travel time paths city. construct third-order tensor based real-time trajectories road network data. address high sparsity tensor authors build another denser tensor based historical trajectories auxiliary matrices storing geographic similarity dierent road segments correlation among various time slots. tucker-based coupled matrix tensor factorization algorithm used jointly decomposition recovery estimation. information diusion. information diusion refers process piece information knowledge spread initiated sender reaches receivers medium interactions linked problem modeling predicting spatiotemporal dynamics online memes tensor completion. meme data constructed third-order tensor uncover full distribution incorporate latent auxiliary relationships among locations memes times novel recovery framework. experiments real-world twier hashtag dataset three kinds data missing scenarios locations missing entire locations) validate eectiveness tensor-based framework auxiliary spatiotemporal information. computer network. network trac matrix records amount information exchanged source destination pairs computers routers. third-tensor object could constructed since matrix evolves time. acar address completion problem computer network trac missing data arises high expense data collection process. dataset formed third-order tensor denoted source routers destination routers time entries indicate amount trac sent sources destinations specic time intervals. proposed method-cp weighted optimization-enjoys recovery accuracy scalability. computer vision signal processing area computer vision signal processing various problems could formulated tensor completion problems image inpainting video decoding compressed sensing spectral data analysis image completion. image completion image inpainting problem actively discussed computer vision practicability many real-world applications visual surveillance human-machine interaction image retrieval biometric identication. facial image analysis important research topics multilinear image analysis signoreo consider problem facial image completion using benchmark olivei face dataset. called pure completion problem hard completion problem. corresponding completion problem semi-supervised problem informs partial label information i.e. simultaneously complete images label matrix images. spectral regularized framework tackles pure completion problems multi-task cases. earlier works conducted geng consider hard completion problem. leverage hooi method model face images face completion recognition facial estimation. address missing value problem owing data collection apply em-like approach impute missing entries demonstrate extraordinary performance multilinear subspace analysis method estimation. hyperspectral data completion also widely discussed image completion work. early approach using hyperspectral images proposed gandy urban hyperspectral dataset image represents dierent band collected light wavelengths spatial resolution pixels. another benchmark dataset called ribeira hyperspectral image dataset used utilized signoreo tensor completion. datasets consist collections reectances sensed contiguous spectral bands. could apply tensor-based approaches treating scene third mode. also completion approaches dataset. besides kinds images many types images considered tensor completion problems building facade image reectance data ctmri images either natural images reconstructed datasets additional channel mode scene mode. leave discussion medical image analysis section healthcare medical application. video completion. videos naturally represented multidimensional arrays. besides application video inpainting video compression could also treated completion problem uncompress process. user remove unimportant unwanted pixels frame original videos recover using completion tools. earliest application tensor completion methods video datasets trace norm approach proposed also apply completion methods color videos naturally represented four-mode tensors kasai focuses streaming subspace tracking incomplete data exploits recursive least square method online decomposition. evaluate tracking performances using airport hall surveillance video frames video arrive sequential order. furthermore test performance proposed method tracking dynamic moving background. reconstruct input video stream reordering cropped incomplete frames show rapid adaptivity method olstec changed background. video completion examples could found compressed sensing. compressed sensing initiated allows recovery sparse signal small number nonadaptive linear random measurements ecient convex algorithms -minimization iterative hard thresholding exact recoveries high-dimensional sparse vectors dimensionality reduction. treat videos images signals compressions could treated special cases compressed sensing. extension compressed sensing kronecker-based compressed sensing model proposed explored order practical implementation compressed sensing high-order tensors. researchers focus building generalized tensor compressed sensing algorithms exploiting kronecker structure various approaches -minimization. sidiropoulos recovers sparse low-rank tensors measurements leveraging compressed sensing multilinear models tensor data shows identiability properties tensor decomposition model used recover low-rank tensor kronecker measurements concretely evolves steps low-rank model compressed domain performing per-mode decompression. rauhut reconstructs low-rank tensor relatively small number measurements utilizing framework hierarchical tensor formats based hierarchical svd-based approach followed similar work however noted method hard scale tensors large mode sizes strongly relies computing svds large matrices. friedland exploit unied framework compressed sensing high-order tensors preserving intrinsic structure tensor data provides ecient representation multi-dimensional simultaneous acquisition compression tensor modes. focus incorporating structured sparsity tensor representation compress/recover multi-dimensional signals varying non-stationary statistics inheriting merit tensor-based compressed sensing alleviate computational storage burden sampling recovery. caiafa cichocki study fast non-iterative tensor compressed sensing method based multilinear-rank model without assuming certain sparsity paerns suitable large-scale problems. healthcare bioinformatics medical applications medical images magnetic resonance imaging computed tomography scans powerful tools medical diagnosis. fast acquisition process image datasets incomplete. early applications gandy performed three medical imaging bazerque apply probability parafac approach corrupted brain images. incorporating prior information bayesian framework method able enhance smoothing prediction capabilities. similarly also brain dataset testing completion method combination trace-norm based decomposition-based approach. furthermore dynamic algorithm proposed mardani also applied streaming cardiac data fast online subspace tracking. besides applications medical images several types datasets analytics also conducted realm bioinformatics healthcare. dauwels apply decomposition handle missing data medical questionnaires. bazerque perform experiments sequencing data represents gene expression levels yeast. want focus problem computational phenotyping. incorporating types knowledgeguided constraints-guidance constraint pairwise constraint-into tensor completion process proposed framework rubik convert heterogeneous electronic health records meaningful concepts clinical analysis proceeding completion denoising tasks simultaneously. finally interdisciplinary related brain analysis signal processing acar multi-channel signals missing data encountered disconnections electrodes demonstrate usefulness scalable factorization method capturing underlying brain dynamics incomplete data. chemometrics chemometrics area using mathematical statistical tools improve chemical analysis. appellof davidson credited applies tensor factorization chemometrics since then tensor analysis become actively researched including low-rank approximation well missing data imputation etc. famous dataset tensor factorization completion semi-realistic amino acid uorescence data contributed andersson consists laboratory-made solutions three amino acids tomasi consider missing data problem using dataset. carry experiments three dierent types missing elements randomly missing values randomly missing spectra/vectors systematically missing spectra/vectors. similar spirit narita test kinds assumptions missing paerns using dierent chemical benchmark dataset injection. dataset chemical substances dataset represented third-order tensor substances wavelen❕ths reaction times. incorporating auxiliary similarity information mode approach greatly improves completion accuracy comparing existing methods especially observations sparse. explorations tensor completion problem chemometrics also found applications still many interdisciplinary applications hard categorize categories above. means integrate introduce leave others interested readers explore. climate data analysis. missing entries climatological data also direction applying tensor-based methods. silva apply hierarchical tucker algorithms careful choice dimension tree interpolate frequency slices generated seismic data. bahadori analyze datasets u.s. historical climatology network monthly comprehensive climate datasets collection climate records north america. completion method spatial-temporal auxiliary information tested cokriging forecasting tasks proved ecient also eective achieving lower estimation error. numerical analysis. applications directly related numerical analysis reconstruction function data solving parameterized linear system. example former recover compress tensors related functions singularities example laer could found paper complete solution tensor parametrized linear system obtained discretized stochastic elliptic karhunen-lo`eve expansion. experimental setup facilitate employment methods practitioners researchers section introduce several important components experimental seings including ways synthetic data constructions evaluation metrics available repositories. synthetic data construction ideal real-world tensor dataset known rank sampling assumption usually hard acquire. based dierent assumptions tensor rank outside noises sampling methods auxiliary information researchers provided dierent ways construct synthetic tensors. introduce three ways constructions including generate rank-r tensor rank rank-r tensor. rank commonly used could found construct third-order tensor rank-r randomly sampling entries factor matrices ri×r rj×r rk×r standard normal distribution column factor matrices normalized unit length constructed tensor denoted gaussian noise tensor represents noise parameter sometimes described signal-to-noise ratio signal-to-interference rate acar also mentioned construction ensures uniqueness decomposition probability since columns factor matrix linearly independent probability satises necessary conditions uniqueness dened missing entries could denoted binary tensor uniformly sampled bernoulli distribution. structured missing entries form missing bers slices also considered. concrete missing ratio sampling methods base incoherence assumptions described section specic completion methods use. rank tucker tensor. similarly construct tenor tucker rank utilizing tucker decomposition entries core tensor rr×r×r factorized matrices could randomly sampled factorized matrices usually orthogonalized. ways generate missing entries mentioned above. rank-r tensor auxiliary information. introduced several methods tensor completion auxiliary similarity information section utilizing auxiliary information might able perform exact recovery even situations missing slices. narita propose constructing synthetic tensors tri-diagonal similarity matrices. factor matrices generated following linear formula ...r constants generated synthetic tensor dened since factor matrix linear constructed column column neighboring rows similar other similarity matrix mode tri-diagonal matrix dened follows evaluation metrics completion problem could generalized prediction problem metrics usually selected based concrete applications rmse recommender systems binary classication ndcg information retrieval introduce several commonly used metrics below. mse/rmse/mae. mean-square error root-mean-square deviation mean absolute error frequently-used metrics especially recommendation problems formal denitions percision/recall/area curve. applications could treated classication problems link predictions recommendations. several metrics classication problems precision recall f-measure score could found tensor completion papers calculate metrics score could either extract missing entries average scores slice based requirements real-world applications. sowares repositories tensor analysis developed various platforms recent years table gives brief integration comparison tensor decomposition completion repositories dierent platforms. since lack work done developing python packages tensor decomposition completion develop open-source python package covering several widely used tensor decomposition completion algorithms researchers practitioners. details open-source package could found ocial website conclusion future outlook tensor completion problem permeates wide range real-world applications become actively studied aracting incremental aention. recent advances theory practice provide versatile potent weapons apply varies problems even auxiliary information informed large-scale dynamical paerns. however deeper delving challenges confront. reviewing past oers beer vision look forward infusive future directions volume tensor completion large-scale high-order data. large-scale highorder properties usually involved real-world tensors. increasing volume tensorstructured data puts higher demands completion algorithms time space www.pyten.tk perspectives. described section main challenges underlying large-scale tensor completion problem intermediate data explosion data decentralization. ough multiple approaches proposed address scalability problem still exists vast room innovation promotion. first space time complexities still highly limit practical application tensor decomposition/completion methods. second existing work focuses low-dimensional tensor completion problem. although methods based hierarchical tucker representations utilized address high-order problems generalizability robustness remain unclear. methods sampling methods require specic assumptions hard verify guarantee real-world complex tensors. velocity tensor completion dynamic data analytics. rapid velocity data growth dynamic tensor completion problem paid aention recently outlined section still riddled various open problems line intricate multi-aspect dynamic paerns. first handle dynamically changing tensors? recent aempts made solving streaming tensor completion problem eorts still needed coordinating multidimensional dynamics induced uncertainty tensor size modes. second inuences dierent dynamical paerns model construction parameter selection imputation eectiveness? example dynamically changed tensors cause rank assumption inapplicable. arbitrarily adopting static strategies might inappropriate reduce completion eectiveness. whether exist theoretical guarantees proposed algorithm dynamic seings not? best knowledge bare existing eorts provide theoretical analysis convergence sucient conditions statistical assumptions dynamic tensor completion. variety tensor completion heterogeneous data sources. practice tensor structured data exhibit heterogeneous properties viewed obtained dierent routes viewed dierent perspectives. critical challenging consider heterogeneous data sources benet tensor completion problem. first introduced section heterogeneous data sources could serve auxiliary information mitigate statistical assumptions enhancing completion eectiveness. exploring potential ways incorporate heterogeneous data source could valuable direction. second wide variety real-world data-driven applications domain knowledge expertise benecial modeling analysis understanding completion. several recent works tried incorporate human knowledge completion tasks prove acquire favorable performance area healthcare combine domain knowledge tensor completion problems could another promising future direction exploitation exploration. finally incorporating heterogeneous data large-scale dynamic seings also interesting meaningful pursue. acknowledgments work part supported darpa views opinions and/or ndings expressed author interpreted representing ocial views policies department defense u.s. government. arindam banerjee sugato basu srujana merugu. multi-way clustering relation graphs. icdm boaz barak ankur moitra. tensor prediction rademacher complexity random -xor. corr rasmus bro. parafac. tutorial applications. chemometrics intelligent laboratory systems rasmus bro. multi-way analysis food industry models algorithms applications. emmanuel candes yaniv plan. matrix completion noise. proc. ieee emmanuel cand`es justin romberg terence tao. robust uncertainty principles exact signal reconstruc andrzej cichocki rafal zdunek phan shun-ichi amari. nonnegative matrix tensor factorizations applications exploratory multi-way data analysis blind source separation. john wiley &amp; sons. pierre comon. canonical tensor decompositions. workshop tensor decompositions palo alto curt silva felix herrmann. hierarchical tucker tensor optimization-applications tensor completion. david donoho. compressed sensing. ieee transactions information theory marco duarte richard baraniuk. kronecker compressive sensing. ieee transactions image processing yonina eldar kutyniok. compressed sensing theory applications. cambridge university press. yonina eldar deanna needell yaniv plan. uniqueness conditions low-rank matrix recovery. applied hadi fanaee-t jo˜ao gama. multi-aspect-streaming tensor analysis. knowledge-based systems marko filipovi´c ante juki´c. tucker factorization missing data application low-n-rank tensor rainer gemulla erik nijkamp peter haas yannis sismanis. large-scale matrix factorization distributed stochastic gradient descent. proceedings sigkdd international conference knowledge discovery data mining. geng kate smith-miles. facial estimation multilinear subspace analysis. icassp geng kate smith-miles zhi-hua zhou liang wang. face image modeling multilinear subspace johan h˚astad. tensor rank np-complete. journal algorithms ren´e henrion. n-way principal component analysis theory algorithms applications. chemometrics joyce joydeep ghosh steve steinhubl walter stewart joshua denny bradley malin jimeng sun. limestone high-throughput candidate phenotype generation tensor factorization. journal biomedical informatics prateek jain sewoong provable tensor factorization missing data. nips swayambhoo jain alexander gutierrez jarvis haupt. noisy tensor completion tensors sparse charles johnson. matrix completion problems survey. symposia applied mathematics kang evangelos papalexakis abhay harpale christos faloutsos. gigatensor scaling tensor analysis alexandros karatzoglou xavier amatriain linas baltrunas nuria oliver. multiverse recommendation n-dimensional tensor factorization context-aware collaborative ltering. fourth conference recommender systems henk kiers. weighted least squares using ordinary least squares algorithms. psychometrika henk kiers. towards standardized notation terminology multiway analysis. journal chemometrics morris kline. mathematical ought ancient modern times volume usa. tamara kolda bader. tensor decompositions applications. siam review tamara kolda jimeng sun. scalable tensor decompositions multi-aspect data mining. ieee icdm pieter kroonenberg. applied multiway data analysis. vol. john wiley &amp; sons. pieter kroonenberg leeuw. principal component analysis three-mode data means monique laurent. matrix completion problems. encyclopedia optimization john lee. introduction smooth manifolds. springer science &amp; business media. schonfeld shmuel friedland. generalized tensor compressive sensing. ieee icme wu-jun yeung. relation regularized matrix factorization. ijcai yong wenrui hongkai xiong. compressive tensor sampling structured sparsity. ieee phan andrzej cichocki. parafac algorithms large-scale problems. neurocomputing holger rauhut reinhold schneider ˇzeljka stojanac. tensor completion hierarchical tensor representa benjamin recht. simpler approach matrix completion. journal machine learning research benjamin recht maryam fazel pablo parrilo. guaranteed minimum-rank solutions linear matrix sael inah jeon kang. scalable tensor mining. data research ruslan salakhutdinov andriy mnih. bayesian probabilistic matrix factorization using markov chain monte nicholas sidiropoulos lieven lathauwer xiao kejun huang evangelos papalexakis christos faloutsos. tensor decomposition signal processing machine learning. arxiv preprint arxiv. ajit singh georey gordon. relational learning collective matrix factorization. smilde rasmus paul geladi. multi-way analysis applications chemical sciences. john wiley ledyard tucker. mathematical notes three-mode factor analysis. psychometrika vasilescu demetri terzopoulos. multilinear analysis image ensembles tensorfaces. computer kenan yılmaz cemgil umut simsekli. generalised coupled tensor factorisation. nips jiaxi ying hengfa qingtao jian-feng jihui zhong chen xiaobo hankel matrix nuclear norm regularized tensor completion n-dimensional exponential signals. ieee transactions signal processing hsiang-fu cho-jui hsieh inderjit dhillon. scalable coordinate descent approaches parallel matrix factorization recommender systems. data mining ieee international conference ieee zemin zhang gregory shuchin aeron ning misha kilmer. novel methods multilinear data completion de-noising based tensor-svd. ieee conference computer vision paern recognition qibin zhao guoxu zhou liqing zhang andrzej cichocki shun-ichi amari. robust bayesian tensor", "year": 2017}