{"title": "Gated XNOR Networks: Deep Neural Networks with Ternary Weights and  Activations under a Unified Discretization Framework", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "There is a pressing need to build an architecture that could subsume these networks undera unified framework that achieves both higher performance and less overhead. To this end, two fundamental issues are yet to be addressed. The first one is how to implement the back propagation when neuronal activations are discrete. The second one is how to remove the full-precision hidden weights in the training phase to break the bottlenecks of memory/computation consumption. To address the first issue, we present a multistep neuronal activation discretization method and a derivative approximation technique that enable the implementing the back propagation algorithm on discrete DNNs. While for the second issue, we propose a discrete state transition (DST) methodology to constrain the weights in a discrete space without saving the hidden weights. In this way, we build a unified framework that subsumes the binary or ternary networks as its special cases.More particularly, we find that when both the weights and activations become ternary values, the DNNs can be reduced to gated XNOR networks (or sparse binary networks) since only the event of non-zero weight and non-zero activation enables the control gate to start the XNOR logic operations in the original binary networks. This promises the event-driven hardware design for efficient mobile intelligence. We achieve advanced performance compared with state-of-the-art algorithms. Furthermore,the computational sparsity and the number of states in the discrete space can be flexibly modified to make it suitable for various hardware platforms.", "text": "high power consumption greatly challenged applications. know dnns computing overheads result complex multiplication realvalued synaptic weight real-valued neuronal activation well accumulation operations. therefore compression methods binary/ternary networks emerge recent years dnns efﬁcient devices. former ones reduce network parameters connections change full-precision multiplications accumulations. latter ones replace original computations accumulations even binary logic operations. particular binary weight networks ternary weight networks constrain synaptic weights binary space ternary space respectively. multiplication operations removed. binary neural networks constrain synaptic weights neuronal activations binary space directly replace multiply-accumulate operations binary logic operations i.e. xnor. kind networks also called xnor networks. even advanced models issues remain unsolved. firstly reported networks based specially designed discretization training methods pressing need build architecture could subsume networks uniﬁed framework achieves higher performance less overhead. implement back propagation online training algorithms activations constrained discrete space addressed. side networks save full-precision hidden weights training phase causes frequent data exchange external memory parameter storage internal buffer forward backward computation. paper propose discretization framework multi-step discretization function constrains neuronal activations discrete space method implement back propagation introducing approximated derivative non-differentiable activation function; discrete state transition methodology probabilistic projection operator constrains synaptic weights discrete space without storage full-precision hidden weights whole training phase. discretization framework heuristic algorithm provided website https//github.com/acrossv/gated-xnor abstract—although deep neural networks revolutionary power open notoriously huge hardware overhead challenged applications. recently several binary ternary networks complex multiply-accumulate operations replaced accumulations even binary logic operations make on-chip training dnns quite promising. therefore pressing need build architecture could subsume networks uniﬁed framework achieves higher performance less overhead. fundamental issues addressed. ﬁrst implement back propagation neuronal activations discrete. second remove full-precision hidden weights training phase break bottlenecks memory/computation consumption. address ﬁrst issue present multistep neuronal activation discretization method derivative approximation technique enable implementing back propagation algorithm discrete dnns. second issue propose discrete state transition methodology constrain weights discrete space without saving hidden weights. build uniﬁed framework subsumes binary ternary networks special cases heuristic algorithm provided website https//github.com/acrossv/gated-xnor. particularly weights activations become ternary values dnns reduced gated xnor networks since event non-zero weight non-zero activation enables control gate start xnor logic operations original binary networks. promises event-driven hardware design efﬁcient mobile intelligence. achieve advanced performance compared state-of-the-art algorithms. furthermore computational sparsity number states discrete space ﬂexibly modiﬁed make suitable various hardware platforms. deep neural networks rapidly developing data sets powerful models/tricks gpus widely applied various ﬁelds vision speech natural language game multimodel tasks etc. however huge hardware overhead also notorious enormous memory/computation resources deng jiao contribute equally work. deng jiao ping department precision instrument center brain inspired computing research tsinghua university beijing china jiaopmails.tsinghua.edu.cn peijmail.tsinghua.edu.cn wuzhenzhimail.tsinghua.edu.cn liguoqimail.tsinghua.edu.cn corresponding liguoqimail.tsinghua.edu.cn forward backward passes complicated ﬂoat multiplications accumulations change simple logic operations xnor. however different xnor networks gxnor regarded sparse binary network existence zero state number zero state reﬂects networks’ sparsity. pre-neuronal activation synaptic weight nonzero forward computation required marked seen fig. indicates computation resources switched reduce power consumption. enable signal determined corresponding weight activation acts control gate computation. therefore network called gated xnor network. suppose layers gxnor network synaptic weights neuronal activations restricted discrete space except zeroth input layer activations layer. shown fig. last layer i.e. layer followed lsvm output layer standard hinge loss shown perform better softmax several benchmarks state number weights activations reconﬁgurable make suitable various hardware platforms. extreme case weights activations constrained ternary space form ternary neural networks multiplication operation weight activation zero zeros corresponding computation unit resting nonzero weight non-zero activation enable wake required computation unit. words computation trigger determined weight activation acts control signal/gate event computation. therefore contrast existing xnor networks tnns proposed paper treated gated xnor networks. test network model mnist cifar svhn datasets achieve comparable performance state-of-the-art algorithms. efﬁcient hardware architecture designed compared conventional ones. furthermore sparsity neuronal activations ﬂexibly modiﬁed improve recognition performance hardware efﬁciency. short gxnor networks promise ultra efﬁcient hardware future mobile intelligence based reduced memory computation especially event-driven running paradigm. training samples given label sample work going propose general deep architecture efﬁciently train dnns synaptic weights neuronal activations restricted discrete space deﬁned remark note different values denote different discrete spaces. speciﬁcally belongs binary space belongs ternary space also seen states constrained interval without loss generality range easily extended multiplying scaling factor following subsections ﬁrst investigate problem formulation gxnor networks constrained ternary space later investigate implement back propagation dnns ternary synaptic weights neuronal activations. finally uniﬁed discretization framework extending weights activations multi-level states presented. fig. gxnor network. gxnor network preneuronal activation synaptic weight non-zero forward computation required marked red. indicates gxnor network sparse binary network computation units switched reduces power consumption. enable signal determined corresponding weight activation acts control gate computation. denotes activation function represents synaptic weight neuron layer neuron layer training sample represents element input vector i.e. layer gxnor connected l-svm output layer neuronal activation fig. ternary discretization neuronal activations derivative approximation methods. quantization function together ideal derivative approximated mentioned introduction section order implement back propagation backward pass neuronal activations discrete need obtain derivative quantization function however well known continuous nondifferentiable shown fig. makes difﬁcult implement back propagation gxnor case. address issue approximate derivative respect follows small positive parameter representing steep degree derivative neighbourhood real applications many ways approximate derivative. example also approximated convenience presentation denote discrete space describing synaptic weight neuronal activation discrete weight space discrete activation space respectively. then special ternary space synaptic weight neuronal activation become respective ternary weight space ternary activation space ternary space deﬁned objective minimize cost function gxnor networks constraining synaptic weights neuronal activations forward backward passes. forward pass ﬁrst investigate discretize neuronal activations introducing quantized activation function. backward pass discuss implement back propagation ternary neuronal activations approximating derivative non-differentiable activation function. that discrete state transition methodology weight update aiming solve presented. nonlinear factor positive constant adjust transition probability probabilistic projection. among κij. example ij)) ∆wij happens sign ∆wij happens probability basically pgrad describes transition operation among discrete states deﬁned i.e. fig. illustration tws. weight directly transit current discrete state next discrete state updating weight without storage full-precision hidden weight. different current weight states well direction magnitude weight increment totally transition cases discrete space tws. approximated mentioned layer followed l-svm output layer hinge foss function applied training. then error back propagates output layer anterior layers gradient information layer obtained accordingly. investigate solve constraining iterative training process. weight state k-th iteration step weight increment derived based gradient information. guarantee next weight jump deﬁne establish boundary restriction transfer stay means probability stay probability transfer probability stay probability transfer probability transfer holds another boundary state based results solve optimization model based discrete state transition methodology. main idea update synaptic weight based ternary space exploiting projected gradient information. main difference ideas recent works bwns twns bnns xnor networks illustrated fig. works frequent switch data exchange continuous weight space binary weight space ternary weight space required training phase. fullprecision weights saved iteration gradient computation based binary/ternary version stored full-precision weights termed binarization ternary discretization step. stark contrast weights always constrained discrete weight space probabilistic gradient projection operator introduced directly transform continuous weight increment discrete state transition. illustration discretization synaptic weights. shows fig. existing schemes frequently switch spaces i.e. bsw/tws iteration. shows weights using always constrained whole training phase. remark since synaptic weights neuronal activations ternary space logic operations required forward pass. training phase remove saving full-precision hidden weights drastically reduces memory cost computation cost breaks overhead bottleneck. addition number zero state i.e. sparsity controlled adjusting makes framework efﬁcient real applications event-driven paradigm. actually binary ternary networks whole story since limited deﬁned non-negative integer. many hardware platforms support multi-level discrete space powerful processing ability interval similarly deﬁned implement back propagation algorithm derivative approximated discontinuous point illustrated fig. thus forward pass backward pass dnns implemented. fig. discretization neuronal activations multi-level values derivative approximation methods. multiple level quantization function together ideal derivative approximated mnist cifar svhn datasets. results shown table network structure mnist c-mp-c-mp-fc-svm cifar svhn ×-mp-×-mp×-mp-fc-svm. inputs normalized range cifar svhn adopt similar augmentation i.e. pixels padded side training images crop randomly sampled padded image horizontal version. inference phase test using single view original images. batch size mnist cifar svhn respectively. inspired learning rate decays training epoch decay factor initial ﬁnal learning rate respectively epochs number total training epochs. transition probability factor equation satisﬁes derivative approximation uses rectangular window fig. base algorithm gradient descent adam presented performance accuracy testing set. networks weights always constrained without saving full-precision hidden weights like reported networks table neuronal activations constrained results indicate really possible perform well even kind extremely hardware-friendly network architecture. furthermore fig. presents graph error curve evolves function training epoch. gxnor network achieve comparable ﬁnal accuracy converges slower full-precision continuous networks comparison table listed follows gxnor networks paper bnns xnor networks twns bwns full-precision seen proposed gxnor networks achieve comparable performance state-of-the-art algorithms networks. fact accuracy outperformed existing binary ternary methods. gxnor analyze inﬂuence several parameters section. firstly study nonlinear factor equation probabilistic projection. results shown fig. larger indicates stronger nonlinearity. seen properly increasing would obviously improve network performance large helps little. obtains best accuracy reason value experiments. secondly rectangular approximation fig. example explore impact pulse width recognition performance shown fig. large small value would cause worse performance simulation achieves highest testing performance signiﬁcantly degrades sparsity increases approaches sparsity approaches indicates exists best sparse space speciﬁed network data probably fact proper increase zero neuronal activations reduces network complexity overﬁtting avoided great extent like dropout technology valid neuronal information reduce signiﬁcantly network becomes sparse causes performance degradation. based analysis easily understand reason gxnor networks paper usually perform better bwns bnns twns. side sparser network hardware friendly means possible achieve higher accuracy less hardware overhead meantime conﬁguring computational sparsity. inﬂuence pulse width derivative approximation. fig. pulse width derivative approximation non-differentiable discretized activation function affects network performance. ‘not wide narrow pulse’ achieves best accuracy. inﬂuence sparsity neuronal activations. properly fig. increasing zero neuronal activations i.e. computation sparsity recognition performance improved. exists best sparse space neuronal activations speciﬁc network dataset. finally investigate inﬂuence sparsity network performance results presented fig. controlling width sparse window fig. sparsity neuronal activations ﬂexibly modiﬁed. observed network usually performs better state sparsity properly increases. actually fig. comparisons hardware computing architectures. neural network example neuron three inputs corresponding synaptic weights full-precision neural network multipliers accumulator. binary weight network multiplexers accumulator. ternary weight network multiplexers accumulator event-driven control. binary neural network xnor bitcount operations. gxnor network xnor count operations event-driven control. fig. implementation gxnor network example. introducing event-driven paradigm operations efﬁciently kept resting state valid gate control signals wake signal determined whether weight activation non-zero. different networks table hardware computing architectures quite different. illustrated fig. present typical hardware implementation examples triple-input-single-output neural network corresponding original network shown fig. conventional hardware implementation full-precision based multipliers multiplications activations weights accumulator dendritic integration shown fig. although unit nonlinear activation function required ignore cases fig. focus inﬂuence implementation architecture different discrete spaces. recent fig. replaces multiply-accumulate operations simple accumulation operation help multiplexers. neuron accumulates otherwise neuron accumulates −xi. contrast fig. implements accumulation event-driven paradigm adding zero state binary weight space. neuron regarded resting; weight non-zero also termed event neuron accumulation activated. sense acts control gate. constraining synaptic weights neuronal activations binary space fig. simpliﬁes accumulation operations efﬁcient binary logic xnor bitcount operations. similar event control proposed paper introduces event-driven paradigm based binary xnor network. shown fig. weight input non-zero xnor count operations enabled started. words whether equals zero plays role closing opening control gate hence name gated xnor network granted. table shows required operations typical networks fig. assume input number neuron i.e. inputs neuron output. removes multiplications original full-precision replaces arithmetical operations efﬁcient xnor logic operations. while full-precision bwns bnns/xnor networks states activations weights non-zero. resting probability furthermore gxnor network introduce event-driven paradigm. states test inﬂuence mnist dataset fig. presents results larger circle denotes higher test accuracy. weight direction observed network performs best; activation direction best performance occurs indicates exists best discrete space either weight direction activation direction similar conclusion inﬂuence analysis fig. fig. sparsity fig. sense discretization also efﬁcient avoid network overﬁtting improves algorithm performance. investigation section used guidance theory help choose best discretization implementation particular hardware platform considering computation memory resources. work provides uniﬁed discretization framework synaptic weights neuronal activations dnns derivative multi-step activation function approximated storage full-precision hidden weights avoided using probabilistic projection operator directly realize discrete state transition based this complete back propagation learning process conveniently implemented weights activations discrete. contrast existing binary ternary methods model ﬂexibly modify state number weights activations make suitable various hardware platforms limited special cases binary ternary values. test model case ternary weights activations mnist cifar svhn datesets achieve comparable performance state-of-theart algorithms. actually non-zero state weight activation acts control signal enable computation unit keep resting. therefore gxnor networks regarded kind sparse binary networks networks’ sparsity controlled adjusting pregiven parameter. what’s more gated control behaviour promises design efﬁcient hardware implementation using event-driven paradigm compared several typical neural networks hardware computing architectures. computation sparsity number states discrete space properly increased improve recognition performance gxnor networks. although gxnor networks promise event-driven efﬁcient hardware implementation quantitative advantages huge based current digital technology. generation control gate signals also requires extra overhead. power consumption reduced certain extent less state ﬂips digital circuits optimized increasing computation sparsity. even promising emerging nanodevices similar event-driven behaviour gated-control memristive devices using devices multi-level multiply-accumulate operations directly implemented computation controlled event signal injected third terminal ternary space follow uniform distribution resting probability accumulation operations reaches resting probability xnor bitcount operations gxnor reaches speciﬁcally twns synaptic weight three states neuronal activation fully precise. resting computation occurs synaptic weight average probability gxnor networks neuronal activation synaptic weight three states resting computation could occur either neuronal activation synaptic weight average probability remove special overlapped case ‘activation weight considered twice fig. demonstrates example hardware implementation gxnor network fig. original xnor operations reduced xnor operations required width bitcount operations also reduced. words gxnor network operations keep resting state valid gate control signals wake determined whether weight activation non-zero. sparse property promises design ultra efﬁcient intelligent devices help event-driven paradigm like famous event-driven truenorth neuromorphic chip inﬂuence state number discrete space. state fig. number discrete spaces weights activations multi-level values i.e. das. state parameters weight space activation space denoted respectively similar deﬁnition exists best discrete space respect either weight direction activation direction locates according fig. fig. know discrete spaces synaptic weights neuronal activations multi-level states. similar deﬁnition denote state parameters discrete weight space discrete activation space respectively. then available state number weights activations respectively. corresponds binary ternary weights corresponds binary ternary activations. merolla arthur alvarez-icaza cassidy sawada akopyan jackson imam nakamura brezzo esser appuswamy taba amir flickner risk manohar modha million spiking-neuron integrated circuit scalable communication network interface science esser merolla arthur cassidy appuswamy andreopoulos berg mckinstry melano barch nolfo datta amir taba flickner modha convolutional networks fast energy-efﬁcient neuromorphic computing proceedings national academy science united states america benjamin mcquinn choudhary chandrasekaran bussat alvarez-icaza arthur merolla boahen neurogrid mixed-analog-digital multichip system largescale neural simulations. proceedings ieee chen wang chen temam diannao small-footprint high-throughput accelerator ubiquitous machine-learning international conference architectural support programming languages operating systems prezioso merrikh-bayat hoskins adam likharev strukov training operation integrated neuromorphic network based metal-oxide memristors nature srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting. journal machine learning research burgt lubberman fuller keene faria agarwal marinella alec talin salleo non-volatile organic electrochemical device low-voltage artiﬁcial synapse neuromorphic computing nature materials control gate. characteristics naturally match well model multi-level weights activations modifying number states discrete space well event-driven paradigm ﬂexible computation sparsity. szegedy sermanet reed anguelov erhan vanhoucke rabinovich going deeper convolutions proc. ieee conference computer vision pattern recognition hinton deng dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition ieee signal proc. mag. huang siniscalchi uniﬁed approach transfer learning deep neural networks applications speaker adaptation automatic speech recognition neurocomputing devlin zbib huang lamar schwartz makhoul fast robust neural network joint models statistical machine translation proc. annual meeting association computational linguistics silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot dieleman grewe nham kalchbrenner sutskever lillicrap leach kavukcuoglu graepel hassabis mastering game deep neural networks tree search nature venkataramani ranjan raghunathan axnn energyefﬁcient neuromorphic systems using approximate computing proc. international symposium power electronics design qian tsui lradnn high-throughput energyefﬁcient deep neural network accelerator using rank approximation ieee asia south paciﬁc design automation conference courbariaux bengio david binaryconnect training deep neural networks binary weights propagations advances neural information processing systems courbariaux hubara soudry el-yaniv bengio binarized neural networks training deep neural networks weights activations constrained arxiv preprint arxiv. rastegari ordonez redmon farhadi xnor-net imagenet classiﬁcation using binary convolutional neural networks european conference computer vision", "year": 2017}