{"title": "Learning Approximate Inference Networks for Structured Prediction", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs. Prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them. We replace this use of gradient descent with a neural network trained to approximate structured argmax inference. This \"inference network\" outputs continuous values that we treat as the output structure. We develop large-margin training criteria for joint training of the structured energy function and inference network. On multi-label classification we report speed-ups of 10-60x compared to (Belanger et al, 2017) while also improving accuracy. For sequence labeling with simple structured energies, our approach performs comparably to exact inference while being much faster at test time. We then demonstrate improved accuracy by augmenting the energy with a \"label language model\" that scores entire output label sequences, showing it can improve handling of long-distance dependencies in part-of-speech tagging. Finally, we show how inference networks can replace dynamic programming for test-time inference in conditional random fields, suggestive for their general use for fast inference in structured settings.", "text": "structured prediction energy networks neural network architectures deﬁne energy functions capture arbitrary dependencies among parts structured outputs. prior work used gradient descent inference relaxing structured output continuous variables optimizing energy respect them. replace gradient descent neural network trained approximate structured argmax inference. inference network outputs continuous values treat output structure. develop large-margin training criteria joint training structured energy function inference network. multi-label classiﬁcation report speed-ups compared also improving accuracy. sequence labeling simple structured energies approach performs comparably exact inference much faster test time. demonstrate improved accuracy augmenting energy label language model scores entire output label sequences showing improve handling long-distance dependencies part-of-speech tagging. finally show inference networks replace dynamic programming test-time inference conditional random ﬁelds suggestive general fast inference structured settings. energy-based modeling associates scalar measure compatibility conﬁguration input output variables. given input predicted output chosen minimizing energy function structured prediction parameterization energy function leverage domain knowledge structured output space. however learning prediction become complex. structured prediction energy networks energy function score structured outputs perform inference using gradient descent iteratively optimize energy respect outputs. belanger develop end-to-end method unrolls approximate energy minimization algorithm ﬁxed-size computation graph trainable gradient descent. learning energy function however still must gradient descent test-time inference. replace gradient descent approach neural network trained inference call inference network. architecture takes input returns output interpretable prior work relax discrete continuous. multi-label classiﬁcation feed-forward network outputs vector. assign single label dimension vector interpreting value probability predicting label. sequence labeling output distribution predicted labels position sequence. adapt energy functions operate discrete ground truth outputs outputs generated inference networks. deﬁne large-margin training objectives jointly train energy functions inference networks. training objectives resemble alternating optimization framework generative adversarial networks inference network analogous generator energy function analogous discriminator. approach avoids argmax computations making training test-time inference faster standard spens. experiment multi-label classiﬁcation using setup belanger mccallum demonstrating speed-ups training time test-time inference also improving accuracy. design spen inference network sequence labeling using recurrent neural networks perform comparably conditional random ﬁeld using energy function faster test-time inference. also experiment richer energy includes label language model scores entire output label sequences using showing improve handling long-distance dependencies part-of-speech tagging. finally show inference networks replace dynamic programming test-time inference crfs suggestive general inference networks speed inference traditional structured prediction settings. structured prediction energy networks denote space inputs given input denote space legal structured outputs denote entire space structured outputs ∪x∈xy. spen deﬁnes energy function parameterized uses functional architecture compute scalar energy input/output pair. describe spen multi-label classiﬁcation belanger mccallum here ﬁxed-length feature vector. assume labels input energy function terms eloc elab. eloc linear models however solving requires combinatorial algorithms discrete structured space. becomes intractable decompose small parts belanger mccallum relax problem allowing discrete vector continuous. denote relaxed output space. solve relaxed problem using gradient descent iteratively optimize energy respect since train structured large-margin objective repeated inference required learning. note using gradient descent inference step time-consuming makes learning less stable. belanger propose end-to-end learning procedure inspired domke approach performs backpropagation step gradient descent. compare methods experiments below. belanger mccallum relaxed discrete continuous vector used gradient descent inference. also relax different strategy approximate inference. deﬁne inference network parameterized train goal architecture depend task. labels applicable every input length inputs. feed-forward network vector output treating dimension prediction single label. sequence labeling different length must network architecture permits different lengths predictions. returns vector position interpret vector probability distribution output labels position. note output must compatible energy function typically deﬁned terms original discrete output space require generalizing energy function able operate elements change required. sequence labeling change straightforward described section training pairs structured cost function returns nonnegative value indicating difference loss often referred margin-rescaled structured hinge loss however loss expensive minimize structured models cost-augmented inference step prior work spens step used gradient descent. replace cost-augmented inference network suggested notation cost-augmented inference network inference network typically functional form different parameters write optimization problem treat optimization problem minimax game saddle point game. following goodfellow implement using iterative numerical approach. alternatively optimize holding ﬁxed. optimizing completion inner loop training computationally prohibitive lead overﬁtting. alternate mini-batch optimizing optimizing also regularization terms objective cost-augmented inference network update yields output energy high cost order mimic cost-augmented inference. energy parameters kept ﬁxed. analogy generator gans trained produce high-cost structured output also appealing current energy function. help stabilize training several terms objective discussed section update widen cost-augmented ground truth outputs. analogy discriminator gans. energy function updated enable distinguish fake outputs produced real outputs training iterates updating using objectives above. initialize additional training training validation set. step helps resulting inference network produce outputs lower energy longer affected cost function. since procedure output labels could also applied test data transductive setting. approach also permits large-margin structured prediction slack rescaling slack rescaling yield higher accuracies margin rescaling requires cost-scaled inference training intractable many classes output structures. however notion inference networks circumvent tractability issue approximately optimize slack-rescaled hinge loss yielding following optimization problem optimize structured perceptron version using margin-rescaled hinge loss ﬁxing using loss cost-augmented inference network actually test-time inference network cost always zero using loss lessen need retune inference network training. margin-rescaled hinge equivalent slack-rescaled hinge. using useful standard max-margin training exact argmax inference potentially useful setting. consider spen objectives always nonzero difference energies never exactly equal discrete vector since explicit minimization discrete vectors case similar contrastive hinge loss seeks make energy true output lower energy particular negative sample margin least found alternating nature optimization difﬁculties training. similar observations noted alternative optimization settings especially underlying generative adversarial networks describe several techniques found help stabilize training optional terms added objective regularization regularization adding penalty term entropy regularization entropy-based regularizer lossh) deﬁned problem consideration. output vector scalars label scalar interpreted label probability. entropy regularizer lossh entropies label binary distributions. sequence labeling length unique labels output length-n sequence length-l vectors represents distribution labels position then lossh entropies label distributions across positions sequence. tuning coefﬁcient regularizer consider positive negative values permitting favor either lowhigh-entropy distributions task prefers. local cross entropy loss local cross entropy lossce deﬁned problem consideration. experiment loss sequence labeling. encouraging lower entropy distributions worked better sequence labeling higher entropy better similar effect found pereyra research required gain understanding role entropy regularization alternating optimization settings. label cross entropy losses positions sequence. loss provides explicit feedback inference network helping optimization procedure solution minimizes energy function also correctly classifying individual labels. also viewed multi-task loss inference network. regularization toward pretrained inference network penalty pretrained network e.g. local classiﬁer trained independently predict part methods reminiscent alternating optimization problems like underlying generative adversarial networks gans based minimax game value function agent seeks maximize another seeks minimize. analysis loss discriminator converges degenerate uniform solution. using hinge loss non-degenerate discriminator matching data distribution formulation closer hinge loss version gan. approach also related knowledge distillation refers strategies model trained mimic another typically teacher larger accurate model computationally expensive test time. urban train shallow networks using image classiﬁcation data labeled ensemble deep teacher nets. geras train convolutional network mimic lstm speech recognition. others explored knowledge distillation sequence-to-sequence learning parsing since train single inference network entire dataset approach also related amortized inference methods precompute save solutions subproblems faster overall computation. inference networks likely devote modeling capacity frequent substructures data. kind inference network used variational autoencoders approximate posterior inference generative models. methods also related work structured prediction seeks approximate structured models factorized ones e.g. mean-ﬁeld approximations graphical models like inference networks efforts designing differentiable approximations combinatorial search procedures structured losses training since relax discrete output variables continuous also connection recent work focuses structured prediction continuous valued output variables also propose formulation yields alternating optimization problem based proximal methods. settings gradient descent used inference e.g. image generation applications like deepdream neural style transfer well machine translation related settings gradient descent started replaced inference networks especially image transformation tasks results provide evidence making transition. alternative pursue would obtain easier convex optimization problem inference input convex neural networks sec. compare approach previous work training spens mlc. compare accuracy speed ﬁnding approach outperform prior work. perform experiments sequence labeling tasks sec. datasets used belanger mccallum bibtex delicious bookmarks. dataset statistics shown table appendix. bibtex delicious follow belanger mccallum tune hyperparameters using different sampling train test data standard train/test split ﬁnal experimentation using tuned hyperparameters. bookmarks train/dev/test split evaluation report example averaged measure. spen described section also used belanger mccallum feature representation network feed-forward networks hidden layers using layer widths bibtex/bookmarks delicious. pretrain feature networks minimizing independent-label cross entropy epochs using adam learning rate training spens update parameters energy function inference network keeping feature network parameters ﬁxed. adam learning rate train inference networks feed-forward networks hidden layers using architectures feature networks permits initialize inference network parameters using pretrained feature network parameters. output afﬁne transformation layer sigmoid nonlinearity function output values range interpret value probability predicting corresponding label. obtain discrete predictions thresholding threshold tuned maximize development data. three terms inference network objective section regularization entropy regularization regularization toward pretrained feature network. margin rescaling slack rescaling squared distance additional details provided sec. appendix. comparison prior work. table shows results comparing prior work. spen baseline results taken obtained spen results running code available authors datasets. method constructs recurrent neural network performs gradient-based minimization energy respect noted software release that method stable prone overﬁtting actually performs worse original spen. indeed case spen underperforms spen three datasets. method achieves best average performance across three datasets. performs especially well bookmarks largest three. results contrastive hinge loss retune inference network development data energy trained; decisions made based tuning described sec. four hinge losses similarly strong results. speed comparison. table compares training test-time inference speed among different methods. report speeds methods ran. spen times obtained using code obtained belanger mccallum. suspect spen training would comparable slower spen method process examples training times fast end-to-end spen times fast test-time inference. fact test time method roughly speed baseline since inference networks architecture feature networks form baseline. compared training method takes signiﬁcantly time overall joint training energy function inference network fortunately test-time inference comparable. also evaluate methods sequence labeling. report experiments twitter part-ofspeech tagging here. named entity recognition experiments reported appendix. energy functions sequence labeling input space sequences symbols drawn vocabulary. input sequence length possible output labels position output space notation represents containing ﬁrst positive integers. deﬁne ranges possible output labels i.e. deﬁning energy sequence labeling take inspiration bidirectional lstms conditional random ﬁelds linear chain uses types features capturing connection output label capturing dependence neighboring output labels. blstm compute feature representations denote input feature vector position deﬁning d-dimensional blstm hidden vector parameter vector label parameter matrix rl×l contains label pair parameters. full parameters includes vectors parameters blstm. energy permits discrete general case permits relaxing continuous treat vector. one-hot ground truth vector label probabilities relaxed y’s. general energy function entry vector discrete case entry single others energy reduces case. continuous case scalar indicates probability position labeled label label pair terms general energy function bilinear product vectors using parameter matrix also reduces one-hot vectors. language model. order capture long-distance dependencies entire sequence labels train language model large corpus automatically-tagged tweets include term energy function representing log-probability given sequence language model. details provided section table twitter accuracies blstm spen using tuned spen conﬁguration though slowest train spen matches test-time speed blstm achieving highest accuracies. twitter part-of-speech tagging annotated data gimpel owoputi contains tags. training combine tweet octtrain -tweet octdev set. validation -tweet octtest testing -tweet daily test set. -dimensional skip-gram embeddings trained million english tweets wordvec blstm compute input feature vector position using hidden vectors dimensionality also blstms inference networks. output layer inference network softmax function every position inference network produces distribution labels position. train inference networks using stochastic gradient descent momentum train energy parameters using adam. distance. tune hyperparameters validation set; full details tuning provided appendix. found cross entropy stabilization term worked well setting; details empirical comparison provided section compare standard blstm baselines. train blstm baseline minimize per-token loss; often called blstm tagger. train baseline using energy standard conditional log-likelihood objective using standard dynamic programming algorithms compute gradients training. details provided appendix. loss function comparison. table shows results comparing spen training objectives. larger difference among losses tasks. using perceptron loss margin leads overﬁtting validation test contrastive loss strives achieve margin better test also margin rescaling slack rescaling outperform contrastive hinge unlike tasks. suspect case input/output different length using cost captures length important. comparison standard baselines. table compares ﬁnal tuned spen conﬁguration standard baselines blstm tagger crf. spen achieves higher validation test accuracies faster test-time inference. method slower baselines here spen using functional form energy functions namely energy given note spen outperforms despite using form energy. factors explain this. first losses different. uses conditional log-likelihood spen results slack-rescaled hinge outperforms hinge loss variants second stabilization terms used training inference network providing regularizing effect model. motivation experiments show impact differences keeping form energy function ﬁxed. turn richer energies. results pairwise energy; results used language model compute energy term ﬁrst automatically unlabeled tweets train lstm language model automatic sequences. deﬁne input embeddings l-dimensional one-hot vectors specifying tags training sequences. nonstandard compared standard language modeling. standard language modeling train observed sequences compute likelihoods fully-observed sequences. however case train sequences want model sequences distributions produced inference network. train sequences one-hot vectors compute likelihoods sequences distributions. details training provided section appendix. deﬁne additional energy term etlm based pretrained tlm. argument consisted one-hot vectors could simply compute likelihood. however support relaxed need deﬁne general function start-of-sequence symbol y|y|+ end-of-sequence symbol tlmy yt−) returns softmax distribution tags position given preceding vectors. one-hot vector energy reduces negative log-likelihood sequence speciﬁed deﬁne joint energy energy function energy function learning keep parameters ﬁxed pretrained values tune weight energy joint energy. train spens joint energy using margin-rescaled hinge training inference network cross entropy term. table shows results. adding energy leads gain test set. settings showed variance; using slack-rescaled hinge found small drop test simply training inference networks ﬁxed pretrained joint energy tuned mixture coefﬁcient found gain test adding energy. investigated improvements found involve corrections seemingly stem handling non-local dependencies better. table appendix shows examples model appears better using broader context making tagging decisions. results suggest method baseline results differ slightly earlier results found could achieve higher accuracies spen training avoiding using pretrained feature network parameters inference network. table comparison test-time inference algorithms trained show test accuracy inference network setting best validation. inference networks architecture therefore essentially speed. note inference networks used prediction problem. explore inference network approximate test-time inference trained crf. results shown table results trained energy function trained minimize loss using forward-backward algorithm exact inference training. ﬁrst shows accuracy speed using viterbi test-time inference setting table subsequent rows show results training inference networks mimic viterbi various stabilization terms. training inference networks train training tune based early stopping validation set. energy stays ﬁxed inference networks trained. using either entropy cross entropy inference networks outperform viterbi doubling speed. using squared distance term accuracy reduces closer blstm reaches validation using stabilization terms inference network learning fails reaching development showing importance using stabilization term training inference network. presented ways jointly train structured energy functions inference networks using largemargin objectives. energy function captures arbitrary dependencies among labels inference networks learns capture properties energy efﬁcient manner yielding fast test-time inference. future work includes exploring space network architectures inference networks balance accuracy efﬁciency experimenting additional global terms structured energy functions exploring richer structured output spaces trees sentences. krzysztof geras abdel rahman mohamed rich caruana gregor urban shengjie wang ozlem aslan matthai philipose matthew richardson charles sutton. blending lstms cnns. proc. iclr kevin gimpel nathan schneider brendan o’connor dipanjan daniel mills jacob eisenstein michael heilman dani yogatama jeffrey flanigan noah smith. part-of-speech tagging twitter annotation features experiments. proc. olutobi owoputi brendan o’connor chris dyer kevin gimpel nathan schneider noah smith. improved part-of-speech tagging online conversational text word clusters. proc. naacl gregor urban krzysztof geras samira ebrahimi kahou ozlem aslan shengjie wang rich caruana abdel-rahman mohamed matthai philipose matthew richardson. deep convolutional nets really need deep? arxiv preprint arxiv. table shows dataset statistics multi-label classiﬁcation datasets. hyperparameter tuning. regularization strength chosen also done belanger mccallum tune coefﬁcients three stabilization terms inference network objective section follow ranges regularization entropy regularization regularization toward pretrained feature network comparison loss functions impact inference network retuning. table shows results comparing four loss functions section development bookmarks largest three datasets. performance highly similar across losses contrastive loss appearing slightly better others. training retune inference network speciﬁed development epochs using smaller learning rate table shows slightly higher losses retuning. surprised ﬁnal cost-augmented inference network performs well test-time inference network. suggests training cost-augmented network approaching argmin much need retuning. using retuning leads small gain using margin-rescaled slack-rescaled losses. gain presumably adjusting inference network inputs rather converting cost-augmented test-time inference network. training inference networks spens twitter tagging following hyperparameter tuning. tune inference network learning rate regularization entropy regularization term cross entropy regularization term squared distance train energy functions adam learning rate regularization table compares cross entropy entropy stabilization terms training inference networks spen margin-rescaled hinge. cross entropy works better entropy setting though retuning permits latter bridge halfway. obtain training data training language model twitter tagger owoputi dataset randomly-sampled english tweets. train language model tweets remaining tuning hyperparameters early stopping. train lstm language model sequences using stochastic gradient descent momentum early stopping validation set. used dropout rate lstm hidden layer. tune learning rate number lstm layers hidden layer size table shows examples spen includes appears using broader context making tagging decisions. examples test labeled models spen without spen example token that predicted determiner based local context correctly labeled pronoun using tlm. example difﬁcult noun/verb ambiguity next word impact that. examples show corrections token like highly ambiguous word twitter tagging. broader context makes much clearer intended. next examples cases noun/verb ambiguity resolvable larger context. last four examples show improvements nonstandard word forms. shortened form difﬁcult collision model able correctly. example ambiguous token frequently used short form twitter since comes context verb interpretation encouraged. however broader context makes clear verb tlm-enriched model tags correctly. words last examples nonstandard word forms observed training data likely reason erroneous predictions. using better handle rare forms based broader context. table named entity recognition blstm spen slack-rescaled hinge inference networks used cross entropy stabilization term. though slowest train spen matches test-time speed blstm improving points though lags behind crf. figure shows learned pairwise potential matrix twitter tagging. strong correlations labels neighborhoods. example adjective likely followed noun verb named entity recognition perform experiments english data conll shared task task contains sentences annotated named entities types containing training sentences development test set. four named entity types person location organization misc. bioes tagging scheme instead original following prior work classes. -dimensional pretrained glove embeddings trained billion words wikipedia text work better pretrained embeddings results shown table large -point blstm suggesting importance structured information problem. though spen still lags behind matches test-time speed blstm improving points.", "year": 2018}