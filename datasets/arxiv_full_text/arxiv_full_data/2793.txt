{"title": "Block-diagonal Hessian-free Optimization for Training Neural Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Second-order methods for neural network optimization have several advantages over methods based on first-order gradient descent, including better scaling to large mini-batch sizes and fewer updates needed for convergence. But they are rarely applied to deep learning in practice because of high computational cost and the need for model-dependent algorithmic variations. We introduce a variant of the Hessian-free method that leverages a block-diagonal approximation of the generalized Gauss-Newton matrix. Our method computes the curvature approximation matrix only for pairs of parameters from the same layer or block of the neural network and performs conjugate gradient updates independently for each block. Experiments on deep autoencoders, deep convolutional networks, and multilayer LSTMs demonstrate better convergence and generalization compared to the original Hessian-free approach and the Adam method.", "text": "second-order methods neural network optimization several advantages methods based ﬁrst-order gradient descent including better scaling large mini-batch sizes fewer updates needed convergence. rarely applied deep learning practice high computational cost need model-dependent algorithmic variations. introduce variant hessian-free method leverages block-diagonal approximation generalized gauss-newton matrix. method computes curvature approximation matrix pairs parameters layer block neural network performs conjugate gradient updates independently block. experiments deep autoencoders deep convolutional networks multilayer lstms demonstrate better convergence generalization compared original hessian-free approach adam method. deep neural networks shown great success computer vision natural language processing tasks models typically trained using ﬁrst-order optimization methods like stochastic gradient descent variants. vanilla incorporate curvature information objective function resulting slow convergence certain cases. momentum adaptive gradient-based methods sometimes used rectify issues. adaptive methods seen implicitly computing ﬁnite-difference approximations diagonal entries hessian matrix drawback ﬁrst-order methods general including adaptive ones perform best small mini-batches limits available parallelism makes distributed training difﬁcult. moreover distributed setting gradients must accumulated training update communication among workers become major bottleneck. optimization methods scale best distributed setting reach convergence parameter updates. weakness ﬁrst-order methods metric extends even convex case shown result correlation gradients different data points mini-batch leading overshooting direction correlation case deep neural networks large mini-batch sizes lead substantially increased generalization error although goyal recently successfully trained deep resnets imagenet dataset hour mini-batch size large using momentum-sgd equipped well-designed hyper-parameters also showed severe performance decay even larger mini-batch sizes indeed corroborates difﬁculty training large mini-batches. difﬁculties motivate revisit second-order optimization methods hessian curvature matrices rectify gradient direction. second-order methods employ information local structure loss function approximate quadratically rather linearly scale better large mini-batch sizes. however ﬁnding exact minimum quadratic approximation loss function infeasible deep neural networks involves inverting n-by-n curvature matrix parameter count hessian-free methods minimize quadratic function locally approximates loss using conjugate gradient method instead. involves evaluating sequence curvature-vector products rather explicitly inverting—or even computing—the curvature matrix hessian. hessian-vector product calculated efﬁciently using forward pass backward pass curvature-vector products similarly efﬁcient algorithms normally method requires many hundreds iterations update makes even single optimization step fairly computationally expensive. thus comparing ﬁrst-order methods beneﬁt terms fewer iterations incorporating curvature information often compensate added computational burden. propose using block-diagonal approximation curvature matrix improve hessian-free convergence properties inspired several results link concepts optimization methods. collobert argues training multilayer perceptron hidden layer gradient descent converges faster cross-entropy loss mean squared error hessian closely block-diagonal. block-diagonal approximation fisher information matrix kind curvature matrix also shown improve performance online natural gradient method training one-layer mlp. advantage block-diagonal hessian-free method updates certain subsets parameters independent gradients subsets. makes subproblem separable reduces complexity local search space hypothesize using block-diagonal approximation curvature matrix make hessian-free method robust noise results using relatively small mini-batch curvature estimation. cases collobert roux parameter blocks hessian fisher matrix block-diagonal composed weights biases involved computing activation neuron hidden output layers. thus equates statement gradient interactions among weights affect single output neuron greater weights affect different neurons. order strike balance curvature information provided additional hessian terms potential beneﬁts nearly block-diagonal curvature matrix adapt concept complex contemporary neural network models choose treat layer submodule deep neural network parameter block instead. thus different collobert roux hypothesis becomes gradient interactions among weights single layer useful training weights different layers. introduce block-diagonal hessian-free method detail test hypothesis comparing performance method deep autoencoder deep convolutional network multilayer lstm original hessian-free method adam method throughout paper boldface lowercase letters denote column vectors boldface capital letters denote matrices tensors superscript denote transpose. denote input sample label output network loss refers network parameters ﬂattened single vector. ﬁrst recall second-order optimization works. parameter update second-order method ﬁnds minimizes local quadratic approximation objective function point exist efﬁcient algorithms computing matrix-vector products given computationgraph representation loss function. curvature matrix hessian matrix second-order taylor expansion hessian-vector product computed gradient directional derivative loss function direction operations also known lr-operators l{·} rv{·} respectively r-operator implemented single forward traversal computation graph l-operator requires backward traversal hessian-vector product also computed gradient product vector gradient; method require r-operator twice computational cost. however objective deep neural networks non-convex hessian matrix mixture positive negative eigenvalues makes optimization problem unstable. common generalized gauss-newton matrix substitute curvature matrix always positive semideﬁnite objective function expressed composition functions convex property satisﬁed training objectives. curvature mini-batch data generalized gauss-newton matrix deﬁned jacobian matrix derivatives network outputs respect parameters hessian matrix objective respect network outputs approximation hessian results dropping terms involve second derivatives however still inefﬁcient solve problem deep neural network large number parameters propose block-diagonal hessian-free method. ﬁrst split network parameters parameter blocks. instance block contain parameters layer group adjacent layers. sub-problems corresponding block solved separately solutions concatenated together produce single update. algorithm block-diagonal hessian-free method input training data |st|}; neural network output function parameters loss function hyper-parameters maximum loops max_loops maximum conjugate gradient iterations max_cg_iters stop criterion cg_stop_criterion learning rate block partition partition network parameters blocks i.e. max_loops solve sub-problems separately conjugate gradient concatenate solutions together. hence update b-th sub-problem block-diagonal method equivalent minimizing overall objective constraint since second-order term constrained objective zero terms conﬁrms block-diagonal described equivalent ordinary curvature matrix replaced block-diagonal approximation includes terms involving pairs parameters block. problem separated independent sub-problems block reducing dimensionality search space needs consider. although sub-problems solve update sub-problem smaller size requires fewer iterations. hence total compute needs method mini-batch sizes; independent sub-problems executed parallel potential b-fold speed improvement. demonstrate below block-diagonal hessian-free achieves better performance method deep autoencoders multilayer lstms deep cnns. partition network parameters blocks based architecture network. partitioning network parameters deﬁne roughly equal sized blocks. allows sub-problem make roughly similar progress number iterations. seek partition network parameters whose gradients expect strongly correlated part block. example experiment split autoencoder network blocks encoder decoder. multilayer lstm treat layer recurrent cells block. deep divide convolutional layers three contiguous blocks. truncation. first iterations expensive later iterations provide diminishing improvements. importantly mini-batches evaluate curvature-vector product early termination keeps update overﬁtting speciﬁc mini-batch. reduce computational burden method smaller mini-batch sizes evaluate curvature-vector product still using large mini-batch evaluate objective gradient martens similarly implements method using full dataset evaluate objective gradient mini-batches calculate curvature-vector products. possible newton-like methods tolerant approximations hessian gradient implementation curvature mini-batch chosen strict subset gradient mini-batch shown algorithm however small mini-batches inevitably make curvature estimation deviate true curvature reducing convergence beneﬁts method ﬁrst-order optimization practice trivial choose mini-batch size balances accurate estimation curvature computational burden making hessian-free methods including block-diagonal hessian-free converge well small curvature mini-batches short runs tackle mini-batch overﬁtting. martens suggests using factored tikhonov damping make method stable. damping used curvature matrix make curvature more positive deﬁnite intensity damping. also incorporate damping many experiments. sake comparison damping strength method block-diagonal method choose ﬁxed value experiment. another suggestion made form momentum accelerate method. here momentum means initializing algorithm last solution scaled constant close rather initializing randomly zero vector. change often brings additional speedup little extra computation. apply ﬁxed momentum value experiments. also adopt ﬁxed hyper-parameter settings across experiments rather adaptive schedule. reason statistics control adaptive hyper-parameter scheduling cost gradient curvature-vector product evaluation makes method even slower. furthermore tricks independent often unclear adjust every scenario. ﬁxed hyperparameters work well practice across three different neural network architectures investigated. hessian matrix indeﬁnite nonconvex objectives makes second-order method unstable local quadratic approximation becomes unbounded below. advocates using generalized gauss-newton matrix curvature matrix instead guaranteed positive semi-deﬁnite. another circumvent indeﬁniteness hessian fisher information matrix curvature matrix; approach widely studied name natural gradient descent cases curvature matrices exactly equivalent also argued negative eigenvalues full hessian helpful ﬁnding parameters lower energy e.g. saddle-free newton method approach mixes hessian gauss-newton matrices recently martens grosse grosse martens propose k-fac method approximate natural gradient using block-diagonal block-tridiagonal approximation inverse fisher information matrix demonstrate advantages ﬁrst-order methods specialized version optimizer tailored deep convolutional networks. work parameters partitioned blocks similar size structure used method. figure performance comparison deep feedforward autoencoder mnist. early epochs adam reconstruction error greater models test error increases epochs. evaluate performance block-diagonal method three deep architectures deep autoencoder mnist dataset -layer lstm downsampled sequential mnist classiﬁcation deep based resnet architecture cifar classiﬁcation. three experiments ﬁrst compare performance block-diagonal method adam demonstrate block-diagonal hessian-free able handle large batch size efﬁciently. demonstrate advantage block-diagonal method ordinary hessian-free comparing performance various curvature mini-batch sizes. although block-diagonal method needs solve quadratic minimization problems sub-problem much smaller computation time similar method. note independence sub-problems means block-diagonal method particularly amenable distributed implementation. lasagne deep learning framework based theano implementation block-diagonal methods found software framework support convenient deﬁnition deep neural networks forward-mode automatic differentiation required implement r-operator. ﬁrst experiment conducted deep autoencoder task. goal neural network autoencoder learn low-dimensional representation data input distribution. encoder part multi-layer feedforward network maps input data low-dimensional vector representation decoder part another multi-layer feedforward network reconstructs input data given low-dimensional vector representation. autoencoder trained minimizing reconstruction error. mnist dataset composed handwritten digits size training samples test samples. pixel values training test data rescaled autoencoder composed encoder three hidden layers state sizes followed decoder mirror image encoder. tanh activation function mean squared error loss function. figure performance comparison -layer stacked lstm sequential mnist classiﬁcation task. early epochs adam loss greater models test accuracy decreases epochs. deﬁne blocks block encoder decoder. adam default setting lasagne learning rate performance comparison adam block-diagonal shown figure adam number dataset epochs needed converge ﬁnal achievable reconstruction error heavily affected mini-batch size similar number updates required small-mini-batch large-mini-batch training. block-diagonal method large mini-batch size achieves approximately reconstruction error adam small mini-batches requiring order magnitude fewer updates converge compared adam either small large mini-batches. moreover block-diagonal hessian-free provides consistently better reconstruction error—on train test sets—than method entire course training. advantage holds across different values curvature mini-batch size. multilayer lstm second experiment conducted using three-layer stacked lstm sequential mnist classiﬁcation task. mnist data downsampled average pooling. neural network three lstm layers followed fully-connected layer ﬁnal layer’s last hidden state. lstm hidden units peephole connections block-diagonal ﬁxed learning rate damping strength maximum iterations max_cg_iter block-diagonal method three blocks—one block lstm layer block also containing fully-connected layer. adam learning rate performance comparison block-diagonal adam found figure similar autoencoder case block-diagonal method large mini-batches requires fewer updates achieve lower training loss better test accuracy adam mini-batch size. furthermore compared block-diagonal method requires fewer updates achieves better minima exhibits less performance deterioration small curvature mini-batch sizes. also train deep convolutional neural network cifar- classiﬁcation task three optimization methods. cifar- dataset training samples test samples sample image three channels. model simpliﬁed version resnet architecture convolutional layer bottom followed three residual blocks fully-connected layer top. include batch normalization layers. figure performance comparison simpliﬁed residual cifar- image classiﬁcation task. early epochs adam loss greater models test accuracy decreases epochs. block-diagonal ﬁxed learning rate damping strength maximum iterations max_cg_iter block-diagonal method three blocks— residual block bottom blocks also containing fully-connected convolution layers respectively. default adam hyperparameters. common practice training deep cnns using custom-tuned learning rate decay schedules straightforwardly extend second-order case. however grosse martens suggests polyak averaging obviate need learning rate decay still achieving high test accuracy. order ensure fair comparison apply polyak averaging exponential decay rate evaluating test accuracy three algorithms. performance comparison block-diagonal adam found figure similar autoencoder case block-diagonal method large mini-batches requires fewer updates achieve lower training loss better test accuracy adam mini-batch size. furthermore compared method block-diagonal method requires fewer updates achieves better minima exhibits less performance deterioration small curvature mini-batch sizes. performance comparison block-diagonal hessian-free hessian-free adam found figure block-diagonal large mini-batches obtains comparable test accuracy adam small ones. furthermore block-diagonal method achieves slightly better training loss higher test accuracy—and substantially stable training—than hessian-free three different curvature mini-batch sizes. although plotted ﬁgures time consumption block-diagonal comparable experiments. time iteration block-diagonal times larger adam method. however total number iterations block-diagonal much smaller adam potential beneﬁt parallelization large mini-batches. propose block-diagonal method training neural networks. approach divides network parameters blocks separates conjugate gradient subproblem independent parameter block. extension original method reduces number updates needed training several deep learning models improving training stability reaching better minima. compared ﬁrst-order methods including popular adam optimizer block-diagonal scales signiﬁcantly better large mini-batches requiring order magnitude fewer updates large-batch regime. results strengthen claim collobert block-diagonal hessian easier train neural network showing that case hessian-free optimization simply ignoring off-block-diagonal curvature terms improves convergence properties. separability subproblems different parameter blocks block-diagonal method introduce inherently parallelizable ordinary method. future work take advantage feature apply block-diagonal method large-scale machine learning problems distributed setting. references al-rfou alain almahairi angermueller bahdanau ballas bastien bayer belikov belopolsky theano python framework fast computation mathematical expressions. arxiv preprint arxiv. dauphin pascanu gulcehre ganguli bengio. identifying attacking saddle point problem high-dimensional non-convex optimization. advances neural information processing systems pages dieleman schlüter raffel olson sønderby nouri maturana thoma battenberg kelly fauw heilman almeida mcfee weideman takács rivaz crall sanders rasul french degrave. lasagne first release. aug. http//dx.doi.org/./zenodo..", "year": 2017}