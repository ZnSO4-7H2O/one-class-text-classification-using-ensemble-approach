{"title": "A Connection between Feed-Forward Neural Networks and Probabilistic  Graphical Models", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Two of the most popular modelling paradigms in computer vision are feed-forward neural networks (FFNs) and probabilistic graphical models (GMs). Various connections between the two have been studied in recent works, such as e.g. expressing mean-field based inference in a GM as an FFN. This paper establishes a new connection between FFNs and GMs. Our key observation is that any FFN implements a certain approximation of a corresponding Bayesian network (BN). We characterize various benefits of having this connection. In particular, it results in a new learning algorithm for BNs. We validate the proposed methods for a classification problem on CIFAR-10 dataset and for binary image segmentation on Weizmann Horse dataset. We show that statistically learned BNs improve performance, having at the same time essentially better generalization capability, than their FFN counterparts.", "text": "popular modelling paradigms computer vision feed-forward neural networks probabilistic graphical models various connections studied recent works e.g. expressing mean-ﬁeld based inference ffn. paper establishes connection ffns gms. observation implements certain approximation corresponding bayesian network characterize various beneﬁts connection. particular results learning algorithm bns. validate proposed methods classiﬁcation problem cifar- dataset binary image segmentation weizmann horse dataset. show statistically learned improve performance time essentially better generalization capability counterparts. work study connections feed-forward neural networks graphical models latter divided classes directed undirected ones corresponding statistical models known bayesian networks markov random ﬁelds respectively. neural network side perhaps popular types restricted boltzmann machines feed-forward networks. rbms statistical models implement boltzmann distributions nothing mrfs. hence one-to-one correspondence rbms undirected gms. situation quite diﬀerent ffns. although architectures ffns basically same ffns represent probability distributions all. considered deterministic mappings transforming observed values output values. main work bridge ffns showing implements approximation corresponding undirected used successfully decades especially computer vision. directed appear less popular. time deep learning methods found ∗this work supported german federal ministry education research computations performed cluster center information services high performance computing dresden. tremendous success recent years especially convolutional neural networks successful techniques however statistical background yet. hence practical viewpoint goal work establish connection makes wide range elaborate statistical inference learning techniques available modern deep architectures. related works. many works showing certain algorithms graphical models represented applications specially designed networks. popular approach express mean-ﬁeld based inference techniques neural networks allows fast implementations shown express mean-ﬁeld based inference diﬀerentiable layer cnn. proposed directly learn messages energy minimization expressed terms neural computations. works however speciﬁc aspects considered. full power mrfs including uncertainty statistical learning decisions maximum a-posteriori etc. exploited. importantly connections established works particular algorithm corresponding architecture designed. contrast address opposite direction able transform given corresponding thereby taking account latter implements complex probability distribution rather optimization problem. observation incorporating stochastics learning process improve performance new. famous example drop-out method injecting noise references therein) also popular approach. note randomness improves generalization capabilities learned models usual statistical models contrast discriminative ones. however approaches stochastics rather ad-hoc manner proposed procedures derived statistically sound probability model constituents. probably closest work algorithms learning bayesian networks presented. approach employs techniques commonly used undirected graphical models. particular bayesian network transformed corresponding undirected graphical model higher order adapted version mean-ﬁeld approximation applied finally algorithm uses complex message passing scheme which interesting theoretical point view seems eﬃcient enough deep models large data. algorithm contrast rely mean-ﬁeld approximation aims maximize original likelihood using importance sampling. acknowledged authors importance sampling certain requirements order work practice namely proposal distribution small. unfortunately deep architectures seems opposite case. note works connection explicitly given. words statements interpret deterministic computations performed terms corresponding statistical contrast work provides interpretation making possible directly compare ffns statistical counterparts. viewpoint modeling work motivated following observation. recent trends computer vision combine ffns conditional random ﬁelds uniﬁed framework learn jointly diﬀerent ways achieve this. instance seen diﬀerentiable layer another option computing potentials sitting example maximum likelihood based learning scheme presented gradient likelihood approximately computed using belief propagation. eﬃcient sampling-based approach presented. however considered mapping producing potentials input deterministic manner. statistical model interpretation such. obviously ffn-crf combinations would proﬁtable able represent uniﬁed manner could combined transparent learning principles applied parts. contributions. first show i.e. deterministic mapping corresponding probabilistic directed graphical model bayesian network deﬁned implements probability distribution former approximation latter. discuss possible beneﬁts correspondence respect modeling learning inference. second based correspondence derive learning algorithm following properties. side implements statistical learning. consequence learned probabilistic models less vulnerable over-ﬁtting compared corresponding discriminative/deterministic approaches. time unlike many statistical learning algorithms method enjoys perfect sampling makes quite eﬃcient practice. side proposed method similar commonly used error back-propagation algorithm ffns. potentially allows proposed approach numerous ﬁndings made ffns past years like e.g. elaborated general solvers. nonetheless similarity proposed method seamlessly incorporated existing deep learning frameworks like e.g. caﬀe experimental section evaluate proposed approach respect obtained performance generalization capability classiﬁcation task cifar- database binary segmentation weizmann horse dataset. show improved performance great improvement generalization capability proposed method compared corresponding deterministic models. vector weights. statistical interpretation well known logistic regression. introduce random variable conditional probability distribution expx sigmx hence outputs sigmoid neurons interpreted probability values. problems prevent following speciﬁc interpretation neurons following. idea applied many types neurons tanh relu simply neurons compute values negative unbounded. deem next problem even crucial. assume output sigmoid neuron serves input another one. output value multiplied weight arbitrary. operation however statistical interpretation since obtained value arbitrary range depending weight. values neuron usually understood features. hardly possible reinterpret probability values features diﬃcult pose well-founded statistical model clearly deﬁned sample space random variables probability distribution etc. case. concerning relu neuron borrow idea shown relus represented sigmoid neurons appropriately deﬁned weights. since expectations expectation assert applicability relus well. fact possible represent deterministic mapping expectation. summarize neurons represented uniﬁed manner means following constituents random variables conditional probability distribution according representation application neuron consists computation expectation corresponding consider consists neurons vn}. neuron inputs simplest represent directed graph nodes correspond neurons directed edges deﬁned neuron associated output value characterized function denotes vector values neurons neurons input ones namely values neurons computed network given observations. also output neurons serve input neurons network. neurons denoted feed-forward architecture means graph directed cycles. hence possible introduce partial ordering neurons neuron higher rank inputs application network consists computing output values given input values since neurons ordered computation performed sequentially according introduced partial ordering corresponding probabilistic directed graphical model bayesian network built follows. first introduce random variables neuron. denote variables correspond input neurons output variables intermediate variables. variable takes values correspond particular type neuron. example sigmoid neuron tanh neuron input neurons etc. order unify simplify notation also introduce uniﬁed input vectors neurons vector input values neuron values non-input neuron deﬁne conditional probability distribution also corresponds particular type neuron expiv sigmoid neuron tanh weights). elementary event statistical model vector values variables denotes vector input values; deﬁned analogously. model conditional probability distribution ffns approximate corresponding bns. simplicity consider ﬁrst described hidden layer i.e. iv-s contain input variables also output depends i.e. clarity. since auxiliary variables interested conditional probability distribution output variables given input i.e. obtained marginalization probability distribution seen convex combination probability distributions diﬀerent values argument easy extend statement models hidden layer. consider layered structure follows. partition intermediate variables layers according previously introduced order neurons denote vector variable values l-th layer. probability distribution rewritten point would like discuss crucial diﬀerence original model sequential approximation fact latter implements conditionally independent posterior probability distribution since computed deterministically. means able capture dependencies output variables particular input principle. original model indeed desired posterior obtained marginalization hidden i.e. output variables conditionally independent. hence especially structured output could expect essentially better performance original model compared approximation. beneﬁt connection? essence presented correspondence ffns bayesian networks allows ffns rich diverse methods developed graphical models directed undirected ones since former trivially transformed latter. below give brief overview possible research directions context. inference. training ffns typically applied forward-propagation shown equivalent computing sequential approximation however since represent ffns graphical models inference strategies minimizing associated energy order compute maximum a-posteriori estimate. task known hard however many elaborated techniques energy minimization recently developed undirected graphical models thus pursue direction mainly corresponding energy minimization problem higher order hence specialized solver developed/chosen this. general apply bayesian decision theory loss function choice obtain corresponding estimator. associated optimization problem typically intractable. however note admit drawing perfect independent samples eﬃciently posterior hence rather easily approximate bayesian estimators samples. samples approximate maximum marginal decision experiments binary image segmentation learning. many approaches learning graphical models. example could standard conditional likelihood maximization original model however preliminary experiments found work well. hence instead present algorithm following section maximizes lower bound conditional likelihood found quite stable eﬃcient. another possibility would pose learning task discriminative manner using framework structural support vector machines overview) latent variables. important property ssvm framework possibility learn application-speciﬁc losses. unfortunately again order perform kind learning specialized solver corresponding energy minimization problems developed ﬁrst. cnn+crf combinations. using representation graphical model makes essentially easier combine basically means extend energy function adding potential functions associated crf. practical beneﬁts. leveraging connection ffns corresponding gives possibility combine alternate diﬀerent learning algorithms. instance common error back-propagation faster compared learning. moreover scales easily large amounts data. hence reasonable procedure could start learning error back-propagation convert learned corresponding perform ﬁne-tuning training hand stochastic methods known beneﬁcial escape local optima. hence another interesting direction would alternate error back-propagation statistical learning using former speed convergence latter escape local optima. .|t|) training data summarizes unknown parameters obtained marginalization hidden organized layers omit summation simplify notations i.e. concentrate optimization learning example ground truth summations infeasible. therefore employ stochastic gradient ascent follows draw sample according current probability distribution draw sample according current probability distribution compute apply stochastic gradient note stochastic gradient ascent approximation common sense exact limit inﬁnity. would also like point able draw perfect independent samples since directed graphical model behind probability distribution. particular draw sequentially needed note also sampling step performed eﬃciently since probability distributions conditionally independent. contrast sampling general undirected graphical models using e.g. gibbs sampling obtaining independent samples quite time consuming. order learn parameters l-th layer proceed follows. split network parts another output again using jensen’s inequality corresponding lower bound likelihood reads summarize resulting algorithm consists steps draw sample using current model propagate error network compute apply stochastic gradient like output layer. second step thereby exactly commonly used error back-propagation ffns. diﬀerence ﬁrst step deterministically computes output value neuron whereas algorithm output values sampled according corresponding conditional probability distributions. essence core diﬀerence statistical learning following. approximates underlying probability distribution uses exact gradient. approach train original model approximated gradient ascent learning. image classiﬁcation. here validate approach cifar- dataset used caﬀe framework experiments standard network implementation baseline. first experimented network variant built sigmoid neurons. order simplify experiments avoid inﬂuences additional design choices used simple stochastic gradient descent solver without momentum ﬁxed gradient step size. time slightly increased step size learned models essentially higher number iterations. baseline algorithm corresponding learned exactly parameters. also performed several runs methods averaged measurements. figure results cifar- benchmark. left training test accuracies baseline proposed approach dependencies time learning dashed lines training solid lines test. right results summary orig. original network appr. relus approximated sigmoids fig. shows dependencies training test accuracies time. beginning learning process test accuracy slightly outperforms relatively short time however saturates even starts decrease sign over-ﬁtting. contrast bn’s test accuracy constantly increases ﬁnally outperforms however even important opinion shows essentially smaller diﬀerence training test accuracies. hence much robust over-ﬁtting compared deterministic counterpart. results summarized fig. also experimented relu-variant network. used default parameters increasing number learning iterations. remember ﬁrst need approximate relus sigmoid neurons. actually makes test accuracy essentially worse orig. appr.). side original relu network turns extremely over-ﬁtting. previous esperiment sigmoids improves performance generalization deterministic counterpart. among experiments stochastic relus shows best generalization. binary image segmentation. next evaluate proposed method binary image segmentation weizmann horse dataset consists images showing horses side view diﬀerent backgrounds. order accelerate computations downscaled images size pixels. experiment designed convolutional architecture shown fig. formally input variables correspond image pixels whereas rgb-color. output variable pixel assigns label neurons organized layers neuron pixel. refer neurons coordinates layer number image coordinates. inputs neuron consists neurons previous layers within certain radius around neurons ﬁrst layers also access image pixels within radius neurons sigmoids. study model performance well generalization capability respect varying network depth number training examples. experiment consists following. first draw randomly pre-deﬁned number training examples rest used test. learn deterministic variant corresponding proposed algorithm. models learned perform inference using maximum marginal decision strategy compute intersection union models training test sets. experiment repeated times measures averaged. fig. shows dependencies network depth training size first clearly seen statistical model essentially better generalization capabilities since diﬀerence training test accuracies always smaller. concerning performance statistical model also superior cases. performance gain decreases larger training sets size training left weakest training middle strongest training set/model right averages training sets/models. dashed lines training solid lines test. bottom left used network architecture input sets neurons depicted bold shown. connection intermediate variables shown green connections image blue. example middle bottom right). indeed expected since well known discriminative approaches less robust respect over-ﬁtting general compared statistical ones. time less vulnerable misspeciﬁcation i.e. perform better large training data general. work established connection feed-forward neural networks bayesian networks. showed application seen approximation corresponding based connection developed learning algorithm bns. evaluation shows outperform corresponding ffns terms performance generalization capabilities. also characterized various beneﬁts connection ffns motivate work.", "year": 2017}