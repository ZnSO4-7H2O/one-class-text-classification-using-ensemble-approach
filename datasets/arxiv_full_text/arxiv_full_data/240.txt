{"title": "Deeply-Supervised Nets", "tag": ["stat.ML", "cs.CV", "cs.LG", "cs.NE"], "abstract": "Our proposed deeply-supervised nets (DSN) method simultaneously minimizes classification error while making the learning process of hidden layers direct and transparent. We make an attempt to boost the classification performance by studying a new formulation in deep networks. Three aspects in convolutional neural networks (CNN) style architectures are being looked at: (1) transparency of the intermediate layers to the overall classification; (2) discriminativeness and robustness of learned features, especially in the early layers; (3) effectiveness in training due to the presence of the exploding and vanishing gradients. We introduce \"companion objective\" to the individual hidden layers, in addition to the overall objective at the output layer (a different strategy to layer-wise pre-training). We extend techniques from stochastic gradient methods to analyze our algorithm. The advantage of our method is evident and our experimental result on benchmark datasets shows significant performance gain over existing methods (e.g. all state-of-the-art results on MNIST, CIFAR-10, CIFAR-100, and SVHN).", "text": "proposed deeply-supervised nets method simultaneously minimizes classiﬁcation error making learning process hidden layers direct transparent. make attempt boost classiﬁcation performance studying formulation deep networks. three aspects convolutional neural networks style architectures looked transparency intermediate layers overall classiﬁcation; discriminativeness robustness learned features especially early layers; effectiveness training presence exploding vanishing gradients. introduce companion objective individual hidden layers addition overall objective output layer extend techniques stochastic gradient methods analyze algorithm. advantage method evident experimental result benchmark datasets shows signiﬁcant performance gain existing methods much attention given resurgence neural networks deep learning particular unsupervised supervised hybrid form signiﬁcant performance gain observed especially presence large amount training data deep learning techniques used image classiﬁcation speech recognition hand hierarchical recursive networks demonstrated great promise automatically learning thousands even millions features pattern recognition; hand concerns deep learning raised many fundamental questions remain open. potential problems current frameworks include reduced transparency discriminativeness features learned hidden layers training difﬁculty exploding vanishing gradients lack thorough mathematical understanding algorithmic behavior despite attempts made theoretical side dependence availability large amount training data complexity manual tuning training nevertheless capable automatically learning fusing rich hierarchical features integrated framework. recent activities open-sourcing experience sharing also greatly helped adopting advancing machine learning community beyond. several techniques dropout dropconnect pre-training data augmentation proposed enhance performance various angles addition variety engineering tricks used ﬁne-tune feature scale step size convergence rate. features learned automatically algorithm intuitive portion features especially early layers also demonstrate certain degree opacity ﬁnding also consistent observation different initializations feature learning early layers make negligible difference ﬁnal classiﬁcation addition presence vanishing gradients also makes training slow ineffective paper address feature learning problem presenting algorithm deeply-supervised nets enforces direct early supervision hidden layers output layer. introduce companion objective individual hidden layers used additional constraint learning process. formulation signiﬁcantly enhances performance existing supervised methods. also make attempt provide justiﬁcation formulation using stochastic gradient techniques. show improvement convergence rate proposed method standard ones assuming local strong convexity optimization function several existing approaches particularly worth mentioning comparing with. layerwise supervised pre-training performed. proposed method perform pre-training emphasizes importance minimizing output classiﬁcation error reducing prediction error individual layer. important backpropagation performed altogether integrated framework. label information used unsupervised learning. semi-supervised learning carried deep learning classiﬁer used output layer instead standard softmax function framework choice using softmax classiﬁers emphasizes direct supervision intermediate layer. experiments show consistent improvement dsn-svm dsnsoftmax cnn-svm cnn-softmax respectively. observe state-of-the-art results mnist cifar- cifar- svhn. also worth mentioning formulation inclusive various techniques proposed recently averaging dropconnect maxout expect classiﬁcation error reduction careful engineering dsn. section give main formulation proposed deeply-supervised nets focus building infrastructure around supervised style frameworks introducing classiﬁer e.g. model layer. early attempt combine made however different motivation studies output layer preliminary experimental results. motivated following simple observation general discriminative classiﬁer trained highly discriminative features display better performance discriminative classiﬁer trained less discriminative features. features question hidden layer feature maps deep network observation means performance discriminative classiﬁer trained using hidden layer feature maps serve proxy quality/discriminativeness hidden layer feature maps quality upper layer feature maps. making appropriate feature quality feedback hidden layer network able directly inﬂuence hidden layer weight/ﬁlter update process favor highly discriminative feature maps. source supervision acts deep within network layer; proxy feature quality good expect much rapidly approach region good features would case rely gradual backpropagation output layer alone. also expect alleviate common problem gradients explode vanish. concern direct pursuit feature discriminativeness hidden layers might interfere overall network performance since ultimately feature maps output layer used ﬁnal classiﬁcation; experimental results indicate case. basic network architecture similar standard used framework. additional deep feedback brought associating companion local output hidden layer. think companion local output analogous ﬁnal output truncated network would produced. backpropagation error proceeds usual crucial difference backpropagate ﬁnal layer also simultaneously local companion output. empirical result suggests following main properties companion objective acts kind feature regularization leads signiﬁcant reduction testing error necessarily train error; results faster convergence especially presence small training data illustration running example). formulation focus supervised learning case ..n} input training data sample denotes input data corresponding groundtruth label sample drop notational simplicity since sample considered independently. goal deep nets speciﬁcally convolutional neural networks learn layers ﬁlters weights minimization classiﬁcation error output layer. here absorb bias term weight parameters differentiate weights ﬁlters denote recursive function layer denotes total number layers; ﬁlters/weights learned; feature produced layer refers convolved/ﬁltered responses previous feature map; pooling function combining layers weights gives name overall loss companion loss squared hinge losses prediction errors. formulation understood intuitively addition learning convolution kernels weights standard model enforcing constraint hidden layer directly making good label prediction gives strong push discriminative sensible features individual layer. eqn. respectively margin squared hinge loss classiﬁer output layer eqn. respectively margin squared hinge loss classiﬁer hidden layer. note directly depends dependent layer. depends decided entire second term eqn. often goes zero course training; overall goal producing good classiﬁcation output layer altered companion objective acts proxy regularization. achieved threshold second term eqn. hinge loss overall value hidden layer reaches vanishes longer plays role learning process. balances importance error output objective companion objective. addition could simple decay function enforce second term vanish certain number iterations epoch step total number epochs summarize describe optimization problem follows want learn ﬁlters/weights entire network classiﬁer trained output feature maps display good performance. seek output performance also requiring satisfactory level performance part hidden layer classiﬁers. saying restrict attention parts feature space that considered internal layers lead highly discriminative hidden layer feature maps main difference eqn. previous attempts layer-wise supervised training perform optimization altogether robust measure hidden layer. example greedy layer-wise pretraining performed either initialization ﬁne-tuning results overﬁtting stateof-the-art benchmark results demonstrate particular advantage formulation. shown figure indeed reach training error near zero demonstrates clear advantage better generalization capability. focus convergence advantage instead regularization generalization aspect. addition present problem learned features always intuitive discriminative difﬁculty training deep neural networks discussed observe eqn. change bottom layer weights propagated layers functions leading exploding vanishing gradients various techniques parameter tuning tricks proposed better train deep neural networks pretraining dropout provide somewhat loose analysis proposed formulation hope understand advantage effectiveness. objective function deep neural networks highly non-convex. make following assumptions/observations objective/energy function observes large area around optimal solution result similar performance; locally still assume convex function whose optimization often performed stochastic gradient descent algorithm update rule stochastic gradient descent step refers step rate helps project onto space optimum solution upper bounds strongly convex function convex function make attempt understand convergence eqn. w.r.t. presence large area function shown figure convergence rate given m-estimators locally convex function compositional loss regularization terms. terms eqn. refer class label prediction error reason calling second term companion objective. motivation two-fold encourage features learned layer directly discriminative class label prediction keeping ultimate goal minimizing class label prediction output layer; alleviate exploding vanishing gradients problem layer direct supervision ground truth labels. might raise concern learning highly discriminative intermediate stage ﬁlters necessarily lead best prediction output layer. illustration seen figure next give loose theoretical analysis framework also validated comprehensive experimental studies overwhelming advantages existing methods. deﬁnition name γ-feasible function first show feasible solution leads feasible lemma exists ˆw)) ˆw.. ˆw)) proof illustration network architecture shown trivial solution network every layer meaning ﬁlters identity matrices. results ˆw.. ˆw)) remark lemma shows good solution also good case around. makes small necessarily produce discriminative features hidden layers small however viewed regularization term. since observes area near even zero training data ultimately test error really care about thus focus makes small. therefore unreasonable assume share optimal strongly convex around theorem λ-strongly convex λ-strongly convex near optimal denote solution iterations applying respectively. deeply supervised framework eqn. improves speed using layer proof lemma shows compatibility companion objective w.r.t output objective ﬁrst equation directly derived lemma second equation seen lemma general leads great improvement convergence speed constraints hidden layer also helps learning ﬁlters directly discriminative. experiments evaluate proposed method four standard benchmark datasets mnist cifar- cifar- svhn. follow common training protocol used krizhevsky experiments. solver mini-batch size ﬁxed constant momentum value initial value learning rate weight decay factor determined based validation set. fair comparison clear illustration effectiveness match complexity model network architectures used comparable number parameters. also incorporate dropout layers dropout rate companion objective convolutional layers imposed backpropagate classiﬁcation error guidance underlying convolutional layers. learning rates annealed training factor according epoch schedule determined validation set. proposed framework difﬁcult train particular engineering tricks adopted. system built widely used caffe infrastructure network architecture setup adopted mlpconv layer global averaged pooling scheme introduced equipped different types loss functions softmax svm. show performance boost dsn-svm dsn-softmax cnn-svm cnn-softmax respectively performance gain evident presence small training data might partially ease burden requiring large training data overall observe state-of-the-art classiﬁcation error four datasets minist cifar- cifar- svhn results achieved without using averaging exclusive method. figure gives illustration learned features. figure classiﬁcation error mnist test. shows test error competing methods; shows test error w.r.t. training sample size. training testing error comparison. ﬁrst validate effectiveness proposed mnist handwritten digits classiﬁcation task widely extensively adopted benchmark machine learning. mnist dataset consists images different classes size training samples test samples. figure show results four methods namely conventional softmax loss proposed softmax loss max-margin objective proposed max-margin objective dsn-softmax dsn-svm outperform competing algorithms figure shows classiﬁcation error competing methods trained w.r.t. varying sizes training samples shows comparison generalization error dsn. cifar- cifar- cifar- dataset consists color images. total number images split training testing images. dataset preprocessed global contrast normalization. compare results previous state-of-the-art case also augmented dataset zero padding pixels side corner cropping random ﬂipping training. model averaging done test phase crop center also provides added robustness hyperparameter choice early layers guided direct classiﬁcation loss leading faster convergence rate relieved burden heavy hyperparameter tuning. also compared gradients observing times greater gradient variance ﬁrst convolutional layer. consistent observation assumptions motivations make work. features learned select example image categories cifar- dataset forward pass show feature maps learned ﬁrst convolutional layer figure activations shown feature maps. feature maps learned show intuitive cnn. cifar- dataset similar cifar- dataset except classes. number images class instead cifar- makes classiﬁcation task challenging. network settings cifar-. table shows previous best results reported dsn. performance boost consistently shown cifar- cifar- demonstrates advantage method. street view house numbers street view house numbers dataset consists digits training digits testing extra training samples color images. followed previous works data preparation namely select samples class training samples class extra set. remaining images used training. followed preprocess dataset local contrast normalization data augmentation training single model testing. table shows recent comparable results. note dropconnect uses data augmentation multiple model voting. paper presented formulation deeply-supervised nets attempting make transparent learning process deep learning. evident performance enhancement existing approaches obtained. stochastic gradient view also sheds light understanding formulation. work supported award iis- award iis- thank naiyan wang baoyuan wang jingdong wang liwei wang david wipf help discussions. greatful generous donation gpus nvidia.", "year": 2014}