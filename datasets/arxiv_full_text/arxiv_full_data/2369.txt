{"title": "Multiple decision trees", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "This paper describes experiments, on two domains, to investigate the effect of averaging over predictions of multiple decision trees, instead of using a single tree. Other authors have pointed out theoretical and commonsense reasons for preferring the multiple tree approach. Ideally, we would like to consider predictions from all trees, weighted by their probability. However, there is a vast number of different trees, and it is difficult to estimate the probability of each tree. We sidestep the estimation problem by using a modified version of the ID3 algorithm to build good trees, and average over only these trees. Our results are encouraging. For each domain, we managed to produce a small number of good trees. We find that it is best to average across sets of trees with different structure; this usually gives better performance than any of the constituent trees, including the ID3 tree.", "text": "paper describes averaging predictions authors pointed theoretical commonsense reasons preferring· multiple tree approach. ideally trees weighted probability. however vast·number different tree. sidestep trees difficult estimate probability build good estimation problem using modified version algorithm trees average trees. results encouraging. domain managed produce small number good trees. best average across sets trees different perfonnance constituent model make predictions cheeseman call method abduction. many authors pointed method optimal self cheeseman give example involving enemy. abduction corresponds would like predictions possible models weighted model. call method transduction using enemy small amount data case little basis preferring model others give misleading decision trees class models perfonned well classification compton probability. models possible abduction. small number ofmodels models used important models different possible. trouble constructing high school result< fail high school result> tempennent patienc pass tempennent impatienc high school maths result< fail high school maths result> pass success small training sets means strong correlations data must justified priori. also points prior must incorporate occam's razor observation simple trees superior simple trees easier understand effective representation knowledge. weight trees equally calculating average require systematic method building many trees comparable accuracy original tree. recall section generates. highest information gain. adopt interactive allows user select test. still calculate present user ranked list tests guidance highest information default tree independently several ways this. first individual probability estimates average them. average changed categorical result left probability estimate. candidate domains selected particular domains gives moderate performance them several good attributes tree. reasonable appropriate method anyway second performance good difficult improvement. the. student data contains high school university sydney students described mark.high school maths units mark faculty background computing opinions computer science course expectation results. task determine performance student computer science course. pass fail. data contains observations training classes test set. case pruned trees better performance pruned tree trees expained fact student domain significantly better.) attributes yield good infonnation gain levels tree whereas weather domain good attributes. used weather _data experiment different trees. weather data trees built fairly similar-either nodes. allows compare ·the performance combinations combinations effect using trees investigate increase number pruned unpruned trees. unpruned trees since individual· return cate·gorical trees average percentage combinations possible 'combinations limited numbers considered performance. additional significant weather student. established pruned trees using multiple trees gives student single class· probability used votipg method return categobcal effect returning average measure error. give feel for. half-brier half-brier half-brier", "year": 2013}