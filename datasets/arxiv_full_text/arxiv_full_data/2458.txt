{"title": "Interactive Restless Multi-armed Bandit Game and Swarm Intelligence  Effect", "tag": ["cs.AI", "cs.LG", "physics.data-an", "stat.ML"], "abstract": "We obtain the conditions for the emergence of the swarm intelligence effect in an interactive game of restless multi-armed bandit (rMAB). A player competes with multiple agents. Each bandit has a payoff that changes with a probability $p_{c}$ per round. The agents and player choose one of three options: (1) Exploit (a good bandit), (2) Innovate (asocial learning for a good bandit among $n_{I}$ randomly chosen bandits), and (3) Observe (social learning for a good bandit). Each agent has two parameters $(c,p_{obs})$ to specify the decision: (i) $c$, the threshold value for Exploit, and (ii) $p_{obs}$, the probability for Observe in learning. The parameters $(c,p_{obs})$ are uniformly distributed. We determine the optimal strategies for the player using complete knowledge about the rMAB. We show whether or not social or asocial learning is more optimal in the $(p_{c},n_{I})$ space and define the swarm intelligence effect. We conduct a laboratory experiment (67 subjects) and observe the swarm intelligence effect only if $(p_{c},n_{I})$ are chosen so that social learning is far more optimal than asocial learning.", "text": "abstract obtain conditions emergence swarm intelligence eﬀect interactive game restless multi-armed bandit player competes multiple agents. bandit payoﬀ changes probability round. agents player choose three options exploit innovate observe agent parameters specify decision threshold value exploit pobs probability observe learning. parameters uniformly distributed. determine optimal strategies player using complete knowledge rmab. show whether social asocial learning optimal space deﬁne swarm intelligence eﬀect. conduct laboratory experiment observe swarm intelligence eﬀect chosen social learning optimal asocial learning. probability bandit information change period payoﬀ bandit information changes probability expected payoﬀ bandit given summing values dividing", "year": 2015}