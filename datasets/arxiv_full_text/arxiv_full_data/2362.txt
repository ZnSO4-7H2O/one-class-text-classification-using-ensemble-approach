{"title": "Multi-class Generalized Binary Search for Active Inverse Reinforcement  Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "This paper addresses the problem of learning a task from demonstration. We adopt the framework of inverse reinforcement learning, where tasks are represented in the form of a reward function. Our contribution is a novel active learning algorithm that enables the learning agent to query the expert for more informative demonstrations, thus leading to more sample-efficient learning. For this novel algorithm (Generalized Binary Search for Inverse Reinforcement Learning, or GBS-IRL), we provide a theoretical bound on sample complexity and illustrate its applicability on several different tasks. To our knowledge, GBS-IRL is the first active IRL algorithm with provable sample complexity bounds. We also discuss our method in light of other existing methods in the literature and its general applicability in multi-class classification problems. Finally, motivated by recent work on learning from demonstration in robots, we also discuss how different forms of human feedback can be integrated in a transparent manner in our learning framework.", "text": "paper addresses problem learning task demonstration. adopt framework inverse reinforcement learning tasks represented form reward function. contribution novel active learning algorithm enables learning agent query expert informative demonstrations thus leading sampleeﬃcient learning. novel algorithm provide theoretical bound sample complexity illustrate applicability several diﬀerent tasks. knowledge gbs-irl ﬁrst active algorithm provable sample complexity bounds. also discuss method light existing methods literature general applicability multi-class classiﬁcation problems. finally motivated recent work learning demonstration robots also discuss different forms human feedback integrated transparent manner learning framework. social learning agent uses information provided individuals polish acquire anew skills likely become primary form programming complex intelligent systems paralleling social learning ability human infants artiﬁcial system retrieve large amount task related information observing and/or interacting agents engaged relevant activities. example behavior expert bias agent’s exploration environment improve knowledge world even lead reproduce parts observed behavior demonstration. particular form social learning commonly associated imitation emulation behaviors nature also possible numerous successful examples robot systems learn demonstration simplest form interaction demonstration consist examples right action take diﬀerent situations. approach learning demonstration adopt formalism inverse reinforcement learning task represented reward function representation agent construct policy solve target task. however unlike many systems learn demonstration paper propose combine ideas active learning order reduce data requirements learning. fact many agents able learn demonstration designed process batches data typically acquired actual learning takes place. data acquisition process fails take advantage information learner acquire early stages learning guide acquisition data. several recent works proposed interactive learning actually lead improved learning performance. adopt bayesian approach following ramachandran amir allow learning agent actively select query expert desired behavior informative situations. contribute theoretical analysis algorithm provides bound sample complexity learning approach illustrate method several problems literature. accommodate diﬀerent forms feedback also useful. fact situations user unable properly demonstrate intended behavior instead prefers describe task terms reward function customary reinforcement learning example suppose user wants agent learn navigate complex maze. user experience diﬃculties navigating maze instead allow agent explore maze reward exiting maze. additionally recent studies behavior naïve users instructing agents showed feedback provided humans often ambiguous obvious manner either reward function policy instance observed human users tend provide learning agents anticipatory guidance rewards situation seldom considered reinforcement learning study concludes robust agents able successfully learn human users ﬂexible accommodate diﬀerent forms feedback user. order address issues above discuss forms expert feedback integrated seamless manner framework learner able recover eﬃciently target task. particular show combine policy reward information learning algorithm. approach thus provides useful bridge reinforcement learning imitation learning line work seldom explored literature remainder section provide overview related work social learning particularly learning demonstration. also discuss relevant research active learning discuss contributions light existing work. section revisits core background concepts introducing notation used throughout paper. section introduces active algorithm gbs-irl provides theoretical analysis sample complexity. section illustrates application gbs-irl several problems different complexity providing empirical comparison methods literature. finally section concludes paper discussing directions future research. extensive literature reporting research intelligent agents learn expert advice. many examples feature robotic agents learn simple tasks diﬀerent forms human feedback. examples include robot leonardo able learn tasks observing changes induced world human demonstrating target task breazeal learning leonardo provides additional feedback current understanding task human user provide additional information. refer survey works argall lopes comprehensive discussion learning demonstration. paper already mentioned adopt inverse reinforcement learning formalism introduced seminal paper russel appealing aspect approach learning demonstration learner mimicking observed actions. instead learner infers purpose behind observed behavior sets purpose goal. also enables learner accommodate diﬀerences demonstrator appealing features discussed several researchers address learning demonstration perspective. abbeel explored inverse reinforcement learning context apprenticeship learning purpose learning agent replicate behavior demonstrator able observe sequence states experienced task execution. formalism allows learner reason tasks could lead demonstrator visit observed states infer replicate inferred behavior. syed explored line reasoning game-theoretic perspective proposed algorithms learn demonstration provable guarantees performance learner. ramachandran introduced bayesian inverse problem cast bayesian inference problem. given prior distribution possible target tasks algorithm uses demonstration expert evidence compute posterior distribution tasks identify target task. unfortunately monte-carlo markov chain algorithm used approximate posterior distribution computationally expensive requires extensive sampling space possible rewards. avoid complexity several posterior works departed birl formulation instead determine task maximizes likelihood observed demonstration aforementioned maximum likelihood approaches lopes babes take advantage underlying problem structure derive simple gradient-based algorithms determine maximum likelihood task representation. closely related works maximum entropy approach ziebart gradient approach szepesvari former selects task representation maximizes likelihood observed expert behavior maximum entropy distribution latter explores gradient-based approach task representation selected induce behavior similar possible expert behavior. finally ross bagnell propose learning algorithm reduces imitation learning classiﬁcation problem. classiﬁer prescribes best action take possible situation learner encounter successively improved enriching data-set used train classiﬁer. works designed learn whatever data available learning time data typically acquired actual learning takes place. data acquisition process fails take advantage information learner acquires early stages learning guide acquisition informative data. active learning aims reduce data requirements learning algorithms actively selecting potentially informative samples contrast random sampling predeﬁned distribution case learning demonstration active learning used reduce number situations expert/human user required demonstrate. instead learner proactively expert demonstrate desired behavior informative situations. conﬁdence-based autonomy proposed chernova veloso also enables robot learn task human user building mapping situations robot encountered adequate actions. work already incorporates mechanism enables learner expert right action encounters situation less conﬁdent correct behavior. system also allows human user provide corrective feedback robot executes learned task. querying strategy classiﬁed stream-based mellow stream-based since learner presented stream samples asks labels samples feels uncertain about. mellow since seek highly informative samples queries sample informative. literature active learning ﬁrst explored preliminary version paper early version learner actively queries expert correct action states uncertain correct behavior. unlike active sampling approach aggressive uses membership query synthesis. aggressive since actively selects highly informative samples. unlike select queries whole input space. judah propose similar approach imitation query-bycommittee algorithm diﬀers previous active sampling approach fact learner able accommodate notion states i.e. states avoided task execution. cohn propose another closely related approach that however uses diﬀerent criterion select situations query. emg-aqs queries expert correct action states expected gain information potentially larger. unfortunately discussed cohn determination expected gain information requires extensive computation rendering emg-aqs computationally costly. diﬀerent line work ross judah address imitation learning using no-regret framework propose algorithms direct imitation learning provable bounds regret. finally melo lopes active learning metric approach learning demonstration. approach paper modiﬁed version original active sampling algorithm depart generalized binary search algorithm nowak adapt setting. purpose cast classiﬁcation problem extend algorithm nowak multi-class setting. analyze sample complexity gbs-irl approach thus providing ﬁrst active algorithm provable bounds sample complexity. also extent knowledge gbs-irl ﬁrst aggressive active learning algorithm non-separable multi-class data conclude discussion related work pointing works describe systems learn human feedback. however forms expert advice also explored agent learning literature. price boutilier explored learning agent improve performance observing similar agents could seen implicit imitation learning. works demonstrator purposes oblivious fact actions observed learned from. instead learned observes behavior agents extracts information useful learning general setting barto rosenstein discuss diﬀerent forms supervisory information integrated reinforcement learning architecture improve learning. finally knox stone introduce tamer paradigm enables reinforcement learning agent human feedback guide learning process. sample-complexity analysis gbs-irl. establish suitable conditions exponential convergence active learning method function number samples. pointed earlier knowledge ﬁrst work providing sample complexity bounds active irl. several experimental results conﬁrm good sample performance approach. general discussion diﬀerent forms expert information integrated setting. illustrate applicability ideas several simple scenarios discuss applicability section introduces background material markov decision processes bayesian inverse reinforcement learning formalism upon contributions developed. markov decision problem describes sequential decision problem agent must choose sequence actions maximizes rewardbased optimization criterion. formally tuple represents state-space ﬁnite action space represents transition probabilities reward function positive discount factor. denotes probability transitioning state state action taken i.e. write denote indicator function words greedy policy respect deﬁned probability distribution uniform zero complement. assume without loss generality least consider perturbed version where action selected probability where typically note uniform policy obtained limits setting respectively. following bayesian paradigm likelihood observing action demonstrator state given target task given policy assigns probability actions that particular reward optimal state similarly assigns probability corresponding sub-optimal actions. perturbed version convenient simplicity facilitates analysis. however versions perturbed policies considered literature—see example works ramachandran amir szepesvari lopes seen above describes sequential decision making problem agent must choose actions maximize total discounted reward. sense reward function encodes task agent. inverse reinforcement learning deals problem recovering task representation given demonstration behavior learned paper adopt formulation ramachandran amir cast bayesian inference problem agent provided samples desired policy must identify target reward function general possible functions prior observation policy sample given measurable initial belief encoded form probability density function deﬁned i.e. indexed samples write denote index clear context. prior distribution induces equivalent distribution abusively also denote history observations time-step develop active learning algorithm setting convert problem determining equivalent classiﬁcation problem. mostly amounts rewriting bayesian problem section using diﬀerent notation. deﬁne hypothesis space follows. every hypothesis }|a| deﬁned function write denote component intuitively identiﬁes greedy actions respect assigning value actions. take functions note that since every reward prescribes least optimal action state holds every every least write denote target hypothesis corresponding note that accommodate possibility having access inaccurate estimates respectively. consider partition state-space disjoint family sets hypotheses constant words states lying indistinguishable since means hypothesis space induces equivalence relation elements equivalent write denote representative while partition perhaps little relevance problems small state-space central problems large state-space since state queried selected alternatives instead alternatives. analysis gbs-irl relies following fundamental lemma generalizes lemma nowak multi-class settings. lemma denote hypothesis space deﬁned assumed k-neighborly. deﬁne coherence parameter deﬁnition pair k-neighborly sets xixj sequence k-neighbors. notion k-neighborhood structures statespace terms hypotheses space structure exploited active learning purposes. deﬁning active algorithm ﬁrst consider simpliﬁed setting following assumption holds. postpone section discussion general case. assumption every every |ah| words focus case hypothesis considered prescribe unique optimal action state. single optimal action state implies noise model simpliﬁed. particular noise model constant across hypothesis since prescribes number optimal actions state denote estimates respectively consider bayesian update form position introduce ﬁrst version active learning algorithm inverse reinforcement learning generalized binary search gbs-irl summarized algorithm ﬁrst version algorithm relies great uncertainty concerning optimal action alternatively k-neighboring states except hypothesis predict action predicted optimal action strikingly diﬀerent states. either case possible select query highly informative. coherence parameter multi-class equivalent coherence parameter introduced nowak quantiﬁes informativeness queries. always exists established noting partition ﬁnite therefore minimization conducted exactly. hand include trivial hypotheses constant holds position establish convergence properties algorithm denote probability measure corresponding expectation governing underlying probability noise possible algorithm randomizations query selection. theorem denote possible history observations obtained gbs-irl. update verify theorem establishes consistency active learning multi-class classiﬁcation. proof relies fundamental lemma that roughly speaking ensures sequence increasing expectation. fundamental lemma generalizes related result nowak that consideration multiple classes gbs-irl apply. generalization requires particular stronger assumptions noise implies diﬀerent rate convergence soon become apparent. also worth mentioning statement theorem could alternatively proved using adaptive sub-modularity argument using results golovin krause theorem extends theorem nowak multi-class case. however existence multiple actions constants obtained bounds diﬀer obtained aforeinterestingly mentioned work close zero convergence rate obtained nearoptimal exhibiting logarithmic dependence dimension hypothesis space. fact following straightforward corollary theorem conclude section note reduction standard classiﬁcation problem implies algorithm specialized particular problems—in particular used general classiﬁcation problems. additionally guarantees theorems also generally applicable multi-class classiﬁcation problems verifying corresponding assumptions. discuss general applicability results section particular discuss assumptions considered theorem namely neighborly condition assumption also discuss additional forms expert feedback integrated seamless manner gbsirl approach learner able recover eﬃciently target task. assumption formulated theorem -neighborly assumption states neighborly meaning possible structure state-space manner coherent hypothesis space assess validity assumption general start recalling sets xixj -neighbors single hypothesis prescribes diﬀerent optimal actions then -neighborly every sets xixj connected sequence -neighbor sets. general given multi-class classiﬁcation problem hypothesis space -neighborly assumption investigated verifying connectivity -neighborhood graph induced refer work nowak detailed discussion case similar arguments carry multi-class extension. particular case inverse reinforcement learning important assess whether -neighborly assumption reasonable. given ﬁnite state-space ﬁnite action-space possible build total |a||x| diﬀerent hypothesis. shown work melo hypothesis always possible build non-degenerate reward function yields hypothesis optimal policy. therefore suﬃciently rich reward space ensures corresponding hypothesis space includes |a||x| possible policies already alluded trivially implies -neighborly. unfortunately also shown aforementioned work consideration possible policies also implies states must suﬃciently sampled since generalization across states possible. observation supports option research focus problems rewards/policies selected restricted particular case active learning approaches consideration full rewards/policies also implies little hope active sampling provide negligible improvement sample complexity. related observation found work dasgupta context active learning binary classiﬁcation. situations -neighborly assumption veriﬁed lemma cannot used ensure selection highly informative queries however still possible main approach gbs-irl detailed algorithm situation specialize sample complexity results following immediate corollary. multiple optimal actions presentation assumed that |ar| informally corresponds assuming that every reward function considered single optimal action assumption considered either explicitly implicitly several previous works learning demonstration closer work active several works recast classiﬁcation problem focusing deterministic policies therefore although explicitly also consider single optimal action state. state). situation properties resulting algorithm follow previous analysis since existence multiple optimal actions necessarily requires general noise model. immediate extension noise model scenario multiple optimal actions allowed poses several diﬃculties optimal actions across policies sampled diﬀerent probabilities. order overcome diﬃculty consider conservative bayesian update enables seamless generalization results scenarios admit multiple optimal actions state. update arises considering likelihood observing action state given equivalently likelihood observing action given before correspond values target hypothesis aggregated noise model enables consideration approximate noise model constant across hypothesis deﬁned terms estimates given noise model described bayesian update unfortunately allowing multiple optimal actions state also much easier situations case bounds void. however focus identifying state least optimal action able retrieve guarantees sample complexity active learning approach. thus consider another version gbs-irl described algorithm uses uses threshold that consider optimal action identiﬁed. done outputs likely hypothesis. least optimal action identiﬁed states algorithm stops. note result longer formulated terms identiﬁcation correct hypothesis terms identiﬁcation optimal actions. also following result sample complexity version gbs-irl. finally worth noting that presentation admits queries what optimal action state however possible devise diﬀerent types queries enable recover stronger results theorem fact query exempliﬁed reduces problem binary classiﬁcation problem existing active learning methods nowak readily applied. discussed possible approach agent provided demonstration consisting pairs states corresponding actions. demonstration agent must identify underlying target task represented reward function depart bayesian formalism introduced describe reward information also integrated. addition reward information demonstrations include state-reward pairs indicating reward state takes value seen similar approach thomaz breazeal knox stone reinforcement learning. main diﬀerence that aforementioned works actions experienced learner receives rewards environment teacher. another related approach introduced regan boutilier context reward design mdps. i.e. state would queried algorithm time-step user instead wishes provide reward information would like replace query alternative query disambiguates much possible actions state xt+—much like direct query would. purpose partition space rewards less disjoint sets r|a| contains precisely rewards select state reward best discriminates sets r|a|. algorithm query demonstrator reward state. many situations rewards allow poor discrimination sets r|a|. particularly evident reward sparse since couple informative reward samples states contain similar reward information. section illustrate inconvenience comparing performance active method presence sparse dense reward functions. order illustrate applicability proposed approach conducted series experiments gbs-irl used determine reward function underlying given perturbed demonstration corresponding policy. experiment illustrate discuss performance gbs-irl. results presented correspond averages independent monte-carlo trials trial consists learning steps algorithm required select state query provided corresponding action. gbs-irl initialized independently generated random rewards. prior probabilities proportional level sparsity reward implies random rewards larger prior probability simplicity considered exact noise model i.e. pointed section iqbc core similar gbs-irl main diﬀerences terms selection criterion fact iqbc able accommodate notion states. since notion used examples expect performance methods essentially similar. algorithm queries expert correct action states expected gain information potentially larger requires evaluating state possible outcome associated gain information. method therefore fundamentally diﬀerent gbs-irl expect method yield crisper diﬀerences approach. additionally estimation computationally heavy requires evaluation policy state-action pair. ﬁrst experiments evaluate performance gbs-irl several small-sized mdps particular structure speciﬁcally considered mdps either size consider random independently generated mdps conducted independent learning trials. ﬁrst experiments serves purposes. hand illustrates applicability gbs-irl arbitrary settings evaluating performance method random mdps particular structure. hand initial experiments also enable quick comparative analysis gbs-irl relevant methods active literature. figures depict learning curve three methods terms policy accuracy. performance three methods essentially similar early stages learning process. however gbs-irl slightly outperforms methods although diﬀerences iqbc expected smaller emg. policy accuracy gives clear view learning performance algorithms conveys less clear idea ability learned policies complete task intended demonstrator. evaluate performance three learning algorithms terms target task also measured loss learned policies respect optimal policy. results depicted figs. results also conﬁrm performance gbs-irl essentially similar. particular diﬀerences observed terms policy accuracy little impact terms ability perform target task compeconclude section also compare computation time methods smaller problems. results depicted fig. emphasize results portrayed herein indicative algorithms implemented relatively straightforward manner particular concerns optimization. still comparison conﬁrm computational complexity associated many times superior involved remaining methods. this discussed earlier heavy computations involved estimation expected myopic gain grows directly size |a|. observation also line discussion already found original work cohn second experiments investigate performance gbs-irl aﬀected dimension domain considered. purpose evaluate performance gbs-irl arbitrary medium-sized mdps particular structure specifically consider mdps either take either size consider random independently generated mdps conducted independent learning trials. given results ﬁrst experiments computation time already associated remaining experiments opted comparing gbs-irl iqbc only. learning curves terms policy accuracy task execution depicted fig. figure classiﬁcation value performance gbsirl iqbc medium-sized random mdps. solid lines correspond gbs-irl dotted lines correspond iqbc. classiﬁcation performance. value performance. indicated values correspond dimensions mdps. severely number actions gbsirl. although signiﬁcantly tendency could already observed smaller environments dependence number actions completely unexpected. fact number hypothesis since disagreement taken possible actions dependence performance iqbc number actions. gbs-irl hand focused toward identifying optimal action state. renders approach less sensitive number actions seen corollaries illustrated fig. analyzed performance gbs-irl random mdps particular structure terms transition probabilities reward function. third experiments look scalability gbs-irl considering large-sized domains. consider structured problems selected literature. particular evaluate performance gbs-irl trap-world puddle-world driver domains. puddle-world domain introduced work boyan moore depicted consists grid-world fig. puddles exist puddle agent receives penalty proportional squared distance nearest edge puddle ranges agent must reach goal state top-right corner environment upon receives reward refer original description boyan moore details. domain described four actions correspond motion commands four possible directions. transitions stochastic described follows. selecting action corresponding moving direction agent roll back cell probability probability action fail agent remain position. agent move adjacent position direction probability probability move cells direction probability move three cells direction used discount trap-world domain introduced work judah depicted fig. consists grid-world separated rooms. darker rooms correspond trap rooms agent leave reaching corresponding bottomleft cell dark lines correspond walls agent cannot traverse. dotted lines used delimit trap-rooms safe rooms otherwise meaningless. agent must reach goal state bottom-right corner environment. refer work judah detailed description. domain described four actions correspond motion commands four possible directions. transitions deterministic. target reward function everywhere except goal used discount mdp. finally driver domain introduced work abbeel instance depicted fig. environment agent corresponds driver blue bottom moving speed greater cars. figure classiﬁcation value performance gbsirl iqbc three large domains. solid lines correspond gbs-irl dotted lines correspond iqbc. classiﬁcation performance. value performance. cars move constant speed scattered across three central lanes. goal agent drive safely possible—i.e. avoid crashing cars turning suddenly possible driving shoulder lanes. purposes tests represented driver domain actions correspond driving lanes. transitions deterministic. target reward function penalizes agent value every crash value driving shoulder lanes. additionally lane change costs agent penalty previous scenarios used discount mdp. observe that previous scenarios performance methods similar. scenarios feature relatively small number actions attenuates negative dependence iqbc number actions observed previous experiments. also interesting observe trap-world domain seems harder learn domains spite diﬀerences dimension. example driver domain required around samples gbs-irl single correct hypothesis trap-world required around attain similar performance. fact trap-world domain features sparsest reward. since rewards hypothesis space selected similarly sparse possible many would lead similar policies large parts statespace thus hardening identiﬁcation correct hypothesis. conclude still interesting observe that spite dimension problems considered methods eﬀectively able single correct hypothesis samples. fact overall performance superior observed medium-sized domains indicates domain structure present scenarios greatly contributes disambiguate hypothesis given expert demonstration. ﬁrst experiment illustrates integration reward policy information bayesian setting described section consider simple grid-world depicted fig. agent must navigate top-right corner environment. ﬁrst experiment random sampling which time step expert adds mance sparsity reward function testing gbs-irl distinct conditions. ﬁrst condition depicted fig. corresponds reward function sparse i.e. states except goal states discussed section sparsity rewards learning performance greatly impacts bayesian approach. phenomenon however exclusive active learning approach—in fact seen fig. random sampling also exhibits poor performance. still possible nonetheless detect advantage using active sampling approach. contrast possible design informative rewards resorting technique proposed reinforcement learning literature designation reward shaping considering shaped version reward obtain learning performance depicted fig. note latter case convergence extremely fast even presence random sampling. conclude noting that case reward information setting essentially equivalent standard reinforcement learning setting efﬁcient exploration techniques proposed provide fruitful avenues future research. paper introduce gbs-irl novel active algorithm allows agent learn task demonstration expert. using generalization binary search algorithm greedily queries expert demonstrations highly informative states. seen section following designation dasgupta gbs-irl aggressive active learning algorithm. additionally given consideration noisy samples gbs-irl naturally designed consider non-separable data. pointed dasgupta aggressive active learning algorithms exist provable complexity bounds nonseparable case. gbs-irl comes guarantees summarized corollary suitable conditions given long figure compares performance bayesian demonstrations consisting state-action pairs only state-reward pairs only also demonstrations include state-action state-reward pairs. ﬁrst observe demonstration types enable learner slowly improve performance target task. indicates three sources information give useful information accurately identify target task another important observation direct comparison learning performance obtained diﬀerent demonstration types misleading since ability agent extract useful information reward samples greatly depends sparsity reward function. except situations reward extremely informative action-based demonstration generally informative. second experiment analyze performance active learning method querying reward information grid-world environment. particular analyze dependence perforsince k-neighborly assumption sequence every sets xkmxkm+ k-neighborly. additionally point sequence signal must change. implies k-neighboring sets phkj additionally brieﬂy remarked section possible adaptive sub-modularity argument establish near-optimality gbs-irl. fact given target hypothesis consider objective function theorem proof shown strongly adaptive monotone adaptive modular results golovin krause provide similar bound sample complexity gbs-irl. knowledge gbs-irl ﬁrst active algorithm provable sample complexity bounds. additionally discussed section reduction standard classiﬁcation problem implies algorithm specialized particular problems. particular results generally applicable multi-class classiﬁcation problems verifying corresponding assumptions. finally main contributions focused simplest form interaction demonstration consist examples right action take different situations. however also discuss forms expert feedback integrated seamless manner gbsirl framework. particular discussed combine policy reward information learning algorithm. approach thus provides interesting bridge reinforcement learning imitation learning particular brings forefront existing results eﬃcient exploration reinforcement learning additionally general bayesian framework used paper also amenable integration additional information sources. example human agent provide trajectory information indicate states frequently visited following optimal path. parameters generally possible associate likelihood feedback turn integrated bayesian task estimation setting. however extending active learning approach sources information less straightforward left important avenue future research. abusively write denote quantity corresponds fraction probability mass concentrated hypotheses prescribing action optimal state xt+. normalizing factor consider case -neighboring sets holds. case according algorithm selected randomly either probability moreover since -neighbors single hypothesis prescribes diﬀerent optimal actions denote optimal action optimal action prescribed three situations possible situation a∗i) work partially supported portuguese fundação para ciência tecnologia project pestoe/eei/la/. manuel lopes flowers team joint inria ensta-paristech lab.", "year": 2013}