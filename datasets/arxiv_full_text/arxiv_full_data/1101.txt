{"title": "On the exact relationship between the denoising function and the data  distribution", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "We prove an exact relationship between the optimal denoising function and the data distribution in the case of additive Gaussian noise, showing that denoising implicitly models the structure of data allowing it to be exploited in the unsupervised learning of representations. This result generalizes a known relationship [2], which is valid only in the limit of small corruption noise.", "text": "prove exact relationship optimal denoising function data distribution case additive gaussian noise showing denoising implicitly models structure data allowing exploited unsupervised learning representations. result generalizes known relationship valid limit small corruption noise. denoising task reconstructing original data samples corrupted samples recently gained popularity unsupervised task learning representations deep learning besides practical success theoretical basis learning denoising becoming better understood. shown optimizing denoising performance leads representations implicitly model structure data manifold precisely optimal denoising function corresponds score data distribution limit small corruption noise note generalize result derive exact relationship data distribution denoising function case additive gaussian noise valid arbitrarily large noise. result ﬁrst published curious company blog post assume clean samples drawn i.i.d. data distribution corrupted samples produced clean ones corruption process ˜x|x assumed known. task denoising reconstruct clean samples corrupted ones denoising function optimized match reconstructions clean samples unsupervised learning usually interested learning data distribution latent representations. show denoising function contains information therefore learning learn model data distribution denotes arbitrary contour normalization constant. contour integral yields unique value green’s theorem since curl gradient always vanishes furthermore given depends diﬀerence data distribution principle solved terms corrupted distribution deconvolution. combined eqs. leads exact invertible relationship data distribution optimal denoising function proving latter captures exactly information former. worth noting learning denoising function principle learn arbitrarily complex structures data distribution instance regression task learns expectation value output conditioned input. formal expression applies corruption distribution result involving gradient log-probability density valid additive gaussian corruption noise. would interesting explore similar relationships data distribution optimal denoising function corruption processes multiplicative gaussian noise dropout corruption.", "year": 2017}