{"title": "Empirical Evaluation of Rectified Activations in Convolutional Network", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68\\% accuracy on CIFAR-100 test set without multiple test or ensemble.", "text": "object detection tracking. despite depth characteristics modern deep learning system non-saturated activation function replace saturated counterpart advantage using non-saturated activation function lies aspects ﬁrst solve called exploding/vanishing gradient. second accelerate convergence speed. non-saturated activation functions notable rectiﬁed linear unit brieﬂy speaking piecewise linear function prunes negative part zero retains positive part. desirable property activations sparse after passing relu. commonly believed superior performance relu comes sparsity paper want questions first sparsity important factor good performance? second design better non-saturated activation functions could beat relu? consider broader class activation functions namely rectiﬁed unit family. particular interested leaky relu variants. contrast relu negative part totally dropped leaky relu assigns noon-zero slope ﬁrst variant called parametric rectiﬁed linear unit prelu slopes negative part learned form data rather predeﬁned. authors claimed prelu factor surpassing human-level performance imagenet classiﬁcation task. paper investigate performance diﬀerent types rectiﬁed activation functions convolutional neural network standard rectiﬁed linear unit leaky rectiﬁed linear unit parametric rectiﬁed linear unit randomized leaky rectiﬁed linear units evaluate activation function standard image classiﬁcation task. experiments suggest incorporating nonzero slope negative part rectiﬁed activation units could consistently improve results. thus ﬁndings negative common belief sparsity good performance relu. moreover small scale dataset using deterministic negative slope learning prone overﬁtting. eﬀective using randomized counterpart. using rrelu achieved accuracy cifar- test without multiple test ensemble. second variant called randomized rectiﬁed linear unit rrelu slopes negative parts randomized given range training ﬁxed testing. recent kaggle national data science bowl competition reported rrelu could reduce overﬁtting randomized nature. paper empirically evaluate four kinds activation functions. based experiment conclude small dataset leaky relu variants consistently better relu convolutional neural networks. rrelu favorable randomness training reduces risk overﬁtting. case large dataset investigation done future. section introduce four kinds rectiﬁed units rectiﬁed linear leaky rectiﬁed linear parametric rectiﬁed linear randomized rectiﬁed linear illustrate fig. comparisons. sequel denote input channel example denote corresponding output passing activation function. following subsections introduce rectiﬁed unit formally. parametric rectiﬁed linear proposed authors reported performance much better relu large scale image classiﬁcation task. leaky relu exception learned training back propagation. randomized leaky rectiﬁed linear randomized version leaky relu. ﬁrst proposed used kaggle ndsb competition. highlight rrelu training process random number sampled uniform distribution formally have evaluate classiﬁcation performance convolutional network structure diﬀerent activation functions. large parameter searching space state-of-art convolutional network structure hyper parameters diﬀerent activation setting. models trained using cxxnet. cifar- cifar- dataset tiny nature image dataset. cifar datasets contains diﬀerent classes images cifar- datasets contains diﬀerent classes. image image size training images test images. images directly without pre-processing augmentation. result single view test without ensemble. refer network augmentation setting team auroraxie competition winners. network structure shown table single view test experiment diﬀerent original multi-view multi-scale test. cifar- experiment also tested rrelu batch norm inception network subset inception network started inception-a module. network achieved test accuracy without ensemble multiple view test task national data science bowl competition classify plankton animals image award labeled gray scale images classes test data. since test private divide training parts images training images validation. competition uses multi-class log-loss evaluate classiﬁcation performance. table national data science bowl competition network. layers convolutional layers otherwise speciﬁed. activation function followed convolutional layer. table show results cifar-/cifar dataset respectively. table shows ndsb result. relu network baseline compare convergence curve three activations pairwisely fig. respectively. three leaky relu variants better baseline test set. following observations based experiment reasons superior performances still lack rigorous justiﬁcation theoretic aspect. also activations perform large scale data still need investigated. open question worth pursuing future. superiority rrelu signiﬁcant cifar-/cifar-. conjecture ndsb dataset training smaller cifar/cifar- network even bigger. validates eﬀectiveness rrelu combating overﬁtting. paper analyzed four rectiﬁed activation functions using various network architectures three datasets. ﬁndings strongly suggest popular activation function relu story three types leaky relu consistently outperform original relu. however", "year": 2015}