{"title": "Closed-Form Learning of Markov Networks from Dependency Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Markov networks (MNs) are a powerful way to compactly represent a joint probability distribution, but most MN structure learning methods are very slow, due to the high cost of evaluating candidates structures. Dependency networks (DNs) represent a probability distribution as a set of conditional probability distributions. DNs are very fast to learn, but the conditional distributions may be inconsistent with each other and few inference algorithms support DNs. In this paper, we present a closed-form method for converting a DN into an MN, allowing us to enjoy both the efficiency of DN learning and the convenience of the MN representation. When the DN is consistent, this conversion is exact. For inconsistent DNs, we present averaging methods that significantly improve the approximation. In experiments on 12 standard datasets, our methods are orders of magnitude faster than and often more accurate than combining conditional distributions using weight learning.", "text": "markov networks powerful compactly represent joint probability distribution structure learning methods slow high cost evaluating candidates structures. dependency networks represent probability distribution conditional probability distributions. fast learn conditional distributions inconsistent inference algorithms support dns. paper present closed-form method converting allowing enjoy efﬁciency learning convenience representation. consistent conversion exact. inconsistent present averaging methods signiﬁcantly improve approximation. experiments standard datasets methods orders magnitude faster often accurate combining conditional distributions using weight learning. joint probability distributions useful representing reasoning uncertainty many domains robotics medicine molecular biology. powerful popular ways represent joint probability distribution compactly markov network undirected probabilistic graphical model. inference compute marginal conditional probabilities probable conﬁguration subset variables given evidence. although exact inference usually intractable numerous approximate inference algorithms exist exploit structure present representation. case weight learning convex optimization problem cannot done closed form except special circumstances e.g. structure learning even harder since typically involves search large number candidate structures weights must learned candidate scored increasingly popular alternative local methods select structure ﬁnding dependencies variable separately combining using weight learning dependency networks represent joint distribution conditional probability distributions variable. terms representational power comparable since every represented consistent vice versa. advantage much easier learn since conditional probability distribution learned separately. however local distributions consistent joint distribution making model difﬁcult interpret. furthermore inference algorithms support dns. result remain much less popular mns. paper present method converting dependency networks markov networks allowing enjoy efﬁciency learning convenience representation. surprisingly translation done closed-form without kind search optimization. consistent conversion exact. general case inconsistent present averaging methods signiﬁcantly improve approximation. long maximum feature length bounded constant method runs linear time space respect size original method similar spirit previous work learning combining local conditional distributions weight learning however instead ignoring parameters local distributions directly compute parameters joint distribution. hulten proposed method converting bayesian networks. however conversion process reform variable value variable. sometimes abbreviate tests boolean variables written written ¬xi. conjunctive feature equals arguments satisfy conjunction otherwise. factor represented table converted conjunctive features feature entry table. factors repeated values types structure feature-based representation often compact tabular one. markov blanket variable denoted variables render independent variables domain. consists variables appear factor feature independencies others entailed factorization given often wish answer queries probability variables given evidence. general computing exact marginal conditional probabilities p-complete approximate inference algorithms commonly used instead. popular gibbs sampling gibbs sampling markov chain monte carlo method generates random samples uses answer queries. sampler initialized random state consistent evidence. samples generated resampling non-evidence variable turn according conditional probability given current states variables. early samples sampler unlikely non-representative state burn-in samples typically excluded consideration. probability query estimated fraction samples consistent query. positive distributions gibbs sampling eventually converge correct probabilities take long time convergence difﬁcult detect. maximum likelihood parameter estimation goal select parameters maximizes log-likelihood model training data. assume expressed log-linear model ﬁxed features variables observed training data. since log-likelihood concave function weights parameter learning framed convex optimization problem solved standard gradient-based approaches. since log-likelihood gradient usually intractable compute exactly slow approximate commonly-used alternative pseudo-likelihood. pseudo-log-likelihood conditional log-likelihood variable given experiments standard datasets dnmn conversion method orders magnitude faster weight learning often accurate. decision tree conditional distributions converted often accurate original logistic regression conditional distributions converted signiﬁcantly accurate performing weight learning. method potential applications outside structure learning well. domains data limited unavailable expert could specify conditional distributions could translated joint distribution could much easier less error-prone attempting specify entire joint distribution directly. paper organized follows. begin background markov networks dependency networks sections section describe convert consistent dependency networks markov networks represent exact probability distribution. section discuss inconsistent conditional probability distributions methods improving approximation quality averaging. evaluate methods empirically section conclude section partition function normalization constant make distribution one; factor sometimes referred potential function; variables domain probability distribution said positive. represents positive distribution also written log-linear model. loglinear model probability distribution expressed exponentiated weighted feature functions rather product factors dependency network consists conditional probability distributions deﬁning probability single variable given markov blanket. said consistent exists probability distribution consistent dn’s conditional distributions. inconsistent sometimes called general dependency networks. since gibbs sampling uses conditional probability variable given markov blanket easily applied dns. probability distribution represented deﬁned stationary distribution gibbs sampler given ﬁxed variable order. consistent conditional distributions must consistent gibbs sampling converges distribution inconsistent stationary distribution depend order variables resampled. furthermore joint distribution determined gibbs sampler inconsistent conditional probabilities cpds. inference algorithms deﬁned dns. heckerman describe method using gibbs sampling values together estimate rare probabilities lower variance; toutanova maximum posteriori inference chain-structured adaptation viterbi algorithm; lowd shamaei propose mean ﬁeld inference dns. know methods. constructed constructing conditional probability distribution variable given markov blanket. also learned data learning probabilistic classiﬁer variable. makes learning trivial parallelize separate process variable. however learned data resulting cpds inconsistent other. standard method learning probabilistic classiﬁer used. heckerman learn probabilistic decision tree variable. probabilistic decision tree interior nodes tree variable tests branches labeled test outcomes leaves specify marginal distribution target variable given tests path root leaf. probabilistic decision trees learned greedily maximize conditional log-likelihood target variable. hulten investigated problem converting bayesian network however rather trying match distribution closely possible tried highest-scoring acyclic subset structure. proving problem np-hard proposed greedy algorithm removing least essential edges. resulting consistently less accurate variables except unlike log-likelihood gradient evaluated efﬁciently. like log-likelihood concave function local optimum also global optimum. main disadvantage tends handle long-range dependencies poorly. optimizing optimizes model’s ability predict individual variables given large amounts evidence lead worse performance less evidence available. control overﬁtting zero-mean gaussian prior typically placed weight. goal structure learning factors parameters features weights high score training data. weight learning pseudolikelihood common choice intractability partition function. many structure learning variations proposed touch algorithms main themes. common approach structure learning perform greedy search space possible structures. della pietra conduct search top-down manner starting atomic features extending combining simple features convergence. davis domingos propose bottom-up search instead creating many initial features based training data repeatedly merging features learning progresses. search-based algorithms tend slow since scoring candidate structures expensive many candidate structures consider. another approach local models markov blanket variable separately combine features local models global structure. ravikumar learning logistic regression model regularization variable. regularization encourages sparsity interaction weights zero. non-zero interactions turned conjunctive features. words logistic regression model predicting nonzero weight feature added model. lowd davis adopt similar approach probabilistic decision trees local model. decision tree converted conjunctive features considering path tree root leaf conjunction variable tests. works authors select feature weights performing weight learning. addition much faster search-based methods local approaches often lead accurate models. therefore markov network factors {φi} exactly represents probability distribution since factors deﬁned terms conditional distributions variable given evidence consistent dependency network converted markov network representing exact distribution. holds ordering base instance note that unlike ﬁxed vector values instance state space. necessary function following subsection make clearer example. discuss construct joint distribution positive conditional distributions. begin with assume conditional distributions conditionals unknown joint probability distribution later relax assumption discuss effective approximation. consider instances differ state variable words express ratio probabilities follows note ﬁnal expression involves conditional distribution full joint. conditional distribution must positive order avoid division zero. differ multiple variables express probability ratio product multiple singlevariable transitions. construct sequence instances instance differing variable previous instance sequence. order permutation numbers refer number order. deﬁne inductively words instance simply changes variable otherwise identical previous element thus ﬁrst variables order values latter values note since variables changed values intermediate instances express ratio matches conditional distributions. therefore factors represents exact distribution conditional distributions since original consistent choice lead result. previous example showed convert using tables factors. however inefﬁcient conditional distributions structure decision trees logistic regression models. rather treating type separately discuss convert distribution represented log-linear model conjunctive features. uses values ﬁrst variables rest. values constant free variables resulting distribution function simplifying fj−o) constant values violate variable tests always zero removed entirely. otherwise conditions satisﬁed constant values always satisﬁed removed. note ﬁnal factor always log-linear model feature subset features original conditional distribution. therefore converting conditional distribution manner yields represented log-linear model. summarize complete method consistent algorithms tables dnmn takes conditional probability distributions {pi} base instance inverse variable ordering inverse variable ordering mapping variables corresponding indices desired ordering dnmn ﬁrst converts conditional distributions sets weighted features. conditions feature values depending variable order produce simpliﬁed distributions numerator denominator handled simplifyfeature accepts parameter indicate simpliﬁed feature numerator denominator. features denominator assigned negative weights since exp. encourages equal encourages opposite values. symmetry easy gibbs sampler transition probabilities must converge uniform distribution. however conditional distributions consistent uniform distribution inconsistent. choice affect converted unsurprisingly changing ordering also changes resulting probability distribution example. however single choice leads true uniform distribution. obtain uniform distribution must average several conversions. however instead merely summing small number base instances base instances linear time space exploiting structure conjunctive features. given feature intermediate instance suppose binary-valued variables constant values x−o. possible fraction consistent instance. therefore rather enumerating possible simply multiply weight reﬂect fact would included fraction converted mns. inconsistent uniform distribution good approach since inconsistent less probable regions state space leading worse behavior. found product marginals distribution works well. training data estimate marginal distribution variable marginals compute modiﬁed weight conditioned feature. uniform approach remains special case. also experimented averaging training examples found using marginals worked slightly better. since conversion depends ordering would like average possible orderings well. since every feature ﬁnal model subset original features feature variable tests subsets consider. converting conditional distribution variable variables come ordering constant variable tests involving variables removed. consider speciﬁc conjunctive subfeature contains original variable tests. total orderings variables conditions. ways order remaining conditions ways order conditions removed. therefore fraction orderings consistent particular subfeature model long conjuncts exponential number subfeatures leads prohibitively large model sizes. alternately linear number orderings. given base ordering orderings form note orderings weighted equally. weights four subfeatures multiplied respectively. symmetry average rotations several different orderings. experiments average rotations orderings features length equivalent summing orderings since subfeatures included symmetry opposite orderings ensures balanced weights. used standard structure learning datasets collected prepared davis domingos omitting eachmovie since longer publicly available. variables binary-valued. datasets vary widely size number variables. table summary davis domingos details origin. datasets increasing order number variables. dataset learned decision tree cpds logistic regression cpds. decision tree cpds probabilistic decision tree greedily learned maximize conditional likelihood target variable given others. avoid overﬁtting used structure prior heckerman number free parameters model tunable parameter. parameters estimated using add-one smoothing. learned logistic regression cpds regularization using orthant-wise limited-memory quasinewton method tuned using held-out validation data. started value increased factor score validation decreased. used values selected model highest score validation set. score validation computed product conditional probabilities variable according respective cpds. ordering; base instance opposite orderings; base instance ordering; expectation instances opposite orderings; expectation instances rotations ordering; expectation inexpectation instances done using marginal distribution variables estimated using training data. fair possible include time read entire training data timing results well time write model disk. also converted using weight learning similar methods ﬁrst converted cpds conjunctive features removed duplicate features. learned weights features maximize training data using gaussian prior weights reduce overﬁtting. standard deviation gaussians tuned maximize validation set. decision tree cpds used standard deviations best-performing models always standard deviations logistic regression cpds slightly wider range standard deviations required ranging optimization done using limited memory bfgs algorithm quasi-newton method convex optimization weight learning terminated iterations converged. table shows relative accuracy converting learning weights direct conversion orderings marginal distribution base instances result better performing algorithm marked bold. differences individual datasets always statistically signiﬁcant overranking trend shows relative peroformance methods. dnmn similarly well tree cpds neither cmll signiﬁcantly different according wilcoxon signed ranks test. logistic regression cpds dnmn achieves higher datasets cmll differences signiﬁcant two-tailed wilcoxon signed ranks test. table shows time converting using closed-form solution weight learning. dnmn times faster weight learning decision tree cpds times faster weight learning logistic regression cpds. results include tuning time necessary obtain best results weight learning. tuning time excluded weight learning times slower tree cpds times slower logistic regression cpds average. dnmn learns similar performance original quickly. decision tree cpds often accurate original logistic regression cpds signiﬁcantly accurate performing weight learning done ravikumar makes dnmn competitive better state-of-the-art methods learning structure. furthermore dnmn exploit many types conditional distributions including boosted trees rule sets easily take advantage algorithmic improvements learning conditional distributions. another important application dnmn model speciﬁed experts since conditional distributions could much easier specify joint distribution. thank chlo´e kiddon jesse davis anonymous reviewers helpful comments. research partly funded grant wnf--- grant iis- grant oci-. views conclusions contained document author interpreted necessarily representing ofﬁcial policies either expressed implied u.s. government. addition computed conditional marginal log-likelihood common evaluation metric markov networks cmll conditional log-likelihood variable given subset others evidence. experiments followed approach davis domingos partition variables four sets marginal distribution variable computed using variables three sets evidence marginals computed using gibbs sampling. used rao-blackwellized gibbs sampler adds counts states variable based conditional distribution before selecting next value variable. found burn-in sampling iterations sufﬁcient gibbs sampling additional iterations affect results much. first compare different approaches converting inconsistent compare accuracy resulting original dns. figure shows result converting decision tree cpds logistic regression cpds measure relative accuracy comparing original test data. order better show differences among datasets different numbers variables show normalized divides number variables domain. graphs rescaled height tallest cluster actual height listed graph. consistent methods would equivalent differences would zero. since inconsistent converting result altered distribution less accurate test data. empirically using many orders summing base instances leads accurate models. decision tree bars negative indicating accurate source dns. happen conditional distributions combine several conditional distributions original leading complex dependencies single decision tree. orderings converted figure difference normalized original converted different orderings base instances divided largest difference. results decision tree left; logistic regression right. smaller values indicate better performance. largest difference values listed dataset’s results rounded nearest /th.", "year": 2012}