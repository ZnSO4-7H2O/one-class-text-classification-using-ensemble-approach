{"title": "Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis  and Application to Information Retrieval", "tag": ["cs.CL", "cs.IR", "cs.LG", "cs.NE"], "abstract": "This paper develops a model that addresses sentence embedding, a hot topic in current natural language processing research, using recurrent neural networks with Long Short-Term Memory (LSTM) cells. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model is found to automatically attenuate the unimportant words and detects the salient keywords in the sentence. Furthermore, these detected keywords are found to automatically activate different cells of the LSTM-RNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These automatic keyword detection and topic allocation abilities enabled by the LSTM-RNN allow the network to perform document retrieval, a difficult language processing task, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform several existing state of the art methods. We emphasize that the proposed model generates sentence embedding vectors that are specially useful for web document retrieval tasks. A comparison with a well known general sentence embedding method, the Paragraph Vector, is performed. The results show that the proposed method in this paper significantly outperforms it for web document retrieval task.", "text": "abstract—this paper develops model addresses sentence embedding topic current natural language processing research using recurrent neural networks long short-term memory cells. proposed lstm-rnn model sequentially takes word sentence extracts information embeds semantic vector. ability capture long term memory lstm-rnn accumulates increasingly richer information goes sentence reaches last word hidden layer network provides semantic representation whole sentence. paper lstm-rnn trained weakly supervised manner user click-through data logged commercial search engine. visualization analysis performed understand embedding process works. model found automatically attenuate unimportant words detects salient keywords sentence. furthermore detected keywords found automatically activate different cells lstmrnn words belonging similar topic activate cell. semantic representation sentence embedding vector used many different applications. automatic keyword detection topic allocation abilities enabled lstm-rnn allow network perform document retrieval difﬁcult language processing task similarity query documents measured distance corresponding sentence embedding vectors computed lstm-rnn. search task lstm-rnn embedding shown signiﬁcantly outperform several existing state methods. emphasize proposed model generates sentence embedding vectors specially useful document retrieval tasks. comparison well known general sentence embedding method paragraph vector performed. results show proposed method paper signiﬁcantly outperforms document retrieval task. input data important task machine learning. text language processing problem learning embedding vector sentence; train model automatically transform sentence vector encodes semantic meaning sentence. word embedding learned using loss function deﬁned word pairs sentence embedding learned using loss function deﬁned sentence pairs. sentence embedding usually relationship among words sentence i.e. context information taken consideration. therefore sentence embedding suitable tasks require computing semantic similarities text strings. mapping texts uniﬁed semantic representation embedding vector used different language processing applications machine translation sentiment analysis information retrieval machine translation recurrent neural networks long short-term memory cells lstm-rnn used encode english sentence vector contains semantic meaning input sentence another lstm-rnn used generate french sentence vector. model trained best predict output sentence. paragraph vector learned unsupervised manner distributed representation sentences documents used sentiment analysis. sentence embedding also applied information retrieval contextual information properly represented vectors space fuzzy text matching paper propose sequentially accept word sentence recurrently latent space together historical information. reaches last word sentence hidden activations form natural embedding vector contextual information sentence. incorporate lstm cells model address difﬁculty learning long term memory rnn. learning model performed weakly supervised manner click-through data logged commercial search engine. although manually labelled data insufﬁcient machine learning logged data limited feedback signals massively available widely used commercial search engines. limited feedback information click-through data provides weak supervision signal indicates semantic similarity text query side clicked text document side. exploit signal objective training maximize similarity vectors mapped lstm-rnn query clicked document respectively. consequently learned embedding vectors query clicked document speciﬁcally useful document retrieval task. important contribution paper analyse embedding process lstm-rnn visualizing internal activation behaviours response different text inputs. show embedding process learned lstm-rnn effectively detects keywords attenuating less important words sentence automatically switching gates within lstm-rnn cells. show different cells learned model indeed correspond different topics keywords associated similar topic activate cell unit model. lstm-rnn reads sentence topic activation accumulates hidden vector last word encodes rich contextual information entire sentence. reason natural application sentence embedding search ranking embedding vector query used match embedding vectors candidate documents according maximum cosine similarity rule. evaluated real document ranking task proposed method signiﬁcantly outperforms many existing state methods ndcg scores. please note refer document paper mean title document. inspired word embedding method authors proposed unsupervised learning method learn paragraph vector distributed representation sentences documents used sentiment analysis superior performance. however model designed capture ﬁne-grained sentence structure. unsupervised sentence embedding method proposed great performance large corpus contiguous text corpus e.g. bookcorpus main idea encode sentence decode previous next sentences i.e. using separate decoders. encoder decoders rnns gated recurrent unit however sentence embedding method designed document retrieval task supervision among queries clicked unclicked documents. semi-supervised recursive autoencoder proposed used sentiment prediction. similar proposed method need language speciﬁc sentiment parsers. greedy approximation method proposed construct tree structure input sentence. assigns vector word. become practically problematic large vocabularies. also works unlabeled data supervised sentiment data. similar recurrent models paper dssm clsm models developed information retrieval also interpreted sentence embedding methods. however dssm treats input sentence bag-of-words model word dependencies explicitly. clsm treats sentence n-grams deﬁned window capture local word dependencies. max-pooling layer used form global feature vector. methods also convolutional based networks natural language processing models design cannot capture long distance dependencies i.e. dependencies among words belonging non-overlapping ngrams. dynamic convolutional neural network proposed sentence embedding. similar clsm dcnn rely parse tree easily applicable language. however different clsm regular max-pooling used dcnn dynamic k-max-pooling used. means instead keeping largest entries among word vectors vector largest entries kept different vectors. dcnn shown good performance sentiment prediction question type classiﬁcation tasks. convolutional neural network architecture proposed sentence matching. shown great performance several matching tasks. bilingually-constrained recursive auto-encoders proposed create semantic vector representation phrases. experiments shown proposed method great performance end-to-end tasks. long short-term memory networks developed address difﬁculty capturing long term memory rnn. successfully applied speech recognition achieves state-of-art performance text analysis lstm-rnn treats sentence sequence words internal structures i.e. word dependencies. encodes semantic vector sentence incrementally differs dssm clsm. encoding process performed left-to-right word-by-word. time step word encoded semantic vector word dependencies embedded vector updated. process reaches sentence semantic vector embedded words dependencies hence viewed feature vector representation whole sentence. machine translation work input english sentence converted vector representation using lstm-rnn another lstmrnn used generate output french sentence. model trained maximize probability predicting correct output sentence. main composition models model words model summation bi-gram pairs plus non-linearity. proposed model instead simple summation used lstm model letter tri-grams keeps valuable information long intervals throws away useless information. encoder-decoder approach proposed jointly learn align translate sentences english french using rnns. concept attention decoder discussed paper closely related proposed model extracts keywords document side. explanations please section v-a. visualizations presented rnns without lstm cells grus. different work target task sentence embedding document retrieval target tasks character level sequence modelling text characters source codes. interesting observations interpretability lstm cells statistics gates activations presented. section show results visualization consistent observations reported also present detailed visualization speciﬁc document retrieval task using click-through data. also present visualizations proposed model used keyword detection. different aforementioned studies method developed paper trains model sentences paraphrase close semantic embedding vectors description sec. ahead. another reason lstm-rnn particularly effective sentence embedding robustness noise. example document ranking task noise comes sources every word query document equally important want remember salient words using limited memory. word phrase important given query want remember related words useful compute relevance document given query. illustrate robustness lstm-rnn paper. structure lstmrnn also circumvent serious limitation using ﬁxed window size clsm. experiments show difference leads signiﬁcantly better results document retrieval task. furthermore advantages. allows capture keywords topics effectively. models paper also need extra max-pooling layer required clsm capture global contextual information effectively. section introduce model recurrent neural networks long short-term memory version learning sentence embedding vectors. start basic proceed lstm-rnn. type deep neural networks deep temporal dimension used extensively time sequence modelling main idea using sentence embedding dense dimensional semantic representation sequentially recurrently processing word sentence mapping dimensional vector. model global contextual features whole text semantic representation last word text sequence figure t-th word coded -hot vector ﬁxed hashing operator similar used converts word vector letter tri-gram vector input weight matrix wrec recurrent weight matrix hidden activation vector used semantic representation t-th word associated last word semantic representation vector entire sentence. note different approach bag-of-words representation used whole text context information used. also different sliding window ﬁxed size used capture local features max-pooling layer capture global features. neither ﬁxed-sized window max-pooling layer; rather recurrence used capture context information sequence fig. basic architecture sentence embedding temporal recurrence used model contextual information across words text string. hidden activation vector corresponding last word sentence embedding vector bias vector assumed tanh. note architecture proposed sentence embedding slightly different traditional word hashing layer convert high dimensional input relatively lower dimensional letter tri-gram representation. also word supervision training instead whole sentence label. explained detail section although performs transformation sentence vector principled manner generally difﬁcult learn long term dependency within sequence vanishing gradients problem. effective solutions problem rnns using memory cells instead neurons originally proposed long short-term memory completed adding forget gate peephole connections architecture. architecture lstm illustrated fig. proposed sentence embedding method. ﬁgure input gate forget gate output gate cell state vector respectively peephole connections wreci input connections recurrent connections bias values respectively tanh function sigmoid function. architecture word corresponding last word sentence semantic vector entire sentence. learn good semantic representation input sentence objective make embedding vectors sentences similar meaning close possible meanwhile make sentences different meanings apart possible. challenging practice since hard collect large amount manually labelled data give semantic similarity signal different sentences. nevertheless widely used commercial search engine able massive amount data limited user feedback signals. example given particular query clickinformation user-clicked document among many candidates usually recorded used weak supervision signal indicate semantic similarity sentences section explain leverage weak supervision signal learn sentence embedding vector achieves aforementioned training objective. please also note objective make sentences similar meaning close possible similar machine translation expression logistic loss ∆rj. upper-bounds pairwise accuracy i.e. loss. since similarity measure cosine function larger range scaling. helps penalize prediction error more. value empirically experiments held dataset. train lstm-rnn back propagation time update equations parameter epoch follows µk−λk− \u0001k−∇lλk−) gradient cost function learning rate momentum parameter determined scheduling scheme used training. equations equivalent nesterov method please refer appendix nesterov method derived momentum method. gradient cost function fig. click-through signal used indication semantic similarity sentence query side sentence document side. negative samples randomly sampled training data. describe train model achieve objective using click-through data logged commercial search engine. complete description click-through data please refer section begin with adopt cosine similarity semantic vectors sentences measure similarity lengths sentence sentence respectively. context training click-through data denote query document respectively. figure show sentence embedding vectors corresponding query subscript denotes positive sample among documents subscript denotes j-th negative sample. embedding vectors generated feeding sentences lstm-rnn model described sec. take corresponding last word blue figure denotes collection model parameters; regular case includes wrec figure lstm-rnn case includes wrec wrec wrec wrec figure clicked |qr) probability document r-th query error signals different parameters lstm-rnn necessary training presented appendix full derivation gradients models presented section supplementary materials. accelerate training parallelization minibatch training large update instead incremental updates back propagation time. resolve gradient explosion problem gradient inputs fixed step size scheduling gradient clip threshold maximum number epochs nepoch total number query clicked-document pairs total number un-clicked documents given query maximum sequence length truncated bptt outputs trained models query side document side initialization parameters small random numbers procedure lstm-rnn used simple effective scheduling lstm-rnn models ﬁrst last parameter updates parameter updates used ﬁxed step size training ﬁxed step size training lstm-rnn. understand lstm-rnn performs sentence embedding visualization tools analyze semantic vectors generated model. would like answer following questions word dependencies context information captured? lstm-rnn attenuate unimportant information detect critical information input sentence? keywords embedded semantic vector? global topics identiﬁed lstm-rnn? answer questions train without lstm cells click-through dataset logged commercial search engine. training method described sec. description corpus follows. training includes positive query document pairs clicked signal used weak supervision training lstm. relevance judgement constructed follows. first queries sampled year search engine logs. adult spam queries removed. queries de-duped unique queries remain. reﬂex natural query distribution control quality queries. example query sets around misspelled queries around navigational queries transactional queries etc. second query collect documents judged issuing query several popular search engines fetching top- retrieval results each. finally query-document pairs judged group well-trained assessors. study queries preprocessed follows. text white-space tokenized lower-cased numbers retained stemming/inﬂection treatment performed. unless stated otherwise experiments used negative samples i.e. fig. proceed perform comprehensive analysis visualizing trained lstm-rnn models. particular visualize on-and-off behaviors input gates output gates cell states semantic vectors lstm-rnn model reveals model extracts useful information input sentence embeds properly semantic vector according topic information. learning formula model parameters previous section remove peephole connections forget gate lstm-rnn model current task. length sequence i.e. number words query document known advance state cell zero beginning sequence. therefore forget gates great help here. also long order words kept precise timing sequence great concern. therefore peephole connections important well. removing peephole connections forget gate also reduce amount training time since smaller number parameters need learned. attenuating unimportant information first examine evolution semantic vector unimportant words attenuated. speciﬁcally feed following input sentences test dataset trained lstm-rnn model activations input gate output gate cell state embedding vector cell query document shown fig. fig. respectively. vertical axis cell index horizontal axis word index numbered left right sequence words color codes show activation values. figs.– make following observations semantic representation cell states evolving time. valuable context information gradually absorbed information vectors becomes richer time semantic information entire input sentence embedded vector obtained applying output gates cell states sentence. example fig. input gate values corresponding word word word small values corresponds words accommodation discount reservation respectively document sentence. interestingly input gates reduce effect three words ﬁnal semantic representation semantic similarity sentences query document sides affected words. section show trained lstm-rnn extracts important information i.e. keywords input sentences. backtrack semantic representations time. focus active cells ﬁnal semantic representation. whenever large enough change cell activation value assume important keyword detected model. illustrate result using example evolution active cells activation time shown fig. query document sentences.from fig. also observe different words activate different cells. tables i–ii show number cells word clearly visible please refer fig. section supplementary materials. adjusted color ﬁgures range reason structure might clearly visible. visualization examples could also found section supplementary materials speciﬁc cell. simplicity following simple approach given query look keywords extracted active cells lstm-rnn list table iii. interestingly cell collects keywords speciﬁc topic. example cell table extracts keywords related topic food cells mainly focus keywords related topic health. document retrieval task section apply proposed sentence embedding method important document retrieval task commercial search engine. speciﬁcally models embed sentences query document sides corresponding semantic vectors compute cosine similarity vectors measure semantic similarity query candidate documents. experimental results task shown table using standard metric mean normalized discounted cumulative gain evaluating ranking performance lstm-rnn standalone human-rated test dataset. also trained several strong baselines dssm clsm training dataset evaluated performance task. fair comparison proposed lstm-rnn models trained number parameters dssm clsm models besides also include table well-known information retrieval models plsa sake benchmarking. model uses bag-of-words representation queries documents state-of-the-art document ranking model based term matching widely used baseline society. plsa topic model proposed trained using maximum posterior estimation documents side training dataset. experimented varying number topics plsa gives similar performance report table results using topics. results language model based method uni-gram activates. used bidirectional lstm-rnn results tables ﬁrst lstm-rnn reads sentences left right second reads sentences right left. tables labelled word keyword active cells directions declare keyword. boldface numbers table show number cells assigned word i.e. active cells. tables observe keywords activate cells unimportant words meaning selectively embedded semantic vector. topic allocation show trained lstm-rnn model detects keywords also allocates properly different cells according topics belong this test dataset using trained lstm-rnn model search keywords detected note presenting ﬁrst word sequence activation values initially zero always considerable change cell states presenting ﬁrst word. reason indicated number cells detecting ﬁrst word keyword. moreover another keyword extraction example found section supplementary materials. compare performance proposed method general sentence embedding methods document retrieval task also performed experiments using general sentence embedding methods. ﬁrst experiment used method proposed generates embedding vectors known paragraph vectors. also known docvec. maps word vector uses vectors representing words inside context window predict vector representation next word. main idea method additional paragraph token previous sentences document inside context window. paragraph token mapped vector space using different matrix used words. primary version method known wordvec proposed difference wordvec include paragraph token. docvec dataset ﬁrst trained docvec model train test gives embedding vector every query document dataset. used following parameters training make sure meaningful model trained used trained docvec model similar words sample words dataset e.g. words pizza infection. resulting words corresponding scores presented section supplementary materials. observed resulting words trained model meaningful model recognise semantic similarity. docvec also assigns embedding vector query document test set. used embedding vectors calculate cosine similarity score query-document pair test set. used scores calculate ndcg values reported table docvec model. comparing results docvec model proposed method document retrieval task shows proposed method paper signiﬁcantly outperforms docvec. reason used general sentence embedding method docvec document retrieval task. experiment shows good idea general sentence embedding method using better task oriented cost function like proposed paper necessary. second experiment used skipthought vectors proposed training skip-thought method gets tuple encodes sentence using encoder tries reconstruct previous next sentences i.e. using separate decoders. model uses rnns gated recurrent unit shown perform good lstm. paper authors emphasized that model depends training corpus contiguous text. therefore training training barely sentence query document title fair. however since model trained books bookcorpus dataset includes million sentences trained model off-the-shelf sentence embedding method authors concluded conclusion paper. downloaded trained models word embeddings available https//github.com/ ryankiros/skip-thoughts. encoded query corresponding document title test vector. used combine-skip sentence embedding method vector size concatenation uni-skip i.e. unidirectional encoder resulting vector biskip i.e. bidirectional encoder resulting vector forward encoder another vector backward encoder. authors reported best results combineskip encoder. using embedding vectors query document calculated scores ndcg whole test reported table proposed method paper performing signiﬁcantly better off-the-shelf skipthought method document retrieval task. nevertheless since used skip-thought offthe-shelf sentence embedding method result good. result also conﬁrms learning embedding vectors using model cost function speciﬁcally designed document retrieval task necessary. shown table lstm-rnn signiﬁcantly models exceeds best outperforms baseline model ndcg score statistically signiﬁcant improvement. pointed sec. improvement comes lstm-rnn’s ability embed contextual semantic information sentences ﬁnite dimension vector. table also presented results different number negative samples used. generally increasing expect performance improve. negative samples results accurate approximation partition function results using bidirectional lstm-rnn also presented table model lstm-rnn reads queries documents left right lstm-rnn reads queries documents right left. embedding vectors left right right left lstm-rnns concatenated compute cosine similarity score ndcg values. comparison value cost function training lstm-rnn clickdata shown fig. ﬁgure conclude lstm-rnn optimizing cost function effectively. please note parameters models initialized randomly. speciﬁc topic. ﬁndings supported using extensive examples. concrete sample application proposed sentence embedding method evaluated important language processing task document retrieval. showed that task proposed method outperforms existing state methods signiﬁcantly. work motivated earlier successes deep learning methods speech semantic modelling adds evidence effectiveness methods. future work extend methods include using proposed sentence embedding method important language processing tasks believe sentence embedding plays role e.g. question answering task. exploit prior information structure different matrices fig. develop effective cost function learning method. exploiting attention mechanism proposed model improve performance words query aligned words document. appendix present ﬁnal gradient expressions necessary training proposed models. full derivations gradients presented section supplementary materials. comparisons ndcg performance measures proposed models series baseline models nhid refers number hidden units ncell refers number cells refers window size number negative samples paper addresses deep sentence embedding. propose model based long short-term memory model long range context information embed information sentence semantic vector. show semantic vector evolves time takes useful information input. made possible input gates detect useless information attenuate general limitation available human labelled data proposed implemented training model weak supervision signal using user click-through data commercial search engine. performing detailed analysis model showed that proposed model robust noise i.e. mainly embeds keywords ﬁnal semantic vector representing whole sentence proposed model cell usually allocated keywords update lstm-rnn model parameters. here weight matrices bias vectors wrec wrec wrec wrec lstm-rnn architecture. general format gradient cost function deﬁnition have omit subscripts simplicity present different parameters cell lstmrnn following subsections. complete process calculating update lstm-rnn model parameters. subsequent subsections vectors deﬁned p.-s. huang deng acero heck learning deep structured semantic models search using clickthrough data proceedings international conference conference information knowledge management ser. cikm mikolov sutskever chen corrado dean distributed representations words phrases compositionality proceedings advances neural information processing systems kiros zemel salakhutdinov urtasun torralba fidler aligning books movies towards story-like visual explanations watching movies reading books arxiv preprint arxiv. socher pennington huang manning semi-supervised recursive autoencoders predicting sentiment distributions proceedings conference empirical methods natural language processing ser. emnlp collobert weston uniﬁed architecture natural language processing deep neural networks multitask learning international conference machine learning icml kalchbrenner grefenstette blunsom convolutional neural network modelling sentences proceedings annual meeting association computational linguistics june zhang zhou zong bilinguallyconstrained phrase embeddings machine translation proceedings annual meeting association computational linguistics baltimore maryland senior beaufays long short-term memory recurrent neural network architectures large scale acoustic modeling proceedings annual conference international speech communication association chen deng primal-dual method training recurrent neural networks constrained echo-state property proceedings international conf. learning representations mesnil deng bengio investigation recurrent-neural-network architectures learning methods spoken language understanding proc. interspeech lyon france august yuan deng j.-y. smoothing clickthrough data search ranking proceedings international sigir conference research development information retrieval ser. sigir york j¨arvelin kek¨al¨ainen evaluation methods retrieving highly relevant documents proceedings annual international sigir conference research development information retrieval ser. sigir. ˇreh˚uˇrek sojka software framework topic modelling large corpora proceedings lrec workshop challenges frameworks. valletta malta elra http//is.muni.cz/ publication//en. dahl deng acero context-dependent pre-trained deep neural networks large-vocabulary speech recognition audio speech language processing ieee transactions vol. jan. hinton deng dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine vol. november excellent. example rated good match dataset. score pair assigned score assigned lstm-rnn please note score means score assigned lstm-rnn correspondent human generated label. second compare number assigned neurons cells word lstm-rnn respectively. this rely active cells neurons ﬁnal semantic vectors models. results presented table table query document respectively. interesting observation sometimes assigns neurons unimportant words e.g. neurons assigned word table query bath wont turn example rated match dataset human. good know score pair assigned score assigned lstm-rnn shows score generated lstm-rnn closer human generated label. number assigned neurons cells word lstm-rnn presented table table viii query document. active neurons cells semantic vector lstm-rnn. examples assigning neurons unimportant words neurons word neurons word table viii. section present clear ﬁgure part fig. shows structure input gate document side hotels shanghai example. clearly visible ﬁgure input gate values cells corresponding word word word document side lstm-rnn small values corresponding words accommodation discount reservation respectively document title. interestingly input gates trying reduce effect three words ﬁnal representation lstm-rnn model trained maximize similarity query document good match. first compare scores assigned trained lstm-rnn hotels shanghai example. average query test dataset associated documents query document pair relevance label human generated. relevance labels fair good part fig. observe input gate values cells corresponding word word word word document side lstm-rnn small values corresponding words paint paint respectively document title. interestingly input gates trying reduce effect words ﬁnal representation lstm-rnn model trained maximize similarity query document good match. make sure meaningful model trained used trained docvec model similar words sample words dataset words pizza infection. resulting words corresponding scores follows clarify difference proposed method general sentence embedding methods section present diagram illustrating training procedure proposed model. presented fig. ﬁgure number negative", "year": 2015}