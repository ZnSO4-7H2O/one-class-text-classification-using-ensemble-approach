{"title": "Towards an automated method based on Iterated Local Search optimization  for tuning the parameters of Support Vector Machines", "tag": ["cs.AI", "cs.LG"], "abstract": "We provide preliminary details and formulation of an optimization strategy under current development that is able to automatically tune the parameters of a Support Vector Machine over new datasets. The optimization strategy is a heuristic based on Iterated Local Search, a modification of classic hill climbing which iterates calls to a local search routine.", "text": "provide preliminary details formulation optimization strategy current development able automatically tune parameters support vector machine datasets. optimization strategy heuristic based iterated local search modiﬁcation classic hill climbing iterates calls local search routine. performance support vector machine strongly relies initial setting model parameters parameters usually training speciﬁc dataset ﬁxed applied certain application. automatic conﬁguration algorithms faced problem hyper-parameter tuning machine learning ﬁnding optimal setting parameters much research topic explored literature techniques used grid search common methods approximate optimal parameter values grid search involves exhaustive search manually speciﬁed subset hyperparameter space learning algorithm guided performance metric traditional approach however several limitations vulnerable local optimum; provide global optimality guarantee; setting appropriate search interval ad-hoc approach; computationally expensive approach especially search intervals require capture wide ranges. short contribution describe preliminary investigation optimization method tackles parameter setting problem svms using iterated local search popular explorative local search method used solving discrete optimization problems. belongs class trajectory optimization methods i.e. iteration algorithm search process designs trajectory search space starting initial state dynamically adding better solution curve discrete time step. iterated local search mainly consists steps. ﬁrst step local optimum reached performing walk search space referred perturbation phase. second step efﬁciently escape local optima using appropriate local search phase application acceptance criterion decide local candidate solutions chosen continue search process also important aspect algorithm. next section provide preliminary details formulation optimization strategy based parameter tuning task svms. given input parameters corresponding output parameters separation classes svms achieved ﬁtting hyperplane optimal distance nearest data point used training class total number parameters. goal svms hyperplane maximizes minimum distances samples side plane penalty associated instances misclassiﬁed added minimization function. done parameter minimization formula varying trade-off accuracy stability function deﬁned. larger values result smaller margin leading potentially accurate classiﬁcations however overﬁtting occur. mapping data appropriate kernel functions richer feature space including non-linear features applied prior hyperplane ﬁtting. among several kernels literature consider gaussian radial-basis function deﬁnes variance practically deﬁning shape kernel function peaks lower values bias corresponding high high bias. proposed current implementation tuning uses grid search inner local search routine iterated order make ﬁne-grained ﬁnally producing best parameters found date. given training dataset model procedure ﬁrst generates initial solution. initial solution produced grid search. grid search exhaustively generates candidates grid parameter values speciﬁed arrays rangeγ rangec choose arrays containing different values parameter grid search method look different parameters combinations. range values taken different powers solution quality evaluated accuracy means k-fold cross validation stored variable acc. afterwards perturbation phase represents core idea applied incumbent solution. goal provide good starting point next local search phase based previous search experience algorithm obtain better balance exploration search space wasting time areas giving good results. ranges rangeγ rangec incumbent solution whose imagine grid search gets parameters evaluated accuracy acc′. acceptance criterion solution produces better quality increased accuracy best solution date. happen incumbent solution rejected ranges updated automatically following values −down rand −down rand rand csup−down rand. indifferently values -down sup-up components random values always taken farther current parameter order increase diversiﬁcation capability metaheuristic; values sup-down components random values always taken closer current parameter order increase intensiﬁcation strength around current parameter. perturbation setting allow good balance among intensiﬁcation diversiﬁcation factors. better current i.e. acc′ solution becomes best solution rangeγ rangec updated usual. procedure continues date iteratively termination conditions imposed user satisﬁed producing best combination output. considered parameter setting task svms automated heuristic looks promising approach. aware detailed description algorithm deemed necessary along thorough computational investigation. currently object ongoing research including statistical analysis comparison proposed algorithm standard grid search order quantify qualify improvements obtained. research explore application strategy kernels considering also variety heterogenous datasets.", "year": 2017}