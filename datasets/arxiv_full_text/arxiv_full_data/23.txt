{"title": "Aligned Image-Word Representations Improve Inductive Transfer Across  Vision-Language Tasks", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "stat.ML"], "abstract": "An important goal of computer vision is to build systems that learn visual representations over time that can be applied to many tasks. In this paper, we investigate a vision-language embedding as a core representation and show that it leads to better cross-task transfer than standard multi-task learning. In particular, the task of visual recognition is aligned to the task of visual question answering by forcing each to use the same word-region embeddings. We show this leads to greater inductive transfer from recognition to VQA than standard multitask learning. Visual recognition also improves, especially for categories that have relatively few recognition training labels but appear often in the VQA setting. Thus, our paper takes a small step towards creating more general vision systems by showing the benefit of interpretable, flexible, and trainable core representations.", "text": "figure sharing image-region word representations across multiple vision-language domains svlr module projects images words shared representation space. resulting visual textual embeddings used tasks like visual recognition vqa. models individual tasks formulated terms inner products region word representations enforcing alignment shared space. work propose shared vision-language representation module improves inductive transfer related vision-language tasks apply approach visual recognition attentionbased visual question answering formulate terms joint embedding textual visual representations computed svlr module. region mapped closest correct class label. example embedding closer embedded region showing object label. formulate predicting answer relevant region relevance answer scores computed embedded word-region similarities. example region considered relevant elephant wearing pink blanket? embedded pink either elephant blanket close embedded region. similarly answer score considers embedded similarities comprehensive manner. emphasize word-region embedding learned vqa. experiments show formulating tasks terms svlr module leads better cross-task transfer features shared multitask learnimportant goal computer vision build systems learn visual representations time applied many tasks. paper investigate visionlanguage embedding core representation show leads better cross-task transfer standard multitask learning. particular task visual recognition aligned task visual question answering forcing word-region embeddings. show leads greater inductive transfer recognition standard multitask learning. visual recognition also improves especially categories relatively recognition training labels appear often setting. thus paper takes small step towards creating general vision systems showing beneﬁt interpretable ﬂexible trainable core representations. consider designing vision system solves many tasks. ideally system able reuse representations different applications. system trained solve problems core representations become complete accurate facilitating learning additional tasks. vision research often focuses designing good representations given task good core representations facilitate learning next? application knowledge learned solving task solve another task known transfer learning inductive transfer. inductive transfer demonstrated recent vision-language tasks hidden output layers deep networks learned pre-training multitask learning serve foundation learning tasks. however relations features task needs re-learned using task’s data. goal work transfer knowledge related tasks without need re-learn mapping. further working vision-language tasks transfer knowledge vision language across tasks. summary main contribution show proposed svlr module leads better inductive transfer unaligned feature sharing multitask learning. added beneﬁt attention model highly interpretable show words cause system score particular region relevant. take small step towards lifelong-learning vision systems showing beneﬁt interpretable ﬂexible trainable core representation. related work never-ending learning aims continuously learn multiple tasks learning solve newer problems becomes easier. representation learning multitask learning curriculum learning different aspects larger paradigm. inductive transfer shared representations necessary ﬁrst step nel. works focus building transferable representations within single modality language vision only. extend framework learn joint vision-language representation enables much larger class vision-language tasks easily build contribute shared representation. using vision-language embeddings traditionally visual recognition posed multiclass classiﬁcation discrete labels using recognizers tasks like image captioning challenging open-vocabulary nature problems. however availability continuous word embeddings allowed reformulation visual recognition nearest neighbor search learned image-language embedding space embeddings successfully applied variety tasks require recognition image captioning phrase localization referring expressions related previous openvocabulary recognition/localization models learn visual features continuous word vector representations. however speciﬁcally focus multitask setting forms part higher-level vision-language task vqa. since svlr module reused tasks inner products embedding space forming basis models joint training provides weak supervision recognition well. fang also learn object attribute classiﬁers weak supervision form image-caption pairs using multiple instance learning framework vision-language embedding. similarly annotation flickrk entities co-supervise attention caption-generation model dataset. work goes allowing supervision come separate datasets thereby increasing amount training data available shared parameters. additionally look task beneﬁted jointly training other. visual question answering involves responding natural language query image. model closely related attention-based models attempt compute distribution regions/pixels image using inner product imageregion full query embedding region relevance used weight pool relevant visual information usually combined language representation create multimodal representation. various methods pooling elementwise-addition multiplication outer-products explored attention models active area research applications visual recognition object localization caption generation question answering machine comprehension translation neural turing machines model explicitly formulates attention image localization nouns adjectives mentioned candidate pair. ilievski related approach attention. wordvec individual words question class labels pre-trained object detector generates attention identifying regions labels. tommasi similarly pre-trainined vision-language embedding model localize noun phrases extracts scene attribute object features answer questions. model differs methods ways vision-language embeddings allow end-to-end trainability jointly training provides additional supervision attention different dataset. andreas rely heavily syntactic parse dynamically arrange parametrized neural modules. module performs speciﬁc function localizing speciﬁc word verifying relative locations. contrast approach uses static model relies language parse make interpretable modular. propose svlr module facilitate greater inductive transfer across vision-language tasks. shown fig. word region representations required object recognition attribute recognition computed svlr module. speciﬁcally formulating task terms inner products word region representations training tasks jointly ensure task provides consistent non-conﬂicting training sigfigure joint training visual recognition visual question answering proposed svlr module ﬁgure depicts sharing image word representations svlr module joint training object recognition attribute recognition vqa. recognition tasks object attribute labelled regions visual genome uses images annotated questions answers dataset. beneﬁt joint training dataset provide region groundings nouns adjectives complementary supervision provided genome recognition dataset. models task involve image word embeddings produced svlr module inner products aligning words region representations. during training joint-task model batches containing training examples task’s dataset. shared vision language representation region representations region represented using dimensional feature vectors separately encode objects attributes contained. used representations instead encourage disentangling factors variation. example expect similar apple expect similar depicts apple. features constructed extracting average pooled features resnet pretrained imagenet passing separate object attribute networks. networks consist fully connected layers batch normalization relu activations. visual recognition task classify image regions object attribute categories. classiﬁcation score region object category classiﬁcation score attribute catef attributes include adjectives gory adverbs though recognition dataset limited object categories attribute categories model produce classiﬁcation scores object attribute label given wordvec representation. experiments consist frequent object attribute categories visual genome dataset model trained using visual genome dataset provides image regions annotated object attribute labels. uses parameters embedding functions part svlr module. parameters receive gradients object loss receive gradients attribute loss. parameters word embedding model receive gradients losses. thus region’s attention score maximum adjective noun scores words mentioned question answer image representation score answer content region encoded using scores objects attributes presence unmentioned objects attributes help answer question. image representation attention-weighted average scores across regions image scores objects image region scores attributes attention score. question/answer representation construct representations question answer follow shih dividing question words bins averaging word representations concatenating representations resulting dimensional vector answer representation obtained averaging word representations answer words. word representations used produced svlr module. answer scoring combine image representations jointly score triplet. ensure equal contribution language visual features apply batch normalization linear transformations features adding together bimodal representation training dataset training parameters model scales offsets batch normalization layers. addition loss backpropagates part svlr module. sample dataset consists question image list answer options including positive answer negative answers {a−|i loss encourages correct answer scored higher incorrect answer options {a−|i margin ηans. given batch samples object loss multi-label loss object classes mutually exclusive region denote annotated object categories hypernyms extracted wordnet object loss forces true labels hypernyms score higher object labels margin ηobj. batch samples object loss attribute loss attribute loss multi-label classiﬁcation loss differences object classiﬁcation. attribute labels even less likely mutually exclusive object labels. such predict attribute independent cross entropy losses. also weigh samples based fraction positive labels batch balance positive negative labels dataset. batch samples attributes annotated region attribute loss model illustrated fig. input model image question candidate answer. regions extracted image using edge boxes svlr module used explicitly applied attention answer scoring. system assigns attention scores region according well matches words question/answer scores answer based question answer attention-weighted scores objects attributes attention scoring unlike attention models free learn correlation regions question/answers attention model encodes explicit notion vision-language grounding. region proposals extracted image denote nouns adjectives pair. region assigned attention score follows figure inference model image ﬁrst broken edge region proposals. region represented visual category scores obtained using visual recognition model. using svlr module regions also assigned attention score using inner products region features representations nouns adjectives question answer. region features pooled using relevance scores weights construct attended image representation. finally image question/answer representations combined passed neural network produce score input question-image-answer triplet. region proposals resized experiments. resnet- used image feature extraction experiments except tab. used resnet-. nouns adjectives extracted lemmatized using part-of-speech tagger wordnet lemmatizer nltk stanford dependency parser parse question bins detailed models implemented trained using tensorflow train model jointly recognition tasks minimizing following loss function using adam observe values αobj αatr relative αans used trade-off performance visual recognition tasks. experiments analyze effect transfer αans αobj αatr genome baselines corresponding others experiments dealing transfer direction αans αobj αatr margins used object answer losses ηans ηobj object attribute losses computed visual genome regions batch size answer loss computed batch size questions sampled vqa. exponentially decaying learning rate schedule initial learning rate decay rate every representations produced svlr module directly usable related vision-language tasks withadditional learning. demonstrate zeroshot cross-task transfer train svlr module using genome data apply vqa. since bimodal pooling scoring layers cannot learned without data proxy scoring function constructed using region-word scores only. region compute scores maximally aligned question nouns question adjectives score similarly computed using answer nouns adjectives. ﬁnal score answer deﬁned attention score computed using therefore highest score given pairs question well answer nouns adjectives localized image. note since model trained even single question zero-shot task also shows model image answer questions instead solely relying language prior common concern models figure interpretable inference model produces interpretable intermediate computation region relevance object/attribute predictions relevant regions. region relevance explicitly grounds nouns adjectives input image. also show object attribute predictions relevant region identiﬁed correctly answered questions. relevance masks generated relevance scores projected back source pixels locations. experiments investigate extent using svlr core representation improves transfer multitask learning. ﬁrst analyze including task improves using svlr doubles improvement compared standard multitask learning demonstrate performance well chance zero-shot setup analyze improvement training moderate overall improvements largest improvements classes training examples. also quantitatively evaluate well attention maps correlate humans using data provided table include results system trained resnet- architecture test-dev teststd along state-of-the-art model trained separate datasets supervision visual recognition image-question-answer annotation triplets antol bounding annotations object attribute categories visual genome train-val-test splits datasets follows. split train train-subset trainheld-out latter model selection. trainsubset consists samples whereas trainheld-out contains samples. test contain samples respectively. exactly questions image. evaluating speciﬁc question types. visual genome images visual genome selected images divided train-val-test using split yielding annotated regions each. selecting model evaluating recognition performance. inductive transfer table analyze role svlr module inductive transfer joint training zero-shot settings. joint training joint training models model simultaneously trained using object attribute annotations genome annotations dataset. common approach joint training common network extracting image features feeds task-speciﬁc networks input. refer approach table joint multitask. baseline implemented replacing ﬁxed vectors predetermined object attribute categories models. embedding still model longer shared across tasks. proposed joint svlr outperforms vqa-only doubling improvement achieved joint multitask. formulation tasks terms shared word-region representations effectively trans. table inductive transfer svlr joint training zero-shot settings evaluate performance model svlr module trained jointly supervision task. compare jointly-trained model model trained data. also compare traditional multitask learning setup jointly trained shares visual features object attribute word embeddings recognition. multitask learning outperforms vqa-only model using svlr module doubles improvement. model suited question types bold require visual recognition without specialized skills like counting reading. formulation attention terms inner products word region representations enables zero-shot vqa. setting train genome data apply zero-shot evaluate zero-shot further highlight transfer vqa. train genome annotations test val. model seen training data achieves overall accuracy random guessing yields zero-shot system exploit language priors alone score high shows knowledge directly applied related tasks using svlr without additional training. compare performance svlr based model trained jointly data model trained genome data analyze transfer genome test used evaluation. observe increase overall object recognition accuracy whereas average attribute accuracy remained unchanged fig. show nouns rare genome examples beneﬁt weak supervision provided vqa. average measure improvement classes fewer examples genome train occur times questions. conducted analysis genome attributes observe notable pattern possibly inherent difﬁcult evaluating multi-label attribute classiﬁcation problem figure transfer object recognition cell’s color reﬂects mean change accuracy classes within corresponding frequency ranges datasets’ training split. gains nouns rare genome common suggesting weak supervision provided training attention augments recognition performance svlr. numbers cell show genome-only mean accuracy +/change svlr multitask training followed number classes cell parentheses. shown fig. model produces interpretable intermediate outputs region relevance visual category predictions similar answer choice explained object attribute predictions associated relevant regions. relevance posed explicit localization words question answer qualitatively evaluate relevance prediction verifying predicted regions match said words. also provides greater insight failure modes shown fig. figure failure modes model cannot count read though still identify relevant regions. blind relations thus fails recognize birds present drinking water. model give score correct answer despite accurate visual recognition. instance model observes asphalt predicts concrete likely language bias. clear example error language bias top-left image believes lady holding baby rather even though visual recognition conﬁrms evidence dog. finally model fails answer questions require complex reasoning comparing multiple regions. table human attention comparison compare attention maps human attentions collected comparison done resizing attention maps computing spearman rank correlation include strong baseline using synthetic center-focused heatmap center column. human column represents inter-annotator agreement. scores hico recomputed using released data differ slightly originally reported. model leads signiﬁcantly higher correlation human annotations existing models. figure table includes correlation scores attention maps various attention models human attention subset question-answer pairs validation set. proposed svlr model signiﬁcantly outperforms models compare with. however note strong center-focused heatmap baseline still outperforms models signifying main topic question often located center image. such also evaluate correlations multiple subsets human attention maps figure thresholding based correlation center heatmap. note learned attention models appear better correlation human attention lower thresholds human attention correlates poorly center-focused heatmap result also demonstrated hico trained train+val train table external comparisons include external comparisons note internal comparisons controlled informative. results implementation test accuracy unclear whether uses train. original implementation using resnet- yields test-dev test-std respectively. reports test-dev accuracy directly comparable model note overall performance model slightly worse dataset beneﬁts visual attention. model achieves color questions using attention outperforming wtl’s mlp’s table compare word representations svlr model wordvec showing several nearest neighbors embeddings. observe shift non-visual neighborhoods meanings visual ones neighbors computed using cosine distance mean centering. conclusion shlens steiner sutskever talwar tucker vanhoucke vasudevan vi´egas vinyals warv wattenberg wicke zheng. tensorflow large-scale machine learning heterogeneous systems software available tensorﬂow.org. antol agrawal mitchell batra lawrence zitnick parikh. visual question answering. proceedings ieee international conference computer vision pages agrawal zitnick parikh batra. human attention visual question answering humans conferdeep networks look regions? ence empirical methods natural language processing deng dong socher l.-j. feifei. imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference pages ieee fang gupta iandola srivastava deng doll´ar mitchell platt captions visual concepts back. proceedings ieee conference computer vision pattern recognition pages figure mean spearman rank-correlation model predicted human attention various thresholds. threshold point deﬁnes subset dataset human attention correlation synthetic center heatmap threshold value. example ﬁrst sample point curve mean correlation model human attention measured subset human attention’s correlation center heatmap less equal seen attention maps produced proposed svlr model correlate human attention signiﬁcantly models. threshold approaches synthetic center heatmap baseline outperforms proposed models conﬁrming majority questions something center image. note slight differences implementations subsets differ slightly used class vision-language problems using enhance inductive transfer propose sharing core vision language representations across tasks exploits word-region alignment. plan extend method larger sets vision-language tasks.", "year": 2017}