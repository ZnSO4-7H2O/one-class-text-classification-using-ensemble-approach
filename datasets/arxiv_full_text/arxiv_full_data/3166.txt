{"title": "Total Variation and Euler's Elastica for Supervised Learning", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "In recent years, total variation (TV) and Euler's elastica (EE) have been successfully applied to image processing tasks such as denoising and inpainting. This paper investigates how to extend TV and EE to the supervised learning settings on high dimensional data. The supervised learning problem can be formulated as an energy functional minimization under Tikhonov regularization scheme, where the energy is composed of a squared loss and a total variation smoothing (or Euler's elastica smoothing). Its solution via variational principles leads to an Euler-Lagrange PDE. However, the PDE is always high-dimensional and cannot be directly solved by common methods. Instead, radial basis functions are utilized to approximate the target function, reducing the problem to finding the linear coefficients of basis functions. We apply the proposed methods to supervised learning tasks (including binary classification, multi-class classification, and regression) on benchmark data sets. Extensive experiments have demonstrated promising results of the proposed methods.", "text": "recent years total variation euler’s elastica successfully applied image processing tasks denoising inpainting. paper investigates extend supervised learning settings high dimensional data. supervised learning problem formulated energy functional minimization tikhonov regularization scheme energy composed squared loss total variation smoothing solution variational principles leads euler-lagrange pde. however always high-dimensional cannot directly solved common methods. instead radial basis functions utilized approximate target function reducing problem ﬁnding linear coeﬃcients basis functions. apply proposed methods supervised learning tasks benchmark data sets. extensive experiments demonstrated promising results proposed methods. supervised learning infers function maps inputs desired outputs guidance training data. main tasks supervised learning classiﬁcation regression. huge number supervised learning methods developed several decades existing methods roughly divided statistics based function learning based advantage function learning methods powerful mathematical theories functional analysis utilized rather optimizations discrete data points. function learning methods derived tikhonov regularization minimizes loss term plus smoothing regularizer. successful classiﬁcation regression method whose cost function composed hinge loss rkhs norm determined kernel. replacing hinge loss squared loss modiﬁed algorithm called regularized least squares method addition manifold regularization introduced regularizer squared gradient magnitude manifolds. discrete version amounts graph laplacian regularization approximates original energy functional. recent work geometric level classiﬁer energy functional composed margin-based loss geometric regularization term based surface area calculus. total variation widely used image processing tasks denoising inpainting. pioneering work rudin osher fatemi’s image denoising model input image noise desired output image regulation parameter balances terms image domain. ﬁrst ﬁtting term measures ﬁdelity input second p-sobolev regularization term machine learning literature p-sobolev regularizer found nonparametric smoothing splines generalized additive models projection pursuit regression models speciﬁcally belkin proposed manifold regularization term euler ﬁrst introduced elastica energy curve modeling torsion-free elastic rods. mumford reintroduced elastica computer vision. later elastica based image inpainting methods developed paper supervised learning problem formulated energy functional minimization under tikhonov regularization scheme energy composed squared loss total variation penalty euler’s elastica penalty. since models achieved great success image denoising image inpainting natural question whether success models image processing applications transferred high dimensional data analysis supervised learning. paper investigates question extending models supervised learning settings evaluating performance benchmark data sets state-ofthe-art methods. figure shows classiﬁcation result popular moon data classiﬁer learned target function. interestingly classiﬁer also motivated image processing techniques gradient descent time marching leads mean curvature ﬂow. paper organized follows. begin brief review section section proposed models described numerical solutions developed section section presents experimental results section concludes paper. element. euler obtained energy studying steady shape thin torsion-free external forces. curve implies lowest elastica energy thus getting name. according link elastica image inpainting relies interpolation capability elastica. elastica comply connectivity principle better total variation. kinds nonlinear splines like classical polynomial splines natural tools completing missing occluded edges. posed literature hinge loss squared loss logistic loss logistic regression huber loss exponential loss among others. throughout paper squared loss used models rather simpler diﬀerential form. using calculus variation minimization reduced nonlinear euler-lagrange equation. ﬁnite diﬀerence scheme used give numerical implementation experimental results show based inpainting performs better version. elastica regarded extension total variation since elastica degenerates total variation fact elastica combination total variation suppressing oscillations gradient direction curvature regularization term penalizes non-smooth level curves model widely used image processing literatures. using calculus variations minimization reduced following eulerlagrange partial diﬀerential equation natural boundary condition along vector along boundary relatively simple solved using common methods three dimensions. next section provides function approximation method solving high dimensions. goal explore applied classiﬁcation regression problems high dimensional data sets. typical procedure three steps function learning problem continuous setting design proper energy functional; derive euler-lagrange calculus variations; solve discrete data points. corresponding target varibles goal estimate unknown function point diﬀerence classiﬁcation regression lies corresponding target values discrete continuous. widely used tikhonov regularization framework supervised learning formulated continuous fashion powerful mathematical analysis tools make sense. speciﬁcally calculus variations exploited minimize energy functional leading euler-langrange pde. nonlinearity regularizer model corresponding complicated eﬃciently solved. even though associated model solved finite diﬀerence method finite element method spaces currently tools deal high dimensional data. therefore take function approximation idea using radial basis functions similar treatment represents function using polynomials. widely used radial basis function simple expressions powerful ﬁtting ability. target function expressed elastica regularizer resulting decision boundary model lowest elastica energy. model degenerates model. therefore uniﬁed solution implemented model model described next section. remarks curvature high dimensional spaces. curve image inpainting tasks determines level curve according implicit function theorem. surface curvature given amounts mean curvature surface. mean curvature deﬁned average principal curvatures. abstractly expressed trace second fundamental form divided intrinsic dimension table summarizes curvature expressions high dimensional spaces. hence expression used high dimensional situd− transferred ations since constant model special case model describe solutions complicated model section. algorithms developed tackle nonlinearity gradient descent time marching lag-linear equation iteration. expression model. time complexity iteration number data points dimension. maximal number iterations parameters algorithm parameter regularization parameter elastica weight parameter note since absorbed test data sets binary classiﬁcation libsvm website. originally data sets scaled serve benchmark test libsvm implementation. downloaded seven data sets evaluate performance methods kinds implementations gradient descent method lagged linear equation method optimal parameters algorithm selected grid search using -fold cross-validation. make grid search practical common parameters searched except bpnn. empirically parameter parameter ﬁxed excluding bpnn common table bpnn performs worst lagle solution outperforms others data sets. implementation competitive svm. similar accuracies achieved partially close connections. multi-class tests collected data sets libsvm website machine learning repository including frequently used small data sets usps handwritten digital set. usps data used reduce dimension randomly select samples experiments. except bpnn built-in ability multiclass tasks almost function learning approaches originally designed binary classiﬁcation. order handle multi-class situations usually versus versus strategies adopted. using needs learn functions fulﬁll multi-class task number classes. recently eﬃcient binary encoding strategy proposed represent decision boundary logm functions. experiments strategy used. binary problems -fold cross-validation choose optimal parameters method. except bpnn methods common parameters searched logafollowing spirit lagged diﬀusivity ﬁxedpoint iteration method develop following lagged linear equation iteration method. empirically original lagged diﬀusivity ﬁxed-point iteration often yields poor performance brute-force linearization nonlinear pde. data dimension. using lagged idea obtain lagged linear equation iteration algorithm ﬁxing solve system linear equations respect compute updated iterate convergence maximal iteration number. similarly two-step lagged iteration procedure developed model ﬁxing solve linear system respect compute updated iterate convergence maximal iteration number. three parameters regularization parameter least squares problems. accuracy results demonstrate bpnn performs worst comparable test data sets. compared tv/ee achieves higher accuracies data sets performs worse data sets oﬀers best accuracies data sets. reason might complex even wiggly decision boundaries preferred multi-class data sets. strong regularization geometric shapes tv/ee adapt yield complex decision hypersurfaces data sets. seven regression data sets machine learning repository validate proposed methods compared bpnn data sets scaled note gradient descent method used take experimental settings running times fold cross-validation data set. table shows regression results using mean square errors clearly achieve lower regularization framework function learning approaches become popular recent machine learning literature. great success total variation euler’s elastica models image processing area extend models supervised classiﬁcation regression high dimensional data sets. regularizer permits steeper edges near decision boundaries elastica smoothing term penalizes non-smooth level hypersurfaces target function. compared bpnn proposed methods demonstrated competitive performance commonly used benchmark data sets. speciﬁcally models achieve better performance data sets binary classiﬁcation regression. currently main disadvantage slow convergence speed iteration procedures. future work explore authors would like thank anonymous reviewers helpful suggestions. work supported national basic research program china national science foundation china", "year": 2012}