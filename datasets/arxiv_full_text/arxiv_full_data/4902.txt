{"title": "Improving Policy Gradient by Exploring Under-appreciated Rewards", "tag": ["cs.LG", "cs.AI"], "abstract": "This paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties. Current policy-based methods use entropy regularization to encourage undirected exploration of the reward landscape, which is ineffective in high dimensional spaces with sparse rewards. We propose a more directed exploration strategy that promotes exploration of under-appreciated reward regions. An action sequence is considered under-appreciated if its log-probability under the current policy under-estimates its resulting reward. The proposed exploration strategy is easy to implement, requiring small modifications to an implementation of the REINFORCE algorithm. We evaluate the approach on a set of algorithmic tasks that have long challenged RL methods. Our approach reduces hyper-parameter sensitivity and demonstrates significant improvements over baseline methods. Our algorithm successfully solves a benchmark multi-digit addition task and generalizes to long sequences. This is, to our knowledge, the first time that a pure RL method has solved addition using only reward feedback.", "text": "paper presents novel form policy gradient model-free reinforcement learning improved exploration properties. current policy-based methods entropy regularization encourage undirected exploration reward landscape ineffective high dimensional spaces sparse rewards. propose directed exploration strategy promotes exploration under-appreciated reward regions. action sequence considered under-appreciated log-probability current policy under-estimates resulting reward. proposed exploration strategy easy implement requiring small modiﬁcations reinforce algorithm. evaluate approach algorithmic tasks long challenged methods. approach reduces hyper-parameter sensitivity demonstrates signiﬁcant improvements baseline methods. proposed algorithm successfully solves benchmark multi-digit addition task generalizes long sequences which knowledge ﬁrst time pure method solved addition using reward feedback. humans reason symbolic objects solve algorithmic problems. learning count manipulate numbers simple arithmetic people eventually learn invent algorithms even reason correctness efﬁciency. ability invent algorithms fundamental artiﬁcial intelligence although symbolic reasoning long history recently statistical machine learning neural network approaches begun make headway automated algorithm discovery would constitute important milestone path nevertheless recent successes depend strong supervision learn mapping training inputs outputs maximizing conditional log-likelihood much like neural machine translation systems dependence strong supervision signiﬁcant limitation match ability people invent algorithmic procedures based solely trial error. contrast reinforcement learning methods hold promise searching discrete objects symbolic representations algorithms considering much weaker feedback form simple veriﬁer tests correctness program execution given problem instance. despite recent excitement around tackle atari games standard methods able consistently reliably solve algorithmic tasks simplest cases property algorithmic problems makes challenging reward sparsity i.e. policy usually long action sequence exactly right obtain non-zero reward. believe factors limiting effectiveness current methods sparse reward setting undirected exploration strategies \u0001-greedy entropy regularization long action sequences delayed sparse reward hopeless explore space uniformly blindly. instead propose formulation encourage exploration action sequences under-appreciated current policy. formulation considers action sequence under-appreciated model’s log-probability assigned action sequence under-estimates resulting reward action sequence. exploring underappreciated states actions encourages policy better calibration logprobabilities observed reward values even action sequences negligible rewards. effectively increases exploration around neglected action sequences. term proposed technique under-appreciated reward exploration show objective given urex combination mode seeking objective mean seeking term provides well motivated trade-off exploitation exploration. empirically evaluate method take algorithmic tasks sequence reversal multi-digit addition binary search. choose focus tasks because although simple present difﬁcult sparse reward setting limited success standard approaches. experiments demonstrate urex signiﬁcantly outperforms baseline methods entropy regularized reinforce one-step q-learning especially difﬁcult tasks multi-digit addition. moreover urex shown robust changes hyper-parameters makes hyper-parameter tuning less tedious practice. addition introducing variant policy gradient improved performance paper ﬁrst demonstrate strong results method algorithmic tasks. knowledge addition task solved model-free reinforcement learning approach. observe policies learned urex successfully generalize long sequences; e.g. random restarts policy learned urex addition task correctly generalizes addition numbers digits mistakes even though training sequences digits long. although research using neural networks learn algorithms witnessed surge recent interest problem program induction examples long history many ﬁelds including program induction inductive logic programming relational learning regular language learning rather presenting comprehensive survey program induction here focus neural network approaches algorithmic tasks highlight relative simplicity neural network architecture. successful applications neural networks algorithmic tasks rely strong supervision inputs target outputs completely known priori. given dataset examples learns network parameters maximizing conditional likelihood outputs backpropagation kaiser sutskever vinyals however target outputs available novel tasks prior algorithm known available. desirable approach inducing algorithms followed paper advocates using self-driven learning strategies receive reinforcement based outputs produced. hence access veriﬁer algorithmic problem learn algorithm. example know sort array check extent array sorted provide reward signal necessary learning sorting algorithms. formulate learning algorithms problem make model-free policy gradient methods optimize parameters associated algorithm. setting goal learn policy given observed state step estimates distribution next action denoted actions represent commands within algorithm states represent joint state algorithm environment. previous work area focused augmenting neural network additional structure increased capabilities contrast utilize simple architecture based standard recurrent neural network lstm cells depicted figure episode environment initialized latent state unknown agent determines subsequent state transition reward functions. agent observes figure agent’s architecture represents policy. environment initialized latent vector time step environment produces state agent takes input previously sampled action produces distribution next action then sample action apply environment. input network outputs distribution action sampled. action applied environment agent receives state observation state previous action process repeats episode. upon termination reward signal received. start discussing common form policy gradient reinforce entropy regularized variant reinforce applied model-free policy-based learning neural networks algorithmic domains goal learn policy that given observed state step estimates distribution next action denoted environment initialized latent vector determines initial observed state transition function note nondeterministic transitions markov decision processes recovered assuming includes random seed nondeterministic functions. given latent state model probability action sequence expressed environment provides reward episode denoted ease readability drop subscript simply write objective used optimize policy parameters consists maximizing expected reward actions drawn policy plus optional maximum entropy regularizer. given distribution initial latent environment states express regularized expected reward reward using monte carlo samples. using monte carlo samples ﬁrst draws i.i.d. samples latent environment states {h}n approximate gradient using known baseline sometimes called critic. note subtracting offset rewards simply results shifting objective constant. unfortunately directly maximizing expected reward prone getting trapped local optimum. combat tendency williams peng augmented expected reward objective including maximum entropy regularizer promote greater exploration. refer variant reinforce ment since non-negative zero said given particular form ﬁnding exactly characterizes divergence known mode seeking even entropy regularization learning policy optimizing direction prone falling local optimum resulting sub-optimal policy omits modes although entropy regularization helps mitigate issues conﬁrmed experiments effective exploration strategy undirected requires small regularization coefﬁcient avoid much random exploration. instead propose directed exploration strategy improves mean seeking behavior policy gradient principled way. start considering alternate mean seeking direction divergence considered direction directly learn policy optimizing norouzi argue structured prediction problems draw samples optimizing effective since sampling non-stationary policy required. log-linear model features oraml convex whereas even log-linear case. unfortunately scenarios reward landscape unknown computing normalization constant intractable sampling straightforward. problems reward landscape completely unknown hence sampling intractable. paper proposes approximate expectation respect using selfnormalized importance sampling proposal distribution reference distribution view importance weights evaluating discrepancy scaled rewards policy’s log-probabilities among samples sample least appreciated model i.e. largest receives largest positive feedback practice found using importance sampling raml objective always yield promising solutions. particularly beginning training still away variance importance weights large self-normalized importance sampling procedure results poor approximations. stabilize early phases training ensure model distribution achieves large expected reward scores combine expected reward raml objectives beneﬁt best mode mean seeking behaviors. accordingly propose following objective call under-appreciated reward exploration expected reward raml objectives. preliminary experiments considered composite objective oraml found removing entropy term beneﬁcial. hence ourex objective include entropy regularization. accordingly oraml. appendix derives optimum policy ourex longer optimal policy ourex function optimal policy orl. optimal policy urex sharply concentrated high reward regions action space advantage urex leave analysis behavior future work. compute gradient ourex self-normalized importance sampling estimate outlined assume importance weights constant contribute gradient dθourex. approximate gradient draws i.i.d. samples latent environment states {h}n obtain m=wτ reinforce rewards shifted offset gradient model logprobability sample action sequence reinforced corresponding reward large corresponding importance weights large meaning action sequence under-appreciated. normalized importance weights computed using softmax operator softmax. presenting experimental results brieﬂy review pieces previous work closely relate urex approach. reward-weighted regression. raml urex objectives bear similarity method continuous control known reward-weighted regression using notation objective expressed optimize orwr peters schaal propose technique inspired algorithm maximize variational lower bound based variational distribution objective interpreted correlation contrast raml urex objectives based divergence optimize objective formulates gradient exp{ self-normalized importance sampling note critical difference estimate gradients different objectives hence importance weights correct sampling distribution opposed beyond important technical differences optimal policy orwr distribution probability mass concentrated action sequence maximal reward whereas optimal policies raml urex everywhere nonzero probability different action sequences assigned proportionally exponentiated reward further notion under-appreciated reward exploration evident ourex urex’s performance missing formulation. exploration. literature contains many different attempts incorporating exploration compared method. common exploration strategy considered valuebased \u0001-greedy q-learning step agent either takes best action according current value approximation probability takes action sampled uniformly random. like entropy regularization approach applies undirected exploration achieved recent success game playing environments prominent approaches improving exploration beyond \u0001-greedy value-based model-based focused reducing uncertainty prioritizing exploration toward states actions agent knows least. basic intuition underlies work counter recency methods exploration methods based uncertainty estimates values methods prioritize learning environment dynamics methods provide intrinsic motivation curiosity bonus exploring unknown states contrast value-based methods exploration policy-based methods often by-product optimization algorithm itself. since algorithms like reinforce thompson sampling choose actions according stochastic policy sub-optimal actions chosen non-zero probability. q-learning algorithm also modiﬁed sample action softmax values rather argmax asynchronous training also reported exploration effect valuepolicy-based methods. mnih report asynchronous training stabilize training reducing bias experienced single trainer. using multiple separate trainers agent less likely become trapped policy found locally optimal local conditions. spirit osband multiple value approximators sample episode implicitly incorporate exploration. relating concepts value policy exploration strategy propose tries bridge discrepancy two. particular urex viewed hybrid combination value-based policy-based exploration strategies attempts capture beneﬁts each. per-step reward. finally restrict episodic settings reward associated entire episode states actions much work done take advantage environments provide per-step rewards. include policy-based methods actor-critic value-based approaches based qlearning value-based methods proposed softening q-values interpreted adding form maximum-entropy regularizer episodic total-reward setting consider naturally harder since credit assignment individual actions within episode unclear. assess effectiveness proposed approach algorithmic tasks openai well binary search problem. task summarized below details available website corresponding open-source code. case environment hidden tape hidden sequence. agent observes sequence pointer single character moved pointer control actions. thus action represented tuple denotes move boolean denoting whether write output symbol write. copy agent emit copy sequence. pointer actions move left right. duplicatedinput hidden tape character repeated twice. agent must dedupli. repeatcopy agent emit hidden sequence once emit sequence reverse order emit original sequence again. pointer actions move left right. reverse agent emit hidden sequence reverse order. before pointer reversedaddition hidden tape grid digits representing numbers base little-endian order. agent must emit numbers little-endian order. allowed pointer actions move left right down. openai provides additional harder task called reversedaddition involves adding three numbers. omit task since none methods make much progress tasks input sequences encountered training range length characters. reward given correct emission. incorrect emission small penalty incurred episode terminated. agent also terminated penalized reward episode exceeds certain number steps. experiments using urex ment associate episodic sequence actions total reward deﬁned per-step rewards. experiments using q-learning hand used per-step rewards. tasks success threshold determines required average reward episodes agent considered successful. also conduct experiments additional algorithmic task described below binarysearch given integer environment hidden array distinct numbers stored ascending order. environment also query number unknown agent contained somewhere array. goal agent query number array small number actions. environment three integer registers initialized step agent interact environment four following actions increment value register divide value register replace value register average registers. compare value register receive signal indicating value greater. agent succeeds calls array cell holding value table cell shows percentage trials different hyper-parameters random restarts successfully solve algorithmic task. urex robust hyper-parameter changes ment. evaluate ment temperatures urex agent terminated number steps exceeds maximum threshold steps recieves reward agent ﬁnds step recieves reward maximum number steps allow agent perform full linear search. policy performing full linear search achieves average reward chosen uniformly random elements array. policy employing binary search number steps. selected uniformly random range binary search yields optimal average reward success threshold task average reward compare policy gradient method using under-appreciated reward exploration main baselines reinforce entropy regularization termed ment value determines degree regularization. standard reinforce obtained. one-step double q-learning based bootstrapping step future rewards. hyper-parameter tuning often tedious algorithms. found proposed urex method signiﬁcantly improves robustness changes hyper-parameters compared ment. experiments perform careful grid search hyper-parameters ment urex. hyper-parameter setting ment urex methods times different random restarts. explore following main hyper-parameters learning rate denoted chosen possible values maximum norm gradients beyond gradients clipped. parameter denoted matters training rnns. value selected temperature parameter controls degree exploration ment urex. ment urex consider consistently performs well across tasks. experiments ment urex treated exactly same. fact change implementation lines code. given value task training jobs comprising learning rates clipping values random restarts. algorithm maximum number steps determined based difﬁculty task. training jobs copy duplicatedinput repeatcopy reverse reversedaddition binarysearch stochastic gradient steps respectively. running trainer longer result better performance. policy network comprises single lstm layer nodes. adam optimizer experiments. table presents number successful attempts expected reward values algorithm given best hyper-parameters. onestep q-learning results also included table. also present training curves ment urex figure clear urex outperforms baselines tasks. difﬁcult tasks reverse reverseaddition urex able consistently appropriate algorithm ment q-learning fall behind. importantly binarysearch task exhibits many local maxima necessitates smart exploration urex method solve consistently. q-learning baseline solves simple tasks makes little headway harder tasks. believe entropy regularization policy gradient \u0001greedy q-learning relatively weak exploration strategies long episodic tasks delayed rewards. tasks random exploratory step wrong direction take agent optimal policy hampering ability learn. contrast urex provides form adaptive smart exploration. fact observe variance importance weights decreases agent approaches optimal policy effectively reducing exploration longer necessary; appendix figure average reward training ment urex best hyper-parameters method algorithm times random restarts. curves present average reward well single standard deviation region clipped max. conﬁrm whether method able correct algorithm multi-digit addition investigate generalization longer input sequences provided training. evaluate trained models inputs length digits even though training sequences characters. length test model randomly generated inputs stopping accuracy falls models trained addition urex models generalize numbers digits without observed mistakes. best urex hyper-parameters random restarts able generalize successfully. detailed results generalization performance different tasks including copy table results several algorithmic tasks comparing q-learning policy gradient based ment urex. best hyper-parameters method algorithm times random restarts. number successful attempts achieve reward threshold reported. expected reward computed last iterations training also reported. duplicatedinput reversedaddition appendix evaluations take action largest probability time step rather sampling randomly. also looked generalization models trained binarysearch task. found none agents perform proper binary search. rather solved task perform hybrid binary linear search ﬁrst actions follow binary search pattern agent switches linear search procedure narrows search space; appendix execution traces binarysearch reversedaddition. thus longer input sequences agent’s running time complexity approaches linear rather logarithmic. hope future work make progress task. task especially interesting reward signal incorporate correctness efﬁciency algorithm. experiments make curriculum learning. environment begins providing small inputs moves longer sequences agent achieves close maximal reward number steps. policy gradient methods including ment urex provide agent reward episode notion intermediate reward. value-based baseline implement one-step q-learning described mnih -alg. employing double q-learning \u0001-greedy exploration. policy-based approaches estimate values. grid search exploration rate exploration rate decay learning rate sync frequency conducted best hyper-parameters. unlike methods q-learning baseline uses intermediate rewards given openai per-step basis. hence q-learning baseline slight advantage policy gradient methods. tasks except copy stochastic optimizer uses mini-batches comprising policy samples model. samples correspond different random sequences drawn environment random policy trajectories sequence. words deﬁned ment samples subtract mean coefﬁcient regularization. urex trajectories subtract mean reward normalize importance sampling weights. subtract mean normalized importance weights. copy task mini-batches samples using experiments conducted using tensorﬂow present variant policy gradient called urex promotes exploration action sequences yield rewards larger model expects. exploration strategy result importance sampling optimal policy. experimental results demonstrate urex signiﬁcantly outperforms value policy based methods robust changes hyper-parameters. using urex solve algorithmic tasks like multi-digit addition episodic reward methods cannot reliably solve even given best hyper-parameters. introduce algorithmic task based binary search advocate research area especially computational complexity solution also interest. solving tasks important developing human-like intelligence learning algorithms also important generic reinforcement learning smart efﬁcient exploration successful methods. references mart´ın abadi paul barham jianmin chen zhifeng chen andy davis jeffrey dean matthieu devin sanjay ghemawat geoffrey irving michael isard tensorﬂow system largescale machine learning. arxiv. alex graves greg wayne malcolm reynolds harley danihelka agnieszka grabskabarwinska sergio colmenarejo edward grefenstette tiago ramalho john agapiou adria badia karl hermann yori zwols georg ostrovski adam cain helen king christopher summerﬁeld phil blunsom koray kavukcuoglu demis hassabis. hybrid computing using neural network dynamic external memory. nature volodymyr mnih koray kavukcuoglu david silver alex graves ioannis antonoglou daan wierstra martin riedmiller. playing atari deep reinforcement learning. arxiv. volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. icml mohammad norouzi samy bengio zhifeng chen navdeep jaitly mike schuster yonghui dale schuurmans. reward augmented maximum likelihood neural structured prediction. nips peters stefan schaal. reinforcement learning reward-weighted regression operational space control. proceedings international conference machine learning tables provide details different cells table table presents results ment using best temperature urex variety learning rates clipping values. cell number trials random restarts succeed solving task using speciﬁc table provides detailed look generalization performance trained models copy duplicatedinput reversedaddition. tables show number models solve task correctly drops length input increases. table generalization results. cell includes number runs different hyperparameters random initializations achieve accuracy input length speciﬁed length. bottom maximal length least model achieves accuracy. figure graphical representation trained addition agent. agent begins left corner grid ternary digits. time step move left right optionally write output. table example trace binarysearch task number position time agent observes environment samples action also include inferred range indices agent narrowed position ﬁrst several steps agent follow binary search algorithm. however point agent switches linear search figure plot shows variance importance weights urex updates well average reward successful runs. variance starts high reaches near zero towards optimal policy found. ﬁrst plot rise variance corresponds plateau increase average reward. figure plot present average performance urex ment repeats bandit-like task choosing optimal hyperparameters method. task agent chooses actions step receives payoff corresponding entry reward vector sampled randomly independently uniform distribution. parameterize policy weight vector basis vectors action sampled standard normal distribution. plot shows average rewards obtained setting experiments consisting repeats redrawn start repeat) random restarts within repeat thus task presents relatively simple problem large action space urex outperforms ment.", "year": 2016}