{"title": "A Greedy Approach to Adapting the Trace Parameter for Temporal  Difference Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "One of the main obstacles to broad application of reinforcement learning methods is the parameter sensitivity of our core learning algorithms. In many large-scale applications, online computation and function approximation represent key strategies in scaling up reinforcement learning algorithms. In this setting, we have effective and reasonably well understood algorithms for adapting the learning-rate parameter, online during learning. Such meta-learning approaches can improve robustness of learning and enable specialization to current task, improving learning speed. For temporal-difference learning algorithms which we study here, there is yet another parameter, $\\lambda$, that similarly impacts learning speed and stability in practice. Unfortunately, unlike the learning-rate parameter, $\\lambda$ parametrizes the objective function that temporal-difference methods optimize. Different choices of $\\lambda$ produce different fixed-point solutions, and thus adapting $\\lambda$ online and characterizing the optimization is substantially more complex than adapting the learning-rate parameter. There are no meta-learning method for $\\lambda$ that can achieve (1) incremental updating, (2) compatibility with function approximation, and (3) maintain stability of learning under both on and off-policy sampling. In this paper we contribute a novel objective function for optimizing $\\lambda$ as a function of state rather than time. We derive a new incremental, linear complexity $\\lambda$-adaption algorithm that does not require offline batch updating or access to a model of the world, and present a suite of experiments illustrating the practicality of our new algorithm in three different settings. Taken together, our contributions represent a concrete step towards black-box application of temporal-difference learning methods in real world problems.", "text": "process challenging essential good performance. online setting study here agent-environment interaction produces unending stream temporally correlated data. setting testing-training split thus agent’s learning process must robust adapt situations considered human designer. robustness often critically related values small parameters control learning process real-world applications however cannot expect test large range theses parameter values situations agent face ensure good performance—common practice empirical studies. unfortunately safe values parameters usually problem dependent. example oﬀ-policy learning large importance sampling ratios destabilize provably convergent gradient temporal diﬀerence learning methods parameters particular situations turn meta-learning algorithms adapt parameters agent continuously based stream experience notion agent’s learning progress. meta-learning approaches potentially improve robustness also help agent specialize current task thus improve learning speed. temporal diﬀerence learning methods make important parameters step-size parameter tracedecay parameter. step-size parameter used stochastic gradient descent algorithms available adjusting parameter online reinforcement learning trace decay parameter hand generally applicable meta-learning algorithms compatible function approximation incremental processing oﬀ-policy sampling. diﬃculty adapting trace decay parameter mainly arises fact seemingly multiple roles also inﬂuences ﬁxed-point solution. parameter introduced samuel’s checker player later described interpolation parameter ofﬂine monte-carlo sampling sutton. empirically demonstrated values zero often perform best practice trace parameter also viewed bias-variance trade-oﬀ parameter closer less biased likely higher variance closer zero biased likely lower variance. however also described credit-assignment parameter method encode probability transitions incorporate agent’s conﬁdence value funcone main obstacles broad application reinforcement learning methods parameter sensitivity core learning algorithms. many large-scale applications online computation function approximation represent strategies scaling reinforcement learning algorithms. setting eﬀective reasonably well understood algorithms adapting learning-rate parameter online learning. meta-learning approaches improve robustness learning enable specialization current task improving learning speed. temporaldiﬀerence learning algorithms study here another parameter similarly impacts learning speed stability practice. unfortunately unlike learning-rate parameter parametrizes objective function temporal-diﬀerence methods optimize. diﬀerent choices produce diﬀerent ﬁxed-point solutions thus adapting online characterizing optimization substantially complex adapting learningrate parameter. meta-learning method achieve incremental updating compatibility function approximation maintain stability learning oﬀ-policy sampling. paper contribute novel objective function optimizing function state rather time. derive incremental linear complexity λ-adaption algorithm require oﬄine batch updating access model world present suite experiments illustrating practicality algorithm three different settings. taken together contributions represent concrete step towards black-box application temporaldiﬀerence learning methods real world problems. trace decay function speciﬁes trace parameter function state. trace parameter averages estimate return λ-return starting next step becomes return value function estimated averaging rollouts state. becomes equal one-step λ-return γt+x value function estimated linear algorithm. λ-return often easier estimate yields accurate predictions using one-step return. intuition large estimate high-variance averaging possibly long trajectories noisy rewards less bias initial biased estimates value function participate less computation return. case estimate lower-variance because fewer potentially noisy rewards participate bias increase role initial value function estimates. discuss intuition parameter next section. generalization state-based widely considered though concept introduced decade generalization shown useful bellman operator generalized include state-based choice per-state inﬂuences ﬁxed point. time-based hand would result well-deﬁned ﬁxed point. therefore ensure welldeﬁned ﬁxed point design objective algorithm learn state-based paper considers onoﬀ-policy policy evaluation. conventional on-policy learning setting estimate based samples generated selecting actions according target policy oﬀ-policy case estimate based samples generated selecting actions according behavior policy order learn tion estimates averaging n-step returns selecting complicated fact part problem deﬁnition solution bellman ﬁxed point equation dependent choice approaches setting existing work limited special cases. instance several approaches analyzed setting variants introduced simplify analysis including phased though provide valuable insights role analysis easily extend conventional algorithms. sutton singh investigated tuning learning rate parameter proposed meta-learning algorithms. ﬁrst assumes problem modeled acyclic requires access transition model mdp. singh dayan kearns singh contributed extensive simulation studies interaction agent parameters chain relied access model oﬄine computation. recent study explores bayesian variant learning requires batch samples used oﬀ-line. finally konidaris introduce method remove parameter altogether. approach however extended oﬀ-policy setting full algorithm computationally expensive incremental estimation incremental variant introduces sensitive meta-parameter. although long-history prior work helped develop intuitions available solutions still cases outlined above. paper introduces objective based locally optimizing bias-variance develop eﬃcient incremental algorithm learning state-based forward-backward analysis derive incremental algorithm estimate variance return. using estimate obtain closed-form estimate time-step. finally empirically demonstrate generality approach suite on-policy oﬀ-policy experiments. results show algorithm λgreedy consistently amongst best performing adapting problem changes whereas ﬁxed approach works well settings poorly anothers. discounting per-state discrete number timesteps agent observes current state selects action according target policy environment transitions state emits reward rt+. state transitions governed transition function s×a×s denotes probability transitioning action timestep future rewards summarized monte carlo optimizing bias-variance trade-oﬀ however difﬁcult aﬀects return approximating. jointly optimizing across time-steps generally feasible. strategy take batch approach optimal determined seeing data goal however develop approaches online setting future states actions rewards inﬂuence observed. propose take greedy approach time step select optimize bias-variance trade-oﬀ step. greedy objective corresponds minimizing mean-squared error unbiased return notice interpolates current value estimate unbiased return recursive. picking gives unbiased estimate since would estimating greedily decide step locally optimize mean-squared error greedy decision made given available choosing λt+. simplify notation section assume given expectations. step-sizes arbitrary initial importance sampling ratio facilitates learning rewards generated following instead ratio large small compound destabilize learning. obtain objective selecting need clarify role. although introduced goal trading bias variance several algorithms signiﬁcant theory developed role roles suggested; however discuss below still thought bias-variance trade-oﬀ. parameter described credit assignment parameter allows perform multi-step updates time step. update controls amount credit assigned previous transitions using eligibility trace close assigns credit current reward previous transitions resulting updates many states along current trajectory. conversely eligibility trace cleared credit assigned back time performing single-step update. fact intuition still thought bias-variance trade-oﬀ. terms credit assignment ideally always want send maximal credit decayed current reward also unbiased. practice however often leads high variance thus mitigate variance choosing less speed learning overall introduce bias. another interpretation reﬂect conﬁdence value function estimates conﬁdence value estimate state high close meaning trust estimates provided conﬁdence suspecting inaccurate close meaning trust observed rewards more. example states indistinguishable function approximation trust much. intuition similarly translates bias-variance. accurate decreasing incur bias signiﬁcantly decrease variance since gives correct value. inaccurate increased bias worth reduced variance closer actual samples. finally less commonly discussed interpretation acts parameter simulates form experience replay imagine sending back information eligibility traces like simulating experience model model could trajectories experience replay traces longer update gets trajectory information experience replay. trajectory point however unlikely want information. approach taken sutton singh transition probabilities. even model-based interpretation goal setting becomes mitigating variance without incurring much bias. always feasible unless variance error zero though importance sampling ratio aﬀect choice current time step dramatic eﬀect future eligibility trace. example target behavior policy strongly mis-matched large multiplies eligibility trace several steps large large. case equation would select small signiﬁcantly decreasing variance. approximate solution proposed optimization need approximate error variance terms equation estimate error need estimate expected return state estimate variance need obtain estimate ]−e. estie werr approach seem problematic sub-step appears solving problem originally aimed solve. however many meta-parameter optimization approaches approximation inaccurate still adequately guide selection discuss experimental results section. return however extensively studied. sobel provides bellman equation variance λ-return also extensive literature risk-averse learning variance return often used measure however explicit estimate variance return given. also work estimating variance value function general though related diﬀerent estimating variance λ-return. next section provide derivation algorithm called variance temporal diﬀerence learning approximate second moment return statebased general updates given section λ-greedy estimate variance complete algorithm summarized algorithm simple meta-parameter settings additional parameters introduced. stepsize used main weights update werr wsq. addition weights werr reﬂect priori estimates error variance. reasonable rule-of-thumb werr larger reﬂect initial value estimates inaccurate. results naturally investigate second moment return guaranteed ﬁnite. condition facilitate identifying theoretical conditions target behavior policies enable ﬁnite variance return. theoretical characterization outside scope work reason diﬀerent settings provide well-deﬁned ﬁnite ﬁxed point. first clearly setting every state ensures ﬁnite second moment given ﬁnite regardless policy mis-match. on-policy setting standard assumptions oﬀ-policy setting ¯γt+ similarly case. otherwise solution still exist ensuring maximum singular value ¯pπγ less one; hypothesize property unlikely large mis-match target behavior policy causing many large important future avenue understand required similarity enable ﬁnite variance return given interestingly λ-greedy algorithm adapt inﬁnite variance settings derivation projected bellman error objective ¯λ-squaredreturn derive optimize objective. given deﬁnition generalized bellman operator derivation parallels ﬁrst moment main diﬀerence obtaining unbiased estimates parts objective; therefore focus results novel aspect summarized theorems corollary. section derive general algorithm approximate second moment λ-return. though algorithm nonetheless provide general algorithm model-free variance estimation approach general λ-returns. novelty determining bellman operator squared return deﬁnes ﬁxed-point objective called var-mspbe. bellman operator recursive form squared return derive gradient algorithm called estimating second moment. avoid confusion parameters main algorithm general rule throughout document additional parameters used estimate second moment bar. example discount main problem ¯γt+ discount second moment. bellman operator squared return estimating deﬁne generalized bellman operator squared-return using recursive form. goal obtain ﬁxed point ¯t¯v ﬁxed point exists operator contraction. ﬁrst moment bellman operator known contraction result however immediately extend because thought ¯rt+ valid ﬁnite reward ¯γt+ satisfy nonetheless deﬁne bellman operator ¯λ-squared-return determine ﬁxed point exists. interestingly ¯γt+ fact larger still obtain contraction. deﬁne bellman operator recent generalization enables discount deﬁned function rather function ﬁrst deﬁne expected ¯λ-squaredreturn ﬁrst assume underlying cycles third makes estimate transition probabilities thus interest tabular domains. singh dayan provided analytical expression bias variance given model. suggest largest feasible step-size bias converges zero variance converges non-zero value bias and/or variance diverge. downey sanner used bayesian variant learning requiring batch samples oﬀ-line computation provide empirical demonstration optimally setting obtaining samples. kearns singh compute bias-variance error bound modiﬁcation called phased discrete phase algorithm given trajectories state. trajectories state eﬀective learning rate removing complexities sample averaging conventional online td-update. error bounds useful among things computing value phase outperforms ﬁxed value empirically demonstrating utility changing also signiﬁcant eﬀort theoretically characterizing notably work schapire warmuth contributed ﬁnite sample analysis incremental td-style algorithms. analyze variant called although still linear incremental computes value estimates quite diﬀerently. resulting ﬁnite sample bound particularly interesting rely model assumptions using access sequence feature vectors rewards returns. unfortunately bound cannot analytically minimized produce optimal value. simulate bound verifying intuition larger best linear predictor inaccurate small accurate intermediate value otherwise. later derived similar bounds another gradient descent algorithm called residual gradient. algorithm however utilize eligibility traces converges diﬀerent solution methods function approximation used another approach involves removing parameter altogether eﬀort improve robustness. konidaris introduced method called tdγ. work deﬁnes plausible assumptions implicitly made constructing λ-returns relaxes assumptions. derive exact algorithm longer depends choice performs well empirically variety policy learning benchmarks. incremental approximation also performs reasonably well appears somewhat sensitive choice meta parameter often requires large values obtain good performance. problematic complexity grows length trajectories—not linearly feature vector size. nonetheless constitutes reasonable reduce parameter sensitivity on-policy setting. garcia serre proposed variant q-learning optimal value computed online. analysis however restricted tabular case. finally mahmood introduced weighted importance sampling oﬀ-policy learning; though indirect strategy enabling larger selected without destabilizing oﬀ-policy learning. simpler assume access estimate ﬁrst moment λ-return. setting fact estimate simultaneously learn werr. include general expectation equivalence theorem proofs appendix. previous gradient algorithms learn auxiliary weights estimate part objective e−e. obtain estimate notice corresponds solution goal obtain estimates |xt]. therefore update investigated tuning proposed three algorithms gies adapting next section existing work empirical demonstration λ-greedy ﬁrst λ-adaptation algorithm oﬀ-policy incremental learning developed well-deﬁned greedy objective. investigate λ-greedy ring-world oﬀ-policy sampling. ring-world previously introduced suitable domain investigating varied length ring-world reward zero every state except adjoining states reward terminal states. agent teleported back middle ring-world upon termination. target policy take action right probability action left probability. feature representation tabular encoding states binary identity vectors also examine eﬀects aliasing state values simulate poor generalization common case true value function cannot represented. length experiment function compared ﬁxed values time-decay schedules worked well compared several tested settings. discount factor onpolicy chains oﬀ-policy chains. include optimal λ-greedy computes using closed form solutions deﬁned respective bellman operators. λ-greedy initialization werr rmax thumb initializing high lambdas opposite oﬀ-policy match rule-of-thumb caution oﬀpolicy domains. return value common choice optimistic initialization prevented inadvertent evaluator bias overly tuning parameter. ﬁxed learning-rate parameter λ-greedy equal equal used learning value function demonstrate performance less ideal settings. sweeping step-size λ-greedy would improve performance. performance results oﬀ-policy summarized figure report absolute value error compared true value function computable domain settings. average runs report results best parameter settings algorithms values general λ-greedy works well across settings. optimal λ-greedy consistently performs best indicating merit objective. estimating typically cause λ-greedy perform poorly indicating opportunity improve algorithms match performance optimal idealistic version. particular optimize meta-parameters λ-greedy. ﬁxed values decay schedules eﬀective speciﬁc instances perform well across problem settings. particular ﬁxed decay schedules settings appear un-robust increasing chain length ﬁxed robust change on-policy oﬀ-policy. converge zero over-time since value function approximated point bias introduced using variance reduced. algorithm converge state-based states) goals ensure well-deﬁned ﬁxed point. second aliased features expect ﬁnal per-state larger states aliased. intuition bootstrap values states introduces bias. demonstrate indeed occur figure expected λ-greedy robust state aliasing compared ﬁxed strategies. state aliasing provides concrete example less conﬁdence speciﬁc states eﬀective strategy mitigate situation high states. paper proposed ﬁrst linear-complexity adaptation algorithm incremental oﬀ-policy reinforcement learning. proposed objective greedily tradeoﬀ bias variance derived eﬃcient algorithm obtain solution objective. demonstrate efﬁcacy approach on-policy oﬀ-policy setting versus ﬁxed values time-decay heuristics. work opens many avenues eﬃciently selecting providing concrete greedy objective. important direction extend analysis recently introduced true-online traces focused well-understood λ-return proposed objective restricted setting. also numerous future directions improving optimization objective. used learn second moment λ-return inside λ-greedy. another possible direction simply explore using even least-squares methods improve estimation inside λ-greedy. also opportunities modify objective consider variance future diﬀerently. current objective highly cautious assumes modiﬁed assumes agent forced unbiased λt+i future time steps. consequence algorithm often prefer small future variance controlled setting λt+. natural relaxation strictly greedy setting recognize agent fact control future λt+i agent could remain cautious number steps assuming λt+i likely high horizon however assume steps λt+i reduced small value cutting traces mitigating variance. horizon could encoded aggressive discount; multiplying current discount ¯γt+ value less commonly multiplicative factor less indicates horizon gives horizon variance term objective could modiﬁed include horizon providing method incorporate level caution. overall work takes small step toward goal automatically selecting trace parameter thus step closer toward parameter-free black-box application reinforcement learning many avenues future research. acknowledgements would like thank david silver helpful discussions reviewers helpful comments. figure on-policy learning performance diﬀerent methods adapting three sizes ringworld domain. ﬁrst results plots mean squared value error verses time several approaches adapting optimal λ-greedy algorithm approximation algorithm several ﬁxed values ﬁxed decay schedules. second results plots values over-time used algorithm. highlight early learning taking axis algorithms able exploit variance learn quickly. oﬀ-policy learning variance importance sampling prefer long-term stability. therefore highlight later learning using x-axis. figure oﬀ-policy learning performance diﬀerent methods adapting diﬀerent conﬁgurations size ringworld domain. ﬁrst results plots mean squared value error verses time several approaches adapting oﬀ-policy learning variance importance sampling prefer long-term stability. therefore highlight later learning using x-axis. second results plots values over-time used algorithm. figure closer examination eﬀects state aliasing choice state-based length chain on-policy sampling. state three aliased last state. left graph shows learning curves steps right graph shows learned function state run. mahmood hasselt sutton. weighted importance sampling oﬀ-policy learning linear function approximation. advances neural information processing systems morimura sugiyama kashima hachiya tanaka. parametric return density estimation reinforcement learning. conference uncertainty artiﬁcial intelligence previous result requires ¯rt+ given algorithm step. result theorem requires estimate next theorem estimate provided. general trace functions estimate requires matrix stored updated; however addressed work corollary indicates simpliﬁes linear-space time algorithm. theorem enable recursive form need ensure ¯rt+ computable step given st+. characterize expectation ¯rt+ using current unbiased estimate obtain unbiased sample ¯rt+ step. )xt] remark derive e[(gλ provides update rule above without bootstrapping corresponds minimizing mean-squared error using samples returns. however previously observed found performed poorly using bootstrapping.", "year": 2016}