{"title": "On the Inductive Bias of Dropout", "tag": ["cs.LG", "cs.AI", "cs.NE", "math.ST", "stat.ML", "stat.TH"], "abstract": "Dropout is a simple but effective technique for learning in neural networks and other settings. A sound theoretical understanding of dropout is needed to determine when dropout should be applied and how to use it most effectively. In this paper we continue the exploration of dropout as a regularizer pioneered by Wager, et.al. We focus on linear classification where a convex proxy to the misclassification loss (i.e. the logistic loss used in logistic regression) is minimized. We show: (a) when the dropout-regularized criterion has a unique minimizer, (b) when the dropout-regularization penalty goes to infinity with the weights, and when it remains bounded, (c) that the dropout regularization can be non-monotonic as individual weights increase from 0, and (d) that the dropout regularization penalty may not be convex. This last point is particularly surprising because the combination of dropout regularization with any convex loss proxy is always a convex function.  In order to contrast dropout regularization with $L_2$ regularization, we formalize the notion of when different sources are more compatible with different regularizers. We then exhibit distributions that are provably more compatible with dropout regularization than $L_2$ regularization, and vice versa. These sources provide additional insight into how the inductive biases of dropout and $L_2$ regularization differ. We provide some similar results for $L_1$ regularization.", "text": "dropout simple effective technique learning neural networks settings. sound theoretical understanding dropout needed determine dropout applied effectively. paper continue exploration dropout regularizer pioneered wager et.al. focus linear classiﬁcation convex proxy misclassiﬁcation loss minimized. show order contrast dropout regularization regularization formalize notion different sources compatible different regularizers. exhibit distributions provably compatible dropout regularization regularization vice versa. sources provide additional insight inductive biases dropout regularization differ. provide similar results regularization. since prominent role imagenet large scale visual recognition challenge intense interest dropout deng dahl wager baldi sadowski erven paper studies inductive bias dropout chooses train dropout prior preference models results? show dropout training shapes learner’s search space much different regularization. results shed insight dropout prefers rare features dropout probability affects strength regularization dropout restricts co-adaptation weights. theoretical study concern learning linear classiﬁer convex optimization. learner wishes parameter vector that random feature-label pair drawn joint distribution probability sign small. using training data minimize loss function associated logistic regression. chosen focus problem several reasons. first inductive bias dropout well understood even simple setting. second linear classiﬁers remain popular choice practical problems especially case high-dimensional data. third view thorough understanding dropout setting mandatory prerequisite understanding inductive bias dropout applied deep learning architecture. especially true preference deep learning models decomposed preferences node. case setting studying faithfully describes inductive bias deep learning system output nodes. borrow following clean illuminating description dropout artiﬁcial noise wager algorithm linear classiﬁcation using loss dropout updates parameter vector online using stochastic gradient descent. given example dropout algorithm independently perturbs feature probability replaced probability replaced xi/p. equivalently replaced stochastic gradient descent known converge broad variety conditions thus abstract away sampling issues done breiman zhang bartlett long servedio consider dropout viewed stochastic gradient update global objective function. call objective dropout criterion viewed risk dropout-induced distribution. popular style learning algorithm minimizes objective function like regdq replaced norm motivation algorithms family ﬁrst replace training error convex proxy make optimization tractable regularize using convex penalty norm objective function remains convex. show regdq formalizes preference classiﬁers assign large weight single feature. preference stronger gets penalty proportional ||w||. fact show that despite convexity dropout risk regdq convex dropout provides realize inductive bias arising non-convex penalty still enjoying beneﬁt convexity overall objective function figure shows even surprising result dropout regularization penalty even monotonic absolute values individual weights. hard regdq thus regdq greater expected loss incurred might well inﬁnity dropout prefer however cases dropout never reaches extreme remains willing model even parameter large unlike methods convex penalty. particular shorthand refer algorithm minimizes note probability dropping input feature plays role dropout analogous particular goes zero examples remain unperturbed dropout regularization effect. informally joint probability distributions separate dropout parameters used using dropout leads much accurate hypothesis using leads much accurate hypothesis enables illustrate inductive biases algorithms contrasting sources either align incompatible algorithms’ inductive bias. comparing another regularizer helps restrict illustrative examples reasonable sources handled using another regularizer. ensuring values regularization parameter used controls amount regularization ensures difference model preferences respective regularizers. style analysis know useful tool studying inductive biases algorithms settings. related previous work. research builds work wager analyzed dropout random pairs distribution given comes member exponential family quality model evaluated using log-loss. pointed that cases dropout criterion decomposed original loss term depend therefore viewed regularizer. proposed approximation dropout regularizer discussed relationship regularizers training algorithms evaluated experimentally. baldi sadowski exposed properties dropout viewed ensemble method erven showed applying dropout online learning experts setting leads algorithms adapt important properties input without requiring doubling parameter-tuning techniques abernethy analyzed class methods including dropout viewing methods smoothers. impact dropout generalization studied wager latter paper considers variant dropout compatible poisson source shows assumptions dropout variant converges quickly inﬁnite sample limit non-dropout training bayes-optimal predictions preserved modiﬁed dropout distribution. results complement focusing effect original dropout algorithm’s bias. section deﬁnes notation characterizes dropout criterion unique minimizer. section presents many additional properties dropout regularizer. section formally deﬁnes distributions separate algorithms regularizers. sections give sources separate dropout section provides plots demonstrating distributions separated dropout regularization. sections give separation results many features. preliminaries optimizer dropout criterion probability feature dropped probability feature kept throughout paper. introduction joint distribution deﬁne ||w|| term implies always well-deﬁned. hand always well-deﬁned seen considering distribution concentrated single example. motivates following deﬁnition. deﬁnition joint distribution support contained rn×{− feature perfect modulo ties either support support although paper focuses logistic loss deﬁnitions used loss function since dropout criterion expectation following obvious consequence. proposition loss convex dropout criterion also convex function another perfect modulo ties linear classiﬁer pays attention feature perfect part nonzero. proposition ﬁnite domains distributions support e∼pr x))) unique minimum feature perfect modulo ties unique proof assume contradiction feature perfect modulo ties minimizer e∼pr x))). assume w.l.o.g. support increasing keeps loss unchanged examples decreases loss examples support contradicting assumption unique minimizer expected loss. suppose feature examples examples support since support ﬁnite positive lower bound probability example support. probability component random vector non-zero remaining components zero. therefore increases without bound positive negative direction e∼prx))) also increases without bound. since e∼prx))) value depending distribution dropout probability minimizing e∼pr x))) equivalent minimizing e∼pr x))) since full rank therefore e∼pr x))) strictly convex. since strictly convex function deﬁned compact unique minimum e∼pr x))) unique minimum therefore loss function generally logistic loss source distributions pairs varies section marginal distribution feature dropout probability probability keeping feature regularization parameter additive dropout noise {−xi xi/p multiplicative dropout noise component-wise product minimizer dropout criterion minimizer expected loss x))) minimizer l-regularized loss regularization dropout criteria optimized varies sub-section gradients current criterion classiﬁcation generalization error sign start rederiving regularization function corresponding dropout training previously presented wager specialized context using notation. ﬁrst step write alternative exposes symmetries approximation suggests properties strength regularization penalty decreases exponentially prediction conﬁdence regularization penalty goes inﬁnity dropout probability goes however quite large making second-order taylor expansion inaccurate. fact analysis section suggests regularization penalty decrease conﬁdence regularization penalty increases linearly vector regdq implying proposition regdq thus regularization penalty greater effectively equivalent regularization penalty present results based analyzing exact regdq. next properties show dropout regularizer emphatically like convex norm-based regularization penalties dropout regularization penalty always remains bounded single component weight vector goes inﬁnity theorem dropout probabilities marginal distributions n-feature vectors indices proposition shows regularization penalty starting non-zero initial weight vector remains bounded components goes inﬁnity. hand unless small bound larger dropout criterion zero vector. natural consequence starting weight vector could already large regularization penalty. derivative proof theorem implies dropout regularization penalty monotonic |wi| weights zero. surprisingly hold general. dropout regularization penalty single example written therefore increasing weight makes second logarithm increase faster expectation ﬁrst regularization penalty decreases even weight increases. happens wixi products tend sign. regularization penalty function single example various values plotted figure gives following. proposition unlike p-norm regularizers dropout regularization penalty regdq always monotonic individual weights. proposition proved appendix turn dropout regularization’s behavior weights vary together. features always zero weights without affecting either predictions regdq. linearly dependent features might well feature. ruling degeneracies like these arrive following theorem proved appendix theorem arbitrary distribution support weight vector non-dropout probability positive probability non-zero different signs regularization penalty regdq goes inﬁnity goes corollary distribution support positive probability regularization penalty regdq goes inﬁnity components nonzero distribution bounded support together theorems demonstrate regdq convex fact regdq cannot approximated within factor convex function even dependence allowed. example theorem shows that bounded support regdq regdq remain bounded goes inﬁnity whereas theorem shows regdq unbounded goes inﬁnity. theorem relies wixi products different signs. following shows regdq remain bounded multiple components inﬁnity corresponding features compatible sense signs wixi always alignment. theorem weight vector discrete distribution wixi index support limit regdq goes inﬁnity bounded lnpx∼d= bounds preceding theorems propositions suggest several properties dropout regularizer. first factors indicate strength regularization grows linearly dropout probability second px∼d= factors several bounds suggest weights rare features encouraged penalized less strongly weights frequent features. preference rare features sometimes seen algorithms like second-order perceptron adagrad wager discussed relationship dropout algorithms based approximation empirical results indicate dropout performs well domains like document classiﬁcation rare features high discriminative value theorems section suggest exact dropout regularizer minimally penalizes rare features. finally theorem suggests dropout limits co-adaptation strongly penalizing large weights wixi products often different signs. hand wixi products usually sign proposition indicates dropout encourages increasing smaller weights help share prediction responsibility. intuition reinforced figure dropout penalty large weights much less single large weight features highly correlated. regularizer regularization parameter governs strongly regularizes. want describe qualitatively preferred regularizer another need control amount regularization. pr∼p recall minimizers dropout sources c-separate dropout exist distribution weight vectors classify examples perfectly distribution optimizing l-regularized criterion leads perfect hypothesis weight vectors optimizing dropout criterion make prediction errors one-third distribution. intuition behind behavior distribution described weight vectors positive multiples classify data correctly. however dropout regularization data points encourage second weight negative ﬁrst component dropped out. negative push second weight strong enough prevent minimizer dropoutregularized criterion correctly classifying data point. figure illustrates loss dropout regularization dropout criterion data source. figure using data favoring expected loss plotted upper-left dropout regularizer upper-right regularized criterion lower-left dropout criterion lower-right functions weight vector. bayes-optimal weight vectors green region marks show optimizers criteria. intuition behind distribution data point encourages large weight ﬁrst feature. means negative pressure second weight data point much smaller positive pressure second weight example. regularized criterion emphasizes short vectors prevents ﬁrst weight growing large enough correctly classify data point. hand ﬁrst feature nearly perfect; wrong sign second example means that light theorem proposition dropout much willing large weight giving advantage source plots figure illustrate intuition. theorem erp) distribution deﬁned results previous section show distributions deﬁned strongly separate dropout regularization. theorem shows distribution analyzed section theorem shows distribution whenever contrast distribution deﬁned previous section theorem shows erq) whenever distribution theorem shows erq) whenever sections figure plots criterion distributions deﬁned deﬁned like regularization regularization produces bayes-opitmal classiﬁer therefore argument shows distributions also strongly separate dropout regularization. figure source favoring dropout expected loss plotted upper-left dropout regularizer upper-right expected loss plus regularization lower-left dropout criterion lower-right functions weight vector. bayes-optimal weight vectors green region marks show optimizers criteria. note minimizer dropout criterion lies outside middle-right plot shown bottom plot figure plot criterion distributions deﬁned section deﬁned section before bayes optimal classiﬁers denoted region shaded green minimizer criterion denoted consider source deﬁned follows. number features even. examples labeled random example drawn follows ﬁrst feature takes value probability otherwise subset exactly remaining features takes value remaining ﬁrst features take value majority vote last features achieves perfect prediction accuracy. despite ﬁrst feature strongly correlated label voters optimal ensemble. dropout bias single good features discrimination multiple disagreeing features puts much weight ﬁrst feature. contrast regularization leads bayes optimal classiﬁer placing less weight ﬁrst feature others. theorem accuracy erp) deﬁne source depends positive real parameters follows. random label generated ﬁrst equally likely. features conditionally independent given ﬁrst feature tends accurate small probability probability remaining features larger less accurate feature probability otherwise. small enough relative bayes’ optimal prediction predict ﬁrst feature. small requires concentrating weight outvote features. dropout capable making weight large regularization not. theorem free parameter. theorem shows erpη therefore goes distribution deﬁned start section call here provides contrasting behavior theorem shows error erq) theorem shows therefore distributions strongly separate dropout regularization parameters built interpretation dropout regularizer wager prove several interesting properties dropout regularizer. interpretation decomposes dropout criterion minimized training loss term plus regularization penalty depends feature vectors training started characterization dropout criterion unique minimum turn properties dropout regularization penalty. veriﬁed dropout regularization penalty desirable properties regularizer zero vector contribution feature vector training non-negative. cases dropout regularization penalty diverge multiple weights inﬁnity. characterize sending weights inﬁnity causes dropout regularization penalty diverge remain ﬁnite. particular dropout willing large weights multiple features wixi products tend sign. form analytical bounds suggest strength regularizer grows linearly dropout probability provide additional support claim dropout favors rare features. found important check intuition working small examples. make rigorous needed deﬁnition source favored dropout regularization standard regularizer like deﬁnition needs deal strength regularization difﬁculty complicated fact dropout regularization parameterized dropout probability regularization parameterized solution consider pairs sources pair separates dropout dropout particular parameter performs better particular parameter source performs better dropout source deﬁnition uses generalization error natural interpretation performs better. sections devoted proving dropout strongly separated certain pairs distributions. section shows dropout regularization also strongly separated. proving strong separation non-trivial even ﬁnds right distributions. several factors minimizers criteria closed forms wish prove separation ranges regularization values binomial distributions induced dropout amenable exact analysis. despite difﬁculties separation results reinforce intuition dropout willing large weight order better training data regularization. however features often different signs dropout less willing even moderate weight features. side beneﬁt analyses plots figure figure provide dramatic illustration dropout regularizer’s non-convexity preference making single weight large. consistent insight provided theorems analysis logistic regression case corresponding single output node. would interesting similar analysis multi-layer neural networks. however dealing non-convex loss networks major challenge. another open problem suggested work deﬁnition separation used gain insight regularizers settings.", "year": 2014}