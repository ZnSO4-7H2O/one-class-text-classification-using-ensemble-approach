{"title": "Nonparametric Basis Pursuit via Sparse Kernel-based Learning", "tag": ["cs.LG", "cs.CV", "cs.IT", "math.IT", "stat.ML"], "abstract": "Signal processing tasks as fundamental as sampling, reconstruction, minimum mean-square error interpolation and prediction can be viewed under the prism of reproducing kernel Hilbert spaces. Endowing this vantage point with contemporary advances in sparsity-aware modeling and processing, promotes the nonparametric basis pursuit advocated in this paper as the overarching framework for the confluence of kernel-based learning (KBL) approaches leveraging sparse linear regression, nuclear-norm regularization, and dictionary learning. The novel sparse KBL toolbox goes beyond translating sparse parametric approaches to their nonparametric counterparts, to incorporate new possibilities such as multi-kernel selection and matrix smoothing. The impact of sparse KBL to signal processing applications is illustrated through test cases from cognitive radio sensing, microarray data imputation, and network traffic prediction.", "text": "abstract—signal processing tasks fundamental sampling reconstruction minimum mean-square error interpolation prediction viewed prism reproducing kernel hilbert spaces. endowing vantage point contemporary advances sparsity-aware modeling processing promotes nonparametric basis pursuit advocated paper overarching framework conﬂuence kernel-based learning approaches leveraging sparse linear regression nuclear-norm regularization dictionary learning. novel sparse toolbox goes beyond translating sparse parametric approaches nonparametric counterparts incorporate possibilities multi-kernel selection matrix smoothing. impact sparse signal processing applications illustrated test cases cognitive radio sensing microarray data imputation network trafﬁc prediction. reproducing kernel hilbert spaces provide orderly analytical framework nonparametric regression optimal kernel-based function estimate emerging solution regularized variational problem pivotal role rkhs appreciated connections workhorse signal processing tasks nyquistshannon sampling reconstruction result involves sinc kernels alternatively spline kernels replace sinc kernels smoothness rather bandlimitedness present underlying function space kernel-based function estimation also seen bayesian viewpoint. rkhs linear minimum meansquare error function estimators coincide pertinent covariance matrix equals kernel gram matrix. equivalence leveraged context ﬁeld estimation spatial lmmse estimation referred kriging tantamount two-dimensional rkhs interpolation finally rkhs based function estimators linked gaussian processes obtained upon deﬁning covariances kernels another seemingly unrelated increasingly popular theme contemporary statistical learning signal processing matrix completion data organized matrix missing entries e.g. limitations acquisition process. article builds assertion imputing missing entries amounts interpolation classical sampling theory low-rank constraint replacing bandlimitedness. point view rkhs interpolation emerges prudent framework matrix completion allows effective incorporation priori information kernels including sparsity attributes. recent advances sparse signal recovery regression motivate sparse kernel-based learning redux purpose core present paper. building blocks sparse signal processing include least-absolute shrinkage selection operator weighted versions compressive sampling nuclear norm regularization common denominator behind operators sparsity signal’s support ℓnorm regularizer induces. exploiting sparsity leads several innovations regarding selection multiple kernels additive modeling collaborative ﬁltering matrix tensor completion dictionary learning well nonparametric basis selection context main contribution paper nonparametric basis pursuit tool unifying advancing number sparse approaches. constrained space limitations sample applications stemming encompassing analytical tool also delineated. sparse various forms contribute computer vision cognitive radio sensing management user preferences bioinformatics econometrics forecasting electric prices load renewables name few. remainder paper organized follows. section reviews theory rkhs connection describing representer theorem kernel trick presenting nyquist-shannon theorem example kbl. section deals sparse including sparse additive models multiple kernel learning examples additive nonparametric models. introduced section basis expansion model capturing general framework sparse kbl. blind versions matrix completion dictionary learning developed sections finally section presents numerical tests using real simulated data including spectrum measurements expression levels yeast network trafﬁc loads. conclusions drawn section viii technical details deferred appendix. instead deterministic treatment previous subsection unknown considered random process. estimate offered representer theorem linked lmmse-based estimator random ﬁelds term kriging predict value exploration point kriging predictor modeled linear combination noisy samples measurement points {xn}n expansion coefﬁcients collects data. criterion adopted optimal minβ solving latter yields zero-mean white noise power expressed terms unobserved hence lmmse estimate takes form linearity assumption unnecessary modeled zero-mean instances ﬁeld arbitrary points jointly gaussian. zero-mean speciﬁed determines covariance matrix vector comprising instances ﬁeld thus speciﬁc zero-mean gaussian distribution. particular vector collecting ﬁeld exploration measurement points gaussian vector hence mmse estimator given expectation conditioned reduces analogous spectral decomposition matrices mercer’s theorem establishes symmetric positive definite kernel square-integrable admits possibly inﬁnite λieiei δi−i′ stands context reproducing kernel hilbert spaces nonparametric estimation function deﬁned measurable space performed interpolation training points purpose kernel function selected symmetric positive deﬁnite speciﬁes linear space interpolating functions given many choices exhaustive respect families functions obeying certain regularity conditions. spline kernel example generates sobolev space low-curvature functions likewise sinc kernel gives rise space bandlimited functions. space becomes hilbert space equipped inner product associated norm kfkhx result context so-termed representer theorem optimal interpolator asserts based sense result nice simplicity since functions space compound numerable arbitrarily large number kernels combination ﬁnite number kernels around training points. addition regularizing term µkfk controls smoothness thus reduces overﬁtting. substituting coefﬁcients minimizing regularized least-squares cost given upon recognizing deﬁning well kernel dependent gram matrix entries knn′ remark ﬁnite-dimensional expansion solves general ﬁtting costs regularizing terms. general form representer theorem asserts solution loss function replacing cost selected serve either robustness application dependent objectives accommodating non-gaussian noise models viewing bayesian angle. hand regularization term chosen increasing function norm kfkhx turn crucial introducing notion sparsity described ensuing sections. kernel trick allows approaches depend inner products functions recast implemented using ﬁnite dimensional covariance matrices. simple demonstration valuable property provided kernel-based ridge regression. starting standard ridge estimator µkβk possible rewrite solve minβ∈rd kz−φt βk+µkβk −φz. obtained training phase used preβ given using diction ensuing matrix inversion lemma written states sinc qualiﬁes kernel characterizes eigenfunctions used kernel trick shows rkhs norm restriction norm establish space bandlimited functions indeed rkhs. thus decomposed numerable combination eigenfunctions coefﬁcients eigenfunctions obey nst. consequently existence eigenfunctions {φn} spanning direct consequence rkhs require unless explicit form desired. finally strict adherence requires inﬁnite number samples reconstruct alternatively representer theorem ﬁnite samples regularizing power expressing linear predictor terms inner products instrumental mapping kernel-based version. although mapping entails eigenfunctions {φi} explicitly present given solely terms crucial since inﬁnite dimensional would render method computationally intractable importantly explicit form available. kernel trick demonstrated context ridge regression. however trick used vectorial regression classiﬁcation method whose result expressed terms inner products only. example offered support vector machines kernel-based version optimal linear classiﬁer sense minimizing vapnik’s ǫ-insensitive hinge loss function shown equivalent lasso kernels clearly viewed interpolating bases viewpoint appreciated considers family bandlimited functions e−iωxdx ∀|ω| denotes class square-integrable functions deﬁned family constitutes linear space. moreover generated linear combination sinc functions; account sparse methods begins spams approaches. model function learned sparse nonparametric components rely group lasso additive models considered section naturally lend general model introduced section used henceforth. additive function models offer generalization linear regression nonparametric setup premise dealing curse dimensionality inherent learning high dimensional data consider learning multivariate function deﬁned cartesian product measurable spaces denote point kernel deﬁned associated rkhs. although interpolated data substituting ﬁdelity severely degraded high dimensions. indeed accuracy depends availability nearby points function data proximity points high dimensions challenged curse dimensionality demanding excessively large dataset. instance consider positioning datapoints randomly hypercube repeatedly growing unbounded constant. limp minn=n′ ekxn xn′k expected distance points equal side hypercube holds kuik hence follows thus sparsifying effect additive model revealed. selected large enough optimal sub-vectors null corresponding ˆγnik identically zero thus estimation provides nonparametric counterpart lasso offering ﬂexibility selecting informative component-function regressors additive model. separable structure postulated facilitates subset selection nonparametric setup mitigates problem interpolating scattered data high dimensions. however model reduction render inaccurate case extra components depending variables added turning anova model specifying kernel shapes thus judiciously determines prerequisite kbl. different candidate kernels would produce different function estimates. convex combinations also employed aiki conserve deﬁning properties kernels. data-driven strategy select best resembles differing components depend variable taking account difference reducible thus solvable admom substituting hand general case presented convex hull inﬁnite possibly uncountable family kernels. additive form also amenable subsect selection yields spam. sparse linear regression spams involve functions expressed using entries learned using variational version lasso given denoting entry representer theorem applied component yielding γniki scalar coefﬁcients {γni fact yields spam demonstrated substituting expansions back solving obtain problem constitutes weighted version group lasso formulation sparse linear regression. solution found either block coordinate descent applying alternatingsubstituting direction method multipliers convergence guaranteed convexity separable structure non-differentiable term case group lasso regularizes sub-vectors separately effecting group-sparsity estimates; vectors identically zero. gain intuition this rewritten using change variables tiui kuik argued exceeds threshold optimal thus null. focusing minimization w.r.t. particular sub-vector algorithm substitute variables minimize argued section group lasso property effects group-sparsity subvectors {γi}p inherited capability selecting bases nonparametric setup. indeed zeroing corresponding γink driven zero correspondingly drops expansion remark single kernel associated rkhs used components since summands differentiated bases. speciﬁcally common different coefﬁcient yields distinct diagonal matrix diag deﬁning individual renders vector identiﬁable. particular characteristic contrast lemma designed require multiple kernels. remark different sparse kernel-based approaches presented namely spams viewed competing rather complementary choices. multiple kernels used basis pursuit separable model high dimensions. nbpmkl hybrid applied spectrum cartography illustrates point section bases utilized frequency domain kernel-based matrix completion scheme developed section using blind version bases {bi} prescribed learned together coefﬁcient functions {ci}. matrix completion task entails imputation missing entries data matrix rm×n entries index matrix }m×n specify whether datum available missing rank popular attribute relates missing available data thus granting feasibility imputation task. low-rank matrix imputation achieved solving stands hadamard product. low-rank constraint corresponds upperbound number nonzero singular values matrix given ℓ-norm. speciﬁcally denotes vector singular values cardinality |{si min{m n}}| deﬁnes ℓnorm ball radius namely replace rank feasible convex proper norm example applied wireless communications offered section different kernels employed estimating path-loss shadowing propagation effects cognitive radio sensing paradigm. viewed bases expansion coefﬁcient functions. given ﬁnite number training data learning sparsity constraints constitutes goal approaches developed following sections. ﬁrst method sparse related nonparametric counterpart basis pursuit goal ﬁtting function data {bi} prescribed {ci}s learned. designer’s degree conﬁdence modeling assumptions deciding whether {bi}s prescribed learned data. prescribed {bi}s unreliable model inaccurate performance suffer. neglecting prior knowledge conveyed {bi}s also damaging. parametric basis pursuit hints toward addressing tradeoff offering compromising alternative. certainly leveraging overcomplete bases {bi} accommodate uncertainty. practical merits basis pursuit however hinge capability learn {bi}s best explain given data. crux hand basis expansion domain learn dependence nonparametric means. model comes handy generally overcomplete purpose {bi}p collection prescribed bases. need estimated kernel-based strategy adopted end. accordingly optimal function searched cibi} constitutes feasible nbp-tailored nonparametric lasso rkhs functions associated reduce problems equivalent upon identifying cmi. according lemma intricacy rewriting introduce beneﬁt kernel selected kronecker delta. argued next equivalence estimators generalizes nicely matrix completion problem sparse missing data arbitrary kernels. recast bayesian framework suppose available entries obey additive white gaussian noise model entries independent identically distributed according zero-mean gaussian distribution matrix factorized without loss generality then gaussian prior assumed columns respectively independent across trace trace. invariance across justiﬁable since columns priori interchangeable trace trace introduced w.l.o.g. remove scalar ambiguity solving requires combinatorial search nonzero entries convex relaxation thus well motivated. ℓ-norm surrogated ℓ-norm corresponding ball becomes convex hull original feasible set. singular values non-negative deﬁnition since singular values equals dual norm ℓ-norm deﬁnes norm matrix itself namely nuclear norm denoted kak∗. upon substituting kak∗ rank transformed lagrangian form placing constraint objective regularization term i.e. next step towards kernel-based matrix completion relies alternative deﬁnition kak∗. consider bilinear factorizations matrix rm×p constraint rank implicit. nuclear norm redeﬁned matrix completion factorized form reformulated terms rkhss. following deﬁne spaces associated kernels respectively. represent entry approximant matrix prescribed overestimate rank. consider estimating family cibi coincides estimator solving coefﬁcients kernel-based matrix completion provided covariance gram matrices coincide. bayesian perspective matrix completion method provides generalization accommodate priori knowledge form correlation across rows columns incomplete prescribed correlation matrices even perform smoothing prediction. indeed column completely missing still estimate relying covariance missing available columns. feature available since latter relies rank-induced colinearities cannot reconstruct missing column. prediction capability useful instance collaborative ﬁltering group users rates collection items enable inference newuser preferences items entering system. additionally bayesian reformulation provides explicit interpretation regularization parameter variance model error thus obtained training data. kernel-based matrix completion method summarized algorithm solves upon identifying solves changing variables lines algorithm detailed derivations updates algorithm provided appendix. high-level description columns updated cyclically solving iterations. procedure converges stationary point principle guarantee global optimality. opportunely established local minima global minima transforming convex problem change variables proposed analysis observation implies algorithm yields global optimum thus kernel-based matrix completion method offers alternative low-rank constraint introduced indirectly kernel trick. furthermore bypassing nuclear norm using instead renders generalizable basis pursuit approaches advocate overcomplete bases cope model uncertainty thus learning data concise subset bases represents signal interest. extensive candidate bases still needs prescribed. next step towards model-agnostic learn dictionary data along sparse regression coefﬁcients. sparse linear model dictionary bases vector coefﬁcients goal dictionary learning obtain data swift count equations unknowns yields scalar variables learned data goal plausible overcomplete design unless exploited. proper conditions sparsity {γm}m possible recover sparse containing nonzero entries reduced number equations proportionality constant. hence number equations needed specify reduces represented darkened region fig. possible crucial collect sufﬁciently large number data vectors order ensure thus accommodating additional equations needed determine enable learning dictionary. collected sufﬁcient training data possible approach data cost regularized ℓ-norm order cbtk effect sparsity coefﬁcients dictionary leaning approach recast form blind ini= kcik |ci|. regularizer functions depends values measurement points only absorbed loss part thus optimal {ci} {bi} conserve ﬁnite expansion representations dictated representer theorem. coefﬁcients {γmp βnp} must adapted according cost frequency sub-bands occupied. wireless propagation simulated according pathloss model affected shadowing described parameters awgn variance −db. fig. depicts distribution power across space generated sources transmitting bands center frequencies respectively. fig. shows seen representative radio located center remark kernel-based dictionary learning inherits attractive properties kernel matrix completion blind namely ﬂexibility introduce priori information well capability cope missing data. estimate bases {bi} coefﬁcients {ci} jointly difference lies size dictionary. principal component analysis presumes low-rank model approximant compressing signals {zm} components rank required dictionary learning approach signals {zm} spanned dictionary atoms {bi} provided composed atoms only. model adopted collaborative sensing representing spatial frequency variables respectively. bases {bi} prescribed hann-windowed pulses accordance distribution power across space sub-band given {ci} interpolating measurements obtained radios exponential kernels selected convex combinations considered candidate interpolators strategy intended capturing different levels resolution produced pathloss shadowing. correspondingly decomposed functions regularized separately solving generates maps fig. solution take nonzero values correctly reveals frequency bands occupied shown fig. estimated across space depicted fig. band respectively compared ground truth depicted fig. multi-resolution components depicted fig. demonstrating kernel captures coarse pathloss distribution reﬁnes revealing locations affected shadowing. results demonstrate usefulness model collaborative spectrum sensing bases abiding multi-resolution kernels. sparse nonparametric estimator serves purpose revealing occupied frequency bands capturing across space source. compared spline-based approach adaptation provides appropriate multiresolution capability capture pathloss shadowing effects interpolating data across space. bayesian interpretation brings close bernoulli-gaussian model accounts sparsity beta distribution introduced learning distribution hyperparameters. although assumes independent gaussian variables across time samples underlying model generalization correlated variables straightforward. bernoulli parameters controlling sparsity assumed invariant across amounts stationarity cmp. although dictionary learning indeed viewed blind counterpart compressive sampling capability recovering data typically illustrated examples rather theoretical guarantees. recent efforts establishing identiﬁability local optimality dictionary learning found related strategy proposed data dictionary atoms organized classes regularized learning criterion designed promote cohesion atoms within class. consider setup radios distributed area measure ambient power spectral density frequencies equally spaced band speciﬁed ieee wireless standard radios collaborate sharing ncnf measurements goal obtaining across space frequency specifying time appropriate estimating alternatively sample estimate formed microarray data genes aside used place solving available data shown fig. results matrix depicted fig. imputed missing data introduce average recovery error producing smoothing capability recover completely missing rows corroborated. missing rows cannot recovered nuclear norm regularization alone even padded expression levels discarded genes. fig. presents case conﬁrming performance dagrades w.r.t. nbp; fig. illustrates sensitivity estimation error cross-validated regularization parameter estimators. similar degraded results observed imputing missing entries using impute.knn svdimpute methods implemented packages pcamethods bioconductor-impute. methods applied padded requisite discarding missing rows resulting recovery errors remaining missing entries −.db −.db respectively. imputation method tested microarray data described expression levels yeast across genes sampled time points cell cycle considered. subset genes extracted expression levels organized matrix rm×n depicted fig. severe data losses simulated discarding entries including nearly actually missing data. study effect hydrogen peroxide cell cycle arrest extra microarray datasets rm×n synchronized collected matrices employed form estimate used instead neglecting noise term since presence hydrogen peroxide samples induces cell cycle arrest correlation samples across time altered thus samples abilene network fig. a.k.a. internet comprising nodes links utilized testbed trafﬁc load prediction. aggregate link loads recorded every minute intervals morning december collected ﬁrst columns matrix rm×n samples used predict link loads hours ahead capitalizing mutual cross-correlation periodic correlation across days interdependence across links dictated network topology. contributes efforts advancing cornerstone sparse including blind versions emerge nonparametric nuclear norm regularization dictionary learning. connected analysis promoting bayesian viewpoint kernels convey prior information. alternatively regarded interpolation toolset though connection suggesting impact prior model choice attenuated size dataset large especially kernel selection also incorporated. proof white noise ideal low-pass ﬁlter cutoff frequency ωmax sinc autocorrelation output hence equals covariance matrix proof rewrite kernel sinc function parameterized then applied correlation matrix represented fig. estimated training samples collected previous weeks december december substituted according singular point trafﬁc curve depicted black fig. reﬂected sharp transition noticed fig. hand estimated derived network structure. supposing i.i.d. ﬂows across network holds represents network routing matrix adjusted satisfy used instead tr). fig. shows link loads predicted december representative link along actually recorded samples day. prediction accuracy compared fig. base strategy comprising independent lmmse estimators link yield relative prediction error aggregated across links results strong correlation among samples renders lmmse prediction accurate interval relying single-link data only. beneﬁt considering links jointly appreciated subsequent interval trafﬁc correlation morning samples fades away network structure comes valuable information form stabilize prediction. methodology outlined paper cross fertilizing sparsity-aware signal processing tools kernelbased learning. goes well beyond translating sparse vector regression techniques nonparametric counterparts generate series unique possibilities kernel selection kernel-based matrix completion. present article kekatos veeramachaneni light giannakis dayahead electricity market forecasting using kernels proc. ieee-pes innovative smart grid technologies washington feb. koltchinskii yuan sparsity multiple kernel learning ravikumar lafferty wasserman sparse additive models roy. stat. soc. vol. oct. shapira segal botstein disruption yeast forkheadassociated cell cycle transcription oxidative stress molecular biology cell vol. dec. shrivastava nguyen patel chellappa design non-linear discriminative dictionaries image classiﬁcation proc. asian conf. computer vision daejeon korea sindhwani lozano non-parametric group orthogonal matching pursuit sparse learning multiple kernels advances neural information processing systems granada spain deﬁning ¯cibt discarding regularization terms depending denote vector operator concatenates columns diag diagonal matrix operator hadamard product bypassed deﬁning diag] substituting kxkf kveck using following identities available http//internet.edu/observatory/archive/data-collections.html. ieee standard info. tech.-telecomms. info. exchchange systems-local metropolitan area nets. part wir. speciﬁcations ieee standard mar. abernethy bach evgeniou j.-p. vert approach collaborative ﬁltering operator estimation spectral regularization machine learning res. vol. mar.", "year": 2013}