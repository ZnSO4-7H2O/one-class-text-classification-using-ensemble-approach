{"title": "Multi-criteria Anomaly Detection using Pareto Depth Analysis", "tag": ["cs.LG", "cs.CV", "cs.DB", "stat.ML", "I.5; G.3; H.2.8"], "abstract": "We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria using a linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria.", "text": "consider problem identifying patterns data exhibit anomalous behavior often referred anomaly detection. anomaly detection algorithms dissimilarity data samples calculated single criterion euclidean distance. however many cases exist single dissimilarity measure captures possible anomalous patterns. case multiple criteria deﬁned test anomalies scalarizing multiple criteria using linear combination them. importance different criteria known advance algorithm need executed multiple times different choices weights linear combination. paper introduce novel non-parametric multi-criteria anomaly detection method using pareto depth analysis uses concept pareto optimality detect anomalies multiple criteria without algorithm multiple times different choices weights. proposed approach scales linearly number criteria provably better linear combinations criteria. anomaly detection important problem studied variety areas used diverse applications including intrusion detection fraud detection image processing many methods anomaly detection developed using parametric non-parametric approaches. non-parametric approaches typically involve calculation dissimilarities data samples. complex high-dimensional data multiple dissimilarity measures corresponding different criteria required detect certain types anomalies. example consider problem detecting anomalous object trajectories video sequences. multiple criteria dissimilarity object speeds trajectory shapes used detect greater range anomalies single criterion. order perform anomaly detection using multiple criteria could ﬁrst combine dissimilarities using linear combination. however many applications importance criteria known advance. difﬁcult determine much weight assign dissimilarity measure choose multiple weights using example grid search. furthermore weights changed anomaly detection algorithm needs re-executed using weights. paper propose novel non-parametric multi-criteria anomaly detection approach using pareto depth analysis uses concept pareto optimality detect anomalies without choose weights different criteria. pareto optimality typical method deﬁning optimality multiple conﬂicting criteria comparing items. item said pareto-optimal exist another item better equal criteria. item pareto-optimal optimal usual sense combination necessarily linear criteria. hence able detect anomalies multiple combinations criteria without explicitly forming combinations. figure left illustrative example training samples test samples center dyads training samples along ﬁrst pareto fronts criteria |∆x| |∆y|. pareto fronts induce partial ordering dyads. dyads associated test sample marked circle concentrate around shallow fronts right dyads associated test sample marked triangle concentrate around deep fronts. approach involves creating dyads corresponding dissimilarities pairs data samples dissimilarity measures. sets pareto-optimal dyads called pareto fronts computed. ﬁrst pareto front non-dominated dyads. second pareto front obtained removing non-dominated dyads i.e. peeling ﬁrst front recomputing ﬁrst pareto front remaining. process continues dyads remain. dyad assigned pareto front depth nominal anomalous samples located near different pareto front depths; thus computing front depths dyads corresponding test sample discriminate nominal anomalous samples. proposed approach scales linearly number criteria signiﬁcant improvement compared selecting multiple weights grid search scales exponentially number criteria. assumptions multi-criteria dyads modeled realizations smooth k-dimensional density provide mathematical analysis behavior ﬁrst pareto front. analysis shows precise sense outperform test uses linear combination criteria. furthermore theoretical prediction experimentally validated comparing several state-of-the-art anomaly detection algorithms experiments involving synthetic real data sets. rest paper organized follows. discuss related work section section provide introduction pareto fronts present theoretical analysis properties ﬁrst pareto front. section relates pareto fronts multi-criteria anomaly detection problem leads anomaly detection algorithm. finally present experiments section evaluate performance pda. several machine learning methods utilizing pareto optimality previously proposed; overview found methods typically formulate machine learning problems multi-objective optimization problems ﬁnding even ﬁrst pareto front quite difﬁcult. methods differ pareto optimality consider multiple pareto fronts created ﬁnite items need employ sophisticated methods order fronts. hero fleury introduced method gene ranking using pareto fronts related approach. method ranks genes order interest biologist creating pareto fronts data samples i.e. genes. paper consider pareto fronts dyads correspond dissimilarities pairs data samples rather samples themselves distribution dyads pareto fronts perform multi-criteria anomaly detection rather ranking. another related area multi-view learning involves learning data represented multiple sets features commonly referred views. case training view helps improve learning another view. problem view disagreement samples take different classes different views recently investigated views similar criteria problem setting. however setting different criteria orthogonal could even give contradictory information; hence severe view disagreement. thus training view could actually worsen performance another view problem consider differs multi-view learning. similar area multiple kernel learning typically applied supervised learning problems unlike unsupervised anomaly detection setting consider. finally many anomaly detection methods previously proposed. hodge austin chandola provide extensive surveys different anomaly detection methods applications. nearest neighbor-based methods closely related proposed approach. byers raftery proposed distance sample kth-nearest neighbor anomaly score sample; similarly angiulli pizzuti eskin proposed distances sample nearest neighbors. breunig used anomaly score based local density nearest neighbors sample. hero sricharan hero introduced non-parametric adaptive anomaly detection methods using geometric entropy minimization based random k-point minimal spanning trees bipartite k-nearest neighbor graphs respectively. zhao saligrama proposed anomaly detection algorithm k-lpe using local p-value estimation based k-nn graph. k-nn anomaly detection schemes depend data pairs data points deﬁne edges k-nn graphs. aforementioned methods designed single-criteria anomaly detection. multicriteria setting single-criteria algorithms must executed multiple times different weights unlike anomaly detection algorithm propose section method proposed paper utilizes notion pareto optimality studied many application areas economics computer science social sciences among others introduce pareto optimality deﬁne notion pareto front. consider following problem given items denoted criteria evaluating item denoted functions select minimizes fk]. settings possible identify single item simultaneously minimizes minimizer found combining criteria using linear combination fi’s ﬁnding minimum combination. different choices weights linear combination could result different minimizers; items minimizers linear combination created using grid search weights example. powerful approach involves ﬁnding pareto-optimal items. item said strictly dominate another item greater criterion less least criterion. relation written pareto-optimal items called pareto front items strictly dominated another item contains minimizers found using linear combinations also includes items cannot found linear combinations. denote pareto front call ﬁrst pareto front. second pareto front constructed ﬁnding items strictly dominated remaining items members generally deﬁne pareto front survey recent results concerned properties ﬁrst pareto front relevant anomaly detection algorithm thus considered literature. independent identically distributed density function measurable denote points ﬁrst pareto front belong simplicity denote cardinality general pareto framework points images feasible solutions optimization problem vector objective functions length context paper point corresponds dyad deﬁne section number criteria. common approach multi-objective optimization linear scalarization constructs single criterion convex combination criteria. well-known easy linear scalarization identify pareto points although common motivation pareto methods best knowledge results literature regarding many points pareto front missed scalarization. present result here. deﬁne subset contains pareto-optimal points obtained selection weights linear scalarization. study large compared expectation. context paper pareto-optimal points identiﬁed anomaly score artiﬁcially inﬂated making likely non-anomalous sample rejected. hence size measure much anomaly score inﬂated degree pareto methods outperform linear scalarization. pareto points result non-convexities pareto front. study kinds non-convexities induced geometry domain induced randomness. ﬁrst consider geometry domain. bounded open smooth boundary suppose density vanishes outside point denote unit inward normal deﬁne given hard pareto-optimal points almost surely large enough provided density strictly positive ∂ωh. hence enough study asymptotics e|fth| theorem open connected proof theorem postponed appendix theorem shows asymptotically many pareto points contributed average segment number points contributed depends geometry direction normal vector otherwise independent convexity hence using pareto methods identify signiﬁcantly pareto-optimal points linear scalarization geometry includes non-convex regions. example non-convex satisﬁes hypotheses theorem large enough pareto points neighborhood unattainable scalarization. quantitatively dimensional hausdorff measure recently come attention theorem appears general form unpublished manuscript baryshnikov yukich study non-convexities pareto front occur inherent randomness samples. show that even case convex still numerous small-scale non-convexities pareto front detected pareto methods. illustrate case pareto problem figure left non-convexities pareto front induced geometry domain right non-convexities randomness samples case larger points pareto-optimal large black points cannot obtained scalarization. proof theorem also postponed appendix proof e|f| found hence theorem shows that asymptotically expectation pareto-optimal points obtained linear scalarization pareto problem. experimentally observed true fraction points close means least pareto points obtained pareto methods even convex. figure gives example sets theorems. assume training nominal data samples available. given test sample objective anomaly detection declare anomaly signiﬁcantly different samples suppose different evaluation criteria given. criterion associated measure computing dissimilarities. denote dissimilarity computed using measure corresponding criterion deﬁne dyad dk]t dyad corresponds connection samples therefore different dyads. convenience denote dyads space dyads deﬁnition strict dominance section dyad strictly dominates another dyad di∗j∗ ﬁrst pareto front corresponds dyads strictly dominated dyads second pareto front corresponds dyads strictly dominated dyads deﬁned section recall refer deeper front pareto fronts dyads sample dyads corresponding connections samples. deﬁne dyads associated dyads located shallow pareto fronts dissimilarities samples small combination criteria. thus likely nominal sample. basic idea proposed multi-criteria anomaly detection method using pda. construct pareto fronts dyads training total number fronts required number fronts dyad member front. test sample obtained create dyads corresponding connections training samples illustrated figure similar many anomaly detection methods connect test sample nearest neighbors. could different criterion denote dyads denote declare anomaly dnew {dnew corresponding connections union nearest neighbors criterion words create dyad among nearest neighbors criterion dnew front dnew strictly dominates least single dyad deﬁne depth dnew min{l dnew therefore large dnew near deep fronts distance corresponding training sample large combinations criteria. small dnew near shallow fronts distance corresponding training sample small combination criteria. k-nn based anomaly detection algorithms mentioned section anomaly score function nearest neighbors test sample. multiple criteria could deﬁne anomaly score scalarization. probabilistic properties pareto fronts discussed section know pareto methods identify pareto-optimal points linear scalarization methods signiﬁcantly pareto-optimal points single weight scalarization. motivates develop multi-criteria anomaly score using pareto fronts. start observation figure dyads corresponding nominal test sample typically located near shallower fronts dyads corresponding anomalous test sample. test sample associated dyads dyad dnew depth test sample deﬁne anomaly score mean ei’s corresponds average depth dyads associated thus anomaly score easily computed compared decision threshold using test pseudocode anomaly detector shown algorithm appendix provide details implementation well analysis time complexity heuristic choosing ki’s performs well practice. training time time required test theorems require i.i.d. samples dyads independent. however dyads dyad dependent dyads. suggests theorems also hold non-i.i.d. dyads well supported experimental results presented appendix table comparison different methods experiments. best shown bold. require selecting weights single auc. median best aucs shown four methods. outperforms methods even best weights known advance. sample using linear number criteria handle multiple criteria anomaly detection methods ones mentioned section need re-executed multiple times using different linear combinations criteria. grid search used selection weights linear combination required computation time would exponential approach presents computational problem unless small. since scales linearly encounter problem. compare method four nearest neighbor-based single-criterion anomaly detection algorithms mentioned section methods linear combinations criteria different weights selected grid search compare performance pda. first present experiment simulated data set. nominal distribution given uniform distribution hypercube anomalous samples located outside hypercube. four classes anomalous distributions. class differs nominal distribution four dimensions; distribution anomalous dimension uniform draw training samples nominal distribution followed test samples mixture nominal anomalous distributions probability selecting particular anomalous distribution. four criteria experiment correspond squared differences dimension. criteria combined using linear combinations combined dissimilarity measure reduces weighted squared euclidean distance. different methods evaluated using receiver operating characteristic curve area curve mean aucs simulation runs shown table grid points criterion corresponding different sets weights used select linear combinations single-criterion methods. note best performer outperforming even best linear combination. present experiment real data contains thousands pedestrians’ trajectories open area monitored video camera trajectory approximated cubic spline curve seven control points represent trajectory time samples figure left curves attainable region k-lpe choices weights. outperforms k-lpe even best choice weights. right subset dyads training samples along ﬁrst pareto fronts. fronts highly non-convex partially explaining superior performance pda. criteria computing dissimilarity trajectories. ﬁrst criterion compute dissimilarity walking speed. compute instantaneous speed time steps along trajectory ﬁnite differencing i.e. speed trajectory time step given manner. take dissimilarity trajectories squared euclidean distance speed histograms. second criterion compute dissimilarity shape. trajectory select points uniformly positioned along trajectory. dissimilarity trajectories given squared euclidean distances positions points. training sample experiment consists trajectories test sample consists trajectories. table shows performance compared algorithms using uniformly spaced weights linear combinations. notice higher methods choices weights criteria. detailed comparison curve attainable region k-lpe shown figure along ﬁrst pareto fronts pda. k-lpe performs slightly better false positive rate best weights used performs better situations resulting higher auc. additional discussion experiment found appendix paper proposed multi-criteria anomaly detection method. proposed method uses pareto depth analysis compute anomaly score test sample examining pareto front depths dyads corresponding test sample. dyads corresponding anomalous sample tended located deeper fronts compared dyads corresponding nominal sample. instead choosing speciﬁc weighting performing grid search weights different dissimilarity measures proposed method efﬁciently detect anomalies manner scales linearly number criteria. also provided theorem establishing pareto approach asymptotically better using linear combinations criteria. numerical studies validated theoretical predictions pda’s performance advantages simulated real data. thank zhaoshi meng assistance labeling pedestrian trajectories. also thank daniel dewoskin suggesting fast algorithm computing pareto fronts criteria. work supported part grant wnf---. independence built assumptions theorems clear dyads independent. dyad represents connection independent samples given dyad corresponding dyads involving clearly independent dij. however dyads independent dij. dyads dyad independent dyads except size since theorems deal asymptotic results suggests hold dyads even though i.i.d. section present experimental results support non-rigorous statement. figure sample means versus expected logarithmic half-power growth respectively. dotted lines indicate best curves described section. best curve closely aligned experimental data visible. domain resulting dyads shown figure experiment tests theorem case theorem suggests grow logarithmically. figure shows sample means versus number dyads best logarithmic curve form denotes number dyads. linear regression versus next looked criteria induce domains boxes order test theorem somewhat contrived example involves criteria |∆x| |∆y| |∆x| |∆y| which applied uniformly sampled data yields dyads sampled diamond domain shown figure case theorem suggests grow figure shows sample means versus number dyads best curve form αnβ. linear regression versus gave although example practical simply meant illustrate applicability theorem non-independent samples. experiment varied number dyads increments computed size increment. experiment times compute sample means shown figure pairwise dissimilarities criterion requires ﬂoating-point operations denotes number dimensions involved computing dissimilarity. pareto fronts constructed non-dominated sorting. section present fast algorithm nondominated sorting criteria; criteria non-dominated sort constructs pareto fronts using comparisons worst case. testing phase involves creating dyads test sample nearest training samples criterion requires ﬂops. dyad dnew need calculate depth involves comparing test dyad training dyads multiple fronts training dyad dominated test dyad. front training dyad part using binary search select front another binary search select training dyads within front compare need make comparisons compute anomaly score computed taking mean ei’s corresponding test sample; score compared threshold determine whether sample anomalous. mentioned section training testing phases scale linearly number criteria dyads. experience memory requirement largest obstacle applying pareto methods large data sets. algorithm runs time average requires memory. based following observation data sorted ascending order ﬁrst criterion ﬁrst point pareto-optimal subsequent pareto-optimal point found searching next point sorted list dominated recent addition pareto front. criteria average pareto fronts ﬁnding front algorithm requires visiting points hence average complexity. worst case complexity occurring pareto front consists single point. pseudocode algorithm shown algorithm recently come attention algorithm exists canonical anti-chain partition problem equivalent non-dominated sorting criteria also used quickly construct pareto fronts. parameters selected denote number nearest neighbors criterion. connect test sample training sample nearest neighbors terms dissimilarity measure deﬁned criterion discuss parameters selected. simplicity ﬁrst assume criterion single parameter selected. able detect anomaly distribution dyads respect pareto fronts differs nominal sample. speciﬁcally mean depths dyads corresponding anomalous sample must higher nominal sample. chosen small case especially training samples present near anomalous sample case dyads corresponding anomalous sample reside near shallow fronts much like nominal sample. hand chosen large many dyads correspond connections training samples away even test sample nominal also makes mean depths nominal anomalous samples similar. propose properties k-nearest neighbor graphs constructed training samples select number training samples connect test sample. construct symmetric k-nngs i.e. connect samples nearest neighbors nearest neighbors begin increase k-nng training samples connected i.e. single connected component. forcing k-nng connected ensure isolated regions training samples. isolated regions could possibly lead dyads corresponding anomalous samples residing near shallow fronts like nominal samples undesirable. keeping small retaining connected k-nng trying avoid problem many dyads even nominal sample many dyads located near deep fronts. method choosing retain connectivity used heuristic unsupervised learning problems spectral clustering note requiring k-nng connected implicitly assuming training samples consist single class multiple classes close proximity. training samples contain multiple well-separated classes approach work well. let’s return situation designed different criteria. criterion construct ki-nng using corresponding dissimilarity measure increase training samples. note choosing independent criteria probably optimal approach. principle approach chooses ki’s jointly could perform better; however approach would complexity. choose separate ki’s criterion necessary obtain good performance different dissimilarities varying scales properties. however pathological examples independent approach could choose ki’s poorly well-known example moons. examples typically involve multiple well-separated classes problematic previously mentioned. choose ki’s training samples contain multiple well-separated classes beyond scope paper area future work. proposed heuristic work well practice including examples presented section figure shows abnormal trajectories nominal trajectories detected using pda. recall criteria used walking speed trajectory shape. anomalous trajectories could anomalous speeds shapes anomalous trajectories figure look anomalous shape alone. heuristic proposed section choosing ki’s performs quite well experiment shown figure speciﬁcally obtained using parameters chosen proposed heuristic close obtained using", "year": 2011}