{"title": "A New Training Algorithm for Kanerva's Sparse Distributed Memory", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "The Sparse Distributed Memory proposed by Pentii Kanerva (SDM in short) was thought to be a model of human long term memory. The architecture of the SDM permits to store binary patterns and to retrieve them using partially matching patterns. However Kanerva's model is especially efficient only in handling random data. The purpose of this article is to introduce a new approach of training Kanerva's SDM that can handle efficiently non-random data, and to provide it the capability to recognize inverted patterns. This approach uses a signal model which is different from the one proposed for different purposes by Hely, Willshaw and Hayes in [4]. This article additionally suggests a different way of creating hard locations in the memory despite the Kanerva's static model.", "text": "approaches explained article overcome limit kanerva’s original handling eﬃciently random data. main reason problem important solved real world random data rare. approaches mathematically translated diﬀerent training algorithms inspired thinking human capability recognize inverted patterns utilization short term long term memory recognizing pattern depending information person has. seeing black logo white background suﬃcient brain recognize logo inverted colors. said able thing? training algorithm presented hereunder real values counters integers kanerva’s model. memory best approach kanerva’s original statical model creating hard locations construction dinamic locations created storage pattern. details every pattern stored abstract sparse distributed memory proposed pentii kanerva thought model human long term memory. architecture permits store binary patterns retrieve using partially matching patterns. however kanerva’s model especially eﬃcient handling random data. purpose article introduce approach training kanerva’s handle eﬃciently non-random data provide capability recognize inverted patterns. approach uses signal model different proposed diﬀerent purposes hely willshaw hayes article additionally suggests diﬀerent creating hard locations memory despite kanerva’s static model. pentii kanerva introduced sparse distributed memory trying modelize human memory particular long term memory. idea diﬀerent concepts minds points high-dimensional space distance higher concepts diﬀerent. consists reasonable large number memory locations randomly distributed throughout address space lenght patterns. furthermore every location vector counters initialized lenght patterns kanerva’s paper pattern memorized location diﬀerent binary string representing pattern itself anyway article pattern stored always addresses itself patterns self-addressing. storage pattern consists updating vector counters every locations lower distance selected called radius. distance binary strings intended hamming distance number diﬀerent bits between those. location whithin hypersphere centered input pattern counters every locations updated follows input pattern increases value counter corresponding position vector decreases value retrieval consists summing vector counters whose addresses within hypersphere centered retrieval pattern applying kronecker’s delta function every sum-vector percentage maximum lowered signal. tests value demonstrated achieves good performances. graph function visible ﬁgure instead increasing decreasing counter counter increased decreased signal hamming distance input pattern address location corresponding vector counters. necessity generate also locations explained previous section justiﬁed utilization sine wave function described generating locations infact permit memorize informations complemented pattern needs memorized input pattern. said using example corrupted patterns black white background train permit retrieve pattern using corrupted black white background using corrupted white black background training algorithm removes need select radius storage continues require radius retrieval. infact retrieval algorithm identical kanerva’s original diﬀerence instead summing integer values summed real values. generates memory locations order maximize usefulness. memory locations create generated corrupting input pattern given percentage noise. percentages corruption depend training algorithm number generated addresses depends size pattern. tests performed author addresses generated follows. indicates number hard locations percentage corruption value corruption vectors counters addresses updated hence useless locations. values suitable kanerva’s signal decay model described hereunder addresses identiﬁed input pattern also needed number vectors percentage corruption modiﬁed follows dinamic creating hard locations guarantees every locations used. values indicative could surely chosen better. anyway tests demonstrated values already satisfactory comparing performances kanerva’s original despite models. signal loses percentage strenght every location reached strength signal function hamming distance strength signal increase reaching minimum strength indicates hamming dis= tance increaseses heading patterns chosen tests visible ﬁgure performed tests pattern size selected radius pattern size every tested tested diﬀerent patterns diﬀering q-factor q-factor intended number given pattern. q-factor values chosen tests every trained trained pattern particular using patterns corrupted corrupted corrupted tested kanerva’s original trained original starting discuss results tests performed straight-trained sdms using retrieval pattern percentage corruption modela better performances despite kanerva’s original possibile ﬁgure diﬀerence performances moreover visible q-factor low. kanerva’s cannot even recognize pattern q-factor using corrupted input also important emphasize results exposed refer number bit-errors ﬁrst self-addressing pattern recalled retrieval. reason behind choice real retrieval real pattern unknown times convenient stop retrieval iteration self-addressing pattern recalled. figure results test performed using retrieval pattern percentage corruption visualizes better performances achieved model despite kanerva’s sdm. figure results test performed using retrieval pattern percentage corruption corrupted input retrieval pattern case q-factor retrieval ended successfully. seem strange q-factor performances sdms deteriorate little important remember patterns used training retrieval randomly chosen deterioration could imputated unlucky combination choice. going analize results tests ﬁgure possibile signal decay model able retrieve patterns corrupted percentage corruption committing less bit-errors. obviously signal decay model able retrieve pattern highly corrupted input. accomplishment tests possible notice model always better performances kanerva’s sdm. addition signal decay model able recognize highly corrupted patterns. denning said many ways enhance model spare distributed memory capability recognize patterns independently traslations rotations zooms. hope article could arouse attention problems using model sparse distributed memory. signal decay model presented article overcome kanerva’s limit eﬃcently handling random data give characteristic inspired human’s brain capacity recognition inverted pattern. model also ﬂexible dynamic thanks utilization hard locations creation process depend input patterns. desire article could help make step towards future hopefully able replicate human memory moreover human recognition capability using promising kanerva’s theory.", "year": 2012}