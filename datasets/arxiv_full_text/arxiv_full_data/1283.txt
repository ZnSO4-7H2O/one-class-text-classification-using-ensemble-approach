{"title": "Improving neural networks by preventing co-adaptation of feature  detectors", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.", "text": "large feedforward neural network trained small training typically performs poorly held-out test data. overﬁtting greatly reduced randomly omitting half feature detectors training case. prevents complex co-adaptations feature detector helpful context several speciﬁc feature detectors. instead neuron learns detect feature generally helpful producing correct answer given combinatorially large variety internal contexts must operate. random dropout gives improvements many benchmark tasks sets records speech object recognition. feedforward artiﬁcial neural network uses layers non-linear hidden units inputs outputs. adapting weights incoming connections hidden units learns feature detectors enable predict correct output given input vector relationship input correct output complicated network enough hidden units model accurately typically many different settings weights model training almost perfectly especially limited amount labeled training data. weight vectors make different predictions held-out test data almost worse test data training data feature detectors tuned work well together training data test data. overﬁtting reduced using dropout prevent complex co-adaptations training data. presentation training case hidden unit randomly omitted network probability hidden unit cannot rely hidden units present. another view dropout procedure efﬁcient performing model averaging neural networks. good reduce error test average predictions produced large number different networks. standard train many separate networks apply networks test data computationally expensive training testing. random dropout makes possible train huge number different networks reasonable time. almost certainly different network presentation training case networks share weights hidden units present. standard stochastic gradient descent procedure training dropout neural networks mini-batches training cases modify penalty term normally used prevent weights growing large. instead penalizing squared length whole weight vector upper bound norm incoming weight vector individual hidden unit. weight-update violates constraint renormalize weights hidden unit division. using constraint rather penalty prevents weights growing large matter large proposed weight-update makes possible start large learning rate decays learning thus allowing thorough search weight-space methods start small weights small learning rate. test time mean network contains hidden units outgoing weights halved compensate fact twice many active. practice gives similar performance averaging large number dropout networks. networks single hidden layer units softmax output layer computing probabilities class labels using mean network exactly equivalent taking geometric mean probability distributions labels predicted possible networks. assuming dropout networks make identical predictions prediction mean network guaranteed assign higher probability correct answer mean probabilities assigned individual dropout networks similarly regression linear output units squared error mean network always better average squared errors dropout networks. initially explored effectiveness dropout using mnist widely used benchmark machine learning algorithms. contains training images individual hand written digits test images. performance test greatly improved enhancing training data transformed images wiring knowledge spatial transformations convolutional neural network using generative pre-training extract useful features training images without using labels without using tricks best published result standard feedforward neural network errors test set. reduced errors using dropout separate constraints incoming weights hidden unit reduced errors also dropping random pixels dropout also combined generative pre-training case small learning rate weight constraints avoid losing feature detectors discovered pre-training. publically available pre-trained deep belief described errors ﬁne-tuned using standard back-propagation errors ﬁne-tuned using dropout hidden units. publically available code used prefig. error rate mnist test variety neural network architectures trained backpropagation using dropout hidden layers. lower lines also dropout input layer. best previously published result task using backpropagation without pre-training weight-sharing enhancements training shown horizontal line. train deep boltzmann machine times unrolled network errors ﬁne-tuned using standard backpropagation errors using dropout hidden units. mean errors record methods prior knowledge enhanced training sets applied dropout timit widely used benchmark recognition clean speech small vocabulary. speech recognition systems hidden markov models deal temporal variability need acoustic model determines well frame coefﬁcients extracted acoustic input possible state hidden markov model. recently deep pre-trained feedforward neural networks short sequence frames probability distribution states shown outperform tradional gaussian mixture models timit variety realistic large vocabulary tasks figure shows frame classiﬁcation error rate core test timit benchmark central frame window classiﬁed belonging state given highest probability neural net. input adjacent frames advance frame. neural fully-connected hidden layers units fig. frame classiﬁcation error rate core test timit benchmark. comparison standard dropout ﬁnetuning different network architectures. dropout hidden units input units improves classiﬁcation. layer softmax output units subsequently merged distinct classes used benchmark. dropout hidden units signiﬁcantly improves classiﬁcation variety different network architectures frame recognition rate class probabilities neural network outputs frame given decoder knows transition probabilities states runs viterbi algorithm infer single best sequence states. without dropout recognition rate dropout improves record methods information speaker identity. cifar- benchmark task object recognition. uses downsampled color images different object classes found searching names class subclasses images labeled hand produce training images test images single dominant object could plausibly given class name best published error rate test without using transformed data achieved error rate using neural network three convolutional hidden layers interleaved three max-pooling layers report maximum activity local pools convolutional units. layers followed locally-connected layer using dropout last hidden layer gives error rate imagenet extremely challenging object recognition dataset consisting thousands high-resolution images thousands classes object subset classes roughly examples class basis object recognition competition winning entry actually average separate models achieved error rate test set. current state-of-the-art result dataset achieved comparable performance error using single neural network convolutional hidden layers interleaved max-pooling layer followed globally connected layers ﬁnal -way softmax layer. layers weight constraints incoming weights hidden unit. using dropout sixth hidden layer reduces record speech recognition dataset object recognition datasets necessary make large number decisions designing architecture net. made decisions holding separate validation used evaluate performance large number different architectures used architecture performed best dropout validation assess performance dropout real test set. reuters dataset contains documents labeled hierarchy classes. created training test sets containing documents mutually exclusive classes. document represented vector counts common non-stop words count transformed log. feedforward neural network fully connected layers hidden units trained backpropagation gets error test set. reduced using dropout tried various dropout probabilities almost improve generalization performance network. fully connected layers dropout hidden layers works better dropout hidden layer extreme probabilities tend worse used throughout paper. inputs dropout also help though often better retain inputs. also possible adapt individual dropout probability hidden input unit comparing average performance validation average performance unit present. makes method work slightly better. datasets required input-output mapping number fairly different regimes performance probably improved making dropout probabilities learned function input thus creating statistically efﬁcient mixture experts combinatorially many experts parameter gets adapted large fraction training data. dropout considerably simpler implement bayesian model averaging weights model posterior probability given training data. complicated model classes like feedforward neural networks bayesian methods typically markov chain monte carlo method sample models posterior distribution contrast dropout probability assumes models eventually given equal importance combination learning shared weights takes account. test time fact dropout decisions independent unit makes easy approximate combined opinions exponentially many dropout nets using single pass mean net. efﬁcient averaging predictions many separate models. popular alternative bayesian model averaging bagging different models trained different random selections cases training models given equal weight combination bagging often used models decision trees quick data quick test time dropout allows similar approach applied feedforward neural networks much powerful models. dropout seen extreme form bagging model trained single case parameter model strongly regularized sharing corresponding parameter models. much better regularizer standard method shrinking parameters towards zero. familiar extreme case dropout naive bayes input feature trained separately predict class label predictive distributions features multiplied together test time. little training data often works much better logistic classiﬁcation trains input feature work well context features. finally intriguing similarity dropout recent theory role evolution possible interpretation theory mixability articulated breaks sets co-adapted genes means achieving function using large co-adapted genes nearly robust achieving function perhaps less optimally multiple alternative ways uses small number co-adapted genes. allows evolution avoid dead-ends improvements ﬁtness require coordinated changes large number co-adapted genes. also reduces probability small changes environment cause large decreases ﬁtness phenomenon known overﬁtting ﬁeld machine learning. jaitly nguyen senior vanhoucke application pretrained deep neural networks large vocabulary conversational speech recognition tech. rep. department computer science university toronto thank jaitly help timit larochelle neal swersky c.k.i. williams helpful discussions nserc google microsoft research funding. members canadian institute advanced research. details dropout training mnist dataset consists digit images training testing. objective classify digit images correct digit class. experimented neural nets different architectures evaluate sensitivity dropout method choices. show results nets architectures dropout rates dropout hidden units dropout visible units. stochastic gradient descent -sized minibatches crossentropy objective function. exponentially decaying learning rate used starts value learning rate multiplied epoch training. incoming weight vector corresponding hidden unit constrained maximum squared length result update squared length exceeds vector scaled make squared length using cross validation found gave best results. weights initialzed small random values drawn zero-mean normal distribution standard deviation momentum used speed learning. momentum starts value increased linearly ﬁrst epochs stays also learning rate multiplied factor weight decay used. weights updated minibatch. training done epochs. weight update takes following form using constant learning rate also gives improvements standard backpropagation starting high learning rate decaying provided signiﬁcant boost performance. constraining input vectors ﬁxed length prevents weights increasing arbitrarily magnitude irrespective learning rate. gives network opportunity search good conﬁguration weight space. learning rate decays algorithm able take smaller steps ﬁnds right step size make learning progress. using high ﬁnal momentum distributes gradient information large number updates making learning stable scenario gradient computation different stochastic network. apart training neural network starting random weights dropout also used ﬁnetune pretrained models. found ﬁnetuning model using dropout small learning rate give much better performace standard backpropagation ﬁnetuning. deep belief nets took neural network pretrained using deep belief network architecture trained using greedy layer-wise contrastive divergence learning instead ﬁne-tuning usual backpropagation algorithm used dropout version dropout rate hidden units visible units. constant small learning rate used. constraint imposed length incoming weight vectors. weight decay used. hyper-parameters before. model trained epochs stochstic gradient descent using minibatches size standard back propagation gave errors dropout decreased errors deep boltzmann machines also took pretrained deep boltzmann machine ﬁnetuned using dropout-backpropagation. model uses architecture details). ﬁnetuning hyperparameters ones used deep belief network. able mean errors dropout whereas usual ﬁnetuning gives errors. reason dropout gives major improvements backpropagation encourages individual hidden unit learn useful feature without relying speciﬁc hidden units correct mistakes. order verify better understand effect dropout feature learning look ﬁrst level features learned neural network without generative pre-training. features shown figure panel shows random features learned network. features dropout learns simpler look like strokes whereas ones learned standard backpropagation difﬁcult interpret. conﬁrms dropout indeed forces discriminative model learn good features less co-adapted leads better generalization. timit acoustic-phonetic continuous speech corpus standard dataset used evaluation automatic speech recognition systems. consists recordings speakers dialects american english reading phonetically-rich sentences. also comes word phone-level transcriptions speech. objective convert given speech signal transcription sequence phones. data needs pre-processed extract input features output targets. used kaldi open source code library speech pre-process dataset results reproduced exactly. inputs networks ﬁlter bank responses. extracted speech windows strides dimension input representation normalized mean variance minibatches size used pretraining dropout ﬁnetuning. tried several network architectures varying number input frames number layers neural network number hidden units layer figure shows validation error curves number combinations. using dropout consistently leads lower error rates. visible biases initialized zero weights random numbers sampled zeromean normal distribution standard deviation variance visible unit learned. learning done minimizing contrastive divergence. momentum used speed learning. momentum started increased linearly epochs. learning rate average gradient used weight decay used. model trained epochs. subsequent layers trained binary rbms. learning rate used. visible bias unit initialized log) mean activation unit dataset. hyper-parameters used gaussian rbm. layer trained epochs. pretrained rbms used initialize weights neural network. network ﬁnetuned dropout-backpropagation. momentum increased linearly epochs. small constant learning rate used hyperparameters mnist dropout ﬁnetuning. model needs epochs converge. network also ﬁnetuned standard backpropagation using smaller learning rate keeping hyperparameters figure shows frame classiﬁcation error cross-entropy objective value training validation sets. compare performance dropout standard backpropagation several network architectures input representations. dropout consistently achieves lower error cross-entropy. signiﬁcantly controls overﬁtting making method robust choices network architecture. allows much larger nets trained removes need early stopping. also observed ﬁnal error obtained model sensitive choice learning rate momentum. reuters corpus volume archive newswire stories manually categorized topics. corpus covers four major groups corporate/industrial economics government/social markets. sample topics include energy markets accounts/earnings government borrowings disasters accidents interbank markets legal/judicial production/services etc. topic classes form tree typically depth three. training examples. also removed category covered huge chunk examples. left classes documents. divided documents equal-sized training test sets randomly. document represented using frequent non-stopwords dataset. trained neural network using dropout-backpropagation compared standard backpropagation. used architecture. training hyperparameters mnist dropout training training done epochs. figure shows training test errors learning progresses. show nets another architecture trained without dropout. previous datasets discussed obtain signiﬁcant improvements too. learning results better generalization also proceeds smoothly without need early stopping. tiny images cifar- tiny images dataset contains million color images collected web. images found searching various image search engines english nouns image comes unreliable label noun used cifar- dataset subset tiny images dataset contains images divided among classes. class contains training images testing images. classes airplane automobile bird deer frog horse ship truck. cifar- dataset obtained ﬁltering tiny images dataset remove images incorrect labels. cifar- images highly varied canonical viewpoint scale objects appear. criteria including image image contain dominant instance cifar- class object image easily identiﬁable belonging class indicated image label. imagenet dataset millions labeled images thousands categories. images collected labelled human labellers using amazon’s mechanical turk crowd-sourcing tool. subset roughly images classes basis object recognition competition part pascal visual object challenge. version imagenet performed experiments. roughly million training images validation images testing images. dataset similar spirit cifar- much bigger scale. images full-resolution categories instead ten. another difference imagenet images often contain multiple instances imagenet objects simply sheer number object classes. reason even human would difﬁculty approaching perfect accuracy dataset. experiments resized images pixels. models cifar- imagenet deep feed-forward convolutional neural networks feed-forward neural networks models consist several layers neurons neuron given layer applies linear ﬁlter outputs neurons previous layer. typically scalar bias added ﬁlter output nonlinear activation function applied result neuron’s output passed next layer. linear ﬁlters biases referred weights parameters network learned training data. cnns differ ordinary neural networks several ways. first neurons organized topographically bank reﬂects organization dimensions input data. images neurons laid grid. second neurons apply ﬁlters local extent centered neuron’s location topographic organization. reasonable datasets expect dependence input dimensions decreasing function distance case pixels natural images. particular expect useful clues identity object input image found examining small local neighborhoods image. third neurons bank apply ﬁlter mentioned apply different locations input image. reasonable datasets roughly stationary statistics natural images. expect kinds structures appear positions input image reasonable treat positions equally ﬁltering way. bank neurons applies convolution operation input. single layer typically multiple banks neurons performing convolution different ﬁlter. banks neurons become distinct input channels next layer. distance pixels boundaries receptive ﬁelds neighboring neurons convolutional bank determines stride convolution operation applied. larger strides imply fewer neurons bank. models stride pixel unless otherwise noted. important consequence convolutional shared-ﬁlter architecture drastic reduction number parameters relative neural neurons apply different ﬁlters. reduces net’s representational capacity also reduces capacity overﬁt dropout less advantageous convolutional layers. cnns typically also feature pooling layers summarize activities local patches neurons convolutional layers. essentially pooling layer takes input output convolutional layer subsamples pooling layer consists pooling units laid topographically connected local neighborhood convolutional unit outputs bank. pooling unit computes function bank’s output neighborhood. typical functions maximum average. pooling layers units called max-pooling average-pooling layers respectively. pooling units usually spaced least several pixels apart fewer total pooling units convolutional unit outputs previous layer. making spacing smaller size neighborhood pooling units summarize produces overlapping pooling. variant makes pooling layer produce coarse coding convolutional unit outputs found generalization experiments. refer spacing stride pooling units analogously stride convolutional units. pooling layers introduce level local translation invariance network improves generalization. analogues complex cells mammalian visual cortex pool activities multiple simple cells. cells known exhibit similar phase-invariance properties. networks also include response normalization layers. type layer encourages competition large activations among neurons belonging different banks. particular activity neuron bank position topographic organization divided runs adjacent banks neurons position topographic organization. ordering banks course arbitrary determined training begins. response normalization layers implement form lateral inhibition found real neurons. constants hyper-parameters whose values determined using validation set. neurons networks utilize max-with-zero nonlinearity. output total input neuron nonlinearity several advantages traditional saturating neuron models including signiﬁcant reduction training time required reach given error rate. nonlinearity also reduces need contrastnormalization similar data pre-processing schemes neurons nonlinearity saturate activities simply scale presented unusually large input values. consequently data pre-processing step take subtract mean activity pixel data centered. train networks values pixels. networks maximize multinomial logistic regression objective equivalent minimizing average across training cases cross-entropy true label distribution model’s predicted label distribution. initialize weights model zero-mean normal distribution variance high enough produce positive inputs neurons layer. slightly tricky point using max-with-zero nonlinearity. input neuron always negative learning take place output uniformly zero derivative output respect input. therefore it’s important initialize weights distribution sufﬁciently large variance neurons likely positive inputs least occasionally. practice simply different variances initialization works. usually takes attempts. also initializing biases neurons hidden layers positive constant helps learning ground reason. iteration index momentum variable learning rate average batch derivative objective respect publicly available cuda-convnet package train models single nvidia gpu. training cifar- takes roughly minutes. training imagenet takes roughly four days dropout days without. equal learning rate layer whose value determine heuristically largest power produces reductions objective function. practice typically order reduce learning rate twice factor shortly terminating training. model cifar- without dropout three convolutional layers. pooling layers follow three. pooling layers summarize neighborhood stride pooling layer follows ﬁrst convolutional layer performs max-pooling remaining pooling layers perform average-pooling. response normalization layers follow ﬁrst pooling layers upper-most pooling layer connected ten-unit softmax layer outputs probability distribution class labels. convolutional layers ﬁlter banks ﬁlter size model cifar- dropout similar dropout imposes strong regularization network able parameters. therefore fourth weight layer takes input third pooling layer. weight layer locallyconnected convolutional. like convolutional layer ﬁlters bank share weights. layer contains banks ﬁlters size layer dropout. softmax layer takes input fourth weight layer. models imagenet model imagenet dropout trained patches randomly extracted images well horizontal reﬂections. form data augmentation reduces network’s capacity overﬁt training data helps generalization. network contains seven weight layers. ﬁrst convolutional last globally-connected. max-pooling layers follow ﬁrst second ﬁfth convolutional layers. pooling layers summarize neighborhood stride response-normalization layers follow ﬁrst second pooling layers. ﬁrst convolutional layer ﬁlter banks ﬁlters applies stride pixels second convolutional layer ﬁlter banks ﬁlters. layer takes inputs. ﬁrst input layer output ﬁrst convolutional layer. banks layer divided arbitrarily groups group connects unique random channels ﬁrst convolutional layer. second input layer subsampled version original image ﬁltered layer stride pixels. maps resulting ﬁltering inputs summed elementwise max-with-zero nonlinearity applied usual way. third fourth ﬁfth convolutional layers connected another without intervening pooling normalization layers max-with-zero nonlinearity applied layer linear ﬁltering. third convolutional layer ﬁlter banks divided groups group connecting unique random subset channels produced outputs second convolutional layer. fourth ﬁfth convolutional layers similarly ﬁlter banks divided groups group connecting unique random subset channels produced layer below. next weight layers globally-connected neurons each. last layers dropout. finally output last globally-connected layer -way softmax produces distribution class labels. test model averaging prediction patches input image center patch four corner patches horizontal reﬂections. even though make passes image test time able system real-time. order achieve state-of-the-art performance validation found necessary complicated network architecture described above. fortunately complexity architecture main point paper. wanted demonstrate dropout signiﬁcant help even complex neural nets developed joint efforts many groups many years really good object recognition. clearly demonstrated fact using non-convolutional higher layers parameters leads improvement dropout makes things worse without dropout.", "year": 2012}