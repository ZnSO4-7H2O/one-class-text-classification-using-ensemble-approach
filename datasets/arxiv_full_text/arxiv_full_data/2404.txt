{"title": "Distributed Flexible Nonlinear Tensor Factorization", "tag": ["cs.LG", "cs.AI", "cs.DC", "stat.ML", "I.5.1; I.5.4"], "abstract": "Tensor factorization is a powerful tool to analyse multi-way data. Compared with traditional multi-linear methods, nonlinear tensor factorization models are capable of capturing more complex relationships in the data. However, they are computationally expensive and may suffer severe learning bias in case of extreme data sparsity. To overcome these limitations, in this paper we propose a distributed, flexible nonlinear tensor factorization model. Our model can effectively avoid the expensive computations and structural restrictions of the Kronecker-product in existing TGP formulations, allowing an arbitrary subset of tensorial entries to be selected to contribute to the training. At the same time, we derive a tractable and tight variational evidence lower bound (ELBO) that enables highly decoupled, parallel computations and high-quality inference. Based on the new bound, we develop a distributed inference algorithm in the MapReduce framework, which is key-value-free and can fully exploit the memory cache mechanism in fast MapReduce systems such as SPARK. Experimental results fully demonstrate the advantages of our method over several state-of-the-art approaches, in terms of both predictive performance and computational efficiency. Moreover, our approach shows a promising potential in the application of Click-Through-Rate (CTR) prediction for online advertising.", "text": "tensor factorization powerful tool analyse multi-way data. compared traditional multi-linear methods nonlinear tensor factorization models capable capturing complex relationships data. however computationally expensive suffer severe learning bias case extreme data sparsity. overcome limitations paper propose distributed ﬂexible nonlinear tensor factorization model. model effectively avoid expensive computations structural restrictions kronecker-product existing formulations allowing arbitrary subset tensorial entries selected contribute training. time derive tractable tight variational evidence lower bound enables highly decoupled parallel computations high-quality inference. based bound develop distributed inference algorithm mapreduce framework key-value-free fully exploit memory cache mechanism fast mapreduce systems spark. experimental results fully demonstrate advantages method several state-of-the-art approaches terms predictive performance computational efﬁciency. moreover approach shows promising potential application click-through-rate prediction online advertising. tensors multidimensional arrays generalizations matrices high-order interactions multiple entities. example extract three-mode tensor online advertising data. analyze tensor data people usually turn factorization approaches latent factors represent entity model latent factors interact generate tensor elements. classical tensor factorization models include tucker candecomp/parafac decompositions widely used real-world applications. however assume multilinear interaction latent factors unable capture complex nonlinear relationships. recently proposed inﬁnite tucker decomposition generalizes tucker model inﬁnite feature space using tensor-variate gaussian process thus powerful model intricate nonlinear interactions. however inftucker variants computationally expensive kronecker product covariances modes requires model entire tensor structure. addition suffer extreme sparsity real-world tensor data i.e. proportion nonzero entries extremely low. often case zero elements real tensors meaningless simply indicate missing unobserved entries. incorporating training process affect factorization quality lead biased predictions. address issues paper propose distributed ﬂexible nonlinear tensor factorization model several important advantages. first capture highly nonlinear interactions tensor ﬂexible enough incorporate arbitrary subset tensorial entries training. achieved placing gaussian process priors tensor entries input constructed concatenating latent factors mode intricate relationships captured using kernel function. using construction covariance function free kronecker-product structure result users freely choose subset tensor elements training process incorporate prior domain knowledge. example choose combination balanced zero nonzero elements overcome learning bias. second tight variational evidence lower bound derived using functional derivatives convex conjugates subsumes optimal variational posteriors thus evades inefﬁcient sequential updates enables highly efﬁcient parallel computations well improved inference quality. moreover bound allows develop distributed gradient-based optimization algorithm. finally develop simple efﬁcient procedure avoid data shufﬂing operation major performance bottleneck sorting procedure mapreduce. rather sending key-value pairs mapper simply calculates sends global gradient vector without keys. keyvalue-free procedure general effectively prevent massive disk fully exploit memory cache mechanism fast mapreduce systems spark. evaluation using small real-world tensor data fully demonstrated superior prediction accuracy model comparison inftucker state-ofthe-art. large tensors millions nonzero elements approach signiﬁcantly better than least good popular large-scale nonlinear factorization methods based uses hierarchical modeling perform distributed inﬁnite tucker decomposition enhances inftucker using dirichlet process mixture prior latent factors employs online learning scheme method also outperforms gigatensor typical large-scale factorization algorithm large margin. addition method achieves faster training speed enjoys almost linear scalability number computational nodes. apply model prediction online advertising achieves signiﬁcant improvement popular logistic regression linear approaches. ﬁrst introduce background knowledge. convenience notations speciﬁcally denote k-mode tensor rd×...×dk k-th mode dimension tensor entry location denoted introduce tucker decomposition need generalize matrixmatrix products tensor-matrix products. speciﬁcally tensor rr×...×rk multiply matrix rs×t mode dimension mode-k consistent number columns i.e. product tensor size element calculated tucker decomposition model uses latent factor matrix rdk×rk mode core tensor rr×...×rk assumes whole tensor generated note multilinear function uk}. simpliﬁed restricting off-diagonal elements case tucker model becomes candecomp/parafac inﬁnite tucker decomposition generalizes tucker model inﬁnite feature space tensor-variate gaussian process speciﬁcally probabilistic framework assign standard normal prior element core tensor marginalize obtain probability tensor given latent factors vectorized whole tensor kroneckerproduct. next apply kernel trick model nonlinear interactions latent factors replaced nonlinlatent factors feature transformation thus equivalent nonlinear covariance matrix used replace covariance function. nonlinear feature mapping original tucker decomposition performed inﬁnite feature space. further since covariance function latent factors equation actually deﬁnes gaussian process tensors namely tensor-variate input based finally different noisy models sample observed tensor example gaussian models probit models continuous binary observations respectively. despite able capture nonlinear interactions inftucker suffer extreme sparsity issue real-world tensor data sets. reason full covariance kronecker-product covariances modes—{σ size full covariance size factors including zero nonzero elements rather subset them. however real-world tensor data usually extremely sparse huge number zero entries tiny portion nonzero entries. hand zero entries meaningless—they either missing unobserved using adversely affect tensor factorization quality lead biased predictions; hand incorporating numerous zero entries models result large covariance matrices high computational costs. although proposed improve scalability modeling subtensors instead sampled subtensors still sparse. even worse subtensors typically restricted small dimension efﬁciency considerations often possible encounter contain nonzero entry. incur numerical instabilities model estimation. address issues propose ﬂexible gaussian process tensor factorization model. inheriting nonlinear modeling power model disposes kronecker-product structure full covariance therefore select arbitrary subset tensor entries training. speciﬁcally given tensor rd×...×dk tensor entry construct input concatenating corresponding latent facik-th tors modes mode assume underlying function function unknown complex nonlinear. learn function assign gaussian process prior tensor entries function values distributed according multivariate gaussian distribution mean covariance determined kroneckerproduct structure constraint subset tensor entries selected training. prevent learning process biased toward zero entries balanced zeros nonzeros. furthermore useful domain knowledge also incorporated select meaningful entries training. note however still tensor entries intensionally impose kronecker-product structure full covariance model reduced inftucker. therefore modeling perspective proposed model general. assign standard normal prior latent factors given selected tensor entries observed entries sampled noise model paper deal continuous binary observations. continuous data gaussian model joint probability real-world tensor data often comprise large number entries millions nonzeros billions zeros. even using nonzero entries training exact inference proposed model still intractable. motivates develop distributed variational inference algorithm presented follows. since covariance term intertwines latent factors exact inference parallel difﬁcult. therefore ﬁrst derive tractable variational evidence lower bound following sparse gaussian process framework titsias idea introduce small inducing points latent targets augment original model joint multivariate gaussian distribution latent tensor entries targets variational posterior latent targets )dmij bbk. note decomσ posed summation terms involving individual tensor entries additive form enables distribute computation across multiple computers. binary data introduce variational posterior make meanone simply standard expectation-maximization framework optimize model inference i.e. step updates variational posteriors step updates latent factors inducing points kernel parameters. however sequential updates fully exploit paralleling computing resources. strong dependencies step step sequential updates take large number iterations converge. things become worse binary case step updates also dependent other making parallel inference even less efﬁcient. section derive tight elbos subsume optimal variational posteriors thereby avoid sequential updates perform decoupled highly efﬁcient parallel inference. moreover inference quality likely improved using tighter bounds. space limit present ideas results here. detailed discussions given section supplementary material. tight elbo continuous tensors. take functional derivative respect setting derivative zero obtain optimal substitute manipulating terms achieve following tighter elbo. tight elbo binary tensors. binary case difﬁcult coupled together following steps ﬁrst plug optimal continuous case. obtain intermediate elbo contains however quadratic term intertwines {q}j making infeasible analytically derive parallelly compute optimal {q}j. overcome difﬁculty exploit convex conjugate quadratic term introduce extra variational parameter decouple dependences {q}j. that able derive optimal {q}j using functional derivatives obtain following tight elbo. data) kernel parameters. distribute computations multiple computational nodes collect results calculate elbo gradient standard routine gradient descent l-bfgs used solve optimization problem. apparently updating efﬁciently performed parallel moreover convergence guaranteed following lemma. proof given section supplementary material. experience ﬁxed-point iterations much efﬁcient general search strategies identity appropriate step length along gradient direction. calculate gradients respect ﬁrst optimize using ﬁxed point iteration outer control employ gradient descent l-bfgs optimize lead even tighter bound model maxqq empirically converges must faster feeding optimization algorithms altogether. section present detailed design mapreduce procedures fulﬁll distributed inference. basically ﬁrst allocate tensor entries mapper corresponding components elbo gradients calculated. reducer aggregates local results mapper obtain integrated global elbo gradient. ﬁrst consider standard design. brevity take gradient computation latent factors example. tensor entry mapper calculate corresponding gradients send keyvalue pairs indicates mode index latent factors. reducer aggregates gradients recover full gradient respect latent factor. although mapreduce successfully applied numerous applications relies expensive data shufﬂing operation reduce step sort mappers’ output keys aggregation. since sorting usually performed disk signiﬁcant data size intensive disk i/os network communications become serious computational overheads. overcome deﬁciency devise key-value-free map-reduce scheme avoid on-disk data shufﬂing operations. speciﬁcally mapper complete gradient vector maintained parameters including kernel parameters. however relevant components gradient speciﬁed tensor entries allocated mapper updated. updates mapper send full gradient vector reducer simply together obtain global gradient vector without perform extra data sorting. note similar procedure also used perform ﬁxed point iteration efﬁcient mapreduce systems spark fully optimize nonshufﬂing reduce data buffered memory disk i/os circumvented utmost; contrast performance data shufﬂing degrades severely veriﬁed evaluations small tensor size key-value-free mapreduce gains times speed acceleration traditional key-value process. therefore algorithm fully exploit memory-cache mechanism achieve fast inference. suppose tensor entries training inducing points mapper time complexity mapper node since ﬁxed constant time complexity linear number mjrj order store latent factors gradients covariance matrix inducing points indices latent factors tensor entry. again space complexity linear number tensor entries. comparison inftucker utilizes kronecker-product properties calculate gradients perform eigenvalue decomposition covariance matrices tensor mode. therefor higher time space complexity details) scalable large dimensions. classical tensor factorization models include tucker based many excellent works proposed deal data several distributed factorization algorithms recently developed gigatensor dfacto despite widespread success methods underlying multilinear factorization structure limit capability capture complex nonlinear relationship real-world applications. inﬁnite tucker decomposition distributed online extensions address issue modeling tensors subtensors tensor-variate gaussian process however methods suffer extreme sparsity real-world tensor data kronecker-product structure covariance requires modeling entire tensor space matter elements meaningful not. contrast ﬂexible factorization model eliminates kroneckerproduct restriction model arbitrary subset tensor entries. theory nonlinear factorization models belong random function prior models exchangeable multidimensional arrays. distributed variational inference algorithm based sparse efﬁcient approximation framework scale models. sparse uses small inducing points break dependency random function values. recently titsias proposed variational learning framework sparse based derived tight variational lower bound distributed inference regression gplvm derivation tight elbo model continuous tensors similar however gradient calculation substantially different input factorization model concatenation latent factors. many tensor entries partly share latent factors causing large amount key-value pair sent distributed gradient calculation. incur expensive data shufﬂing procedure takes place disk. improve computational efﬁciency develop non-key-value map-reduce avoid data shufﬂing fully exploit memory-cache mechanism efﬁcient mapreduce systems. strategy also applicable map-reduce based learning algorithms. addition continuous data also develop tight elbo binary data optimal variational posteriors. introducing extra variational parameters convex conjugates inference performed efﬁciently distributed manner avoids explicit optimization large number variational posteriors latent tensor entries inducing targets. method also useful classiﬁcation problem. evaluation ﬁrst compared method various existing tensor factorization methods. used four small real datasets methods computationally feasible alog real-valued tensor size representing three-way interaction access log. contains nonzero entries. adclick real-valued tensor size describing clicks online advertising. contains nonzero entries. enron binary tensor extracted enron email dataset depicting three-way relationship contains elements nonzero. nellsmall binary tensor extracted nell knowledge base size depicts knowledge predicates data contains nonzero elements. tucker inﬁnite tucker extension uses dirichlet process mixture prior model latent clusters local perform scalable online factorization note inftucker inftuckerex nonlinear factorization approaches. used training remaining non-zero entries zero entries used testing number non-zero entries comparable number zero entries. this zero nonzero entries treated equally important testing evaluation dominated large portion zeros. inftucker inftuckerex carried extra cross-validations select kernel form kernel parameters. inftuckerex randomly sampled subtensors tuned learning rate following model number inducing points used balanced training generated follows addition nonzero entries randomly sampled number zero entries made sure would overlap testing zero elements. model used kernel kernel parameters estimated jointly latent factors. thus expensive parameter selection procedure needed. implemented distributed inference algorithm optimization frameworks gradient descent l-bfgs comprehensive evaluation also examined balanced training entries generated model denoted cp-. mean squared error used evaluate predictive performance alog click area-under-curve enron nell. averaged results -fold cross validation reported. model achieves higher prediction accuracy inftucker better comparable accuracy inftuckerex t-test shows model outperforms inftucker signiﬁcantly almost situations. although inftuckerex uses prior improve factorization model still obtains signiﬁcantly better predictions alog adclick comparable better performance enron nellsmall. might attributed ﬂexibility model using balanced training entries prevent learning bias similar improvements observed cp-. finally model outperforms remaining methods demonstrating advantage nonlinear factorization approach. examine scalability proposed distributed inference algorithm used following large real-world datasets real-valued tensor describing threeway interactions code repository management system tensor size nonzero. dblp binary tensor depicting three-way bibliography relationship tensor extracted dblp database contains elements nonzero entries. nell binary tensor representing knowledge predicates form tensor size nonzero. scalability distributed inference algorithm examined regard number machines dataset. number latent factors algorithm using gradient descent. results shown figure y-axis shows reciprocal running time multiplied constant—which compared approach three state-of-the-art large-scale tensor factorization methods gigatensor distributed inﬁnite tucker decomposition inftuckerex gigatensor dintucker developed hadoop inftuckerex uses online inference. model implemented spark. gigatensor dintucker approach large yarn cluster inftuckerex single computer. number latent factors dblp data nell data set. following settings randomly chose nonzero entries training sampled test data sets remaining entries. dblp test data comprises nonzero elements zero elements; nell test data contains nonzero elements zero elements. running gigatensor based default settings software package. dintucker inftuckerex randomly sampled subtensors distributed online inference. parameters including number size subtensors learning rate selected kernel form parameters chosen cross-validation training tensor. model used setting small data. mappers gigatensor dintucker model. figure shows predictive performance methods. observe approach consistently outperforms gigatensor dintucker three datasets; approach outperforms inftuckerex dblp slightly worse inftuckerex nell. note inftuckerex uses prior enhance factorization model doesn’t; ﬁnally nonlinear factorization methods outperform gigatensor distributed factorization algorithm large margin conﬁrming advantages nonlinear factorizations large data. terms speed algorithm much faster gigatensor dintucker. example dblp dataset average per-iteration running time minutes model gigatensor dintucker respectively. surprising model uses data sparsity exclude numerous meaningless zero elements training; algorithm based spark efﬁcient mapreduce system hadoop; algorithm gets data shufﬂing fully exploit memory-cache mechanism spark. used online click major internet company extracted four mode tensor used ﬁrst three days’s trained model day’s data used predict click behaviour next day. sizes extracted tensors three days respectively. tensors sparse words observed clicks rare. however want prediction completely bias toward zero otherwise ranking recommendation infeasible. thus sampled non-clicks quantity clicks training testing. note training prediction models comparable clicks non-click samples common online advertising systems number training testing entries used three days respectively. compared popular methods prediction including logistic regression linear tensor entry represented binary features according indices mode entry. results reported table terms auc. shows model improves logistic regression linear large margin average respectively. therefore although incorporated side features user proﬁles advertisement attributes tentative experiments shown promising potential model prediction task. paper proposed nonlinear ﬂexible tensor factorization model. disposing kronecker-product covariance structure model properly exploit data sparsity ﬂexible incorporate subset meaningful tensor entries training. moreover derived tight elbo continuous binary problems based developed efﬁcient distributed variational inference algorithm mapreduce framework. future consider applying asynchronous inference tight elbo improve scalability model.", "year": 2016}