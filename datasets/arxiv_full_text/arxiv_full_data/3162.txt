{"title": "Towards Conceptual Compression", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. The system represents the state-of-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'.", "text": "introduce simple recurrent variational autoencoder architecture signiﬁcantly improves image modeling. system represents stateof-the-art latent variable models imagenet omniglot datasets. show naturally separates global conceptual information lower level details thus addressing fundamentally desired properties unsupervised learning. furthermore possibility restricting storing global information image allows achieve high quality ‘conceptual compression’. images contain large amount information priori stored independently pixels. semisupervised learning regime large number images available small number labels would like leverage information create representations allow better generalization. intuitively expects representations explicitly extract global conceptual aspects image. paper propose method able transform image progression increasingly detailed representations ranging global conceptual aspects level details time model greatly improves latent variable image modeling compared earlier implementations deep variational auto-encoders furthermore advantage simple homogeneous architecture requiring complex design choices similar recurrent structure draw last provides important insight building good variational auto-encoder models images multiple layers stochastic variables ‘close’ pixfigure conceptual compression shows full reconstructions model. subsequent rows obtained storing ﬁrst groups latent variables generating remaining ones model group four columns shows different samples given compression level. variations later samples small details precise placement strokes. reducing number stored bits tends preserve overall shape increases symbol variation. eventually varied symbols generated. nevertheless even ﬁrst clear difference variations produced given symbol different symbols. system’s ability stratify information enables perform high quality lossy compression storing subset latent variables starting high level ones generating remainder decompression currently ultimate arbiter lossy compression remains human evaluation. simple measures distance compressed original images inappropriate example particular generated grass texture sharp different original image yield large distance should variational auto-encoding framework. since also interested compression present information-theoretic perspective. variational autoencoders typically consist neural networks generates samples latent variables infers latent variables observations networks share latent variables. intuitively speaking might think variables specifying given image different levels abstraction whether particular object present input perhaps exact position intensity edge given location might recognition phase network acquires information input stores latent variables reducing uncertainty. example ﬁrst knowing whether present image network observes input becomes nearly certain cat. reduction uncertainty quantitatively equal amount information network acquired input. generation network starts uncertain latent variables selects values prior distribution speciﬁes uncertainty different choices produce different samples. variational auto-encoders provide natural framework unsupervised learning build networks layers stochastic variables expect that learning representations become increasingly abstract higher levels hierarchy. questions framework indeed discover representations principle practice networks powerful enough modeling real data techniques needs make work well. variational auto-encoders used representation learning also compression. training objective variational auto-encoders compress total amount information needed encode input. achieve using information-carrying latent variables express what compression encoded using larger amount information input. information layers remaining information input encoded practice explained later paper. amount lossless compression able achieve bounded underlying entropy image distribution. additionally image information measured bits contained details image. thus might reasonably expect lossless compression never improve factor comparison current performance. figure conceptual compression analogous figure applied natural images. originals placed bottom compare easily ﬁnal reconstructions nearly perfect. latent variables generated zero variance. iterations model steps shown. achieving good lossy compression storing high level latent variables would imply representations learned high level contain information similar used humans judge images. humans outperform best machines learning abstract representations human evaluation lossy compression obtained generative models constitutes reasonable test quality representations learned models. numerous techniques exist unsupervised learning deep networks e.g. sparse auto-encoders sparse coding denoising autoencoders deconvolutional networks restricted boltzmann machines deep boltzmann machines generative adversarial networks variational autoencoders importantly solve deep representation learning latent variable generative models generate high quality samples achieve objective lossy compression mentioned above. follows. assume network learned hierarchy progressively abstract representations. then different levels compression store corresponding number topmost layers generate rest. solving unsupervised deep learning network would order information according importance store priority. ultimate goal unsupervised learning remains elusive make step direction show network learns order information rather global level precise details images without hand-engineered explicitly illustrated figures information separation already allows achieve better compression quality jpeg jpeg shown figure bound constraints algorithms speed memory results demonstrate potential method better latent variable generative models improve. challenges involved turning latent variable models state-of-the-art generative models images? many successful vision architectures highly over-complete representations contain many neurons hidden layers pixels. representations need combined sharp distribution pixel level pixels modeled independently. distribution corresponds salt pepper noise present natural images perceptible level. poses major challenge. experimenting deep variational auto-encoders concluded exceedingly difﬁcult obtain satisfactory results single computational pass network. instead propose network needs ability correct number time steps. thus sharp reconstructions property high-precision values network rather result iterative feedback mechanism robust network parameter change. mechanism provided draw algorithm recurrent type variational auto-encoder. time step draw maintains provisionary reconstruction takes information given image stores latent variables updates reconstruction. keeping track reconstruction aids iterative feedback mechanism learned backpropagation. computation deep iterations lossy compression hand holds much potential improvement. case want compress image certain amount allowing information loss maximizing quality similarity original image. example level compression could start reducing pixel precision e.g. bits bits. then jpeg could express local neighborhood discrete cosine transform basis store signiﬁcant components. instead introducing quantization artifacts image would appear kept decreasing pixel precision preserve higher level structures lower level precision. however beyond keep pushing compression? would like preserve important aspects image. determines important? imagine compressing images cats dogs would like compress image bit. would would imagine represent whether image contains either dog. would image single bit? good generative model simply generate entire image latent variable image corresponds ‘cat’ image otherwise. imagine instead compressing wanted compress bits. store important properties animal well e.g. type color basic pose. rest would ‘ﬁlled generative model conditioned information. increase number bits further preserve image generating details hair exact pattern ﬂoor etc. bits fact level details. call kind compression compressing giving priority higher levels representation generating remainder ‘conceptual compression’. suggest ultimate objective lossy compression. introduce convolutional draw. features convolutions latent prior modeling gaussian input distribution experiments multilayer architecture. however explicit attentional mechanism. note even single-layer version already deep generative model decide process higher level information ﬁrst focusing details demonstrate degree. also experiment making convolutional draw hierarchical similar would build conventional deep variational auto-encoders stacking layers latent deterministic variables. believe recurrence important accurate pixel reconstructions also higher levels. example network decides generate edges different locations needs make sure aligned. hard imagine happening single computational pass network. similarly higher levels decides generate objects need generated right relationship another. ﬁnally scene level probably generate entire scenes once rather step time. relate discussion families generative models speciﬁcally generative adversarial networks auto-regressive pixel models. gans demonstrated able generate realistic looking images properly aligned edges using simple feedforward generative network gans also contain networks generative network variational auto-encoders classiﬁcation network. classiﬁcation network presented real model-generated images tries classify according true nature real modelgenerated. generative network gets gradients classiﬁcation network changing weights attempt make generated images judged real ones classiﬁcation network. makes generation network produce realistic images ‘fool’ classiﬁcation network. needs produce wide diversity images realistic image produced classiﬁcation network would classify image generated others realistic almost always correct. actually happens practice apply variety techniques e.g. obtain sufﬁcient image diversity. however extent gans’ sampling diversity unknown currently satisfactory measure given network doesn’t produce image possible produces tiny subset possible realistic images simply competes power classiﬁer. example generates sharp image unclear whether system also capable generating translated version simply slightly distorted version. finally another uncertainty pixel level instead predicting pixels independently given latents decide latents iterate sequentially pixels predicting given pixel previous ones autoregressive fashion ‘close’ pixels possibly furthermore procedure purely deterministic. disadvantage conceptual information decisions done conceptual level pixel level. example generating cats dogs decisions ﬁrst pixels contain information hypothetical dog. region objects start choosing pixels slowly probability generating other. start generating likely cat’s dog’s however pixel level approach approach orthogonal easily combined example feeding output convolutional draw conditional computation pixel level model. paper study latent variable approach make pixels independent given latents. section describe details single-layer version algorithm. convolutional draw contains following variables input reconstruction reconstruction error state encoder recurrent state decoder recurrent latent variable variables recurrent initialized learned biases. time step convolutional draw performs following updates show turn variational auto-encoders including convolutional draw compression algorithms. built actual compressor however explain strong reasons believe would perform well calculated here. basic approaches exist. ﬁrst less convenient needs extra data bitstream compressing image essentially guaranteed compression rate. require experimentation expected yield similar compression rate used given image without needing extra data. underlying compression mechanism cases arithmetic coding arithmetic coding takes input sequence discrete variables probabilities predict variable time previous variables. compresses sequence log) bits plus constant orcompressing inputs using variational auto-encoders proceeds follows discretize latent variable layer using width treat resulting variables sequence predictions compress using arithmetic coding. work explained several things needed. first discretization independent input. achieved training network variance learned constant depend input. found much effect likelihood. second evaluate likelihood using discretization. decide exact manner computed discrete value. signiﬁcant tuning might required here convolutional operators instead usual linear ones. ﬁnal value inal contains parameters input distribution. binary images bernoulli distribution. natural images gaussian distribution mean variance given splitting vector obtain input cost total cost handling real valued-ness inputs explained below standard setting. algorithm schematically illustrated ﬁrst layer figure network trained calculating gradient loss using stochastic gradient descent algorithm. stochastic back-propagation sampling function done variational auto-encoders approximate posterior prior gaussian mean variance linear functions respectively. discuss handle input distribution natural images. pixel values. could soft-max distribution model would result rather large output vector every time step also take advantage underlying real valuedness intensities therefore opted gaussian distribution. however still needs converted discrete distribution values calculate negative likelihood loss instead this uniform noise input width corresponding spacing discrete values calculate inputs scaled interval next explain stack convolutional draw layer example. ﬁrst layer introduced. second layer structure recurrent encoder recurrent decoder stochastic layer. input second layer mean approximate posterior ﬁrst layer. output second layer biases prior latent variable ﬁrst layer also passed input ﬁrst layer decoder recurrent net. illustrated figure don’t reconstruction error second layer. second approach uses bits-back coding explain basic idea here. discretize latents high level precision transmit information. discretization precision high probabilities discrete values easily assigned. preserve information cost many bits namely probability discretization. instead sampling approximate posterior encoding input encode bits information choice also want transmit. recovered receiving information current input information recovered thus information needed encode current input log. expectation quantity kl-divergence therefore measures amount information stored given latent layer. disadvantage approach cannot encode given input without also information want transmit. however coding scheme works even variance approximate posterior dependent input. natural images models except otherwise speciﬁed single-layer kernel size stride convolutions input layers hidden layers latent feature maps. trained models cifar- imagenet lstm feature maps respectively. version imagenet presented soon released standard dataset. train network adam algorithm learning rate occasionally cost suddenly increases dramatically. probably gaussian nature distribution given variable produced mean relative sigma. observed happening approximately run. able keep training store older parameters detect jumps revert parameters occur. network keeps training nothing happened. figure lossy compression. example images various methods levels compression. block original images. subsequent block four rows corresponding four methods compression jpeg jpeg convolutional draw full prior variance generation convolutional draw zero prior variance. block corresponds different compression level; bottom average number bits input dimension ﬁrst block jpeg left gray compress level. images size appendix images. many ‘inverse mnist’ designed study conceptual representations generative models low-data regime. table shows likelihoods different models compared ours. model calculate upper bound therefore true likelihood actually better. samples generated model shown figure table shows likelihoods different models cifar-. method outperforms previous methods except released pixel model mentioned advantage model compared auto-regressive models latent variable model used representation learning lossy compression. time approaches orthogonal combined example feeding output convolutional draw recurrent network pixel rnn. also report likelihood variational auto-encoder standard draw. variational auto-encoder tested architectures multiple layers deterministic stochastic standard functional forms best result obtained. convolutional draw performs signiﬁcantly better. additionally trained version imagenet prepared created making standardized dataset test generative models. results table note dataset methods reported figure figure show generations model. trained networks varying input cost scales explained next section. generations sharp contain many details unlike previous versions variational auto-encoder tend generate blurry images. figure generated samples different input cost scales. convolutional draw trained imagenet. scale input cost respective block rows standard maximum likelihood smaller values network less compelled explain details images produces cleaner larger structures. pixel data consists values such likelihood lossless compression well deﬁned. compressing image much gained capturing precise correlations between nearby pixels. bits level details higher level structure actually interested learning higher level represenone make focus less details scale cost input relative latents setting generations different cost scalings shown figure original objective scale lower scales indeed ‘cleaner’ high level structure. scale contains information precise pixel values network tries capture that good enough properly align details produce realistic patterns. might simply matter scaling making layers larger networks deeper using iterations using better functional forms. convolutional draw uses many iterations might considered expensive. however found networks larger number time steps train faster data example shown figure study train respect real time multiply time scale input number iterations seen figure despite several iterations convolutional draw take wall clock time train architecture smaller larger training slows down eventually reach better performance lower figure dependence training times different number draw iterations. convolutional draw performs several recurrent steps given input image. left graph shows training curves different function number data presentations right graph displays function real training time. despite several iterations draw take wall clock time train draw smaller larger training slows down however eventually reach better performance lower look much information different levels time steps contain. information simply divergence layer system convolutional fully connected layer shown figure higher level contains information mainly beginning computation whereas lower layer starts information gradually increases. desirable conceptual point view. suggests network ﬁrst ﬁnds overall structure image explains details contained within structure. understanding overall structure rapidly also convenient algorithm needs respond observations timely manner. compress image loss information storing subset latent variables typically high levels hierarchy. multilayer convolutional draw storing higher levels. however also store subset time steps speciﬁcally given number time steps beginning network generate rest. sion obtained using imagemagick. found however compressors unable produce reasonable results small images high compression rates. instead concatenated images image compressed extracted back compressed small images. number bits image reported number bits image divided actually unfair algorithm since correlations nearby images exploited. nevertheless show comparison figure algorithm shows better quality jpeg jpeg levels corruption easily detectable. note even algorithm trained speciﬁc image size used arbitrarily sized images networks contain convolutional operators. paper introduced convolutional draw stateof-the-art generative model demonstrates potential sequential computation recurrent neural networks scaling latent variable models. inference algorithm sequentially arrives natural stratiﬁcation information ranging global aspects lowlevel details. interesting feature method that restrict storing high level latent variables arrive ‘conceptual compression’ algorithm rivals quality jpeg. generative model outperforms earlier latent variable models omniglot imagenet datasets. denton emily chintala soumith fergus deep generative image models using laplacian pyramid adversarial networks. advances neural information processing systems goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. advances neural information processing systems figure amount information different layers time steps. two-layer convolutional draw trained imagenet convolutional ﬁrst layer fully connected second layer. amount information given layer iteration measured kl-divergence prior posterior presented image ﬁrst layer acquires information second slowly increases suggesting network ﬁrst acquires information ‘what image’ subsequently encodes details. distribution however also generate likely image lowering variance prior gaussian. show generations full variance block figure variance zero ﬁgure. using original variance network generates sharp details. generative model perfect resulting images less realistic looking lower number stored time steps. zero variance network starts rough details making smooth image reﬁnes time steps. generations produced singlelayer convolutional draw thus despite singlelayer achieves level ‘conceptual compression’ ﬁrst capturing global structure image focusing details. another dimension vary lossy compression input scale introduced subsection even store latent variables reconstructed images less detailed scale input cost. build really good compressor compression rate need networks input scales number time steps would produce visually good images. several compression levels looked images produced different methods selected qualitatively network gave best looking images. done image compression level. display compressed images seen selection. graves alex schmidhuber j¨urgen. ofﬂine handwriting recognition multidimensional recurrent neural networks. advances neural information processing systems quoc building high-level features using large scale unsupervised learning. acoustics speech signal processing ieee international conference ieee gregor karol danihelka mnih andriy blundell charles wierstra daan. deep autoregressive networks. proceedings international conference machine learning gregor karol danihelka graves alex rezende danilo jimenez wierstra daan. draw recurrent neural network image generation. proceedings international conference machine learning hinton geoffrey camp drew. keeping neural networks simple minimizing description length weights. proceedings sixth annual conference computational learning theory kavukcuoglu koray sermanet pierre boureau y-lan gregor karol mathieu micha¨el lecun yann. learning convolutional feature hierarchies visual recognition. advances neural information processing systems radford alec metz luke chintala soumith. unsupervised representation learning deep convolutional generative adversarial networks. arxiv preprint arxiv. rezende danilo mohamed shakir wierstra daan. stochastic backpropagation approximate inference deep generative models. proceedings international conference machine learning sohl-dickstein jascha weiss eric maheswaranathan niru ganguli surya. deep unsupervised learning using nonequilibrium thermodynamics. arxiv preprint arxiv. oord aaron schrauwen benjamin. factoring variations natural images deep gaussian mixture models. advances neural information processing systems vincent pascal larochelle hugo lajoie isabelle bengio yoshua manzagol pierre-antoine. stacked denoising autoencoders learning useful representations deep network local denoising criterion. journal machine learning research figure generated samples network trained imagenet input scaling qualitatively asking model less precise seems lead visually appealing samples. figure generated samples network trained imagenet input scaling value system dedicates resources explain details losing higher level coherence. models better problem might disappear. figure lossy compression part analogous figure main paper inputs. example images various methods amounts compression. block original images. subsequent block four methods compression jpeg jpeg convolutional draw full prior variance generation convolutional draw zero prior variance. different blocks correspond different compression levels bottom bits input dimension ﬁrst block jpeg left gray compress level.", "year": 2016}