{"title": "Distributional Reinforcement Learning with Quantile Regression", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In reinforcement learning an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.", "text": "theoretical contributions work proof distributional bellman operator contraction maximal form wasserstein metric probability distributions. context wasserstein metric particularly interesting suffer disjoint-support issues arise performing bellman updates. unfortunately result directly lead practical algorithm noted authors developed bellemare wasserstein metric viewed loss cannot generally minimized using stochastic gradient methods. negative result left open question whether possible devise online distributional reinforcement learning algorithm takes advantage contraction result. instead algorithm ﬁrst performs heuristic projection step followed minimization divergence projected bellman update prediction. work therefore leaves theory-practice understanding distributional reinforcement learning makes difﬁcult explain good performance thus existence distributional algorithm operates end-to-end wasserstein metric remains open question. paper answer question afﬁrmatively. appealing theory quantile regression show exists algorithm applicable stochastic approximation setting perform distributional reinforcement learning wasserstein metric. method relies following techniques transpose parametrization whereas former uses ﬁxed locations approximation distribution adjusts probabilities assign ﬁxed uniform probabilities adjustable locations; formally prove contraction mapping results overall algorithm results conclude method performs distributional end-to-end wasserstein metric desired. reinforcement learning agent interacts environment taking actions observing next state reward. sampled probabilistically state transitions rewards actions induce randomness observed long-term return. traditionally reinforcement learning algorithms average randomness estimate value function. paper build recent work advocating distributional approach reinforcement learning distribution returns modeled explicitly instead estimating mean. examine methods learning value distribution instead value function. give results close number gaps theoretical algorithmic results given bellemare dabney munos first extend existing results approximate distribution setting. second present novel distributional reinforcement learning algorithm consistent theoretical formulation. finally evaluate algorithm atari games observing signiﬁcantly outperforms many recent improvements including related distributional algorithm reinforcement learning value action state describes expected return discounted rewards obtained beginning state choosing action subsequently following prescribed policy. knowing value optimal policy sufﬁcient optimally object modelled classic value-based methods sarsa qlearning bellman’s equation efﬁciently reason value. recently bellemare dabney munos showed distribution random returns whose expectation constitutes aforementioned value described distributional analogue bellman’s equation echoing previous results risk-sensitive reinforcement learning previous work however authors argued usefulness modeling value distribution itself. claim asserted exhibiting distributional reinforcement learning algorithm achieved state-ofc algorithm models using discrete distribution supported comb ﬁxed locations uniformly spaced predetermined interval. parameters distribution probabilities expressed logits associated location given current value distribution algorithm applies projection step target onto ﬁnite element support followed kullback-leibler minimization step achieved state-of-the-art performance atari games clear disconnect theoretical results bellemare dabney munos review results extending case approximate distributions. wasserstein metric p-wasserstein metric also known mallows metric earth mover’s distance integral probability metric distributions. p-wasserstein distance characterized metric inverse cumulative distribution functions p-wasserstein metric distributions given main interest original distributional algorithm state-of-the-art performance despite still acting maximizing expectations. might naturally expect direct minimization wasserstein metric rather heuristic approximation yield even better results. derive q-learning analogue method apply suite atari games achieves even better performance. using smoothed version quantile regression huber quantile regression gain impressive median score increment already state-of-the-art model agent-environment interactions markov decision process state action spaces random variable reward function probability transitioning state state taking action discount factor. policy maps state distribution random variable representing discounted rewards observed along trajectory states following standard algorithms estimate expected value value function distributional distribution returns plays central role replaces value function. refer value distribution random variable. value function mean value distribution saying value function expected value taken sources intrinsic randomness value distribution. highlight value distribution designed capture uncertainty estimate value function parametric uncertainty rather randomness returns intrinsic mdp. parameterize value distribution mean scale gaussian laplace distribution minimize divergence target prediction demonstrate value distributions learned sufﬁcient perform risk-sensitive qlearning. however theoretical guarantees derived method asymptotic; bellman operator best non-expansion divergence. recall approximates distribution state attaching variable probabilities ﬁxed locations approach transpose parametrization considering ﬁxed probabilities variable locations. speciﬁcally take uniform weights effectively approximation aims estimate quantiles target distribution. accordingly call quantile distribution space quantile distributions ﬁxed denote cumulative probabilities associated distribution also write simplify notation. formally parametric model. quantile distribution maps state-action pair uniform probability distribution supported {θi}. compared original parametrization beneﬁts parameterized quantile distribution threefold. first restricted prespeciﬁed bounds support uniform resolution potentially leading signiﬁcantly accurate predictions range returns vary recently wasserstein metric focus increased research appealing properties respecting underlying metric distances outcomes unlike kullback-leibler divergence wasserstein metric true probability metric considers probability distance various outcome events. properties make wasserstein well-suited domains underlying similarity outcome important exactly matching likelihoods. lemma tells useful metric studying behaviour distributional reinforcement learning algorithms particular show convergence ﬁxed point moreover lemma suggests effective practice learn value distributions attempt minimize wasserstein distance distribution bellman update analogous tdlearning attempts iteratively minimize distance between unfortunately another result shows cannot general minimize wasserstein metric using stochastic gradient descent. theorem empirical distribution derived samples drawn bernoulli distribution bernoulli distribution parametrized probability variable taking value minimum expected sample loss general different minimum true wasserstein loss; issue becomes salient practical context value distribution must approximated. crucially algorithm guaranteed minimize pwasserstein metric. theory practice distributional restricted morimura however method widely used economics machine learning unbiased stochastic approximation quantile function. quantile regression conditional quantile regression methods approximating quantile functions distributions conditional distributions respectively methods used variety settings outcomes intrinsic randomness food expenditure function household income studying value-at-risk economic models quantile regression loss quantile asymmetric convex loss function penalizes overestimation errors weight underestimation errors weight distribution given quantile characterized value quantile function minimizer quantile regression loss particular loss gives unbiased sample gradients. result minimizing stochastic gradient descent. quantile huber loss quantile regression loss smooth zero; gradient equation stays constant. hypothesized could limit performance using non-linear function approximation. also consider modiﬁed quantile loss called quantile huber loss. quantile regression loss acts asymmetric squared loss interval around zero reverts standard quantile loss outside interval. greatly across states. also lets away unwieldy projection step present issues disjoint supports. together obviate need domain knowledge bounds return distribution applying algorithm tasks. finally reparametrization allows minimize wasserstein loss without suffering biased gradients speciﬁcally using quantile regression. quantile approximation well-known reinforcement learning function approximation result instabilities learning process speciﬁcally bellman update projected onto approximation space longer contraction. case analyze distributional bellman update projected onto parameterized quantile distribution prove combined operator contraction. quantile projection interested quantifying projection arbitrary value distribution onto particular inverse −)/) always valid minimizer continuous −)/) unique minimizer. quantile midpoints denoted τi−+τi therefore lemma values minimize given figure shows example quantile projection minimizing -wasserstein distance quantile regression original proof theorem states existence distribution whose gradients biased. result might hope quantile parametrization leads unbiased gradients. unfortunately true. combining projection bellman update position prove main result states combination projection implied quantile regression bellman operator contraction. result ∞-wasserstein metric i.e. size largest cdfs. proposition quantile projection deﬁned above applied value distributions gives projection state-value distribution. value distributions countable state action spaces combined operator unique ﬁxed point repeated application operator stochastic approximation converges conclude convergence occurs interestingly contraction property directly hold lemma appendix. distributional using quantile regression form complete algorithmic approach distributional consistent theoretical results. approximating value distribution parameterized quantile distribution quantile midpoints deﬁned lemma then training location parameters using quantile regression allows update estimated value function single unbiased sample following quantile regression also allows improve estimate quantile function target distribution observing samples minimizing equation furthermore shown estimating quantile function well-chosen values obtain approximation minimal -wasserstein distance original finally combine distributional bellman operator give target distribution quantile regression. gives quantile regression temporal difference learning algorithm summarized simply update quantile distribution estimated value state important note update value deﬁned single sample next state value distribution. general better draw many samples minimize expected update. natural approach case practice compute update pairs θj)). next turn control algorithm non-linear function approximation. quantile regression q-learning off-policy reinforcement learning algorithm built around directly learning optimal action-value function using bellman optimality operator concrete algorithm build architecture focus minimal changes necessary form distributional version dqn. speciﬁcally require three modiﬁcations dqn. first nearly identical neural network architecture changing output layer size hyper-parameter giving number quantile targets. second replace huber loss used quantile huber loss finally replace rmsprop adam call algorithm quantile regression unlike qr-dqn require projection onto approximating distribution’s support instead able expand contract values arbitrarily cover true range return values. additional advantage means qr-dqn require additional hyper-parameter giving bounds support required additional hyper-parameter qr-dqn shared number quantiles controls resolution approximate value distribution. increase qr-dqn goes increasingly able estimate upper lower quantiles value distribution. becomes increasingly capable distinguishing probability events either cumulative distribution returns. figure two-room windy gridworld wind magnitude shown along bottom row. policy trajectory shown blue path additional cycles caused randomness shown dashed line. value distribution start state estimated introduction claimed learning distribution returns distinct advantages learning value function alone. given theoretically justiﬁed algorithms performing distributional reinforcement learning qrtd policy evaluation qr-dqn control. section empirically validate proposed distributional reinforcement learning algorithms learn true distribution returns show increased robustness training signiﬁcantly improve sample complexity ﬁnal performance baseline algorithms. value distribution approximation error begin experimental results demonstrating qrtd actually learns approximate value distribution minimizes -wasserstein ground truth distribution returns. although theoretical results already establish convergence former latter empirical performance helps round understanding. variant classic windy gridworld domain modiﬁed rooms randomness transitions. figure shows version domain combined transition stochasticity wind doorway produce multimodal distribution returns anywhere ﬁrst room. state transition probability moving random direction otherwise transition affected wind moving agent northward. reward function zero reaching goal state terminates episode gives reward discount factor compute ground truth value distribution optimal policy learned policy iteration state performing monte-carlo rollouts recording observed returns empirical distribution shown figure next qrtd following episodes. episode begins designated start state algorithms started learning rate qrtd used drop half every episodes. figure show approximation errors algorithms respect number episodes. evaluated qrtd squared error show -wasserstein expected returns value distribution state estimated algorithm. expected algorithms converge correctly mean qrtd minimizes -wasserstein distance evaluation atari provide experimental results demonstrate practical advantages minimizing wasserstein metric end-to-end contrast approach. atari games arcade learning environment qr-dqn build standard architecture expect beneﬁt recent improvements dueling architectures prioritized replay however evaluations compare pure versions qr-dqn without additions. present results strict quantile loss huber quantile loss performed hyper-parameter tuning training games evaluated full games using best settings target network computing distributional bellman update. also allow decay rate lower value common recent work training procedure follows mnih present results evaluation protocols best agent performance online performance. evaluation protocols consider performance atari games transform scores humannormalized scores figure online evaluation results human-normalized scores atari games million training samples. testing performance seed showing median games. training performance averaged three seeds showing percentiles games. best agent performance provide comparable results existing work report test evaluation results under best agent protocol. every million training frames learning frozen agent evaluated frames recording average return. evaluation episodes begin random no-ops agent uses lower exploration rate training progresses keep track best agent performance achieved thus far. table gives best agent performance million frames trained qr-dqn double prioritized replay dueling architecture qr-dqn outperforms previous agents mean median human-normalized score. online performance evaluation protocol track average return attained testing training iteration. testing performance single seed algorithm show online performance form early stopping. training performance values averages three seeds. instead reporting median performance look distribution human-normalized scores full games. represents score distribution ﬁxed percentile upper percentiles show similar trend omitted visual clarity scale dwarfs informative lower half. this infer interesting results. early learning algorithms perform worse random least games. qrtd gives similar improvements sample complexity prioritized replay also improving ﬁnal performance. even million frames games algorithms reach less human. ﬁnal point particular shows recent advances continue severely limited small subset atari games. importance distribution returns reinforcement learning discovered highlighted many times now. bellemare dabney munos idea taken step further argued central part approximate reinforcement learning. however paper left open question whether exists algorithm could bridge wasserstein-metric theory practical concerns. paper closed theoretical contributions algorithm achieves stateof-the-art performance atari remain many promising directions future work. exciting expand promise richer policy class made possible action-value distributions. mentioned examples policies often used risk-sensitive decision making. however many possible decision policies consider action-value distributions whole. additionally qr-dqn likely beneﬁt improvements made recent years. instance similarity loss functions bellman operators might expect qr-dqn suffers similar overestimation biases double designed address natural next step would combine qr-dqn nondistributional methods found table authors acknowledge vital contributions colleagues deepmind. special thanks schaul audrunas gruslys charles blundell benigno uria early suggestions discussions topic quantile regression. additionally grateful feedback david silver whye georg ostrovski joseph modayil matt hoffman hado hasselt osband mohammad azar stepleton olivier pietquin bilal piot; second acknowledgement particular schaul detailed review previous draft.", "year": 2017}