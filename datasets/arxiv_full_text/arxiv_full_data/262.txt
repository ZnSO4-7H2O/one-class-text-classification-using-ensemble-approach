{"title": "SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for  Predicting Chemical Properties", "tag": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "abstract": "Chemical databases store information in text representations, and the SMILES format is a universal standard used in many cheminformatics software. Encoded in each SMILES string is structural information that can be used to predict complex chemical properties. In this work, we develop SMILES2vec, a deep RNN that automatically learns features from SMILES to predict chemical properties, without the need for additional explicit feature engineering. Using Bayesian optimization methods to tune the network architecture, we show that an optimized SMILES2vec model can serve as a general-purpose neural network for predicting distinct chemical properties including toxicity, activity, solubility and solvation energy, while also outperforming contemporary MLP neural networks that uses engineered features. Furthermore, we demonstrate proof-of-concept of interpretability by developing an explanation mask that localizes on the most important characters used in making a prediction. When tested on the solubility dataset, it identified specific parts of a chemical that is consistent with established first-principles knowledge with an accuracy of 88%. Our work demonstrates that neural networks can learn technically accurate chemical concept and provide state-of-the-art accuracy, making interpretable deep neural networks a useful tool of relevance to the chemical industry.", "text": "introduction chemical industry designing chemicals desired characteristics boleneck development products. despite decades research much modern chemical design still driven serendipity chemical intuition although improvements rational chemical design incrementally improving time. currently exists contributing factors current state physics-based rule-based computational chemistry methods accounts towards true rational chemical design. factor driven technical limitations compute capacity incomplete partial understanding underlying chemical concepts. factor addressed development gpuaccelerated code molecular modeling special-purpose chemistry supercomputers second factor requires solutions grounded fundamental research slower process. prediction chemical properties cannot easily computed physics-based rule-based methods modern insilico modeling chemistry therefore predicated correlating engineered features activity property chemical formally known antitative structureactivity structure-property relationship modeling feature engineering chemistry sophisticated science stretches back late molecular descriptors termed chemists basic computable properties sophisticated descriptions chemical’s structure engineered features developed based rst-principles knowledge. date molecular descriptors developed addition molecular ngerprints also designed instead computing basic property provides description specic part chemical’s structure since various machine learning algorithms used predict activity property chemicals using molecular desciptors and/or ngerprints input features. recently deep learning models also developed general models either perform parity slightly outperform prior state-of-the-art models based traditional algorithms chemical applications abstract chemical databases store information text representations smiles format universal standard used many cheminformatics soware. encoded smiles string structural information used predict complex chemical properties. work develop smilesvec deep automatically learns features smiles predict chemical properties without need additional explicit feature engineering. using bayesian optimization methods tune network architecture show optimized smilesvec model serve general-purpose neural network predicting distinct chemical properties including toxicity activity solubility solvation energy also outperforming contemporary neural networks uses engineered features. furthermore demonstrate proof-of-concept interpretability developing explanation mask localizes important characters used making prediction. tested solubility dataset identied specic parts chemical consistent established rst-principles knowledge accuracy work demonstrates neural networks learn technically accurate chemical concept provide state-of-the-art accuracy making interpretable deep neural networks useful tool relevance chemical industry. concepts computing methodologies natural language processing; neural networks; transfer learning; applied computing chemistry; computational biology; bioinformatics; eory computation bayesian analysis; permission make digital hard copies part work personal classroom granted without provided copies made distributed prot commercial advantage copies bear notice full citation page. copyrights third-party components work must honored. uses contact owner/author. london copyright held owner/author. ---//.... compared computer vision natural language processing research models chemistry relies heavily engineered features. problematic limits neural network’s search space potentially learnable representations. exacerbated situations engineered features appropriate inadequate lack well-developed domain knowledge originates second factor limits impact existing computational chemistry methods. contrast dominant approach cv/nlp research train models directly typically unaltered data large datasets lile minimal feature engineering research. example unaltered images used input various models similarly unaltered text used lstm-based models erefore developing models leverage representation learning logical advance chemistry well. lastly additional challenge associated current ml/dl models lack interpretability. typically operated opaque black-box models dicult gain scientic understanding algorithm predicts particular chemical property. typical applications cv/nlp research issue. however chemical industry particularly regulated products requesting approval drugs explanation chemical works erefore models increased interpretability explainability industrial relevance also enable chemists formulate hypothesis improve possibly accelerate pace fundamental research. related work chemistry unaltered data would typically refer representation describes structure orientation chemical. basic chemistry education students taught draw diagram chemical also serves primary medium communication amongst chemist. alternatively structural information encoded graphs. indeed convolutional neural network models chemical images models molecular graphs recently developed. addition chemical’s structural information also encoded text format smiles also basis interoperability between various cheminformatics soware packages. terms text representations acknowledge prior work direction terms interpretable models seen advances conventional cv/nlp applications time writing aware interpretable models chemistry learns directly data. contributions work improves existing state learning directly chemical text representations also interpretable neural network works chemical text. process work also addresses following question smiles representation sucient capture order distinction dierent chemical properties? assuming hypothesis true would possible validate neural network learns developed explanation mask explain show optimal smilesvec network architecture generalized predict broad range properties relevance multiple industries including pharmaceuticals biotechnology materials consumer goods. demonstrate smilesvec models despite feature engineering achieves beer accuracy contemporary multi-layer perceptron models uses engineered features. organization rest paper follows. section examine datasets broad applicability chemicalaiated industries well bayesian optimization process used rening network architecture training protocols neural network. section document experiments used develop smilesvec network architecture constructing interpretability masks. lastly section perform additional experiments quantify accuracy interpretability using solubility dataset example evaluate smilesvec accuracy chemical property predictions contemporary models. methods here document methods used development smilesvec. first provide brief introduction smiles. provide details datasets used data spliing preparation. examine details rening neural network architecture using bayesian methods well training protocol evaluation metrics neural network. introduction smiles smiles chemical language encodes structural information chemical compact text representation. regular grammar smiles. example alphabets denote atoms cases also type atoms. example denote aromatic aliphatic carbons respectively. special characters like denote type bonds rings denoted encapsulating numbers side chains round brackets. sucient training chemist read smiles infer structure chemical. structural information complex properties predicted. inspired language translation work explicitly encode information grammar smiles. instead expect learn paerns necessary develop intermediate features would relevant predicting variety chemical properties. properties without signicant network topology changes. ensure results comparable contemporary models reported literature earlier work chemception models used freesolv dataset moleculenet benchmark predicting toxicity activity solvation free energy respectively. datasets represent good dataset sizes type chemical properties regression classication tasks. addition also used esol solubility dataset evaluate interpretability smilesvec. relevance chemical industries terms relevance chemical-aiated industries toxicity prediction importance chemicals require approval includes drugs therapeutics well cosmetics activity measurement well chemical binds intended target factors determine well chemical perform drug. erefore accurate activity predictions relevance pharmaceuticals biotechnology industries. predicting solubility important consideration developing formulations products relevant pharmaceuticals consumer goods also aects bioavailability drugs. lastly free energy values computable physics-based simulations methods currently employed pharmaceuticals consumer goods materials industries. data preparation length smiles string directly impacts compute resources required train models. maintain balance between maximum amount smiles data also rapid training time surveyed chembl database collection industriallyrelevant chemicals million entries. using database proxy relevant chemicals calculated setting maximum length characters would encompass existing entries. erefore above-listed datasets excluded entries characters dataset. next created dictionary mapped unique characters one-hot encodings. zero padding also applied ensure shorter strings uniform size characters. addition extra padding zeroes added right data splitting used dataset spliing approach similar reported previous work separate test partitioned serve test model generalizability. dataset partitioned form test freesolv esol dataset used create test set. remaining dataset used random -fold cross validation approach training. model performance early stopping criterion determined validation loss. lastly oversampled minority class classication tasks mitigate class imbalance. achieved computing ratio classes appending additional data smaller class ratio. oversampling step performed stratication ensure molecule repeated across training/validation/test sets. used bayesian optimizer sigopt optimize hyperparameters related neural network topology. dierent network hyperparameters dened separate trial trial trained model completion using standardized supervised training protocol. trial validation metric cation tasks rmse regression tasks) used input bayesian optimizer suggesting network designs. prevent overing optimization process splitting dataset training validation sets governed random seed. however test maintained throughout also used optimization process. comparing dierence validation test metrics would thus allow determine network design being overed training/validation data. hyperparameters optimization performed learning protocol. lastly noted subset dataset used bayesian optimization. specically used single task dataset freesolv dataset. training neural network smilesvec trained using tensorow backend acceleration using nvidia cudl libraries. network created executed using keras functional interface rmsprop algorithm used train epochs using standard seings recommended batch size also included early stopping reduce overing. done monitoring loss validation improvement validation loss epochs last best model saved model. classication tasks used binary crossentropy loss function training. performance metric reported work area curve regression tasks used mean average error loss function training. performance metric reported rmse. unless specied otherwise reported results work denote mean value performance metric obtained runs -fold cross validation. experiments section conduct several bayesian optimization experiments optimize smilesvec’s architecture hyperparameters. conduct experiments develop explanation mask improving interpretability model. smilesvec neural network design rnns particularly based lstms grus eective neural network designs learning text data. eectiveness demonstrated examples like google neural translation machine uses architecture layers residual lstm units reported application rnns research similarly used model sequence-tosequence predictions fewer layers found suciently accurate tasks. work diers conventional research modeling sequence-to-vector predictions sequence smiles string vector measured chemical property. this also smiles fundamentally dierent language commonly-used techniques research embeddings like wordvec cannot easily adapted work. erefore substantial component work design architecture specic smiles. architectural class exploration explore model’s architecture class primarily includes high-level design choices type units used type layers arrangement layers etc. lstms grus major units used literature form basis architectural classes. template design class starts embedding layer feeds -layer bidirectional -layer bidirectional lstm illustrated figure addition explored utility adding convolutional layer embedding gru/lstm layers. design forms template architectural classes explored. separate bayesian optimization used optimize hyperparameters architectural class. specically varied size embedding intervals number units gru/lstm layers ranged intervals number units convolutional layer ranged intervals convolutional layer size stride used based design principles modern convolutional neural network additional optimization performed size stride convolutional layers. addition specic shape network topology enforced. bayesian optimization hyperparameters order bayesian methods eective sucient number trials dierent neural network design performed. practice recommended minimum trials performed number tunable hyperparameters. work performed trials architectural class. addition manually seeded initial designs class. specically used initial designs embedding size convolution layer lters lstm/rnn layer units. addition developing general-purpose neural network design re-used broad range property prediction would feasible include conceivable training data optimize network design within limits available computing resources. erefore subset datasets used bayesian optimization separate optimizations performed classication freesolv regression tasks. results bayesian optimization across classes tasks indicated figure classication observed additional convolutional layer embedding rnn/lstm layers improved model performance relative counterparts best performing model cnn-lstm class cnn-gru trailing slightly behind. freesolv regression observed gru-based networks outperform lstm-based networks. taking considerations generalization type chemical properties selected cnn-gru architectural class remainder work. selected best network design class summarized table lastly bayesian algorithm uses validation metric means optimize network’s hyperparameters possibility progresses overing towards validation set. determine extent overing examined correlation validation metrics test metrics never used bayesian optimization). illustrated figure correlation validation test metrics dataset freesolv dataset. lower correlation dataset relative freesolv dataset explained noting performance metric optimization performed crossentropy loss function used training network. training explanation mask train neural network generated mask identify important characters input. procedure follows first smilesvec model base network. next construct another neural network produce mask input data objective train mask output base neural network remains masks much data possible. freeze base neural network train explanation mask end-to-end shown figure figure structure training explanation network. smiles input passed embedding layer explainer. produces mask placed original embedding sent pre-trained base model. weights base network frozen mask learned specic smilesvec model. input produce dierent mask. avoid mask trivial added forms regularization small regularization also penalized mask high entropy equal weight inputs). overall loss function single element mini-batch follows lossi sol|| smilesvec interpretation gain beer insight smilesvec model developed method gain level interpretability. here objective identify part smiles string responsible neural network’s decision. methods explaining black models exist methods tend require explicit combinatorial analysis. approach provide provides insight neural network analyzes data without combinatorially probing input. achieved training explanation mask whereby separate explanation network learns mask input data produce near identical output would obtained original data. explanation network used create mask layer residual network selu activations. padding length input remained layer. input network embedding smiles string. last layer convolution length followed batch normalization soplus activation. observed batch normalization layer important trainability. provides output innity smiles position. mask output would prevent base smilesvec receiving input character. mask would cause smilesvec aention input character. trained adam convergence. started learning rate divided training error plateaued ultimately training performance section quantity accuracy interpretability solubility dataset demonstrate generalizability smilesvec model evaluating performance datasets. figure colored circles increasing darkness indicate locations increasing attention molecule. explanation mask validates established knowledge focusing atoms known hydrophobic hydrophilic functional groups. interpreting chemical solubility demonstrate proof-of-concept interpretable smilesvec network using esol solubility dataset. chemical solubility well-understood simple chemical property established rst-principles knowledge. briey parts chemical typically classied either hydrophilic hydrophobic. hydrophilic water-loving groups like alcohols amines carboxyl form strong interactions water increase overall solubility compound typically contain non-carbon atoms like nitrogen oxygen. reverse true hydrophobic water-hating groups tend make chemicals insoluble typically carbonbased chains/rings halogens used pre-trained smilesvec base model attained validation rmse solubility values reported scale less soluble compounds negative number. mask outputs normalized aention value denotes importance particular character network’s decision. smiles string identied top- characters separated dataset soluble insoluble compounds. using established knowledge chemical solubility establish ground truth expect soluble compounds higher aention atoms insoluble compounds higher aention atoms ground truth labeling expected atoms computed top- accuracy smilesvec interpretability addition also qualitatively examined outputs masks mapping smiles character corresponding atom molecular structure examples shown figure molecules solubility characters tend receive aention others correspond hydrophobic groups. contrast molecules high solubility aention focused characters correspond hydrophilic groups. localization appropriate atoms functional group type depending chemical’s predicted solubility value indicates smilesvec learned representations correspond known chemistry concepts. lastly emphasize smilesvec developed representations without provided explicit chemical information. chemical information implicitly encoded smiles string decoding solution provided network neither feature engineering required. work therefore demonstrates eectiveness representation learning data chemical sciences. generalization smilesvec models evaluated performance smilesvec esol solubility dataset. unlike solubility datasets complex properties simple rule-based methods exist chemistry literature. without ability generate ground-truth labels quantifying accuracy smilesvec interpretation nontrivial beyond scope work. nevertheless accuracy model’s predictions still evaluated. also note architectural optimization smilesvec included small fraction datasets identied work; dataset included toxicity tasks included. hence section also determines generalizable bayesian optimized network design chemical tasks. first determined eectiveness generalizing smilesvec remaining datasets. following validation performance metrics obtained full dataset dataset rmse kcal/mol freesolv dataset. furthermore note models dierence validation test metrics small conrming generalization model compounds seen either model training bayesian hyperparameter optimization. using recently developed pre-training approach also able improve performance smilesvec slightly aaining full dataset dataset rmse kcal/mol freesolv dataset. based results conclude bayesian optimization network architectural design eective developing general-purpose smilesvec network design chemical properties. also note recent literature trend towards using black approaches solution network architecture design example using rnns reinforcement learning optimize design target neural network. however methods typically require order trials much trials used work. addition given template architectural class adaptive methods automtically grow shrink neural network also viable alternatives network design. next compare performance best smilesvec model contemporary deep neural networks reported results datasets evaluated model compare typical network uses engineered features chemistry-specic molecular graph convolutional neural network chemception deep uses images figure performance smilesvec contemporary deep neural networks trained engineered features image graph data. higher better. freesolv lower rmse better. results presented figure validation metrics evaluate quality model. comparing methods observed standard models uses engineered features ngerprints) performed worst. smilesvec outperformed models chemception classication tasks slightly underperformed regression tasks. indicated previous work likely lack atomic number information embedded smiles format responsible lower performance predicting calculable physical properties. addition smilesvec also outperformed rst-principles models computing solvation free energy rst-principles models computing toxicity/activity) especially noteworthy since smilesvec predict values much faster traditional computational chemistry simulations typically require minutes hours calculation. convolutional graphs current state-of-the-art many chemical tasks smilesvec either matches classication tasks outperforms regression task erefore smilesvec accurate current state-of-the-art chemistry models importantly also interpretable model. conclusion paper develop smilesvec general-purpose deep neural network uses chemical text data predicting chemical property explanation mask improves interpretability. performing extensive bayesian optimization experiments identied specic cnn-gru neural network architecture eective predicting wide range properties. smilesvec achieved validation toxicity activity prediction respectively validation rmse kcal/mol solvation energy solubility. using solubility dataset illustration smilesvec interpretability construct explanation masks indicate smilesvec localizes specic characters hydrophilic hydrophobic groups top- accuracy identication functional groups relationship chemical solubility rst-principles concept chemistry smilesvec able discover own. compared models smilesvec’s accuracy outperforms typical models uses engineered features input. current state-ofthe-art smilesvec outperforms regression tasks matches classication tasks. results indicate smilesvec accurately predict broad range properties learn technically accurate chemical concepts suggest used interpretable tool future deep learning driven chemical design. acknowledgments authors would like thank nathan baker helpful discussions. work supported following pnnl ldrd programs pauling postdoctoral fellowship deep learning scientic discovery agile investment. references mart´ın abadi paul barham jianmin chen zhifeng chen andy davis jerey dean mahieu devin sanjay ghemawat georey irving michael isard tensorflow system large-scale machine learning.. osdi vol. esben jannik bjerrum. smiles enumeration data augmentation neural network modeling molecules. arxiv preprint arxiv. peter buchwald nicholas bodor. computer-aided drug design role quantitative structure–property structure–activity structure–metabolism relationships drugs future artem cherkasov eugene muratov denis fourches alexandre varnek igor baskin mark cronin john dearden paola gramatica yvonne martin roberto todeschini qsar modeling been? going journal medicinal chemistry sharan chetlur woolley philippe vandermersch jonathan cohen john tran bryan catanzaro evan shelhamer. cudnn ecient primitives deep learning. arxiv preprint arxiv. kyunghyun bart merri¨enboer dzmitry bahdanau yoshua bengio. properties neural machine translation encoder-decoder approaches. arxiv preprint arxiv. john chodera david mobley michael shirts richard dixon branson vijay pande. alchemical free energy methods drug discovery progress challenges. current opinion structural biology neural networks qsar predictions. arxiv preprint arxiv. dewancker michael mccourt clark patrick hayes alexandra johnson george evaluation system bayesian optimization service. arxiv preprint arxiv. david duvenaud dougal maclaurin jorge iparraguirre rafael bombarell timothy hirzel al´an aspuru-guzik ryan adams. convolutional networks graphs learning molecular ngerprints. advances neural information processing systems. anna gaulton louisa bellis patricia bento chambers mark davies anne hersey yvonne light shaun mcglinchey david michalovich bissan al-lazikani chembl large-scale bioactivity database drug discovery. nucleic acids research d–d. yonghui mike schuster zhifeng chen mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey google’s neural machine translation system bridging human machine translation. arxiv preprint arxiv. zhenqin bharath ramsundar evan feinberg joseph gomes caleb geniesse aneesh pappu karl leswing vijay pande. moleculenet benchmark molecular machine learning. arxiv preprint arxiv. garre charles siegel abhinav vishnu nathan hodas. chemnet transferable generalizable deep neural network smallmolecule property prediction. arxiv preprint arxiv. garre charles siegel abhinav vishnu nathan hodas nathan baker. chemception deep neural network minimal chemistry knowledge matches performance expert-developed qsar/qspr models. arxiv preprint arxiv. garre charles siegel abhinav vishnu nathan hodas nathan baker. much chemistry deep neural network need know make accurate predictions? arxiv preprint arxiv. kaiming xiangyu zhang shaoqing jian sun. delving deep rectiers surpassing human-level performance imagenet classication. proceedings ieee international conference computer vision. hinton srivastava swersky. rmsprop divide gradient running average recent magnitude. neural networks machine learning coursera lecture steven kearnes kevin mccloskey marc berndl vijay pande patrick riley. molecular graph convolutions moving beyond ngerprints. journal computer-aided molecular design g¨unter klambauer omas unterthiner andreas mayr sepp hochreiter. self-normalizing neural networks. advances neural information processing systems. naomi kruhlak joseph contrera daniel benz edwin mahews. progress qsar toxicity screening pharmaceutical impurities regulated products. advanced drug delivery reviews andreas mayr g¨unter klambauer omas unterthiner sepp hochreiter. deeptox toxicity prediction using deep learning. frontiers environmental science tomas mikolov chen greg corrado jerey dean. ecient estimation word representations vector space. arxiv preprint arxiv. john pla. inuence neighbor bonds additive bond properties parans. journal chemical physics bharath ramsundar steven kearnes patrick riley dale webster david konerding vijay pande. massively multitask networks drug discovery. arxiv preprint arxiv. marco tulio ribeiro sameer singh carlos guestrin. trust you? explaining predictions classier. proceedings sigkdd international conference knowledge discovery data mining. david shaw grossman joseph bank brannon batson adam jack chao martin denero dror amos even christopher fenton anton raising performance programmability special-purpose molecular dynamics supercomputer. proceedings international conference high performance computing networking storage analysis. ieee press charles siegel daily abhinav vishnu. adaptive neuron apoptosis accelerating deep learning large scale systems. data ieee international conference ieee john stone david hardy ivan umtsev klaus schulten. gpuaccelerated molecular modeling coming age. journal molecular graphics modelling christian szegedy yangqing pierre sermanet reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision paern recognition. tors. vol. john wiley sons. izhar wallach michael dzamba abraham heifets. atomnet deep convolutional neural network bioactivity prediction structure-based drug discovery. arxiv preprint arxiv.", "year": 2017}