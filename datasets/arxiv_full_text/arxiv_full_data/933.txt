{"title": "Is Joint Training Better for Deep Auto-Encoders?", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Traditionally, when generative models of data are developed via deep architectures, greedy layer-wise pre-training is employed. In a well-trained model, the lower layer of the architecture models the data distribution conditional upon the hidden variables, while the higher layers model the hidden distribution prior. But due to the greedy scheme of the layerwise training technique, the parameters of lower layers are fixed when training higher layers. This makes it extremely challenging for the model to learn the hidden distribution prior, which in turn leads to a suboptimal model for the data distribution. We therefore investigate joint training of deep autoencoders, where the architecture is viewed as one stack of two or more single-layer autoencoders. A single global reconstruction objective is jointly optimized, such that the objective for the single autoencoders at each layer acts as a local, layer-level regularizer. We empirically evaluate the performance of this joint training scheme and observe that it not only learns a better data model, but also learns better higher layer representations, which highlights its potential for unsupervised feature learning. In addition, we find that the usage of regularizations in the joint training scheme is crucial in achieving good performance. In the supervised setting, joint training also shows superior performance when training deeper models. The joint training framework can thus provide a platform for investigating more efficient usage of different types of regularizers, especially in light of the growing volumes of available unlabeled data.", "text": "abstract—traditionally generative models data developed deep architectures greedy layer-wise pre-training employed. well-trained model lower layer architecture models data distribution conditional upon hidden variables higher layers model hidden distribution prior. greedy scheme layerwise training technique parameters lower layers ﬁxed training higher layers. makes extremely challenging model learn hidden distribution prior turn leads suboptimal model data distribution. therefore investigate joint training deep autoencoders architecture viewed stack single-layer autoencoders. single global reconstruction objective jointly optimized objective single autoencoders layer acts local layer-level regularizer. empirically evaluate performance joint training scheme observe learns better data model also learns better higher layer representations highlights potential unsupervised feature learning. addition usage regularizations joint training scheme crucial achieving good performance. supervised setting joint training also shows superior performance training deeper models. joint training framework thus provide platform investigating efﬁcient usage different types regularizers especially light growing volumes available unlabeled data. attention machine learning applied statistics literature recent years state-of-the-art results achieved various applications object recognition speech recognition face recognition etc. exception convolutional neural networks training multi-layered networks general unsuccessful breakthroughs made three seminal works ﬁeld hinton bengio ranzato addressed notion greedy layer-wise pre-training initialize weights entire network unsupervised manner followed supervised back-propagation step. inclusion unsupervised pre-training step appeared missing ingredient lead signiﬁcant improvements conventional training schemes. recent progress deep learning towards supervised learning algorithms algorithms obviate need additional pre-training step supervised settings large amount labeled data still critical. given ever-growing volumes unlabeled data cost labeling remains challenge develop better unsupervised learning techniques exploit copious amounts unlabeled data. model data. typical method accomplishing decompose generative model latent conditional generative model prior distribution hidden variables. interpretation extended deep neural network implies lower layers model conditional distribution higher layers model distribution latent variables. shown local-level learning accomplished pre-training important training deep architectures improves performance similarly initializing every layer supervised multi-layer network also improves performance greedy layer-wise scheme major disadvantage higher layers signiﬁcantly less knowledge original data distribution bottom layers. hence bottom layer capture data representation sufﬁciently well higher layers learn something useful. furthermore error/bias propagate layers i.e. higher layer errors incur. summarize greedy layer-wise training focuses local constraints introduced learning algorithm loses sight original data distribution training higher layers. compensate disadvantage levels deep architecture trained simultaneously. type joint training challenging done naively fail learn thus paper present effective method jointly training multi-layer auto-encoder end-to-end unsupervised fashion analyze performance greedy layer-wise training various settings. unsupervised joint training method consists global reconstruction objective thereby learning good data representation without losing sight original input data distribution. addition also easily cope local regularization parameters hidden layer similar layer-wise training therefore allow powerful regularizations proposed recently. method also viewed generalization singlemulti-layer auto-encoders. approach achieves global reconstruction deep network feature representations learned better. attribute also makes good feature extractor conﬁrm extensive analysis. representations learned joint training approach consistently outperform obtained greedy layer-wise pre-training algorithms unsupervised settings. supervsied setting joint training scheme also demonstrate superior performance deeper models. basic autoencoder one-hidden-layer neural network objective reconstruct input using hidden activations reconstruction error small possible. takes input puts encoding function encoding input decodes encodings decoding function recover original input. formally input encoding decoding functions respectively weights encoding decoding layers biases layers. elementwise nonlinear functions general common choices sigmoidal functions like tanh logistic. training want parameters minimize reconstruction error loss function measures error reconstructed input actual input denotes training dataset. also encoding decoding weights setting weights common choices includes sum-of-squared-errors real valued inputs cross-entropy binary valued inputs etc.. however model signiﬁcant drawback number hidden units greater dimensionality input data model perform well training fail test time trivially copied input layer hidden copied back. work-around force model learn something meaningful performance still efﬁcient. vicent proposed efﬁcient overcome shortcomings basic autoencoder namely denoising autoencoder. idea corrupt input passing network still require model reconstruct uncorrupted input. model forced learn representations useful since trivially copying input optimize denoising objective formally corrupted version input corruption process input objective tries optimize supervised methods typically require large amounts labeled data cost acquiring labels expensive time consuming task remains challenge continue develop improved unsupervised learning techniques exploit large volumes unlabeled data. although greedy layerwise pre-training procedure till date successful engineering perspective challenging train monitoring training process difﬁcult. layerwise method apart bottom layer unsupervised model learning directly input training cost measured respect layer below. hence changes error values layer next little meaning user. global objective joint training technique training cost consistently measured respect input layer. readily monitor changes training errors even respect post-training tasks classiﬁcation prediction. also stated earlier unsupervised training network interpreted learning data distribution probabilistic generative model data. order unsupervised method learn common strategy decompose models optimize conditional generating model prior model since covers wide range distributions hard optimize general tends emphasize optimization assume later learning could compensate loss occurred imperfect modeling. note prior also decomposed exactly resulting additional hidden layers. thus recursively apply trick delay learning prior. motivation behind recursion expect learning progresses layers prior gets simpler thus makes learning easier. greedy layer-wise training employs idea fail learn optimum data distribution following reason layerwise training parameters bottom layers ﬁxed training prior model observe ﬁxed hidden representations. hence learning preserve informatin regarding possible learn prior leads optimum model data distribution. reasons therefore explore possibility jointly training deep autoencoders joint training scheme makes adjustments possible respect other thus alleviating burden models. consequence prior model observe input making possible prior distribution better. hence joint training makes global optimization possible. fig. illustration deep autoencoder considered paper. left illustration greedy layerwise training employed training layered deep autoencoder bottom layer ﬁrst trained followed second layer using previous layer’s encoding input. loss layerwise training always measured respect input intermediate layers trying reconstruct back representation input data ﬁnal deep autoencoder viewed stacking encoding layers followed decoding layers. right joint training scheme start deep autoencoder optimize jointly minimize loss input ﬁnal reconstruction. therefore parameters tuned towards better represent input data. addition regularization enforced local layer help learning. note simplicity biases excluded illustration. exc∼q)] conditional distribution input arbitrary regularization function parameters. choice good regularization term seems ingredient recent successes several autoencoder variants example). straightforward recover previous autoencoder objectives equation example basic autoencoder obtained setting dirac delta data point since objective general going forward refer objective rather speciﬁc autoencoder ones presented earlier. mentioned previously deep autoencoder optimized layer-wise training i.e. bottom layer objective jgae optimized jgae desired depth reached. would like jointly train layers natural question would combine objectives train layers jointly? directly combing jgae) appropriate since goal reconstruct input accurate possible intermediate representations. aggregated loss regularization hinder training process since objective deviated goal. furthermore training representation lower layers varying continuously making optimization difﬁcult. penalty term measured change within hidden activations respect input. thus penalty term alone would prefer hidden activations stay constant input varies loss term small reconstruction close original possible hidden activation change according input. therefore hidden representation tries represent data manifold would preferred since otherwise would high costs terms. deep autoencoder architecture therefore contains multiple encoding decoding stages made sequence encoding layers followed stack decoding layers. notice deep autoencoder therefore total layers. type deep autoencoder investigated several previous works examples). deep autoencoder also viewed unwrapped stack multiple autoencoders higher layer autoencoder taking lower layer encoding input. hence viewed stack autoencoders could train stack bottom top. straight forward single layer case proposed training objective equation equivalent greedy layerwise training. would interesting investigate relationship method layerwise training multilayer cases. therefore construct scenario make method similar modify joint training algorithm slightly setting training schedule learning rates indicate regularizer corresponding layer training iteration. objective equation gradient update follows joint training scheme similar layerwise training scheme i.e. tune parameter single layer time bottom top. however since loss measured domain instead learning still behave differently. apparent write joint loss layerwise loss training given layer follows represent trained decoding encoding function respectively. words optimize parameters function training. note that loss resulting parameter update different equation general since gradient joint loss modiﬁed previous layers parameters somewhat surprising even constructing similar situation methods still behave differently. note even special case uses linear activation losses still equivalent. hence joint training perform differently layerwise general. words method optimizing reconstruction error directly multi-layer neural network time enabling conveniently apply powerful regularizers single autoencoders layer. example want train two-layered denoising autoencoder using method need corrupt input feed ﬁrst layer corrupt hidden output ﬁrst layer feed second layer; reconstruct second layer hidden activations followed reconstruction original input space using ﬁrst layer reconstruction weight; measure loss reconstruction input gradient update parameters formulation train layers jointly optimizing global objective hidden activations adjust according original input; also take account local constraints single layer autoencoders locally isolate autoencoder stacks look individually training process optimizing almost exactly single layer objective layerwise case without unnecessary constraint reconstructing back intermediate representations. globally loss measured reconstruction input parameters must tuned minimize global loss thus resulting higher level hidden representations would representative input. approach addresses drawbacks layerwise training since parameters tuned simultaneously toward reconstruction error original input optimization longer local layer. addition reconstruction error input space makes training process much easier monitor interpret. formulation provides easier modular train deep autoencoders. relate joint objective existing well-known deep learning paradigms implement deterministic objective functions. also relate work techniques literature explain deep learning probabilistic perspective. easy replace regularizers norms dirac delta distribution point equation recover standard multi-layer perceptron weight decay exception commonly used empirical distribution data. model decomposed bottom layer models distribution higher layers models prior notice that apply layerwise training possible learn prior ﬁxed thus prior optimal respect preserve information regarding hand joint training generative distribution prior tuned together therefore likely obtain better estimations true addition training layers greedily taking account fact capacity added later improve prior hidden units. problem also alleviated joint training since architecture ﬁxed beginning training parameters tuned towards better representation data. bengio recently proposed alternative maximum likelihood training probabilistic models generative stochastic networks idea learning data distribution directly hard since highly multi-modal general. hand learn approximate markov chain transition operator intuition move markov chain mostly local thus distributions likely less complex even unimodal make easier learning problem. example denoising autoencoder learns corrupted example. show consistent estimator following implied markov chain stationary distribution chain converge true data distribution like denoising autoencoder deep denoising autoensection empirically analyse unsupervised joint training method following questions joint training lead better data models? joint training result better representations would helpful tasks? role recent usage regularizers autoencoder play joint training? joint training affect performance supervised ﬁnetuning? tested approach mnist digit classiﬁcation dataset contains training test examples used standard training validation split. addition also used mnist variation datasets data points training validation test. additional shape datsets employed also employed datasets contains shape classiﬁcation tasks. classify short tall rectangles classify convex non-convex shapes. datasets testing examples. rectangle dataset training validation examples respectively convex dataset training validation respectively. rectangle dataset also variation uses image foreground rectangles training validation examples respectively experiments layer hidden units using logistic activations cross-entropy loss applied training cost. optimized deep network using rms-prop decay factor estimate samples mini-batch. hyperparameters chosen validation model obtained best validation result used obtain test result. hyper-parameters considered learning rate noise level deep denoising autoencoders contraction level deep contractive autoencoders gaussian noise applied dae. mentioned previous sections hypothesize joint training alleviate burden bottom layer distribution layer priors hence result better data model experiment inspect goodness modeling data distribution samples model. since deep denoising autoencoder follows framework follow implied markov chain generate samples. models trained epochs using layerwise joint training method quality samples estimated measuring log-likelihood test parzen window density estimator measuerment seen lower bound true log-likelihood converge true likelihood number samples increase appropriate parzen window parameter. consecutive samples generated datasets models trained using layerwise joint training method used gaussian parzen window density estimation. estimated log-likelihoods respective test sets shown table sufﬁx denote layerwise joint training method respectively. qualitative purposes generated samples dataset shown figure quality samples comparable cases however models trained joint training shows faster mixing less spurious samples general. also interesting note that cases log-likelihood test improved each layer trained epochs layerwise training note estimate little high variance knowledge best available method estimating generative models generate samples estimate data likelihood directly. fig. samples generated deep denoising autoencoder best estimated log-likelihood trained mnist image classiﬁcation datasets used bottom samples mnist mnist-rotation mnist-bg-image mnist-bg-random mnist-bg-rot-image rectangle rectangle-image convex dataset. left training samples corresponding dataset. middle consecutive samples generated deep denoising autoencoder trained using layerwise scheme. right consecutive samples generated deep denoising autoencoder trained joint training. notice every fourth sample shown last column consecutive samples shows closest image training illustrate model memorizing training data. figure samples longer runs. deeper models joint training case whereas layerwise settings likelihood dropped additional layers. illustrates advantage using joint training scheme model data since accommodates additional capacity hidden prior training whole model. training objective focuses reconstruction expected reconstruction errors models trained joint training less obaited layerwise training. conﬁrm also record training testing errors training progresses. seen figure clearly case joint training achieves better performance cases. also interesting note models joint training less prone overﬁtting compare layerwise case true even case less training examples previous experiment illustrated advantage joint training greedy layerwise training learning data distribution also conﬁrmed joint training better reconstruction error training testing compare layerwise case since focuses reconstruction. natural follow-up question would tuning parameters deep unsupervised model towards better representing input lead better higher level representations. answer question following experiment conducted. trained twothree-layered deep autoencoder using layerwise joint training methods epochs. weights used deep autoencoders feature extractors data i.e. encoding layer representation feature corresponding data. support vector machine linear kernel trained features test classiﬁcation performance evaluated. model achieved best validation performance used report test error performance different models shown tables iii. report test classiﬁcation error along conﬁdence interval classiﬁcation experiments. since computation contractive penalty higher layers respect input expensive deep contractive autoencoder ∇hi− save computations. result tables suggest representations learned joint training generally better. also important note model achieved error rate without ﬁnetuning using labeled data; words feature extraction completely unsupervised. interestingly three-layered models seemed perform worse two-layered models cases methods contradicts generative performance. fig. training testing reconstruction errors -layered deep denoising autoencoder various datasets using layerwise joint training. mnist mnist-basic mnist-rotation mnist-bg-image mnist-bg-random mnist-bg-rot-image rectangle rectangle-image convex dataset. goal good unsupervised data model capture information regarding whereas supervised tasks goal learn good model contains useful information regarding information might helpful. therefore good generative performance necessarily translate good discriminative performance. trained weights. therefore investigate whether joint training beneﬁcial situation performed another experiments initializing weights deep autoencoders using pre-trained weights denoising contractive autoencoders performed unsupervised joint training epochs without corresponding regularizations. results shown tables iii. sufﬁx indicate results training procedure following joint training preformed without regularization ‘uj’ used indicate trained various schemes. suffix used denote models trained layerwise method used denote models jointly trained without regularization initialized pre-trained weights layerwise scheme used denote models jointly trained corresponding regularizations randomly initialized parameters case joint training performed corresponding regularization. results scheme similar layerwise case whereas performance ‘uj’ clearly better compare layerwise case. result also suggest performance improvement joint training signiﬁcant combined corresponding regularizations irrespective parameter initialization. summary representation learned unsupervised joint training better compared layerwise case. addition important apply appropriate regularizations joint training order beneﬁt. previous experiments clear joint training scheme several advantages layerwise training scheme also suggest proper regularizations important however role regularization training deep autoencoder still unclear possible achieve good performance applying joint training without regularization? investigate trained twothree-layered deep autoencoders epochs constraints weights fair comparison previous experiments. trained linear using top-layer representation reported results table performance signiﬁcantly worse compared case powerful regularizers autoencoders employed especially case noise presented dataset. hence results strongly suggest unsupervised feature learning powerful regularization required achieve superior performance. addition beneﬁcial incorporate powerful regularizations denoising contractive joint training training affects ﬁnetuning. experiment unsupervised deep autoencoders used initialize parameters multi-layer perceptron supervised ﬁnetuning ﬁnetuning performed previously trained models maximum epochs early stopping validation error. expected performance standard deep autoencoder impressive except mnist contained ‘cleaner’ samples signiﬁcantly training examples. also reasonable expect similar performance layerwise joint training since supervised ﬁnetuning process adjusts parameters better partially true observed results table performance -layer models close almost cases. however -layer case models trained joint training appear perform better. true models pre-trained joint training without regularization might suggests joint training beneﬁcial deeper models. hence even case would tuning parameters model supervised tasks unsupervised joint training still beneﬁcial especially deeper models. results also suggest long appropriate regularization employed joint pre-training initialization inﬂuence supervised performance signiﬁcantly. circumstances. formulated single objective deep autoencoder consists global reconstruction objective local constraints hidden layers layers could trained jointly. could also viewed generalization training singlemulti-layer autoencoders provided straightforward stack different variants autoencoders. empirically showed joint training method learned better data models also learned representative features classiﬁcation compared layerwise method highlights potential unsupervised feature learning. addition experiments also showed success joint training technique dependent powerful regularizations proposed recent variants autoencoders. supervised setting joint training also shows superior performance training deeper models. going forward framework jointly training deep autoencoders provide platform taigman yang ranzato wolf deepface closing human-level performance face veriﬁcation cvpr hinton osindero fast learning algorithm deep belief nets neural computation vol. fig. expanded samples shown figure show every fourth sample show every consecutive sample longer runs. last column shows closest sample training illustrate model memorizing training data. bottom samples mnist mnist-rotation mnist-bg-image mnist-bg-random mnist-bg-rot-image rectangle rectangle-image convex dataset. left consecutive samples generated deep denoising autoencoder trained using layerwise scheme. right consecutive samples generated deep denoising autoencoder trained joint training. joint trained models show sharper diverse samples general.", "year": 2014}