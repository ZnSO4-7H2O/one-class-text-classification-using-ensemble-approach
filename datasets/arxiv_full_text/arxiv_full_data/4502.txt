{"title": "Nearly optimal exploration-exploitation decision thresholds", "tag": ["cs.AI", "cs.LG"], "abstract": "While in general trading off exploration and exploitation in reinforcement learning is hard, under some formulations relatively simple solutions exist. Optimal decision thresholds for the multi-armed bandit problem, one for the infinite horizon discounted reward case and one for the finite horizon undiscounted reward case are derived, which make the link between the reward horizon, uncertainty and the need for exploration explicit. From this result follow two practical approximate algorithms, which are illustrated experimentally.", "text": "abstract. general trading exploration exploitation reinforcement learning hard formulations relatively simple solutions exist. optimal decision thresholds multi-armed bandit problem inﬁnite horizon discounted reward case ﬁnite horizon undiscounted reward case derived make link reward horizon uncertainty need exploration explicit. result follow practical approximate algorithms illustrated experimentally. reinforcement learning dilemma selecting actions maximise expected return according current world model improve world model potentially able achieve higher expected return referred exploration-exploitation trade-oﬀ. subject much interest before earliest developments theory sequential sampling statistics developed dealt mostly making sequential decisions accepting among particular hypothesis view towards applying jointly decide termination experiment acceptance hypothesis. general overview sequential decision problems bayesian viewpoint oﬀered optimal intractable bayesian solution bandit problems given recently tight bounds sample complexity exploration found approximation full bayesian case general reinforcement learning problem given alternative technique based eliminating actions conﬁdently estimated low-value given following section formulates intuitive concept trading exploration exploitation natural consequence deﬁnition problem reinforcement learning. problem deﬁnitions correspond either extreme identiﬁed sec. derives threshold switching exploratory greedy behaviour bandit problems. threshold found depend ﬀective reward horizon optimal policy current belief distribution expected rewards action. sketch extension mdps presented sec. section uses upper bound value exploration derive practical algorithms illustrated experimentally sec. conclude discussion relations methods. assume standard multi-armed bandit setting reward distribution conditioned actions discover policy selecting actions maximised. follows optimal gambler oracle problem would constitute policy always chooses given conditional expectations implementing oracle trivial. however tells little optimal select actions expectations unknown. turns optimal action selection mechanism depend upon problem formulation. initially consider simplest cases order illustrate exploration/exploitation tradeoﬀ viewed terms problem model deﬁnition. optimal choice select estimated expected value reward highest according current belief choice necessarily lead lower expectation. thus stating bandit problem allow exploration seemingly lower potentially higher value actions results greedy policy. point estimates expected reward requires sampling uniformly actions thus represents purely exploratory policy. problem stated simply minimising discrepancy asymptotically uniformity required necessary sample actions inﬁnitely often. condition holds satisﬁed mixing optimal policies formulations probability using uniform action selection probability using greedy action selection. results well-known ǫ-greedy policy parameter used control exploration. formulation exploration-exploitation problem though leading intuitive result lead obvious optimally select actions. following section shall consider bandit problems functional maximised formulation problem interested maximising expected reward next time step subsequent steps function providing another convenient weigh preference among short long-term rewards. intuitively expected optimal policy problem diﬀerent depending long-term rewards interested shown later lengthening eﬀective reward horizon manipulation i.e. changing deﬁnition problem wish solve exploration bias increased automatically. want know better decision take action rather action given estimates respectively. shall attempt conditions better take action diﬀerent whose expected reward greatest. shall need following assumption assumption necessary imposing lower bound expected return exploratory actions matter action taken guaranteed without condition exploratory actions would risky taken all. given possible actions take action currently estimated lower expected reward other might worthwhile pursue lower-valued action following conditions true degree uncertainty lower-valued action potentially better higher-valued interested maximising expectation next reward expectation weighted future rewards able accurately determine whether action better quickly enough resources wasted exploration. start viewing random variables hold belief distributions problem deﬁned deciding action better taking action condition allows determine whether high probability exploratory actions. reason need following bound expected return exploration. without loss generality sequel assume convenience write must take action expected return simply taking action smaller expected return taking action steps behaving greedily i.e. following holds thus inﬁnite horizon discounted reward maximisation problems known expected rewards non-negative need used make decision whether worthwhile perform exploration. although might seem strange omitted expression value implicitly expressed value decision making function diﬀerent nature since depends estimates. however cases longer eﬀective horizon becomes larger uncertainty bias towards exploration increased. furthermore note ﬁnite horizon case backward induction procedure used make optimal decisions sec. condition satisﬁed exploration must performed. observe ﬁrst term maximised inequality satisﬁed satisfy thus attempt examine distributions determined. shall restrict distributions bounded below assumption unfortunately closed form solution related lambert function iterative solutions exist found solution plugged whether conditions exploration satisﬁed. general reinforcement learning setting reward distribution depend action taken additionally state variable. state transition distribution conditioned actions markov property. particular task within framework summarised markov decision process simplest extend bandit case general mdps conditions latter reduces former. done example considering choices simple actions temporally extended actions refer options following shall need simpliﬁed version framework possible option corresponds policy suﬃcient sketching conditions equivalence arises. particular examine case options. ﬁrst option always select actions according exploratory principle picking uniform distribution. second always select actions greedily i.e. picking action highest expected return. deﬁnition deﬁne exploration mixing time particular policy expected number time steps state distribution close stationary state distribution taken exploratory action time step i.e. expected number steps following condition holds course necessary ergodic ﬁnite. consider switching options time periods greater option framework’s roughly corresponds bandit framework former latter. means whenever take exploratory action distribution states would remain signiﬁcantly diﬀerent time steps. thus could consider exploration taking place would free continue exploration not. although direct correspondence cases limited equivalence could suﬃcient motivating similar techniques determining optimal exploration exploitation threshold full mdps. order utilise lemma practical setting must deﬁne sense. simplest solution results optimistic estimate exploratory actions shown below. rearranging nevertheless testing existence suitable costly since barring analytic procedure requires exhaustive search. hand possible achieve similar result sampling diﬀerent values herein following sampling method considered firstly determine action greatest ¯qj. then action take sample distribution ¯qj. quite arbitrary sampling method expect obtain high probability high probability signiﬁcantly better method summarised alg. alternative exploration method given alg. samples action probability equal probability expected reward highest. perhaps viewed crude approximation alg. advantage extremely simple. small experiment performed n-armed bandit problem rewards drawn bernoulli distribution. alg. used agreement distribution. compared alg. perhaps viewed crude approximation alg. performance ǫ-greedy action selection evaluated reference. ǫ-greedy algorithm used point estimates updated gradient descent step size action-reward observation tuple initial estimates uniformly distributed cases complete distribution maintained population point estimates point estimate population maintained manner single point estimates ǫ-greedy approach. sampling actions performed sampling uniformly members population action. results diﬀerent bandit tasks arms averaged runs summarised fig. expected reward bandit sampled uniformly seen ﬁgure ǫ-greedy approach performs relatively well used reasonable ﬁrst initial estimates. sampling greedy approach complexity appears perform better asymptotically. importantly alg. exhibits better long-term versus short-term performance eﬀective reward horizon increased fig. average reward multi-armed bandit task averaged experiments smoothed moving average time-steps. results shown ǫ-greedy sampling-greedy alg. paper presented formulation optimal exploration-exploitation threshold n-armed bandit task links need exploration eﬀective reward horizon model uncertainty. additionally practical algorithm based optimistic bound value exploration introduced. experimental results show algorithm exhibits expected long-term versus short-term performance trade-oﬀ eﬀective reward horizon increased. formulation well within reinforcement learning framework useful formulations exist. budgeted learning exploratory action results ﬁxed cost. formulation used bandit problem active learning case). problem essentially becomes best sample actions next moves expected return optimal policy moves maximised corresponds framework presented paper. alternative described stop exploring parts state-action space lead sub-optimal returns high probability. distribution conﬁdence interval available expected returns common optimistic side conﬁdence interval action selection practice partially justiﬁed framework presented herein alternatively considering maximising expected information gained exploration proposed similar manner methods represent uncertainty simple additive factor normal expected reward estimates acquire meaning viewed statistical decision making framework. example dyna-q+ algorithm chap. includes slowly increasing exploration bonus state-action pairs recently explored. statistical viewpoint general conditions deﬁned sec. require maintaining type belief distribution expected return actions natural choice would fully analytical bayesian framework. unfortunately makes diﬃcult calculate might better consider simple numerical approaches outset. previously considered simple estimates relied estimating gradient expected return respect parameters. estimated gradient used measure uncertainty. research populationbased methods explicitly representing distribution estimates currently way.", "year": 2006}