{"title": "Residual Networks: Lyapunov Stability and Convex Decomposition", "tag": ["cs.LG", "cs.NE", "math.DS", "math.OC", "stat.ML"], "abstract": "While training error of most deep neural networks degrades as the depth of the network increases, residual networks appear to be an exception. We show that the main reason for this is the Lyapunov stability of the gradient descent algorithm: for an arbitrarily chosen step size, the equilibria of the gradient descent are most likely to remain stable for the parametrization of residual networks. We then present an architecture with a pair of residual networks to approximate a large class of functions by decomposing them into a convex and a concave part. Some parameters of this model are shown to change little during training, and this imperfect optimization prevents overfitting the data and leads to solutions with small Lipschitz constants, while providing clues about the generalization of other deep networks.", "text": "training error deep neural networks degrades depth network increases residual networks appear exception. show main reason lyapunov stability gradient descent algorithm arbitrarily chosen step size equilibria gradient descent likely remain stable parametrization residual networks. present architecture pair residual networks approximate large class functions decomposing convex concave part. parameters model shown change little training imperfect optimization prevents overﬁtting data leads solutions small lipschitz constants providing clues generalization deep networks. neural network architectures expressiveness network improves depth increases. however training test errors networks shown deteriorate practice depth exceeds layers discrepancy indicates problem method used train networks. given gradientbased iterative algorithms almost exclusive choice training likely cause poor stability dynamical systems created algorithms sense lyapunov. substantiated sharp falls observed training error time-varying step size used known sensitivity loss function respect different parameters become disproportionate training neural network. example gradients might vanish others explode prevents effective training. remedy changing geometry optimization suggested regularized descent algorithm introduced algorithm shown converge fewer iterations data required computation scaling constant node network time step depended almost nodes network fully connected. residual networks appear exception training error improves depth even standard gradient method used explain different behavior linear versions networks shown possess crucial properties optimization. particular shown local optima linear residual networks also global optima gradient cost function vanish away local optima later equivalent results derived conditions nonlinear residual networks well lyapunov stability used past understand improve training neural networks success problems state-of-the-art networks analyzed perspective. paper show residual networks indeed right architecture trained gradient-based algorithms terms lyapunov stability. precisely show given arbitrary step size equilibria gradient descent algorithm likely remain stable sense lyapunov residual networks. also reveal equilibria gradient descent algorithm could unstable deep neural networks case algorithm might approach optimum converge even algorithm stochastic thereby providing level regularization. result fundamentally different previous works address whether local optima actually achieved gradient-based methods rather well local optima are. seen example exists critical value step size gradient descent algorithm becomes unstable iterations cannot converge local optimum. critical value equilibria linear system varies dynamical system nonlinear. since gradient descent deep networks leads nonlinear dynamics well equilibria might become unstable others still stable step size following proposition shows. proposition nonzero linear function i.e. given points {xi}i∈ assume estimated multiplication scalar parameters {wj}j∈ minimizing denotes higher order terms argument. lyapunov’s indirect method stability equilibrium nonlinear system asymptotically stable linear approximation around equilibrium unstable. therefore converges zero introduce architecture pair residual networks used approximate large class functions decomposing convex concave part. decomposition elucidates layer improves approximation provides interpretable model. analyze properties local optima show bias parameters likely change little during training. though seems like problem fact prevents overﬁtting leads solutions lipschitz constants also associated generalization. claims veriﬁed testing suggested model mnist data explicit regularization. organization rest paper follows. stability analysis given section model decomposition introduced section properties model derived section results test mnist data given section lastly results discussed future directions provided section satisﬁes equilibrium point said stable sense lyapunov every exists implies state system close equilibrium initially always stays close equilibrium shown figure addition state converges equilibrium said asymptotically stable sense lyapunov. even though evolution element seems depend entries matrices system described decomposes independent systems assumptions given theorem theorem denote spectral radius matrix assume diagonalizable real eigenvalues initial matrices identical eigenspaces matrix step size satisﬁes proof. exists common invertible matrix rn×n diagonalize matrices λrm− λwim− update rule turns independent update rules diagonal elements {λwi}i∈. proposition systems converge proposition shows arbitrarily chosen equilibria |λ|/l likely stable. suggests example known positive large setting good choice initialization. fact gradient descent converges exponentially fast initialization chosen appropriately shown next. proposition assume step size chosen less equal always larger always smaller distance decreases factor step. since monotonic function condition holds holds gives identical result also holds |wi| cardinality odd. however without knowing sign cannot decide whether include initialization. proof. bringing update equation diagonal form proposition applied systems involving diagonal elements. since proposition monotonically decreasing bound maximum eigenvalue guarantees linear convergence. matrix identity eigenvectors eigenspaces update rules gradient descent remain coupled dynamics parameters become complex. furthermore stochastic gradient method used update rules still decouple even input points {xi}i∈ orthonormal unless lies eigenspace case taking batch points {xj}j∈j step shown matrices {wi}i∈ close identity gradient cannot vanish unless close also seen however stability gradient descent algorithm addressed. proofs theorem theorem observe keeping eigenvalues matrices {wi}i∈ close other possible step size maintain stability gradient descent providing effective convergence rate. note used distinct unitary matrices instead identity gradients would still vanish therefore every local optima would still global optima. addition gradients respect different parameters would likely become disproportionate since unitary matrices amplify attenuate eigenvectors matrices either. therefore using unitary matrices instead identity could possibly yield results comparable residual networks although dynamics parameters would harder analyze. previous section seen parametrization linear residual networks well suited optimization gradient descent. section show large class functions actually decomposed parts approximated residual network. following theorem provides sufﬁcient conditions function written sequence functions close identity. theorem consider function bounded domain. suppose theorem provides existence result function space without assuming ﬁxed structure estimator. neural network used example width network might need large certain nonlinearity might needed layer depending function estimated. sequel show residual network contains rectiﬁed linear units nonlinearities could used approximate strictly convex functions. note twice differentiable strictly convex function bounded domain gradient satisﬁes conditions theorem therefore represent residual network. first consider univariate function continuously differentiable strictly convex domain strictly positive derivative i.e. ﬁrst order approximation around however given strictly convex approximation underestimates function particularly larger values increase estimate derivative estimate larger values instead denotes max{ estimate strictly increasing gets large derivative estimate gradually increases provided large. result provides better estimate original function. vector positive elements positive constant large enough make hessian positive deﬁnite everywhere domain then pair residual networks used approximate coordinate consequently pairs residual networks used approximate architecture tested mnist data section given single residual network shown perform well practice could question necessity decomposing functions parts. similar decomposition linear mappings shown section necessary improve convergence parameters. section theorem stated linear mappings positive eigenvalues. even though could argue might well initialize diagonal elements weight matrices estimate negative eigenvalues possible without knowing signs eigenvalues priori. hand weight matrices initialized identity diagonal elements corresponding negative eigenvalues converge negative value shown next. proposition assume used initialize gradient descent algorithm solve bi]+ rn×mi rn×mi nonnegative elements positive elements function described convex because every coordinate obtained taking nonnegative combination pointwise maximum coordinates operations preserve convexity note level curves shown figure resemble level curves bowl-shaped function. used different matrix instead identity matrix would columns determine normals lines beyond gradient incremented elements determine distance lines origin. even though sequence functions given shown represent convex functions used building block represent much broader class functions. show this ×···× twice-differentiable function denote coordinate written bi]+ strictly positive entries rn×n nonnegative entries given points {xi}i∈ labels {yi}i∈ assume train network described minimizing mean squared error rn×n minimization matrices zi}i∈. gradient descent algorithm initialized identity matrices expect convergence training error double network faster single network. conﬁrm this generated random diagonalizable matrices random eigenvectors drawn normal distribution random eigenvalues drawn uniform distribution compared training errors networks choosing expected convergence rate double network consistently better single network matrices generated. also observed gradient descent single network became unstable matrices algorithm remained stable double network. figure shows typical comparison training error networks. described layers scalar weight parameters. initial values {bi} drawn uniform distribution estimators respectively. figure shows function estimates obtained nesterov’s accelerated gradient descent algorithm figure estimates network architecture different initialization bias parameters. depending initialization bias parameters estimate fails data perfectly case provides estimate lipschitz constant. initialized larger range bias parameters data points. estimate hand fails data perfectly even though network layers could segments. nevertheless region fails data close linear estimate points belonging region. consequently lipschitz constant estimate certain point either less original function. small lipschitz constant correspondingly small spectral norm estimator indicator excess risk therefore expect estimator decomposed structure generalize well conﬁrmed mnist data next section. tested decomposed model introduced section mnist data contains images handwritten ﬁgures since classes constructed pairs residual networks total. instead feeding images networks ﬁrst used convolutional layer ﬁlters size activated deactivated simultaneously points. result afﬁne piece likely solution weighted-least-squares problem points corresponding piece. bias parameters distributed spanned vectors error vector orthogonal estimator data points perfectly. however unlikely happen optimization bias parameters. gradient cost function respect around local optimum indicator function. number layers network large expected close residual network. addition points relu function inactive contribute gradient. result gradient cost function respect bias parameters likely vanish quickly consequently bias parameters change little training. suggests ﬁnal values bias parameters heavily depend initialization. verify claims trained estimators identical architectures approximate piecewise afﬁne function whose derivative extract edges features reduced dimension output layer taking maximum every non-overlaping window. output layer given input residual networks layers. trained network epochs recorded accuracy training test data epoch plotted figure number parameters much larger standard networks since used residual networks total. furthermore explicit regularization methods drop-out batch-normalization used. nevertheless training test errors remarkably close throughout training. epoch training test accuracy respectively. analyzing dynamics optimization algorithms crucial complete understanding deep neural networks. step size gradient descent example factor lyapunov stability equilibria disregarded determines whether algorithm converge local optima. taking effect step size account showed local optima deep linear networks actually cannot discovered gradient-based methods even though known global optimum similarly equilibria networks could also unstable given step size gradient-based algorithms might close local minimum converge even algorithm stochastic. consequently inexact approximate solution obtained optimization problem naturally contributes also observed cost function used training residual network easily become insensitive bias parameters ﬁnal values parameters might heavily depend initialization. though seems like problem another factor contributing generalization. happens bias parameters residual networks could happen weight parameters well types networks already know vanishing gradient problem. respect hardness optimization provides level regularization deep neural networks. showed parametrization residual networks allows equilibria gradient descent algorithm remain stable thereby facilitating optimization. unitary matrices used instead identity similar results could possibly obtained. addition using orthonormal data points step gradient descent seen help decouple dynamics parameters. might partial explanation improvements provided using batch-normalization practice proposed network architecture provides understanding layer improves approximation deep neural network. chose convexity property decompose functions parts. fact function layer remained invertible decomposition critical. decompositions could alternatively generated tested. showed architecture introduced generalizes well mnist data set. training however slow large number parameters small gradients bias parameters. convergence could possibly improved using alternatives gradient-based algorithms update bias parameters although might risk overﬁtting data. known deep linear networks produce solutions small lipschitz constants conditions lipschitz constant estimators could used explain generalization demonstrated solutions obtained residual networks also property hence generalize well. lastly enlarging region attraction equilibria choosing speciﬁc control standard problem nonlinear control theory. finding state dependent step size improve convergence gradient descent neural networks ongoing work. zhang chiyuan bengio samy hardt moritz recht benjamin vinyals oriol. understanding deep learning requires rethinking generalization. international conference learning representations references bartlett peter evans steve long phil. representational optimization properties deep residual networks https//simons.berkeley. edu/talks/peter-bartlett---. gunasekar suriya woodworth blake bhojanapalli srinadh neyshabur behnam srebro nati. implicit advances regularization matrix factorization. neural information processing systems ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing internal covariate shift. proceedings international conference machine learning zhihong hong sophie xinghuo. adaptive backpropagation algorithm based lyapunov stability theory neural networks. ieee transactions neural networks", "year": 2018}