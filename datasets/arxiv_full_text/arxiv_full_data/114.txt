{"title": "Sequence-to-Sequence Learning as Beam-Search Optimization", "tag": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beam-search training scheme, based on the work of Daume III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation.", "text": "sequence-to-sequence modeling rapidly become important generalpurpose tool proven effective many text-generation sequence-labeling tasks. seqseq builds deep neural language modeling inherits remarkable accuracy estimating local next-word distributions. work introduce model beamsearch training scheme based work daum´e marcu extends seqseq learn global sequence scores. structured approach avoids classical biases associated local training uniﬁes training loss test-time usage preserving proven model architecture seqseq efﬁcient training approach. show system outperforms highlyoptimized attention-based seqseq system baselines three different sequence sequence tasks word ordering parsing machine translation. sequence-to-sequence learning deep neural networks rapidly become useful surprisingly general-purpose tool natural language processing. addition demonstrating impressive results machine translation roughly model training also proven useful sentence compression parsing dialogue systems additionally underlie dominant approach training seqseq system conditional language model training maximizing likelihood successive target word conditioned input sequence gold history target words. thus training uses strictly word-level loss usually cross-entropy target vocabulary. approach proven effective efﬁcient training neural language models seqseq models similarly obtain impressive perplexities word-generation tasks. notably however seqseq models used conditional language models test-time; must instead generate fully-formed word sequences. practice generation accomplished searching output sequences greedily beam search. context ranzato note combination training generation scheme described leads least major issues work develop non-probabilistic variant seqseq model assign score possible target sequence propose training procedure inspired learning search optimization framework daum´e marcu deﬁnes loss function terms errors made beam search. furthermore provide efﬁcient algorithm backpropagate beam-search procedure during seqseq training. approach offers possible solution three aforementioned issues largely maintaining model architecture training efﬁciency standard seqseq learning. moreover scoring sequences rather words approach also allows enforcing hard-constraints sequence generation training time. test effectiveness proposed approach develop general-purpose seqseq system beam search optimization. experiments three different problems word ordering syntactic parsing machine translation compare highlytuned seqseq system attention version beam search optimization shows signiﬁcant improvements three tasks particular improvements tasks require difﬁcult search. issues exposure bias label bias received much attention authors structured prediction community brieﬂy review work here. prominent approach combating exposure bias searn meta-training algorithm learns search policy form cost-sensitive classiﬁer trained examples generated interpolation oracle policy model’s current policy. thus searn explicitly targets mismatch oracular training non-oracular test-time inference training output model’s policy. dagger similar approach differs terms training examples generated aggregated additionally important reﬁnements style training past several years comes training rnns searn/dagger applied name scheduled sampling involves training generate token target sequence consuming either true t’th token probability increases throughout training predicted t’th token. uncommon beam search training searn/dagger. early-update laso training strategies however explicitly account beam search describe strategies updating parameters gold structure becomes unreachable search. early update laso differ primarily former discards training example ﬁrst search error whereas laso resumes searching error state includes gold partial structure. context feed-forward neural network training early update training recently explored feedforward setting zhou andor work differs adopt laso-like paradigm apply training seqseq rnns also note watanabe sumita apply maximumviolation training similar early-update parsing model recurrent components yazdani henderson beam-search training discriminative locally normalized dependency parser recurrent components. recently authors also proposed alleviating exposure bias using techniques reinforcement learning. ranzato follow approach train decoders seqseq model obtain consistent improvements performance even models trained scheduled sampling. daum´e marcu note laso similar reinforcement learning except require exploration way. exploration unnecessary supervised text-generation since typically know gold partial sequences time-step. shen minimum risk training simply note models typically locally normalized unaware speciﬁcally seqseq work rnns locally-normalized scores. model introduce here however locally normalized suffer label bias. also note exceptions trend locally normalized rnns work voigtlaender train lstms context hmms speech recognition using sequence-level objectives; work consider search however. simplest seqseq scenario given collection source-target sequence pairs tasked learning generate target sequences source sequences. instance might view machine translation particular attempt generate english sentences french sentences. seqseq models part broader class encoder-decoder models ﬁrst encoding model transform source object encoded representation many different sequential encoders proven effective different source domains. work agnostic form encoding model simply assume abstract source representation input sequence encoded seqseq models generate target sequence using decoder. decoder tasked generating target sequence words target vocabulary particular words generated sequentially conditioning input representation previously generated words history. notation refer arbitrary word sequence seqseq systems utilize recurrent neural network decoder model. formally recurrent neural network parameterized nonlinear function recursively maps sequence vectors sequence hidden states. sequence vectors initial state vector. applying sequence yields hidden states time-step follows decoders typically trained conditional language models. attempts model probability t’th target word conditioned target history stipulating parameterized function typically computed afﬁne layer followed softmax. computing probabilities state represents target history typically function complete model trained analogously neural language model minimize cross-entropy loss time-step conditioning gold history training data. model trained minimize discrete sequence generation performed approximately maximizing probability target sequence conditional distribution argbeamwt notation argbeam emphasize decoding process requires heuristic search since model non-markovian. practice simple beam search procedure explores prospective histories time-step proven effective decoding approach. however noted above decoding manner conditional languagemodel style training potentially suffers isabove term denotes mistake-speciﬁc cost-function allows scale loss depending severity erroneously predicting assumed return margin quirement satisﬁed positive number otherwise. term allows sequencerather word-level costs training instance training seqseq model machine translation desirable inversely related partial sentence-level bleu score experiment along lines section optimize loss using two-step process forward pass compute candidate sets record margin violations backward pass backpropagate errors seqseq rnns. unlike standard seqseq training ﬁrst-step requires running search margin violations. second step done adapting back-propagation time next discuss details process. begin making small change seqseq modeling framework. instead predicting probability next word instead learn produce scores ranking sequences. deﬁne score sequence consisting history followed single word parameterized function examining current hidden-state relevant time well input representation experiments identical form without ﬁnal softmax transformation thereby allowing model avoid issues associated label bias problem. importantly also modify model trained. ideally would train comparing gold sequence highest-scoring complete sequence. however ﬁnding argmax sequence according model intractable propose adopt laso-like scheme train refer beam search optimization particular deﬁne loss penalizes gold sequence falling beam training. proposed training approach simple expose model incorrect histories match training procedure test generation. furthershow implemented efﬁciently without changing asymptotic run-time training beyond factor beam size search-based loss formalize notion search-based loss training. assume candidate sequences length calculate score sequence using scoring function parameterized above k’th ranked deﬁne sequence using non-probabilistic model allows incur loss gold sequence beam; contrasts models based loss andor zhou though training models simply updated gold sequence remains beam. time step margin violations. follow laso build candidates recursive margin violation manner. constructed using standard beam search update. margin violation constructed best sequences assuming gold history time-step formally assume function succ maps sequence valid sequences length formed appending valid word simplest unconstrained case important aside note problems preferable deﬁne succ function imposes hard constraints successor sequences. instance would like seqseq models parsing hard constraints sequences model output namely represent valid parses. hard constraints would difﬁcult standard seqseq training time framework naturally added succ function allowing train hard constraints; experiment along lines section refer model trained constrained beam search conbso. margin violation topk considers scores given search procedure illustrated portion figure forward pass training algorithm shown ﬁrst part algorithm version beam search collect sequences hidden states lead losses. found training early-update rather laso work well even pre-training. given success early-update many tasks somesurprising. leave question future work. figure possible formed training beam size gold sequence runs quickly today. gold sequence highlighted yellow predicted preﬁxes involved margin violations gray. note time-step uses different loss criterion. bottom preﬁxes actually participate loss arranged illustrate back-propagation process. backward merge sequences collected margin violations backpropagation compute parameter updates. assume margin violation occurs time-step between predicted history gold history standard seqseq training must back-propagate error gold history; however unlike seqseq also gradient wrongly predicted history. determining total computational cost back-propagation here ﬁrst note worst case violation time-step leads independent incorrect sequences. since need call brnn times sequence naive strategy running bptt incorrect sequence would lead backward pass rather time required standard seqseq approach. informally illustrate process figure diagram shows possible sequence formed search beam size target sequence runs quickly today. gold sequence falls beam search resumes succ subsequent predicted sequences preﬁx thus functions moreover because loss function involves scores gold preﬁx violating preﬁx relatively simple computation tree shown bottom figure evident backpropagate single pass accumulating gradients sequences diverge gold time-step precedes divergence. second half algorithm shows explicitly single sequence though straightforward extend algorithm operate batch. method describe applies seqseq rnns general experiments global attention model luong consists lstm encoder lstm decoder global attention model baseline seqseq model model computes sequence-scores luong also input feeding involves feeding attention distribution previous time-step decoder current step. model architecture found highly performant neural machine translation seqseq tasks. also note update parameters search step training procedure differs slightly laso aspect essentially equivalent delayed laso update bj¨orkelund kuhn distinguish models refer system baseline seqseq. apply constrained training refer model conbso. providing results also distinguish beam size model trained beam size used test-time. general plan evaluating beam size makes sense train beam size since objective requires gold sequence scored higher last sequence beam. detail additional techniques found necessary ensure model learned effectively. first found model failed learn trained random initialization. therefore found necessary pre-train model using standard word-level cross-entropy loss described secsimilarly clear smaller beam used training less room model make erroneous predictions without running afoul margin loss. accordingly also found useful curriculum beam strategy training whereby size beam increased gradually training. particular given desired training beam size began training beam size increased every epochs reaching ktr. experiments trained seqseq models mini-batch adagrad renormalized gradients exceed updating parameters. extensively tune learning-rates found initial rates encoder decoder lstms rate ﬁnal linear layer work well across tasks considered. code implementing experiments described found https//github.com/ harvardnlp/bso. tasks results experiments primarily intended evaluate effectiveness beam search optimization standard seqseq training. such experiments model across three difhowever important ensure mask applied time-step forward search also applied corresponding step backward pass. accomplish pre-computing masks time-step sharing partial sequence lstms. ferent problems word ordering dependency parsing machine translation. include features extensions necessary reach state-of-the-art performance even baseline seqseq model generally quite performant. word ordering task correctly ordering words shufﬂed sentence recently gained attention test capabilities text-generation systems cast task seqseq problem viewing shufﬂed sentence source sentence correctly ordered sentence target. word ordering somewhat synthetic task interesting properties purposes. first task plausibly requires search second clear hard constraint output sequences namely permutation source sequence. baseline models enforce constraint testtime. however also experiment constraining model training described section deﬁning succ function allow successor sequences containing un-used words source sentence. experiments dataset evaluation procedure zhang clark later work performance reported terms bleu score correctly ordered sentences. word-ordering experiments -layer encoder decoder lstms hidden units dropout rate lstm layers. simple costs deﬁning function. inspired similar analysis daum´e marcu examine relationship training conbso table larger hurt greedy inference results continue improve least initially using bigger dependency parsing next apply model dependency parsing also hard constraints plausibly beneﬁts search. treat dependency parsing arc-standard transitions seqseq task attempting source sentence target sequence source sentence words interleaved arc-standard reduce-actions parse. example attempt source sentence standard penn treebank dataset splits stanford dependency labels standard uas/las evaluation metric following chen manning models thus words source decoding actions emitted far; features used. -layer encoder decoder lstms hidden units layer table dependency parsing. uas/las seqseq conbso baselines test set. andor current state-of-the-art model data note beam size obtain ./.. experiments dropout rate lstm layers. replace singleton words training token normalize digits single symbol initialize word embeddings source target words publicly available wordvec embeddings. simple costs deﬁning function. word-ordering case also experiment modifying succ function order train hard constraints namely emitted target sequence valid parse. particular constrain output time-step obey stack constraint ensure words source emitted order. show results test-set table conbso show signiﬁcant improvements seqseq conbso improving improving las. achieve reasonable ﬁnal score lags behind state-of-the-art promising general-purpose word-only model. translation ﬁnally evaluate model small machine translation dataset allows experiment cost function consider baselines attempt mitigate exposure bias seqseq setting. dataset work ranzato uses data german-to-english portion iwslt machine translation evaluation campaign data comes translated talks dataset contains roughly training sentences development sentences test sentences. preprocessing dataset splits ranzato table machine translation experiments test set; results middle line mixer model ranzato sb-∆ indicates sentence bleu costs used deﬁning xent similar seqseq model convolutional encoder simpler attention. trains seqseq scheduled sampling sb-∆ experiments like also single-layer lstm decoder units. also dropout rate lstm layer. emphasize however decoder lstm size ranzato results directly comparable lstm encoder slightly different attention mechanism input feeding yr+t) last margin violation denotes smoothed sentence-level bleu setting penalize erroneous predictions relatively sentence-level bleu score relatively high sentence-level bleu score. table show ﬁnal results ranzato start improved baseline similarly large increases accuracy obtained mixer particular sequence-level costs table compares using sentence-level bleu costs deﬁning using costs. sophisticated sequence-level costs moderate effect bleu score. timing given algorithm would expect training time increase linearly size beam. task highly tuned seqseq baseline processes average tokens/second gpu. beams size implementation processes average tokens/second respectively. thus appear initial constant factor complicated forward backward passes training scales size beam. batch beam predictions however practice training time scales sub-linearly beam-size. introduced variant seqseq associated beam search training scheme addresses exposure bias well label bias moreover allows training sequencelevel cost functions well hard constraints. future work examine scaling approach much larger datasets. thank yoon helpful discussions providing initial seqseq code implementations based. thank allen schmaltz help word ordering experiments. also gratefully acknowledge support google research award.", "year": 2016}