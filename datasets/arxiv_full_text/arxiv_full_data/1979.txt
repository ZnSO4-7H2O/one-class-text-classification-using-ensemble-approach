{"title": "Playing Doom with SLAM-Augmented Deep Reinforcement Learning", "tag": ["cs.AI", "cs.CV", "stat.ML"], "abstract": "A number of recent approaches to policy learning in 2D game domains have been successful going directly from raw input images to actions. However when employed in complex 3D environments, they typically suffer from challenges related to partial observability, combinatorial exploration spaces, path planning, and a scarcity of rewarding scenarios. Inspired from prior work in human cognition that indicates how humans employ a variety of semantic concepts and abstractions (object categories, localisation, etc.) to reason about the world, we build an agent-model that incorporates such abstractions into its policy-learning framework. We augment the raw image input to a Deep Q-Learning Network (DQN), by adding details of objects and structural elements encountered, along with the agent's localisation. The different components are automatically extracted and composed into a topological representation using on-the-fly object detection and 3D-scene reconstruction.We evaluate the efficacy of our approach in Doom, a 3D first-person combat game that exhibits a number of challenges discussed, and show that our augmented framework consistently learns better, more effective policies.", "text": "number recent approaches policy learning game domains successful going directly input images actions. however employed complex environments typically suffer challenges related partial observability combinatorial exploration spaces path planning scarcity rewarding scenarios. inspired prior work human cognition indicates humans employ variety semantic concepts abstractions reason world build agent-model incorporates abstractions policy-learning framework. augment image input deep qlearning network adding details objects structural elements encountered along agent’s localisation. different components automatically extracted composed topological representation using on-the-ﬂy object detection d-scene reconstruction. evaluate efﬁcacy approach doom ﬁrst-person combat game exhibits number challenges discussed show augmented framework consistently learns better effective policies. recent approaches policy learning games shown great promise success number different scenarios. particular feature approaches ability take visual game state directly input learn mapping actions agent effectively explores world solves predetermined tasks. success largely made possible thanks ability deep reinforcement learning networks neural networks acting function approximators within reinforcement-learning framework. particular variant deep q-learning networks widely used range different settings excellent results. employs convolutional neural networks building block effectively extract features observed input images subsequently learning policies using features. figure motivation agent explores environment ﬁrst-person-view sees restricted portion scene whereas semantic effect exploration cumulative indicating type position. majority scenarios tackled thus common characteristic domain -dimensional. here going directly input image pixels learned policy works well important factors reasonable amount game’s state directly observable image combination lowerdimensional action space smaller exploration requirements result smaller search space. former ensures feature extraction always sufﬁcient information inﬂuence policy learning latter makes learning consistent features easier. despite stellar success domain models struggle complicated domains games. domains exhibit multitude challenges cause standard approaches discussed struggle. introduction additional spatial dimension ﬁrst introduces notions partial observability occlusions secondly causes complications viewpoint variance. agent viewing relatively smaller portion environment also must reconcile observing variety objects different contexts projective transformations. furthermore adding extra dimension also combinatorially complicates matters terms exploration environment. typically manifests form sparse feedback learning process agent’s inability explore environment directly penalizes learning capacity. moreover complications exploration directly affect planning required tasks actions. finally larger search state spaces comes likelihood rewards might help move learning along also harder come sutton propose extension framework potentially learn hierarchical policies. however this similar methods able scale beyond small gridworld domains kulkarni proposes tackle environments delayed rewards coupling options learning intrinsically driven exploration methods. options however notoriously hard train requiring great deal effort intrinsically motivated agents safely deal generic hierarchical spatial domains. prior work behavioural modelling cognitive neuroscience suggests humans employ particular highly specialised mechanisms construct representations reason about world. typically take form semantic concepts abstractions object identity categories localisation. freedman miller review evidence neurophysiology explore learning representation object categories. burgess discusses evidence neuroscience presence combination different viewpoints role representing layouts spatial cognition process. moser also discuss presence highly specialised representation regions brain encode localisation spatial reasoning. denis loomis provide review behavioural psychology subject spatial cognition related topics. paper take inspiration work propose system explicitly constructs joint semantic topological representation state augment input representation attempt learn policies effectively complex domains. construct novel model incorporates automatic on-the-ﬂy scene reconstruction component standard deep-reinforcement learning framework. work provides streamlined system immediately enhance current state-of-the-art learning algorithms spatial domains additionally obtaining insight efﬁcacy spatially enhanced representations learned purely bottom-up manner. interactive decision making. mathematical framework based markov decision processes tuple states actions agent take time step transition probability going state using action reward function deﬁning signal agent receives taking actions changing states discount factor. goal reinforcement learning learn policy maximises expected discounted average reward agent run. commonly used technique learn policy learn actionvalue function iteratively gradually approximate expected reward model-free fashion. have however traditionally struggled deal high-dimensional environments large part curse dimensionality. deep reinforcement learning algorithms deep-q networks extend model-free algorithms like q-learning deep neural networks function approximators implicitly capturing hierarchies state representation make problem scale even visual input states. unfortunately still suffer problems standard cannot deal with delayed reward signals require non-stochastic explo learning abstract policies hierarchically currently unsolved problem make scale tasks requiring long-term planning partial observability state requires models encode least short-term memory specially training end-to-end recent work also explored ways develop agents learn play doom. lample chaplot take approach using variant drqn together game features extracted directly game environment data structures. method similar spirit applied environment signiﬁcant navigation component slam object recognition pipeline intrinsically dependent vizdoom platform. another interesting approach presented dosovitskiy koltun approach featured winner vizdoom competition changed supervision signal single scalar reward vector measurements provided game engine. used train network that given visual input current measurements goal predicts future measurements. action perform chosen greedily according predicted future measurements. orthogonal approach; algorithms could beneﬁt novelties introduced other however leave extension part future. early approaches camera-pose estimation relied matching limited number sparse feature points between consecutive video frames common drawback solutions relatively quick error accumulation results signiﬁcant camera drift. addressed ptam achieved globally optimal solutions real-time rates running bundle adjustment background thread. improvements include re-localization loop-closure detection faster matching binary features recently matching hand-crafted features replaced semi-dense direct methods however approaches provide limited information environment. complete representation provided dense approaches estimate dense depth monocular camera input regular voxel grid limits reconstruction small volumes memory requirements. kinectfusion-based approaches sense depth measurements directly using active sensors fuse time recover high-quality surfaces suffer issue. drawback since removed scalable approaches allocate space voxels fall within small distance perceived surfaces avoid storing unnecessary data free space approaches mentioned assume observed scenes static. assumption relaxed full slam generalized objects efﬁcient variants beyond scope paper. approaches ratslam derivatives propose instances slam based models inspired biological agents showing promising results environments navigation task requires reasoning landmarks non-cartesian grid representations. object detection early approaches object detection include constellation models pictorial structures ﬁrst object detector capable real-time detection rates solved inherent problem sliding-window approaches learning sequential decision process rapidly rejects locations unlikely contain objects. concept since evolved distinct algorithms called proposals whose goal quickly localize potential objects locations complex classiﬁers determine class label deformable part models prominent example such able represent highly variable object classes. recently shown deformable part models interpreted convolutional network replacement handcrafted features convolutional feature maps finally faster-rcnn combines region proposals object detector single uniﬁed network trainable end-to-end shared convolutional features leads fast detection rates. paper introduce algorithm based deep network successfully applied many atari games inspired prior work human cognition indicates humans employ variety semantic concepts abstractions reason world build agentmodel incorporates abstractions policylearning framework. augment ﬁrst-person image input adding details objects structural elements encountered along agents localization cope complex environments. represented encoding three distinct sources information positions static structures obstacles walls position orientation agent iii) positions class labels important objects health packs weapons enemies. representation updated time agent explores environment. allows agent keep information areas observed past build aggregated model environment indicated fig. representation allows agent behave properly even respect elements longer present ﬁrst-person view. semantic representation. agent explores environment simultaneously estimate localization agent obstacles order build surrounding environment ﬁrst-person-view frame. parallel detect important objects scene weapons ammunition. since want minimize dimensionality augmented representation allow efﬁcient learning project semantic information onto single common ﬁxed size. essentially ﬂoor-plan positions objects agents. achieved encoding different entities different gray-scale values form heatmaps representation encodes position walls obstacles extracted directly depth data provided vizdoom api. information agent’s position orientation represented green directed arrow. also want provide agent semantic information variety objects present environment. doom encode following object categories monsters health packs high-grade weapons high-grade ammunition weapons ammunition figure system overview observing image depth vizdoom. running faster-rcnn object detection slam pose estimation. reconstruction using pose bounding boxes. semantic maps built projection trained using inputs. since objects could either move picked another player project objects visible current view onto common map. could addressed advanced data association techniques beyond scope paper. here describe process automatically creating semantic maps on-the-ﬂy. fig. depicts architecture pipeline. input image data provided vizdoom i.e. video frames visualizing environment agents perspective z-buffer providing depth information observed scene. order build environment need detect remove objects z-buffer since want provide explicit semantic information various objects avoid nuisance visual events weapon discharges depth buffer. also need know current pose camera camera-pose tracker parallel object detector. then project observed scene common provide visualization agent. note mapping system could work even without access z-buffer i.e. using solely data describe components pipeline greater detail. object detection detect objects faster-rcnn object detector convolutional network combines attention mechanism object detector single uniﬁed network trainable end-toend. ﬁrst module deep fully-convolutional network simultaneously predicts object bounds objectness scores position second module fast r-cnn detector uses proposed regions. since modules share features offers fast detection rates. input image resized standard resolution pixels. next image pushed network convolutional feature extracted. model zeiler fergus extract feature maps. generate region proposals feature processed sliding-window manner fully-connected layers predicting position region proposal binary class label indicating objectness. region proposal corresponding feature maps fully-connected layers units produce soft-max probabilities object classes positions bounding boxes detected objects. trained object detector classes corresponding objects monsters projected onto common map. despite using ground-truth depth maps provided z-buffer icp-like approaches work well game environments since environments lack many geometrical features hence sparse feature-based orb-slam -dof camera-pose estimation input images downsampled pixels z-buffer. first build eight-level image pyramid scale factor then extract sparse local features representing corner-like structures. this oriented multi-scale fast detector adaptivelychosen threshold detect sufﬁcient number features. feature extraction step biased bucketing ensure features uniformly distributed across space scale constant-velocity motion model predicting camera pose used constrain matching onto local search windows. extracted features associated local binary-patterns matched using mutual-consistency check. robust estimate performed ransac least-squares reﬁnement inliers. robustness increased keyframes reduce drift camera viewpoint change signiﬁcantly. tracking lost current frame converted bag-of-words queried database keyframe candidates global re-localization. camera re-localized using algorithm ransac. global consistency achieved loop-closing pose-graph optimization distributes loop-closing error along graph background thread camera poses object-masked depth project current frame common map. frame back-project image pixels current camera reference frame obtain vertex here denotes inverse camera calibration matrix denote image pixels homogeneous coordinates depth. also want maintain previously-visited areas memory project cammogenized) vertex global reference frame rigid body transfortgk mation mapping camera coordinate frame time global frame since ﬁxed volumetric representation severely limits reconstruction size handled hash-based method resulting generated placing virtual camera top-down view ignoring points height thresholds remove areas would otherwise occlude ceilings ﬂoors. section demonstrate advantage adding semantic presented sec. standard ﬁrstperson view working inside doom environment. code results experiments made available online. vizdoom platform experiments. built ﬁrst person combat game doom allows easy synchronous control original game execution user-controlled getting table best mean test rewards different frameworks run. note pipeline performs strongly comparison baselines ablated versions considered. also note although best artiﬁcial systems considered pipeline closer others. ﬁrst-person-view engine current step stepping forward sending keystrokes. environment player performs speciﬁed scenario. paper focus deathmatch scenario simple arena seen fig. goal eliminate many opponents possible eliminated. proﬁcient agent scenario would efﬁcient eliminating enemies whilst able collect effective weapons keep health high possible. scenario basis competition different autonomous agent competed deathmatch tournament. quantitative results experiments carried summarised tab. individual features experiments insights obtained runs described subsequent sections following detailed discussion various components framework. recognition reconstruction. described sec. faster-rcnn detector feed image given platform. network pre-trained imagenet ﬁne-tuned dataset consisting training validation examples extracted vizdoom engine performing -fold cross-validation. images manually annotated ground-truth bounding boxes corresponding classes monsters health packs high-grade weapons highgrade ammunition weapons/ammunition monsters’ ammunition agent’s ammunition. ﬁne-tuning model achieved average precision reconstruction system presented sec. uses rgbimages provided vizdoom platform. policy learning. framework perform policy learning augmented features. modiﬁcation original algorithm architecture needs able cope extended state. ﬁrst person view images resized pixel converted grayscale normalized. semantic represented single channel image resolution. different object categories encoded different grayscale values. experiments concatenate along channel dimension. network composed convolutional layers respectively output channels ﬁlters sizes strides. fully-connected layer units followed output softmax layer. hidden layers followed rectiﬁed linear units adding associated image changes input channels ﬁrst convolutional layer thus increase number parameters increase. training hyper-parameters rmsprop experiments. action space. action space environment order magnitude larger atari environment. indeed doom accepts combination unique keystrokes input. following observation human player uses small subset combinations play game recorded actions performed humans selected representative subset. actions divided three groups actions corresponding single keystroke allowing agent move shoot combinations keystrokes corresponding moving shooting time iii) actions associated switching weapons. arbitrarily chose actions performed humans categorising groups mentioned above. primarily constrain action space reasonably tractable size still maintaining richness actions could performed environment. reward function. reward function designed capture primary goal agent eliminate opponents. represent indicator variable opponent eliminated since last step. encourage agent live longer also consider health variation current step previous step. explicitly structure health reward zero-sum order remove biases towards preserving health detriment primary goal. reward incorporating terms written reward function allows observing agent’s behaviour respect primary objective. second reported metric number steps agent lived. important living increases agent’s chance kill opponents increase reward longer term. reported metrics mean values test games. time complexity. complete framework fast enough allow playing game’s native speed. object detector parallel camerapose estimation. average detector requires process image camera-pose estimation latency take respectively. semantic construction takes training requires process frame perform learning step. complete pipeline able process average images second. given inside vizdoom platform step represents frames game system plays approximately frames second exceeds typical demands gameplay. experiments intel core machine nvidia titan gpu. tion pipeline extracting ground-truth information classes positions objects used semantic representation. words experiment presents results would perfect detection reconstruction used oracle. baseline standard approach trained solely ﬁrst person view images baseline compared model trained both ﬁrst person view encoding ground-truth walls player position model trained both ﬁrst person view augmented complete maps containing ground-truth walls positions player objects. seen fig. baseline able learn good policy model semantic maps. moreover baseline model quickly reaches plateau improve afterwards. adding environment allows agent learn signiﬁcantly better policy reward almost doubled compared baseline. adding objects seen agent onto gives another signiﬁcant improvement leading reward compared achieved baseline. moreover network provided complete able learn faster models provided fewer information. result proves providing higher level complex representation surrounding agent allows learn faster converge better policy. figure maps column taken oracle. maps bottom oracle noise player objects’ positions. oracle noise walls. semantic reconstructed independent oracle pipeline. unfortunately detection reconstruction pipelines often imperfect real world scenarios. next study impact providing poor spatial representation agent. that signiﬁcant amount noise first consider case gaussian noise agent’s objects’ positions referenced nosm meaning elements properly positioned respect static objects. fig. shows results adding noise. shown noisy version shown below. thing note maps gray scale pixel values deﬁne different abstractions objects. gray scaled format used training discussed previous sections. next gaussian noise positions walls referenced nosm meaning element appear accessible cannot reached real environment. fig. shows results adding noise. seen fig. high amount noise maps prevent framework learn good policy. however important note worst case noisy version matches performances baseline network learns ignore sec. shown efﬁcacy q-learning ground-truth version semantic maps. proof concept evaluate performance real maps generated on-the-ﬂy approach described sec. experiment allows evaluate quality policy learned using standard detection mapping techniques without extra engineering. words measure drop performance caused imperfect object detection slam real world scenario respect oracle. difference seen fig. here semantic categories coloured instead greyscale levels emphasis. seen fig. reconstructed leads signiﬁcantly better results baseline. even though doesn’t match oracle clearly much closer baseline. remaining reduced progress ﬁeld. combination prioritized experience replay dueling network architecture demonstrated superior results atari games compared vanilla approach baseline considered above. experiment compare successful model basic model augmented semantic maps. basic outperformed dual trained ﬁrst person views. also interesting note approaches orthogonal could combined. leave study future work. mean length seen fig. agent trained semantic maps able typically live longer trained ﬁrst-person view. consequence fact agent inherently attempts build representation environment helps adapt better arbitrary initialisation points. baseline however access capabilities hence performs incoherently situations. keeping general characteristics results seen thus agent typically underperforms relation agent still signiﬁcantly outperforms baseline. proposed augment standard model semantic maps; representation provides aggregated information environment around agent. demonstrated efﬁcacy approach oracle maps automatically reconstructed maps using object detection slam demonstrating efﬁcacy approach standard computer-vision recognition reconstruction pipeline standard off-the-shelf policy learner central thesis exploring beneﬁts semantic representations augmenting directly-from-pixels learning approach typically employed. claim major contributions policy-learning algorithms themselves effort nonetheless provides insight efﬁcacy representations learned purely bottom-up manner. also potentially serves benchmark effectiveness representations learned purely bottom-up manner. moreover approach potential extend scale beyond doom environment virtue applicability environment reasonable number potential entities extractability information. terms future directions would like extend framework along variety different axes. particular direction improving resilience layered environments currently unable represent environments buildings another direction involves relaxing metric constraints maps currently constructed under. better localisation semantic representations could exist necessarily require metric reconstruction perhaps relativistic graph-based approach. ﬁnally interested extending experiments incorporate maps elicit qualitative judgments learned gameplay.", "year": 2016}