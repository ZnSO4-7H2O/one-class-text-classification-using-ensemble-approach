{"title": "Multi-timescale memory dynamics in a reinforcement learning network with  attention-gated memory", "tag": ["q-bio.NC", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Learning and memory are intertwined in our brain and their relationship is at the core of several recent neural network models. In particular, the Attention-Gated MEmory Tagging model (AuGMEnT) is a reinforcement learning network with an emphasis on biological plausibility of memory dynamics and learning. We find that the AuGMEnT network does not solve some hierarchical tasks, where higher-level stimuli have to be maintained over a long time, while lower-level stimuli need to be remembered and forgotten over a shorter timescale. To overcome this limitation, we introduce hybrid AuGMEnT, with leaky or short-timescale and non-leaky or long-timescale units in memory, that allow to exchange lower-level information while maintaining higher-level one, thus solving both hierarchical and distractor tasks.", "text": "learning memory intertwined brain relationship core several recent neural network models. particular attention-gated memory tagging model reinforcement learning network emphasis biological plausibility memory dynamics learning. augment network solve hierarchical tasks higher-level stimuli maintained long time lower-level stimuli need remembered forgotten shorter timescale. overcome limitation introduce hybrid augment leaky short-timescale non-leaky long-timescale units memory allow exchange lower-level information maintaining higher-level thus solving hierarchical distractor tasks. memory spans various timescales plays crucial role human animal learning cognitive neuroscience memory system enables manipulation storage information period seconds called working memory correlated activity prefrontal cortex basal ganglia computational neuroscience several standalone models dynamics also supervised reinforcement learning models augmented working memory memory mechanisms implemented enriching subset artiﬁcial neurons slow time constants gating mechanisms recent memory-augmented neural network models like neural turing machine differentiable neural computer employ addressable memory matrix works repository past experiences neural controller able store retrieve information external memory improve learning performance. here study extend attention-gated memory tagging model augment augment trained reinforcement learning scheme learning based reward signal released response selection. representation stimuli accumulated memory states memory reset trial main advantage augment network computational neuroscience community resides biological plausibility learning algorithm. uses memory-augmented version biologically plausible learning rule mimicking backpropagation learning result joint action factors neuromodulation attentional feedback inﬂuencing synaptic plasticity. former global reward-related signal released homogeneously across network inform synapse reward prediction error response selection neuromodulators dopamine inﬂuence synaptic plasticity novelty augment compared three-factor rules attentional feedback system order keep track synaptic connections cooperated selection winning action overcome so-called structural credit assignment problem augment includes memory system units accumulate activity across several stimuli order solve temporal credit assignment tasks involving delayed reward delivery attentional feedback mechanism augment works with synaptic eligibility traces decay slowly time non-decaying neuronal traces store history stimuli presented network current time augment network solves saccade-antisaccade task equivalent temporal task different decay constants work different temporal scales network learns weight usage based requirements speciﬁc task. simulations employed subgroups cells memory half memory non-leaky leaky uniform decay time constant; however generally hybrid augment architecture contain several subgroups distinct leakage behaviours. section presents architectural mathematical details hybrid augment. section describes simulated results hybrid augment network standard augment network fully leaky control network cognitive tasks nonhierarchical task involving sequence prediction hierarchical task finally section discuss main achievements comparison state-of-the-art models present possible future developments work. network controls agent which time step receives reward response previous action processes next stimulus takes next action figure time step distinguish phases called feedforward pass feedback pass depicted figure case complex tasks long trials multiple stimuli like depicted figure accumulation information augment lead memory interference loss performance. hence question whether modiﬁed augment model would lead broader applicability attention-gated reinforcement learning. propose variant augment network named hybrid augment introduces timescales forgetting leakage memory dynamics overcome kind learning limitation. employ memory units augment information processed network three layers shown left panel figure unit output layer corresponds action. pathways output layer regular branch memory branch. figure overview augment network operation. example trials task task symbols appear sequentially screen organized outer loops start either digit random number inner loops presentation associated target non-target correct response. output correct response coincide agent receives positive reward otherwise gets negative reward figure adapted figure o’reilly frank augment operates discrete time steps comprising reception reward input state stimulus action taken implements state-action-reward-state-action reinforcement learning algorithm. time step reward obtained previous action taken time step network weights updated next action chosen. augment network structured three layers different types units. iteration learning process consists feedforward pass feedback pass feedforward pass sensory information current stimulus bottom layer regular units without memory units memory middle layer whose activities turn weighted compute q-values activity layer. based q-values current action selected reward obtained previous action used compute reward prediction error modiﬁes connection weights contributed selection previous action proportion eligibility traces. this temporal eligibility traces tags connections updated reﬂect correlations current post activities. then feedback pass spatial eligibility traces updated attention-gated current action feedback weights. task allowing time long-time maintenance fast decay information memory. contrast forget gate long short-term memory gated recurrent unit memory leak co-efﬁcient trained gated ﬁxed. branches converge onto output layer. activity output unit index approximates q-value action given input denoted qsa. q-values formally deﬁned future expected discounted reward conditioned stimulus action finally q-values different actions participate \u0001-greedy winner-take-all competition select response network. probability next action maximal q-value memory branch driven transitions between stimuli instead stimuli themselves. sensory input memory branch consists transient units i.e. units encode onset stimus+ units encode offset introduce factor here extension standard augment incorporate decay forgetting time. setting memory state obtain non-leaky memory dynamics original augment network memory composed subpopulations neurons cooperate different ways solve figure architectures standard augment hybrid augment networks. difference networks consist memory dynamics memory layer standard augment conservative units hybrid augment possesses memory composed leaky non-leaky units. augment follows sarsa updating scheme updates q-values previous action taken time action time known q-values depend weights equation temporal difference error deﬁned action chosen current time reward obtained action taken time temporal difference error acts global reinforcement signal modify weights connections learning rate synaptic eligibility traces deﬁned below. superscript denotes regular memory branch respectively. symbol eligibility traces input-to-hidden follow update rule feedforward partner therefore even initializations feedforward feedback weights different strengths become similar learning suggested neurophysiological ﬁndings networks hidden layer onehot coding output attentional feedback equivalent backpropagation show equations eligibility traces tagging along weight update equations reduce rpe-based loss function deﬁned speciﬁcally discuss case tagging equations update rule associated weight sensory input memory contains memory decay factor introduced analogous discussion holds also weights note memory beyond time step i.e. anew time step. nevertheless since depends previous times memory units link across time steps. since activities input potentials quantities available synapse biological synapse implement updates eligibility traces tags locally. emphasize eligibility traces tags interpreted ’hebbian’ correlation detectors. original augment model eligibility traces tags said updated feedback pass. here without changing order operations algorithm conceptually shifted update traces tags depend correlations activities last step feedforward pass activities still available. note activities could principle change attention-gating feedback pass action selection updates weights tags temporal eligibility traces feedforward pass synapses contributed currently selected action update spatial eligibility traces attentional feedback step. synapses input hidden layer equation combined spatial eligibility trace interpreted attentional feedback signal memory cells second half. reduce standard augment network leaky control network general leak co-efﬁcients tuned adapt overall memory dynamics speciﬁc task. augment includes differentiable memory system trained framework learning rules based joint effect synaptic tagging attentional feedback neuromodulation here study proposed variant augment named hybrid augment additional leak factor subset memory units compare original augment control network leaky memory units. ﬁrst step validated implementations standard hybrid augment networks saccade-antisaccade task used reference paper next simulated networks cognitive tasks different structure memory demands sequence prediction task task former agent predict ﬁnal letter sequence depending starting letter latter agent identify target pairs inside sequence hierarchical symbols. s-as task maps temporal task thus hidden layer essential task also resembles structure complex however feedback step weight replaced feedback counterpart discussed above valid approximation become similar learning. finally starting equation write additional dimension distractors inner loop complexity sequence prediction task less compared task effectively solved augment. show hybrid augment performs well cognitive tasks whereas standard augment fails task. parameters involving architecture networks task reported table discuss tasks detail. sequence prediction task letters appear sequentially screen trial agent correctly predict last letter. sequence starts either followed ﬁxed sequence letters trial ends prediction ﬁnal letter depends initial sequence started ﬁnal letter initial ﬁnal letter case correct prediction agent receives reward unit otherwise punished negative reward scheme task presented figure sequences four letters. network learn task given sequence length kept ﬁxed throughout training. agent must learn maintain memory initial sequence trial solve task. time agent learn neglect information coming intermediate cues thus difﬁculty task correlated length sequence. variant sequence prediction task. mean trend rpe-based energy function deﬁned equation shows models converge hundreds iterations. control also simulated variant also memory units leaky. noticed hybrid standard augment networks efﬁcient purely leaky control. surprising point sequence prediction task consists maintenance initial stimulus simpler non-leaky memory leaky one. notice hybrid model behaviour similar augment. also analyzed effect temporal length sequences network performance varying number distractors sequence sequence length network retrained initio. required consecutive correct predictions criterion convergence. simulations starting different initializations sequence length averaged figure convergence sequence prediction task. time course error models sequence prediction task sequences letters mean squared decays zero networks leaky control network much slower augment hybrid augment convergence time augment network variants sequence prediction task increasing number distractors i.e. intermediate cues ﬁnal prediction. figure memory weights augment networks sequence prediction task. memory weight matrices convergence augment hybrid augment networks sequence prediction task sequences length note ﬁrst memory units hybrid augment leaky last ones conservative non-leaky convergence time. again augment hybrid augment show good learning performance maintaining average trials convergence sequences containing distractors whereas network purely leaky units much slower converge. leaky dynamics helpful sequence prediction task intermediate cues relevant ﬁnal model performance. therefore sufﬁcient supress weight values matrix distractors increase initial letter. conﬁrmed structure weight matrices memory networks shown convergence simulations sequence prediction task sequences letters. weight values highest absolute value letters units. finally also notice hybrid augment employs mainly conservative non-leaky memory units rather leaky ones solve task showing network able focus update dynamics connections suitable speciﬁc task. task standard cognitive task used test working memory diagnose behavioral cognitive deﬁcits related memory dysfunctions basically problem consists identifying target sequences among group symbols appear screen. general procedure task schematized figure details involving construction dataset collected table possible stimuli consists symbols digit cues context cues target cues ﬁnally distractors trial starts digit followed random number inner loops composed inner loops. patterns context-target cues like b-y. distractors task-relevant cues invalidate subsequence creating wrong inner loops like c-x. cues presented screen agent possible responses them target non-target valid target cases trials start digit target associated target preceded context otherwise case initial digit target occurs target comes context dots inserted stress target inner loop occur even long time digit happens following example sequence -a-z-b-y-c-x-a-x variability temporal length trial main issue solving task temporal credit assignment problem. moreover since -a-x -b-y target sequences whereas -a-x -a-y task seen generalization temporal probability either starting digit trial overall probability target pair since target response associated stimulus appears correct sequence number non-targets generally much larger average non-targets target. rewarded correct predictions non-target targets punished wrong predictions reward effect balanced positive reward approximately equally targets non-targets based relative frequencies aids convergence. simulated hybrid augment network base augment leaky control task order whether case introduction leaky dynamics improves learning performance. figure shows evolution mean squared three networks. sharp descent networks converge error level non-zero indicating learning task completely achieved possibly memory interference. however hybrid augment leaky control saturate lower error value base augment. difference attributed mostly errors responding target cues non-target cues note that since continuous performance task error computed iteration including frequent trivial non-target predictions averaged consecutive predictions. networks quickly learn recognize non-target cues however hybrid augment leaky control learn complex identiﬁcation target patterns within trial presented network better base augment mean squared hybrid augment leaky control versus base augment wider potential target cues considered mean-squared figure non-targets considered figure average learning time figure three networks. standard augment network unable match convergence condition simulation despite presenting outer loop trials simulation. however hybrid augment leaky control performed consistently suggesting leaky memory units necessary task. leaky control learned slightly faster hybrid augment order understand hybrid memory works task analyzed weight structure connectivity matrices belong memory branch hybrid augment network unlike sequence prediction task hybrid network employs leaky non-leaky memory units. however overall separation memory activity groups cells non-leaky cells emphasize information coming potential target cues storage initial digit ’assigned’ leaky units interference following letters reduced thanks gradual loss information digit information survive sufﬁcient increasing digit-related input weights. memory interference problem mitigated crucial digit information maintained time without interference leaky units identiﬁcation inner loops done conservative part memory. result memory units contribute deﬁnition activity q-values particular memory units active ones strongly discriminate non-targets targets giving positive contribution negative other. figure learning convergence augment variants task. minimization rpe-based energy function training task. networks show good decay mean-squared seem converge non-zero regime particular base augment network maintains higher mean-squared level compared leaky control hybrid augment mean-square associated potential target cues mean-squared related non-target cues. figure comparative statistics augment variants performance task. barplot description learning behavior three networks task according convergence criterion given alexander brown simulations measured fraction times model satisﬁes convergence condition average number training trials needed meet convergence criterion although training dataset consists outer loops base augment network never manages satisfy convergence criterion leaky hybrid models similar convergence performance learning time trials. instance strong positive weight intensity xshows contrary negative weight intensity network tries reduce problems memory interference subsequent inner cycles adding memory deactivation opposite amount information stored previous activation effectively erasing memory. further difference absolute value activation deactivation higher case leaky cells deactivation next iteration remove lower amount information memory leakage. however digit cues weights activation deactivation typically sign order reinforce digit signal memory subsequent timesteps figure memory weights hybrid augment task. plot weight matrices memory branch hybrid augment network convergence task. left weights transient stimulus memory units right weights memory cells output units. conclusion conservative dynamics memory standard augment limitation learning ability model especially cases complex tasks many data store long trials. fact even though complexity task limited compared typical tasks augment network fails maintain sufﬁciently stable performance satisfy required convergence criterion. introduction leaky co-efﬁcient hybrid augment leads network solving task overcoming memory interference. however loss information leaky memory improve learning tasks lower risk memory interference like sequence prediction task. hybrid augmentcan adapted different task structures different temporal scales varying size composition memory example considering multiple subpopulations neurons distinct memory timescales power distribution. goal computational neuroscience community develop neural networks time biologically plausible able learn complex tasks similar humans. embedding memory certainly important step direction memory plays central role human learning decision making. interest augment network derives mainly biological plausibility learning memory dynamics. particular biological setting learning algorithm based synaptic tagging attentional feedback neuromodulation providing possible biological interpretation backpropagation-like methods. developed hybrid augment introducing leaky dynamics memory system improving learning performance extending variety solvable tasks. hybrid augment leaky non-leaky units memory system solves task standard augment fails. solve simpler saccade-antisaccade sequence prediction tasks. hybrid augment inherits biological plausibility base augment addition consistent learning decaying memory units requires decay synaptic traces memory unit equation timescale decay unit’s memory state equation despite improvement hybrid learning ability augment variant still limited compared state-of-the-art memory-augmented networks. instance hierarchical temporal memory network presents greater ﬂexibility sequence learning experienced augment simple sequence prediction task. utilizing complex column-based architecture efﬁcient system inner inhibitions network able maintain dual neural activity column level unit level allows sparse representations input give multi-order predictions using unsupervised hebbian-like learning rule. nonetheless unclear network applied reward-based learning particular tasks like variable number inner loops. although hybrid memory augment network remarkably improved convergence performance task learning efﬁciency still lower reference hierarchical error representation model fact simulations hybrid augment showed mean time convergence equal outer loops average learning time convergence condition around outer loops. reason large learning performance resides gating mechanism network speciﬁcally developed hierarchical tasks used decide iteration whether store input maintain previous content memory. unlike model memory augment include gating mechanism meaning network learn store recall information memory dynamics entirely developed standard weight modulation. hand model biologically plausible augment network because although hierarchical structure inspired supposed organization prefrontal cortex learning scheme artiﬁcial based standard backpropagation. recent delta-rnn network presents interesting similarities hybrid augment employing timescales maintaining memory interpolation fast slow changing inner representations. delta-rnn whose memory dynamics generalization gating mechanisms lstm outperforms popular recurrent architectures. thus likely better learning ability hybrid augment though requires higher number parameters network based biological considerations. lack memory gating system great limitation augment variants compared networks equipped gated memory like lstm especially complex tasks high memory demand. still even though cannot properly deﬁned gating system forgetting dynamics introduced hybrid augment similar effect activity forget gates lstm gru. however unlike forget gates decay coefﬁcients learnable input-dependent memory cell. hybrid augment network could developed adding gating control leakage leak gates could output controller branch network applied gate decay co-efﬁcient memory branch. gating value becomes stimulus-dependent leakage adjusted optimize model performance. hand gating system would make network complex learning gate variables implies error backpropagation multiple layers compromise biological plausibility augment learning dynamics chitecture memory augment could divided multiple levels memory dynamics memory level could associated distinct synaptic decay leaky coefﬁcients learning rates gates order cover different temporal scales encourage level specialization. compared hybrid augment differentiation memory involve leaky dynamics also temporal dynamics associated attentional feedback synaptic potentiation. past years reinforcement learning community proposed several deep networks like deep q-networks alphago model combine learning advantages deep neural networks reinforcement learning thus interesting consider deep version augment network additional hidden layers neurons. conventional error backpropagation augment yield local synaptic plasticity locality might retained alternative backpropagation methods thank vineet jain helpful discussions. financial support provided european research council swiss national science foundation european commission horizon framework program", "year": 2017}