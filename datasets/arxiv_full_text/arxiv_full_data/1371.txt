{"title": "The Role of Typicality in Object Classification: Improving The  Generalization Capacity of Convolutional Neural Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Deep artificial neural networks have made remarkable progress in different tasks in the field of computer vision. However, the empirical analysis of these models and investigation of their failure cases has received attention recently. In this work, we show that deep learning models cannot generalize to atypical images that are substantially different from training images. This is in contrast to the superior generalization ability of the visual system in the human brain. We focus on Convolutional Neural Networks (CNN) as the state-of-the-art models in object recognition and classification; investigate this problem in more detail, and hypothesize that training CNN models suffer from unstructured loss minimization. We propose computational models to improve the generalization capacity of CNNs by considering how typical a training image looks like. By conducting an extensive set of experiments we show that involving a typicality measure can improve the classification results on a new set of images by a large margin. More importantly, this significant improvement is achieved without fine-tuning the CNN model on the target image set.", "text": "test-a table state-of-the-art convolutional neural networks numbers next columns errors atypical test-a typical test-t images. signiﬁcant drops performance especially test atypical images show limited generalization capacity cnn. goal enhance visual classiﬁers reducing gap. model misclassiﬁed high conﬁdence. evaluate performance state-of-the-art cnns purpose object classiﬁcation atypical images. humans capable perceiving atypical objects reasoning them even though seen experiments shown state-of-the-art cnns failed drastically recognize atypical objects. table shows results experiment took off-theshelf cnns applied atypical images. signiﬁcant performance drop tested atypical images rooted limited generalization power models versus human visual system. might argue issue cross-dataset generalization implicitly rooted dataset biases limited models however argue huge number labeled images training models alleviate drawback. providing wide range variation terms visual appearances objects training images effect biases fades away. support argument testing networks images disjoint training imagenet look typical. results experiment reported columns test-t table show much smaller drop accuracy compared case testing atypical images conclude dataset bias affect performance cnns object categorization main reason behind poor generalization datasets. deep artiﬁcial neural networks made remarkable progress different tasks ﬁeld computer vision. however empirical analysis models investigation failure cases received attention recently. work show deep learning models cannot generalize atypical images substantially different training images. contrast superior generalization ability visual system human brain. focus convolutional neural networks state-of-the-art models object recognition classiﬁcation; investigate problem detail hypothesize training models suffer unstructured loss minimization. propose computational models improve generalization capacity cnns considering typical training image looks like. conducting extensive experiments show involving typicality measure improve classiﬁcation results images large margin. importantly signiﬁcant improvement achieved without ﬁnetuning model target image set. introduction convolutional neural networks made remarkable progress variety computer vision tasks. name recent advances cnn-based models greatly improved performance object classiﬁcation object detection image retrieval ﬁne-grained recognition scene classiﬁcation action classiﬁcation generating image descriptions despite surpassing human categorization performance large-scale visual object datasets convolution neural networks cannot emulate generalization power human visual system real-world object categorization especially comes objects differ substantially training examples. figure shows examples atypical images human subjects categorize correctly instead inspired humans learn object categories empower models ability categorize extremely difﬁcult cases atypical images. humans begin form categories abstractions early age. mechanisms underlying human category formation subject many competing accounts including based prototypes exemplars density estimation bayesian inference. modern models agree human category representations involve subjective variations typicality probability objects within categories. words typicality graded concept simple decision boundary typical atypical examples. category like bird would include highly typical examples robins well extremely atypical examples like penguins ostriches belonging category seem like subjectively atypical examples. visual images also seem atypical exhibit features depart typical categories belong. humans learn object categories form visual biases looking typical samples able recognize atypical/abnormal objects show signiﬁcant visual variations training without even observing learning stage. computer vision machine learning perspectives state-of-the-art object classiﬁcation detection based discriminative models rather generative ones. discriminative training focuses learning boundaries object classes instead ﬁnding common characteristics class. training models based minimization loss function deﬁned misclassiﬁcation training samples. sense implicitly emphasizes boundary classes. however training samples weighted based typical look like equivalently representative given category. work hypothesize images equally important purpose training visual classiﬁers particular deep convolutional neural networks. instead show training images weighted based typical look learn visual classiﬁers better generalization capacity. ﬁnal model ﬁne-tuned typical images outperforms baseline model dataset atypical images. also empirically compare large functions used weighting samples conclude even-degree polynomial function typicality ratings best strategy weight training images. also investigate effect loss functions depth network conducting experiments datasets imagenet pascal. main contributions paper following evaluating models datasets images different training data characterizing failure cases poor generalization capacity models. especially contrasting failures superior performance humans categorizing atypical objects. inspired theories psychology machine learning propose three hypotheses improve generalization capacity models. hypotheses based weighting train images depending typical look. related work space allow encyclopedic review prior literature deep learning refer interested readers literature reviews research focus convolutional neural networks state-ofthe-art deep learning model task object recognition. roots neocognitron hierarchical model based classic notion simple complex cells visual neuroscience however additional hidden layers model complex nonlinearities visual data overall architecture reminiscent hierarchy visual cortex ventral pathway. additionally uses end-toend supervised learning algorithm called backpropagation learn weights layers. different variations models made breakthrough performance improvements variety tasks ﬁeld computer vision. mance continue derive improved representations learning algorithms make efﬁcient large datasets however leaves open question visual classiﬁer weight training images equally not? computational framework section ﬁrst theoretical background compelling theories learning visual concepts ﬁelds psychology computer vision. explain measure typicality objects image. propose three hypotheses typicality scores improve generalization capacity visual classiﬁers. framework motivation humans learn visual object class looking examples representative object category called typical samples shown children learn category looking typical samples later recognize members better training examples look typical fall close underlying space visual features. learning strategy helps humans form concept also allows easily apply concept novel images. great ability human visual system allows recognize completely different variations object even extent atypical objects. suggests emphasizing typical examples might helpful improving generalization ability classiﬁers. however state-of-the-art object classiﬁers computer vision discriminative models distinguish different objects learning category boundaries. models discriminative deep neural networks multiple layers learn hierarchy visual features categorize objects based loss minimization function misclassiﬁcation errors. words image classiﬁed correctly little impact loss function hence ignored training. implies examples close decision boundary likely atypical images play substantial role learning models. suggests training emphasize atypical images learn visual classiﬁers better performance. figure illustrates concept typicality examples classes shown diamonds crosses dotted line possible decision boundary. main points taken illustration first discussed section typicality graded concept directly relates likelihood observation given class distribution typical examples expected located close mean class distribution high probability moreover move away form center still observe examples category. every member category shows different rate typicality visualized smooth transition moving away center class. importantly clear boundary typical atypical members. figure illustration notion typicality. examples classes cross diamond show different shades typicality. classiﬁer discriminate classes cannot decision boundary between atypical typical samples category interest. despite extensive amount prior works application proposed variations theoretical understanding remains limited. importantly even models achieve human-level performance visual recognition tasks difference computer human vision? hand szegedy demonstrated classiﬁcation severely altered small changes images leading radically different classiﬁcation images indistinguishable human visual system. hand nguyen generated images completely unrecognizable humans model would classify conﬁdence. strategy fool models raises questions true generalization capabilities models investigate paper. addition recent studies ﬁeld neuroscience cognition shown connection deep neural networks visual system human brain. yamins shown correlation activation middle layers brain responses inferior temporal layers ventral visual hierarchy. cadieu proposed kernel analysis approach show deep neural networks rival representational performance cortex visual recognition tasks. khaligh-razavi kriegeskorte studied computational model representations found model came closest explaining brain representation. interestingly amount correlation human layers increases moving higher layers concluded weighted combination features last fully connected layer explain full extent. also shown models predict human brain activity accurately early intermediate stages visual pathway prior works ﬁnding right features choosing appropriate train order training examples learning better classiﬁers also shown models beneﬁt training larger datasets images. greatest gains detection performultiple categories. results smaller bigger consequently loss would implicitly favor atypical examples generate larger losses. measuring typicality objects approaches measuring typicality objects. hand compute probability score typical object based visual features case class-speciﬁc typicality infer indicates category independent class complement probability atypicality. implement probability one-class positive samples category used negative training example. model understood density estimation model prior knowledge family underlying distribution. learn one-class scenarios general class-independent typicality images used; class-speciﬁc typicality category trained based typical images category interest. refer models external score typicality. scores computed using model distinct based visual features different object categorization. scores computed ofﬂine training images changing different epochs training. hand judge typicality training images directly output visual classiﬁers. lake showed output last layer models used signal typical input image looks like. words typicality ratings proportional strength classiﬁcation response category interest. assuming loss deﬁned object categories nodes last layer compute internal probability typicality second atypicality happens variety reasons. visualized unique axis transition darker brighter shades gray. although examples close decision boundary might atypical category; atypical examples diverse limited boundary examples. conclusion sets atypical boundary examples equal. sample-based weighted loss architecture consists multiple blocks block convolution layer followed pooling normalization layers. blocks fully connected layers designed learn complex structures object categories. last layer computes loss function mismatch model prediction ground truth label. training formulated minimization loss function however work ﬁrst study analyze effect weighting samples using different loss functions incorporating typicality scores improving generalization cnn. associate sample weight function typicality explain later. build models weighting samples based loss functions softmax multi-class structured hinge. ﬁrst fastest widely used prior works later suitable purpose. softmax loss classiﬁcation problems using deep learning techniques common softmax encodings layer network number classes. assuming output i-th node last layer image goal minimize weighted multinomial logistic loss softmax training images alternatively entropy category prediction measure uncertainty responses punishes uncertain classiﬁcations. call internal entropy typicality compute −zilog. hypotheses propose three hypotheses improve generalization visual classiﬁers especially test image looks substantially different training images first inspired prototype theories psychology hypothesize learning emphasis towards representative samples would increase generalization capacity visual classiﬁer. second learning emphasis atypical examples training would enhance generalization capacity. complements loss function emphasizes boundary examples. hypothesis places additional emphasis possible directions atypicality training data might boundary. multi-class structured hinge loss also known crammer-singh loss widely used problem structured prediction. loss function similar hinge-loss computed based margin score desired category prediction scores aggregate loss function weighted summation training samples multi-class hinge loss particularly interest considers margin class predictions. importance piece information want generalize visual classiﬁers case atypical objects. examples harder classiﬁed result class prediction distribution peak around desired class. fact object might high class conﬁdence table object classiﬁcation accuracy alexnet test sets typical atypical images. loss functions compared training samples weighted four functions typicality atypicality class-speciﬁc typicality class-speciﬁc atypicality third hypothesize emphasizing typical atypical examples might better generalization performance used learning visual classiﬁers. main idea behind hypothesis fact visual classiﬁer learn object category formed much variation would allow members implement ﬁrst hypotheses multiply loss sample function typicality atypicality investigate effect different functions typicality score evaluate exponential gamma functions emphasize typicality versus logarithmic function emphasize atypicality. helps evaluate generalization capacity model trained non-linear weighting. evaluate last hypothesis implementing weighting function even-degree polynomial datasets used three image datasets imagenet challenge abnormal object dataset pascal train validation set. conducted experiments object categories aeroplane boat chair motorbike sofa. able verify generalization enhancement atypical images abnormal objects dataset contains categories. merged related synsets ilsvrc images categories resulting images refer train additionally experimented train validation pascal needed higher level supervision pascal data collection process images likely look typical. however imagenet data shows signiﬁcant variations terms visual appearance make image object look less typical. collected images pascal dataset refer train also used subset images ilsvrc detection challenge call test typical completely disjoint used training. images form test atypical represents conﬁrmed atypical/abnormal objects. typicality estimation measured typicality images one-class svms settings general class-speciﬁc. ﬁrst case independent objectcategory measures typical input image looks general. latter trained one-class svms typical images category interest. extracted kernel descriptors three scales input features. visual classiﬁer evaluated three hypotheses model alexnet nevertheless approach used state-of-theart models object classiﬁcation well. acquired implementation caffe alexnet ﬁne-tuned network following experiments. comparison loss functions order proper loss function ﬁne-tuning network conducted experiment losses softmax multi-structured hinge loss. experiment ﬁne-tuned last fully-connected layer train table shows performance comparison based using different loss functions weighting methods. conclude independent weighting strategy multi-structured hinge loss performs better softmax loss. consequently rest experiments conducted based ﬁne-tuning ms-hinge loss. comparison weighting functions conducted experiments compare performance models task object classiﬁcation ﬁne-tuned using different weighting functions. table shows result experiments test sets typical atypical. report mean accuracy ﬁrst tenth epochs. result ﬁrst epoch indicates fast network learn category tenth epoch elaborates performance network matured ﬁne-tuned without weighting. second shows weighting training images random number zero one. result shows randomly weighting training data effect improving generalization performance trained network. next shows results using typicality atypicality probabilities. conclude ﬁne-tuning atypicality/typicality weighting signiﬁcantly enhance generalization even ﬁrst epoch. however ﬁne-tuning typicality degrade performance tested typical images. third similar results typicality atypicality computed based class-speciﬁc one-class svms. fourth table investigates importance nonlinear weighting functions. first second results using logarithmic functions either typicality score class-speciﬁc atypicality scores conclude networks gain much non-linear functions either typicality atypicality scores test atypical images. non-linearities help stabilizing performance typical images. last fourth indicates ﬁne-tuning alexnet memorability score increase generalization performance however ﬁne-tuning memorability outperform typicality weightings. ﬁfth table evaluates third hypothesis three polynomials used weighting training samples. general strategy outperforms methods atypical test degrade performance typical last table classiﬁcation performance networks ﬁnetuned internal signal typicality. scores either normalized class predictions call internal probability typicality ﬁrst row; internal entropy class distribution second row. last experiment follows hybrid approach ﬁrst epoch samples weighted atypicality scores starting second epoch samples weighted internal scores. table evaluation effect depth generalization. comparison alternative models ﬁrst fully connected layer. changing ﬁne-tuning three fully connected layers. models ﬁne-tuned train mshinge loss used. weighting training examples based functions typicality scores. interestingly performance gain ﬁrst epoch higher ﬁne-tuned pascal rather imagenet relate bigger diversity visual appearance imagenet collection. investigated importance ﬁne-tuning deeper layers train models better generalization capacity. table shows results ﬁne-tuning top-two topthree fully connected layers alexnet. ﬁrst changed nodes. similarly second halved number nodes three models used ms-hinge loss learn parameters network. experiments show going deeper hurt ﬁne-tuned network especially tested atypical images. would partially relate limited number images available ﬁne-tuning therefore network overﬁts training date. digging deeper experiment training examples considered future work. conclusion several points conclude study. atypicality necessary equivalent samples boundary typical loss functions emphasize learning. main result paper involving information typicality/atypicality training samples weighting term loss function helps greatly enhancing performance unseen atypical examples training using typical examples. propose different ways achieve weighting samples based external internal signals network. also found symmetrically weighting highly typical highly atypical examples training gives better generalization performance. believe typicality/atypicality scoring data include information distribution samples therefore incorporates generative hints discriminative classiﬁer. typicality weighting helps generalization also helps faster learning network shown converge signiﬁcantly better results single epoch. references pulkit agrawal dustin stansbury jitendra malik jack gallant. pixels voxels modeling visual representation human brain. arxiv preprint arxiv. yoshua bengio j´erˆome louradour ronan procollobert jason weston. curriculum learning. ceedings annual international conference machine learning pages charles cadieu hong daniel yamins nicolas pinto diego ardila ethan solomon najib majaj james dicarlo. deep neural networks rival representation primate cortex core visual object recognition. plos computational biology deng dong richard socher li-jia fei-fei. imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference pages ieee jacob feldman. bias toward regular form mental shape spaces. journal experimental psychology human perception performance kunihiko fukushima. neocognitron selforganizing neural network model mechanism pattern recognition unaffected shift position. biological cybernetics masoud ghodrati amirhossein farzmahdi karim rajaei reza ebrahimpour seyed-mahdi khalighrazavi. feedforward object-vision models tolerate small image variations compared human. frontiers computational neuroscience kaiming xiangyu zhang shaoqing jian sun. delving deep rectiﬁers surpassing humanarxiv preprint level performance imagenet classiﬁcation. arxiv. yangqing evan shelhamer jeff donahue sergey karayev jonathan long ross girshick sergio guadarrama trevor darrell. caffe convolutional architecture fast feature embedding. arxiv preprint arxiv. alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems pages yann lecun bernhard boser john denker donnie henderson richard howard wayne hubbard lawrence jackel. backpropagation applied handwritten code recognition. neural computation john paul minda david smith. prototypes category learning effects category size category structure stimulus complexity. journal experimental psychology learning memory cognition nicolas pinto david james dicarlo. real-world visual object recognition hard? shaoqing kaiming ross girshick jian sun. faster r-cnn towards real-time object detection arxiv preprint arxiv. region proposal networks. babak saleh farhadi ahmed elgammal. object-centric anomaly detection attribute-based reasoning. conference computer vision pattern recognition ieee pierre sermanet david eigen xiang zhang micha¨el mathieu fergus yann lecun. overfeat integrated recognition localization detection using coninternational conference learning volutional networks. representations page cbls sharif razavian josephine sullivan atsuto maki stefan carlsson. baseline visual instance retrieval deep convolutional networks. international conference learning representations diego iclr christian szegedy wojciech zaremba ilya sutskever joan bruna dumitru erhan goodfellow fergus. intriguing properties neural networks. arxiv preprint arxiv. daniel yamins hong charles cadieu ethan solomon darren seibert james dicarlo. performance-optimized hierarchical models predict neural responses higher visual cortex. proceedings national academy sciences", "year": 2016}