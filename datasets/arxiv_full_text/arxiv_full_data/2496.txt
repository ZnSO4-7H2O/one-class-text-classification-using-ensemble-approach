{"title": "Diversifying Sparsity Using Variational Determinantal Point Processes", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We propose a novel diverse feature selection method based on determinantal point processes (DPPs). Our model enables one to flexibly define diversity based on the covariance of features (similar to orthogonal matching pursuit) or alternatively based on side information. We introduce our approach in the context of Bayesian sparse regression, employing a DPP as a variational approximation to the true spike and slab posterior distribution. We subsequently show how this variational DPP approximation generalizes and extends mean-field approximation, and can be learned efficiently by exploiting the fast sampling properties of DPPs. Our motivating application comes from bioinformatics, where we aim to identify a diverse set of genes whose expression profiles predict a tumor type where the diversity is defined with respect to a gene-gene interaction network. We also explore an application in spatial statistics. In both cases, we demonstrate that the proposed method yields significantly more diverse feature sets than classic sparse methods, without compromising accuracy.", "text": "propose novel diverse feature selection method based determinantal point processes model enables ﬂexibly deﬁne diversity based covariance features alternatively based side information. introduce approach context bayesian sparse regression employing variational approximation true spike slab posterior distribution. subsequently show variational approximation generalizes extends mean-ﬁeld approximation learned eﬃciently exploiting fast sampling properties dpps. motivating application comes bioinformatics identify diverse genes whose expression proﬁles predict tumor type diversity deﬁned respect gene-gene interaction network. also explore application spatial statistics. cases demonstrate proposed method yields signiﬁcantly diverse feature sets classic sparse methods without compromising accuracy. modern technology enables capture increasingly large amounts data critically important eﬃcient ways create compact functional interpretable representations. feature selection promising approach since reducing feature space improves interpretability prevents over-ﬁtting; result received considerable attention literature paper diverse feature sets potential compact easier interpret without sacriﬁcing performance. diversity also plays fundamental role real-world applications; example breast cancer increasingly recognized present highly heterogeneous group malignancies subgroups involve diﬀerent mechanisms action. common task identifying gene expressionbased biomarkers diﬀerent tumor subtypes maximizing diversity selected genes helps identifying disparate mechanisms action. diversity case deﬁned respect separate gene-gene interaction network existing techniques feature selection generally explicitly consider feature diversity. optimization point view feature selection viewed search possible subsets features identify optimal subset according pre-speciﬁed metric often balancing model model complexity. avoid enumerating entire combinatorial search space embedded approaches lasso relax problem combination sparsity term data ﬁdelity term. however methods typically encourage diversity explicitly. fact lasso shown unstable face nearly collinear features several variants proposed ameliorate issue alternative approach search greedily successively adding best feature orthogonal matching pursuit proceeds using forward step-wise feature selection selected feature chosen orthogonal possible previously selected features view orthogonality measure implicitly maximize diversity spite wellestablished performance procedure lacks underlying generative model therefore ﬂexibility deﬁne diversity paper take probabilistic view problem assigning probability measure feature subsets. seek maximum posteriori estimate optimal subset. particular probability measure variational approximation posterior spike-and-slab variable selection model. imposing particular form approximation obtain measure assigns higher scores feature subsets relevant regression classiﬁcation task also diverse. challenge form achieves goal remaining computationally tractable. variational approximation takes form determinantal point process dpps appealing context since naturally encourage diversity deﬁned terms kernel matrix example feature covariance matrix alternatively derived application-speciﬁc notions similarity dpps also oﬀer computationally appealing properties eﬃcient sampling approximate estimation result eﬃciently approximate optimal feature also provide sampling-based credible intervals. unlike mean ﬁeld approximations fully factorize posterior distribution approximate posterior complex dependency structure. makes ﬁtting challenging task. kulesza used optimization approach learn conditional dpps aﬀandi proposed parameterize kernel dpps learn parameters using sampling approach; however neither approach allows learning full unparameterized kernel. contrast algorithm based ﬂexible variational framework proposed salimans knowles requires basic operations eﬃcient evaluation joint likelihood eﬃcient sampling current estimate posterior. fortunately dpps operations eﬃcient. regression marginal likelihood takes closed form many models including classiﬁcation approximated eﬃciently. best knowledge work presents ﬁrst dpps within variational framework. closest work george al.. suggested variations so-called dilution priors that instead assigning prior uniformly across models assign probability uniformly across neighborhoods distribute neighborhood probabilities across models within them. diluting priors proposed proportional determinant features resembling form dpp. prior guarantee diversifying properties posteriori selected features. although possible instead prior suggest approximate posterior dpp. investigate model choice section work makes following contributions. propose approximate posterior bayesian feature selection. variational approximation draw connection dpps exponential family using decomposition proposed kulesza al.. connection makes many tools developed exponential family available dpps including variational learning methods distributions fully factorizable. proposed method brings number advantages including ability propose multiple sets relevant diverse features viewed alternative feature selection solutions; characterize feature selection uncertainty posterior sampling; ﬂexibly deﬁne feature diversity based side information rather covariance; compute marginal probability inclusion features conditioned presence existing feature thanks computational properties dpps. remainder paper organized follows. ﬁrst review dpps section idea diverse sparsity illustrated context bayesian variable selection section section show learn parameters models. section apply method identify diverse genes predict tumor type diversity deﬁned respect gene-gene interaction network. finally section explore application learning optimal distribution grid points spatial process convolution model. space ﬁnite {··· deﬁnes probability mass subsets. speciﬁcally probability subset proportional determinant denotes submatrix containing columns rows rm×m positive semideﬁnite kernel matrix. strictly speaking representation deﬁnes subclass dpps called l-ensembles. measurement similarity elements assigns higher probabilities subsets diverse. precisely feature function probability element-wise product. assuming exchangeable bernoulli prior conjugate prior i.e. random variable γmβm deﬁnes so-called spikeand-slab prior drawn spike probability slab probability approximation. variational approaches approximate form posterior; example mean ﬁeld approach employs fully factorized function approximating distribution marginalizing mean ﬁeld approximates posterior probability variable inclusion encourage diversity needs model interactions features. propose elegant deﬁne probability mass possible subsets naming convention xx-yy refers prior speciﬁcation variational figure l-ensemble problem items. items dissimilar except ﬁrst three items. leads block diagonal kernel empirical average number elements selected block. although green block bigger blocks becomes collinear sample compute empirical average samples falling block function interaction probabilities proportional sizes blocks selecting item ﬁrst block decreases rest blocks. following standard notation consider regresm= xmβm regressors {x··· collected design matrix rn×m residual noise view variable selection deciding coeﬃcients nonzero. suﬃcient statistic normalizer base measure. general form member exponential family parameterizing dpps allows employ framework. ﬁrst summarize algorithm context detγ) det. notational convenience deﬁne normalized posterior otherwise unnormalized kl-divergence respect obtain setting zero salimans knowles linked linear regression variational bayes method. namely optimal solution satisfy linear system timated weighted monte carlo sampling drawing single sample current posterior approximation step size. interestingly closed form nevertheless experiments clear advantage substituting current estimate directly versus using empirical estimate. suggested initial l-ensemble adjusted make sure initial samples empty sets. note diagonal elements marginal probability inclusion element therefore i’th eigenvector expected cardinality subsets preset value solve equation distribution hence refer setting bernoulli-dpp. possible deﬁne prior approximate posterior fully factorized mean ﬁeld method i.e. bernoulli however prior guarantee posterior exhibits diversifying property. also straightforward prior posterior focus investigating eﬀective prior versus posterior. similarity features whose similarity feature vector item l-ensemble matrix diag. example discourages collinearity. also deﬁned side information propose algorithm variational approximation dpp-bernoulli bernoulli-dpp. learning bernoulli-dpp challenging dpp-bernoulli. note variational approach minimizes divergence computing ﬁrst term entropy entails summands which best knowledge closedform. focus approximating bernoulli-dpp show straightforward modiﬁcation resulting algorithm eﬀectively approximate posterior dpp-bernoulli. learn borrow stochastic approximation algorithm allows approximate distribution given closed-form. structured ﬁxed-form variational bayes posterior distribution chosen speciﬁc member exponential family namely closed-form conditioned computation equivalent computing marginal likelihood linear kernel gaussian process model approximated using expectation propagation .joint distribution encode inlearning compute estimate using relevant diverse set. easily compute credible interval approximation drawing samples approximate posterior predicting draw computing variance prediction. algorithm focused bernoulli prior posterior i.e. bernoulli-dpp. adapt algorithm dpp-bernoulli modify changing prior detγ setting posterior identity matrix results eq.. rest algorithm stays intact. computational complexity perform inversion line algorithm conjugate gradient complexity condition number number nonzero entries. initialize solver warm initialization helps greatly currently rely matlab implementation prove concept; lowrank approximation alleviate memory complexity. lowrank complexity sampling reduced iteration rank similarity matrix number elements. computing marginal likelihood regression closed form involves inversion matrix number selected elements iteration). classiﬁcation expectation propagation approximate marginal likelihood complexity deﬁned smart initialization previous iteration converges quickly. addition smart bookkeeping previous iterations reduce number marginal likelihood computations. show results experiments covering classiﬁcation regression experiments provided supplementary material. section features themselves used deﬁne diversity penalize collinearity section deﬁne diversity rank numerical scale optimal quality score local optimization strategy failed hence greedy approach proposed approximate map. compared models baseline methods orthogonal matching pursuit generalized linear model lasso elastic forward selection spike-and-slab using prior fully factorized mean ﬁeld lasso standard approaches feature selection using convex greedy optimization. elastic chosen since extra norm better copes collinearity better. chosen since orthogonality procedure induces notion diversity spikeslab dpp-bernoulli assume bernoulli priors inclusion features respectively deploy mean ﬁeld approximate posterior. parameters methods adjusted match number section turn motivating application method ﬁnding diverse genes distinguish stage breast tumors. constructing accurate classiﬁers help identify important biomarkers breast cancer progression furthermore increasing functional diversity selected genes likely identify comprehensive cancer-related pathways. main idea follows. gene expression proﬁles readily available data predicting breast cancer prognosis. however correlation gene expression relatively poor predictor correlation gene function poor feature deﬁne functional similarity genes. distance gene pairs proteinprotein interaction network predictive gene function networks tend form communities genes belonging community perform similar functions. however since community detection challenging using network distance deﬁne similarity avoids community detection step. subset genes selected method yields diverse subset without compromising accuracy. asterisks indicate statistical signiﬁcance using wilcox rank test. networks genes bernoulli-dpp respectively. genes sorted according number times present optimal repeats. radius proportional number times gene selected color indicates gene. ﬁrst collected subjects cancer genome atlas stage breast cancer. computed normalized expression levels genes least physical protein-protein interaction found biogrid database focused genes smallest pvalue respect tumor stages. used biogrid gene interaction network compute pairwise similarities genes follows given scale-free nature network ﬁrst identiﬁed hubs network nodes total degree higher gene deﬁned network proﬁle -dimensional vector component speciﬁes shortest path gene hub. feature similarity matrix measures similarity figure figure show bernoulli-dpp identiﬁes gene sets signiﬁcantly diverse methods without compromising prediction accuracy. note imposing approximate posterior leads diverse prior dpp-bernoulli. also randomly select genes p-value leads diverse although random range method diversity bernoulli-dpp spikeslab produces good accuracy selected genes basically genes according p-value gene diverse. volved breast cancer. ﬁrst divided genes communities using biogrid network. based diﬀerent cross-validation runs using methods identiﬁed communities within network selected often bernoulli-dpp method. found dpp-preferred communities enriched genes related cell cycle checkpoints metabolism repair predicted breast cancer gene modules interactors several known cancer genes brca aatf anpb hdac prkdc among others also observed enrichment genes down-regulated activated t-cells relative naive t-cells immune cell types. although role t-cells tumor immunity fully understood recent work implicated immune cell activity breast cancer survival results support ﬁndings potentially widen genes need investigated anti-tumor properties. demonstrate method applied problem spatial statistics namely constructing nonstationary gaussian process model computationally eﬃcient way. construct convolve white noise continuous funcfigure show examples gridding prediction bernoulli-dpp omp. grid points spread methods suﬀers overshooting prediction show average pairwise distance selected grid points respectively diﬀerent number measurements. diversity promoting methods performs similarly term diversity dpp-related method slightly better term mse. spikeslab enet outperform methods term shown selected grid points much closer other. dpp-related methods seem better balance distributed grids particularly large number measurements. regularly spaced) grid points. assuming observed noise problem equivalent regression feature selection equivalent ﬁnding optimal locations spatial bases. objective optimal gridding spatial domain prediction also ensuring broad spread across spatial domain. grid point covers areas domain. grid points centers basis vectors isotropic gaussian bumps three diﬀerent scales. notice spatially spread basis functions boils basis functions little overlap. hence diversity simply computed inner product basis vectors i.e. experiment temperature measured month july sensors located across united states. randomly choose varying number sensors training evaluate performance left sensors methods. procedure repeated times report average performance figure figure reports average pairwise distance selected points. term dpp-bernoulli bernoulli-dpp perform better slightly better lasso spikeslab outperforms methods. however evident figure spikeslab produce spread grid points main objective. contrast dpp-bernoulli bernoulli-dpp strike good balance prediction diversity. examples reconstructions shown figure bernoulli-dpp figureb omp. tends overshoot areas measurements trait also observed simulation also interesting prior approximate posterior perform similarly. paper proposed probabilistic method diverse feature selection. proposed approximate posterior distribution dpps computationally elegant encourage diversity. approach selects representative items communities relevant items. similarity items encoded inner product features discourage collinearity learning parameters dpps active research area shown computationally eﬃcient strategy learning parameters variational approach. know method ﬁrst variational method used learn parameters distribution. algorithm relies sampling involves singular value decomposition iteration. stable matrices large condition numbers hence would interesting explore parametrization dpps alternative parametrization hopefully improve condition number optimal kernel matrix improve performance approximation conclusion imposing approximate posterior selects diverse features without compromising accuracy; further allows samplingbased quantiﬁcation uncertainty. posterior distribution multi-modal sampling model provide alternative solution something possible omp. fact simulated examples demonstrated robust carbonetto stephens scalable variational inference bayesian variable selection regression accuracy genetic association studies. bayesian analysis garcia-closas hall nevanlinna pooley morrison richesson bojesen nordestgaard axelsson arias heterogeneity breast cancer associations susceptibility loci clinical pathological characteristics. plos genetics edward george dilution priors compensating model space redundancy. borrowing strength theory powering applications–a festschrift lawrence brown pages institute mathematical statistics gillenwater kulesza taskar. nearoptimal inference determinantal point processes. peter bartlett fernando pereira christopher burges l´eon bottou kilian weinberger editors nips pages honkela raiko kuusela tornio karhunen. approximate riemannian conjugate gradient learning ﬁxed-form variational bayes. journal machine learning research chandra pati rezaiifar krishnaprasad. orthogonal matching pursuit recursive function approximation applications wavelet decomposition. signals systems computers conference record twenty-seventh asilomar conference pages ieee peguillet milder high numbers differentiated eﬀector cells found patients cancer correlate clinical response neoadjuvant therapy breast cancer. cancer res. subramanian tamayo mootha mukherjee ebert gillette paulovich pomeroy golub lander mesirov. gene enrichment analysis knowledge-based approach interpreting genome-wide expression proﬁles. pnas", "year": 2014}