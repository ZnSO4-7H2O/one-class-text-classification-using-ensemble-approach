{"title": "Fast Meta-Learning for Adaptive Hierarchical Classifier Design", "tag": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML", "68T05, 62H30, 62C10", "I.2.6"], "abstract": "We propose a new splitting criterion for a meta-learning approach to multiclass classifier design that adaptively merges the classes into a tree-structured hierarchy of increasingly difficult binary classification problems. The classification tree is constructed from empirical estimates of the Henze-Penrose bounds on the pairwise Bayes misclassification rates that rank the binary subproblems in terms of difficulty of classification. The proposed empirical estimates of the Bayes error rate are computed from the minimal spanning tree (MST) of the samples from each pair of classes. Moreover, a meta-learning technique is presented for quantifying the one-vs-rest Bayes error rate for each individual class from a single MST on the entire dataset. Extensive simulations on benchmark datasets show that the proposed hierarchical method can often be learned much faster than competing methods, while achieving competitive accuracy.", "text": "propose splitting criterion meta-learning approach multiclass classiﬁer design adaptively merges classes tree-structured hierarchy increasingly diﬃcult binary classiﬁcation problems. classiﬁcation tree constructed empirical estimates henze-penrose bounds pairwise bayes misclassiﬁcation rates rank binary subproblems terms diﬃculty classiﬁcation. proposed empirical estimates bayes error rate computed minimal spanning tree samples pair classes. moreover meta-learning technique presented quantifying onevs-rest bayes error rate individual class single entire dataset. extensive simulations benchmark datasets show proposed hierarchical method often learned much faster competing methods achieving competitive accuracy. bayes error rate central concept statistical theory classiﬁcation. represents error rate bayes classiﬁer assigns label object corresponding class highest posterior probability. deﬁnition bayes error represents smallest possible average error rate achieved decision rule properties great interest benchmarking classiﬁcation algorithms well practical design classiﬁcation algorithms. example accurate approximation used classiﬁer parameter selection data dimensionality reduction variable selection. however accurate approximation diﬃcult especially high dimension thus much attention focused tight tractable bounds. paper proposes model-free approach designing multiclass classiﬁers using bias-corrected bound estimated directly multiclass data. exists several useful bounds functions classdependent feature distributions. include information theoretic divergence measures chernoﬀ α-divergence bhattacharyya divergence jensen-shannon divergence alternatively arbitrarily tight bounds performance constructed using sinusoidal hyperbolic recently berisha introduced divergence measure belonging family -divergences tightly bounds bayes error rate binary classiﬁcation problem. bounds obtained measure tighter bounds derived bhattacharyya chernoﬀ bounds. moreover divergence measure estimated nonparametrically data without resorting density estimates distribution functions. inspired friedman-rafsky multivariate runs test estimation based computing euclidean minimal spanning tree data done approximately time. paper propose improvements estimator problems unequal class priors apply improved estimator adaptive design hierarchical multiclass classiﬁer. furthermore fast method proposed bounding bayes error rate individual classes requires computing single minimal spanning tree entire samples. thus proposed method faster competing methods density plug-in estimation divergence observed misclassiﬁcation rates algorithms logistic regression involve expensive parameter tuning. quantifying complexity classiﬁcation problem signiﬁcant interest clear fast accurate estimate complexity many practical applications. instance accurate complexity estimator allows researcher assess priori whether given classiﬁcation problem diﬃcult classify not. multiclass problem pair classes diﬃcult disambiguate could potentially merged could designated additional data collection. moreover accurate estimate could used variable selection application explored previously berisha section applications estimates multiclass classiﬁcation presented evaluated. many methods available design multiclass classiﬁcation algorithms including logistic regression support vector machines neural networks often case classiﬁer performance better classes others instance sample imbalance training set. often classiﬁer designs apply weights diﬀerent classes order reduce eﬀect imbalances average classiﬁer accuracy take diﬀerent general approach incorporates empirical determination relative diﬃculties classifying diﬀerent classes. accurate empirical estimates used purpose. multiclass classiﬁer presented section uses mst-based estimates create hierarchy binary subproblems increase diﬃculty algorithm progresses. classiﬁer initially works easily decidable subproblems moving diﬃcult multiclass classiﬁcation problems. paper organized follows. theory nonparametric bayes error estimator berisha reviewed section introduce bias correction estimator motivate estimator multiclass classiﬁcation discuss computational complexity. section introduce applications estimator meta-learning multiclass classiﬁcation. novel hierarchical classiﬁcation method introduced evaluated section section provides concluding remarks. motivation theory estimator bayes error rate reviewed introduced berisha improvement estimator proposed case class prior probabilities unequal. next application estimator multiclass classiﬁcation problems considered. finally computational considerations robustness analyses presented. recently berisha derived tight bound estimated directly data without parametric model density density estimation. bound based divergence measure introduced berisha hero deﬁned henze penrose denote multidimensional features classes. deﬁne class sample sizes |xk| combined sample denoted total number samples classes. deﬁne complete graph graph connecting nodes {xi}n edge weights |ekj| equal euclidean distances. euclidean minimal spanning tree spans denoted deﬁned subgraph connected whose edge figure illustration estimates bayes error rate diﬀerent values prior probability spherical bivariate gaussian distributions increasing separation classes measured euclidean distance class means compare true bhattacharyya bound requiring true class distributions hp-estimator based empirical sample. figures show estimates respectively without proposed bias correction hp-estimator. hp-estimator illustrate performance hp-estimator motivate proposed modiﬁcation consider binary classiﬁcation problem samples drawn independent bivariate gaussian distributions equal covariance matrices. example associated bounds computed exactly figure compare hp-estimator popular bhattacharyya bound figure shows hp-estimator closer true bhattacharyya bound. result illustrated berisha case conﬁrmed case estimates true prior probabilities. figure shows eﬀect bias correction accuracy hp-estimator. seen bias correction signiﬁcantly improves accuracy hp-estimator class distributions well separated. apply hp-estimate multiclass classiﬁcation problems extending bias corrected hp-estimator multiclass bayes error rate. original multiclass hp-estimator deﬁned wisler show framework applied hierarchical multiclassiﬁer design. prior probabilities density functions then estimated pair classes using biascorrected hp-estimator using binary classiﬁcation problem largest estimate deﬁned diﬃcult. recall largest achieved binary classiﬁcation problem unequal class priors equal value smallest prior probability. makes diﬃcult compare empirical estimates class sizes imbalanced. correct this hp-estimator pairwise classiﬁcation bers normalized class sizes using practice also interest understand diﬃcult discriminate individual class. reducing multiclass problem one-vs-rest classiﬁcation problem straightforward deﬁne confusion rate given class represents fraction instances erroneously assigned class fraction instances truly class assigned diﬀerent class. formally deﬁne confusion rate class predicted class instance recall bayes error rate error rate bayes classiﬁer assigns instance class maxl plfl. hence single class equals error assigning class true class total error assigning class true class figure variance hp-estimator varying number orthogonal msts function separation classes. results based repetitions binary classiﬁcation problem class distributions bivariate spherical gaussians diﬀerent means. construction minimal spanning tree lies heart hp-estimator important fast algorithm construction. since hp-estimator based euclidean dual-tree algorithm march applied. algorithm based construction bor˚uvka implements euclidean approximately time. larger datasets beneﬁcial partition space hypercubes construct partition. simple improve robustness hp-estimator multiple orthogonal msts average number cross-connections computing orthogonal msts straightforward dual-tree algorithm march easy implement algorithms pairwise distance matrix whitney figure shows empirical variance hp-estimator diﬀerent numbers orthogonal msts function separation classes. expected variance decreases number orthogonal msts increases although beneﬁt including orthogonal msts also decreases adding msts. therefore orthogonal msts typically used practice. figure heat maps illustrating diﬃculty distinguishing diﬀerent handwritten digits mnist dataset lecun brighter squares correspond higher values. heat estimates training data shown. heat shown misclassiﬁcations made lenet- test data readily identiﬁed numbers diﬃcult distinguish well easy distinguish number pairs instance pattern reﬂected results test data shown applying hp-estimator meta-learning problems creates number opportunities assess diﬃculty classiﬁcation problem training classiﬁer. example given multiclass classiﬁcation problem useful know classes diﬃcult distinguish classes easy distinguish. figure shows illustration handwritten digits well-known mnist dataset ﬁgure shows heat square corresponds estimate binary problem training set. ﬁgure seen digits diﬃcult distinguish well digits information useful design classiﬁer ensure instance higher weights placed misclassiﬁcations diﬃcult number pairs correct classiﬁcation pairs importance end-task. figure similar heat shown based misclassiﬁed instances lenet- test set. ﬁgure shows symmetric confusion matrix based misclassiﬁed instances. seen ﬁgure closely corresponds heat training data conﬁrms predictive accuracy hp-estimator real data. another example accuracy estimates multiclass classiﬁcation ﬁgure ovr-ber estimates ˆpek class acproblems given figure curacy scores shown chess dataset obtained repository dataset split training dataset test dataset ovr-ber estimates computed training dataset. estimates compared class error rates obtained out-of-sample predictions test dataset using gensvm ﬁgure shows ovr-ber estimates accurate predictors classiﬁcation performance. classes relatively diﬃcult figure ovr-ber estimates test error rates class chess dataset. seen classes ovr-ber estimate accurate predictor test performance. classes diﬀerence test error generally gives lower bound test performance. test results obtained gensvm classiﬁer estimates also applied feature selection particular identiﬁcation useful feature transformations data. feature selection strategy based forward selection outlined berisha feature selection stage algorithm adds feature gives smallest increase estimate. berisha show feature selection strategy quickly yields subset useful features classiﬁcation problem. estimate fast asymptotically consistent estimate bound classiﬁcation performance easy number potential feature transformations smallest estimate classiﬁer. useful traditional feature transformations laplacian eigenmaps also commonly used kernel transformations svms. researcher signiﬁcantly reduce time needed train classiﬁer diﬀerent transformations data. multiclass setting one-vs-one strategy used even consider diﬀerent feature transformation binary subproblem. using uniﬁed classiﬁcation method consider feature transformations reduce average estimate worst-case estimate. note feature transformation reduces dimensionality dataset without increasing estimate considered beneﬁcial many classiﬁcation methods faster low-dimensional datasets. instance applying components chess dataset slightly increases estimates classes remaining classes. thus classiﬁer likely achieve comparable accuracy transformed dataset much faster train since dimensionality reduced figure illustration min-cut procedure multiclass classiﬁcation problem classes. ﬁrst splits vertices sets used binary classiﬁer. figure illustrates next step split cuts complete graph subset. ducible uncertainty classiﬁcation problem high indicates intrinsically diﬃcult problem. used construct tree binary classiﬁcation problems increase diﬃculty along depth tree. ﬁtting binary classiﬁer internal node tree classiﬁcation method obtained proceeds easier binary subproblems diﬃcult binary problems. similar divide-and-conquer algorithms proposed lorena review. approaches often apply clustering method create grouping dataset clusters repeating process recursively form binary tree classiﬁcation problems. lorena carvalho several empirical distance measures used indicators separation diﬃculty classes applied bottom-up procedure construct classiﬁcation tree. finally el-yaniv etzion-rosenberg jensen-shannon divergence used bound inequalities classiﬁcation tree constructed using randomized heuristic procedure. unfortunately jensen-shannon divergence implementation requires parametric estimation distribution functions. moreover equiprobable case upper bound obtained jensen-shannon divergence shown less tight obtained hp-divergence this estimates less accurate obtained proposed hp-estimator. edges equals hp-estimate binary problem. formally deﬁne edge weight hp-estimator bias correction normalization used. recursively applying min-cuts graph tree binary classiﬁcation problems obtained increase diﬃculty along depth tree. min-cuts weighted graph computed using instance method stoer wagner figure illustrates process multiclass classiﬁcation problem figure illustration tree induced graph cutting procedure illustrated figure hypothetical problem classes. dashed line tree indicates point binary classiﬁer trained. joint vertex sets pair vertex sets forms binary classiﬁcation problem datasets recursively applying procedure sets splits possible yields tree binary classiﬁcation problems illustrated figure remainder section results extensive simulation study presented aims evaluate performance hierarchical classiﬁer multiclass classiﬁcation problems. classiﬁer used binary problem tree linear support vector machine practice binary classiﬁer could used algorithm. implementation hierarchical classiﬁer based linear binary called smartsvm. experimental setup comparable used burg groenen nested cross-validation approach used reduce bias classiﬁer performance original dataset independent training test datasets generated. subsequently classiﬁcation method trained using fold training datasets. finally model retrained entire training dataset using optimal hyperparameters model used predict test set. experiments datasets used varying dimensions independent test sets constructed. train test datasets generated using stratiﬁed split proportions classes correspond full dataset. table shows descriptive statistics datasets used. datasets collected repository keel repository smartsvm compared linear multiclass svms experiments. three alternatives heuristic methods binary underlying classiﬁer others single-machine multiclass svms. commonly used heuristic approaches multiclass svms smartsvm classiﬁer meta-learning estimation techniques presented previous sections implemented smartsvm python package available https// github.com/heroresearchgroup/smartsvm. table dataset summary statistics datasets used experimental study. ﬁnal columns denote size smallest largest class respectively. datasets marked asterisk collected keel dataset repository others repository. single class others. directed acyclic graph proposed platt extension approach. similar training procedure uses diﬀerent prediction strategy. method voting scheme used class votes binary classiﬁer becomes predicted label. contrast dagsvm method uses voting scheme least likely class voted away remains. finally single-machine multiclass svms also compared method crammer singer gensvm methods implemented either ensure speed methods accurately compared. methods binary internally implemented liblinear liblinear also implements fast solver method crammer singer using algorithm proposed keerthi smartsvm bayes error rates corresponding classiﬁcation tree calculated training dataset preprocessing step. datasets bers computed based orthogonal msts using algorithm whitney largest datasets computed based single using algorithm march computing msts done parallel using cores. results training time presented training time smartsvm augmented preprocessing time. experiments compared training time out-of-sample predictive performance. table shows results training time averaged nested cross validation folds dataset. seen smartsvm fastest method datasets. attributed smaller number binary problems smartsvm needs solve compared fact binary problems smaller solved ovr. method fastest classiﬁcation method remaining datasets. single-machine multiclass svms crammer singer burg groenen larger computation times heuristic methods. since gensvm larger number hyperparameters interesting look average time hyperparameter conﬁguration well. case gensvm average faster crammer singer warm starts classiﬁcation performance methods reported using adjusted rand index corrects chance index classiﬁcation metric proposed previously santos embrechts table shows predictive performance measured ari. seen smartsvm obtains maximum performance sixteen datasets. however smartsvm outperforms datasets outperforms rest datasets. methods often used default heuristic approaches multiclass svms respectively default strategies popular libsvm liblinear libraries. since smartsvm often faster methods results indicate clear practical beneﬁt using smartsvm multiclass classiﬁcation. table out-of-sample predictive performance measured adjusted rand index. although smartsvm doesn’t often achieve maximum performance several datasets smartsvm outperforms diﬀers maximum performance second third decimal. work practical applicability nonparametric bayes error estimates metalearning hierarchical classiﬁer design investigated. estimate introduced berisha bias correction derived improves accuracy estimator classiﬁcation problems unequal class priors. furthermore normalization term proposed makes estimates comparable multiclass problems. expression ovr-ber given represents exact bayes error single class multiclass problem shown error eﬃciently estimated using hp-estimator well. robustness analysis hp-estimator performed showed beneﬁt using orthogonal msts estimator. many potential applications estimates meta-learning problems. above several possibilities explored including prediction pairs classes diﬃcult distinguish individual classes yield highest error rate. preliminary experiments feature transformations also performed showed estimates useful tool determining beneﬁcial transformations classiﬁer trained. based weighted graph pairwise estimates hierarchical multiclass classiﬁcation method proposed. classiﬁer uses top-down splitting approach create tree binary classiﬁcation problems increase diﬃculty along depth tree. using linear classiﬁcation problem hierarchical multiclass obtained named smartsvm. extensive simulation studies showed smartsvm often faster existing approaches yields competitive predictive performance several datasets. used construct better classiﬁcation methods. discussed section estimates could also used deﬁne class weights multiclass classiﬁer. moreover min-cut strategy used smartsvm optimal construct classiﬁcation tree. evaluating diﬀerent approaches constructing classiﬁcation hierarchies applications estimates multiclass classiﬁcation problems topics research. computational experiments work performed dutch national lisa compute cluster supported dutch national science foundation authors thank surfsara support using lisa cluster. research partially supported army research oﬃce grant wnf--- dept. energy grant de-na. section proof given statement henze-penrose upper bound bayes error rate tighter jensen-shannon upper bound derived presenting proof following lemma presented. follows hence increasing decreasing moreover limt→+ thus follows negative positive attains maximum decreases limt→∞ since limt→+ follows decreasing increasing deﬁnition according el-yaniv jensen-shannon divergence probability density functions prior probabilities stated terms kullback-leibler divergence section additional simulation results presented smartsvm experiments presented section table shows average time hyperparameter conﬁguration methods. especially useful comparing gensvm methods larger hyperparameters consider. commonly used tool summarize results simulation experiments rank plots dataset methods ranked best method receiving rank worst method receiving rank case ties fractional ranks used. averaging ranks datasets visual summary results obtained. figures show average ranks predictive performance total training time average training time respectively. ordering smartsvm rank plots training time seem counterintuitive considering smartsvm often fastest method. explained fact cases smartsvm slower usually also slower dag. contrast smartsvm fastest method usually second fastest method. this smartsvm obtains slightly higher average rank ovo. figure rank plots classiﬁer performance simulation study. figure shows average ranks out-of-sample predictive performance measured ari. figures respectively show average ranks total training time average training time hyperparameter conﬁguration.", "year": 2017}