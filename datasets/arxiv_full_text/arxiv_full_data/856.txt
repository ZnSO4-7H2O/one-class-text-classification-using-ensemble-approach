{"title": "Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Model-based methods and deep neural networks have both been tremendously successful paradigms in machine learning. In model-based methods, problem domain knowledge can be built into the constraints of the model, typically at the expense of difficulties during inference. In contrast, deterministic deep neural networks are constructed in such a way that inference is straightforward, but their architectures are generic and it is unclear how to incorporate knowledge. This work aims to obtain the advantages of both approaches. To do so, we start with a model-based approach and an associated inference algorithm, and \\emph{unfold} the inference iterations as layers in a deep network. Rather than optimizing the original model, we \\emph{untie} the model parameters across layers, in order to create a more powerful network. The resulting architecture can be trained discriminatively to perform accurate inference within a fixed network size. We show how this framework allows us to interpret conventional networks as mean-field inference in Markov random fields, and to obtain new architectures by instead using belief propagation as the inference algorithm. We then show its application to a non-negative matrix factorization model that incorporates the problem-domain knowledge that sound sources are additive. Deep unfolding of this model yields a new kind of non-negative deep neural network, that can be trained using a multiplicative backpropagation-style update algorithm. We present speech enhancement experiments showing that our approach is competitive with conventional neural networks despite using far fewer parameters.", "text": "model-based methods deep neural networks tremendously successful paradigms machine learning. model-based methods problem domain knowledge built constraints model typically expense difﬁculties inference. contrast deterministic deep neural networks constructed inference straightforward architectures generic unclear incorporate knowledge. work aims obtain advantages approaches. start modelbased approach associated inference algorithm unfold inference iterations layers deep network. rather optimizing original model untie model parameters across layers order create powerful network. resulting architecture trained discriminatively perform accurate inference within ﬁxed network size. show framework allows interpret conventional networks mean-ﬁeld inference markov random ﬁelds obtain architectures instead using belief propagation inference algorithm. show application non-negative matrix factorization model incorporates problem-domain knowledge sound sources additive. deep unfolding model yields kind non-negative deep neural network trained using multiplicative backpropagation-style update algorithm. present speech enhancement experiments showing approach competitive conventional neural networks despite using fewer parameters. successful general approaches machine learning model-based methods deep neural networks offers important well-known advantages disadvantages. goal paper provide general strategy obtain advantages approaches avoiding many disadvantages. general idea summarized follows given model-based approach requires iterative inference method unfold iterations layer-wise structure analogous neural network. untie model parameters across layers obtain novel neural-network-like architectures easily trained discriminatively using gradient-based methods. resulting formula combines expressive power conventional deep network internal structure model-based approach allowing inference performed ﬁxed number layers optimized best performance. call approach deep unfolding. chief advantage generative model-based approaches probabilistic graphical models allow prior knowledge intuition reason problem level devising inference algorithms david marr called computational theory level analysis important assumptions problem constraints often incorporated model-based approach straightforward way. examples include constraints world linear additivity signals visual occlusion three-dimensional geometry well subtle statistical assumptions conditional independence latent variable structure sparsity low-rank covariances hypothesizing testing different problem-level constraints insight nature problem gained used inspiration improve modeling assumptions unfortunately inference probabilistic models mathematically computationally intractable. approximate methods belief propagation variational approximations allow derive iterative algorithms infer latent variables interest. however despite greatly improving situation iterative methods still often slow time-sensitive applications. cases rigorous discriminative optimization models challenging involve bi-level optimization parameter optimization depends turn iterative inference algorithm deterministic deep neural networks recently become state many applications formulated inference deﬁned ﬁnite closed-form expression organized layers typically executed sequence. discriminative training networks used optimize speed versus accuracy trade-off become indispensable producing systems perform well particular application. however well-known disadvantage conventional dnns closer mechanisms problem-level formulations considered essentially black-box methods. difﬁcult incorporate prior knowledge world problem. moreover even working system clear actually achieves results discovering modify architecture achieve better results could considered much science. proposed methodology addresses difﬁculties bringing problem-level formulation model-based methods task designing deep neural network architectures. step process solved well-known methods deriving iterative inference methods given probabilistic model follows long tradition makes many well-known tools unfolding iterations applying chain rule gradient-based training also relatively straightforward. remainder paper present general framework applied arbitrary models inference algorithms. apply framework case inference generic markov random ﬁelds showing ﬁrst unfolding mean ﬁeld inference binary mrfs leads conventional sigmoid networks contrast unfolding belief propagation leads alternative deep architecture. show architectures uniﬁed generalized using power-mean formulation. focus generative model sound embodies problem-level assumption signals linearly therefore power spectra approximately additive. despite simplicity assumptions model based non-negative matrix factorization closed-form solution relies iterative inference methods typically formulated multiplicative updates. novel non-negative deep network architecture results unfolding iterations untying parameters. architecture powerful original model-based method still incorporating basic additivity assumption problem-level analysis. optimize non-negative parameters show form back-propagation based multiplicative updates used preserve non-negativity without need constrained optimization. finally present experiments domain speech enhancement using unfolded model showing competitive terms accuracy conventional sigmoid dnns requiring tenth number parameters. relationship literature recent work addressed idea unfolding inference algorithms using gradient descent optimize context variety models inference methods. sparse coding non-negative matrix factorization addressed using unfolding back-propagation discriminative optimization methods. gradient-based optimization loopy belief propagation applied binary pair-wise markov random ﬁelds. tree-reweighted belief propagation mean ﬁeld inference unrolled trained gradient descent. inference graphical model implemented ensemble unfolded inference algorithms trained predict held view untying parameters critical step creating deep architectures competitive conventional deep networks. recent scene labeling paper learning belief propagation mean ﬁeld message passing algorithms considered conditional random ﬁelds mrfs units either inputs labels. case belief propagation rather following belief-propagation updates updates replaced standard logistic regression yields conventional sigmoid networks rather novel architectures. simultaneously work also introduced similar untying approach applied mean-ﬁeld inference methods mrfs. again results conventional sigmoid networks whose graph structure derived schedule mean ﬁeld updates. writing novel deep architectures developed unfolding untying. main contributions paper novel contributions paper include framework deriving novel deep network architectures model-based inference algorithms unfolding steps algorithm untying model parameters across iterations; generalization classical sigmoid networks based uniﬁcation different unfolded inference algorithms; novel non-negative deep network non-negative parameters; multiplicative form back-propagation updates training non-negative deep networks; ﬁnally experiments showing beneﬁt approach domain speech enhancement. general setting consider models inference optimization problem. example variational inference lower bound data likelihood optimized estimate approximate posterior probabilities used compute conditional expected values hidden variables. another example loopy belief propagation iterative algorithm enforces local consistency constraints marginal posteriors. converges ﬁxed points correspond stationary points bethe free energy finally non-negative matrix factorization non-negative basis expansion model whose objective function optimized simple multiplicative update rules. here present general formulation based model determined parameters speciﬁes relationships hidden quantities interest observed variables data instance parameter contains parameters used model markov random ﬁelds contains potential functions gaussian-based models contains means variances basis expansion models contains basis functions. quantities interest typically estimates latent variables important particular task. example scene labeling task might labels pixels; denoising might posterior mean latent clean signal. test time estimating quantities interest involves optimizing inference objective function intermediate variables computed estimator many interesting cases optimization cannot easily done leads iterative inference algorithm. probabilistic generative models might approximation negative likelihood could taken represent hidden variables represent estimate posterior distribution. example variational inference algorithms could taken variational parameters. sum-product loopy belief propagation would posterior marginal probabilities. hand non-probabilistic formulation taken activation coefﬁcients basis functions updated inference time. note sequences underlying structure simplicity ignore structure. loss function settings also consider discriminative objective bi-level optimization problem since determined optimization problem depends parameters assume intermediate variables optimized iteratively using update steps form beginning note that although steps assumed composed smaller steps different. occur loopy belief propagation different messages passed step variational inference different variational parameters updated step. although various optimization methods used simplest ﬂexible involves applying gradient descent sequence iterations approximate inference algorithm optimize parameters. make tractable match training-time inference algorithm test-time procedure iterations truncated way. however rather considering iterations algorithm consider unfolding sequence layers neural network-like architecture iteration index interpreted index neural network layer. intermediate variables nodes layers determines transformation activation function layers. finally unfolded model presented parameters tied across layers optimizing different optimizing original model. although discriminative optimization help overcoming mismatch assumed model data lead fundamentally different model performance particular competitive deep networks unless model start already deep. however deep unfolding framework hypothesize powerful model obtained explicitly untying parameters across layers allow network embody complex range inference functions original model original class inference representing special case. course cost untying possibility over-ﬁtting situation deep neural networks handled similar ways. easy show conventional sigmoid networks obtained unfolding untying mean ﬁeld inference binary pair-wise markov random ﬁelds. although generic mrfs good example incorporating problem-level knowledge instructive consider general graphical model formulation order understand conventional networks terms unfolding mrfs generalize conventional deep networks changing model and/or inference algorithm prior unfolding. ﬁrst review mean-ﬁeld updates lead conventional sigmoid networks. show belief propagation leads different deep architecture. finally unify architectures using general power mean formulation. simplicity restrict discussion pairwise mrfs. general mrfs higher-order factors easily expressed pairwise mrfs creating auxiliary random variable higher-order factor. first give general formulation arbitrary state spaces discuss special case binary mrfs lead sigmoid networks unfolded. also simplicity partition variables hidden observed random variables omit connections observed variables since affect inference. pairwise represented undirected graph whose vertices index hidden random variables taking values observed variables taking values abuse notation using refer random variables values omitting ranges summations. factors probability distribution associated edges graph. edges hidden variables identiﬁed unordered pairs indices edge connected}. edges hidden observed connected}. neighborhoods ehv} visible node nodes. edge factors hidden variables parameterized potential functions ψhihj hidden visible potentials ψhivl abuse notation indexing functions using arguments. discrete potential functions typically represented using scalar parameters combination values taken arguments formulated exponential family model using indicator functions features. variational methods mean ﬁeld approximation special case perform tractable approximate inference minimizing kullback-leibler divergence approximate posterior true posterior ph|v. equivalently maximize lower bound likelihood obtained jensen’s inequality note messages unnormalized normalized needed numerical purposes however used must normalized. order maintain variational bound updates must done according update schedule avoids synchronous updates directly interdependent functions. however context discriminative training unfolded model maintaining bound necessary. nevertheless speciﬁc ordering updates strong effect rate convergence inference. implement arbitrary schedule using scalar previous value message kept parameters formulated schedule implemented power mean previous message parameterized using exponent allow choose type mean used including example arithmetic geometric consider optimizing schedule desired allowing example perform synchronous updates setting used message-passing inference algorithm different choices arithmetic versus geometric mean might convenient different messages. mean ﬁeld messages geometric mean seems convenient given log-linear form choice take inside message yield suitable choices derived factor comes fact edge potential counted twice sum. matrix notation {aij}ij∈ih similarly matrix vector {bi}i∈ih. write desired posterior recognized sigmoid network special structure inputs connected layers. structure consequence unfolding model hidden variables directly connected observations. however untie parameters please emulate conventional case ﬁrst layer depends inputs subsequent layer depends non-zero ﬁrst frame initial distribution previous allow zero. also relax constraint original model reach full generality conventional sigmoid network. worth noting conventional feed-forward sigmoid networks also derived simply starting deep layer-wise binary performing single forward pass mean-ﬁeld updates starting input ending last layer. corresponds special case structure update schedule framework. point illustrate general case arbitrary structure also lead feed-forward sigmoid network. looking given conventional neural network then able interpret different ways. either interpret approximate inference structure neural network cases able interpret deeper unfolding model compact structure. either case identiﬁed model inference algorithm unfold given neural network consider changing inference algorithm model structure order generate alternative variants neural network. example instead using mean-ﬁeld inference could unfold model using belief propagation. belief propagation algorithm computing posterior probabilities leads exact solution tree-structured graphical models applied graphs loops known loopy belief propagation. interpreted ﬁxed point algorithm stationary points bethe free energy turn seen approximation kullback-leibler divergence approximated posterior true posterior distribution. algorithms style belief propagation thought produce better results general markov random ﬁeld problems hence motivation investigate deep network architectures based previous work explored unfolding without untying parameters focused extension loopy based trw-bp approaches simplicity begin standard sum-product version back ensuring message incorporated given belief updates yield exact marginals full posterior computed. general case mrfs cycles exclusion incoming messages longer completely prevents feedback approximate marginals longer guaranteed converge true marginals. however loopy works well practice problems appropriate message-passing schedule. mean-ﬁeld updates formalize message-passing schedule parameterizing update scalar messages unfolding untying algorithm across layers implementing update schedule accomplished using unfolded messages consider optimizing message passing schedule part deep unfolding method rather using heuristics commonly done. since messages linear instead log-linear choice arithmetic mean seems convenient. leads messages form case obtain similar updates starting layer-wise graph structure update schedule passes sequentially layers manner feed-forward neural network. however case general unfolding framework allows consider structured base models still obtain feedforward architecture. general untying training discriminatively also potentially neglect elimination incoming message consider unifying belief propagation messages mean-ﬁeld messages order develop architecture encompasses special cases. leaving aside update schedule comparing functions related general weighted power parameters also possible interpolate sum-product max-product varieties applying power mean differently done combination cost clarity yield even ﬂexible general form interesting empirical question extent distinctions affect performance context deep unfolding. works generalized different forms generalization similar trw-bp described negative also considered. interesting note trw-bp related algorithms maintain bound inference time objective function using edge weights similar well speciﬁc messagepassing schedules implemented using however context training deep network maintaining bound nearly important general form activation function. rather optimizing objective function test time instead optimize parameters according discriminative objective function training time inference algorithm implemented network obtains best result training data. generalized form raises possibility starting trained conventional sigmoid network re-training perform updates incrementally changing globally tied order investigate differences performance architectures. case must gracefully handle numerical instability example interpolation near existing sigmoid network deﬁned according generalized update easily derive shown appendix optimization parameters chain rule relatively straightforward unfolded algorithms optimized supplying task-related objective function computing derivatives message passing formulae above. optimizing schedule remain accomplished example parameterizing also consider training back-propagation. clearly schedule parameters message style parameters signiﬁcant number parameters. assuming uniform state spaces variables size |ih| whereas general could maximum size |ih||h| |ih||iv||h||v|. consequence judicious tying necessary keep complexity check. example layer might reasonable level tying depending network architecture. training individual connection might excessive states node binary units. however multinomial networks many states tuning λk∗j might reasonable. caveat optimization sigmoid networks known difﬁcult train relative simpler activation functions maxout vanishing gradients. normalized activation function generalizations especially binary form shown appendix even complicated corresponding sigmoid activation. however unnormalized form using generalized equations neglecting schedule considering reciprocal messages appears comparable commonly used forms similar spirit softmaxout max-product version similar spirit maxout despite similarity methods same remains seen whether differences signiﬁcant practice. leave experiments generic mrfs work rest paper turn models incorporate speciﬁc problem domain knowledge. discrete mrfs interesting general case main points work incorporate problem-level knowledge novel deep architecture. apply proposed deep unfolding framework non-negative matrix factorization model applied non-negative signal. although applied many domains focus task single-channel source separation aims recover source signals mixtures. context encompasses simple problem-level assumptions power magnitude spectra different sources approximately together source described linear combination non-negative basis functions. operates matrix -dimensional non-negative spectral features usually power magnitude spectrogram mixture number frames obtained short-time fourier transformation time-domain signal. sources source represented using matrix containing nonnegative basis column vectors multiplied matrix activation column vectors time contains activations corresponding basis basic assumptions written general bases trained independently source combined combination trained discriminatively good separation performance mixture. recently discriminative methods applied sparse dictionary based methods achieve better performance particular tasks similar discriminatively train bases source separation. following optimization problem training bases termed discriminative proposed controls divergence used bottom-level analysis objective controls divergence used top-level reconstruction objective. weights account applicationdependent importance source example speech de-noising focus reconstructing speech signal. ﬁrst part minimizes reconstruction error given second part ensures activations arise test-time inference objective. given bases activations uniquely determined convexity nonetheless remains difﬁcult bi-level optimization problem since bases occur levels. bi-level problem approached directly solving derivatives lower level problem convergence. problem approached untying bases used reconstruction analysis bases used training reconstruction bases. addition incorporated discriminative criteria here based framework unfold entire model deep non-negative neural network untie parameters across layers call model deep nmf. cast general formulation deﬁning order train network respecting non-negativity constraints derive recursively deﬁned multiplicative update equations back-propagating split positive negative parts gradient. multiplicative updates often derived using heuristic approach uses ratio negative part positive part multiplication factor update value variable interest. matrix unfolded network deep method evaluated along competitive models chime speech separation recognition challenge corpus task speech enhancement reverberated noisy mixtures background mostly non-stationary noise sources children household appliances television radio recorded home environment. training development test sets noisy mixtures along noise-free reference signals created wall street journal corpus read speech corpus training noise recordings. speech recordings convolved time-varying room impulse responses estimated environment noise. training consists utterances snrs steps development test sets consist utterances snrs total utterances. construction wsj- corpus evaluation speaker-independent. background noise recordings development test different training noise recordings different room impulse responses used convolve utterances. paper present results development set. reduce complexity training utterances methods. evaluation measure source-to-distortion ratio feature vector concatenates consecutive frames left context ending target frame obtained short-time fourier spectral magnitudes using window size window shift square root hann window. leads feature vectors size number frequencies. similarly features column corresponds sliding window consecutive reconstructed frames. last frame sliding window reconstructed leads on-line algorithm. nmf-based approaches number basis vectors speech noise consider look regimes maximum iterations nmf-based approaches still signiﬁcant room improvement performance which based preliminary experiments close asymptotic performance. compare deep architecture standard k-layer deep neural networks used following setting. feed-forward dnns hidden layers hyperbolic tangent activation functions output layer logistic activation functions. denoting output layer activations time index computes deterministic function input feature vectors tanh denote element-wise logistic hyperbolic tangent functions. deep experiments consecutive frames context concatenated together vectors logarithmic magnitude spectra. thus difference input feature representation respect deep compression spectral amplitudes generally considered useful speech processing breaks linearity assumption nmf. previous attempts dnns focused direct estimation clean speech without taking account mixture output layer direct estimation masking function without considering effect upon speech estimate. here based experience model-based approaches train masking function that applied mixture best reconstructs clean speech also proposed amounts optimizing following objective function training mixture magnitudes speech magnitudes. thus sequence output layer activations interpreted time-frequency mask magnitude spectral domain similar ‘wiener ﬁlter’ output layer deep experiments approach leads improvements relative mask estimation. although comes model-based approach include results comparable solely context deep architecture output layer. implementation based open-source software currennt. training objective function minimized chime training using back-propagation stochastic gradient descent momentum discriminative layer-wise pre-training. early stopping based cross-validation chime development gaussian input noise used prevent aggressive over-optimization training set. unfortunately current experiments deep cross-validation despite advantage gives shown deep nevertheless performs better. investigate different topologies terms performance chime development set. results shown table source shall denote snmf. multiplicative update algorithm optimize arbitrary given training spectrograms concatenated noise-free chime training corresponding background initial solution noise multi-condition training set. yields snmf bases exemplar bases sampled random training data source. sparsity weight performed well snmf dnmf algorithms experiments snmf experiments basis matrix used determining according reconstruction using deep experiments divergence used update equations squared error discriminative objective since corresponds closely evaluation metric combination performed well deep models initialize basis sets layers using snmf bases trained described section consider last layers discriminatively trained various values means untie bases ﬁnal layers train bases using multiplicative back-propagation updates described section thus corresponds snmf corresponds deep special case previously described dnmf experiments non-discriminatively trained layers full bases contain multiple context frames. contrast discriminatively trained layers restricted single frame context. network trained reconstruct single target frame whereas using full context would enforce additivity constraints across instead wk>k−c size reconstructions full context layer. initialized last rows matrix consisting last rows used place deep ﬁxed basis functions contain parameters trained discriminatively whereas ﬁnal layers together discriminatively trained parameters total relative snmf. comparing deep approaches best deep topology achieves outperforming best result comparable number parameters smallest deep topology outperforms best topology obtained achieves using least order magnitude fewer parameters analyzing effect topology performance deep discriminative training ﬁrst layer gives biggest improvement training layers consistently improves performance especially conditions adding modest number parameters layer. moving lead much gain might expect despite huge increase parameter size. could currently training data used fairly conservative convergence criterion. model size using layers leads large gains performance without increasing training time complexity. however comes price increased computation cost inference time. intermediate topology regimes need explored better sense best speed/accuracy trade-off. potential work speech enhancement domain could consider investigating application framework models continuity constraints factorial structure general future research directions within deep unfolding paradigm include experiments unfolding inference algorithms loopy belief propagation markov random ﬁelds variational inference algorithms intractable generative models. conclusion general framework introduced allows model-based approaches guide exploration space deep network architectures would otherwise difﬁcult navigate. shown conventional sigmoid networks could seen unfolded mean-ﬁeld inference markov random ﬁelds leading possible generalizations inference algorithms belief propagation variants. finally showed model-based problem constraints non-negative matrix factorization incorporated deep unfolding novel deep architecture. reasoning problem level model-based approach methodology allows derive inference architectures training methods otherwise would difﬁcult obtain. hope framework inspire generation novel deep network architectures suitable tackling difﬁcult problems require high-level domain insights.", "year": 2014}