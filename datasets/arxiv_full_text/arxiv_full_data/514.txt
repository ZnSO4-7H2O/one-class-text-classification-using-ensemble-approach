{"title": "A Fixed-Size Encoding Method for Variable-Length Sequences with its  Application to Neural Network Language Models", "tag": ["cs.NE", "cs.CL", "cs.LG"], "abstract": "In this paper, we propose the new fixed-size ordinally-forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence of words into a fixed-size representation. FOFE can model the word order in a sequence using a simple ordinally-forgetting mechanism according to the positions of words. In this work, we have applied FOFE to feedforward neural network language models (FNN-LMs). Experimental results have shown that without using any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform not only the standard fixed-input FNN-LMs but also the popular RNN-LMs.", "text": "paper propose ﬁxedsize ordinally-forgetting encoding method almost uniquely encode variable-length sequence words ﬁxed-size representation. fofe model word order sequence using simple ordinally-forgetting mechanism according positions words. work applied fofe feedforward neural network language models experimental results shown without using recurrent feedbacks fofe based fnnlms signiﬁcantly outperform standard ﬁxed-input fnn-lms also popular recurrent neural network lms. language models play important role many applications like speech recognition machine translation information retrieval nature language understanding. traditionally back-off n-gram models standard approach language modeling. recently neural networks successfully applied language modeling achieved state-of-the-art performance many tasks. neural network language models feedforward neural networks recurrent neural networks popular architectures. basic idea nnlms projection layer project discrete words continuous space estimate word conditional probabilities space smoother better generalize unseen contexts. language models usually limited history within ﬁxed-size context window predict next word. language models adopt time-delayed recursive architecture hidden layers memorize long-term dependency language. therefore widely reported rnn-lms usually outperform fnnlms language modeling. rnns theoretically powerful learning rnns needs so-called back-propagation time internal recurrent feedback cycles. bptt signiﬁcantly increases computational complexity learning algorithms cause many problems learning gradient vanishing exploding recently architectures proposed solve problems. long short term memory enhanced architecture implement recurrent feedbacks using various learnable gates obtained promising results handwriting recognition sequence modeling moreover socalled temporal-kernel recurrent neural networks proposed handle gradient vanishing problem. main idea tkrnn direct connections units time steps every unit implemented efﬁcient leaky integrator makes easier learn long-term dependency. along line temporal-kernel model successfully used language modeling comparing rnn-lms fnn-lms learned simpler efﬁcient way. however fnn-lms model long-term dependency language ﬁxed-size input window. paper propose novel encoding method discrete sequences named ﬁxedsize ordinally-forgetting encoding almost uniquely encode variable-length word sequence ﬁxed-size code. relying constant forgetting factor fofe model word order sequence based simple ordinally-forgetting mechanism uses position word sequence. theoretical analysis experimental simulation shown fofe provide alunique codes variable-length word sequences long forgetting factor properly selected. work apply fofe neural network language models ﬁxedsize fofe codes fnns input predict next word enabling fnn-lms model long-term dependency language. experiments benchmark tasks penn treebank corpus large text compression benchmark shown fofe-based fnn-lms signiﬁcantly outperform standard ﬁxed-input fnn-lms also achieve better performance popular rnn-lms without using lstm. moreover implementation also shows fofe based fnn-lms learned efﬁciently gpus without complex bptt procedure. assume vocabulary size nnlms adopt -of-k encoding vectors input. case word vocabulary represented onehot vector -of-k representation context independent encoding method. -of-k representation used model word sequence model history context. fixed-size ordinally forgetting encoding propose simple context-dependent encoding method sequence consisting discrete symbols namely ﬁxed-size ordinally-forgetting encoding given sequence words w··· word ﬁrst represented -of-k representation ﬁrst word sequence fofe encodes partial sequence based simple recursive formula obviously fofe encode variablelength discrete sequence ﬁxed-size code. moreover recursive context dependent encoding method smartly models order information various powers forgetting factor. furthermore fofe appealing property modeling natural languages far-away context gradually forgotten nearby contexts play much larger role resultant fofe codes. uniqueness fofe codes given vocabulary sequence length based fofe code computed above always decode original sequence unambiguously fofe unique. proof simple fofe code value i-th element determine word occurs position without ambiguity since matter many times occurs far-away contexts appears closer context i-th element architecture fofe based neural network language model shown figure similar standard bigram fnnlms except uses fofe code feed neural network time instance. moreover fofe easily scaled ngram based neural network lms. example figure illustration ﬁxed-size ordinally forgetting encoding based tri-gram neural network language model. fofe simple recursive encoding method direct sequential implementation efﬁcient parallel computation platform like gpus. here show fofe computation efﬁciently implemented sentenceby-sentence matrix multiplications particularly suitable mini-batch based stochastic gradient descent method running gpus. given sentence w··· word represented -of-k code fofe codes partial sequences computed based following matrix multiplication matrix formulation easily extended mini-batch consisting several sentences. assume mini-batch composed sequences compute fofe codes sentences mini-batch follows complete proof given appendix based theorem fofe unique almost everywhere except countable isolated choices practice chance exactly choose isolated values extremely slim realistically almost impossible quantization errors system. verify this simulation experiments possible sequences symbols count number collisions. collision deﬁned maximum element-wise difference fofe codes less small threshold figure shown number collisions various values simulation experiments shown chance collision extremely small even allow word appear times context. obviously natural language word normally appear repeatedly within near context. moreover simulation examine whether collisions actually occur real text corpora namely ltcb using bytes enwiki--pages-articles.xml. split three parts training validation testing sets. limit vocabulary size ltcb replace out-of-vocabulary words <unk> token. details datasets found table experimental results ﬁrst evaluated performance traditional fnn-lms taking previous several words input denoted n-gram fnn-lms here. trained neural networks linear projection layer hidden layers hidden units networks rectiﬁed linear activation function i.e. max. nets initialized based normalized initialization without using pre-training. mini-batch size initial learning rate learning rate kept ﬁxed long perplexity validation decreases least that continue epochs training learning rate halved epoch. performance various n-gram fnn-lms shown table fofe-fnnlms architecture parameter setting above. mini-batch size also minibatch composed several sentences words sentences corpus randomly shufﬂed beginning epoch. experiment ﬁrst investigate forgetting factor affect performance lms. trained fofe-fnnlms st-order nd-order experimental results figure shown good choice lies denotes word embedding matrix projects word indices onto continuous lowdimensional continuous space. above done efﬁciently looking embedding matrix. therefore computational efﬁciency purpose apply fofe word embedding vectors instead original highdimensional one-hot vectors. backward pass calculate gradients standard back-propagation algorithm rather bptt. result fofe based fnn-lms standard fnn-lms terms computational complexity training much efﬁcient rnn-lms. evaluated fofe method nnlms benchmark tasks penn treebank corpus words following setup vocabulary size limited preprocessing method split data training/validation/test sets large text compression benchmark ltcb enwik dataset composed ﬁrst -gram -gram ones); iii) rnn-lm hidden layer nodes using toolkit used spliced sentence bunch speed training gpus. moreover examined four fofe based fnn-lms various model sizes input window sizes nnlms used output layer full vocabulary experiments used initial learning rate bigger mini-batch fnnlmms sentences fofe models. experimental results table shown fofe-based fnn-lms signiﬁcantly outperform baseline fnn-lms also slightly overtake popular rnn-based large small forgetting factor hurt performance. small forgetting factor limit memory encoding large confuse far-away history. following experiments rest experiments paper. table summarized perplexities test various models. proposed fofe-fnnlms signiﬁcantly outperform baseline fnn-lms using architecture. example perplexity baseline bigram fnnlm fofefnnlm improve moreover fofe-fnnlms even overtake well-trained rnnlm lstm indicates fofe-fnnlms effectively model longterm dependency language without using recurrent feedback. last nd-order fofefnnlm provide improvement yielding perplexity ptb. also outperforms higher-order fnn-lms bigger model size. knowledge best reported results without model combination. examined fofe based fnnlms much larger text corpus i.e. ltcb contains articles wikipedia. trained several baseline systems n-gram using modiﬁed kneser-ney smoothing without count cutoffs; several traditional fnn-lms different model sizes input context windows total number therefore total ﬁnite number values satisfy least equation i.e. possible roots among them fraction roots lies except countable choices values never holds values result case never happens decoding except isolated points proves resultant fofe code almost unique references slava katz. estimation probabilities sparse data language model component speech recognizer. ieee transactions acoustics speech signal processing volume pages reinhard kneser hermann ney. improved backing-off m-gram language modeling. proc. international conference acoustics speech signal processing pages yoshua bengio patrice simard paolo frasconi. learning long-term dependifﬁcult. ieee dencies gradient descent transactions neural networks volume pages yoshua bengio rejean ducharme pascal vincent christian jauvin. neural probabilistic language model. journal machine learning research volume pages tomas mikolov martin karaﬁ´at lukas burget cernock`y sanjeev khudanpur. recurrent neural network based language model. proc. interspeech pages paper propose ﬁxed-size ordinallyforgetting encoding method almost uniquely encode variable-length sequence ﬁxed-size code. work fofe successfully applied neural network language modeling. next fofe combined neural networks tasks sentence modeling/matching paraphrase detection machine translation question answer etc. work supported part science technology development anhui province china fundamental research funds central universities china well nserc discovery grant canadian federal govenment. appreciate barlas oguz microsoft insightful comments constructive suggestions theorem proof decode given fofe code unknown sequence single value i-th position fofe code possible cases lead ambiguity decoding word appears current location word appears multiple times history total contribution happens case happen forgetting factor needs satisfy least following polynomial equations yong-zhe wei-qiang zhang meng liu. temporal kernel neural network language model. proc. international conference acoustics speech signal processing pages", "year": 2015}