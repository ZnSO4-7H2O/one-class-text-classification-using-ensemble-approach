{"title": "A Comparison of learning algorithms on the Arcade Learning Environment", "tag": ["cs.LG", "cs.AI"], "abstract": "Reinforcement learning agents have traditionally been evaluated on small toy problems. With advances in computing power and the advent of the Arcade Learning Environment, it is now possible to evaluate algorithms on diverse and difficult problems within a consistent framework. We discuss some challenges posed by the arcade learning environment which do not manifest in simpler environments. We then provide a comparison of model-free, linear learning algorithms on this challenging problem set.", "text": "reinforcement learning agents traditionally evaluated small problems. advances computing power advent arcade learning environment possible evaluate algorithms diverse diﬃcult problems within consistent framework. discuss challenges posed arcade learning environment manifest simpler environments. provide comparison model-free linear learning algorithms challenging problem set. reinforcement learning general environments core problems goal reinforcement learning develop agents learn interact adapt complex environments based feedback form rewards. ‘general’ environment? agents streams exist spectrum whereas methods highly tuned speciﬁc problem classes exist other. work considering arcade learning environment consists large diverse arcade games. games possess substantial structure; state presented player lowresolution images consisting basic bitmap graphics. however varied enough diﬃcult program agent work well across games. diversity arcade learning environment poses challenges well addressed reinforcement learning literature. methods work well-worn test problems necessarily handle additional complexity arcade learning environment. work single number classic modern reinforcement learning algorithms. methods chosen representative methods respective classes. methods consider learns linear policy value function equal footing regards representational power possess. restriction linear methods party computational partly pragmatic. methods consider principle non-linear function approximation instead. also evaluated number high-level variations -game subset using sarsa. include varying discounting decay exploration frequency length. results present useful guidance researchers working environment. arcade learning environment wrapper around stella emulator atari wraps emulator traditional reinforcement learning interface agent acts environment response observed states rewards. atari games remarkably well suited used reinforcement learning evaluation problems; majority games consist short episodic game-play present score player interpreted reward without issue. modern video game consoles longer game-play episodes moved away score based game-play. agents interact actions actions considered basic actions remaining actions combinations basic actions. basic action consists movement directions ﬁring no-op action. state presented agent consists pixel high wide screen pixel color value exception small number games slightly diﬀerent screen height. atari console supported reduced color space known secam used early european sets. mapping colors secam colors gives reduced state space size without much loss information. straight forward build linux environments. work required build windows published precompiled executable ﬁrst author’s website. software supports atari games version game speciﬁcally targeted game’s score needs extracted game’s episode termination conditions must identiﬁed. supported games singled bellemare training setting hyper-parameters others test set. games three non-standard screen height arcade learning environment substantially complex text-book reinforcement learning problems. exempliﬁes many attributes found hard reinforcement learning problems. section discuss number practical issues need addressed agents. need exploration well known issue must addressed reinforcement learning agent. found problem particularly exacerbated arcade learning environment. using model-free methods following approaches typically used acting rate arcade learning environment. single random action slightly perturbs state system resulting little exploration. example game space invaders player’s ship restricted dimension movement. even then exploratory actions exhibit slow movement random-walk like behavior. rarely ship move position range other. complex exploratory action sequences necessary high-level game-play also occur. example game seaquest player’s submarine must surface occasionally order obtain human-level performance. sequence actions required surface re-submerge performed exploration epsilon-greedy approach. order achieve best results practice on-policy methods parameter needs reduced time. however rarely done published research determining reduction schedule requires tedious parameter search extremely problem dependent. interesting standard exploration amount used literature also gives good results here. degenerate levels course give poor results reasonable policy still learned random policies believe caused short time scales agent acting noise averaged longer time-scales game-play occurs softmax policy another simple approach exploration q-values used form gibbs distribution actions step. chosen action sample distribution. approach avoids need choose exploration reduction schedule instead requiring scalar temperature parameter set. arcade learning environment found approach unworkable practice. main issue extreme sensitivity temperature. order work temperature needs tuned game agent. figure shows eﬀect varying temperature seaquest. clearly agent successful small range temperatures roughly none values tried gave results comparable \u0001-greedy policy best optimistic initialization perhaps eﬀective approach simplistic grid-world like-environments initialize agent believes unvisited states unreasonably good. encourages exploration states least once. environments small number states grid-world mazes like extremely eﬀective. arcade learning environment clear similar results achieved. position agent alone captured fairly directly state encodings position property needs explored. matter complicated non-linear value function approximation used value states pushed original optimistic values learning even visited. imitation learning another approach exploration guide agent someway using outside ‘teacher’. explored depth robotics known imitation learning demonstration learning simplest variant reinforcement learning agent query teacher action teacher suggests take state. number practical issues arise setting however. human teacher used large number games rate play agents. would require stella capable simulating atari around frames second xeon machine; much faster real time running additional overhead communicating pipes separate python process found basic agent could roughly actions second. using frame skip= discussed section used experiments detailed work reduced actions second. practice including overhead agents calculations able agents approximately actions second. given simulation rate training episodes single agent/environment pair usually took days. believe carefully coded implementation would several times faster this even then simulations quite computationally prohibitive. would recommend experimenting without access computing cluster experiments episode limit used bellemare large necessary. methods experimented learned reasonable parameters episodes still improving episode limit. course much larger human player requires without providing prior knowledge general game mechanics spatio-temporal relationships reinforcement learning agent always slower learn. given large size state space running non-linear value function learning algorithms also quite computationally expensive. screen images similar size datasets used current deep learning research several million frames encountered training also comparable. even using reduced state spaces tile coding practical feature extraction technique. also experimented convolutional features predeﬁned ﬁlters image step. large number convolutions required slow least using opencv theano convolutional codes. performed experiments using variant basic representation limited secam color set. representation simply encoding screen courser grid resolution colors occur block encoded using indicator features secam colors. background subtraction used encoding detailed bellemare brieﬂy outline algorithms tested. considering standard reinforcement learning framework; time step agent observes state reward action selected time step denoted state-action pair processed using state encoder function vector binary vector contains basic secam representation discussed above length followed cartesian product representation -of- action indicator vector. additional bias feature always active used. agent. parameter vector corresponding function change time indexed time-step also. parameter vector associated step size parameter reward discounting used denoted used unless noted otherwise. agents consider eligibility traces able learn delayed rewards introduces parameter discuss section implemented replacing fashion following update used sarsa ﬁrst algorithm tested sarsa perhaps widely used reinforcement learning algorithm. although since discovery algorithms developed better theoretical guarantees better performance speciﬁc problems sarsa still gives state-of-the-art performance. sarsa deﬁned update equations q-learning potentially learn better policy sarsa games death easily caused random moves included \u0001-greedy policies learned on-policy methods. downside stochastic policy give better results games. oﬀpolicy methods also known diverge cases function approximation used ettr also implemented shorted-sighted agent aims minimize expected time next positive reward instead discounted expected future reward optimized agents. advantage potentially easier learn gets non-noisy signal whenever actually reaches positive reward. disadvantage lack long term planning poorer risk-aversion. within temporal diﬀerence framework decaying eligibility traces ettr uses following update discounting used. applied ettr games reward structure contained positive rewards primary motivator agent. excluded games mainly games negative reward given goal achieved games could approached similar scheme expected length episode minimized instead. another class reinforcement learning agents seek optimize expected reward time step instead. r-learning primary example method oﬀ-policy case formulation avoids need discounting introduces additional step size parameter controls rate estimate expected reward time-step changes. update equations gradient temporal diﬀerence class algorithms attempt improve convergence properties robustness classical temporal diﬀerence algorithms. phrase learning stochastic gradient descent problem stronger convergence properties known. applied algorithm environment. gtd-style algorithms maintain weight vector well parameter vector vectors updated step step sizes respectively. update equations modiﬁed actor-critic actor-critic methods decouple policy value function predictors. critic used value function estimation actor maintains policy. consider simplest case linear. advantage actor critic methods diﬀerent learning rate parameters actor critic. typically actor evolve slower critic value function estimates time stabilize policy changes. tested non-standard variant uses epsilon-greedy policies rather gibbs/boltzmann methods common literature. choice made based results section denote critic’s weight vector. remains sarsa value function approximation changed methods several families methods could tested computational considerations. brieﬂy discuss here. least-squares methods lspi perhaps prominent alternative within class linear value function approximation methods. unfortunately require quadratic time operations state vector size. even though using smallest screen based state representation considered literature quadratic time operations slow use. also large literature methods store past visited state/action information main example fitted q-iteration given millions states visited training small subset history tractably stored ale. investigating tractable variants history-based methods interesting avenue research outside scope work. results section computed averaging number trials. trial consisted running episodes training mode episodes test mode learned policy executed. on-policy methods test episodes included exploration steps oﬀ-policy test policy purely greedy. figure shows eﬀect diﬀering discounting amount training games sarsa. optimal discounting factor consistent games best value diﬀering quite substantially seaquest asterix example. general trend apparent; using extremely high discounting ineﬀective little discounting also gives poor results. encountered convergence problems discounting used value omitted plot. roughly convex dependence discounting consistent literature simpler environments. figure shows eﬀect varying decay training games sarsa. clear best decay although values poor. decay range seems eﬀective. less sensitivity exact value decay parameter discounting parameter action taken single step. varied exploration length steps. somewhat mixed results. figure shows sarsa q-learning depending value length exploration periods better standard \u0001-greedy diﬀerence close noise ﬂoor. otherwise downward trend average reward obtained small up-tick around length six. algorithm described section trials games could applied. issues arose needed addressed. several game algorithm combinations exhibited behaviour agent would make progress consistently episode limit zero reward. happened particular zaxxon montezuma’s revenge. cases excluded comparisons. also encountered divergent behaviour q-learning roughly time surprising time. runs divergence occured excluded. consider sarsa baseline. first compare performance consistency methods. algorithm computed average reward test episodes trail game. computed relative performance method average game ratio trial average baseline. average computed middle games remove outliers. also computed test reward standard deviation across trails game measure consistency. median quartiles across games given together relative performance table sarsa ettr performed similarly relative performance metric. suggests additional complexity narrower focus ettr help. hypothesis ettr would perform consistently large variances potentially eﬀect relative performance ﬁgures significantly. robust method comparing pair algorithms count number games average test reward higher game other. table contains comparisons. cases games failed converge timed-out excluded table. results table support inferences made using relative performance measure. sarsa appears better r-learning larger margin possibly chance otherwise on-policy methods show similar results. would expected little correlation sarsa ettr suggesting perform well diﬀering problems. moderate correlation sarsa would expected given similarities. learning highly correlated whereas negatively correlated r-learning. suggests behaviour r-learning closer on-policy methods given it’s correlation sarsa. relevant properties learning algorithm speed convergence. method computed average reward last episodes training episodes compared average across episodes preceding those. average within considered method converged. percentage converged trails ﬁnished method sarsa ettr convergence rates similar across methods considered. results also suggests step size constants chose reasonable. less stable games zaxxon agents could stuck degenerate reward policies. makes comparisons agents diﬃcult. unsatisfying average eﬀects hides true nature learned. oﬀ-policy methods q-learning known convergence issues function approximation used even linear case considered. normally considered theoretical issue \u0001-greedy policies used diﬀerence \u0001-greedy pure on-policy minor. however high probability divergence practice q-learning detailed results section. surprisingly also experienced divergence despite theoretical convergence results. implementation projection step detail. theory requires projection however suggest needed practice. believe likely cause convergence issues combined perhaps sensitivity step parameters. interestingly oﬀ-policy r-learning method divergence issues performed nearly well sarsa experiments. appears best choice among oﬀ-policy methods considered. even within class linear value function approximation clearly experiments sub-optimal policies learned many trials. plotting reward episode time shows average reward episode stops improving level optimal reward level level varying trials. speculate epsilon-greedy policy resulting much exploration rather eﬀect introduce jitter helps prevents degenerate policies followed repeated ineffective actions behavioural loops. based need large learning rates seems noise introduced value function taking large steps parameter space contributing exploration. looking point scores give clear indication policy actually doing. better idea sorts policies learned examined videos agent’s gameplay number games. majority games examined learned policies consisted simple sequence movements together holding key. example seaquest holding together alternating left-down right-down actions appeared policy learned sarsa time. similar behaviour learned krull repeatedly jumping gives high score. noticed linear algorithms considered capable learning move particular location screen start episode repeating actions. example gopher agent learned move ﬁxed point right starting location repeatedly trigger action. appeared eﬀective simplistic enemy zaxxon seconds score zero unless move center screen away starting position left. turned extremely diﬃcult policy learn negative reinforcement occurs long actions. algorithms looked runs completely failed learn policy discovered later abandoned part training. comparison diverse environments performed allows environments algorithm best worse first consider oﬀ-policy algorithms. tended perform best problems exploration actions could major negative eﬀect outcome consistent expectations. example zaxxon mentioned above consistently able stay safe region screen. looking particular q-learning large improvement performance switching training regime test regime seaquest assault time pilot asterix smaller improvements many games. games sharp drop performance occurs tutankham space invaders. games require level random left right movement particularly problem crazy climber game player move forward time occasional left right movement required around obstacles. potentially soft-max policy could avoid issues mentioned section temperature parameter needs ﬁne-tuned separately game large diﬀerences score magnitudes. ettr method able outperform algorithms number games including gopher q*bert road runner. games common pattern need react movement squares adjacent player character. pattern shared number games also clear conclusions drawn that. ettr agent signiﬁcantly less risk aversion algorithms considered expected behaviour. example seaquest instead moving player’s submarine bottom screen safest location stayed near screen rewards available. terms actual scores didn’t signiﬁcant negative eﬀects lack risk aversion games looked agent able outperform methods number problems also. signiﬁcantly better scores boxing alien frostbite. method also tuned greater degree sarsa good choice tuning per-problem basis used. additional tuning also downside although based it’s consistent performance doesn’t appear case here. arcade learning environment relatively little work directly targets introduces environment. consider performance several state representations sarsa. naddaf presents tables comparing decay learning rates training games. plots section comprehensive. mnih consider deep learning methods subset games showing super-human performance games. results paper provide guide researchers working arcade learning environment. give guidance eﬀective decay discounting epsilon-greedy constants. also provide comparison standard algorithms showing commonly used linear on-policy algorithms give similar performance. also show common oﬀ-policy methods serious issues complexity environment give recommendations based empirical results algorithms used diﬀerent game environments.", "year": 2014}