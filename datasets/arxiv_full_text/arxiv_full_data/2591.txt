{"title": "Towards Representation Learning with Tractable Probabilistic Models", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Probabilistic models learned as density estimators can be exploited in representation learning beside being toolboxes used to answer inference queries only. However, how to extract useful representations highly depends on the particular model involved. We argue that tractable inference, i.e. inference that can be computed in polynomial time, can enable general schemes to extract features from black box models. We plan to investigate how Tractable Probabilistic Models (TPMs) can be exploited to generate embeddings by random query evaluations. We devise two experimental designs to assess and compare different TPMs as feature extractors in an unsupervised representation learning framework. We show some experimental results on standard image datasets by applying such a method to Sum-Product Networks and Mixture of Trees as tractable models generating embeddings.", "text": "abstract. probabilistic models learned density estimators exploited representation learning beside toolboxes used answer inference queries only. however extract useful representations highly depends particular model involved. argue tractable inference i.e. inference computed polynomial time enable general schemes extract features black models. plan investigate tractable probabilistic models exploited generate embeddings random query evaluations. devise experimental designs assess compare diﬀerent tpms feature extractors unsupervised representation learning framework. show experimental results standard image datasets applying method sum-product networks mixture trees tractable models generating embeddings. density estimation unsupervised task learning model estimating joint probability distribution random variables estimator learned inference computing probability queries certain states r.v.s e.g. marginals examples commonly used density estimators comprise probabilistic graphical models like bayesian networks markov networks represent conditional dependence assumptions graph formalism. hand neural models represent factorization means neural architectures like restricted boltzmann machines variations fully visible sigmoid belief networks embed inference algorithm network evaluation like case variational autoencoders accurately estimating complex distributions data eﬃciently querying required qualities good density estimator. performing exact inference however still hard task general. pgms example even approximate inference routines exponential worst case want investigate extract useful representations general purpose density estimators. ﬁnding exploit probabilistic models already learned encode unsupervised fashion using existing learning algorithms order transform initial data another representation space. speciﬁcally want build embedding sample transformation provided probabilistic model learned estimate embeddings could employed tasks clustering classiﬁcation allowing transfer unsupervised learning schemas usually done representation learning usefulness representations stated proxy performance metrics subsequent supervised tasks e.g. accuracy scored predicting previously unseen sample labels moreover could enable ways assess compare diﬀerent models going beyond classic likelihood comparison highly misleading sometimes build mapping take advantage geometric space induced estimate according model straightforward build embedding component result single inference step according query i.e. classic implemented past employ probability space embeddings construction hand-crafted problem dependent kernels like p-kernels another possibility leverage additional ﬁrst order information encoded gradient already done fisher vectors fisher kernels however implement classic approaches derive analytically embedding computation according model parameters task always feasible models interested generic procedure employs diﬀerent density estimators black boxes despite parametrizations inner representations. following ﬁrst line thought capture diﬀerences among samples projected various spaces induced probability densities encoded model basic idea construct features random query generator computing values according evaluation hence tractable inference routines become crucial construct complex enough representations evaluating several times. following sections introduce tractable probabilistic models deﬁne possible schemas generate embeddings leveraging tractability property. discuss possible empirical evaluation framework show results experimental application benchmark image datasets showing promising results. treewidth pgms alleviating computation partition function limiting model expressiveness terms representable conditional dependencies. comprise models hidden markov models tree distributions mixture latent r.v. variants pointwise marginal queries answered time linear number r.v.s. computational models derived knowledge compilation process involving elicitation another representation form usually encoding computation graph. case arithmetic circuits sentential decision diagrams enabling intractable compiled data structures marginals even inference answered time linear size structures neural autoregressive models factorizing according chain rule modeling factor neural network. models like neural autoregressive distribution estimators masked autoencoder distribution estimators leverage constrained feedfoward autoencoder networks provide pointwise inference linear size networks. answer marginal queries polynomial time variants like eonade allowing order agnostic training factors necessary complete evidences marginals even computation partition function computable tractable time sum-product networks spns deep neural architectures equivalent ﬁnite domains compiling network polynomial sophisticated architecture introduces latent variable semantics diﬀerently tractable neural models spns guarantee hidden neuron still models correct probability distribution restriction input r.v.s named scope. constrained form allows direct encoding input space. great interest around spns also motivated increasing arsenal structure learning algorithms arising literature makes spns deep architectures structure directly eﬀectively learned crafted tuned hand. tractable models like hmms leverage emission probabilities evaluated sample generate particular kernel case example model dependent embedding space construction tractability model mandatory also input space deﬁnition neural architectures representations usually extracted activations hidden neurons evaluation input sample. common practice extract embeddings last layers comprising higher level representations hence informative ones could adopt approach neural autoregressive density estimators probabilistic meaning extracted features would lost direct comparison models would possible. extracting node activations would retain probabilistic interpretation however would highly depend learning algorithm employed train model. moreover still clear hidden neurons choose extract meaningful embeddings spn. following layer-wise procedure embedding extraction cope diﬀerent scope semantics spn. general rule thumb could employ nodes nodes kind argue diﬀerent tpms black density estimators used generate vector embeddings common ground deﬁning template queries answered. able evaluate diﬀerent regions probability density surfaces induced diﬀerent queries. instance deﬁning marginalized conditional queries would represent probability distributions diﬀerent joint highlighting local interactions input r.v.s. basic generate queries random fashion. however deﬁning sophisticated schemes possible. domain knowledge guided embedding extraction process could thought sort feature engineering step query template families devised features. propose diﬀerent embedding generation schemas exploiting randomness. ﬁrst feature template constructed random marginal component k-dimensional embedding generate. sample embedding exactly computed indicates restriction sample vector r.v.s again random selection criterion lead domain knowledge e.g. samples images meaningful extract adjacent pixels r.v.s sketch process presented algorithm second approach presented algorithm extracts random randomly chosen input samples. trained reduced dataset. embedding component generated evaluating original case approach would similar dictionary learning proposed case however feature generation scheme directly done pointwise evaluation whole turn directly learned patches. hand second requires tractable pointwise inference. therefore easily adaptable larger models since locally learned models globally evaluated marginal computations involved. models allow diﬀerent tpms evaluated exact feature templates since generation random queries patches done comparative experiment. aspect allows fairer comparisons among diﬀerent models moreover enables additional understand structure models feature selection routines applied generated embedding spaces could reveal particularly eﬀective sets features hinting powerful r.v.s interactions. plan conduct thorough empirical evaluation comprising least tpms kinds presented second section. meaningfulness usefulness generated embeddings shall measured diﬀerent metrics diﬀerent tasks. instance multi-class multi-label classiﬁcation tasks worth investigating them several metrics like accuracy hamming loss exact match shall taken account. main objective intensive empirical comparison investigate different tpms would behave general embedding extraction framework. present smaller empirical evaluation ﬁrst proposed approach classiﬁcation task three standard benchmark image datasets. employ spns mixture trees marginal inference tractable. want determine diﬀerent models diﬀerent expressive capabilities generate embeddings marginal query evaluations. learn supervised classiﬁer employ logistic regressor regularizer one-versus-rest setting embeddings generate. implementation available scikit-learn framework. representation determine regularization coeﬃcient value choosing model best validation accuracy. simplest baseline possible apply classiﬁer directly initial data representation denoting declare extracted representation useful expect surpass accuracy score consider following datasets rectangles introduced convex always introduced letters presented caltech- silhouettes binarized version mnist processed accuracy scores best models bmn. diﬀerently regularized models dataset employing learnspnb structure learning algorithm adopting simpliﬁed iterated clustering strategy determine insertion inner nodes networks. test threshold datasets. eﬀect parameter regularize learned spns therefore governs model capacity. denote models spn-i spn-ii spn-iii respectively. number mixture component vary among ending three models mt-i mt-ii mt-iii. evaluate models randomly generated marginal queries dataset. query select r.v.s corresponding adjacent pixels rectangular image patch minimum sizes pixels maximum pixels pixels remaining datasets. figure shows accuracy scores spn-i spn-ii spn-iii mt-i mt-ii mt-iii dataset adding features time. ﬁrst consideration draw models able beat baselines without using random features. features appear enough better scoring models even features provide nice boost points accuracy compared baselines. features score high accuracy indicating geometric space induced respective models indeed meaningful. considerations hint eﬀectiveness approach despite randomness. also visible embedding accuracies improve number feature increases. true almost scenarios exception con. datasets sort model herding noted model performances growing noticeably features added exception reguarized model spn-i least regularized spn-iii con. adding even features seemed leave performance decreasing pattern embedding length reached behavior likely introduction many irrelevant features curse dimensionality. moreover visible extracted embeddings outperform generated ones almost scenarios exception con. compare likelihood models ones would outperform ones. explanation behavior found better capacity structure learner optimizing marginal loglikelihood nevertheless clearly suggests learned representation comparison informative dimension along assess diﬀerent probabilistic models. concerning performance diﬀerently regularized models note specialized models like spn-iii spn-ii tend perform less well spn-i pattern seen mt-iii well. evidence considered form overﬁtting density estimators. greater degree able reconstruct model training even damage test data likelihood makes extracted embedding probably speciﬁc least marginal queries. devised general model agnostic schema extract embeddings tractable probabilistic models exploiting random query generations evaluations. plan conduct extensive empirical experimentation investigate depth several models behavior according diﬀerent embedding evaluation performances. proposed empirical comparison diﬀerent standard image datasets exploiting random marginal queries hints eﬀectiveness approach. proposed approaches could used compare diﬀerently uncomparable models representation learning framework. could serve role similar parzen windows models likelihood cannot easily computed compared generative framework. sophisticated query types improve general basic schema. would interesting correlate embedding performances diﬀerent query types according diﬀerent models.", "year": 2016}