{"title": "Optimistic Simulated Exploration as an Incentive for Real Exploration", "tag": ["cs.LG", "cs.AI"], "abstract": "Many reinforcement learning exploration techniques are overly optimistic and try to explore every state. Such exploration is impossible in environments with the unlimited number of states. I propose to use simulated exploration with an optimistic model to discover promising paths for real exploration. This reduces the needs for the real exploration.", "text": "example reinforcement learning task figure environment grid world. agent path start goal. environment gives reward goal reached. task repeated. value state represents summed expected reward agent expects continues follow policy form state. q-learning typically used learn values state-action transitions. environment limited number states could exploration optimistic rewards unknown actions. exploration would explore every action every state greater reward. wide exploration impossible environment without boundaries. abstract. many reinforcement learning exploration techniques overly optimistic explore every state. exploration impossible environments unlimited number states. propose simulated exploration optimistic model discover promising paths real exploration. reduces needs real exploration. study exploration environments unlimited number states. simulated exploration incentive real exploration. simulated exploration proposes promising paths explore. describe kind exploration section idea simulated exploration discover promising paths real exploration. reduces needs real exploration. approximate model environment used simulate exploration. approximate model approximation real environment. predicts environment behave executing action state. model predicts next state environment reward obtained action. approximate model could optimistic pessimistic. optimistic model assume greater reward better transitions possible real environment. example optimistic model could assume barriers path. better optimistic models pessimistic ones. using optimistic model lead discovery accurate model environment discovery better paths. pessimistic model would assume better path exists would miss optimistic model accurate possible prevent many mistakes corrections. overly optimistic model would assume every state transition lead state highest reward. model could accurate risk become pessimistic states. risk accept don’t want explore states unlimited state space. example model aiming accurate could assume already tried actions similar effects states. need true wall would corrected model optimistic. example model given section less optimistic models need work incentive exploration own. greedy agent sees reason actions don’t lead directly states higher value. simulated exploration needed discover promising paths. given approximate model simulated exploration promising paths model. tries different actions model explores lead. exploration real simulated without taking actions real environment. simulation done planning method. executed simulated explorations interleaved real acting. following code shows body typical agent. simulated exploration would inside self.planner.plan method possibly planning. simulated exploration could discover promising path leads existing predicted reward. promising path would visible agent states high value. states could still unexplored real environment. describe ways spread simulated exploration trajectory sampling prioritized sweeping. method simulate experience within given approximate model could used. simulated exploration could follow trajectory generated exploration policy. trajectory could start state. example could start agent’s real current state. restricts simulated state space states near agent. code shows example simulated exploration along trajectory. maximal depth sampled trajectory limited limit amount computation done inside planning step. experiments used -state grid world mentioned inside reinforcement learning survey. figure shows used environment. environment fully deterministic. speciﬁed initial suboptimal path inside grid world. suboptimal path gives hint reward unlimited state space. allows simulated exploration without optimistic assumptions reward. simulated exploration policy could completely different policy used acting real environment. simulated exploration policy used sample states state space. approximate model used estimate values sampled states. prioritized sweeping could used simulate exploration combination methods. used needed approximate model able return parents state. related unexplored states also returned possible parents. depth sweeping unexplored states limited setting minimal considered priority. states amount change threshold swept. work unless approximate model repeatedly predicts reward unexplored states. start position agent ﬁxed top-right corner grid world. reduce amount exploration relevant start position ﬁxed. case state space used acting smaller whole available state space. model give information parents still possible prioritized sweeping combination another simulated exploration. needed correct remembered parents optimistic model corrected. otherwise wrong parent would continue updated optimistic values. remembered parents could easily corrected used model produces distribution possible next states. next states possible children parent children could discarded. model based recent action effects records last seen effects action. predicts seen action effects state. also tries optimistic ignores action effects state. could happen wall hit. remember wall. following code shows code model. used initial suboptimal path every action least once. allows model based action effects predict unexplored states directions. alternative would additional small amount exploration beginning task. used models provide different predictions state transitions rewards. model based observations predicts already seen transitions rewards. model based recent action effects tries predict future state transitions assumes extra reward them. prediction based recent action effects wrong wall considered less accurate. accurate prediction available. asks models prediction starting accurate model. asked model knows nothing given transition less accurate model asked. code combined model follows. exploration optimistic initial values assumes value inside every state-action transition. strategy incentive enough optimal policy deterministic environment. initial state-action values exploration algorithms value serve incentive exploration remains possible decrease important able decrease value tried state-action transitions. prevents agent stuck. simulated trajectory sampling tested three different maximal depths used simulated exploration policy selects random action every step. sampled trajectory always starts current agent’s state. simulated exploration tries trajectories real step. prioritized sweeping unexplored states uses parents supplied approximate model environment. approximate model consists multiple models combine parents predicted different models. letting least accurate model predict parents. accurate models used prune list possible parents. second least accurate measured amount exploration done different exploration algorithms. exploration ﬁnished shortest path found used subsequent episodes. initial path steps long. shortest path steps long used algorithms able performance different explorations. columns report number steps ﬁnding ﬁnal policy numbers explored states state-actions. trajectory sampling used maximal depths interesting note relation number explored states number explored state-actions. exploration equipped simulated exploration tried average actions every visited state. hand exploration optimistic initial values tried four actions almost every visited state. trajectory sampling required lowest amount exploration shortest path. guarantee shortest path. maximal depth sampled trajectory limits space search promising paths. limits amount exploration done also allows miss optimal path. prioritized sweeping unexplored states performed well. required explore times less state-actions exploration optimistic initial values. sweeping unexplored states guarantees possible promising path exists used approximate model. lower amount real exploration possible thanks simulated exploration approximate optimistic model. methods suitable environments impossible learn optimistic model unexplored states environments. methods risk miss optimal path pessimistic model used. used approximate model predict unexplored reward. needed start initial suboptimal path show reward. sophisticated approximate models could also predict reward. using observed model planning pioneered dyna architecture sutton. dyna planning continues update seen transitions changes estimated state-action value. extensions provide prioritized sweeping usage linear function approximation represent environment. idea optimistic models came book reinforcement learning introduction. discusses optimistic models miss promising path. also proposes trajectory sampling large tasks. many works touched problem exploration large spaces. smart kaelbling reduced amount exploration using initial knowledge. supplied agent example trajectories. trajectories agent driven human operator piece code. example trajectories needed optimal. give hints reward envelope methods limit amount planning restricting state space known environment. need explore environment fully known still needed limit number states consider. apprenticeship learning aims prevent destructive exploration. teacher ﬁrst demonstrates task. demonstration serves learn approximate model environment. optimal policy learned off-line model tested later real environment. experience serves improve model cycle continues. apprenticeship learning intended propose promising paths unexplored states considered model. extension apprenticeship learning uses learned approximate model policy improvement direction. could viewed search promising direction steer policy parameters. needed amount steering tested real environment. sutton integrated architectures learning planning reacting based approximating dynamic programming. proceedings seventh international conference machine learning sutton szepesvari geramifard bowling dyna-style planning linear function approximation prioritized sweeping. proceedings conference uncertainty artiﬁcial intelligence wiering schmidhuber efﬁcient model-based exploration. proceedings fifth international conference simulation adaptive behavior animals animats danihelka student department cybernetics prague. previously software engineer working billing systems internet games data mining multiple sites. still likes read well written source code.", "year": 2009}