{"title": "Lifted Graphical Models: A Survey", "tag": ["cs.AI", "cs.LG"], "abstract": "This article presents a survey of work on lifted graphical models. We review a general form for a lifted graphical model, a par-factor graph, and show how a number of existing statistical relational representations map to this formalism. We discuss inference algorithms, including lifted inference algorithms, that efficiently compute the answers to probabilistic queries. We also review work in learning lifted graphical models from data. It is our belief that the need for statistical relational models (whether it goes by that name or another) will grow in the coming decades, as we are inundated with data which is a mix of structured and unstructured, with entities and relations extracted in a noisy manner from text, and with the need to reason effectively with this data. We hope that this synthesis of ideas from many different research groups will provide an accessible starting point for new researchers in this expanding field.", "text": "multi-relational data entities different types engage rich relations ubiquitous many domains current interest. example social network analysis entities individuals relate another friendships family ties collaborations; molecular biology frequently interested modeling chemical substances entities interact with inhibit catalyze another; social media applications users interact pages online resources related hyperlinks; natural language processing tasks often necessary reason relationships documents words within sentence document. incorporating relational information learning reasoning rather relying solely entity-speciﬁc attributes usually possible achieve higher predictive accuracy unobserved entity attribute e.g. example exploiting hyperlinks pages improve categorization accuracy developing algorithms representations effectively deal relational information important also many cases necessary predict existence relation entities. example online social network application interested predicting friendship relations people order suggest friends users; molecular biology domains researchers interested predicting newly-developed substances interact. given diversity applications involve learning reasoning multi-relational information surprising ﬁeld statistical relational learning recently experienced signiﬁcant growth. survey provides detailed overview developments ﬁeld. limit discussion representations seen deﬁning graphical model using relational language alternatively lifted analogs graphical models. although omit discussions several important representations stochastic logic programs problog based imposing probabilistic interpretation logical reasoning limiting scope survey able provide focused uniﬁed discussion representations cover. models refer reader great variety existing applications cannot possibly justice them; therefore focus representations techniques applications mentioned passing help illustrate point. survey structured follows. section deﬁne introduce preliminaries. section describe several recently introduced representations based lifting graphical model. goal section establish uniﬁed view available representations deﬁning generic template model discussing particular models implement various aspects. statistical relational learning studies knowledge representations accompanying learning inference techniques allow efﬁcient modeling reasoning noisy uncertain multi-relational domains. classical machine learning settings data consists single table feature vectors entity data. crucial assumption made entities data represent independent identically distributed samples general population. contrast multi-relational domains contain entities potentially different types engage variety relations. thus multi-relational domain seen consisting several tables attribute tables entity type contain feature-vector descriptions corresponding entity relationship tables establish relationships among entities domain. consequence relationships among entities longer independent assumption violated. characteristic multi-relational domains typically noisy uncertain. example frequently uncertainty regarding presence absence relation particular pair entities. summarize effective representation needs support following essential aspects needs provide language expressing dependencies different types entities diverse relations; needs allow probabilistic reasoning potentially noisy environment. establish notation terminology used rest survey. draws probability theory logic programming sometimes term describe different concepts. example word variable could mean random variable logical variable. avoid confusion distinguish different meanings using different fonts summarized table first-order logic first-order logic provides ﬂexible expressive language describing typed objects relations. distinguishes among four kinds symbols constants variables predicates functions constants describe objects domain alternatively call entities. example notation table entities. entities typically typed. logical variables placeholders allow quantiﬁcation e.g. predicates represent attributes relationships evaluate true false e.g. publication establishes relation paper author category provides category paper predicates strings parentheses specify types entities predicates operate. functions evaluate entity domain e.g. motherof. adopt convention names predicates functions start capital letter. number arguments predicate function called arity. term constant variable function terms. predicate applied terms called atom e.g. publication. positive literal atom negative literal negated atom. formula consists positive negative literals connected conjunction disjunction operators e.g. ¬friends friends. variables formulas quantiﬁed either existential quantiﬁer universal quantiﬁer follow typical assumption quantiﬁer speciﬁed variable understood default. formula expressed disjunction positive literal called horn clause; horn clause contains exactly positive literal deﬁnite clause. positive literal deﬁnite clause called head whereas remaining literals constitute body. deﬁnite clauses alternatively re-written implication body head. terms literals formulas called grounded contain variables. otherwise ungrounded. grounding also called instantiation carried replacing variables constants possible type-consistent ways. groundings denoted constraints specify groundings allowed. general predicate function arguments typed type constraints present default. connect terminology probability theory note value ground atom governed random process becomes random variable values {true false}. example represent entities i.e. speciﬁc individuals; grounded atom friends represents assertion friends. given probability distribution governs value friends reason reason ordinary random variables. addition helpful treat unground atoms parameterized sense variables parameters replaced constants become rvs. example logical variables friends parameterized ground replacing parameters actual entities obtain rvs. refer parameterized par-rvs short e.g. friends par-rv. object-oriented representations alternative attributes relations entities described using object-oriented representation again represent speciﬁc entities domain whereas variables entity placeholders. entities typed. attributes relations expressed using notation analogous commonly used object-oriented languages. example x.category refers category paper whereas x.author refers authors. inverse relations also allowed e.g. y.author− refers papers author. using notation chains relations conveniently speciﬁed e.g. x.author.author−.category gives categories papers written authors note author relation typically one-to-many general x.author refers entities i.e. authors paper. this object-oriented languages allow aggregation functions mean mode sum. example write mode. statements grounded instantiated replacing variables entities domain. analogous view ungrounded relation/attribute chains well aggregations thereof par-rvs. structured query language natural manipulate relational data often stored relational database using sql. thus surprisingly used representation models discussed survey. self-sufﬁciency provide brief overview. attributes relations objects viewed deﬁning relational schema attribute table corresponds entity type relationship table corresponds relation type entities engage. therefore natural manipulate data using sql. review select statement used represent relational dependencies models. purposes useful form select statement expressed follows also draws heavily graphical models. therefore next introduce basic concepts area. detailed introduction graphical models refer reader general describe probability distribution binary needs store parameters possible conﬁguration value assignments rvs. however frequently sets conditionally independent another thus many parameters repeated. avoid redundancy representation several graphical models developed explicitly represent conditional independencies. general representations factor graph factor graph consists tuple arbitrary strictly positive functions called factors. typically drawn bipartite graph partitions vertices factor graph consist factors respectively. edge factor necessary computation i.e. factor connected arguments. result structure factor graph deﬁnes conditional independencies variables. particular variable conditionally independent variables share factors given variables participates common factors. network represented directed acyclic graph whose vertices probability distribution speciﬁed providing conditional probability distribution node given values parents. simplest expressing conditional probabilities conditional probability tables list probability associated conﬁguration values nodes. bayesian network converted factor graph straightforward follows. node introduce factor represent conditional probability distribution given parents. thus computed function parents. case product automatically normalized i.e. normalization constant sums markov network undirected graphical model whose nodes correspond variables computes probability distribution product strictly positive potential functions deﬁned cliques graph i.e. variables connected maximal clique potential function takes arguments. convenient representation potential functions log-linear model potential function computed function variables represented exponentiated product exp). expression learnable parameter feature captures characteristics variables evaluate value general potential function deﬁned clique. variety feature functions learnable parameter deﬁned variables. markov networks directly factor graphs—to convert markov network factor graph maximal clique markov network include factor evaluates product potentials deﬁned clique. advantage discussing factor graphs rather bayesian networks markov networks others describing algorithms factor graphs make immediately available representations viewed specializations factor graphs. especially true inference algorithms. hand beneﬁcial discuss least aspects learning techniques separately directed undirected models. existing representations split major groups. ﬁrst group consists lifted graphical models representations structured language deﬁne probabilistic graphical model. representations second group impose probabilistic interpretation logical inference. already discussed allow greater depth limit ﬁrst group languages. provide convenient representation describes common core lifted graphical models start par-factor graphs short parameterized factor graphs deﬁning terminology par-factor graph analogous factor graph generalizes large class models allows much possible present uniﬁed treatment regardless whether based directed undirected representations. par-factor graph lifted factor graph sense that instantiated par-factor graph deﬁnes factor graph. consists par-factors. par-factor represented triple parameterized random variables function operates variables evaluates strictly positive value constraints variables instantiated. par-factor graph par-factors lifted graphical models representations viewed derived generic par-factor graph specifying language used express φi-s ai-s ci-s. probability distribution deﬁned par-factor graph given following expression last line considers possible par-factor graph multiplies together values instantiations subset given assignment needed evaluate expression uses relevant computation i.e. corresponding instantiations parameterized compare equations that indeed instantiating par-factor graph obtain factor graph. however here factors instantiations par-factor share common structure parameters; thus par-factor graphs allow better generalization. remainder section ﬂesh description considering several popular representations discussing viewed speciﬁc instantiations par-factor graphs grouping according type graphical model deﬁne i.e. directed undirected hybrid. meant exhaustive list; rather goal highlight different ﬂavors representations. relational markov networks. name suggests relational markov networks deﬁne markov networks relational representation. rmns object-oriented language particular par-factor deﬁned specify par-factors. using select statement select...from part establishes par-rvs part establishes constraints instantiations. resulting instantiating par-rvs multinomial i.e. take multiple discrete values. tuple returned select statement constitutes instantiation par-factor; thus variables appear returned tuple form clique markov network deﬁned rmn. cliques instantiations par-factor share potential function takes log-linear form. particular particular instantiation par-rvs i.e. returned tuples. then speciﬁc values assigned variables exp) where markov networks arbitrary feature function note however unlike markov networks potential function φ—its feature function parameter—is shared across instantiations par-factor property common languages consider here fact main deﬁning characteristics—that relational languages allow generalization parameter tying. illustrate provide example collective classiﬁcation hyperlinked documents presented goal following par-factor clique label assignments hyperlinked documents order capture intuition documents link another typically correlated labels. select statement sets cliques specify potential function used cliques. deﬁnition used incorporate domain knowledge model. example know pages tend link pages category deﬁne exp) feature function takes form indicator function returns proposition true otherwise. positive encourages hyperlinked pages assigned category negative discourages this. taskar shown associative rmns factors favor labels clique inference learning tractable closely approximated non-binary case. markov logic networks. markov logic networks also deﬁne markov network instantiated. par-factors mlns speciﬁed using ﬁrst-order logic. par-factor represented ﬁrst-order logic rule attached weight consists par-rvs appear rule; therefore instantiated markov network instantiation grounding establishes clique among appear instantiation. instantiated boolean-valued. potential function implicit rule describe next. particular instantiation particular assignment truth values then exp) true given truth assignment otherwise. words clique potentials mlns represented using log-linear functions ﬁrst-order logic rule acts feature function whereas weight associated rule parameter. illustration present example patterns human interactions smoking habits considered. regularity domain friends similar smoking habits i.e. people friends either smokers non-smokers. captured following ﬁrst-order logic rule par-rvs par-factor deﬁned rule {friends smokes smokes} every possible instantiation par-rvs establishes clique instantiated markov network e.g. entities instantiated markov network contain cliques discussed mlns specify constraints par-factor. mlns special mechanism describing constraints constraints implicit rule structure. ways follows. first allow instantiations ground given variable speciﬁc entity replacing variable entity name. example want constrain rule above simply write friends smokes). second general introducing constraints predicates whose values known inference time discriminatively trained models. example suppose know inference time observe evidence truth values groundings friends predicate goal infer people’s smoking habits. then rule friends smokes) seen setting clique smokes values entities friends. because particular pair entities friends false corresponding instantiation rule trivially satisﬁed regardless assignments groundings smokes. therefore instantiations ignored instantiating mln. variant mlns hybird model extends mlns allow real-valued predicates. hybrid mlns formula contain binary-valued real-valued terms. formulas evaluated interpreting conjunction multiplication values. probabilistic similarity logic. another lifted markov network model probabilistic similarity logic allows reasoning similarities. par-factors deﬁned weighted rules expressed ﬁrst-order logic object-oriented languages. thus similar mlns par-factor consists rule attached weight whose par-rvs determine whose structure determines potential function however unlike rmns mlns resulting instantiating par-rv continuous-valued interval thus instantiating rules results continuous-valued markov network. constraints speciﬁed similar manner mlns. unlike previous models additionally supports reasoning similarities entity attributes sets entities. similarity functions real-valued function whose range interval formulas mixed regular relational terms. illustrate consider example task infer document similarities wikipedia based document above represent similarity functions term enclosed curly braces {a.editor} refers entities related variable relation. par-rvs rule before rule deﬁnes clique among possible instantiation par-rvs. evaluation rule assignment instantiation par-rvs involves combining boolean values similarity values. done using t-norms/t-conorms generalize ﬁrst-order logic operations conjunction/disjunction. t-norm/t-conorm pair used lukasiewicz t-norm preferred property linear values combined. letting boolean similarity values lukasiewicz t-norm deﬁned follows generalizes standard formulation joint distribution interpreting penalty distance satisfaction rules given assignment values instantiated rvs. probability observing given joint assignment values given above arbitrary distance function distance satisfaction function computed vector weighted distances satisfaction rule instantiations. pick l-norm distance back standard formulation potential function associated clique given exp)). imperatively deﬁned factor graphs. par-factor graphs also deﬁned using imperative programming language done factorie implementation imperatively deﬁned factor graphs factorie uses scala strongly-typed functional programming language provides model designer enormous ﬂexibility. factorie variable types represented typed objects sub-classed potential relations variables particular types represented instance variables corresponding classes. novel types variables variables deﬁned standard data structures linked lists used variable type implementations. par-factor deﬁned factor template class whose arguments determine par-rvs factor template class contains unroll procedures par-rv return instantiations par-factor corresponding given instantiation arguments. instantiation constraints therefore encoded unroll methods. potential function implemented statistics method factor template class thus arbitrary form consider simpliﬁed version example deﬁnes factor template task coreference resolution. goal factor template evaluate compatibility entity mention canonical representation particular underlying entity assigned. unroll method produces factor given mention entity assigned. given entity unroll method produces factors mention associated entity. subsection describes representations deﬁne bayesian networks instantiated. par-factors representations special form. particular par-factor consists child par-rv parent par-rvs {c}. function represents conditional probability distribution given expression equation automatically normalized i.e. specifying directed models care must taken ensure instantiations different worlds would result cycle-free directed graphs. however discussed section problem undecidable general guarantees exist restricted cases. bayesian logic programs. bayesian logic programs par-rvs expressed logical atoms dependency structure parents represented deﬁnite clause called bayesian clause head consists body consists implication replaced indicate probabilistic dependency. distinction ordinary logical clauses bayesian clauses latter logical atoms restricted evaluating true false illustrated example below. par-factors formed coupling bayesian clause conditional probability distribution values given values kersting raedt give example genetics blood type person depends inheritance single gene copy which inherited mother mother copy inherited father father. blps dependency expressed example take values whereas take values speciﬁcation par-factor completed providing possible combination values assigned probability distribution values e.g. conditional probability table. using blps next illustrate another aspect common directed representations combining rules. following example suppose genetics domain following rules relational bayesian networks. relational bayesian networks also represent par-rvs logical atoms. separate par-factor deﬁned unknown relation domain child par-rv atom par-factors represented probabilistic formulas syntax bears close correspondence ﬁrst-order logic. conditional probability distribution given implicit probabilistic formula range evaluated function values variables particular probabilistic formulas rbns recursively deﬁned consist constants extreme cases correspond true false respectively; indicator functions take tuples logical variables arguments correspond relational atoms; convex combinations formulas correspond boolean operations formulas; ﬁnally combination functions mean combine values several formulas. illustrate consider slight adaptation example task given pedigree individual reason values relations indicate whether inherited dominant allele father mother respectively. probabilistic formula above φknownfather auxiliary sub-formula evaluates true father included pedigree false otherwise; φa−from−father auxiliary sub-formula deﬁned mean values father φa−from−father mean{fa ma|father}; learnable parameter take values range undirected models discussed earlier rbns provide dedicated mechanism specifying constraints however constraints speciﬁed either replacing logical variables actual entity names formulas including tests background information case φknownfather sub-formula above. probabilistic relational models. probabilistic relational models take relational database perspective object-oriented language akin described section specify schema relational domain. entities relations represented classes comes descriptive attributes reference slots classes refer another. using example consider document citation domain consists classes paper class attributes paper.topic paper.words cites class establishes citation relation papers reference slots cites.cited cites.citing. par-rvs prms correspond attributes objects possibly going chains reference slots. par-factor deﬁned specifying par-rvs corresponding child node parent nodes respectively providing conditional probability distribution given example possible par-factor document citation domain p.topic {p.cited.topic p.citing.topic}. thus par-factor establishes dependency topic paper topics papers cites cite directed models prms aggregation functions cases number parents obtained instantiating par-factor varies. instance since papers cite varying number papers would need aggregate topics papers corresponding instantiation p.cited.topic. example reasoning performed attributes objects whereas relations assumed given. however applications necessary reason presence relation objects. example uncertainty regarding cites.citing. prms provide extensions dealing cases situations number links known speciﬁc objects linked well neither number links linked objects known blog. blog short bayesian logic relational language specifying generative models par-rvs blog represented ﬁrst-order logic atoms. dependence par-rv parents expressed listing parent par-rvs specifying distribution child drawn given parents. example milch present blog model entity resolution. model views citations given paper drawn uniformly random known publications. captured following blog statement pubcited uniform. unique characteristic blog assume entities domain known advance instead allows reasoning variable numbers entities. functionality supported allowing number statements number entities given type drawn given distribution. example entity resolution task number researchers known advance instead drawn user-deﬁned distribution. standard distributions poisson also used. representations discussed survey deﬁne either directed undirected graphical model instantiated. representations relative advantages disadvantages analogous directed undirected graphical models terms representation directed models appropriate needs express causal dependence. hand undirected models better suited domains containing cyclic dependencies. causal structure might easier elicit experts model structure learned data special care needs taken directed models ensure acyclicity. structure revisions directed models evaluated much faster typical scoring functions decomposable parameters change locally places dependency structure changed. words parents node change parameters typically need adjusted conditional distribution given parents. contrasts undirected models scoring revision single par-factor requires adjusting parameters entire model. issues pertaining structure learning discussed section undirected models straightforward mechanism combining par-factors shared single par-rv simply multiplying them whereas directed models require separately deﬁned combining functions noisy-or aggregation functions count mode average. hand combining functions directed models allows multiple independent causes given par-rv learned separately combined prediction time hand kind causal independence cannot exploited undirected models. finally factors directed models represent conditional probability distributions automatically normalized. contrast undirected models needs efﬁcient ways computing estimating normalization constant hybrid representations combine positive aspects directed undirected models. model relational dependency networks viewed lifted dependency network model. dependency networks similar bayesian networks that variable contain factor represents conditional probability distribution given parents immediate neighbors pax. unlike bayesian networks however dependency networks contain cycles necessarily represent coherent joint probability distribution. marginals recovered sampling e.g. gibbs sampling respect dependency networks similar markov networks i.e. parents variable render independent variables network. rdns lift dependency networks relational domains. par-factors rdns similar prms represented conditional probability distributions values child par-rv parents analogous dependency networks however cycles allowed thus dependency networks rdns always represent consistent joint probability distribution. also effort unify directed undirected models providing algorithm converts given directed model equivalent model multiple causes variable independently taking advantage variety inference algorithms implemented mlns. bridging directed undirected models important also step towards representations combine directed undirected sub-components. model used draw types inferences analogous supported graphical models. ﬁrst type goal compute marginal probabilities variables; refer computing marginals. second type inference task likely joint assignment values unknown variables also known state. section structured follows. start overview algorithms directly port inference techniques developed graphical models literature instantiating given model operating resulting factor graph. then survey lifted inference approaches exploit symmetries present model. stronger emphasis placed approaches developed speciﬁcally representations. advantage representations discussed survey that since instantiated factor graphs inference algorithms developed graphical models literature directly ported here. earliest techniques used efﬁciently instantiate given model given domain knowledgebased model construction dynamically instantiates knowledge base extent necessary answer particular query interest. kbmc adapted instantiate directed e.g. undirected models e.g. application kbmc frameworks exploit conditional independency properties implied graph structure instantiated model; particular fact answering query random variables needs reason variables rendered conditionally independent given values observed variables. variable elimination earliest simplest algorithms used perform exact inference grounded factor graph variable elimination suppose would like compute marginal probability particular instantiation particular par-rv that need variables call proceeds iterations summing variables one. ordering variables established iteration next selected factors split groups—the ones contain ones not. factors containing multiplied together summed thus effectively removing efﬁciency algorithm affected ordering used; heuristics selecting better orderings available. result normalized. algorithm adapted state maximizing variables rather summing them. particular suppose would like likely joint assignment variables before impose ordering proceed iterations time however eliminating variable maximizing product factors contain remembering value gave maximum value. belief propagation another algorithm used compute marginals factor graph belief propagation also known sum-product algorithm consists series summations products. computes marginals exactly graphs contain cycles. special case frequently arises practice chain graphs known forward-backward algorithm bp’s operation based series messages sent variable nodes factor nodes vise versa. complete derivation messages refer reader provide result using notation paper. messages sent variable node factor node denoted µx→f messages sent factor node variable node denoted µf→x messages sent deﬁned shown below. following expressions denotes neighbors node i.e. factors participates; analogously variables used calculate factor; denotes variables except words sending message neighbor variable waits receive messages neighbors simply sends product messages. variable leaf node thus single neighbor sends trivial message similarly factor sends message variable waits receive messages variables multiplies messages together sums variables except message sent. factor leaf node simply sends itself. marginal variable calculated product incoming messages neighboring factors. also used loopy graphs sequence iterations. although cases guaranteed output correct results practice frequently converges happens values obtained typically correct variable elimination easily adapted compute state replacing summation operator equation maximization operator. called max-product algorithm underlying graph chain viterbi algorithm. sampling exact inference intractable general. alternative approach perform approximate inference based sampling. popular techniques gibbs sampling markov chain monte carlo sampling algorithm. onset gibbs sampling initializes unknown variables. done randomly faster convergence obtained carefully picked values i.e. ones result state. sampling proceeds iterations iteration value unknown variables sampled given current assignments remaining variables. general conditions gibbs sampling converges target distribution conditions often broken practice ergodicity requirement state aperiodically visited state. ergodicity violated domain contains deterministic near-deterministic dependencies case sampling becomes stuck region converges incorrect result. avoiding problem jointly sample values blocks groups variables closely coordinated assignments. another overcoming challenge deterministic near-deterministic dependencies perform slice sampling intuitively slice sampling auxiliary variables used identify slices across modes distribution. sampling uniformly slice technique allows sampling jump across modes thus preventing getting stuck single region. slice sampling used derive mc-sat algorithm performs slice sampling mlns. mc-sat identiﬁes slice sample possible variable assignments satisfy appropriately selected grounded clauses probability selecting grounded clause larger clauses larger weights. mc-sat samples uniformly slice using samplesat algorithm orthogonal concern efﬁciency sampling. approach speeding sampling memoization values past samples stored reused instead generating sample. care taken keep reuses independent another accuracy sampling improved allowing sampler draw larger number samples allotted time. weighted satisﬁability inference mlns equivalent ﬁnding joint assignment parrv instantiations weight satisﬁed formula instantiations maximized. words performing inference mlns equivalent solving weighted satisﬁability problem discussed richardson domingos used maxwalksat algorithm linear programming inference performed also solving integer linear program constructed given factor graph. general construction e.g. boolean-valued variable introduced program factor possible assignment values involved computation constraints added enforce conditions factor specializations general procedure generating constraints factor graph exist cases factor graphs obtained instantiating mlns psl. specialized procedure casting inference mlns integer linear program provided resulting linear program contains variable par-rv instantiation whose value unknown well variable instantiation formula formula instantiations simpliﬁed replacing par-rvs whose values known observed values replacing par-rv instantiations corresponding variables correspondence formula instantiation corresponding variable established requiring logical equivalence effectively rewriting rewritten formulas converted conjunctive normal form disjunction transformed linear constraint. example latter step carried consider disjunction corresponding linear constraint inference cast second-order cone program. done follows. mlns program contains variable unknown par-rv instantiation rule variable rule instantiation par-rv instantiations replaced corresponding variables correspondence rule variables corresponding instantiated rules established including constraint represents distance satisfaction rule instantiation deﬁned description page additional constraints also included. par-rv instantiations continuous rather boolean values variables optimization constrained integers case mlns. result appropriate choices t-norms similarity functions second-order cone program solved efﬁciently polynomial time opposed integer linear programming case known np-hard cutting plane inference described above procedures casting inference mlns optimization problem naive sense ﬁrst fully instantiate given formulas rules convert constraints. reality efﬁciency procedures signiﬁcantly improved making default value false case mlns case realizing rule instantiations thus corresponding constraints satisﬁed setting par-rv instantiations default value. words constraints need included optimization problem corresponding rule instantiations fully satisﬁed current assignment values unknown par-rv instantiations. realization leads iterative procedure whereby series optimization problems solved subsequent problem including additional constraints fully satisﬁed assignment values par-rv instantiations. riedel relates procedure cutting plane algorithms developed operations research community. cutting plane optimization rather solving original problem ﬁrst optimizes smaller problem contains subset original constraints. algorithm proceeds iterations time adding active constraints original problem satisﬁed current solution. process continues solution satisﬁes constraints found. worst case require considering constraints. however many cases possible solution considering small subset constraints. lazy inference lazy inference related meta-inference technique based fact relational domains typically sparse i.e. possible relations actually true. lazy inference originally developed improve memory efﬁciency inference maxwalksat mlns resulting lazysat algorithm lazysat maintains sets active active formula instantiations. active formulas explicitly maintained memory thus dramatically decreasing memory requirements inference. initially false active consists participating formula instantiations satisﬁed initial assignment false values. formula instantiation activated made unsatisﬁed ﬂipping value zero active rvs. thus initial active formula instantiations consists activated initially active rvs. algorithm carries iterations maxwalksat activating value gets ﬂipped activating relevant rule instantiations. lazysat later generalized inference techniques sampling faster instantiation efﬁciency inference techniques described section affected quickly par-factor graph grounded factor graph. case mlns shavlik natarajan introduced frog algorithm preprocesses given improve efﬁciency instantiation. basic idea evidence literal grounded clause satisﬁed clause already true regardless values remaining literals excluded consideration; thus grounding clause needs consider variable substitutions lead evidence literals satisﬁed which many cases results signiﬁcantly lower number instantiations. frog employs heuristics identify groups variable substitutions safely ignored. performing inference instantiated factor graph take advantage many available inference techniques extensively studied graphical models literature. however particularly larger problems approach prohibitive terms memory requirements running time. address issue variety lifted inference techniques developed. lifted inference exploits observation factor graphs obtained grounding par-factors exhibit large degree symmetry would lead repeating computations multiple times. organizing computations exploits symmetries avoids repetition lifted inference techniques lead large efﬁciency gains. earliest techniques based recognizing identical structure would result repeated identical computations performing computation ﬁrst time caching results subsequently reusing next describe several lifted inference approaches organizing according underlying inference algorithm use. lifted variable elimination first-order variable elimination introduced later signiﬁcantly extended series works ordinary goal fove obtain marginal distribution variables summing values remaining ones. however unlike fove sums entire sets variables possibly constrained groundings par-rv. thus crux fove deﬁne operations eliminate entire sets groundings par-rvs result would obtained summing individually. next provide brief discussion several elimination operations deﬁned summarize conditions apply. detailed treatment refer reader papers; uniﬁed treatment presented excellent basic introduction examples presented elimination operations assume following conditions model auxiliary operations achieve them. first par-factors model need shattered means par-factors model sets groundings par-rvs constraints either identical completely disjoint. intuitively necessary order ensure reasoning steps apply factors resulting grounding given par-factor. model shattered using splitting operation second par-factor model containing par-rv eliminated. achieved using fusion operation essentially multiplies together par-factors depend par-rv eliminated. facilitate remainder discussion par-rv trying eliminate par-factor depends elimination operators ﬁrst elimination operation inversion elimination introduced inversion elimination applies one-to-one correspondence established groundings condition violated logical variables appear different logical variables example suppose depends logical variable depends par-rvs inversion elimination would work case depend logical variable nutshell inversion elimination takes products groundings products possible substitutions variables simpliﬁes product sums depends number possible truth assignments general much smaller total number groundings. another elimination operation counting elimination based insight frequently factors resulting grounding form large groups identical members. groups easily identiﬁed considering possible truth assignments groundings arguments. possible truth assignment grounded arguments counting elimination counts number groundings would truth assignment. grounding group needs evaluated result exponentiated total number groundings group. able count number groundings resulting particular truth assignment efﬁciently counting elimination requires choice substitutions grounding par-rvs constrain choice substitutions ones. finally although described counting elimination context eliminating groundings par-rv fact used eliminate par-rvs. finally introduced elimination counting formulas exploits exchangeability between parameterized random variables given par-factor depends. particular property observed par-factor function number arguments particular value rather precise identity arguments. extended algorithm called c-fove. discussed section directed models require aggregation values. happen example par-factor parent par-rvs contain variables appear child par-rv. order aggregate variables lifted fashion introduced aggregation par-factors deﬁned procedure aggregation par-factor converted product par-factors involves counting formula. able handle aggregation using c-fove. alternative setting instantiated factor graph given goal recognize symmetries present order avoid repeated computation. thus ﬁrst setting potential symmetries implicit speciﬁcation ungrounded model task presence evidence breaks symmetries whereas second scenario grounded model provided task recover symmetries. latter case naturally occurs querying probabilistic database e.g. studied method identiﬁes shared factors compute function based observation inputs shared factors values outputs also same. shared factors discovered constructing rv-elim graph simulates operation without actually computing factor values. rv-elim graph provides convenient identifying shared factors whose computations carried cached needed later. lifted algorithms described carry exact computations. speed inference improved performing approximate inference. extended algorithm setting relaxing conditions considering factors shared. declaring factors shared last computations simulated rv-elim graph same. thus approach based intuition effect distant inﬂuences relatively small. another approximation scheme used place factors together bins values compute closer threshold. lifted belief propagation lifted algorithms proceed stages. ﬁrst stage grounded factor graph compressed template graph super-nodes represent groups variable factor nodes send receive messages supernodes connected super-edge respective members connected edge weight super-edge equals number ordinary edges represents. template graph constructed trivial modiﬁcations. message sent variable super-node factor super-node given point invite reader compare equations counterparts standard case equations note lifted case almost identical standard except superedge weight exponents. next describe template factor graph constructed. ﬁrst algorithm given jaimovich algorithm targets scenario evidence provided based insight case factor nodes variable nodes grouped types factor/variable nodes type groundings par-factor/parameterized variable. lack evidence ensures grounded factor graph completely symmetrical nodes type identical local neighborhoods i.e. numbers neighbors type. result using induction iterations loopy seen nodes type send receive identical messages. pointed authors main limitation algorithm requires evidence provided mostly useful learning data likelihood absence evidence computed. singla domingos built upon algorithm jaimovich introduced lifted general case evidence provided. absence evidence algorithm reduces jaimovich case construction template graph complex proceeds stages simulate determine propagation evidence affects types messages sent. initially three variable super-nodes containing true false unknown variables respectively. subsequent iterations super-nodes continually reﬁned follows. first factor super-nodes separated types factor nodes type functions variable super-nodes. variable super-nodes reﬁned variable nodes types participate numbers factor super-nodes type. process guaranteed converge point minimal template factor graph obtained. kersting provide generalized simpliﬁed description algorithm casting terms general factor graphs rather factor graphs deﬁned probabilistic logical languages done singla domingos. finally lifted extended any-time case combines approach clustering alternative approach cluster instantiated factor graph similarity neighborhoods compute marginal probability representative cluster assigning result members cluster. approach taken algorithm neighborhood restricted setting values given distance values thus cutting inﬂuence neighbors away. whose neighborhoods given depth identical clustered. analogous learning graphical models learning par-factor graphs decomposed structure learning parameter learning. structure learning entails discovering dependency structure model i.e. par-rvs participate together par-factors. hand parameter learning involves ﬁnding appropriate parameterization par-factors. parameter learning typically performed multiple times structure learning order score candidate structures. applications designer hard-codes structure par-factors background knowledge training consists parameter learning. algorithms parameter learning graphical models extended straightforward parameter learning lifted graphical models. extension based realization instantiated par-factor graph simply factor graph subsets factors namely ones instantiations par-factor identical parameters. standard terminology par-factors share parameters parameters tied. thus basic form parameter learning par-factor graphs reduced parameter learning factor graphs forcing factors instantiations par-factors parameters tied. complete treatment parameter learning graphical models beyond scope survey next provide brief overview basic approaches discuss easily extended allow learning tied parameters. detailed discussion refer reader consider case fully observed training data. maximum likelihood parameter estimation aims ﬁnding values parameters probability observing training data maximized i.e. interested ﬁnding values maxθ factor graph parameterized helpful consider directed undirected models separately. directed case e.g. bayesian networks parameter learning consists learning conditional probability distribution function node given parents. thus simplest scenario consists conditional probability tables node. maximum likelihood estimate entry node taking value given it’s parents values found simply calculating proportion time conﬁguration values observed undirected models situation slightly complicated parameters cannot calculated closed form needs gradient descent optimization procedure. supposing that introduced section representation log-linear model parameter factor gradient respect parameter potential function given next describe equations extended work tied parameters. nutshell done computing statistics factors share parameters. directed models factors tied parameters share cpds. thus case equation counts computed single node nodes share cpd. nodes parents node equation becomes issue arises learning parameters model described computing sufﬁcient statistics e.g. counts equation sums equation models based database representation take advantage database operations compute sufﬁcient statistics efﬁciently. example prms computation sufﬁcient statistics cast construction appropriate view data running simple database queries caching used achieve speed-ups. discussion focused particular learning criterion maximum likelihood estimation practice criteria exist. example rather optimizing data likelihood dramatically improve efﬁciency instead optimizing pseudo-likelihood pseudolikelihood computed multiplying probability variable conditioned values markov blanket observed data. another example reduce overﬁtting bayesian learning impose prior learned parameters parameter learning prms respect maximum likelihood criterion bayesian criterion discussed lowd domingos present comparison several parameter learning methods mlns max-margin parameter learning approach presented later extended train parameters online fashion goal structure learning skeleton dependencies regularities make par-factors. structure learning builds heavily corresponding work graphical models inductive logic programming basic structure learning procedure viewed greedy heuristic search space possible structures. generic structure learning procedure shown algorithm procedure parameterized hypothesis space could potentially encode language bias; search algorithm e.g. beam search; reﬁnement operator speciﬁes structures derived given one; scoring function assigns score given structure. algorithm starts generating initial candidates. typically consists trivial par-factors e.g. ones consisting single par-rvs. proceeds iterations iteration reﬁning existing candidates scoring them pruning current candidates ones appear promising. details steps carried depend particular choices reﬁnement operator language-speciﬁc typically allows several kinds simple incremental changes addition removal parents par-rv. common choices log-likelihood related measures directly ported graphical models literature. algorithm directly analogous approaches learning graphical models well approaches developed foil algorithm variants algorithm adapted particular representation used several authors. example directed case instantiation algorithm learning prms described case generatecandidaterefinements method checks acyclicity resulting structure employs classic revision operators adding deleting reversing edge. addition greedy hillclimbing algorithm always prefers high-scoring structures lower-scoring ones getoor presents randomized technique simulated annealing ﬂavor beginning learning structure search procedure takes random steps probability greedy steps probability learning progresses decreased gradually steering learning away random choices. case undirected models domingos introduced version algorithm learning structure. algorithm proceeds iterations time searching best clause model. searching performed using possible strategies–beam search shortest-ﬁrst search. beam search used best clause candidates kept step search. hand shortest-ﬁrst search algorithm tries best clauses length moves length candidate clauses algorithm scored using weighted pseudo loglikelihood measure adaptation pseudo log-likelihood weighs pseudo likelihood grounded atom number groundings predicate prevent predicates larger arity dominating expression. another technique developed graphical models community extended par-factor graphs structure selection appropriate regularization. approach large number factors markov network evaluated training parameters using norm regularizer since norm imposes strong penalty smaller parameters effect forces parameters pruned model. huynh mooney extended technique structure learning mlns ﬁrst using aleph off-the-shelf learner generate large potential par-factors performed l-regularized parameter learning set. difﬁculties structure learning greedy search performed algorithm space possible structures large contains many local maxima plateaus. thus much work focused developing approaches address challenge. next discuss groups approaches. ﬁrst group exploits sophisticated search techniques whereas second based constraining search space performing carefully designed pre-processing step. using sophisticated search addressing potential shortcomings greedy structure selection using sophisticated search algorithm example biba used iterated local search avoiding local maxima performing discriminative structure learning mlns. iterative local search techniques alternate types search steps either moving towards locally optimal solution perturbing current solution order escape local optima. alternative approach search structures increasing complexity stage using structures found previous stage constrain search space. strategy employed khosravi learning structure domains contain many descriptive attributes. approach similar technique employed constrain search space prms described below distinguishes types tables attribute tables describe single entity type relationship tables describe relationships entities. algorithm called proceeds three stages. ﬁrst stage dependencies local attribute tables learned. second stage dependencies join attribute table relationship table learned search space constrained requiring dependencies local attribute table found ﬁrst stage remain same. finally third stage dependencies join relationship tables joined relevant attribute tables learned search space similarly constrained. orthogonal characteristic that although goal learn undirected model dependencies learned using bayesian network learner. directed structures converted undirected ones moralizing graphs. advantage approach structure learning directed models signiﬁcantly faster structure learning undirected models decomposability score allows updated locally parts structure modiﬁed thus scoring candidate structures efﬁcient. constraining search space second group solutions based constraining search space structures typically performing pre-processing step that roughly speaking ﬁnds promising regions space. approach used learning constrain potential parents par-rv algorithm proceeds stages stage forming potential parents par-rvs reached chain relations length structure learning stage constrained search potential parent sets. thus algorithm similar techniques described above. however algorithm constrains potential parent candidates requiring value beyond already captured currently learned parents. speciﬁcally potential parents par-rv stage consists parents learned structure stage par-rvs reachable relation chains length lead higher value specially designed score measure. algorithm directly ports scoring functions developed analogous learning technique bayesian networks series algorithms group developed learning structure. ﬁrst series busl busl based observation that instantiated markov network instantiations clause deﬁne identically structured cliques markov network. busl inverts process instantiation constrains search space ﬁrst inducing lifted templates cliques learning markov network template undirected graph dependencies whose nodes ordinary variables par-rvs. clause search constrained cliques markov network template. markov network templates learned constructing perspective predicate table possible instantiation predicate column possible par-rvs value cell data contains true instantiation j’th par-rv variable substitutions consistent i’th predicate instantiation. markov network template learned table markov network learner. learner based constraining search space algorithm limits clause candidates considered using relational pathﬁnding focus promising ones. developed community relational pathﬁnding searches clauses tracing paths across true instantiations relations data. figure gives example clause credits credits workedfor learned tracing thick-lined path brando coppola variablizing appropriately. however real-world relational domains search space relational paths large crucial aspect perform relational pathﬁnding original relational graph data lifted hypergraph formed clustering entities domain agglomerative clustering procedure implemented mln. intuitively entities clustered together tend participate kinds relations entities clusters. structure search limited clauses derived relational paths lifted hypergraph. domingos proposed constraining search space identifying structural motifs capture commonly occurring patterns among densely connected entities domain. resulting algorithm called proceeds ﬁrst identifying motifs searching clauses performing relational pathﬁnding within them. discover motifs starts entity relational graph performs series random walks. entities reachable within thresholded hitting time hyperedges among included motif paths reachable recorded. next entities included motif clustered hitting times groups potential symmetrical nodes. nodes within group clustered agglomerative manner similarity distributions paths reachable process results lifted hypergraph analogous produced lhl; however whereas nodes clustered based close neighborhood relational graph clustered based longer-range connections nodes. motifs extracted lifted hypergraphs depth-ﬁrst search. discussion focused learning structure scratch. approaches based greedy search algorithm easily adapted perform revision starting learning given structure work area also focused approaches speciﬁcally designed structure revision transfer learning. example paes introduced approach revision blps based work theory revision community goal given initial theory minimally modify becomes consistent examples. revision algorithm follows methodology forte theory revision system ﬁrst generating revision points places given rules fails next focusing search revisions ones could address discovered revision points. forte methodology also followed tamar transfer learning system generates revision points clauses performing inference observing ways given clauses fail. tamar designed transfer learning e.g. goal ﬁrst translate given structure representation source domain target revise thus addition revision module also contains mapping module discovers best mapping source predicates target ones. problem mapping source structure target domain also considered constrained setting data target domain extremely scarce rather taking structure learned speciﬁcally source domain trying adapt target domain interest alternative approach transfer learning extract general knowledge source domain applied variety target domains. approach taken uses source data learn general clique templates expressed second-order markov logic clauses i.e. quantiﬁcation predicates variables. step care taken ensure learned clique templates capture general regularities likely speciﬁc source domain. then target domain allows several possible mechanisms using clique templates provide declarative bias. important structure learning problem inducing causal models relational data. problem recently addressed maier whose algorithm works directed models allow causal effects encoded directionality links. extends propositional analog algorithm developed learning causality graphical models. proceeds stages. ﬁrst stage skeleton identiﬁcation performed uncover structure conditional independencies data. second stage edge orientation takes place orientations consistent skeleton considered. ports edge orientation rules relational setting also develops edge orientation rules speciﬁc relational domains particular model uncertainty whether link exists entities. article presented survey work lifted graphical models. reviewed general form lifted graphical model par-factor graph shown number existing statistical relational representations formalism. discussed inference algorithms including lifted inference algorithms efﬁciently compute answers probabilistic queries. also reviewed work learning lifted graphical models data. belief need statistical relational models grow coming decades inundated data structured unstructured entities relations extracted noisy manner text need reason effectively data. hope synthesis ideas many different research groups provide accessible starting point researchers expanding ﬁeld. would like thank galileo namata theodoros rekatsinas comments earlier versions paper. mihalkova supported fellowship grant computing research association. getoor supported grants ccf. opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect views cra.", "year": 2011}