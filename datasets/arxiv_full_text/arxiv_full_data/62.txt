{"title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured  Multi-Turn Dialogue Systems", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response.", "text": "paper introduces ubuntu dialogue corpus dataset containing almost million multi-turn dialogues total million utterances million words. provides unique resource research building dialogue managers based neural language models make large amounts unlabeled data. dataset multi-turn property conversations dialog state tracking challenge datasets unstructured nature interactions microblog services twitter. also describe neural learning architectures suitable analyzing dataset provide benchmark performance task selecting best next response. ability computer converse natural coherent manner human long held primary objectives artiﬁcial intelligence paper consider problem building dialogue agents ability interact one-on-one multi-turn conversations diverse topics. primarily target unstructured dialogues priori logical representation information exchanged conversation. contrast recent systems focus structured dialogue tasks using slot-ﬁlling representation observe several subﬁelds computer vision speech recognition machine translation—fundamental break-throughs achieved recent years using machine learning methods speciﬁcally neural architectures however worth noting many successful approaches particular convolutional recurrent neural networks known many years prior. therefore reasonable attribute progress three major factors public distribution large rich datasets availability substantial computing power development training methods neural architectures particular leveraging unlabeled data. similar progress observed development dialogue systems. hypothesize lack sufﬁciently large datasets overcome barrier providing large corpus research multi-turn conversation. ubuntu dialogue corpus consists almost million two-person conversations extracted ubuntu chat logs used receive technical support various ubuntu-related problems. conversations average turns each minimum turns. conversations carried text form dataset orders magnitude larger structured corpuses dialogue state tracking challenge scale recent datasets solving problems question answering analysis microblog services twitter conversation dataset includes several turns well longer utterances. furthermore targets speciﬁc domain namely technical support used case study development agents targeted applications contrast chatbox agents often lack welldeﬁned goal ment frequency approach sophisticated neural models including recurrent neural network long short-term memory architecture. provide benchmark performance algorithms trained corpus task selecting best next response achieved withrequiring human labeling. dataset ready public release. code developed empirical results also available. brieﬂy review existing dialogue datasets recent learning architectures used structured unstructured dialogues. means exhaustive list surveys resources related contribution. list datasets discussed provided table dialogue datasets switchboard dataset dialogue state tracking challenge datasets used train validate dialogue management systems interactive information retrieval. problem typically formalized slot ﬁlling task agents attempt predict goal user conversation. datasets signiﬁcant resources structured dialogues allowed major progress ﬁeld though quite small compared datasets currently used training neural architectures. recently datasets used containing unstructured dialogues extracted twitter. ritter collected million conversations; extended take advantage longer contexts using a-b-a triples. shang used data similar chinese website called weibo. however knowledge datasets made public furthermore post-reply format microblogging services perhaps representative natural dialogue humans continuous stream messages chat room. fact ritter estimate posts twitter ‘conversational nature’ collected data contained exchanges length hypothesize chat-room style messaging closely correlated human-tohuman dialogue micro-blogging websites forum-based sites reddit. part ubuntu chat logs previously aggregated dataset called ubuntu chat corpus however resource preserves multi-participant structure thus less amenable investigation traditional two-party conversations. also weakly related contribution problem question-answer systems. several datasets question-answer pairs available however interactions much shorter seek study. dialogue research historically focused structured slot-ﬁlling tasks various approaches proposed attempts leverage recent developments neural learning architectures. notable exception work henderson proposes structure initialized denoising autoencoder tackle dstc domain. work unstructured dialogues recently pioneered ritter proposed response generation model twitter data based ideas statistical machine translation. shown give superior performance previous information retrieval approaches idea developed sordoni exploit information longer context using structure similar recurrent neural network encoder-decoder model achieves rather poor performance a-b-a twitter triples measured bleu score performs comparatively better model ritter results also veriﬁed human-subject study. similar encoderdecoder framework presented model uses transform input vector representation another ‘decode’ representation response generating word time. model also evaluated human-subject study although much smaller size overall models description telephone conversations pre-speciﬁed topics ride information system restaurant booking system tourist information system hours tourist info exchange skype post/ replies extracted twitter a-b-a triples twitter replies post/ reply pairs extracted weibo extracted ubuntu chat logs table selection structured unstructured large-scale datasets applicable dialogue systems. faded datasets publicly available. last entry contribution. ubuntu chat logs refer collection logs ubuntu-related chat rooms freenode internet relay chat network. protocol allows real-time chat large number participants. chat room channel particular topic every channel participant messages posted given channel. many channels used obtaining technical support various ubuntu issues. contents channel moderated interactions follow similar pattern. user joins channel asks general question problem ubuntu. then another experienced user replies potential solution ﬁrst addressing ’username’ ﬁrst user. called name mention done avoid confusion channel given time simultaneous conversations happening channels. popular channels almost never time conversation occurring; renders particularly problematic extract dyadic dialogues. conversation pair users generally stops problem solved though users occasionally continue discuss topic related ubuntu. despite nature chat room constant stream messages multiple users fairly rigid structure messages extract dialogues users. figure shows example chat room conversation ubuntu channel well extracted dialogues illustrates users usually state username intended message recipient writing reply example clear users ‘taru’ ‘kuja’ engaged dialogue users ‘old’ ‘burer’ user ‘_pm’ asking initial question ‘livecd’ perhaps elaborating previous comment. dataset creation order create ubuntu dialogue corpus ﬁrst method devised extract dyadic dialogues chat room multi-party conversations. ﬁrst step separate every message -tuples given -tuples straightforward recipient identiﬁcation cases recipient ﬁrst word utterance sometimes located case initial questions. furthermore users choose names corresponding common english words ‘the’ ‘stop’ could lead many false positives. order solve issue create dictionary usernames current previous days compare ﬁrst word utterance entries. match found word correspond common english word assumed user intended recipient message. matches found assumed message initial question recipient value left empty. utterance creation dialogue extraction algorithm works backwards ﬁrst response initial question replied within time frame minutes. ﬁrst response identiﬁed presence recipient name initial question identiﬁed recent utterance recipient identiﬁed ﬁrst response. utterances qualify ﬁrst response initial question discarded; initial questions generate response also discarded. additionally discard conversations longer utterances user says utterances typically representative real chat dialogues. finally consider extracted dialogues consist turns encourage modeling longer-term dependencies. alleviate problem ‘holes’ dialogue user address explicitly figure check whether user talks someone else duration conversation. non-addressed utterances added dialogue. example conversation along extracted dialogues shown figure note also concatenate consecutive utterances given user. special cases limitations often case user post initial question multiple people respond different answers. instance conversation ﬁrst user user replied treated separate dialogue. unfortunate side-effect initial question appear multiple times several dialogues. however number cases sufﬁciently small compared size dataset. another issue note utterance posting time considered segmenting conversations users. even users conversation spans multiple hours even days treated single dialogue. however dialogues rare. include posting time corpus researchers ﬁlter desired. ubuntu chat logs size. crucial research building dialogue managers based neural architectures. another important characteristic number turns dialogues. distribution number turns shown figure seen number dialogues turns dialogue follow approximate power relationship. test generation aside ubuntu dialogue corpus conversations form test used evaluation response selection algorithms. compared rest corpus test processed extract pair triples dialogue. boolean variable indicating whether response actual next utterance given context. response target utterance correctly identify. context consists sequence utterances appearing dialogue prior response. create pair triples triple contains correct response triple contains false response sampled randomly elsewhere within test set. ﬁrst case second case. example pair shown table make task harder move pairs responses larger wrong responses experiments below consider case wrong response wrong responses. since want learn predict parts conversation opposed closing statement consider various portions context conversations test set. context size determined stochastically using simple formula here denotes maximum desired context size last term desired minimum context size parameter actual length dialogue random number corresponding randomly sampled context length selected inversely proportional evaluation metric consider task best response selection. achieved processing data described section without requiring human labels. classiﬁcation task adaptation recall precision metrics previously applied dialogue datasets family metrics often used language tasks recallk agent asked select likely responses correct true response among candidates. metric relevant case binary classiﬁcation although language model performs well response classiﬁcation gauge good performance next utterance generation hypothesize improvements model regards classiﬁcation task eventually lead improvements generation task. section discussion point. provide evidence value dataset research neural architectures dialogue managers provide performance benchmarks neural learning algorithms well naive baseline. approaches considered tf-idf recurrent neural networks long short-term memory prior applying method perform standard pre-processing data using nltk library twitter tokenizer parse utterance. generic tags various word cattrain lstm architectures process full training ubuntu dialogue corpus format test described section extracting triples dialogues. training sample context length instead consider utterance potential response previous utterances context. dialogue length yields training examples. since overlapping clearly independent consider minor issue given size dataset negative responses selected random rest training data. tf-idf term frequency-inverse document frequency statistic intends capture important given word document case context technique often used document classiﬁcation information retrieval. ‘term-frequency’ term simply count number times word appears given context ‘inverse document frequency’ term puts penalty often word appears elsewhere corpus. ﬁnal score calculated product terms form indicates number times word appeared context total number dialogues denominator represents number dialogues word appears. classiﬁcation tf-idf vectors ﬁrst calculated context candidate responses. given candidate response vectors highest cosine similarity context vector selected output. recallk responses returned. recurrent neural networks variant neural networks allows time-delayed directed cycles units leads formation internal state network allows model time-dependent data. internal state updated time step diagram seen figure rnns primary building block many current neural language models rnns encoder decoder. ﬁrst used encode given context second generates response using beam-search initial hidden state biased using ﬁnal hidden state ﬁrst rnn. work concerned classiﬁcation responses instead generation. build upon approach also recently applied problem question answering utilize siamese network consisting rnns tied weights produce embeddings context response. given input context response compute embeddings respectively feeding word embeddings time respective rnn. word embeddings initialized using pre-trained vectors ﬁne-tuned training. hidden state updated step ﬁnal hidden state represents summary input utterance. using ﬁnal hidden states rnns calculate probability valid pair observe lstm outperforms tf-idf evaluation metrics. interesting note tf-idf actually outperforms recall case classiﬁcation. likely limited ability take account long contexts overcome using lstm. example output lstm response correctly classiﬁed shown table also show figure increase performance lstm amount data used training increases. conﬁrms importance large training set. paper presents ubuntu dialogue corpus large dataset research unstructured multiturn dialogue systems. describe construction dataset properties. availability dataset size opens several interesting possibilities research dialogue systems based rich neural-network architectures. present preliminary results demonstrating dataset train lstm task selecting next best response bias matrix rd×d learned model parameters. thought generative approach; given input response generate context product measure similarity actual context using product. converted probability sigmoid function. model trained minimizing cross entropy labeled pairs training used ratio true responses negative responses drawn randomly elsewhere training set. architecture hidden layer neurons. matrix initialized using orthogonal weights initialized using uniform distribution values between adam optimizer gradients clipped found weight initialization well choice optimizer critical training rnns. lstm addition model consider architecture changed hidden units long-short term memory units lstms introduced order model longerterm dependencies. accomplished using series gates determine whether input remembered forgotten used output. error signal back indeﬁnitely gates lstm unit. helps overcome vanishing exploding gradient problems standard rnns error gradients would otherwise decrease increase exponential rate. training used hidden layer neurons. hyper-parameter conﬁguration optimized independently rnns lstms using validation extracted training data. state tracking utterance generation work described focuses task response selection. seen intermediate step slot ﬁlling utterance generation. slot ﬁlling candidate outputs identiﬁed priori knowledge engineering typically smaller responses considered work. candidate responses close size dataset quite close response generation case. several reasons proceed directly response generation. first likely current algorithms able generate good results task preferable tackle metrics make progress. second suitable metric evaluating performance response generation case. option bleu meteor scores machine translation. however using bleu evaluate dialogue systems shown give extremely scores large space potential sensible responses further since bleu score calculated using n-grams would provide score reasonable responses words common ground-truth next utterance. alternatively could measure difference generated utterance actual sentence comparing representations embedding space. however different models inevitably different embeddings necessitating standardized embedding evaluation purposes. standardized embeddings created. another possibility human subjects score automatically generated responses time expense make highly impractical option. summary possible current language models outgrown slot ﬁlling metric currently unable measure ability next utterance generation standardized meaningful inexpensive way. motivates choice response selection useful metric time being. approach conversation disentanglement consists small rules. sophisticated techniques proposed training maximum-entropy classiﬁer cluster utterances separate dialogues however since trying replicate exact conversation users retrieve plausible natural dialogues heuristic method presented paper sufﬁcient. seems supported qualitative examination data could subject formal evaluation. interesting properties response selection task ability alter task difﬁculty controlled manner. demonstrated moving false responses varying recallk parameter. future instead choosing false responses randomly consider selecting false responses similar actual response dialogue model performs well difﬁcult task also manage capture ﬁne-grained semantic meaning sentences compared model naively picks replies words common context tf-idf. sciences engineering research council canada would like thank laurent charlin input paper well gabriel forgues eric crawford interesting discussions.", "year": 2015}