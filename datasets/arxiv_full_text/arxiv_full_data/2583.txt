{"title": "An Efficient Large-scale Semi-supervised Multi-label Classifier Capable  of Handling Missing labels", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Multi-label classification has received considerable interest in recent years. Multi-label classifiers have to address many problems including: handling large-scale datasets with many instances and a large set of labels, compensating missing label assignments in the training set, considering correlations between labels, as well as exploiting unlabeled data to improve prediction performance. To tackle datasets with a large set of labels, embedding-based methods have been proposed which seek to represent the label assignments in a low-dimensional space. Many state-of-the-art embedding-based methods use a linear dimensionality reduction to represent the label assignments in a low-dimensional space. However, by doing so, these methods actually neglect the tail labels - labels that are infrequently assigned to instances. We propose an embedding-based method that non-linearly embeds the label vectors using an stochastic approach, thereby predicting the tail labels more accurately. Moreover, the proposed method have excellent mechanisms for handling missing labels, dealing with large-scale datasets, as well as exploiting unlabeled data. With the best of our knowledge, our proposed method is the first multi-label classifier that simultaneously addresses all of the mentioned challenges. Experiments on real-world datasets show that our method outperforms stateof-the-art multi-label classifiers by a large margin, in terms of prediction performance, as well as training time.", "text": "abstract—multi-label classiﬁcation received considerable interest recent years. multi-label classiﬁers address many problems including handling large-scale datasets many instances large labels compensating missing label assignments training considering correlations labels well exploiting unlabeled data improve prediction performance. tackle datasets large labels embedding-based methods proposed seek represent label assignments low-dimensional space. many state-of-the-art embedding-based methods linear dimensionality reduction represent label assignments low-dimensional space. however methods actually neglect tail labels labels infrequently assigned instances. propose embedding-based method non-linearly embeds label vectors using stochastic approach thereby predicting tail labels accurately. moreover proposed method excellent mechanisms handling missing labels dealing large-scale datasets well exploiting unlabeled data. best knowledge proposed method ﬁrst multi-label classiﬁer simultaneously addresses mentioned challenges. experiments real-world datasets show method outperforms stateof-the-art multi-label classiﬁers large margin terms prediction performance well training time. unlike traditional single-label classiﬁcation instance label multi-label classiﬁcation tasks instance associated labels. multi-label classiﬁers applied instance multi-label text classiﬁcation automated image annotation protein function prediction recognition facial action units facial images simple approach build independent binary classiﬁcation model label. approach referred binary relevance although straightforward approach consider correlations labels highly correlated. shown performance metrics bayes optimal multi-label classiﬁer consider correlations labels. however ﬁnite training samples many state-of-the-art methods take advantage correlations labels outperform real-word multi-label datasets usually large labels thus building independent binary classiﬁcation model label infeasible. therefore capable handling large labels. tackle problem embedding-based methods proposed. labels represent labels associated instance binary vector length label vector dimension vector binary variable shows whether speciﬁc label associated instance not. embedding-based methods seek represent label vector instance low-dimensional space latent space feature vectors along independent regression models predict representation instance latent space. many recent methods like faie leml lcml plst cplst consider linear relationship label vectors representations lower-dimensional latent space. note approach equivalent considering low-rank linear mapping transform input feature vectors label vectors. however methods actually neglect tail labels infrequently assigned instances. many real-world multi-label datasets thousands tail labels. neglecting tail labels dramatically affect prediction performance. tackle problem proposed method considers non-linear relationship label vectors representations latent space stochastic transformation. many real-world multi-label datasets contain millions instances millions features well label sets containing millions labels. dealing largescale datasets challenging. fact dealing large-scale datasets beyond pale many state-of-the-art methods like slrm faie fasttag paper propose probabilistic model handle large-scale datasets. proposed method model mappings mentioned transforms feature space latent space transforms latent space label space stochastic transformations drawn sparse gaussian processes gaussian processes parameterized pseudo-instances. computational cost memory requirements gaussian processes makes prohibitive large instances. however shown parameterizing gaussian process pseudo-instances possible achieve full performance much lower computational costs real-world multi-label datasets valid label assignments thoroughly provided training set. fact multi-label datasets labels often obtained crowd sourcing crawling pages. thus associated labels instance incomplete. indeed although image text document associated many labels labelers provide subset them. problem referred missing labels weak labels partially labeled data. address problem missing labels used eeoe framework previous work application eeoe model conﬁrms eeoe considered general framework handle missing labels. paper propose embedding-based multi-label classiﬁer models transformation maps feature space latent space well transformation maps latent space label space sets stochastic transformations. regard method ﬂexible state-of-the-art linear approaches terms prediction performance outperforms large margin. furthermore modeling mappings using stochastic transformations proposed method addresses problem neglecting tail labels addressed many state-of-the-art embedding-based multi-label classiﬁers. important contributions paper exploit idea parameterizing sparse gaussian processes pseudoinstances leads dramatically decreasing training time well handling large-scale datasets. moreover method effective mechanisms compensate missing labels exploit unlabeled instances. labeled instances training denote representation n-th labeled instance feature space latent space respectively moreover denote unobserved complete label vector observed incomplete label vector n-th labeled instance respectively note vector real valued k-th dimension shows suitability k-th label n-th instance. role random variables elaborated next sections. similarly unlabeled instances training denote representation i-th unlabeled instance feature space latent space respectively moreover denote unobserved complete label vector i-th unlabeled instance given arbitrary shows containing elements except tab. summarizes notations symbols used paper. rest paper organized follows section review related works. section explains proposed method. section compare performance proposed method state-of-the-art multi-label classiﬁers. last section conclude paper. deal large labels types methods proposed label selection methods label transformation methods. label selection methods assume label matrix yn×k reconstructed small subset columns. selecting columns matrix yn×k getting matrix ˜yn×l prediction tasks required feasible acceptable computation matrix ˜yn×l costs. indeed feature matrix xn×f label matrix yn×k matrix ˜yn×l) resonable time. hand label transformation methods seek represent label vectors l-dimensional space i.e. latent space recall vector denotes latent space representation label vector multi-label classiﬁers like leml consider low-rank linear mapping transform feature vector label vector equivalent representing label vectors lower dimensional space. many multi-label classiﬁers like faie leml cplst plst consider linear relationship vectors however tail labels omitted training process. indeed using linear dimensionality reduction label vectors latent space actually fades tail labels. regard many methods proposed avoid problem fading tail labels. instance reml considers full-rank linear mapping predict tail labels feature vectors considers low-rank linear mapping predict labels feature vectors sleec attempts preserve distance label vector nearest neighbors thereby modeling label vectors low-dimensional manifold. avoid problem fading tail labels proposed method models decoder transformation maps latent space representations label vectors stochastic transformations drawn sparse gaussian processes multi-label classiﬁcation valid label assignments thoroughly provided training set. problem referred presence missing labels drastic large-scale datasets. however many state-of-the-art methods like reml sleec faie explicit mechanisms handle missing labels. common approaches tackle problem follows methods like leml well seek generate ones label matrix rather attempting generate ones zeros lcml assumes elements label matrix three types zero unknown. afterwards considers unknown elements latent variables. probabilistic model whose generative process follows given representation instances feature space ﬁrstly generates unobserved complete label vectors instances. then removes labels complete label vectors generate observed incomplete label vectors. although approach related ours explain later approach preferable mpu. matrix completion number labeled instances training set. number unlabeled instances training set. number features. cardinality label set. dimensionality latent space. usually following inequality holds feature vector n-th labeled instance training set. feature vector i-th unlabeled instance training set. label vector n-th instance. k-th label assigned n-th instance embedded representation n-th label vector latent space representation unobserved label vector i-th unlabeled instance. k-th dimension vector shows suitability k-th label n-th labeled instance. k-th dimension vector shows suitability k-th label i-th unlabeled instance. elements vector denote elements vectors accordingly. number pseudo-instances stochastic functions number pseudo-instances stochastic functions pseudo-samples functions pseudo-samples functions containing values functions pseudo-instances. words containing values functions pseudo-instances. words radial-basis function kernel smoothing parameter methods also applied multi-label classiﬁcation tasks. methods like irmmc ﬁrstly make matrix containing feature vectors label vectors training test data. afterwards exploit matrix completion methods missing entries matrix. missing entries include label vectors test missing labels training well unknown features. thus methods handle missing labels unknown features. fasttag assumes observed incomplete label vector linearly transformed unobserved complete one. fasttag learns linear mapping idea training denoising auto-encoder. precisely removes entries label vectors assumes exist linear mapping retrieve original label vectors corrupted ones. context recommender systems problem referred missing ratesarises similar problem missing labels multi-label classiﬁcation. indeed systems user ratings usually unknown missing since user seen rated many items. many approaches proposed capable handling missing rates poisson matrix factorization multi-label classiﬁcation dealing large-scale datasets received considerable interest recent years. common approaches tackle problem follows multi-label classiﬁers like stochastic optimization thereby dealing small subsets training data rather considering whole once. shown making certain assumptions possible divide large-scale multi-label classiﬁcation task simpler multi-label classiﬁcation subtasks minimizing zero-one loss subtasks equivalent minimizing zero-one loss main multi-label classiﬁcation task however presence missing labels minimizing zero-one loss seems inappropriate. leml uses low-rank linear mapping transform feature vectors label vectors. impose low-rank constraint linear mapping leml assumes matrix transformation factorized product matrices exploits sparsity feature matrix learns matrices alternating minimization schema. reml assumes label vector decomposed contains tail label assignments. afterwards reml uses low-rank linear mapping transform feature vector vector uses full-rank transform feature vector vector tail labels faded. moreover reml uses alternating minimization schema along divide-andconquer approach thereby handling large-scale datasets. sleec seeks latent representations label vectors vectors desired properties similarity label vectors nearest neighbors preserved latent space. feature vectors possible predict vectors handle large-scale datasets sleec clusters training data clusters learns separate embedding cluster performs classiﬁcation within test point’s cluster alone section introduce proposed method called efﬁcient semi-supervised multi-label classiﬁcation fig. illustrated graphical model proposed method. dimension vector determined stochastic function indeed assume however previously stated using traditional gaussian processes prohibitive largescale datasets. regard idea parameterizing generative process generate directly vector stochastic function using traditional gaussian process instead parameterize gaussian process pseudo-instances. ﬁrstly function pseudo samples afterwards generate random variable approach parameterize stochastic functions note addition gaussian noise explained useful statistical inference. indeed explained uncertainty modeled drawing noisy samples stochastic functions. words addition gaussian noise eliminates terms likelihood equation makes statistical inference feasible. accordingly parameters small value. exploit unlabeled data slrm seek learn smooth mapping transform feature vectors complete unobserved label vectors. words feature vectors correspond either labeled unlabeled instance. according fig. proposed model attempts learn stochastic functions desired smoothness property vectors close other vectors well vectors probably close other. handle missing labels used eeoe approach previous work eeoe framework tackles problem missing labels introducing auxiliary random variables referred experts denoted {enkb}n vae-dgp well gp-lvm note introducing auxiliary random variables make additional assumptions adapt lower bound probabilistic model. precisely make following assumptions variational distribution assume variational distribution factorized according bayesian network fig. fig. probability assigning k-th label n-th instance versus suitability label instance. variable rate randomly removes unobserved complete label assignments generate observed label vectors. proposed method assumes high probability absolutely proper labels missed. however assumes presence missing labels even absolutely suitable label assignments little chance provided training set. regard mechanism handling missing labels preferable moreover fig. demonstrates that instance model assigns considerable chance case therefore capable handling missing labels. model ﬁnding posterior distribution intractable. moreover model existence factors like mean-ﬁeld variational inference intractable. regard adapted evidence lower bound also used methods performance esmc method previous methods including slrm leml faie fasttag used implementation fasttag provided authors article. moreover implemented slrm faie parameters fasttag default values code. experiments according article faie parameter faie selected similarly slrm selected suggested since slrm faie fast methods using cross validation parameters time consuming specially wants evaluate methods many different settings. regard instead using cross validation experiments selected values parameters faie slrm sets lead best prediction performance using different thresholds convert continuous predictions binary label vectors length test instances. clearly prediction performance obtained methods setting parameters using cross-validation. kernel parameter method twice mean euclidean distance feature vectors kernel parameter faie also similarly. moreover parameter follows test phase slrm leml faie fasttag proposed esmc method produce realvalued label space representation. transforming realvalued vectors binary label vectors challenging task dramatically affects prediction performance. indeed evaluating methods produced binary label vectors lead unfair evaluations. regard avoid problem producing binary label vectors used three rank-based evaluation metrics area curve coverage precisionk metrics widely used evaluating multi-label classiﬁers. evaluate performance methods dataset randomly partitioned instances three times averaged performance obtained runs. partitioning denominator term right hand side nonetheless using sigmoid function generative process model straightforward maximize evidence lower bound. regard approximate sigmoid function function deﬁned follows suppose vectors latent space denoted close vector furthermore assume consequence posterior distribution proposed model tend assign considerable chance case neglects assignment k-th label n-th instance. indeed although terms might lead assigning considerable chance case terms uz)} might lead case learn smoother mean stochastic function avoid problem k-th dimension pseudo-instances constrained subset data however simplicity pseudo}mc instances subset feature vectors moreover iteration variational inference update pseudo-instances validate performance proposed method conducted experiments seven real-world datasets corelk iaprtc espgame lear website nus-wide mediamill delicious mulan statistics datasets provided tab. compared performance esmc method previous methods evaluated different settings. figs. provide coverage performance measures. note negated coverage value ﬁgures larger reported value evaluation metric indicates better prediction performance. randomly partitioned instances training data test data mentioned above. moreover evaluate ability different methods handle missing labels fraction label assignments randomly removed training set. horizontal axis ﬁgures shows percentages removed labels training set. therefore performance methods removal rate presents multi-label classiﬁcation performance corresponding datasets. ﬁgures illustrate almost experiments proposed esmc method outperforms mentioned methods large margin. moreover ﬁgures demonstrate introducing auxiliary random variables improves prediction performance. previously explained proposed esmc fasttag explicit mechanisms handle missing labels. moreover slrm handles missing labels ﬁlling missing entries label correlations intrinsic structure among data however faie explicit mechanism handle missing labels. figs. conﬁrm notion. ﬁgures demonstrate proposed esmc adding experts make proposed method capable ﬁlling missing entries label matrix. however increase number incorrect predicted label assignments. consequently adding experts dramatically affect precision precision. thus experiments table used proposed method experts. paper proposed embedding-based multilabel classiﬁer models transformation mapping feature space latent space well transformation mapping latent space label space sets stochastic transformations. regard method ﬂexible state-of-the-art linear approaches terms prediction performance outperforms large margin. furthermore modeling mappings using stochastic transformations method addresses problem neglecting tail labels addressed many state-of-the-art embedding-based multi-label classiﬁers. important contributions paper exploit idea parameterizing sparse gaussian processes pseudoinstances dramatically decreases training time proposed method. moreover esmc method uses effective mechanisms compensate missing labels exploit unlabeled instances. kong zhang learning incomplete label large-scale multi-label assignments proceedings siam international conference data mining philadelphia pennsylvania april available http //dx.doi.org/./.. alaydie hierarchical multi-label classiﬁcation protein function prediction going beyond traditional approaches ph.d. dissertation detroit aai. randomly partitioned instances training data test data mentioned above. moreover evaluate ability different methods exploit unlabeled instances fraction label vectors randomly selected used training phase. note setting randomly selected training data label vectors instances previous setting randomly selected fraction entries matrix entries zeros. horizontal axis figs. shows percentages used label vectors training set. recall among methods slrm proposed esmc exploit unlabeled data. ﬁgures illustrate almost experiments proposed esmc outperforms mentioned methods. evaluate ability proposed esmc handle large-scale datasets used large-scale dataset nuswide well medium-scale datasets mediamill delicious. table compares performance proposed method leml setting algorithm once. fact used standard training test data provided datasets. leml used advances information processing systems annual conference neural montreal quebec canada december available http//papers.nips.cc/paper/ -sparse-local-embeddings-for-extreme-multi-label-classiﬁcation torre costeira bernardino matrix completion multi-label image classiﬁcation advances neural information processing systems annual conference neural information processing systems proceedings meeting held december granada spain. available http//papers.nips.cc/paper/ -matrix-completion-for-multi-label-image-classiﬁcation wang blei collaborative topic modeling recommending scientiﬁc articles proceedings sigkdd international conference knowledge discovery data mining ser. york available http//doi.acm.org/./. gasse aussem elghazel optimality multi-label classiﬁcation subset zero-one loss distributions satisfying composition property proceedings international conference machine learning icml lille france july available http//jmlr.org/proceedings/papers/v/gasse.html damianou lawrence deep gaussian processes proceedings sixteenth international conference artiﬁcial intelligence statistics aistats scottsdale april available http//jmlr.org/proceedings/papers/v/damianoua.html titsias lawrence bayesian gaussian process latent variable model proceedings thirteenth international conference artiﬁcial intelligence statistics aistats chia laguna resort sardinia italy available http//www.jmlr.org/proceedings/papers/ v/titsiasa.html guillaumin mensink verbeek schmid tagprop discriminative metric learning nearest neighbor models image auto-annotation international conference computer vision available http//lear.inrialpes.fr/pubs/ /gmvs b.-g. multi-label learning missing labels image annotation facial action unit recognition pattern recognition vol. available http//www.sciencedirect.com/science/article/pii/s koyejo natarajan ravikumar dhillon consistent multilabel classiﬁcation advances neural information processing systems annual conference neural information processing systems december montreal quebec canada available http//papers.nips.cc/paper/ -consistent-multilabel-classiﬁcation tsoumakas spyromitros-xiouﬁs vilcek vlahavas mulan java library multi-label learning journal machine learning research vol. available http//research.microsoft.com/en-us/um/people/manik/downloads/ xc/xmlrepository.htmlbhatia ding wang multi-label classiﬁcation feature-aware implicit label space encoding proceedings international conference machine learning beijing china jain dhillon large-scale multith label international conference machine learning icml beijing china june available http//jmlr.org/proceedings/papers/v/yu.html t.kwok multilabel classiﬁcation label correlations missing labels proceedings twenty-eighth aaai conference artiﬁcial intelligence hong king university science technology hong kong jan. label space advances weinberger eds. available http//papers.nips.cc/paper/-feature-aware-label-\\ space-dimension-reduction-for-multi-label-classiﬁcation.pdf chen zheng weinberger fast image tagging. icml ser. jmlr proceedings vol. jmlr.org available http//dblp.uni-trier.de/db/conf/icml/ icml.htmlchenzw snelson ghahramani sparse gaussian processes using pseudo-inputs advances neural information processing systems weiss sch¨olkopf platt eds. press available http//papers.nips.cc/paper/ -sparse-gaussian-processes-using-pseudo-inputs.pdf akbarnejad baghshah probabilistic multi-label classiﬁer missing noisy labels handling capability submitted pattern recognition letters elsevier kwok efﬁcient multi-label classiﬁcation many labels proceedings international conference machine learning dasgupta mcallester eds. vol. jmlr workshop conference proceedings available http//jmlr.org/proceedings/ papers/v/bi.pdf balasubramanian lebanon landmark selection method multiple output prediction. icml. icml.cc omnipress available http//dblp.uni-trier.de/db/conf/ icml/icml.htmlbalasubramanianl", "year": 2016}