{"title": "Deep Networks with Internal Selective Attention through Feedback  Connections", "tag": ["cs.CV", "cs.LG", "cs.NE", "68T45"], "abstract": "Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNets feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model.", "text": "traditional convolutional neural networks stationary feedforward. neither change parameters evaluation feedback higher lower layers. real brains however deep attention selective network architecture. dasnets feedback structure dynamically alter convolutional ﬁlter sensitivities classiﬁcation. harnesses power sequential processing improve classiﬁcation performance allowing network iteratively focus internal attention convolutional ﬁlters. feedback trained direct policy search huge million-dimensional parameter space scalable natural evolution strategies cifar- cifar- datasets dasnet outperforms previous state-of-the-art model. deep convolutional neural networks max-pooling layers trained backprop gpus become state-of-the-art object recognition segmentation/detection scene parsing architectures consist many stacked feedforward layers mimicking bottom-up path human visual cortex layer learns progressively abstract representations input data. low-level stages tend learn biologically plausible feature detectors gabor ﬁlters detectors higher layers learn respond concrete visual objects parts e.g. trained never changes weights ﬁlters evaluation. evolution discovered efﬁcient feedforward pathways recognizing certain objects blink eye. however expert ornithologist asked classify bird belonging similar species think milliseconds answering implying several feedforward evaluations performed evaluation tries elicit different information image. since humans beneﬁt greatly strategy hypothesize cnns too. requires formulation non-stationary adapt behaviour post-training process decides adapt cnns behaviour. paper introduces deep attention selective networks model selective attention deep cnns allowing layer inﬂuence layers successive passes image special connections modulate activity convolutional ﬁlters. weights special connections implement control policy learned reinforcement learning trained usual supervised learning. given input image attentional policy enhance suppress features multiple passes improve classiﬁcation difﬁcult cases captured initially supervised training. system check usefulness internal ﬁlters automatically omitting manual inspection current implementation attentional policy evolved using separable natural evolution strategies instead conventional single agent reinforcement learning method large number parameters required control cnns size typically used image classiﬁcation. experiments cifar- cifar show difﬁcult classiﬁcation instances network corrects emphasizing de-emphasizing certain ﬁlters outperforming previous state-ofthe-art cnn. work maxout networks combined dropout underlying model dasnet. maxout networks represent state-of-the-art object recognition various tasks outperformed averaging committees several convolutional neural networks. similar approach reduce dimensionality favor sparsity representation also recently presented maxout cnns consist stack alternating convolutional maxout layers ﬁnal classiﬁcation layer convolutional layer. input layer image output previous layer consisting input maps width height rc×m×n. output consists output maps rc′×m′×n′. convolutional layer parameterized ﬁlters size denote ﬁlters rk×k indexes input output maps denotes layer. index input output respectively convolutional operator element-wise nonlinear function used index layer. size output determined kernel size stride used convolution pooling layer. pooling layer used reduced dimensionality output convolutional layer. usual approach take maximum value among nonpartially-overlapping patches every therefore reducing dimensionality along height width instead maxout pooling layer reduces every consecutive maps keeping maximum value every pixel-position called block size. thus reduces input maps output maps. rc′×m′×n′ used index layer. output pooling layer either used input another pair convolutionalpooling layers form input ﬁnal classiﬁcation layer. classiﬁcation layer. finally classiﬁcation step performed. first output last pooling layer ﬂattened large vector form input following equations reinforcement learning general framework learning make sequential decisions order maximize external reward signal learning agent anything ability perceive given environment. time agent receives observation current state environment selects action chosen policy spaces possible states observations action respectively. agent enters state receives reward objective dasnet observation action spaces real valued rdim rdim. therefore policy must represented function approximator e.g. neural network parameterized policies used control attention dasnet state actions spaces close thousand dimensions policy parameter vector contain close million weights impractical standard methods. therefore instead evolve policy using variant natural evolution strategies called separable family black-box optimization algorithms parameterized probability distributions search space instead explicit population typically distribution multivariate gaussian parameterized mean covariance matrix epoch generation sampled distribution updated direction natural gradient expected ﬁtness distribution. snes differs standard instead maintaining full covariance matrix search distribution uses diagonal entries. snes theoretically less powerful standard substantially efﬁcient. idea behind dasnet harness power sequential processing improve classiﬁcation performance allowing network iteratively focus attention ﬁlters. first standard maxout augmented allow ﬁlters weighted differently different passes image weight j-th output layer changing strength actiaℓ vation applying maxout pooling operator. vector represents action learned policy must select order sequentially focus attention maxout discriminative features image processed. changing action alter behaviour resulting different outputs even image change. indicate following notation evolved using snes focus attention pass loop represents generation snes. generation starts selecting subset images random. samples drawn snes search distribution representing parameters candidate policy undergoes trials image batch. trial image presented maxout times. ﬁrst pass action maxout network functions would normally action effect. image propagated observation vector constructed concatenating following values extracted averaging activations provides partial state information values still meaningful enough allow selection good actions. candidate policy maps observation action rdim×dim weight matrix neural network softmax. note softmax function scaled dimensionality action space elements action vector average ensuring network outputs positive thereby keeping ﬁlter activations stable. output pass correct classiﬁcation λcorrect λmisclassif constants. measures weighted loss misclassiﬁed samples weighted higher correctly classiﬁed samples λmisclassif λcorrect. simple form boosting used focus ‘difﬁcult’ misclassiﬁed images. input images processed policy assigned ﬁtness figure dasnet network. image classiﬁed passes network. forward propagation maxout output classiﬁcation vector output second last layer averages feature maps combined observation vector used deterministic policy choose action changes weights feature maps next pass image. pass output maxout ﬁnally used classify image. candidate policies evaluated snes updates distribution parameters according natural gradient calculated sampled ﬁtness values snes repeatedly updates distribution course many generations expected ﬁtness distribution improves stopping criterion met. human vision still advanced ﬂexible perceptual system known. architecturally visual cortex areas highly connected including direct connections multiple levels top-down connections. felleman essen constructed hierarchy diagram different visual cortical areas macaque visual cortex. pairs areas considered connected connected areas connected bidirectionally. top-down connections numerous bottom-up connections generally diffuse thought play primarily modulatory role feedforward connections serve directed information carriers analysis response latencies newly-presented image lends credence theory stages visual processing fast pre-attentive phase feedforward processing followed attentional phase inﬂuence recurrent processing feedforward pass recognize localize simple salient stimuli pop-out response times increase regardless number distractors. however effect conclusively shown basic features color orientation; categorical stimuli faces whether pop-out effect remains controversial regarding attentional phase feedback connections known play important roles feature grouping differentiating foreground background perceptual ﬁlling work supports idea top-down projections prefrontal cortex play important role object recognition quickly extracting low-level spatial frequency information provide initial guess potential categories forming top-down expectation biases recognition. recurrent connections seem rely heavily competitive inhibition feedback make object recognition robust context computer vision shown able learn saccades visual scenes learn selective attention learn feedback lower levels improve face recognition shown effective object recognition also combined traditional computer vision primitives iterative processing images using recurrency successfully used image reconstruction face-localization approaches show recurrency processing perspective lead novel algorithms improve performance. however research often applied simpliﬁed datasets demonstration purposes computation constraints aimed improving state-of-the-art. contrast apply perspective directly known state-of-the-art neural networks show approach feasible actually increases performance. experimental evaluation dasnet focuses ambiguous classiﬁcation cases cifar- cifar- data sets where high number common features classes often mistaken other. interesting cases approach. learning already trained model dasnet must ﬁxing erroneous predictions without disrupting forgetting learned. number steps experimentally determined ﬁxed enough steps allow dasnet adapt small enough practical. possible iterate condition could serious limitation real-time applications predictable processing latency critical. experiments λcorrect λmisclassiﬁed maxout network used experiments trained data augmentation following suggested global contrast normalization normalization protocol. model consists three convolutional maxout layers followed fully connected maxout softmax outputs. dropout used layers except input layer input layer. population size snes table shows performance dasnet methods achieves relative improvement respect vanilla cnn. establishes state-of-the-art result challenging dataset. figure shows classiﬁcation cat-image test-set. output activations ﬁnal step shown top. difference activations compared ﬁrst step i.e. emphasis shown bottom. left class probabilities time-step. ﬁrst step classiﬁcation ‘dog’ could indeed mistaken puppy. note ﬁrst step figure dasnets trained cifar- different values allowed iterations image. performance peeks number steps network trained after performance drops explode showing dynamics stable. table classiﬁcation results cifar- cifar- datasets. error test-set shown several methods. note result dropconnect average models. method improves state-of-the-art reference implementation feedback connections added. network received feedback. next step probability ‘cat’ goes dramatically beating ’dog’ subsequently drops following steps. network successfully disambiguated dog. investigate ﬁlters already lower layer emphasis changes signiﬁcantly. ﬁlters focus surroundings whilst others de-emphasize eyes. second layer almost output maps emphasized. third highest convolutional layer complex changes network. level positional correspondence largely lost ﬁlters known code ‘higher level’ features. layer changes inﬂuential closest ﬁnal output layers. hard analyze effect alterations differences simple increases decreases output maps would expect ﬁnal activations corresponding increases largely similar. instead complex emphasis pattern suppression. dynamics investigate dynamics small -layer dasnet network trained different values evaluated allowing steps. figure shows results training dasnet cifar- performance goes vanilla peaks step expected reduces stays stable that. even though dasnet trained using small number steps dynamics stay stable evaluated many steps. verify whether dasnet policy actually making good gates information content estimated following gate values last step taking used directly classiﬁcation. gates used properly activation contain information relevant classiﬁcation would expect dasnet trained used features classiﬁcation. using ﬁnal gate-values classiﬁcation using -nearest neighbour logistic regression performed. resulted performance correct respectively similar figure classiﬁcation dasnet shown. output activations ﬁnal step shown top. changes relative initial activations ﬁrst step shown bottom changes normalized show effects clearly. class probabilities time shown left. network ﬁrst classiﬁes image corrects emphasizing convolutional ﬁlters actually cat. dasnet deep neural network feedback connections learned reinforcement learning direct selective internal attention certain features extracted images. rapid ﬁrst shot image classiﬁcation standard stack feedforward ﬁlters feedback actively alter importance certain ﬁlters hindsight correcting initial guess additional internal thoughts. dasnet successfully learned correct image misclassiﬁcations produced fully trained feedforward maxout network. active selective internal spotlight attention enabled state-of-the-art results.", "year": 2014}