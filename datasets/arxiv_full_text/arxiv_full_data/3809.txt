{"title": "Non-linear Learning for Statistical Machine Translation", "tag": ["cs.CL", "cs.NE"], "abstract": "Modern statistical machine translation (SMT) systems usually use a linear combination of features to model the quality of each translation hypothesis. The linear combination assumes that all the features are in a linear relationship and constrains that each feature interacts with the rest features in an linear manner, which might limit the expressive power of the model and lead to a under-fit model on the current data. In this paper, we propose a non-linear modeling for the quality of translation hypotheses based on neural networks, which allows more complex interaction between features. A learning framework is presented for training the non-linear models. We also discuss possible heuristics in designing the network structure which may improve the non-linear learning performance. Experimental results show that with the basic features of a hierarchical phrase-based machine translation system, our method produce translations that are better than a linear model.", "text": "normalization term equation translation hypotheses source sentence score hypothesis denoted actually linear combination features shown equation log-linear models ﬂexible incorporate features show signiﬁcant advantage traditional source-channel models thus become state-of-the-art modeling method applied various translation worth noticing log-linear models separate good translation hypotheses using linear hyper-plane. however complex interactions features make difﬁcult linearly separate good translation hypotheses ones taking features typical phrase-based machine translation system example language model feature favors shorter hypotheses; word penalty feature encourages longer hypotheses. phrase translation probability feature selects phrases occurs frequently training corpus sometimes long lower translation probability translating named entities idmodern statistical machine translation systems usually linear combination features model quality translation hypothesis. linear combination assumes features linear relationship constrains feature interacts rest features linear manner might limit expressive power model lead under-ﬁt model current data. paper propose nonlinear modeling quality translation hypotheses based neural networks allows complex interaction features. learning framework presented training non-linear models. also discuss possible heuristics designing network structure improve non-linear learning performance. experimental results show basic features hierarchical phrase-based machine translation system method produce translations better linear model. core problems research statistical machine translation modeling translation hypotheses. modeling method deﬁnes score target sentence given source sentence ...fj target word source word. well-known modeling method starts source-channel model scoring decomposes calculation translation model language model. ioms; sometimes short high translation probability translating verbs pronouns. three features jointly decide choice translations. simply weighted values best choice modeling translations. paper propose non-linear modeling translation hypotheses based neural networks. traditional features machine translation system used input network. feeding input features nodes hidden layer complex interactions among features modeled resulting much stronger expressive power traditional log-linear models. employing neural network non-linear models issues tackled. ﬁrst issue parameter learning. loglinear models rely minimum error rate training achieve best performance. scoring function become nonlinear intersection points non-linear functions could effectively calculated enumerated. thus mert longer suitable learning parameters. solve problem present framework effective training including several criteria transform training problem binary classiﬁcation task uniﬁed objective function iterative training algorithm. second issue structure neural network. single layer neural networks equivalent linear models; two-layer networks sufﬁcient nodes capable learning continuous function adding layers network could model complex functions less nodes also brings problem vanishing gradient adapt two-layer feed-forward neural network keep training process efﬁcient. notice major problem prevent neural network training reaching good solution many local minimums parameter space. thus discuss constrain learning neural networks intuition observations features. proposed learning framework. experimental results show framework could achieve better translation quality even traditional features previous linear models. line research attempted reinterpret original features feature transformation additional learning. example maskey zhou deep belief network learn representations phrase translation lexical translation probability features. clark used discretization transform realvalued dense features binary indicator features. learned features using semi-supervised deep auto encoder. work focus explicit representation features usually employ extra learning procedure. proposed method take original feature transformation input. feature transformation combination performed implicitly training network integrated optimization translation quality. second line research attempted non-linear models instead log-linear models similar spirit work. kirchhoff used boosting method combine several results mert achieved improvement proposed additive neural network employed two-layer neural network embedding-based features. avoid local minimum still rely pre-training posttraining mert pro. comparing efforts proposed method takes step integrated iterative training instead re-ranking works without help pre-trained linear models. third line research attempted lognon-linear neural network linear based models trained language models translation models joint language translation models also introduced word embedding source target side translation rule local features. paper focus enhancing expressive power modeling independent research enhancing translation system designed features. believe additional improvement could achieved incorporating features framework. non-linear modeling translation hypotheses could used phrase-based system syntax-based systems. paper take hierarchical phrase based machine translation system example introduce non-linearity system. non-linear translation system different traditional systems calculate score hypothesis. instead calculating score linear combination neural networks perform non-linear combination feature values. algorithm keep decoding efﬁcient. although non-linearity model scores cause search errors ﬁnding highest scoring hypothesis practice still achieves reasonable results. employ two-layer neural network nonlinear model scoring translation hypotheses. structure typical two-layer feed-forward neural network includes input layer hidden layer output layer input layer accept input features hidden layer combine different input features output layer node output model score translation hypothesis based value hidden nodes. speciﬁcally score hypothesis denoted weight matrix bias vector neural nodes respectively; activation function often non-linear functions tanh function sigmoid function; subscript indicates parameters hidden layer output layer respectively. traditional machine translation systems rely mert tune weight different features. mert performs efﬁcient search enumerating score function hypotheses using intersections linear functions form upper-envelope model score function scoring function non-linear feasible intersections functions. section discuss alternatives train parameter non-linear models. task machine translation complex problem structural output space. decoding algorithms search translation hypothesis highest score according given scoring function exponentially large candidate hypotheses. purpose training select scoring function function score hypotheses correctly. correctness often introduced extrinsic metrics bleu denote scoring function simply parametrized denote candidate hypotheses denote extrinsic metric eval. note that linear cases linear function equation non-linear case described paper scoring function equation best v.s. worst score best hypothesis higher worst hypothesis n-best set. objective motivated practice separating hope fear translation hypothesis take simpler strategy uses best worst hypothesis cnbest hope fear hypothesis respectively order avoid multi-pass decoding. pairwise score better hypotheses sampled hypothesis pairs higher worse ones pair. objective adapted pairwise ranking optimization tries ranking hypotheses instead selecting best one. sampling strategy original paper. note criterions transforms original problem selecting best hypotheses exponential space certain pair-wise comparison problem could easily trained standard binary classiﬁers. binary classiﬁcation task hinge loss following watanabe network parameters compared linear model norm instead norm regularization term favor sparse solutions. deﬁne training objective function equation practice candidate exponentially large hard enumerate; correct translation even exist current search space various reasons e.g. unknown source word. result seek following three alternatives approximations ideal objective. standard training algorithm classiﬁcatraining instances stays tion iteration. machine translation decoding algorithms usually return different n-best different parameters. exponentially large size search space. mert extend current nbest merging n-best previous iterations pool enlarged n-best give better approximation true hypothesis lead better stable training results. argue training still focus hypotheses obtained current round iteration searching n-best independent previous iterations. compromise goals practice training hypothesis pairs ﬁrst generated current n-best merged pairs generated previous iterations. order make model focus pairs current iteration assign pairs previous iterations small constant weight assign pairs current iteration relatively large constant weight. inspired adaboost algorithm weighting instances. shown algorithm training procedure starts randomly init model parameters iteration decoding algorithm decodes sentence n-best cnbest training hypothesis pairs extracted cnbest according training criterion described section collected pairs although neural networks bring strong expressive power modeling translation hypothesis training neural network prone resulting local minimum affect training results. speculate reason local minimums structure well-connected network many parameters. take neural network nodes input layer nodes hidden layer example. every node hidden layer connected input nodes. simple structure resulting least parameters. section norm objective function order sparser solutions. section propose constrained network structures according prior knowledge features. structures much less parameters simpler structures comparing original neural networks thus reduce possibility getting stuck local minimums. ﬁrst pitfall standard two-layer neural network node hidden layer receives input every input layer node. features used usually manually designed concrete meanings. network several hidden nodes combining every features every hidden node redundant result take harsh step constrain nodes hidden layer in-degree means hidden node accepts inputs input nodes. prior knowledge features setting. network nodes input layer hidden layer contain nodes accept combinations input layer. name network structure two-degree hidden layer network parameters. note perform -degree combination looks similar spirit combination atomic features large scale discriminative learning tasks tagging parsing. however unlike practice tasks directly combines values different features generate feature type ﬁrst linearly combine value features perform non-linear transformation values activation function. might strong constraint require hidden node in-degree order relax constraint need prior knowledge features. ﬁrst observation different types features. types different terms value ranges sources importance etc. example language model features usually take small value probability word count feature takes integer value usually much higher weight linear case count features. second observation features group basically type complex interaction other. example reasonable combine language model features word count features hidden node. necessary combine count initial phrases count unknown words hidden node. constraints given disjoint partition features every hidden node takes input input nodes nodes come different feature groups. name network structure grouped network practice divide basic features section groups language model features translation probability features lexical probability features word count feature rest count features. conduct experiments large scale machine translation tasks. parallel data comes including ldce ldce ldce ldct ldct consists million sentence pairs. monolingual data includes xinhua portion gigaword corpus. multi-references data training data development data test data. data mainly genre avoiding extra consideration domain adaptation. chinese side corpora word segmented using ictclas. translation system in-house implementation hierarchical phrase-based translation system. beam size train -gram language model monolingual data smoothing. parameter tuning experiments training procedure times present average results. translation quality evaluated -gram case-insensitive bleu test performed using bootstrap re-sampling implemented clark employ twolayer neural network input layer nodes table bleu percentage different training criteria hypothesis pairs sentence. hidden nodes three settings make fair comparison. results presented table ﬁrst rows compare training withweighted combination hypothesis pairs discussed section result suggested weighted combination hypothesis pairs previous iterations performance improves signiﬁcantly test sets. although system performance varies performance test sets alcomparable. suggest although three training criteria based different assumptions basically equivalent training translation systems. also compares three training criteria number instances iteration ﬁnal training accuracy compared tries separate best hypothesis rest hypotheses n-best tries obtain correct ranking hypotheses aims separating best worst hypothesis iteration easier task learning classiﬁers. requires least training instances achieves best performance training. note that accuracy system table accuracy system achieves training stops. calculated instances thus directly comparable. differences accuracy indicator difﬁculties corresponding learning task. ﬁrst compares neural network different number hidden nodes. systems tlayer tlayer tlayer standard two-layer feed forward neural network hidden layer nodes. training larger network lead improvement translation quality. however training larger network often time-consuming. experimented neural networks hidden nodes tlayer takes times longer training time itercompared network structures proposed section two-degree hidden layer network already perform comparable baseline system. constrain input hidden node degree likely restrictive. grouped feature could design networks shows signiﬁcant improvement baseline systems achieves best performance among neural systems. note much larger scale also sparse parameters takes signiﬁcant less training time standard neural networks. paper discuss non-linear framework modeling translation hypothesis statistical machine translation system. also present learning framework including training criterion algorithms integrate modeling state hierarchical phrase based machine translation system. compared previous effort bringing non-linearity machine translation method uses single two-layer neural networks performs training independent previous linear training methods method also trains parameters without pre-training post-training procedure. experiment shows method could improve baseline system even feature input large scale chinese-english machine translation task. future work necessary integrate features learning framework. also interesting non-linear modeling complex learning tasks involves domain speciﬁc learning techniques.", "year": 2015}