{"title": "Kernelized Locality-Sensitive Hashing for Semi-Supervised Agglomerative  Clustering", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Large scale agglomerative clustering is hindered by computational burdens. We propose a novel scheme where exact inter-instance distance calculation is replaced by the Hamming distance between Kernelized Locality-Sensitive Hashing (KLSH) hashed values. This results in a method that drastically decreases computation time. Additionally, we take advantage of certain labeled data points via distance metric learning to achieve a competitive precision and recall comparing to K-Means but in much less computation time.", "text": "large scale agglomerative clustering hindered computational burdens. propose novel scheme exact inter-instance distance calculation replaced hamming distance kernelized locality-sensitive hashing hashed values. results method drastically decreases computation time. additionally take advantage certain labeled data points distance metric learning achieve competitive precision recall comparing k-means much less computation time. proposed research topic clustering scalable dataset semi-supervised approach based hashing methods. particular goal explore underlying data distribution clustering data points differentiating classes. small labeled data come subset classes given want whole data distribution complete classes. example given labels classes separate classes well time discover existence third class. requires using information labeled data transformation metric split classes well; data transformation discover third class exists. suppose handwritten digit recognition task dataset contains digits general agglomerative clustering might clusters cluster other similarity shapes. however small labeled classes given learned degree granularity similarity comparison. using data transformation maximally split clusters able identify existence another cluster digit agglomerative clustering suffers computation inefﬁciency major contribution paper introduce machine learned hashing method kernelized locality-sensitive hashing agglomerative clustering. results efﬁcient computation clustering large-scale dataset. paper structured follows. provide background study related work section section presents algorithms distance metric learning klsh clustering. section describes experiments discussion results followed conclusions section much previous work cluster seeding address limitation iterative clustering techniques sensitive choice initial starting points problem addressed select seed points absence prior knowledge. kaufman rousseeuw propose elaborate mechanism ﬁrst seed instance central data; rest representatives selected choosing instances promise closer remaining instances. pena empirically compare four initialization methods k-means algorithm illustrate random kaufman initializations outperform since make k-means less dependent initial choice seeds. k-means++ random starting points chosen speciﬁc probabilities point chosen seed probability proportional contribution overall potential augmenting k-means using simple randomized seeding technique k-means++ competitive optimal clustering. bradley fayyad propose reﬁning initial seeds taking account modes underlying distribution. reﬁned initial seed enables iterative algorithm converge better local minimum. semi-supervised learning also seen unsupervised learning guided constraints. noticed clustering heavily dependent distance metrics particular algorithm executor follow rules pointed desire systematic learn distance metric clustering labeled data. based posing metric learning convex optimization problem. data size growing exponentially hashing technique especially good solving large scale problems. described locality-sensitive hashing method efﬁcient algorithm approximate exact nearest neighbor problem. goal preprocess dataset objects later given query object quickly return dataset object similar query. technique signiﬁcant interest wide variety areas unsupervised learning. hierarchical clustering tries solve similar problem another perspective. iteratively ﬁnding nearest neighbors groups data clusters. kernelized later proposed fast image search. generalizes accommodate arbitrary kernel functions making possible preserve algorithm’s sub-linear time similarity search guarantees wide class useful similarity functions. section describe methods solve large scale semi-supervised learning problem ﬁrst introducing distance learning metrics fast agglomerative clustering method based kernelized locality-sensitive hashing circumstances data given labeled points know points sure belongs different classes. similarity dissimilarity matrix respectively. entry similarity matrix data class otherwise. similarly dissimilarity matrix based learn distance metric y||a data points positive semi-deﬁnite matrix distance parameters among data points. idea minimize distance similar points keeping dissimilar points apart. input datalabeled data step learn distance metric labeled data step build a-distance klsh table step initialization cluster distribution {{xi} i.e. data point individual cluster proximity matrix hash keys. repeat find minrs=...nr=sd. merge single cluster form deﬁne proximity matrix deleting rows columns corresponding merged clusters adding column cluster. input data distance parameter step randomly select points data denoted build kernel exp/σ) p′a. step apply suppose σ−/u step form p-dim vector dimensions others dimensions chosen randomly. step −/es. created sign algorithm aims solve large scale agglomerative clustering problem. ﬁrst learn distance metric small labeled data second step build klsh table data hashed bits. rest procedure agglomerative clustering performed. instead explicitly computing inter-instance distances clustering done based klsh-hashed data points measuring hamming distance. kernelized locality-sensitive hashing method high probability preserving neighborhoods it’s reasonable substitute exact inter-instance distances. table experiment results compare four methods k-means k-means distancemetric learning agglomerative clustering using klsh agglomerative clustering using klsh distance-metric learning. precision recall computation time reported. data underlying classes hash code -bit. experiments using k-means klsh agglomerative clustering without distance metric learning. hash string length number classes number data points varied time. report precision recall computation time. experiments done machine -core intel processors ram. first table observed klsh agglomerative clustering achieve level precision fraction computational cost. downside recall caused factor decrease recall caused fact klsh cannot recover points nearest neighborhood. addition distance metric learning noticeable beneﬁts performance klsh agglomerative clustering. table analyzed effect increase number classes precision recall computation time. precision remains constant recall decreases. computational costs remains relatively independent number clusters. also able adjust tradeoff efﬁciency effectiveness. notice even binary hash code still possible outcomes. hashing split data well number entries table still large. increases accuracy clustering results meanwhile leads higher computation cost agglomerative clustering. according results clustering klsh superior performance dataset large number real clusters small. comparing k-means large promising improvement speed. true cluster number large achieves high performance speed accuracy. especially lower level linkage tree clustering bias immediately correctly cluster similar data instances. table compare performance various number underlying classes agglomerative clustering using klsh distance metric learning. case data size hash code -bit. increase number classes precision remains constant recall decreases table compare performance various number hash code bits agglomerative clustering using klsh distance metric learning. case data size underlying classes. increasing length hash string increases precision recall. general hierarchical clustering methods cannot scale well large dataset exponentially growing number calculations inter-instance distances. kernelized locality-sensitive hashing provides high probability preserving neighborhoods it’s reasonable substitute exact inter-instance distances. proposed klsh agglomerative clustering alleviates problem calculating reduced-sized hamming distance achieves efﬁcient clustering computation. incorporation distance metric learning marginally improves precision recall.", "year": 2013}