{"title": "Near-optimal Bayesian Active Learning with Correlated and Noisy Tests", "tag": ["cs.LG", "cs.AI"], "abstract": "We consider the Bayesian active learning and experimental design problem, where the goal is to learn the value of some unknown target variable through a sequence of informative, noisy tests. In contrast to prior work, we focus on the challenging, yet practically relevant setting where test outcomes can be conditionally dependent given the hidden target variable. Under such assumptions, common heuristics, such as greedily performing tests that maximize the reduction in uncertainty of the target, often perform poorly. In this paper, we propose ECED, a novel, computationally efficient active learning algorithm, and prove strong theoretical guarantees that hold with correlated, noisy tests. Rather than directly optimizing the prediction error, at each step, ECED picks the test that maximizes the gain in a surrogate objective, which takes into account the dependencies between tests. Our analysis relies on an information-theoretic auxiliary function to track the progress of ECED, and utilizes adaptive submodularity to attain the near-optimal bound. We demonstrate strong empirical performance of ECED on two problem instances, including a Bayesian experimental design task intended to distinguish among economic theories of how people make risky decisions, and an active preference learning task via pairwise comparisons.", "text": "consider bayesian active learning experimental design problem goal learn value unknown target variable sequence informative noisy tests. contrast prior work focus challenging practically relevant setting test outcomes conditionally dependent given hidden target variable. assumptions common heuristics greedily performing tests maximize reduction uncertainty target often perform poorly. paper propose eced novel computationally efﬁcient active learning algorithm prove strong theoretical guarantees hold correlated noisy tests. rather directly optimizing prediction error step eced picks test maximizes gain surrogate objective takes account dependencies tests. analysis relies information-theoretic auxiliary function track progress eced utilizes adaptive submodularity attain near-optimal bound. demonstrate strong empirical performance eced problem instances including bayesian experimental design task intended distinguish among economic theories people make risky decisions active preference learning task pairwise comparisons. optimal information gathering i.e. selectively acquiring useful data central challenges machine learning. problem optimal information gathering studied context active learning bayesian experimental design policy making optimal control numerous domains. typical set-up problems unknown target variable interest tests correspond observable variables deﬁned probabilistic model. goal determine value target variable sequential policy adaptively selects next test based previous observations cost performing tests minimized. deriving optimal testing policy np-hard general however certain conditions approximation results known. particular test outcomes deterministic functions target variable simple greedy algorithm namely generalized binary search guaranteed provide near-optimal approximation optimal policy hand test outcomes noisy outcomes different tests conditionally independent given using informative selection policy greedily selects test maximizes expected reduction uncertainty target variable guaranteed perform near-optimally however many practical problems effect noise complex structural assumptions probabilistic model access tests indirectly informative target variable consequence test outcomes become conditionally dependent given consider medical diagnosis example doctor wants predict best treatment patient carrying series medical tests reveals information patient’s physical condition. here outcomes medical tests conditionally independent given patient’s condition independent given treatment made based patient’s condition. known cases informative selection policy perform arbitrarily poorly. golovin formalize problem equivalence class determination problem show tests’ outcomes noise-free obtain near-optimal expected cost running greedy policy based surrogate objective function. results rely fact surrogate objective function exhibits adaptive submodularity natural diminishing returns property generalizes classical notion submodularity adaptive policies. unfortunately general setting tests noisy efﬁcient policies known provably competitive optimal policy. paper introduce equivalence class edge discounting contribution. novel algorithm practical bayesian active learning experimental design problems prove strong theoretical guarantees correlated noisy tests. particular focus setting tests’ outcomes indirectly depend target variable assume outcome test corrupted random persistent noise prove test outcomes binary noise test outcomes mutually independent eced guaranteed obtain near-optimal cost compared optimal policy achieves lower prediction error develop theoretical framework analyzing sequential policies leverage information-theoretic auxiliary function reason effect noise combine theory adaptive submodularity attain near-optimal bound insight show eced effectively making progress long picks tests even myopic choices tests immediate gain terms reducing uncertainty target variable. demonstrate compelling performance eced real-world problem instances bayesian experimental design task intended distinguish among economic theories people make risky decisions active preference learning task pairwise comparisons facilitate better understanding provide detailed proofs illustrative examples third application pool-based active learning supplemental material. basic model target random variable whose value want learn. value ranges among depends deterministically another random variable supp known distribution concretely deterministic mapping supp gives collection discrete observable variables statistically dependent indexing variable test. performing test produces outcome incurs unit cost. think representing underlying root-cause among possible root-causes joint event representing optimal target action taken root-cause also xe’s test perform whose observation reveals information medical diagnosis example xe’s encode tests’ outcomes encodes treatment encodes patient’s physical condition. crucially assume xe’s conditionally independent given i.e. known parameters. note noise implicitly encoded model equivalently assume xe’s ﬁrst generated deterministic mapping perturbed random noise. example test outcomes binary think resulting ﬂipping deterministic outcome test given probability ﬂipping events tests mutually independent. figure shows illustrative example medical diagnosis problem. initialize drawing edges pairs root-causes mapped different treatments remove edges incident root-causes observe eced instead discounts edge weights accordingly. problem statement consider sequential adaptive policies picking tests. denote policy words policy speciﬁes test pick next well stop picking tests based tests picked corresponding outcomes. pick observations represented partial realization formally policy deﬁned partial mapping partial realizations tests. suppose running till termination returns sequence test-observation pairs length denoted i.e. interpreted random path taken policy observed obtain posterior maxy∈y observing estimator error probability pmap expected error probability running policy deﬁned perr eψπpmap cost policy cost maxψπ |ψπ| i.e. maximum number tests performed possible paths takes. given small tolerance seek policy minimal cost upon termination achieve expected error probability less denote policy opt. formally seek note computing optimal policy problem intractable general. problem reduces equivalence class determination problem here target variables referred equivalence classes since corresponds subset root-causes supp share action. tests noise-free i.e. problem solved near-optimally equivalence class edge cutting algorithm illustrated fig. employs edge-cutting strategy based weighted graph vertices represent root-causes edges link root-causes want distinguish between. formally consists pairs root-causes corresponding different target values deﬁne weight function i.e. product probabilities incident root-causes. extend weight function sets edges weight edges i.e. {θθ}∈e w}). performing test outcome said edge least incident root-causes inconsistent denote edges observing objective formally partial realizations tests’ outcomes. call subrealization denoted every test seen also seen function called adaptive submodular w.r.t. distribution holds exe}) further function called strongly adaptively monotone w.r.t. test seen holds sequential decision problems satisfying adaptive submodularity strongly adaptive monotonicity policy greedily upon observed selects test maxe guaranteed attain near-minimal cost noisy setting however longer attain error probability even exhaust tests. natural approach solving problem would pick tests greedily maximizing expected reduction error probability perr. however objective adaptive submodular; fact show supplemental material policy perform arbitrarily badly complementaries among tests i.e. gain tests better individual gains tests set. therefore motivated objective noise-free setting would like optimize surrogate objective function captures effect noise amenable greedy optimization. natural extension edge-cutting strategy instead cutting edges discount edge weights bayesian updates observing discount weight edge multiplying probabilities incident root-causes likelihoods observation xe]. gives greedy policy that every iteration picks test maximal expected reduction total edge weight. call policy ec-bayes. unfortunately demonstrate later seemingly promising update scheme ideal solving problem tends pick tests noisy help facilitate differentiation among different target values. consider simple example three root-causes distributed target values want evaluate tests purely noisy test i.e. noiseless test easily verify running ec-bayes eced algorithm example hints important principle designing proper objective functions task noise rate increases must take reasonable precautions evaluating informativeness test undesired contribution noise accounted for. suppose performed test observed call root-cause consistent observation likely outcome given otherwise inconsistent. instead discounting maxx weight root-causes likelihoods choose discount root-causes likelihood ratio λθxe intuitively want penalize root-cause inconsistent observation consistent root-cause λθxe discount otherwise inconsistent λθxe test informative root-cause i.e. uniform neutralizes effect test terms edge weight reduction. formally given here choose normalize probabilities posterior probabilities. otherwise non-informative test reveal useful information therefore augment basic value function value non-informative test following offset value observing outcome easy check test non-informative holds δoffset otherwise δoffset motivates following objective function expected amount edge weight effectively reduced performing test call algorithm greedily maximizes ∆eced equivalence class edge discounting algorithm present pseudocode algorithm similar efﬁciency eced depends number root-causes. −maxx noise rate test main theoretical result show basic setting test outcomes binary test noise independent underlying root-causes eced competitive optimal policy achieves lower error probability problem theorem achieve expected error probability less sufﬁces steps supp| denotes number root-causes eced mine∈v characterizes severity noise cost worstcase cost optimal policy achieves expected error probability δopt log) times worst-case cost optimal algorithm achieves lower error probability oδ/). further observe upper noise-free setting sufﬁcient achieve perr running oklog target objective function) greedy policy optimal policy showing that one-step gain greedy policy always makes effective progress towards approaching cumulative gain steps. powerful tool facilitating adaptive submodularity theory imposes lower bound one-step greedy gain optimal policy given objective function consideration exhibits natural diminishing returns condition. unfortunately context target function optimize i.e. expected error probability policy satisfy adaptive submodularity. furthermore nontrivial understand directly relate objectives eced objective utilize selecting informative tests gain reduction error probability evaluating policy. circumvent problems introducing surrogate functions proxy connect eced objective ∆eced expected reduction error probability perr. ideally auxiliary objective faux tests maximal ∆eced also high gain faux; meanwhile faux also comparable error probability perr minimizing faux sufﬁcient achieving error probability. deﬁned faux. test deﬁne ∆aux faux assuming edge weights conﬁgured according posterior distribution prove following result lemma supp| noise rate associated test consider faux deﬁned equation log. holds lifting adaptive submodularity framework recall general strategy bound step gain faux gain optimal policy. order need show surrogate exhibits extent diminishing returns property. lemma relate ∆aux i.e. gain faux noisy setting ∆ecψ i.e. expected weight edges algorithm. since adaptive submodular allows lift adaptive submodularity framework analysis. result relate -step gain w.r.t. faux test selected eced cumulative gain w.r.t. optimal policy noise-free setting. further observe objective satisﬁes suppose want compare eced optimal policy opt. adaptive submodularity relate -step gain eced fecψ cummulative gain opt. combining equation lemma lemma bound -step gain faux eced k-step gain consequently bound cost eced problem defer detailed proof outline full proof supplemental material. demonstrate performance eced real-world problem instances bayesian experimental design task intended distinguish among economic theories people make risky decisions active preference learning task pairwise comparisons. space limitations defer third case study pool-based active learning supplemental material. baselines. ﬁrst baseline consider ec-bayes uses bayes’ rule update edge weights computing gain test note observing outcome test eced ec-bayes update posteriors according bayes’ rule; difference different strategies selecting test. also compare commonly used sequential information gathering policies information gain uncertainty sampling consider picking tests greedily maximizing reduction entropy target variable root-causes respectively. last consider myopic optimization decision-theoretic value information problems policy greedily picks test maximizing expected reduction prediction error preference elicitation behavioral economics ﬁrst conduct experiments bayesian experimental design task intends distinguish among economic theories people make risky decisions. several theories proposed behavioral economics explain people make decisions risk uncertainty. test eced theories subjective valuation risky choices namely expected utility constant relative risk aversion expected value prospect theory cumulative prospect theory weighted moments weighted standardized moments. choices risky lotteries i.e. known distribution payoffs test pair lotteries root-causes correspond parametrized theories predict given test lottery preferable. goal adaptively select sequence tests present human subject order distinguish theories best explains subject’s responses. employ parameters used generate tests root-causes. particular generated tests. given root-cause test compute values denoted then probability root-cause favors modeled +exp). results fig. demonstrates performance eced data set. average error probability computed across random trials methods. observe eced ec-bayes similar behavior data set; however performance algorithm much worse. explained nature data concentrated distribution therefore since tests provide indirect information uncertainty sampling scheme tries optimize actually hence performs quite poorly. second application considers comparison-based movie recommendation system learns user’s movie preference sequentially showing pairs candidate movies letting choose prefers. movielens dataset consists matrix ratings movies users adopt experimental setup proposed chen particular extract movie features computing low-rank approximation user/rating matrix movielens dataset singular value decomposition simulate target categories user interested partitioning movies clusters euclidean space. root-cause corresponds user’s favorite movie tests given form movie pairs i.e. embeddings movie euclidean space. suppose user’s movie represented test realized closer otherwise. simulate effect noise +exp−d)). distance function control level noise system. results fig. shows performance eced compared baseline methods size compute average error probability across random trials methods. eced consistently outperforms baselines. interestingly ec-bayes performs poorly data set. fact noise level still high misguiding heuristics select noisy uninformative tests. fig. shows performance eced vary tests become close deterministic given root-cause eced able achieve error tests. increase noise rate takes eced many queries prediction error converge. high noise rate eced discounts root-causes uniformly hence hardly informative comes cost performing tests hence convergence rate. active learning statistical learning theory. theoretical active learning literature hanneke balcan urner sample complexity bounds characterized terms structure hypothesis class well additional distribution-dependent complexity measures disagreement coefﬁcient etc); comparison paper seek computationally-efﬁcient approaches provably competitive optimal policy. therefore seek bound optimal policy behaves hence make assumptions hypothesis class. persistent noise non-persistent noise. tests repeated i.i.d. outcomes noisy problem effectively reduced noise-free setting modeling non-persistent noise appropriate settings often important consider setting persistent noise many applications. many applications repeating tests impossible repeating test produces identical outcomes. example could unrealistic replicate medical test practical clinical treatment. despite recent development dealing persistent noise simple graphical models strict noise assumptions general settings focus paper much less understood. introduced eced strictly generalizes algorithm solving practical bayesian active learning experimental design problems correlated noisy tests. proved eced enjoys strong theoretical guarantees introducing analysis framework draws upon adaptive submodularity information theory. demonstrated compelling performance eced problem instances including active preference learning task pairwise comparisons bayesian experimental design task preference elicitation behavioral economics. believe work makes important step towards understanding theoretical aspects complex sequential information gathering problems provides useful insight develop practical algorithms address noise. hanneke steve. bound label complexity agnostic active learning. icml hanneke steve. theory disagreement-based active learning. foundations trends machine learning random variable encoding value target variable domain target variable value random variable encoding root-cause ground domain root-causes root-cause function maps root-cause target value ground tests number tests test random variable encoding test outcome observed test outcome number possible target values supp| number root-causes policy i.e. mapping observation vectors tests random variable encoding partial realization i.e. test-observation pairs partial realization i.e. test-observation pairs observed running policy tolerance prediction error error probability observed partial realization discount coefﬁcient root-cause used eced computing ∆eced. maxe basic component eced gain observing observed offset component eced gain observing observed eced gain myopically optimized iteration eced algorithm suppose observed re-initialize graph total edge weight fecψ. then ∆ecψ expected reduction edge weight performing test discounting edges’ weight according eced. renormalized version ∆eced i.e. ∆ecedψ ∆eced/p expected gain fecψ performing test cutting edges weight according interpreted ∆ecedψ test’s outcome noise-free i.e. auxiliary function deﬁned equation parameter faux lemma used analysis. section provide proofs theoretical results full detail. recall theoretical analysis study basic setting test outcomes binary test noise independent underlying root-causes general idea behind analysis show running eced one-step gain learning value target variable signiﬁcant compared cumulative gain optimal policy steps figure left demonstrate sequential policy form decision tree representation. nodes represent tests selected policy edges represent outcomes tests. step policy maps partial realization xe)} next test performed. middle demonstrate tests selected optimal policy length right illustrate change auxiliary function eced selects tests. running step execution eced make faux threshold idea behind proof show greedy policy eced step making effective progress reducing expected prediction error compared opt. appendix show tests greedily selected optimize expected prediction error failing pick tests negligible immediate gain terms error reduction informative long run. eced bypasses issue selecting tests maximally distinguish root-causes different target values. order analyze eced need auxiliary function properly tracks progress eced algorithm; meanwhile auxiliary function allow connect heuristic select tests target objective interest consider auxiliary function deﬁned equation brevity suppress dependence etc) subsequent subsections appendix readers easily relate different parts section proof outline. note annotated color blocks positioning proofs hence readers ignore notations slightly differ ones used proof. section analyze -step gain auxiliary function test section show lowered bounded one-step gain objective recall assume test outcomes binary analysis following section assume outcome test {+−} instead clarity purposes. notations intermediate goal figure performing binary test dots represent root-causes supp circles represent values target variable favorable outcome root-causes solid dots favorable outcome root-causes hollow dots also illustrate short-hand notations used §b.. positive negative root-causes associated target total probability mass positive negative root-causes probability mass positive negative root-causes associated target αi/α βi/β i.e. root-causes share target value brevity ﬁrst deﬁne short-hand notations simplify derivation. distributions convex combination fact using refer posterior distribution observe outcome binary test refer distribution perform convenience readers summarize notations provided table given root-causes denote values target variable associated root-causes different i.e. rewrite auxiliary function follows ﬁnish proof lemma remains bound ∆aux ∆eced. subsection complete proof lemma showing ∆aux ∆ecedψ recall noise rate test discount factor inconsistent root-causes. deﬁnition ∆eced equation ﬁrst expand expected offset value performing test speciﬁcally provide lemma consider policy length assume using stochastic estimator error probability running policy p⊥enoisy average error probability running noisy setting p⊥enoiseless average error probability running noiseless setting. denotes test-outcome pairs denotes path taken given observes happens noiseless setting exactly compute error probability noiseless setting denote noise tests realized noise imagine noiseless setting following equivalent policy exactly noisy setting. upon completion reveal was. thus essentially lemma implies that terms reduction expected prediction error running policy noise-free setting higher gain running exact policy noisy setting. result important since analyzing policy noise-free setting often easier. going lemma next section relate gain optimal objective gain policy lemma supp| number root-causes number target values optimal policy achieves perr) δopt partial realization observed running eced cost denote aux) eψ)] expected value faux) paths cost assume proof lemma path ending level greedy algorithm. recall denotes gain perform test assuming noiseless conditioning partial observation further recall ∆aux) denotes gain faux perform noisy test observing perform bayesian update root-causes. maxe ∆eced test chosen eced maxe test maximizes lemma know reason step error probability stochastic estimator upon observing i.e. equivalent total amount edge weight i.e. fecψ reason step noiseless setting objective always lower-bound error probability stochastic estimator proof theorem following represent optimal policy achieves prediction error δopt worst-cast cost deﬁne realization seen policy realization slight abuse notation opt. running know lemma expected value faux lower bounded δopt. δopt inequality section provide problem instances and/or informative policy fail eced performs well. since noise-free setting eced equivalent sufﬁces demonstrate limitations informative policy even provide examples apply noise-free setting. suppose want solve problem note noise-free setting problem equivalent minimal cost policy achieves prediction error error probability drops know precisely target value realized. case optimal policy needs select test however select tests order running test true root-cause. given uniform prior takes tests expectation happens pays expectation times optimal expected cost instance. note example eced also selects test optimal. section provide treasure-hunt example informative policy pays times optimal cost. example adapted golovin show informative policy well myopic policy greedily maximizes reduction expected prediction error perform badly compared consider problem instance fig. integer target value exists root-causes i.e. denote root-causes belongs target indexed assume uniform prior root-causes {θio}i∈{...t}o∈. figure problem instance maximal informative policy myopic policy greedily maximizes reduction expected prediction error perform signiﬁcantly worse eced suppose want solve problem similarly problem equivalent minimal cost policy achieves prediction error error probability drops know precisely target value realized. three tests binary outcomes unit cost. ﬁrst contains test tells value underlying root-cause θio. hence second tests designed help quickly discover index target value binary search already offer information whatsoever run. total number tests second es}. least-signiﬁcant k−bk. then outcome test third tests designed allow sequential search index target values. speciﬁcally consider running maximal informative policy note beginning single test results change distribution remains uniform matter test performed. hence maximal informative policy picks tests non-zero expected reduction posterior entropy likely event test chosen index left residual problem tests still effect posterior. difference less class prior remains uniform. hence previous argument still applies repeatedly select tests test outcome expectation cost least cost hand smarter policy select test ﬁrst performs binary search running test determine since tests unit cost cost cost experimental setup. demonstrate empirical performance eced conduct experiments pool-based binary active classiﬁcation tasks. active learning application sequentially query pool data points goal learn binary classiﬁer achieves small prediction error unseen data points pool smallest number queries possible. active learning targets root-causes discretize hypotheses space noisy version hit-and-run sampler suggested chen krause hypothesis represented binary vector indicating outcomes data points training set. then construct epsilon-net hypotheses obtain equivalence classes eced assigning hypothesis closest center epsilon-ball measured hamming distances. note hamming distance hypotheses reﬂects difference prediction error. consider epsilon-net ﬁxed radius construction hypotheses equivalence classes away other; therefore hypotheses within epsilon-ball optimal hypotheses considered near-optimal. using terminology paper hypotheses correspond root-causes groups hypothesis correspond target variable interest. running eced ideally help locate near-optimal epsilon-ball quickly possible. baselines. compare eced popular uncertainty sampling heuristic sequentially queries data points closest decision boundary classiﬁer. also compare algorithm sequentially queries data points maximally reduces volume version space. fig. demonstrate different behaviors plane. simple example color-coded equivalence classes ﬁrst sample hypotheses uniformly within unit circle generate equivalence classes constructing epsilon-net sampled hypotheses previously described. fig. illustrates tests selected eced respectively. eced primarily selects tests best disambiguate clusters focuses disambiguate individual hypotheses. results. evaluate eced baseline algorithms wdbc dataset fourclass dataset eced sample ﬁxed number hypotheses random trial. instances assume constant error rate tests. fig. fig. demonstrate eced competitive baselines. results suggests grouping hypotheses could beneﬁcial learning noisy data.", "year": 2016}