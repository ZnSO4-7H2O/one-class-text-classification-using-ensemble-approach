{"title": "Learning Graph Convolution Filters from Data Manifold", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Convolution Neural Network (CNN) has gained tremendous success in computer vision tasks with its outstanding ability to capture the local latent features. Recently, there has been an increasing interest in extending CNNs to the general spatial domain. Although various types of graph and geometric convolution methods have been proposed, their connections to traditional 2D-convolution are not well-understood. In this paper, we show that depthwise separable convolution is the key to close the gap, based on which we derive a novel Depthwise Separable Graph Convolution that subsumes existing graph convolution methods as special cases of our formulation. Experiments show that the proposed approach consistently outperforms other graph and geometric convolution baselines on benchmark datasets in multiple domains.", "text": "convolution neural network gained tremendous success computer vision tasks outstanding ability capture local latent features. recently increasing interest extending cnns general spatial domain. although various types graph geometric convolution methods proposed connections traditional d-convolution wellunderstood. paper show depthwise separable convolution close based derive novel depthwise separable graph convolution subsumes existing graph convolution methods special cases formulation. experiments show proposed approach consistently outperforms graph geometric convolution baselines benchmark datasets multiple domains. convolution neural network proven efﬁcient model family extracting hierarchical local patterns grid-structured data signiﬁcantly advanced state-of-the-art performance wide range machine learning tasks including image classiﬁcation object detection audio recognition recently growing attention paid dealing data underlying graph/non-euclidean structure prediction tasks sensor networks transportation systems shape correspondence application computation graphics replicate success cnns manifold-structured data remains open challenge. many graph geometric convolution methods proposed recently. spectral convolution methods mainstream algorithm developed graph convolution methods. theory based graph fourier analysis major limitations model knowledge learned different graphs transferrable group approaches geometric convolution methods focuses various ways leverage spatial information nodes. existing models mentioned either capable capturing spatial-wise local information standard convolution tend large parameter space hence prone overﬁtting. result spectral geometric convolution methods produced results comparable cnns related tasks. misalignment makes harder leverage rapidly developing d-convolution techniques generic spatial domain. note graph convolution methods also widely used pure graph structure data like citation networks social networks paper focus data spatial information. paper provide uniﬁed view graph convolution traditional d-convolution methods label propagation process helps better understand compare difference them. based propose novel depthwise separable graph convolution inherits strength depthwise separable convolution extensively used different state-of-the-art image classiﬁcation frameworks including inception network xception network mobilenet compared previous graph geometric methods dsgc expressive aligns closer depthwise separable convolution network shares desirable characteristic small parameter size depthwise separable convolution. experiments section evaluate dsgc baselines three different machine learning tasks. experiment results show performance proposed method close standard convolution network image classiﬁcation task cifar dataset. outperforms previous graph geometric convolution methods tasks. furthermore demonstrate proposed method easily leverage advanced technique developed standard convolution network enhance model performance inception module densenet architecture squeeze-and-excitation block novel depthwise separable graph convolution spatial domain data. demonstrate efﬁciency dsgc extensive experiments show facilitate advanced technique standard convolution network improve model performance. provide uniﬁed view label propagation graph convolution showing different ways aggregate local information graphs data manifolds. discuss connections graph convolution depthwise separable convolution d-grid graph motivates propose formulation subsumes methods special cases. unless otherwise speciﬁed denote matrix i-th matrix element matrix xij. superscripts used distinguish different matrices necessary. operations discussed viewed function transforms input feature maps rn×p output feature maps rn×q number nodes graph number input features associated node respectively. denote neighbors i-th node. symmetrically normalized adjacency matrix ridge diagonal deterministic matrix given input data rp×q represents linear transformation. original form graph convolution spectral network derived graph signal processing generalization fourier analysis domain graphs. several limitations spectral network high computation complexity figure visualization different convolution operations three output channels. different colors represent different ﬁlters deﬁned grid graphs. deﬁned generic graphs. dsgc deﬁned generic graphs. compare former utilizes graphical information latter additional linear transformation intermediate representation matrix additional step makes capable capturing dependencies among features yields performance improvement. full d-convolution layer convolution ﬁlters encode channel correlation spatial correlation simultaneously depthwise separable convolution proposed intuition channel correlation spatial correlation could decoupled found successful several modern architectures image classiﬁcation choose focus strong empirical performance small number parameters intimate connections revealed following. discuss full convolution formulation label propagation process section denotes relative position pixel pixel image viewed lookup table pixel-pixel offset according stationarity assumption convolution. context images denotes index surrounding pixels i-th pixel equivalent k-nearest neighbor euclidean distant metric. example size convolution ﬁlter inﬁnite. hand suffers restriction channels share given graph ﬁlter. heavily constrains model capacity would severe deeper network structure used. context graphs would desirable multiple graph ﬁlters—to capture diverse diffusion patterns graph data manifold convolution ﬁlters image domain. slightly abuse notation overloading function maps real number still represents k-nearest neighbor sets. understand proposed formulation notice different stationarity requirement implemented soft manner deﬁning function instead equality constraints. experiment function parameterized two-layer mlp. different channel-speciﬁc convolution enabled learning multiple graph convolution ﬁlters. amounts simultaneously constructing multiple graphs different node-node similarity metrices metrices implicitly deﬁned neural networks hence jointly optimized training. overﬁtting common issue graph-based applications limited data available. alleviate issue propose option group channels groups channels group would share ﬁlter. context node given generic graph namely connection pattern neighbors non-stationary different parts graph constant d-grid graphs. therefore common practice normalize adjacency matrix order make nodes adaptive contexts natural carry normalization dsgc apply softmax function predicted graph ﬁlter weights node written tmax stands i-th graph ﬁlter learned neural network. empirically normalization leads better performance signiﬁcantly speeds convergence. following experiments proposed depthwise separable graph convolution linear highway bypass basic convolution component imitate rest setting standard convolution neural network solve different machine learning tasks. evaluate proposed depthwise separable graph convolution method representative baselines prediction tasks image classiﬁcation time series forecasting document categorization. algorithms implemented pytorch; data code made publicly accessible controlled experiments graph convolution methods share empirical settings unless otherwise speciﬁed including network structures dimension latent factors appendix contains details. conduct experiments cifar cifar popular benchmark datasets image classiﬁcation. sets contain images pixels cifar category labels cifar category labels. image typically treated grid structure standard image-based convolution. enable comparison generic graphs create modiﬁed versions cifar cifar respectively subsampling pixels graph. illustrated figure subsampling results irregularly scattered nodes image. figure construct subsampled cifar datasets example image cifar dataset. subsampled pixels map. blue points indicate points sampled. image sampling black points sampled out. comparison include traditional convolution graph convolution networks baselines including standard cnn; xception network uses depthwise separable convolution; dcnn method using multi-hops random walk graph ﬁlters; chebynet method using chebyshev polynomial approximate fourier transformation graphs; described section monet method using gaussian function deﬁne propagation weights graphs. fair comparison architecture methods basic platform replace convolution layers according methods. pooling layer performed kmean clustering. centroid clusters regarded node pooling hidden vector mean nodes cluster based pooling method. notice that normalize input signals preprocessing data augmentation. experiment results summarized table firstly observe xception best results; surprising methods grid-based convolution naturally suitable image recognition. secondly dsgc outperforms graph-based convolution methods performance close grid-based convolution methods. furthermore contributed depthwise separable convolution sharing graph technique model achieve competitive performance without increasing number parameters smallest number parameters among graph convolution approaches. another important application domain interested effectively utilize locality information sensor networks time series forecasting. example incorporate longitudes/latitudes sensors w.r.t. temporal cloud movement important question spatiotemporal modeling predicting output solar energy farms united states. appendix provides formal deﬁnition task. historical climatology network dataset contains daily climatological data meteorology sensors years sequence length includes subsets climate variable maximum temperature minimum temperature precipitation snowfall snow depth. daily maximum temperature data precipitation data refer ushcn-tmax ushcn-prcp sets respectively. graph convolution methods previous section included form baselines comparison. also traditional methods time series forecasting autoregressive model predicts future signal using window historical data based linear assumption temporal dependencies vector autoregressive model extends multivariate version namely input signals sensors history window lstnet deep neural network model combines strengths none methods capable leveraging locational dependencies graph convolution. exclude xception methods d-grid based convolution could generalized irregular graphs focus here. table summarizes evaluation results methods performance measured using root square mean error best result dataset highlighted boldface. ﬁrst chunk three methods leverage spatial locational information data. second chuck consists neural network models leverage spatial information sensor networks. graph convolution methods second chunk clearly outperforms methods ﬁrst chunk explicitly model spacial correlation within sensor networks. overall proposed method best performance datasets demonstrating strength capturing informative local propagation patterns temporally specially. application text categorization news dataset experiments. consists text documents associated topic labels. individual words document vocabulary nodes graph convolution. node also word embedding vector learned running wordvec algorithm corpus. following experiment settings defferrard select frequent words nodes. table summarizes results graph convolution methods plus three popular traditional classiﬁers dsgc best result dataset. notice traditional classiﬁers trained tested feature words setting graph convolution models. words used traditional classiﬁers would higher performance. proposed convolution method considered equivalent component depthwise separable convolution method. naturally leverage technique developed standard convolution network improve dsgc framework. hence examine dsgc following techniques popular recent years standard convolution images inception module densenet framework squeeze-and-excitation block details architectures included appendix results presented table clearly combined advantageous techniques/architectures performance dsgc image classiﬁcationcan improved. demonstrates dsgc easily enjoy beneﬁt traditional d-convolution network development. section summarize graph convolution methods proposed recent years label propagation process reveals difference traditional d-convolution them. firstly provide formulation full convolution rn×n contains eigenvectors laplacian matrix graph diagonal matrix learned supervision data. spectral network matched full convolution different ﬁlter subspace words different basic ﬁlters. however suffers several limitations. needs conduct eigenvector decomposition laplacian matrix expensive operation. ﬁlters localized spatial domain. number parameters grows linearly number nodes graph. order address previous problems researchers chebyshev polynomial approximate non-parameter ﬁlter referred chebynet written k-th order chebyshev polynomial term. chebynet considered integration depthwise separable convolution components layer. still suffers similar limitation using graph ﬁlter channels graph ﬁlter constant given input. model capacity still cannot compare depthwise separable convolution. larger chebynet approximate non-parameter ﬁlers spectral network. however would require large number parameters face similar limitation spectral network. besides graph convolution methods researchers propose another type models geometric convolution methods deal data general spatial domain. here introduce advanced monet framework also related paper. updating formula monet label propagation process proposed method. uses graph ﬁlter channels. order capture complex propagation patterns layer model requires larger leads much larger number parameters. ﬁnally experiment results show proposed method consistently outperforms monet less parameters multiple tasks. paper propose novel depthwise separable graph convolution network explicitly generalized depthwise separable convolution goes beyond general graph space. extensive experiments multi-ﬁeld benchmark datasets demonstrate method outperform strong baseline methods relatively small number model parameters easily extended leverage advanced techniques/architectures standard convolution networks improvement performance. future work want explore impact broader range applications social networks molecular structures leveraging technical improvements node/edge embedding based graph structure information. davide boscaini jonathan masci emanuele rodol`a michael bronstein. learning shape correspondence anisotropic convolutional neural networks. advances neural information processing systems michael bronstein joan bruna yann lecun arthur szlam pierre vandergheynst. geometric deep learning going beyond euclidean data. ieee signal processing magazine micha¨el defferrard xavier bresson pierre vandergheynst. convolutional neural networks graphs fast localized spectral ﬁltering. advances neural information processing systems andrew howard menglong chen dmitry kalenichenko weijun wang tobias weyand marco andreetto hartwig adam. mobilenets efﬁcient convolutional neural networks mobile vision applications. arxiv preprint arxiv. jonathan masci davide boscaini michael bronstein pierre vandergheynst. geodesic convolutional neural networks riemannian manifolds. proceedings ieee international conference computer vision workshops tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionality. advances neural information processing systems federico monti davide boscaini jonathan masci emanuele rodol`a svoboda michael bronstein. geometric deep learning graphs manifolds using mixture model cnns. arxiv preprint arxiv. david shuman sunil narang pascal frossard antonio ortega pierre vandergheynst. emerging ﬁeld signal processing graphs extending high-dimensional data analysis networks irregular domains. ieee signal processing magazine christian szegedy vincent vanhoucke sergey ioffe shlens zbigniew wojna. rethinking inception architecture computer vision. proceedings ieee conference computer vision pattern recognition xingjian zhourong chen wang dit-yan yeung wai-kin wong wang-chun woo. convolutional lstm network machine learning approach precipitation nowcasting. advances neural information processing systems xiaojin zoubin ghahramani john lafferty. semi-supervised learning using gaussian ﬁelds harmonic functions. proceedings international conference machine learning section conduct experiment cifar cifar datasets. introduce architecture settings dsgc baseline models. table illustrates basic architecture used experiment. dsgc-vgg dsgc-densenet models k-conv refers spatial convolution k-nearest neighbors neighbor setting. -conv conv linear transformation channels. hidden dimensions dsgc-vgg growth rate dsgc-densenet baseline graph geometric convolution methods identical architecture dsgc-vgg. subsampled cifar experiment eliminate ﬁrst convolution transition pooling layer change spatial convolution -conv {-conv -conv -conv -conv}. dsgc-se follow method described block dsgc-vgg architecture. dropout scheme described huang dsgc-densenet model dropout layer pooling layer dsgc-vgg models. dsgcinception model imitate design inception network model still format input signal matrix shape. signals invalid points furthermore perform fair comparison standard subsampled situation append mask matrix additional channel input signals indicate whether pixel valid not. monet also apply softmax trick described section accelerates training process improves ﬁnal result. chebynet polynomial order used dsgc monet dimension feature vector. denote coordinate i-th node xjyij yjdij xij)|xij| signyij)|yij|dij). learning schedule applied models. train model epochs. initial learning rate divided total number training epochs. firstly give formal deﬁnition time series forecasting spatiotemporal regression problem. formulate spatiotemporal regression problem multivariate time series forecasting task sensors’ location input. formally given series time series signals observed sensors y··· number sensors locations sensors l··· indicates coordinate sensor task predict series future signals rolling forecasting fashion. said predict desirable horizon ahead current time stamp assume y··· available. likewise predict signal next time stamp assume y··· available. paper follow setting autoregressive model. deﬁne window size hyper-parameter ﬁrstly. model input time stamp {yt−p+··· rn×p. experiments paper horizon always intuitively different sensors node-level hidden features inﬂuence propagation patterns ﬁnal outputs. node model learns node embedding vector concatenate input signals. using trick node limited freedom interface propagation patterns. trick proven useful task ushcn-prcp solar speciﬁcally. embedding size datasets. thing readers notice data ushcn dataset missing. deal that additional feature channel indicate point missing. time series models tune historical window according validation set. rest models window size solar dataset ushcn datasets. network architecture used task convolution layers followed regression layer. setting previous one. adam optimizer task train model epochs learning rate data preprocessing follows experiment details defferrard network architecture models convolution layers followed layers classiﬁer. convolution layer dropout layer performed dropout rate nodes’ coordinate word embedding method calculate similar previous ones. optimizer used task cifar experiment.", "year": 2017}