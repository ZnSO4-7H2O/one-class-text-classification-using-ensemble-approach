{"title": "Addressing the Rare Word Problem in Neural Machine Translation", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMT14 contest task.", "text": "network reads entire source sentence produces output translation word time. systems appealing minimal domain knowledge makes well-suited problem formulated mapping input sequence output sequence addition natural ability neural networks generalize implies systems also generalize novel word phrases sentences occur training set. addition systems potentially remove need store explicit phrase tables language models used conventional systems. finally decoder system easy implement unlike highly intricate decoders used phrase-based systems despite advantages conventional systems incapable translating rare words ﬁxed modest-sized vocabulary forces symbol large number out-ofvocabulary words illustrated figure unsurprisingly sutskever bahdanau observed sentences many rare words tend translated much poorly sentences containing mainly frequent words. standard phrasebased systems hand suffer rare word problem extent support much larger vocabulary explicit alignments phrase tables allows memorize translations even extremely rare computationally intensive nature softmax systems often limit vocabularies frequent words language. however jean recently proposed efﬁcient approximation softmax allows training ntms large vocabularies. discussed section technique complementary ours. neural machine translation approach machine translation shown promising results comparable traditional approaches. signiﬁcant weakness conventional systems inability correctly translate rare words end-to-end nmts tend relatively small vocabularies single symbol represents every possible out-of-vocabulary word. paper propose implement effective technique address problem. train system data augmented output word alignment algorithm allowing system emit word target sentence position corresponding word source sentence. information later utilized post-processing step translates every word using dictionary. experiments wmt’ english french translation task show method provides substantial improvement bleu points equivalent system technique. bleu points system ﬁrst surpass best result achieved wmt’ contest task. neural machine translation novel approach achieved promising human translation french translation produced neural network systems handling words. highlight words unknown model. token indicates word. also show important alignments pair sentences. motivated strengths standard phrasebased system propose implement novel approach address rare word problem nmts. approach annotates training corpus explicit alignment information enables system emit word pointer corresponding word source sentence. information later utilized post-processing step translates words using dictionary identity translation translation found. experiments conﬁrm approach effective. english french wmt’ translation task approach provides improvement bleu points equivalent system technique. moreover system ﬁrst outperforms winner wmt’ task. neural machine translation system neural network maps source sentence target sentence sentences assumed terminate special end-of-sentence token <eos>. concretely system uses neural network parameterize conditional distributions many ways parameterize conditional example kalchbrenner blunsom used combination convolutional neural network recurrent neural network sutskever used deep long short-term memory model used architecture similar lstm bahdanau used elaborate neural network architecture uses attentional mechanism input sequence similar graves graves sutskever uses deep lstm encode input sequence separate deep lstm output translation. encoder reads source sentence word time produces large vector represents entire source sentence. decoder initialized vector generates translation word time emits end-of-sentence symbol <eos>. none early work neural machine translation systems addressed rare word problem recent work jean tackled efﬁcient approximation softmax accommodate large vocabulary however even large vocabulary problem rare words e.g. names numbers etc. still persists jean found using techniques similar beneﬁcial complementary approach. despite relatively large amount work done pure neural machine translation systems work addressing problem systems notable exception jean work mentioned earlier. propose address rare word problem training system track origins unknown words target sentences. knew source word responsible unknown target word could introduce postprocessing step would replace system’s output translation source word using either dictionary identity translation. model knows second unknown token originates source word ecotax perform word dictionary lookup replace unknown token ´ecotaxe. similarly identity translation source word pont-de-buis applied third unknown token. strategies applied system treat system black train corpus annotated models below. first alignments produced unsupervised aligner. next alignment links construct word dictionary used word translations post-processing step. word appear dictionary apply identity translation. approach introduce multiple tokens represent various unknown words source target language opposed using token. annotate words source sentence order assigning repeating unknown words identical tokens. annotation when source word multiple translations translation highest probability. translation probabilities estimated unsupervised alignment links. constructing dictionary alignment links word pair dictionary alignment count exceeds unknown words target language slightly elaborate unknown target word aligned unknown source word assigned unknown token unknown target word alignment aligned known word uses special null token unk∅. figure example. annotation enables translate every non-null unknown token. copyable model limited inability translate unknown target words aligned known words source sentence pair words portico portique running example. former word known source sentence; whereas latter labelled unk∅. happens often since source vocabularies models tend much larger target vocabulary since large source vocabulary cheap. limitation motivated develop annotation model includes complete alignments source target sentences straightforward obtain since complete alignments available training time. speciﬁcally return using single universal token. however target side insert positional token every word. here indicates relative position denote target word position aligned source word position aligned words apart considered unaligned unaligned words annotated null token annotation illustrated figure main weakness posall model doubles length target sentence. makes learning difﬁcult slows speed parameter updates factor two. however given post-processing step concerned alignments unknown words sensible annotate unknown words. motivates positional unknown model uses unkposd tokens simultaneously denote fact word unknown relative position respect aligned source word. like posall model symbol unkpos∅ unknown target words alignment. universal unknown tokens source language. figure annotated example. comparable results reported previous work neural machine translation systems train models training data parallel sentences obtained subset selected full wmt’ parallel corpora using method proposed axelrod computationally intensive nature naive softmax limit french vocabulary either frequent french words. source side afford much larger vocabulary frequent english words. model treats words unknowns. annotate training data using three schemes described previous section. alignment computed berkeley aligner using default settings. discard sentence pairs source target sentence exceed tokens. hyperparameused choices sutskever details train multi-layer deep lstms cells dimensional embeddings. like sutskever reverse words source sentences shown improve lstm memory utilization results better translations long sentences. hyperparameters summarized follows parameters initialized uniformly -layer models layer models ﬁxed learning rate train epochs size mini-batch rescale normalized gradient ensure norm exceed also follow parallelization scheme proposed allowing reach training speed words second train depth- model source target vocabularies whereas sutskever achieved words second depth- models source target vocabularies. training takes days -gpu machine. report bleu scores based both detokenized translations i.e. wmt’ style comparable results reported website tokenized translations consistent previous work single lstm layers single lstm layers posunk single lstm layers single lstm layers posunk ensemble lstms ensemble lstms posunk single lstm layers single lstm layers posunk ensemble lstms ensemble lstms posunk table tokenized bleu newstest translation results various systems differ terms architecture size vocabulary used training corpus either using full wmt’ corpus sentence pairs subset pairs. highlight performance best system bolded text state improvements obtained technique handling rare words notice that given vocabulary size accurate systems achieve greater improvement post-processing step. case accurate models able pin-point origin unknown word greater accuracy making post-processing useful. translation quality individual lstm models ensemble models. k-word vocabularies performance gains range bleu points. larger vocabularies performance gains diminished technique still provide nontrivial gains bleu points. interesting observe approach useful ensemble models compared individual ones. usefulness posunk model directly depends ability correctly locate given target word corresponding word source sentence. ensemble large models identiﬁes source words greater accu k-vocabulary ensemble combine models layers models layers. kvocabulary ensemble combine models layers models layers. depth- models regularized dropout similar zaremba dropout probability figure rare word translation x-axis order newstest sentences average frequency rank divide sentences groups sentences comparable prevalence rare words. compute bleu score group independently. system purple curve. produces better translations sentences frequent words worse best system sentences many rare words applying unknown word translation technique signiﬁcantly improve translation quality last group sentences greatest proportion words test increase bleu score system bleu points. overall rare word translation model interpolates sota system system sutskever allows outperform winning entry wmt’ sentences consist predominantly frequent words approach performance sentences many words. examine effect different rare word models presented section namely copyable aligns unknown words input target side learning copy indices positional predicts aligned source positions every target word positional unknown predicts aligned source positions unknown target words. also interestracy. vocabulary size better models obtain greater performance gain post-processing step. except recent work jean employs similar unknown treatment strategy ours best result bleu outperforms systems arge margin importanly system established record wmt’ english french translation. analyze quantify improvement obtained rare word translation approach provide detailed comparison different rare word techniques proposed section also examine effect depth lstm architectures demonstrate strong correlation perplexities bleu scores. also highlight translation examples models succeed correctly translating words present several failures. analyze rare words translation quality follow sutskever sort sentences newstest average inverse frequency words. split test sentences groups sentences within group comparable number rare words evaluate group independently. evaluate systems translating words compare standard systems best system wmt’ contest neural systems ensemble systems described section rare word translation challenging neural machine translation systems shown figure speciﬁcally translation quality model applying postprocessing step shown green curve current best their unknown replacement method track locations target unknown words word dictionary post-process translation. however mechanism used achieve tracking behavior different. jean uses attentional mechanism track origins target words unknown ones. contrast focus tracking unknown words using unsupervised alignments. method easily applied sequence-to-sequence models since treat model blackbox manipulate input output levels. align unknown target words source word result post-processing much stronger effect. posunk model achieves better translation results posall model suggests easier train lstm shorter sequences. deep lstm architecture compare posunk models trained different number layers observe gain obtained posunk model increases tandem overall accuracy model consistent idea larger models point appropriate source word accurately. additionally observe average extra lstm layer provides roughly bleu point improvement demonstrated figure figure rare word models translation performance -layer lstms model uses alignment rare word models model show results rare word translation well perplexity posall report perplexities predicting words positions. measure improvement obtained alignment information used training. such include baseline model alignment knowledge simply assume unknown word target sentence aligned unknown word source sentence. results figure simple monotone alignment assumption noalign model yields modest gain bleu points. train model predict alignment copyable model offers slightly better gain bleu. note however english french similar word order structure would interesting experiment language pairs english chinese word order monotonic. harder language pairs potentially imply smaller gain noalign model larger gain copyable model. leave future work. positional models improve translation performance bleu points. proves limitation copyable model forces align unknown output word unknown input word contrast positional modconsiderable. parameters initialize parameters uniformly learning rate maximal gradient norm source vocabulary words target vocabulary lstms achieve best possible performance still useful analyze them. cataracte permettront r´esorber arri´er´e op´erations suppl´ementaires notamment dans domaine chirurgie orthop´edique cataracte aideront rattraper retard trader richard usher left understand given leave current position european head forex spot trading jpmorgan +unk n´egociateur richard usher quitt´e compris autoris´e quitter poste actuel tant leader europ´een march´e points vente jpmorgan trader richard usher quitt´e aurait ´et´e suspendu poste responsable europ´een trading comptant pour devises chez jpmorgan concerns grown mazanga quoted saying renamo abandoning peace accord table sample translations table shows source translations best model unknown word translations. also show human translations italicize words involved unknown word translation process. performance -layer lstm compute perplexity bleu scores different points training. average reduction perplexity gives roughly bleu point improvement. present three sample translations best system table ﬁrst example model translates unknown words correctly orthop´ediques cataracte. interesting observe model accurately predict alignment distances words. second example highlights fact model translate long sentences reasonably well able correctly translate unknown word jpmorgan source sentence. lastly examples also reveal several penalties incurred model incorrect entries word dictionary n´egociateur trader second example incorrect alignment prediction unkpos incorrectly aligned source word abandoning resulted incorrect translation third sentence. shown simple alignment-based technique mitigate even overcome main weaknesses current systems inability translate words vocabulary. advantage technique fact applicable philipp koehn hieu hoang alexandra birch chris callison-burch marcello federico nicola bertoldi brooke cowan wade shen christine moran richard zens moses open source toolkit statistical machine translation. demonstration session. demonstrated empirically wmt’ english-french translation task technique yields consistent substantial improvement bleu points various systems different architectures. importantly bleu points established ﬁrst system outperformed best system wmt’ contest dataset. thank members google brain team thoughtful discussions insights. ﬁrst author especially thanks chris manning stanford group helpful comments early drafts paper. lastly thank annonymous reviewers valuable feedback.", "year": 2014}