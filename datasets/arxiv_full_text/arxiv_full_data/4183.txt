{"title": "Asymmetric Tri-training for Unsupervised Domain Adaptation", "tag": ["cs.CV", "cs.AI"], "abstract": "Deep-layered models trained on a large number of labeled samples boost the accuracy of many tasks. It is important to apply such models to different domains because collecting many labeled samples in various domains is expensive. In unsupervised domain adaptation, one needs to train a classifier that works well on a target domain when provided with labeled source samples and unlabeled target samples. Although many methods aim to match the distributions of source and target samples, simply matching the distribution cannot ensure accuracy on the target domain. To learn discriminative representations for the target domain, we assume that artificially labeling target samples can result in a good representation. Tri-training leverages three classifiers equally to give pseudo-labels to unlabeled samples, but the method does not assume labeling samples generated from a different domain.In this paper, we propose an asymmetric tri-training method for unsupervised domain adaptation, where we assign pseudo-labels to unlabeled samples and train neural networks as if they are true labels. In our work, we use three networks asymmetrically. By asymmetric, we mean that two networks are used to label unlabeled target samples and one network is trained by the samples to obtain target-discriminative representations. We evaluate our method on digit recognition and sentiment analysis datasets. Our proposed method achieves state-of-the-art performance on the benchmark digit recognition datasets of domain adaptation.", "text": "recognition abilities images languages improved dramatically. training deep-layered networks large number labeled samples enables correctly categorize samples diverse domains. addition transfer learning utilized many studies. object detection segmentation transfer knowledge trained large-scale dataset ﬁne-tuning relatively small dataset moreover features trained imagenet useful multimodal learning tasks including image captioning visual question answering problems neural networks although perform well samples generated distribution training samples difﬁcult correctly recognize samples different distributions test time. example images collected internet come abundance fully labeled. distribution different images taken camera. thus classiﬁer performs well various domains important practical use. realize this necessary learn domain-invariantly discriminative representations. however acquiring representations easy often difﬁcult collect large number labeled samples samples different domains domain-speciﬁc characteristics. unsupervised domain adaptation train classiﬁer works well target domain condition provided labeled source samples unlabeled target samples training. previous deep domain adaptation methods proposed mainly assumption adaptation realized matching distribution features different domains. methods aimed obtain domain-invariant features minimizing divergence domains well category loss source domain however shown theoretically classiﬁer works well source target domains exist canexpect discriminative classiﬁer target domain. even distributions matched nondiscriminative representations classiﬁer work deep-layered models trained large number labeled samples boost accuracy many tasks. important apply models different domains collecting many labeled samples various domains expensive. unsupervised domain adaptation needs train classiﬁer works well target domain provided labeled source samples unlabeled target samples. although many methods match distributions source target samples simply matching distribution cannot ensure accuracy target domain. learn discriminative representations target domain assume artiﬁcially labeling target samples result good representation. tri-training leverages three classiﬁers equally give pseudo-labels unlabeled samples method assume labeling samples generated different domain. paper propose asymmetric tri-training method unsupervised domain adaptation assign pseudo-labels unlabeled samples train neural networks true labels. work three networks asymmetrically. asymmetric mean networks used label unlabeled target samples network trained samples obtain targetdiscriminative representations. evaluate method digit recognition sentiment analysis datasets. proposed method achieves state-of-the-art performance benchmark digit recognition datasets domain adaptation. correspondence kuniaki saito <k-saitomi.t.u-tokyo.ac.jp> yoshitaka ushiku <ushikumi.t.u-tokyo.ac.jp> tatsuya harada <haradami.t.u-tokyo.ac.jp>. well target domain. since directly learning discriminative representations target domain absence target labels considered difﬁcult propose assign pseudo-labels target samples train targetspeciﬁc networks true labels. co-training tri-training leverage multiple classiﬁers artiﬁcially label unlabeled samples retrain classiﬁers. however methods assume labeling samples different domains. since goal classify unlabeled target samples different characteristics labeled source samples propose asymmetric tri-training unsupervised domain adaptation. asymmetric mean assign different roles three classiﬁers. paper propose novel tri-training method unsupervised domain adaptation assign pseudolabels unlabeled samples train neural networks utilizing samples. described fig. networks used label unlabeled target samples remaining network trained pseudo-labeled target samples. method need special implementations. evaluate method digit classiﬁcation task trafﬁc sign classiﬁcation task sentiment analysis task using amazon review dataset demonstrate state-of-the-art performance nearly experiments. particular adaptation scenario mnist→svhn method outperformed methods number previous methods attempted realize adaptation utilizing measurement divergence different domains methods based theory proposed states expected loss target domain bounded three terms expected loss source domain; domain divergence source target; minimum value shared expected loss. shared expected loss means loss source target domain. third term usually considered cannot evaluated labeled target samples absent methods minimize ﬁrst term second term. regards training deep architectures maximum mean discrepancy loss domain classiﬁer network utilized measure divergence corresponding second term however third term important training simultaneously extract representations recognize them. third term easily large representations discriminative target domain. therefore focus learn target-discriminative representations considering third term. focus point stated target-speciﬁc classiﬁer constructed using residual network structure. different method constructed target-speciﬁc network providing artiﬁcially labeled target samples. several transductive methods similarity features provide labels unlabeled samples unsupervised domain adaptation method proposed learn labeling metrics using k-nearest neighbors unlabeled target samples labeled source samples. contrast method method explicitly simply backpropagates category loss target samples based pseudo-labeled samples. approach require special modules. many methods proposed give pseudo-labels unlabeled samples utilizing predictions classiﬁer retraining including pseudo-labeled samples called self-training. underlying assumption selftraining one’s high-conﬁdence predictions correct predictions mostly correct utilizing samples high conﬁdence improve performance classiﬁer. co-training utilizes classiﬁers different views sample provide pseudo-labels then unlabeled samples added training least classiﬁer conﬁdent predictions. generalization ability co-training theoretically ensured assumptions applied various tasks idea co-training incorporated domain adaptation. tri-training regarded extension co-training similar co-training tritraining uses output three different classiﬁers give pseudo-labels unlabeled samples. tri-training require partitioning features different views; instead tri-training initializes classiﬁer differently. however label target samples high accuracy expect classify samples based different viewpoints. therefore make constraint weight make inputs different other. cost function determ note fully connected layers’ weights ﬁrst applied feature network learn different features constraint. objective learning deﬁned pseudo-labeled target targetdiscriminative information network. however since certainly contain false labels pick reliable pseudo-labels. labeling learning method aimed realizing this. entire procedure training network shown algorithm first train entire network source training optimized trained standard category loss. training provide pseudo-labels predictions namely obtained denote class maximum predicted probability assign pseudo-label following conditions satisﬁed. first require give pseudo-labels means different classiﬁers agree prediction. second requirement maximizing probability exceeds threshold parameter experiment. suppose unless classiﬁers conﬁdent prediction prediction reliable. figure proposed method includes shared feature extractor classiﬁers labeled samples learn labeled source samples newly labeled target samples. addition target-speciﬁc classiﬁer learns pseudo-labeled target samples. method ﬁrst trains networks labeled source samples labels target samples based output train architectures using correctly labeled samples. tri-training assume unlabeled samples follow different distributions ones labeled samples generated from. therefore develop tritraining method suitable domain adaptation using three classiﬁers asymmetrically. effect pseudo-labels neural network investigated. argued effect training classiﬁer pseudo-labels equivalent entropy regularization thus leading low-density separation between classes. addition experiment observe target samples separated hidden features. section provide details proposed model domain adaptation. construct targetspeciﬁc network utilizing pseudo-labeled target samples. simultaneously expect labeling networks acquire target-discriminative representations gradually increase accuracy target domain. show proposed network structure fig. denotes network outputs shared features among three networks classify features generated predictions utilized give pseudo-labels. classiﬁer classiﬁes features generated target-speciﬁc network. learn source pseudo-labeled target samples learns pseudo-labeled target samples. shared network learns gradients without shared network another option network architecture think training three networks separately inefﬁcient terms training implementation. furthermore building shared network also harness target-discriminative represenalgorithm iter denotes iteration training. function labeling means method labeling. assign pseudo-labels samples predictions agree least conﬁdent predictions. number initial candidates ninit gradually increase number candidates denotes number target samples denotes number steps maximum number pseudo-labeled candidates pseudo-labeled training composed updated objective labeled training then simply optimized category loss discriminative representations learned constructing target-speciﬁc network trained target samples. however noisy pseudo-labeled samples used training network learn useful representations. then source samples pseudo-labeled samples training ensure accuracy. also learning proceeds learn target-discriminative representations resulting improvement accuracy cycle gradually enhance accuracy target domain. batch normalization whitens output hidden layer effective technique accelerate training speed enhance accuracy model. addition domain adaptation whitening hidden layer’s output effective improving performance make distribution input samples include pseudo-labeled target samples source samples. introducing useful matching distribution improves performance. layer last layer equation introduced showing upper bound expected error target domain depends three terms include divergence different domains error ideal joint hypothesis. divergence source target domain h∆h-distance deﬁned follows theorem means expected error target domain upper bounded three terms expected error source domain domain divergence measured disagreement hypothesis error ideal joint hypothesis. existing work disregarded considered negligibly small. provided ﬁxed features need consider term term also ﬁxed. however assume obtained last fully connected layer deep models note determined output layer note necessity considering term. show simple derivation inequality supplementary material. theorem cannot measure absence labeled target samples. approximately evaluate minimize using pseudo-labels. furthermore consider second term right-hand side method expected reduce term. term intuitively denotes discrepancy different domains disagreement classiﬁers. regard certain respectively training baseline methods compare method methods unsupervised domain adaptation including state-of-the methods visual domain adaptation; maximum mean discrepancy domain adversarial neural network deep reconstruction classiﬁcation network domain separation networks k-nearest neighbor based adaptation cite results addition compare method trained source samples. compare method variational fair autoencoder dann amazon reviews experiment. visual domain adaptation visual domain adaptation perform evaluation digits digits datasets datasets trafﬁc signs datasets. include mnist mnist-m street view house numbers synthetic digits evaluate method trafﬁc sign datasets including synthetic trafﬁc signs german trafﬁc signs recognition benchmark total adaptation scenarios evaluated experiment. datasets used evaluation varied previous works extensively evaluate method scenarios. evaluate method ofﬁce commonly used dataset visual domain adaptation. pointed labels dataset noisy images contain classes’ objects. furthermore many previous studies evaluated ﬁne-tuning pretrained networks using imagenet. protocol assumes existence another source domain. work want evaluate situation access source domain target domain. adaptation amazon reviews investigate behavior language datasets also evaluated method amazon reviews dataset preprocessing used dataset contains reviews four experiments image datasets employ architecture used fair comparison separate network hidden layer constructed discriminator networks. therefore considering classiﬁer example architecture identical previous work. also follow protocols. threshold value labeling method mnist→svhn. scenarios momentumsgd optimization momentum learning rate determined validation splits uses either scenarios. supplementary material provide details network architecture hyper-parameters. experiments amazon reviews dataset similar architecture used sigmoid activated dense hidden layer hidden units softmax output. extend architecture method similarly architecture cnn. based validation. since input sparse adagrad optimization. repeat evaluation times report mean accuracy. tables show main results experiments. training source samples effect clear tables however image recognition experiments effect method clear; time effect method also clear network architecture. effect weight constraint obvious mnist→svhn. table results visual domain adaptation experiment digits trafﬁc signs dataset. every setting method outperforms method large margin. source results show results reported parentheses. figure conﬁrm effect method visualization learned representations using t-distributed stochastic neighbor embedding points target samples blue points source samples. samples testing samples. case source samples training. case adaptation method. scenarios mnist→svhn mnist→mnist-m target samples dispersed adaptation. mnist→mnist-m first evaluate adaptation scenario hand-written digits dataset mnist transformed dataset mnist-m. mnist-m composed merging clip background bsds datasets patch randomly taken images bsds merged mnist digits. even simple domain shift adaptation performance much worse case trained target samples. target training samples randomly select labeled target samples validation split tuned hyper-parameters. method outperforms existing method visualization features last pooling layer shown fig. observe target samples dispersed adaptation achieved. show comparison accuracy actual labeling accuracy target samples training test accuracy fig. test accuracy ﬁrst steps increase accuracy becomes closer labeling accuracy. adaptation clearly actual labeling accuracy gradually improves accuracy network. svhn↔mnist increase distributions experiment. evaluate adaptation svhn mnist ten-class classiﬁcation problem. svhn mnist distinct appearance thus adaptation challenging scenario especially mnist→svhn. svhn colored images contain multiple digits. therefore classiﬁer trained svhn expected perform well mnist reverse true. mnist include samples containing multiple digits samples figure comparison actual accuracy pseudo-labels learned network accuracy training. blue curve pseudo-label accuracy curve learned network accuracy. note labeling accuracy computed using green curve number labeled target samples step. comparison accuracy three networks model. three networks almost simultaneously improve accuracy. comparison a-distance different methods. model slightly reduced divergence domain compared source-only trained cnn. evaluate method adaptation scenarios achieved state-of-the-art performance datasets. particular adaptation mnist→svhn outperformed methods fig. visualize representations mnist→svhn. although distributions seem separated domains svhn samples become discriminative using method compared non-adapted embedding. also show comparison actual labeling method accuracy testing accuracy fig. ﬁgure labeling accuracy rapidly drops initial adaptation stage. hand testing accuracy continues improve ﬁnally exceeds labeling accuracy. questions interesting phenomenon. ﬁrst question labeling method continue decrease despite increase test accuracy? target samples given pseudo-labels always include mistakenly labeled samples whereas given labels ignored method. therefore error reinforced target samples included training set. second question test accuracy continue increase despite lower labeling accuracy? assumed reasons network already acquires target discriminative representations fig. also show comparison accuracy three networks svhn→mnist. accuracy three networks nearly every step. thing observed scenarios. result state target-discriminative representations shared three networks. digits→svhn experiment aimed address common adaptation scenario synthetic images real images. datasets synthetic numbers consist images generated windows fonts varying text positioning orientation background stroke colors amount blur. source samples target samples training target samples testing. svhn samples validation set. method also outperforms methods experiment. experiment effect clear compared scenarios. domain considered small scenario performance source-only classiﬁer shows. fig. although labeling accuracy dropping accuracy learned network’s prediction improving mnist↔svhn. table results gradient stop experiment. stopping gradients backward gradients learns stopping gradients backward gradients learns none denotes proposed method backward gradients branches three adaptation scenarios method shows stable performance. vious setting adaptation synthetic images real images larger number classes namely classes instead signs dataset source dataset gtsrb dataset target dataset consist real trafﬁc sign images. select randomly samples target training samples evaluate accuracy rest samples. total labeled target samples used validation. scenario method outperforms methods. result shows method effective adaptation synthesized images real images fig. tendency diverse classes. mnist↔svhn observed adaptation scenario. gradient stop experiment evaluate effect target-speciﬁc network method. stop gradient upper layer networks examine effect table shows three scenarios including case stop gradient scenarios backward gradients obtain clear performance improvements. experiment mnist→mnist-m assume backpropagation cannot construct discriminative representations target samples conﬁrm effect adaptation mnist→svhn best performance realized receives gradients upper networks. backwarding gradients ensure target-speciﬁc discriminative representations difﬁcult adaptations. signs→gtsrb backwarding produces worst performance domains similar noisy pseudo-labeled target samples worsen performance. a-distance a-distance usually used measure domain discrepancy. estimating empirical a-distance simple train classiﬁer classify domain domains’ feature. then approximate distance calculated generalization error fig. show a-distance classiﬁer. calculated features. used linear calculate distance. graph method certainly reduces a-distance compared trained source samples. addition comparing dann method although dann reduces a-distance much method method shows superior performance. indicates minimizing domain discrepancy necessarily appropriate achieve better performance. amazon reviews reviews encoded dimensional vectors bag-of-words unigrams bigrams binary labels. negative labels attached samples ranked stars. positive labels attached ranked stars. labeled source samples unlabeled target samples training samples testing. labeled target samples validation. paper proposed novel asymmetric tri-training method unsupervised domain adaptation simply implemented. aimed learn discriminative representations utilizing pseudo-labels assigned unlabeled target samples. utilized three classiﬁers networks assign pseudo-labels unlabeled target samples remaining network learns them. evaluated method domain adaptation visual recognition task sentiment analysis task outperforming methods. particular method outperformed methods mnist→svhn adaptation task.", "year": 2017}