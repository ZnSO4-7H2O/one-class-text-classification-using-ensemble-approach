{"title": "Deep Self-Taught Learning for Handwritten Character Recognition", "tag": ["cs.LG", "cs.CV", "cs.NE", "68T05", "I.2.6"], "abstract": "Recent theoretical and empirical work in statistical machine learning has demonstrated the importance of learning algorithms for deep architectures, i.e., function classes obtained by composing multiple non-linear transformations. Self-taught learning (exploiting unlabeled examples or examples from other distributions) has already been applied to deep learners, but mostly to show the advantage of unlabeled examples. Here we explore the advantage brought by {\\em out-of-distribution examples}. For this purpose we developed a powerful generator of stochastic variations and noise processes for character images, including not only affine transformations but also slant, local elastic deformations, changes in thickness, background images, grey level changes, contrast, occlusion, and various types of noise. The out-of-distribution examples are obtained from these highly distorted images or by including examples of object classes different from those in the target test set. We show that {\\em deep learners benefit more from out-of-distribution examples than a corresponding shallow learner}, at least in the area of handwritten character recognition. In fact, we show that they beat previously published results and reach human-level performance on both handwritten digit classification and 62-class handwritten character recognition.", "text": "recent theoretical empirical work statistical machine learning demonstrated importance learning algorithms deep architectures i.e. function classes obtained composing multiple non-linear transformations. self-taught learning already applied deep learners mostly show advantage unlabeled examples. explore advantage brought out-of-distribution examples. purpose developed powerful generator stochastic variations noise processes character images including afﬁne transformations also slant local elastic deformations changes thickness background images grey level changes contrast occlusion various types noise. out-of-distribution examples obtained highly distorted images including examples object classes different target test set. show deep learners beneﬁt out-of-distribution examples corresponding shallow learner least area handwritten character recognition. fact show beat previously published results reach human-level performance handwritten digit classiﬁcation -class handwritten character recognition. deep learning emerged promising area research statistical machine learning review). learning algorithms deep architectures centered learning useful representations data better suited task hand organized hierarchy multiple levels. part inspired observations mammalian visual cortex consists chain processing elements associated different representation visual input. fact found recently features learnt deep architectures resemble observed ﬁrst stages become invariant factors variation higher layers learning hierarchy features increases ease practicality developing representations tailored speciﬁc tasks able borrow statistical strength related tasks finally learning feature representation lead higher-level features robust unanticipated sources variance extant real data. self-taught learning paradigm combines principles semi-supervised multi-task learning learner exploit examples unlabeled possibly come distribution different target distribution e.g. classes interest. already shown deep learners clearly take advantage unsupervised learning unlabeled examples needs done explore impact out-of-distribution examples multi-task setting uses different kind learning algorithm). particular relative advantage deep learning settings evaluated. hypothesis discussed conclusion deep hierarchy features better able provide sharing statistical strength different regions input space different tasks. whereas deep architecture principle powerful shallow terms representation depth appears render training problem difﬁcult terms optimization local minima. also recently successful algorithms proposed overcome difﬁculties. based unsupervised learning often greedy layer-wise unsupervised pre-training stage layer initialization techniques applied here denoising auto-encoder performed similarly better previously proposed restricted boltzmann machines terms unsupervised extraction hierarchy features useful classiﬁcation. layer trained denoise input creating layer features used input next layer. paper following questions good results previously obtained deep architectures mnist digit images generalize setting much larger richer dataset nist special database classes around examples? extent perturbation input images make resulting classiﬁers better similarly perturbed images also original clean examples? study question context -class -class tasks nist special database deep architectures beneﬁt out-of-distribution examples i.e. beneﬁt self-taught learning framework? highly perturbed examples generate out-of-distribution examples. similarly feature learning step deep learning algorithms beneﬁt training moderately different classes corresponding shallow purely supervised architecture? train classes test answer question. experimental results provide positive evidence towards questions well classiﬁers reach human-level performance -class isolated character recognition beat previously published results nist dataset achieve results introduce next section sophisticated system stochastically transforming character images explain methodology based training without transformed images testing clean ones. measure relative advantage out-of-distribution examples deep learner supervised shallow one. code generating transformations well deep learning algorithms made available http//hg.assembla.com/ift. estimate relative advantage deep learners training classes interest comparing learners trained classes learners trained subset conclusion discusses general question deep learners beneﬁt much self-taught learning framework. since out-ofdistribution data common conclusion practical importance. section describes different transformations used stochastically transform source images order obtain data larger distribution covers domain substantially larger clean characters distribution start. although character transformations used improve character recognizers effort large scale number classes complexity transformations hence complexity learning task. code transformations available http//hg.assembla.com/ift. modules pipeline share global control parameter allows modulate amount deformation noise introduced. main parts pipeline. ﬁrst slant pinch below performs transformations. second part blur contrast adds different kinds noise. change character thickness morphological operators dilation erosion applied. neighborhood pixel multiplied elementwise structuring element matrix. pixel value replaced maximum minimum resulting matrix respectively dilation erosion. different structural elements increasing dimensions used. image randomly sample operator type equal probability structural element subset round smallest structuring elements dilation erosion neutral element always present set. afﬁne transform matrix sampled according complexity. output pixel takes value input pixel nearest producing scaling translation rotation shearing. marginal distributions tuned forbid large rotations give good variability transformation pinch module applies whirl pinch gimp ﬁlter whirl pinch similar projecting image onto elastic surface pressing pulling center surface square input image draw radius-r disk around center pixel belonging disk value replaced value source pixel original image line goes distance deﬁne distance source position thus found. pinch motion blur module gimp’s linear motion blur parameters length angle. value pixel ﬁnal image approximately mean ﬁrst length pixels found moving angle direction angle degrees length normal). occlusion module selects random rectangle occluder character image places original occluded image. pixels combined taking i.e. keeping lighter ones. rectangle corners sampled larger complexity gives larger rectangles. destination position occluded image also sampled according normal distribution. module skipped probability gaussian smoothing module different regions image spatially smoothed. achieved ﬁrst convolving image isotropic gaussian kernel size variance chosen uniformly ranges ﬁltered image normalized also create isotropic weighted averaging window kernel size maximum value center. image sample uniformly +×complexity pixels averaging centers original image ﬁltered one. initialize zero mask matrix image size. selected pixel mask averaging window centered ﬁnal image computed following element-wise operation image+f iltered image×mask module skipped probability module permutes neighbouring pixels. ﬁrst selects fraction complexity pixels randomly image. pixels sequentially exchanged random pixel among four nearest neighbors module skipped probability following larochelle background image module adds random background image behind letter randomly chosen natural image contrast adjustments depending complexity preserve less original character image. scratches module places line-like white patches image. lines heavily transformed images digit chosen random among images randomly cropped rotated angle ormal using bi-cubic interpolation. passes greyscale morphological erosion ﬁlter applied reducing width line amount controlled complexity. module skipped probability probabilities applying patches much previous work deep learning performed mnist digits task examples variants involving examples focus much larger training sets times times larger classes. ﬁrst step constructing larger datasets sample data source nist fonts captchas data character sampled sources second step apply pipeline transformations and/or noise processes described section provide baseline error rate comparison also estimate human performance -class task -class digits task. compare best multi-layer perceptrons best stacked denoising auto-encoders models’ hyper-parameters selected minimize validation error. also provide comparison precise estimate human performance obtained amazon’s mechanical turk service users paid small amounts money perform tasks human intelligence required. mechanical turk used extensively natural language processing vision. users presented character images asked choose corresponding ascii characters. forced choose single character class image. subjects classiﬁed images pair. different humans labelers sometimes provided different label example able estimate error variance effect image classiﬁed different persons. average error humans -class task nist test standard error data sources nist. main source characters nist special database widely used training testing character recognition systems dataset composed digits characters hand checked classiﬁcations extracted handwritten sample forms writers. characters labelled classes corresponding -a-z a-z. dataset contains parts varying complexity. fourth partition experimentally recognized difﬁcult recommended nist testing used work well previous work purpose. randomly split remainder training validation model selection. performances reported previous work dataset mostly digits. classes training testing phase. especially useful estimate effect multi-task setting. distribution classes nist training test sets differs substantially relatively many digits test uniform distribution letters test fonts. order good variety sources downloaded important number free fonts from http//cg.scs.carleton.ca/˜ luc/freefonts.html. including operating system’s fonts total different fonts choose uniformly from. chosen either used input captcha generator producing corresponding image directly input models. dataset. software based random character class generator various kinds transformations similar described previous sections. order increase variability data generated many different fonts used generating characters. transformations applied randomly generated character complexity depending value complexity parameter provided user data source. data. large scanned ocred manually veriﬁed machineprinted characters included additional source. part larger corpus collected image understanding pattern recognition research group thomas breuel university kaiserslautern publicly released. data sets data sets contain grey-level images associated label character classes. nist. nist special database {training validation test} examples. dataset obtained taking characters four sources sending transformation pipeline described section example generate data source selected probability fonts captchas data nist. apply transformations order given above sample uniformly complexity range {training validation test} examples. nistp. equivalent except apply transformations slant pinch. therefore character transformed additional noise added image giving images closer nist dataset. {training validation test} examples. multi-layer perceptrons whereas previous work compared deep architectures shallow mlps svms compared mlps large datasets used preliminary experiments training svms subsets training allowing program memory yielded substantially worse results obtained mlps. training nearly billion examples mlps much convenient classiﬁers based kernel methods. single hidden layer tanh activation functions softmax output layer estimating number hidden units taken training examples presented minibatches size constant learning rate chosen among stacked denoising auto-encoders various auto-encoder variants restricted boltzmann machines used initialize weights layer deep apparently setting parameters basin attraction supervised gradient descent yielding better generalization initial unsupervised pre-training phase uses training images training labels. layer trained turn produce representation input hypothesized advantage brought procedure stems better prior hand taking advantage link input distribution conditional distribution interest hand taking advantage expressive power bias implicit deep architecture figure illustration computations training criterion denoising auto-encoder used pre-train layer deep architecture. input layer corrupted encoded code encoder decoder maps reconstruction compared uncorrupted input loss function whose expected value approximately minimized training tuning chose denoising auto-encoder building block deep hierarchies features simple train explain provides efﬁcient inference yielded results comparable better rbms series experiments training denoising auto-encoder presented stochastically corrupted version input trained reconstruct uncorrupted input forcing hidden units represent leading regularities data. random binary masking corruption trained purely unsupervised hidden units’ activations used inputs training second etc. unsupervised pre-training stage parameters used initialize deep ﬁne-tuned standard procedure used train hyper-parameters addition amount corruption noise separate learning rate unsupervised pre-training stage fraction inputs corrupted selected among another hyper-parameter number hidden layers ﬁxed based previous work sdas mnist size hidden layers kept constant across hidden layers best results obtained largest values could experiment given patience hidden units. figure sdax deep models. error bars indicate conﬁdence interval. indicates model trained nist nistp left overall results models nist nistp test sets. right error rates nist test digits only along previous results literature respectively based nearest neighbors mlps svms. models either trained nist nistp tested either nist nistp either -class task -digits task. training larger datasets takes around gpu-. figure summarizes results obtained comparing humans three mlps three sdas along previous results digits nist special database test literature respectively based artmap neural networks fast nearest-neighbor search mlps svms detailed complete numerical results found appendix. deep learner outperformed shallow ones previously published performance trained perturbed data reaches human performance -class task -class task. error error seem large large majority errors humans out-of-context confusions addition shown left figure relative improvement error rate brought self-taught learning greater differences statistically qualitatively signiﬁcant. left side ﬁgure shows improvement clean nist test error brought out-of-distribution examples relative percent change measured taking right side figure shows relative improvement brought multi-task setting model trained classes target classes interest induced out-of-distribution examples right improvement induced multi-task learning deep learner beneﬁts self-taught learning scenarios compared shallow mlp. classes target classes respectively digits lower-case upper-case characters). again whereas gain multi-task setting marginal negative substantial sda. note simplify multi-task experiments original nist dataset used. example mlp-digits shows relative percent improvement error rate nist digits test single-task model trained outputs seeing digit examples whereas multi-task model trained outputs character classes examples. hence hidden units shared across tasks. multi-task model digit error rate measured comparing correct digit class output class associated maximum conditional probability among digit classes outputs. setting similar target classes found self-taught learning framework beneﬁcial deep learner traditional shallow purely supervised learner. precisely answers positive questions asked introduction. good results previously obtained deep architectures mnist digits generalize much larger richer dataset nist special database classes around examples? systematically outperformed previously published results dataset fact reaching human-level performance around error class task digits beating previously published results data. extent self-taught learning scenarios help deep learners help shallow supervised ones? found distorted training examples made resulting classiﬁer better similarly perturbed images also original clean examples importantly novel deep architectures beneﬁt out-of-distribution examples. mlps helped perturbed training examples tested perturbed input images marginally helped even hurt respect clean examples hand deep sdas signiﬁcantly boosted out-of-distribution examples. similarly whereas improvement multi-task setting marginal negative quite signiﬁcant explained arguments below. original self-taught learning framework out-of-sample examples used source unsupervised data experiments showed positive effects limited labeled data scenario. however many results raina suggest relative gain self-taught learning ordinary supervised learning diminishes number labeled examples increases. note instead that deep architectures experiments show positive effect accomplished even scenario large number labeled examples i.e. here relative gain self-taught learning probably preserved asymptotic regime. would deep learners beneﬁt self-taught learning framework? idea lower layers predictor compute hierarchy features shared across tasks across variants input distribution. theoretical analysis generalization improvements sharing intermediate features across tasks already points towards explanation intermediate features used different contexts estimated allows share statistical strength. features extracted many levels likely abstract suggest) increasing likelihood would useful larger array tasks input conditions. therefore hypothesize depth unsupervised pre-training play part explaining advantages observed here future experiments could attempt teasing apart factors. would deep learners beneﬁt self-taught learning scenarios even number labeled examples large? hypothesize related hypotheses studied erhan whereas erhan found online learning huge dataset make advantage deep learning bias vanish similar phenomenon happening here. hypothesize unsupervised pre-training deep hierarchy self-taught learning initializes model basin attraction supervised gradient descent corresponds better generalization. furthermore good basins attraction discovered pure supervised learning labeled examples allow model poorer basins attraction discovered purely supervised shallow models kind better basins associated deep learning self-taught learning. table overall comparison error rates character classes except last columns digits only deep architecture pre-training ordinary shallow architecture models shown trained using perturbed data using validation select hyper-parameters training choices. {sdamlp} trained nist {sdamlp} trained nistp {sdamlp} trained human error rate digits lower bound count digits recognized letters. comparison results found literature nist digits classiﬁcation using test included. table relative change error rates perturbed training data either using nistp mlp/sda models using mlp/sda models. positive value indicates training perturbed data helped given test clearly deep learning models beneﬁt perturbed training data even testing clean data whereas trained perturbed data performed worse clean digits clean characters. table test error rates relative change error rates multi-task setting i.e. training task isolation training three tasks together mlps sdas. beneﬁts much multi-task setting. experiments unperturbed nist data using validation error model selection. relative improvement single-task error multi-task error.", "year": 2010}