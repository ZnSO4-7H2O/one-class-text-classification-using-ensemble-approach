{"title": "Knowledge Fusion via Embeddings from Text, Knowledge Graphs, and Images", "tag": ["cs.AI", "cs.LG", "stat.ML", "I.2.6; I.2.4"], "abstract": "We present a baseline approach for cross-modal knowledge fusion. Different basic fusion methods are evaluated on existing embedding approaches to show the potential of joining knowledge about certain concepts across modalities in a fused concept representation.", "text": "abstract. present baseline approach cross-modal knowledge fusion. different basic fusion methods evaluated existing embedding approaches show potential joining knowledge certain concepts across modalities fused concept representation. work intends investigate inﬂuence modalities means tri-modal knowledge representations fuse information text documents image collections large knowledge graphs intuitively modalities provides complementing information. based intuition hypothesize potential joint latent knowledge representation constructed multiple embeddings. representation might come closer human perception concepts compared representations extracted single modality alone. test hypothesis propose approach integrate visual textual latent representations embeddings concepts. evaluating resulting latent concept representations standard similarity benchmarks indeed shows higher correlation human notion concept similarity unibi-modal representations. convincingly demonstrates great potential joint latent knowledge representation constructed multiple embeddings detailed following sections. first introduce existing uni-modal embeddings explaining aligned fused demonstrate potential similarity benchmarks summarize ﬁndings discuss related work conclude word embeddings distributed word representations. created unsupervised methods rely huge text corpus input. information co-occurrences words encoded dense vector representation calculating cosine similarity representations similarity score words obtained. examples word representations senna hierarchical log-bilinear models wordvec glove similarly images encoded latent vector space. image representations deep convolutional neural networks shown promising results recent years. deep cnns transfer image dimensional vector space representation example used image classiﬁcation applying softmax function. latent vector representation images correspond layers deep applying softmax. image classiﬁcation cnns inception-v used tensorflow shown good results imagenet classiﬁcation task term ’knowledge graph’ coined google since used graph-based knowledge base popular examples dbpedia wikidata yago survey). again knowledge graph embeddings learned graphs consisting entities typed predicates entities also abstract concepts. entities predicates encoded dimensional vector space facilitating computation probabilities relations within knowledge graph used link prediction tasks examples learning latent vector representations knowledge graphs rescal transe hole suns framework obtaining consolidated tri-modal shared space data representation aligned across multiple modalities needed. existing bi-modal approaches rely manually aligned documents. since manual creation datasets containing large number text documents aligned images entities prohibitively expensive build existing pre-trained uni-modal representations align across modalities respective training. achieved relating modalities abstract concepts instead text images kg-instances. thus combine embeddings three different modalities textual visual knowledge graph. chose well-established wordvec model textual embeddings inception-v visual embeddings since shown state-of-the-art results respective tasks pre-trained models readily available online. knowledge graph embeddings could suitable pre-trained model online trained representations ourselves. chose transe model shown scalable knowledge graphs millions vertices good results showing better captured notion human similarity within approach latent vectors transe representing concepts dbpedia graph. thus transform representation word-level. concept uniquely addressable dbpedia several labels known refer concept. aligning representations word-level take concept commonly used surface forms referring concept. visual representations images imagenet imagenet categories correspond synsets wordnet synsets least images respective topic. combining image representations given synset obtain visual representation synset. alike combine image representations taking max-value vector index also noticed yield better results compared mean values. additionally abstract synset representations built synset representations utilizing wordnet hierarchy example embedding ‘instrument’ created combining embeddings ‘violin’ ’harp’. therefore build hierarchical subtrees wordnet synset contained imagenet synset representations subtree visual representation imagenet combined form abstract synset representation. total abstract additional synset representations. alignment synset representations word-level performed wordnet lexemes since lexemes assigned least synset wordnet. extract lexeme representations utilizing synset representations. intersection wordvec transe leads aligned tri-modal concept space concepts. representations modalities concatenated shared concept fusion techniques resulting concept space applied. obtaining shared space vector representations modalities fused. apart simple concatenation build existing methods like dimensionality reduction. addition propose normalization weighting scheme embeddings multiple modalities. tri-modal concept space different word-phrases represented three matrices text knowledge graph visual combination techniques whole information three modalities deﬁne matrix vertically stacked matrices three latent vector representations vary drastically dimensions visual representations tend dimensions knowledge graph representations typically around dimensions. thus representations higher dimensionalities tend dominate combination techniques. circumvent dimensionality bias combine vectors equal size. furthermore value range features differ depending underlying training objective method. address problems tested pre-processing steps comprising normalization column vector unit length well weighting normalized matrices individually weights stacking. thus take account certain representations might contain useful information might better trained. averaging method uses cosine similarity three modalities calculated separately. averaging three values combined similarity measure also robust respect different vector dimensionalities. conc vector representations combined concatenation single representations similarity calculated cosine similarity concatenated vectors. similarities following techniques also calculated cosine similarity. singular value decomposition factorizes input matrix three matrices unitary matrices diagonal matrix singular values descending order diagonal. taking ﬁrst columns biggest singular values combined kdimensional representation ukσk. principal component analysis uses orthogonal transformation convert correlated variables linearly uncorrelated variables. fixing number uncorrelated principal components results projection lower dimensional vector space. taking principal components highest variance create representation distinctive features. also tested canonical correlation analysis tests always performed superior consistent thus omit tests cca. quantitative qualitative assessments pre-trained representations text images newly trained knowledge graph representations. textual representations obtained wordvec vectors dimensions trained negative sampling google news text corpus containing billion words. visual representations inception-v model pre-trained imagenet classiﬁcation task applied compute representations dimensions. knowledge graph representations obtained transe model created running transe dbpedia knowledge graph. trained transe local closed word assumption type constraints rank= evaluation utilize various word similarity datasets. limiting factor verbs abstract words named entities visual representation available. thus provide subsets simlex- mturk- covered combined space online. aggregated measure also report average performance evaluation datasets weighted respective size. table conﬁrms subsets word evaluation datasets similarly difﬁcult solve compared original datasets. table spearman rank correlation subsets stacking normalization weighting reported. combination methods dimensionality reduction dimensions. stacking none combination methods signiﬁcantly better single textual representation mturk- subset. normalized representations allow ﬁxed combination ratio resembling equal weight information modalities. also conducted additional experiments different dimension parameters pca. thereby concluded dimensions sufﬁcient encode useful information word similarity task. however combination methods normalization also signiﬁcantly consistently outperforming single textual representation shown table representations different modalities likely equally informative word similarity task hold different complementary information. weighting representations normalization order control proportion information induced representation. grid search step size investigated modality composition weighted average evaluation sets. optimal weights conc although weighting schemes include small proportions representations extracted complementary information improves performance signiﬁcantly. fig. illustrates combination three fused weighted modalities produces better results single representations. weighted combination methods substantially outperform unibi-modal embeddings best results obtained conducted experiments simlex- mturk- normalization fig. weighted average spearman rank correlation scores different weightings normalized textual visual representations. combination conc shown fig. observed similar behavior evaluation datasets terms weighting optimum structure. further analyze dimensions. turns weighting normalization even crucial methods conc method. especially exploits information representations weight. nonetheless combined representation three modalities signiﬁcantly better combination bi-modal representations. optimal weights always include three modalities. experiments different transe model parameterizations revealed ﬁnding depending speciﬁcally trained transe embedding attributes information extracted knowledge graph. thus improve concept representations modalities complementary information encoded transe inception-v wordvec embeddings. fig. weighting normalization. shown average plots three evaluation datasets colorbar indicates spearman rank correlation black cross marks optimum. successfully exploited pre-trained concept representations different domains. enables re-usability years research computer vision natural language processing semantic web. plugging models approach incorporating modalities straightforward. performance embeddings word similarity task improved significantly. shared representations always performed best information three modalities included. recently several researchers tried transfer learned knowledge task another combine different approaches. image classiﬁcation important also images classiﬁed visual representations image classiﬁcation task transferred another different classes. learn transfer mid-level image representations cnns. test combination visual textual representations vector stacking similar stacked auto-encoder combine visual textual input. contrast approach evaluated simple vector stacking neither evaluated sophisticated combination techniques incorporation structured resources like kgs. contrast uses textual information text corpus wordnet. purpose wordnet transferred text performing random walks synset hierarchy hereby storing traversal path text neither visual representations work information expressive directly. transformation traversal path text might lose characteristics underlying graph structure used latent vector representations explicit model learned complete approach also goes beyond current retroﬁtting ideas like adjust learned word embeddings incorporating information lexical databases. firstly slightly adapt representation learn combined representation. secondly much information large expressive instead smaller lexical database. lastly also visual information. closest work word-level alignment concept space used autoencoders rank weight tensors create vector representations synsets lexemes wordnet learned vector representation before. achieve treating word synset lexemes. work demonstrates potential mining meaningful concept representations multi-modal data sources. approach builds aligned embeddings extracted million images text corpus billion words million concepts fusing aligned embeddings shared cross-modal concept space. optimized creating holistic representation concept fusing knowledge available modalities. limited small concepts since concepts available modalities far. currently visual embeddings pose bottleneck compared exhaustive concept spaces textual embeddings. introduced two-step process word-level alignment fusion different modalities. demonstrated fused embeddings three modalities outperform unibi-modal embeddings. conﬁrms hypothesis modality contributes complementing information thus fusion modalities shared concept space yield holistic view. also advocates importance semantically meaningful embedding spaces. further results indicate shared multi-modal representation comes closer human notion concept demonstrated quantitatively similarity benchmarks. appears ﬁndings open large number future research directions raising fundamental questions. first foremost became obvious knowledge representations general knowledge graphs particular beneﬁt greatly integrating latent semantics multiple modalities. approach ﬁrst attempt achieve that. however pressing issue remains future work scaled larger number concepts. beyond that numerous cross-disciplinary research challenges areas like sensory neuroscience philosophy perception multimodality research could beneﬁt advances area learned cross-modal knowledge representations vice versa.", "year": 2017}