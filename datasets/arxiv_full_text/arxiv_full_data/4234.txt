{"title": "Saliency Prediction for Mobile User Interfaces", "tag": ["cs.CV", "cs.AI"], "abstract": "We introduce models for saliency prediction for mobile user interfaces. A mobile interface may include elements like buttons, text, etc. in addition to natural images which enable performing a variety of tasks. Saliency in natural images is a well studied area. However, given the difference in what constitutes a mobile interface, and the usage context of these devices, we postulate that saliency prediction for mobile interface images requires a fresh approach. Mobile interface design involves operating on elements, the building blocks of the interface. We first collected eye-gaze data from mobile devices for free viewing task. Using this data, we develop a novel autoencoder based multi-scale deep learning model that provides saliency prediction at the mobile interface element level. Compared to saliency prediction approaches developed for natural images, we show that our approach performs significantly better on a range of established metrics.", "text": "introduce models saliency prediction mobile user interfaces. mobile interface include elements like buttons text etc. addition natural images enable performing variety tasks. saliency natural images well studied area. however given difference constitutes mobile interface usage context devices postulate saliency prediction mobile interface images requires fresh approach. mobile interface design involves operating elements building blocks interface. ﬁrst collected eye-gaze data mobile devices free viewing task. using data develop novel autoencoder based multi-scale deep learning model provides saliency prediction mobile interface element level. compared saliency prediction approaches developed natural images show approach performs signiﬁcantly better range established metrics. mobile devices become ubiquitous recent years accompanied explosion number applications available devices. u.s. mobile apps overtook internet time spent year world moves towards pervasive mobile usage brands increasingly trying provide engaging experience customers developing apps constitutes signiﬁcant cost brands part development process designing applications likely help user performs tasks efﬁciently engaging manner. user interface design process today involves designers creating mocks improved iterative manner. part figure sample image pixel-level ground truth saliency inferred collected front-facing camera video feed element-level saliency map. iterative process feedback focus groups peers. time consuming expensive process presents opportunities automation. build models predict saliency different sections mobile propose feedback tool designers. desktop devices eye-gaze tracking form user engagement feedback studied desktop based eye-gaze tracking technologies rely specialized hardware capturing human face eyes viewing techniques cannot applied mobile device without compromising natural usage pattern devices. however modern mobile devices alalways equipped front facing cameras. using these possible capture user’s face eyes exposed mobile screen. work leverage itracker convolutional neural network based model used predict location users ﬁxation screen using video feed front-facing camera input. approach predicting saliency mobile predict pixel level saliency. designers work pixels work elements. deﬁne mobile element building blocks arranged assemble complete element natural image button text component present design process designer remove edit change relative position element. given fact decide approach problem saliency detection element level. saliency output must present spatial coherence smooth transition between neighbouring pixels. addressing gaze saliency element level preserves spatial coherence correlation pixels element. enables designer modify design based relative saliencies elements. figure shows sample image collected pixel-level ground truth element-level saliency maps. work introduce models trained dataset images along corresponding gaze data collected users. model used predict saliency test provide rapid feedback designer. summary main contributions work follows. propose novel model uses de-noising autoencoders multiple scales elements provide saliency prediction element level. task saliency prediction mobile achieve accuracies signiﬁcantly better state saliency prediction. predicting gaze natural images well explored topic computer vision. early natural image saliency methods based concepts like feature integration theory graph-based normalization method analyzes spectrum image information theory principles like self-information information maximization models supervised learning saliency prediction based manually designed feature sets approaches modelled saliency bottom-up manner using features leads models fail generalize complex scenes domains. recent progress saliency prediction driven deep learning methods trained large datasets allows learning hierarchies feature representations pixel level data. models like salnet trained networks predict saliency scratch others used features pre-trained cnns deepgaze salicon ml-net deepﬁx recent advances include methods like training using adversarial examples neural attentive mechanisms iteratively reﬁne predicted saliency methods designed natural image saliency prediction explore applicability methods mobile saliency prediction. recent models attempted explicitly model neighborhood location affects saliency particular location. mr-cnn presents multi-scale trained image regions centered ﬁxated non-ﬁxated image patches multiple scales. similar model proposed salicon also incorporates features learned scales coarse optimizes divergence last layer. multi-context approach subsampled upsampled image patch super-pixel level proposed methods leverage contrast image region surrounding area saliency prediction. methods mentioned developed exclusively analyzing natural images trained tested graphic designs. attempts understanding visual perception interfaces designs made since last decade work predicts entry point webpages screen-shot dataset using features center surrounded differences colors intensity orientations. linear regression model features extracted html induced generate model predicting visual attention webpages explored model combining multi-scale low-level feature responses explicit face maps positional bias proposed predict ﬁxations webpage image dataset dataset contains total screenshots webpages. work extends replacing speciﬁc object detector features deep neural networks. users’ mouse keyboard input along components used predicting attention manually designed feature predict human visual attention free-viewing webpages studied line work presents semantically closest area research work limited application webpages. given size structural differences desktop webpages mobile apps models cannot directly ported problem. hardware. ubiquity mobile devices pose unique challenges opportunities. recent works explored possibility using front facing cameras mobile devices detect gaze location users looking mobile screen these itracker based model sophisticated approach. itracker system developed modify work android based mobile phones. absence available eye-gaze datasets mobile created dataset assembling mobile images android applications google play store. ensured selected apps represent good spread respect ratings download counts application screenshots taken average leading total images. since goal predict saliency element level bounding boxes elements required. this methods used. ﬁrst method capturing screenshots mobile process logs ofﬁcial android debug tools information underlying code application. code processed obtain bounding boxes elements present this smaller element area considered ’over’ larger element pixel belonging element assigned smaller element. method work scenarios elements overlaps another method involved semi-automated drag drop scheme generate bounding boxes. example output shown figure distribution number elements shown figure mean order collect free viewing eye-gaze data mobile images described previous section developed mobile application displays screenshots mobile images participants natural environment. conducted experiment mechanical turk participants downloaded application mobile devices experiment. participants belonged group participants given comprehensive instructions download application participate experiment. application collected front facing camera’s video feed participant across multiple sessions feed sent server. session began calibration task followed displaying mobile images free viewing instructions given perform speciﬁc tasks. mobile interface screenshots interspersed ﬁller images probability occurrence done remove spatial bias prior images. ﬁller images consisted sceneries abstract video feed collected images. participant shown average different images across span sessions. image persisted seconds second image. participants free pause sessions case wanted take break. experiment screenshot shown average different participants ensuring image shown twice participant. videos participants collected free viewing task previous step generated gaze points correspond participants looking various images shown them. achieve started itracker work introduced tracking software works devices mobile phones tablets without need additional sensors devices. available software designed devices modiﬁed android devices. captured videos split frames. frame crops face eyes generated using required input itracker. output coordinates gaze point corresponding frame video. locate pixel participant looking frame. prediction gaze points itracker found accurate enough task hand solution this included calibration task beginning session app. task showed moving object different positions screen total seconds. participants instructed follow object screen. video participant captured calibration task processed described earlier. linear regression model trained calibration sections session gaze points predicted itracker features predict actual coordinates object shown. divided calibration frames training test sets ratio measured tracking error test set. calibration task helped reducing average error mean standard deviation error range error reported paper used regression output gaze point also generated -dimensional co-variance matrix. utilized processing ground truth eye-gaze ﬁxations. matrix generated calibration used gaussian blurring ﬁxation maps viewed participant session. procedure pixel-level probabilistic heatmap ﬁxation points. converting ﬁxation locations continuous distribution allows uncertainty eye-tracking ground truth measurements incorporated suggested leverage error calibration varied session another based mobile held lighting conditions. indicates top-left bias similar webpage saliency dataset primarily important elements generally present area participants tend browse images top-to-bottom left-to-right. convert pixel-level saliency maps element-level saliency maps. this compute integral pixel-level saliency density area covered element. followed normalization elements ensure vector sums given elements represent element saliency vector probabilities probability element ﬁxated. case element overlaps another element assign saliency pixels overlapping regions element top. sample pixel-level element-level saliency maps presented figure saliency driven visual contrast indicates parts image visually appealing relative rest image. thus saliency model needs capture contrast region image element case surrounding area. therefore extract features every element three scales. ﬁrst scale image element itself. second scale consists element along region surrounding whose boundaries decided mid-point trained autoencoder images dataset. using autoencoders able learn features images time reduce input dimensions needed saliency prediction model. learned separate autoencoder models scales elements independently. three autoencoders share architecture different parameter values. section talk detail autoencoder model contributes saliency prediction. sample elements noisy versions reconstructed versions shown figure also computed low-level features based color distribution size position elements image. features generated element including width height area position pixel coordinates along ﬁrst second color moment color channel element consideration well whole image separately. included area width height feature since rescaling elements input model thus loose information regarding size scale process. features position helps capturing user’s bias towards elements left screen. pixels image element value pixel image color channel ﬁrst color moment analogous mean; second color moment analogous standard deviation calculated primary predict eye-gaze ﬁxations element level. element predict probability ﬁxation element incorporating features learned three scales element low-level features. motivated works combine element’s boundary entire images boundary dimensions. third scale consists entire image. saliency models described detail later sections uses multi-scale features along level features train fully connected neural network layers saliency prediction elementlevel. describe feature generation methods. autoencoder model learning feature representation saliency models provide effective learn good feature representations using large amount unlabeled data autoencoders neural networks consist parts encoder decoder. encoder reduces input lower dimensional representation decoder reconstructs original input. objective autoencoder enforce output close possible corresponding input. however proven reconstruction criterion enough guarantee extraction useful features suffers non-generalizability. good feature representation stable robust corruptions input capture useful structure input distribution. shown feature extractors learned de-noising autoencoders able learn useful structure data regular autoencoders seemed unable learn adopt concept de-noising autoencoders learn representation images input image corrupted setting fraction pixels image let’s call noisy version image ˜xi. de-noising autoencoder tries reconstruct original image producing reconstruction ˆxi. minimizes reconstruction error using euclidean loss architecture autoencoder consists convolutional layers. adopt ﬁlters size stride ﬁrst convolutional layers respectively. succeeded max-pooling layers. max-pooling layers size stride encoder part autoencoder. followed another three convolutional layers size stride ﬁlters respectively. third forth convolutional layers upsampling layers size convolutional layers relu activations encoder part autoencoder converted size input image sized encoded output. figure overall architecture mobile user interface saliency prediction system. ﬁrst segment elements. example predict saliency alarm element. addition element also take high zoomed images element call scales. autoencoder versions three deep model. saliency elements page reconstructed combining saliency individual elements. information three scales incorporate local global contrasts infer saliency. combining features different levels shown increase performance predicting saliency idea behind saliency element depends element itself also content surrounding element. architecture model shown figure element generated crops element scales described earlier. image regions scale ﬁrst resize size disregarding aspect ratio. done autoencoder models resolution level share architecture. then features coming different scales three convolutional streams autoencoder. details autoencoder model mentioned section output three parallel streams concatenated low-level features mention section becomes input subsequent three fully connected layers. layers learn predict saliency element respect appearance well neighborhood. used relu activation layers superior effectiveness efﬁciency. dropout layers used every pair fully connected layers order prevent over-ﬁtting suggested consistent dimension. hence treat prediction element independent others. apply element-wise activation function ﬁnal layer treat element-wise predictions probabilities independent binary random variables. apply binary cross entropy loss function predicted element-wise saliency corresponding ground truth setting. also experimented mean squared error euclidean loss successfully applied similar settings found performed better experiments. described earlier number saliency approaches natural images studied literature. hypothesized leveraging knowledge contained models provide valuable information model. proposed another model called µ-salnc. model uses features salnet generate feature vector dimension providing third level scale element salnet’s penultimate convolutional layer. vector concatenated features autoencoder level features learning performed fully connected dropout layers similar µ-nc model. users mobile screenshot images. generating saliency maps element test image ﬁrst saliency element predicted. predicted saliency value elements test image normalized total saliency done since saliency elements image probability distribution network trained using stochastic gradient descent euclidean loss. used batch size network validated validation every iterations monitor convergence over-ﬁtting. used standard weight regularizer adam optimizer autoencoder took approximately hours saliency model took approximately hours train epochs machine nvidia gpus. evaluate approach using three metrics. describe next. approach makes saliency prediction elements comprise evaluation metrics thus apply vector element level saliencies ground truth predictions. denote vector ground truth further predicted saliencies area receiver operating characteristics curve widely used score saliency model evaluation. estimated saliency used binary classiﬁer separate positive samples negatives varying threshold saliency curve plotted true positive rate false negative rate. calculated area curve. score means perfect prediction indicates chance level. requires discrete ﬁxations calculation. chose percent salient elements image form ground truth continuous saliency actual ﬁxations similar described report average test images. measures linear correlation estimated saliency ground truth ﬁxation i.e. correlation vectors averaged images test set. closer better performance saliency algorithm. divergence measure distance captures distance target predicted distribution. assigns lower score better approximation ground truth saliency map. metrics advantages limitations model performs well relatively high score metrics. lab-signature pretrained models prior work predicting saliency resized images. models provide pixel-level saliency distributions converted element level saliency distributions summing pixels belonging element dividing across elements. model carried -fold cross validation reporting results. didn’t compare prior work related webpage saliency detection webpages different size number elements orientation. further open implementations available comparison. discussions results evaluation metrics mentioned previous sections presented table three metrics observe µ-nc performs best. approach better µ-salnc. words including features saliency models natural images like salnet training model provide better model learning saliency mobile uis. compared next best natural image saliency models proposed approach better metrics respectively. shows best models trained predicting saliency natural images falls short mobile uis. thus justifying need address problem anew. proposed approach advantages existing saliency prediction models. first collecting eyegaze data mobile inform model patterns unique mobile viewing natural environments. second modeling saliency element level optimize model predict element level saliency leading superior predictions. figure presents qualitative analysis different approaches’ performance element level saliency prediction. sample images show original image pixel-level ground truth element-level ground truth predictions form µ-nc columns. brighter shades indicate saliency. observations first ground truth reﬂects left bias images also reﬂected predictions different models. second models tend favour larger elements µ-nc sometimes predicts small elements high saliency. tsalnet µ-nc often predicts skewed distribution elements predicted high saliencies. approaches tend predict ﬂatter distribution. fourth model learnt give higher saliency speciﬁc components like text specially high contrast background. also instead simply relating saliency low-level features color contrast model also incorporates surrounding region consideration making predictions seen prediction element ﬁfth sample learns non-linear combination high level features predict saliency element level. qualitative quantitative comparisons state approaches demonstrate effectiveness proposed model. learning eye-gaze data mobile predicting element level leads accurate saliency model mobile uis. proposed model element level saliency predictions help designer make decisions following manner. designer make changes properties like color size aspect ratio element level; number elements relative positioning level. modiﬁcation designer receive feedback changes terms saliency. designer also compare decide among variants future work explore model learns simultaneously detect components predict saliency. another direction research understand ease task completion mobile eye-gaze patterns.", "year": 2017}