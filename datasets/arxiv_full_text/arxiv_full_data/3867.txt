{"title": "Statistical inference using SGD", "tag": ["cs.LG", "cs.AI", "math.OC", "math.ST", "stat.ML", "stat.TH"], "abstract": "We present a novel method for frequentist statistical inference in $M$-estimation problems, based on stochastic gradient descent (SGD) with a fixed step size: we demonstrate that the average of such SGD sequences can be used for statistical inference, after proper scaling. An intuitive analysis using the Ornstein-Uhlenbeck process suggests that such averages are asymptotically normal. From a practical perspective, our SGD-based inference procedure is a first order method, and is well-suited for large scale problems. To show its merits, we apply it to both synthetic and real datasets, and demonstrate that its accuracy is comparable to classical statistical methods, while requiring potentially far less computation.", "text": "present novel method frequentist statistical inference m-estimation problems based stochastic gradient descent ﬁxed step size demonstrate average sequences used statistical inference proper scaling. intuitive analysis using ornsteinuhlenbeck process suggests averages asymptotically normal. show merits scheme apply synthetic real data sets demonstrate accuracy comparable classical statistical methods requiring potentially less computation. m-estimation minimization empirical risk functions provides point estimates model parameters. statistical inference seeks assess quality estimates; e.g. obtaining conﬁdence intervals solving hypothesis testing problems. within context classical result statistics states asymptotic distribution empirical rf’s minimizer normal centered around population rf’s minimizer thus given mean covariance normal distribution infer range values along probabilities allows quantify probability interval includes true minimizer. bootstrap classical tool obtaining estimates mean covariance distribution. bootstrap operates generating samples distribution repeating estimation procedure different re-samplings. parameter dimensionality data size grow bootstrap becomes increasingly –even prohibitively– expensive. context follow different path show inference also accomplished directly using stochastic gradient descent point estimates inference ﬁxed step size data set. well-established ﬁxed step-size large dominant method used large scale data analysis. prove also demonstrate empirically average sequences obtained minimizing also used statistical inference. unlike bootstrap approach require creating many large-size subsamples data neither re-running scratch subsamples. method uses ﬁrst order information gradient computations require second order information. important large scale problems re-sampling many times computing hessians computationally prohibitive. outline main contributions paper studies analyzes simple ﬁxed step size sgd-based algorithm inference m-estimation problems. algorithm produces samples whose covariance converges covariance m-estimate without relying bootstrap-based schemes also avoiding direct costly computation second order information. much work done asymptotic normality well stochastic gradient langevin dynamics bayesian setting. discuss detail section ﬁrst work provide ﬁnite sample inference results using ﬁxed step size without imposing overly restrictive assumptions convergence ﬁxed step size sgd. remainder paper organized follows. next section deﬁne inference problem m-estimation recall basic results asymptotic normality used. section main body paper provide algorithm creating bootstrap-like samples also provide main theorem work. details involved provide intuitive analysis algorithm explanation main results using asymptotic ornstein-uhlenbeck process approximation process postpone full proof appendix. specialize main theorem case linear regression also logistic regression. logistic regression particular require somewhat different approach logistic regression objective strongly convex. section present related work elaborate work differs existing research literature. finally experimental section provide parts numerical experiments illustrate behavior algorithm corroborate theoretical ﬁndings. using synthetic data linear logistic regression also considering higgs detection libsvm splice data sets. considerably expanded empirical results deferred appendix. supporting theoretical results empirical ﬁndings suggest inference procedure produces results similar bootstrap using fewer operations. thereby produce efﬁcient inference procedure applicable large scale settings approaches fail. statistical inference m-estimators consider problem estimating parameters using samples {xi}n drawn distribution sample space frequentist inference interested estimating minimizer population risk statistical inference consists techniques obtaining information beyond point estimatesθ conﬁdence intervals. performed asymptotic limiting distribution associated withθ problems satisﬁes asymptotic normality. distribution converges weakly normal beyond calculating computation requires inversion matrix-matrix multiplications order compute h−gh−—a computational bottleneck high dimensions. instead method uses directly estimate h−gh−. unbiased estimator gradient i.e. expectation w.r.t. stochasticity calculation. classical example unbiased estimator gradient uniformly random index samples inference procedure uses average consecutive iterations. particular algorithm proceeds follows given sequence iterates ﬁrst iterates θ−b+ burn period; discard iterates. next segment iterates ﬁrst iterates compute discard last iterates indicates i-th segment. procedure step size selection length theorem consistent ﬁxed step size depends number samples taken. experiments however demonstrate choosing constant gives equally accurate results signiﬁcantly reduced running time. conjecture better understanding inﬂuence requires stronger bounds constant step size. heuristically calibration methods parameter tuning subsampling methods could used hyper-parameter tuning procedure. leave problem ﬁnding maximal learning rates future work. discarded length based analysis mean estimation appendix discard iterates every segment correlation consecutive order ce−cηd data dependent constants. used rule thumb reduce correlation samples inference procedure. burn-in period purpose burn-in period ensure samples generated iterates sufﬁciently close optimum. determined using heuristics convergence diagnostics. another approach methods optimum relatively small reach stationarity similar markov chain monte carlo burn-in. here scaling factor depends stochastic gradient computed. show examples mini batch linear regression logistic regression corresponding sections. similar resampling methods bootstrap subsampling quantiles variance statistical inference. minimizer according denote hessian weak strong convexity lipschitz gradient continuity constant bounded taylor remainder bounded hessian spectrum egsgs positive data dependent constants eθ)gsθ) assume teθ)θ) provide full proof appendix also give precise formulas constants. ease exposition leave constants expressions above. further next section relate continuous approximation ornstein-uhlenbeck process give intuitive explanation results. discussion. linear regression assumptions satisﬁed empirical risk function degenerate. mini batch using sampling replacement assumptions satisﬁed. linear regression’s result presented corollary appendix. logistic regression assumption satisﬁed empirical risk function case strictly strongly convex. thus cannot apply theorem directly. instead consider square empirical risk function plus constant; below. empirical risk function degenerate satisﬁes assumptions cannot directly vanilla minimize instead describe modiﬁed procedure minimizing section satisﬁes assumptions believe result interest own. present result specialized logistic regression corollary note theorem proves consistency ﬁxed step size requiring however empirically observe experiments sufﬁciently large constant gives better results. conjecture average consecutive iterates larger constant step size converges optimum consider future work. here describe continuous approximation discrete process relate ornsteinuhlenbeck process give intuitive explanation results. particular regularity bayesian inference first second order iterative optimization algorithms –including gradient descent variants– naturally deﬁne markov chain. based principle related work case stochastic gradient langevin dynamics bayesian inference namely sampling posterior distributions using variant note that well vast majority results rely using decreasing step size. recently uses heuristic approximation bayesian inference provides results ﬁxed step size. problem different important ways bayesian inference problem. parameter estimation problems covariance estimator depends gradient likelihood function. case however general frequentist m-estimation problems cases covariance estimator depends gradient hessian empirical risk function. reason without second order information sgld methods poorly suited general m-estimation problems frequentist inference. contrast method exploits properties averaged computes estimator’s covariance without second order information. connection bootstrap methods classical approach statistical inference bootstrap bootstrap samples generated replicating entire data resampling solving optimization problem generated data. identify algorithm analysis alternative bootstrap methods. analysis also speciﬁc thus sheds light statistical properties widely used algorithm. connection stochastic approximation methods long observed stochastic approximation certain conditions displays asymptotic normality setting decreasing step size e.g. recently also ﬁxed step size e.g. chapter results however provide guarantees requirement stochastic approximation iterate converges optimum. decreasing step size overly burdensome assumption since mild assumptions shown directly. know however clear holds ﬁxed step size regime. side-step issue provides results step-size approaches similarly asymptotic results average consecutive stochastic approximation iterates constant step size assumes convergence iterates assumption unable justify even simple settings. beyond critical difference assumptions majority classical subject matter seeks prove asymptotic results different ﬂavors properly consider inference. exceptions recent work follow rely decreasing step size reasons mentioned above. work uses decreasing step size estimating m-estimate’s covariance. work studies implicit decreasing step size proves results similar however compute conﬁdence intervals. coverage probability deﬁned estimated conﬁdence interval coordinate. average conﬁdence interval width deﬁned estimated conﬁdence interval coordinate. experiments coverage probability average conﬁdence interval width estimated simulation. empirical quantile inference procedure bootstrap compute conﬁdence intervals coordinate parameter. results given pair usually indicates figure compare inference procedure bootstrap normal approximation inverse fisher information univariate models. observe method bootstrap similar statistical properties. figure appendix shows plots samples inference procedure. normal distribution mean estimation figure compares samples inference procedure bootstrap versus distribution using i.i.d. samples used mini batch described sec. parameters used mini batch size inference procedure gives bootstrap gives normal approximation gives exponential distribution parameter estimation figure compares samples inference procedure bootstrap using samples exponential distribution λe−λx used mini batch sampled replacement. parameters used mini batch size inference procedure gives bootstrap gives normal approximation gives poisson distribution parameter estimation figure compares samples inference procedure bootstrap using samples poisson distribution λxe−λx used mini batch sampled replacement. parameters used mini batch size inference procedure gives bootstrap gives normal approximation gives additional simulations comparing covariance matrix computed linear regression experiment results case p/√p samples given table bootstrap gives experiment results case .|i−j| p/√p samples given table bootstrap gives conﬁdence intervals computed using error covariance normal approximation gives logistic regression show results logistic regression trained using vanilla mini batch sampled replacement. results modiﬁed given sec. d... experiment samples given table bootstrap gives conﬁdence intervals computed using inverse fisher matrix error covariance normal approximation gives experiment results here compare covariance matrices computed using inference procedure bootstrap inverse fisher information matrix libsvm splice data observe similar statistical properties. splice data contains distinct features data samples. classiﬁcation problem classes splice junctions sequence. logistic regression model trained using vanilla sgd. figure compare covariance matrix computed using inference procedure bootstrap samples. used samples bootstrap inference procedure mini batch size here train binary logistic regression classiﬁer classify using noisy mnist data demonstrate adversarial examples produced gradient attack detected using prediction intervals. ﬂatten image dimensional vector train linear classiﬁer using pixel values features. noise image original pixel either randomly changed pixels random numbers uniformly next train classiﬁer noisy mnist data generate adversarial examples using noisy mnist data set. figure shows image’s logit value conﬁdence interval computed using quantiles inference procedure. experiments observed using larger step size produces accurate results signiﬁcantly accelerated convergence time. might imply term theorem bound artifact analysis. indeed although theorem applies ﬁxed step size imply step size smaller number consecutive iterates used average larger experiments suggest constant step size require experiments inference procedure uses operations produce sample newton method uses n·)· operations produce sample. experiments therefore suggest inference procedure produces results similar bootstrap using fewer operations. work partially supported grants also usdot data-supported transportation operations planning tier university transportation center. a.k. supported goldstine fellowship. thank chen philipp kr¨ahenb ¨uhl matthijs snel spangenberg insightful discussions.", "year": 2017}