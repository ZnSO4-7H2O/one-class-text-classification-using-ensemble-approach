{"title": "Generalized Product of Experts for Automatic and Principled Fusion of  Gaussian Process Predictions", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this work, we propose a generalized product of experts (gPoE) framework for combining the predictions of multiple probabilistic models. We identify four desirable properties that are important for scalability, expressiveness and robustness, when learning and inferring with a combination of multiple models. Through analysis and experiments, we show that gPoE of Gaussian processes (GP) have these qualities, while no other existing combination schemes satisfy all of them at the same time. The resulting GP-gPoE is highly scalable as individual GP experts can be independently learned in parallel; very expressive as the way experts are combined depends on the input rather than fixed; the combined prediction is still a valid probabilistic model with natural interpretation; and finally robust to unreliable predictions from individual experts.", "text": "work propose generalized product experts framework combining predictions multiple probabilistic models. identify four desirable properties important scalability expressiveness robustness learning inferring combination multiple models. analysis experiments show gpoe gaussian processes qualities existing combination schemes satisfy time. resulting gp-gpoe highly scalable individual experts independently learned parallel; expressive experts combined depends input rather ﬁxed; combined prediction still valid probabilistic model natural interpretation; ﬁnally robust unreliable predictions individual experts. practical theoretical reasons often necessary combine predictions multiple learned models. mixture experts product experts ensemble methods perhaps obvious frameworks prediction fusion. however four desirable properties existing fusion scheme achieves time predictions combined without need joint training training meta models; predictions combined depends input rather ﬁxed; combined prediction valid probabilistic model; unreliable predictions automatically ﬁltered combined model. property allows individual experts trained independently making overall model easily scalable parallelization; property gives combined model expressive power; property allows uncertainty used subsequent modelling decision making; ﬁnally property ensures combined prediction robust poor prediction experts. work propose novel scheme called generalized product expert achieves four properties individual experts gaussian processes consequently excels terms scalability robustness expressiveness resulting model. comparison mixture experts ﬁxed mixing probabilities satisfy experts mixing probabilities generally need learned together satisﬁed either. input dependent gating function used achieve property joint training still needed ability ﬁlter poor predictions crucially depends joint training. depending nature expert model need joint re-training satisfy property without gating function shut-down experts combined prediction easily mislead single expert putting probability true label. ensemble method regime bagging satisfy uses ﬁxed equal weights combine models automatically ﬁlter poor predictions although empirically usually robust equal weight voting. boosting stacking requires sequential joint training training meta-predictor respectively satisfy furthermore boosting satisfy stacking limited ability depends training. demonstrate proposed gpoe gaussian processes enjoys good qualities given four desired properties prediction fusion also retains important attributes many weak uncertain predictions yield sharp combined prediction together; combination closed analytical form another gaussian distribution. generalized product expert start brieﬂy describing product expert model proposed method generalization. models target probability distribution product multiple densities given expert. product renormalized one. context supervised learning distributions conditional contrast mixture models experts hold veto power sense value probability single expert assigns probability particular value. hinton pointed training model general experts maximizing likelihood hard renormalization term however special case gaussian experts product distribution still gaussian mean covariance precision i-th gaussian expert point qualitatively conﬁdent predictions inﬂuence combined prediction less conﬁdent ones. predicted variance always correct conﬁdence used would exactly behavior needed. however slight model misspeciﬁcation could cause expert produce erroneously predicted variance along biased mean prediction. combination rule over-conﬁdence single expert erroneous prediction enough detrimental resulting combined model. gpoe given almost desired behavior except fact expert’s predictive precision necessarily right measure reliability prediction weighting introduce another measure reliability down-weight ignore predictions.like proposed generalized product experts also probability model deﬁned products distributions. focus conditional distribution supervised learning taking form measure i-th expert’s reliability point introduce particular choice gaussian processes next subsection ﬁrst analyze effect raising density power done equation widely used annealing distributions mcmc balancing different parts probabilistic model different degrees freedom balanced gpdm). recover special case. sharpens i-th distribution product whereas broadens limit cases exponents ﬁxed causes largest mode i-th distribution dominate product distribution arbitrarily large veto power; hand causes i-th expert arbitrarily small weight combined model effectively ignoring prediction. gpoe gaussian processes established used control inﬂuence individual experts natural choice gaussian processes reliably detect particular expert generalize well given point change entropy prior posterior point takes almost extra computation since posterior variance already computed expert makes prediction prior variance simply kernel used. entropy change point zero means i-th expert information point comes training observation therefore shall contribute combined prediction achieved model gaussian processes covers case point away training points model misspeciﬁed. quantities could used example difference prior posterior variance reason choose entropy change unit-less sense dimensional analysis physics resulting predictive variance equation correct unit. true difference variances carries unit variance divergence prior posterior distribution point could also potentially used entropy change already effective experiments. explore divergence future work. compare gpoe bagging three different datasets kink sarcos apartment price dataset used svi-gp work hensman three different ways build individual experts random subset data; local around randomly selected point; tree based construction ball tree built training recursively partitions space level tree random subset data drawn build datasets methods expert construction data points expert construct experts total. expert uses kernel kernel white kernel hyperparameters learned scaled conjugate gradient. jointly learn experts gating functions time consuming instead entropy change gating function therefore experts combination schemes could learned independently parallel. -core machine described setup training experts independent hyperparameter learning datasets takes seconds minute including time preprocessing ﬁtting ball tree. terms test performance evaluation commonly used metrics standardized negative probability standardized mean square error table show gpoe consistently out-performs bagging combination rules large margin scores. tree based expert construction method explore heuristic variant gpoe takes experts path deﬁned root node leaf node test point tree. alternatively could viewed deﬁning experts root-to-leaf path. variant gives slight boost performance across board. another interesting observation gpoe performs consistently well almost always poor especially snlp score. empirically conﬁrms previous analysis misguided over-conﬁdence experts detrimental resulting shows correction entropy change gpoe effective problem. finally would like note testimony expressive power given gpoe experts trained points generic kernels could combine give prediction performance close even superior sophisticated sparse gaussian process approximation stochastic variational inference evidenced comparison table ukapt dataset. note also parallelization training case took less seconds problem training points although testing time gpoe much longer sparse approximations. similar results competitive even superior sophisticated fitc cholqr approximations observed sarcos kink well space time constraints included extended abstract future work instead. would like emphasize comparison suggest gpoe silver bullet beating benchmarks using naive expert model demonstrate expressiveness resulting model shows potential used conjunction sophisticated techniques sparsiﬁcation automatic model selection. table rmse comparing root mean square error uk-apt dataset score instead smse snlp measure used hensman methods next name indicates number reported svi-gp paper work proposed principled combine predictions multiple independently learned experts without need training. combined model takes form generalized product experts combined prediction gaussian desirable properties increased expressiveness robustness poor predictions experts. showed gpoe many interesting qualities combination rules. however thing cannot capture multi-modality like mixture experts. future work would interesting explore generalized product mixture gaussian processes captures constraints well constraints. another future work direction explore measures model reliability finally lack change entropy indicator irrelevant prediction converse statement seem true i.e. sufﬁcient change entropy necessarily guarantee reliable prediction potential ways model could mis-speciﬁed. however empirical results suggest least kernels change entropy always reliable even estimated posterior variance accurate. theoretical work needed better understand converse case. acknowledgments thank hensman providing dataset benchmarking well anonymous reviewer insightful comments question reliability issue converse case.", "year": 2014}