{"title": "Label Embedding Network: Learning Label Representation for Soft Training  of Deep Networks", "tag": ["cs.LG", "cs.CL", "cs.CV"], "abstract": "We propose a method, called Label Embedding Network, which can learn label representation (label embedding) during the training process of deep networks. With the proposed method, the label embedding is adaptively and automatically learned through back propagation. The original one-hot represented loss function is converted into a new loss function with soft distributions, such that the originally unrelated labels have continuous interactions with each other during the training process. As a result, the trained model can achieve substantially higher accuracy and with faster convergence speed. Experimental results based on competitive tasks demonstrate the effectiveness of the proposed method, and the learned label embedding is reasonable and interpretable. The proposed method achieves comparable or even better results than the state-of-the-art systems. The source code is available at \\url{https://github.com/lancopku/LabelEmb}.", "text": "sun∗ bingzhen wei∗ xuancheng school electronics engineering computer science peking university beijing china {xusunweibzrenxcshumingma}pku.edu.cn propose method called label embedding network learn label representation training process deep networks. proposed method label embedding adaptively automatically learned back propagation. original one-hot represented loss function converted loss function soft distributions originally unrelated labels continuous interactions training process. result trained model achieve substantially higher accuracy faster convergence speed. experimental results based competitive tasks demonstrate effectiveness proposed method learned label embedding reasonable interpretable. proposed method achieves comparable even better results state-of-the-art systems. source code available https//github.com/lancopku/labelemb. existing methods neural networks one-hot vector representations labels. one-hot vector main restrictions. first restriction discrete distribution label distributed completely different dimension others. second restriction extreme value based representation value dimension either soft value allowed. deficiencies cause following potential problems. first easy measure correlation among labels discrete distribution. able measure label correlation potentially harmful learned models e.g. causing data sparseness problem. given image recognition task image shark often similar image dolphin. naturally expect labels similar. suppose training examples shark training examples dolphin. label shark label dolphin similar representations prediction label dolphin suffer less data sparsity problem. second value encoding easy cause overfitting problem. suppose labels similar types fishes. one-hot label representation prefers ultimate separation labels. example currently system output probability probability good enough make correct prediction however one-hot label representation suggests modification parameters still required probability becomes probability becomes fish fish similar appearance probably reasonable probability rather completely could lead overfitting problem. address problems. propose method automatically learn label representation deep neural networks. training proceeds label embedding iteratively learned optimized based proposed label embedding network back propagation. original one-hot represented loss function softly converted loss function soft distributions originally unrelated labels continuous interactions training process. result trained model achieve substantially higher accuracy faster convergence speed stable performance. related prior studies include traditional label representation methods soft label methods model distillation methods method substantially different existing work detailed comparisons summarized appendix contributions work follows learning label embedding compressed embedding propose label embedding network learn label representation soft training deep networks. furthermore large-scale tasks massive number labels naive version label embedding network suffer intractable memory cost problem. propose solution automatically learn compressed label embedding memory cost substantially reduced. interpretable reusable learned label embeddings reasonable interpretable find meaningful similarities among labels. proposed method learn interpretable label embeddings image processing tasks natural language processing tasks. addition learned label embeddings directly adapted training model improved accuracy convergence speed. general-purpose solution competitive results proposed method widely applied various models including resnet seq-to-seq models. conducted experiments computer vision tasks including cifar- cifar- mnist natural language processing tasks including lcsts text summarization task iwslt machine translation task. results suggest proposed method achieves significantly better accuracy existing methods achieve results comparable even better state-of-the-art systems tasks. neural network typically consists several hidden layers output layer. hidden layers input hidden representations. let’s denote part neural network produces last hidden representation input neural network hidden representation defines mapping input hidden representation including limited resnet seq-to-seq output layer maps hidden representation output predicted category directly given argmax operation. output layer typically consists linear transformation maps hidden representation output number labels. following denote true label category denote one-hot distribution denote softmax denote cross entropy distribution model needs approximate e.g. distribution generated model e.g. number labels. element label embedding vector represents similarity labels. example label embedding vector i-th value represents similarity label label learn embeddings reasonable approach would make label embedding close output neural network whose predicted label output distribution model contains generalization information learned neural network. turn label embedding used refined supervisory signal learning model. however aforementioned approach affects learning model terms discriminative power. essence model supposed distinguish inputs label embedding supposed capture commonness labels based inputs goals conflict. avoid conflict propose separate output representation. output layer denoted used differentiate hidden representation normal used predicting output layer denoted focuses learning similarity hidden representation label embedding learned back propagation gradient kept propagating learning affect hidden representation. this discriminative power maintained even enhanced using label embedding. meanwhile label embedding obtains stable learning target. becomes however approach scale properly training output close one-hot distribution label embedding fails capture similarity between labels. solve this apply softmax temperature soften distribution normalized computed following denote softmax temperature. applying higher temperature label embedding gains details output distribution elements embedding vector label-based i.e. elements diagonal better learned. however annealed distribution also makes difference incorrect labels closer. solve problem propose regularize normalized output highest value distribution high difference labels kept figure illustration proposed method. circle stands vector square stands layer parameters. dashed line means cross entropy operation. square labeled neural network resnet seq-to-seq equals loss hinge regularization. learned embedding turn used training network making output close learned embedding. done minimizing cross entropy loss normalized output normalized label embedding figure shows overall architecture proposed method. various kinds neural networks compatible generate hidden representation. experiments used resnet seq-to-seq. however choice limited architectures. moreover although output architecture significantly re-designed computational cost increase much added operations relatively cheap computation. section seek learn condensed form label embeddings name method compressed label embedding network. task number labels limited proposed approach scales well different models different tasks. however massive number labels embedding takes much memory. suppose neural machine translation task labels label embedding matrix. element matrix stored single point float embedding matrix alone take approximately suitable gpu. means taking y-th matrix resulting vector mdimensional vector used label embedding substitute corresponding part final loss normal label embedding network. matrix seen compressed label embeddings represents compressed label embedding matrix seen projection reconstructs label embeddings compressed forms. technique reduce space needed store label embeddings factor considering previous example space needed reduced .mb. compression achieved without significant raising computational cost introduced operation computational cost regular output layer. moreover works well experiments lcsts text summarization task iwslt machine translation task label space huge. first learning label embedding current output model wrong often happens training begins true label’s embedding learn output model. information incorrect learning label embedding neglected. consideration particularly useful improve performance circumstances model’s prediction often wrong start training e.g. cifar- task neural machine translation task. second suggest using diagonal matrix initialization label embedding matrix. using diagonal matrix provide prior label embedding label’s embedding similar label itself could useful start training beneficial learning. conduct experiments using different models diverse tasks show proposed method general-purpose works different types deep learning models. cifar- cifar- dataset consists color images classes containing images each. dataset split training images test images. image comes fine\" label coarse\" label cifar- cifar- dataset data size cifar color images split training images test images except classes images class. mnist mnist handwritten digit dataset consists pixel gray-scale training images additional test examples. image contains single numerical digit select first images training images development rest training set. social media text summarization dataset lcsts consists social media text-summary pairs. split pairs training pairs development data pairs testing. following evaluation metric rouge- rouge- rouge-l iwslt english-vietnamese machine translation dataset dataset international workshop spoken language translation dataset consists english-vietnam parallel sentences constructed captions. split training development test sentence pairs respectively. evaluation metric bleu score cifar- cifar- test method based resnet layers layers respectively following settings mnist model consists convolutional layers fully-connected layer another fully-connected layer output layer. filter size convolutional layers. first convolutional layer contains filters second contains filters. convolutional layer followed max-pooling layer. following common practice relu activation function hidden layers. lcsts iwslt test approach based sequence-to-sequence model. encoder decoder based lstm unit layer lcsts layer iwslt. character word represented random initialized embedding. lcsts embedding size hidden state size lstm unit iwslt embedding size hidden state size lstm unit beam search iwslt beam size large label sets compressed label embedding network tasks. although several hyper-parameters introduced proposed method simple setting tasks proposed method robust experiments simply works well without fine-tuning. temperature tasks. simplicity form hinge loss tasks. adam optimizer tasks using default hyper-parameters. cifar- divide learning rate epoch epoch shown previous work dividing learning rate certain iterations proves beneficial sgd. find technique also applies adam. apply technique cifar- mnist results similar without technique. experiments conducted using intel xeon .ghz nvidia gpu. configuration times different random seeds tasks. tasks without development sets report results figure error rate curve cifar- cifar- mnsit. times experiments conducted credible results baseline proposed model. average results shown deep color curves. first show results cifar- cifar- summarized table proposed method achieves much better results. cifar- proposed method achieves error reduction ratio baseline cifar- proposed method achieves error reduction ratio baseline training time epoch similar baselines. results mnist summarized table proposed method achieves error rate reduction detailed error rate curves shown figure repeated runs shown lighter color averaged values shown deeper color. figure proposed method achieves better convergence speed resnet baselines. label embedding achieves soft training model conflict features similar labels alleviated learned label embeddings. learned label embeddings enables model share common features classifying similar labels supervisory signal contains information similarity thus making learning easier. besides model required distinguish labels completely avoids unnecessary subtle update parameters. addition using label embedding proposed method much stable training curves baselines. fluctuation proposed method much smaller baselines. one-hot distribution forces label completely different others original objective seeks unique indicators labels hard find prone overfitting thus often leading training astray. proposed method avoids softening cifar- task learned label embeddings interesting. since don’t enough space show heatmap labels randomly selected three groups labels labels total. example similar label label bottle can. label bowl similar labels plate. label similar label woman second similar boy. cifar- task learned label embeddings also meaningful. example similar label label automobile truck. label similar label dog. label deer similar label horse. minst task also interesting patterns learned label embeddings. heatmaps learned labels demonstrate label embedding learning reasonable indeed reveal rational similarities among diversified labels. learned label embedding encodes semantic correlation among labels useful. example pre-trained fixed label embedding used directly train model task similar task need learn label embedding again. experimental results demonstrate achieve improved accuracy faster convergence based pre-trained fixed label embedding. show details appendix first show experimental results lcsts text summarization task. results summarized table performance measured rouge- rouge- rouge-l. proposed method performs much better compared baselines rouge- score rouge- score rouge-l score improving respectively. addition results baseline implemented competitive similar labels bóng trái ngư\u0001i ho\u0001t đi\u0001u nh\u0001c di\u0001n vi\u0001t xinh tuy\u0001t gi\u0001i tuy\u0001t cư\u0001i chuy\u0001n chơi xanh vàng nư\u0001c khơi dư\u0001i tr\u0001i nư\u0001c dòng su\u0001i bi\u0001n băng đư\u0001ng then show experimental results iwslt machine translation task. results summarized table measure quality translation bleu following common practice. proposed method achieves better bleu score baseline improvement points. knowledge highest bleu achieved task surpassing previous best result experimental results clear compressed label embedding improve results seq-to-seq model well works tasks massive number labels. label embedding learned compressed fashion also carries semantic similarities. report sampled similarities results table shown table learned label embeddings capture semantics label reasonably well. example word similar colors i.e. xanh vàng word similar tr\u0001i nư\u0001c semantically related natural phenomenon rain. results label embeddings learned compressed fashion demonstrate re-parameterization technique effective saving space without degrading quality learned label embeddings. also prove proposed label embedding also works tasks. propose method learn label representation training process deep neural networks. furthermore propose solution automatically learn compressed label embedding memory cost substantially reduced. proposed method widely applied different models. conducted experiments tasks including cifar- cifar- mnist also tasks including lcsts iwslt. results suggest proposed method achieves significantly better accuracies existing methods moreover learned label embeddings reasonable interpretable provide meaningful semantics labels. achieve comparable even better results state-ofthe-art systems tasks. djork-arné clevert thomas unterthiner sepp hochreiter. fast accurate deep network learning exponential linear units proceedings international conference learning representations richard hahnloser rahul sarpeshkar misha mahowald rodney douglas seung. digital selection analogue amplification coexist cortex-inspired silicon circuit. nature minh-thang luong christopher manning. stanford neural machine translation systems spoken language domains. proceedings international workshop spoken language translation quang nguyen hamed valizadegan milos hauskrecht. learning classification models soft-label information. journal american medical informatics association adriana romero nicolas ballas samira ebrahimi kahou antoine chassang carlo gatta yoshua bengio. fitnets hints thin deep nets. proceedings international conference learning representations jost tobias springenberg alexey dosovitskiy thomas brox martin riedmiller. striving simplicity convolutional net. proceedings international conference learning representations workshop track figure error rate curve model mnsit. times experiments conducted credible results baseline proposed model. average results shown deep color curves. experimental results summarized table proposed label embedding method achieves better performance baseline error rate reduction results averaged error rates repeated experiments standard deviation results also shown. figure shows detailed error rate curve model. repeated runs shown light color averaged values shown deeper color. shown proposed method also works results consistently better baselines. model proposed method converges faster baseline. figure illustration forward propagation pre-trained label embedding network. note that pre-trained label embedding fixed need learn embedding setting. following section show learned label embedding reasonable also useful applications. example learned label embedding directly used finegrained true label distribution train model dataset. purpose model’s objective function contains parts i.e. original one-hot label based cross entropy objective together label embedding based cross entropy objective. call model pretrained label embedding network. presented section pre-trained label embedding network label embedding pre-trained fixed thus eliminating need learning embedding label embedding network label embedding learned training. implementation main differences. first label embedding fixed requires learning. second sub-network learns label embedding removed need learn label embedding again. thus pre-trained label embedding network loss function follows figure shows results pre-trained label embedding network whose label embedding learned normal label embedding network. pre-trained label embedding network achieve much better result baseline faster convergence. shows learned label embedding effective improving performance model label embedding indeed contains generalization information provides refined supervised signal model. learned label embeddings saved reused improve training different models task need learn label embedding again. cifar- task error rate typically goodfellow achieves error rate maxout network. springenberg achieves error rate replacing max-pooling convolutional layer srivastava achieves error rate highway network. achieves rate layer resnet. -layer resnet system achieves averaged error rate repeated runs. considering good method achieves error rate layers. cifar- task error rate typically improvement achieved finetuning model optimization method show robustness proposed method fine tune hyper-parameters. simply plain resnet- adam optimizer default parameters. achieves error rate applying dropconnect technique. goodfellow achieves error rate maxout network. achieves error rate layer resnet achieves error rate layer resnet. -layer resnet model achieves averaged error rate repeated runs. considering good method achieves mnist task plain convolutional networks typically achieve error rates ranging widely around data augmentation complicated models improve performance models believe also work method. srivastava achieves error rate using highway network. mishkin matas achieves error rate using lsuv initialization fitnets. model achieves averaged error rate considering good model achieves prior studies label representation deep learning limited. existing label representation methods mostly traditional methods deep learning frameworks. label representation methods also adopt name label embedding. however meaning different sense deep learning. label representation methods intend obtain representation function labels. label representation vector data independent learned existing information including training data auxiliary annotations class hierarchies textual descriptions example label embedding fixed independently data random projections several regressors used learn predict elements true label’s embedding reconstructed regular one-hot label representation classification. another example canonical correlation analysis seeks vector vector random variables correlation variables maximized regarded label embeddings several major differences methods proposed method. first methods easy adapt deep learning architectures. previously introduced methods come totally different architecture learning methods easy extend general-purpose models like neural networks. instead proposed method label embedding automatically learned data back propagation. second label representation methods adapting training. label embedding fixed randomly initialized thus revealing none semantics labels. method also adaptively learned training data. learned label representation lacks interaction model parameters label embeddings obtained proposed method reveal semantics labels interact actively parts model back propagation. also prior studies so-called soft labels. soft label methods typically binary classification human annotators assign label example also give information confident regarding annotation. side information used learning procedure alleviate noise data produce better results. main difference method soft label methods require additional annotation information training data method need additional annotation information soft probability learned training simple effective manner. moreover proposed method restricted binary classification. also prior studies model distillation deep learning uses label representation better compress model smaller one. deep learning it’s common sense non-convex property neural network functions different initialization different data order different optimization methods would cause varied results model. model distillation novel method combine different instances model single one. training single model target distribution combination output distributions previously trained models. method substantially different compared model distillation method. motivations designed architectures different. model distillation method adopts pipeline system needs first train large model many different instances models label representation baseline models provide better supervisory signals re-train smaller model. pipeline setting different single-pass process setting. method also enables ability learn compressed label embedding extremely large number labels. moreover given label label representation method different example another. provide universal label representation label different compared setting.", "year": 2017}