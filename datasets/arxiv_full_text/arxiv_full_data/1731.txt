{"title": "Grammar as a Foreign Language", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.", "text": "syntactic constituency parsing fundamental problem natural language processing subject intensive research engineering decades. result accurate parsers domain speciﬁc complex inefﬁcient. paper show domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results widely used syntactic constituency parsing dataset trained large synthetic corpus annotated using existing parsers. also matches performance standard parsers trained small human-annotated dataset shows model highly data-efﬁcient contrast sequence-to-sequence models without attention mechanism. parser also fast processing hundred sentences second unoptimized implementation. syntactic constituency parsing fundamental problem linguistics natural language processing wide range applications. problem subject intense research decades result exist highly accurate domain-speciﬁc parsers. computational requirements traditional parsers cubic sentence length linear-time shift-reduce constituency parsers improved accuracy recent years never matched state-of-the-art. furthermore standard parsers designed parsing mind; concept parse tree deeply ingrained systems makes methods inapplicable problems. recently sutskever introduced neural network model solving general sequenceto-sequence problem bahdanau proposed related model attention mechanism makes capable handling long sequences well. models achieve state-of-the-art results large scale machine translation tasks syntactic constituency parsing formulated sequence-to-sequence problem linearize parse tree apply models parsing well. early experiments focused sequence-to-sequence model sutskever found model work poorly trained standard human-annotated parsing datasets constructed artiﬁcial dataset labelling large corpus berkeleyparser. suspected attention model bahdanau might data efﬁcient found indeed case. trained sequence-to-sequence model attention small human-annotated parsing dataset able achieve score section without ensemble ensemble matches performance berkeleyparser trained data. finally constructed second artiﬁcial dataset consisting high-conﬁdence parse trees measured agreement parsers. trained sequence-to-sequence model attention data achieved score section state-of-the-art. result require ensemble result parser also fast. ensemble improves score ﬁrst recall sequence-to-sequence lstm model. long short-term memory model deﬁned follows. input control state memory state timestep given sequence inputs lstm computes h-sequence m-sequence follows. matrix consists vector representations output symbol symbol kronecker delta dimension output symbol softmaxδbt precisely bt’th element distribution deﬁned softmax. every output sequence terminates special end-of-sequence token necessary order deﬁne distribution sequences variable lengths. different sets lstm parameters input sequence output sequence shown figure stochastic gradient descent used maximize training objective average training probability correct output sequence given input sequence. important extension sequence-to-sequence model adding attention mechanism. adapted attention model which produce output symbol uses attention mechanism encoder lstm states. similar sequence-to-sequence model described previous section separate lstms recall encoder hidden states denoted denote hidden states decoder compute attention vector output time input words deﬁne vector matrices learnable parameters model. vector length i-th item contains score much attention i-th hidden encoder state scores normalized softmax create attention mask encoder hidden states. experiments hidden dimensionality encoder decoder vector becomes hidden state make predictions next time step recurrent model. apply model described parsing need design invertible converting parse tree sequence simple following depth-ﬁrst traversal order depicted figure model parsing following way. first network consumes sentence left-to-right sweep creating vectors memory. then outputs linearized parse tree using information vectors. described below lstm layers reverse input sentence pos-tag normalization. since part-of-speech tags evaluated syntactic parsing score replaced training data. improved score point surprising standard parsers including tags training data helps signiﬁcantly. experiments reported performed normalized tags. input reversing. also found useful reverse input sentences parse trees similarly reversing input small negative impact score development experiments reported performed input reversing. pre-training word vectors. embedding layer vocabulary initialized randomly using pre-trained word-vector embeddings. pre-trained skip-gram embeddings size using wordvec b-word corpus. embeddings used initialize network ﬁxed later modiﬁed training. discuss impact pre-training experimental section. apply special preprocessing data. particular binarize parse trees handle unaries speciﬁc way. also treat unknown words naive words beyond vocabulary single token. potentially underestimates ﬁnal results keeps framework task-independent. experiments trained model described different datasets. trained standard training dataset. small training neural network standards contains sentences still even training managed results match obtained domain-speciﬁc parsers. exceed previous state-of-the-art created another larger training parsed sentences first collected publicly available treebanks. used ontonotes corpus version english treebank updated corrected question treebank note popular wall street journal section penn treebank part ontonotes corpus. total corpora give training sentences addition gold standard data corpus parsed existing parsers using tri-training approach approach parsers reimplementation berkeleyparser reimplementation zpar used process unlabeled sentences sampled news appearing web. select sentences parsers produced parse tree re-sample match distribution sentence lengths training corpus. re-sampling useful parsers agree much often short sentences. call million sentences selected together golden sentences described above high-conﬁdence corpus. earlier experiments used parser reimplementation berkeleyparser create corpus parsed sentences. case parsed million senteces news appearing treebanks available linguistic data consortium ontonotes first remark training setup differs reported previous works. best knowledge standard parsers ever trained datasets numbering hundreds millions tokens would hard efﬁciency problems. therefore cite semi-supervised results analogous spirit less data. table shows performance models results papers bottom. compare variants berkeleyparser self-training unlabeled data built ensemble multiple parsers combine techniques. also include best linear-time parser literature transition-based parser seen that training only baseline lstm achieve reasonable score even dropout early stopping. single attention model gets ensemble lstm+a+d models achieves matching single-model berkeleyparser trained large high-conﬁdence corpus single lstm+a model achieves outperforms best single model also best ensemble result reported previously. ensemble lstm+a models improves score generating well-formed trees. lstm+a model trained dataset produced malformed trees sentences development model trained full high-conﬁdence dataset sentences cases lstm+a outputs malformed tree simply brackets either beginning tree order make balanced. worth noting cases lstm+a produced unbalanced trees sentences sentence fragments proper punctuation. sentences training data surprise model cannot deal well. score sentence length. important concern sequence-to-sequence lstm able handle long sentences well. determine extent problem partitioning development length evaluating berkeleyparser baseline lstm model without attention lstm+a sentences length. results presented figure surprising. difference score sentences length upto upto berkeleyparser baseline lstm lstm+a. already baseline lstm similar performance berkeleyparser degrades length slightly. beam size inﬂuence. decoder uses beam ﬁxed size calculate output sequence labels. experimented different settings beam size. turns almost irrelevant. report report results beam size using beam size lowers score lstm+a development using beam size lowers beam sizes give additional improvements. dropout inﬂuence. used dropout training small dataset inﬂuence signiﬁcant. single lstm+a model achieved score development points lower lstm+a+d model. pre-training inﬂuence. described previous section initialized word-vector embedding pre-trained word vectors obtained wordvec. test inﬂuence initialization trained lstm+a model high-conﬁdence corpus lstm+a+d model corpus starting randomly initialized word-vector embeddings. score development lower lstm+a model lower lstm+a+d model effect pre-training consistent small. performance datasets. evaluation years commonly used compare syntactic parsers. representative text encountered even though model trained news corpus wanted check well generalizes forms text. evaluated additional datasets lstm+a trained high-conﬁdence corpus achieved score web. score higher best score reported best score achieved in-house reimplementation berkeleyparser trained human-annotated data managed achieve slightly higher score in-house berkeleyparser trained large corpus. score lstm+a also lower best score in-house berkeleyparser still taking account questions training data scores show lstm+a managed generalize well beyond news language trained parsing speed. lstm+a model running multi-core using batches sentences generic unoptimized decoder parse sentences second sentences lengths better speed reported batch size figure sentences second even though sentences words. note achieve score subset sentences section model beam-size achieves score subset. figure attention matrix. shown attention matrix column attention vector inputs. bottom show outputs four consecutive time steps attention mask moves right. seen every time terminal node consumed attention pointer moves right. shown paper attention mechanism component especially learning relatively small dataset. found model overﬁt learned parsing function scratch much faster resulted model generalized much better plain lstm without attention. interesting aspects attention allows visualize interpret model learned data. example shown translation attention learns alignment function certainly help translating english french. figure shows example attention model trained dataset. attention matrix column attention vector inputs clear model focuses quite sharply word produces parse tree. also clear focus moves ﬁrst word last monotonically steps right deterministically word consumed. bottom figure model attends current output decoded tree stack procedure learned data quite simple stack decoding. indeed input side model focuses position state information words worth noting that examples model skip words. task syntactic constituency parsing received tremendous amount attention last years. traditional approaches constituency parsing rely probabilistic context-free grammars focus approaches devising appropriate smoothing techniques highly lexicalized thus rare events carefully crafting model structure partially alleviate heavy reliance manual modeling linguistic structure using latent variables learn articulated model. however model still depends backbone thereby potentially restricted capacity. early neural network approaches parsing example also relied strong linguistic insights. introduced incremental sigmoid belief networks syntactic parsing. constructing model structure incrementally able avoid making strong independence assumptions inference becomes intractable. avoid complex inference methods propose recurrent neural network parse trees decomposed stack independent levels. unfortunately decomposition breaks long sentences accuracy longer sentences falls quite signiﬁcantly behind state-of-the-art. used tree-structured neural network score candidate parse trees. model however relies assumption furtherused score candidate trees rather full inference. lstm model signiﬁcantly differs models makes assumptions task. sequence-to-sequence prediction model somewhat related incremental parsing models pioneered extended linear time parsers however typically need task-speciﬁc constraints might build parse multiple passes. relatedly present excellent parsing results single left-to-right pass require stack explicitly delay making decisions parsing-speciﬁc transition strategy order achieve good parsing accuracies. lstm contrast uses short term memory model complex underlying structure connects input-output pairs. recently researchers developed number neural network models applied general sequence-to-sequence problems. ﬁrst propose differentiable attention mechanism general problem handwritten text synthesis although approach assumed monotonic alignment input output sequences. later introduced general attention model assume monotonic alignment applied machine translation applied model speech recognition. used convolutional neural network encode variable-sized input sentence vector ﬁxed dimension used produce output sentence. essentially model used successfully learn generate image captions. finally already experimented applying recurrent neural networks problem syntactic parsing. work shown generic sequence-to-sequence approaches achieve excellent results syntactic constituency parsing relatively little effort tuning. addition found model sutskever particularly data efﬁcient attention model bahdanau found highly data efﬁcient matched performance berkeleyparser trained small human-annotated parsing dataset. finally showed synthetic datasets imperfect labels highly useful models substantially outperformed models used create training data. suspect case different natures teacher model student model student model likely viewed teacher’s errors noise able ignore. approach successful obtained state-of-the-art result syntactic constituency parsing single attention model also means model exceedingly fast. work shows domain independent models excellent learning algorithms match even outperform domain speciﬁc models. acknowledgement. would like thank amin ahmad bikel jonni kanerva.", "year": 2014}