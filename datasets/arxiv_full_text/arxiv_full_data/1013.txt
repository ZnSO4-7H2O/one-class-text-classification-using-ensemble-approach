{"title": "A guide to convolution arithmetic for deep learning", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.", "text": "authors guide would like thank david warde-farley guillaume alain caglar gulcehre valuable feedback. likewise grateful helped improve tutorial helpful comments constructive criticisms code contributions. keep coming feedback welcomed best precise informative point possible anything feel might error could rephrased precise comprehensible please don’t refrain contacting likewise drop line think something might technical report would like discuss make best eﬀort update document. convolution matrix operation transposed convolution zero padding unit strides transposed zero padding unit strides transposed half padding transposed full padding transposed zero padding non-unit strides transposed zero padding non-unit strides transposed deep convolutional neural networks heart spectacular advances deep learning. although cnns used early nineties solve character recognition tasks current widespread application much recent work deep used beat state-of-the-art imagenet image classiﬁcation challenge convolutional neural networks therefore constitute useful tool machine learning practitioners. however learning cnns ﬁrst time generally intimidating experience. convolutional layer’s output shape aﬀected shape input well choice kernel shape zero padding strides relationship properties trivial infer. contrasts fully-connected layers whose output size independent input size. additionally cnns also usually feature pooling stage adding another level complexity respect fully-connected networks. finally so-called transposed convolutional layers employed work late relationship convolutional layers explained various degrees clarity. provide intuitive understanding relationship input shape kernel shape zero padding strides output shape convolutional pooling transposed convolutional layers. order remain broadly applicable results shown guide independent implementation details apply commonly used machine learning frameworks theano chapter brieﬂy reviews main building blocks cnns namely discrete convolutions pooling. in-depth treatment subject chapter deep learning textbook bread butter neural networks aﬃne transformations vector received input multiplied matrix produce output applicable type input image sound clip unordered collection features whatever dimensionality representation always ﬂattened vector transformation. images sound clips many similar kinds data intrinsic properties exploited aﬃne transformation applied; fact axes treated topological information taken account. still taking advantage implicit structure data prove handy solving tasks like computer vision speech recognition cases would best preserve discrete convolutions come play. figure provides example discrete convolution. light blue grid called input feature map. keep drawing simple single input feature represented uncommon multiple feature maps stacked onto another. kernel value slides across input feature map. location product element kernel input element overlaps computed results summed obtain output current location. procedure repeated using diﬀerent kernels form many output feature maps desired ﬁnal outputs procedure called output feature maps. multiple input feature maps kernel -dimensional equivalently feature maps convolved distinct kernel resulting feature maps summed elementwise produce output feature map. convolution depicted figure instance convolution generalized convolutions. instance convolution kernel would cuboid would slide across height width depth input feature map. note strides constitute form subsampling. alternative interpreted measure much kernel translated strides also viewed much output retained. instance moving kernel hops equivalent moving kernel hops retaining output elements example referred earlier channels images sound clips. while distinction convolution cross-correlation signal processing perspective become interchangeable kernel learned. sake simplicity stay consistent machine learning literature term convolution used guide. figure convolution mapping input feature maps three output feature maps using collection kernels left pathway input feature convolved kernel input feature convolved kernel results summed together elementwise form ﬁrst output feature map. repeated middle right pathways form second third feature maps three output feature maps grouped together form output. addition discrete convolutions themselves pooling operations make another important building block cnns. pooling operations reduce size feature maps using function summarize subregions taking average maximum value. pooling works sliding window across input feeding content window pooling function. sense pooling works much like discrete convolution replaces linear combination described kernel function. figure provides example average pooling figure pooling. analysis relationship convolutional layer properties eased fact don’t interact across axes i.e. choice kernel size stride zero padding along axis aﬀects output size axis that chapter focus following simpliﬁed setting deﬁning output size case number possible placements kernel input. let’s consider width axis kernel starts leftmost part input feature slides steps touches right side input. size output equal number steps made plus accounting initial position kernel logic applies height axis. factor zero padding let’s consider eﬀect eﬀective input size padding zeros changes eﬀective input size general case relationship used infer following relationship sometimes referred full padding setting every possible partial complete superimposition kernel input feature taken account. figure provides example relationships derived apply unit-strided convolutions. incorporating unitary strides requires another inference leap. facilitate analysis let’s momentarily ignore zero padding figure provides example again output size deﬁned terms number possible placements kernel input. let’s consider width axis kernel starts usual leftmost part input time slides steps size touches right side input. size output equal number steps made plus accounting initial position kernel logic applies height axis. before ﬂoor function means cases convolution produce output size multiple input sizes. speciﬁcally produce multiple input size output size. note ambiguity applies despite diﬀerent input sizes convolutions share output size. doesn’t aﬀect analysis convolutions complicate analysis case transposed convolutions. neural network pooling layers provide invariance small translations input. common kind pooling pooling consists splitting input patches outputting maximum value patch. kinds pooling exist e.g. mean average pooling share idea aggregating input locally applying non-linearity content patches readers noticed treatment convolution arithmetic relies assumption function repeatedly applied onto subsets input. means relationships derived previous chapter reused case pooling arithmetic. since pooling involve zero padding relationship describing general case follows need transposed convolutions generally arises desire transformation going opposite direction normal convolution i.e. something shape output convolution something shape input maintaining connectivity pattern compatible said convolution. instance might transformation decoding layer convolutional autoencoder project feature maps higher-dimensional space. again convolutional case considerably complex fully-connected case requires weight matrix whose shape transposed. however since every convolution boils eﬃcient implementation matrix operation insights gained fully-connected case useful solving convolutional case. like convolution arithmetic dissertation transposed convolution arithmetic simpliﬁed fact transposed convolution properties don’t interact across axes. take example convolution represented figure input output unrolled vectors left right bottom convolution could represented sparse matrix non-zero elements elements kernel using representation backward pass easily obtained transposing words error backpropagated multiplying loss operation takes -dimensional vector input produces -dimensional vector output connectivity pattern compatible construction. let’s consider would required around i.e. -dimensional space -dimensional space keeping connectivity pattern convolution depicted figure operation known transposed convolution. transposed convolutions also called fractionally strided convolutions deconvolutions work swapping forward backward passes convolution. note kernel deﬁnes convolution whether it’s direct convolution transposed convolution determined forward backward passes computed. instance although kernel deﬁnes convolution whose forward backward passes computed multiplying respectively also deﬁnes transposed convolution whose forward backward passes computed multiplying respectively. term deconvolution sometimes used literature advocate grounds deconvolution mathematically deﬁned inverse convolution diﬀerent transposed convolution. building introduced chapter proceed somebackwards respect convolution arithmetic chapter deriving properties transposed convolution referring direct convolution shares kernel deﬁning equivalent direct convolution. simplest think transposed convolution given input imagine input result direct convolution applied initial feature map. trasposed convolution considered operation allows recover shape initial feature map. let’s consider convolution kernel input unitary stride padding depicted figure produces output. transpose convolution output shape applied input. another obtain result transposed convolution apply equivalent much less eﬃcient direct convolution. example described could tackled convolving kernel input padded border zeros using unit strides shown figure notably kernel’s stride’s sizes remain same input transposed convolution zero padded. understand logic behind zero padding consider connectivity pattern transposed convolution guide design equivalent convolution. example left pixel input direct convolution contribute left pixel output right pixel connected right output pixel maintain connectivity pattern equivalent convolution necessary zero input ﬁrst application kernel touches top-left pixel i.e. padding equal size kernel minus one. note although equivalent applying transposed matrix visualization adds zero multiplications form zero padding. done illustration purposes ineﬃcient software implementations normally perform useless zero multiplications. knowing transpose non-padded convolution equivalent convolving zero padded input would reasonable suppose transpose zero padded convolution equivalent convolving input padded less zeros. half padding transposed applying inductive reasoning before reasonable expect equivalent convolution transpose half padded convolution half padded convolution given output size half padded convolution input size. thus following relation applies full padding transposed knowing equivalent convolution transpose non-padded convolution involves full padding unsurprising equivalent transpose fully padded convolution non-padded convolution figure transpose convolving kernel input padded border zeros using unit strides equivalent convolving kernel input padded border zeros using unit strides using kind inductive logic zero padded convolutions might expect transpose convolution involves equivalent convolution explained valid intuition transposed convolutions sometimes called fractionally strided convolutions. figure provides example helps understand fractional strides involve zeros inserted input units makes kernel move around slower pace unit strides. moment assumed convolution non-padded input size multiple case following relationship holds relationship convolution described whose input size multiple associated transposed convolution described size stretched input obtained adding zeros input unit output size zero padding non-unit strides transposed convolution’s input size multiple analysis extended zero padded case combining relationship relationship figure transpose convolving kernel input padded border zeros using strides equivalent convolving kernel input padded border zeros using unit strides relationship convolution described whose input size multiple associated transposed convolution described size stretched input obtained adding zeros input unit output size relationship convolution described associated transposed convolution described size stretched input obtained adding zeros input unit represents number zeros added bottom right edges input output size figure transpose convolving kernel input padded border zeros using strides equivalent convolving kernel input padded border zeros using unit strides readers familiar deep learning literature noticed term dilated convolutions appear recent papers. attempt provide intuitive understanding dilated convolutions. in-depth description understand contexts applied chen koltun dilated convolutions inﬂate kernel inserting spaces kernel elements. dilation rate controlled additional hyperparameter implementations vary usually spaces inserted kernel elements corresponds regular convolution. dilated convolutions used cheaply increase receptive ﬁeld output units without increasing kernel size especially eﬀective multiple dilated convolutions stacked another. concrete example oord proposed wavenet model implements autoregressive generative model audio uses dilated convolutions condition audio frames large context past audio frames. abadi agarwal barham brevdo chen citro corrado davis dean devin tensorﬂow largescale machine learning heterogeneous systems. software available tensorﬂow.org. shelhamer donahue karayev long girshick guadarrama darrell caﬀe convolutional architecture fast feature embedding. proceedings international conference multimedia pages acm. bottou bengio reading checks multilayer acoustics speech signal processing graph transformer networks. icassp-. ieee international conference volume pages ieee. long shelhamer darrell fully convolutional networks semantic segmentation. proceedings ieee conference computer vision pattern recognition pages saxe chen bhand suresh random weights unsupervised feature learning. getoor scheﬀer editors proceedings international conference machine learning icml pages york usa. acm. zeiler taylor fergus adaptive deconvolutional networks high level feature learning. computer vision ieee international conference pages ieee.", "year": 2016}