{"title": "Brain Tumor Segmentation with Deep Neural Networks", "tag": ["cs.CV", "cs.AI"], "abstract": "In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we've found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data.  We present a novel CNN architecture which differs from those traditionally used in computer vision. Our CNN exploits both local features as well as more global contextual features simultaneously. Also, different from most traditional uses of CNNs, our networks use a final layer that is a convolutional implementation of a fully connected layer which allows a 40 fold speed up. We also describe a 2-phase training procedure that allows us to tackle difficulties related to the imbalance of tumor labels. Finally, we explore a cascade architecture in which the output of a basic CNN is treated as an additional source of information for a subsequent CNN. Results reported on the 2013 BRATS test dataset reveal that our architecture improves over the currently published state-of-the-art while being over 30 times faster.", "text": "paper present fully automatic brain tumor segmentation method based deep neural networks proposed networks tailored glioblastomas pictured images. nature tumors appear anywhere brain almost kind shape size contrast. reasons motivate exploration machine learning solution exploits ﬂexible high capacity extremely eﬃcient. here give description diﬀerent model choices we’ve found necessary obtaining competitive performance. explore particular diﬀerent architectures based convolutional neural networks i.e. dnns speciﬁcally adapted image data. present novel architecture diﬀers traditionally used computer vision. exploits local features well global contextual features simultaneously. also diﬀerent traditional uses cnns networks ﬁnal layer convolutional implementation fully connected layer allows fold speed also describe -phase training procedure allows tackle diﬃculties related imbalance tumor labels. finally explore cascade architecture output basic treated additional source information subsequent cnn. results reported brats test dataset reveal architecture improves currently published state-of-the-art times faster. united states alone estimated cases brain cancer diagnosed gliomas common brain tumors less aggressive patient life expectancy several years aggressive patient life expectancy years. although surgery common treatment brain tumors radiation chemotherapy used slow growth tumors cannot physically removed. magnetic resonance imaging provides detailed images brain common tests used diagnose brain tumors. more brain tumor segmentation images great impact improved diagnostics growth rate prediction treatment planning. tentacle-like structures make diﬃcult segment. anfundamental diﬃculty segmenting brain tumors appear anywhere brain almost shape size. furthermore unlike images derived x-ray computed tomography scans scale voxel values images standardized. depending type machine used acquisition protocol tumorous cells drastically diﬀerent grayscale values pictured diﬀerent hospitals. healthy brains typically made types tissues white matter gray matter cerebrospinal ﬂuid. goal brain tumor segmentation detect location extension tumor regions namely active tumorous tissue necrotic tissue edema done identifying abnormal areas compared normal tissue. since glioblastomas inﬁltrative tumors borders often fuzzy hard distinguish healthy tissues. solution modality often employed e.g. t-contrasted proton density contrast imaging diﬀusion ﬂuid attenuation inversion recovery pulse sequences. conautomatic brain tumor segmentation methods handdesigned features methods implement classical machine learning pipeline according features ﬁrst extracted given classiﬁer whose training procedure aﬀect nature features. alternative approach designing task-adapted feature representations learn hierarchy increasingly complex features directly in-domain data. deep neural networks shown excel learning feature hierarchies work apply approach learn feature hierarchies adapted speciﬁcally task brain tumor segmentation combine information across modalities. speciﬁcally investigate several choices training convolutional neural networks deep neural networks adapted image data. report advantages disadvantages performance using well established metrics. although cnns ﬁrst appeared decades recently become mainstay computer vision community record-shattering performance imagenet large-scale visual recognition challenge cnns also successfully applied segmentation problems previous work focused non-medical tasks many involve architectures well suited medical imagery brain tumor segmentation particular. preliminary work using convolutional neural networks brain tumor segmentation together methods using cnns presented brats‘ workshop. however results incomplete required investigation paper propose number speciﬁc architectures tackling brain tumor segmentation. architectures exploit recent advances design training techniques maxout hidden units dropout regularization. also investigate several architectures take account local shape tumors well context. problem many machine learning methods perform pixel classiﬁcation without taking account local dependencies labels account this employ structured output methods conditional random ﬁelds inference computationally expensive. alternatively model label dependencies considering pixel-wise probability estimates initial additional input certain layers second forming cascaded architecture. since convolutions eﬃcient operations approach signiﬁcantly faster implementing crf. focus experimental analysis fully-annotated miccai brain tumor segmentation challenge dataset using well deﬁned training testing splits thereby allowing compare directly quantitatively wide variety methods. implements novel two-pathway architecture learns local details brain well larger context. also propose two-phase training procedure found critical deal imbalanced label distributions. details contributions described sections employ novel cascaded architecture eﬃcient conceptually clean alternative popular structured output methods. details models presented section noted menze number publications devoted automated brain tumor segmentation grown exponentially last several decades. observation underlines need automatic brain tumor segmentation tools also shows research area still work progress. generative models rely heavily domain-speciﬁc prior knowledge appearance healthy tumorous tissues. tissue appearance challenging characterize existing generative models usually identify tumor being shape signal deviates normal brain typically methods rely anatomical models obtained aligning image atlas template computed several healthy brains typical generative model brain images found prastawa given icbm brain atlas method aligns brain atlas computes posterior probabilities healthy tissues tumorous regions found localizing voxels whose posterior probability certain threshold. post-processing step applied ensure good spatial regularity. prastawa also register brain images onto atlas order probability abnormalities. active contour initialized iterated change posterior probability certain threshold. many active-contour methods along lines proposed depend left-right brain symmetry features and/or alignment-based features. note since aligning brain large tumor onto template challenging methods perform registration tumor segmentation time approaches brain tumor segmentation employ discriminative models. unlike generative modeling approaches approaches exploit little prior knowledge brain’s information learned cnns. produce smooth segmentation predictions regularized using global superpixel segmentation image. like work recent work exploited convolution operations ﬁnal layer network extend traditional architectures semantic scene segmentation medical imaging domain general comparatively less work using cnns segmentation. however notable recent work huang jain used cnns predict boundaries neural tissue electron microscopy images. explore approach similarities various approaches discussed above context brain tumor segmentation. since brains brats dataset lack resolution third dimension consider performing segmentation slice slice axial view. thus model processes sequentially axial image pixel associated diﬀerent image modalities namely; flair. like cnn-based segmentation models method predicts class pixel processing patch centered pixel. input model thus patch several modalities. main building block used construct architecture convolutional layer. several layers stacked forming hierarchy features. layer understood extracting features preceding layer hierarchy connected. single convolutional layer takes input stack input planes produces output number output planes feature maps. feature thought topologically arranged responses particular spatially local nonlinear feature extractor applied identically spatial neighborhood input planes sliding window fashion. case ﬁrst convolutional layer individual input planes correspond different modalities subsequent layers input planes typically consist feature maps previous layer. anatomy instead rely mostly extraction level image features directly modeling relationship features label given voxel. features input pixels values local histograms texture features gabor ﬁlterbanks alignment-based features interimage gradient region shape diﬀerence symmetry analysis classical discriminative learning techniques svms decision forests also used. results editions miccai-brats challenge suggest methods relying random forests among accurate common aspect discriminative models implementation conventional machine learning pipeline relying hand-designed features. methods classiﬁer trained separate healthy non-heatlthy tissues assuming input features suﬃciently high discriminative power since behavior classiﬁer independent nature features. diﬃculty methods based hand-designed features often require computation large number features order accurate used many traditional machine learning techniques. make slow compute expensive memory-wise. eﬃcient techniques employ lower numbers features using dimensionality reduction feature selection methods reduction number features often cost reduced accuracy. nature many hand-engineered features exploit generic edge-related information speciﬁc adaptation domain brain tumors. ideally would like features composed reﬁned higher-level taskadapted representations. recently preliminary investigations shown deep cnns brain tumor segmentation makes promising approach zikic urban three methods divide images patches train predict center pixel class. urban well zikic implemented fairly common consisting series convolutional layers non-linear activation function layer softmax output layer. work here extends preliminary results presented davy using two-pathway architecture building block. computer vision cnn-based segmentation models typically applied natural scene labeling. tasks inputs model channels patch color image. work pinheiro collobert uses basic make predictions pixel improves predictions using extra information input second model. work involves several distinct cnns processing image diﬀerent resolutions. ﬁnal per-pixel class prediction made integrating since convolutional layer associated input channels contains gray-scale values thus kernel contains weights. accordingly number parameters convolutional block consisting feature maps equal performed feature application diﬀerent -dimensional convolution ﬁlters plus bias term added pixel-wise resulting spatial position. though input operation -dimensional tensor spatial topology considered -dimensional axial plane original brain volume. whereas traditional image feature extraction methods rely ﬁxed recipe success convolutional neural networks ability learn weights biases individual feature maps giving rise data-driven customized task-speciﬁc dense feature extractors. parameters adapted stochastic gradient descent surrogate loss function related misclassiﬁcation error gradients computed eﬃciently backpropagation algorithm special attention must paid treatment border pixels convolution operation. throughout architecture employ so-called valid-mode convolution meaning ﬁlter response computed pixel positions less pixels away image border. ﬁlter convolved input patch result output figure thus note size kernels hyperparameters must speciﬁed user. non-linear activation function obtain features non-linear transformations input element-wise non-linearity applied result kernel convolution. multiple choices non-linearity sigmoid hyperbolic tangent rectiﬁed linear functions recently goodfellow proposed maxout nonlinearity shown particularly eﬀective modeling useful features. maxout features associated multiple kernels implies maxout associated feature maps os+k−}. note figure maxout maps associated feature maps. maxout feap determines pooling window size. sub-windows overlapping max-pooling operation shrinks size feature map. controlled pooling size stride hyper-parameter corresponds horizontal vertical increments pooling sub-windows positioned. stride value shape feature max-pooling. output max-pooling operation would size figure since max-pooling operation results output feature map. motivation operation introduce invariance local translations. subsampling procedure found beneﬁcial applications convolutional networks ability extract hierarchy increasingly complex features makes appealing. done treating output feature maps convolutional layer input channels subsequent convolutional layer. neural network perspective feature maps correspond layer hidden units neurons. speciﬁcally coordinate within feature corresponds individual neuron size receptive ﬁeld corresponds kernel’s size. kernel’s value also represents weights connections layer’s neurons neurons previous layer. often found practice learned kernels resemble edge detectors kernel tuned diﬀerent spatial frequency scale orientation appropriate statistics training data. finally perform prediction segmentation labels connect last convolutional hidden layer convolutional output layer followed non-linearity necessary note that segmentation purposes conventional yield eﬃcient test time since output layer typically fully connected. using convolution efﬁcient implementation prediction test time whole brain times faster. convolution uses many kernels diﬀerent segmentation labels kernel thus acts ultimate detector tissue segmentation labels. softmax nonlinearity normalizes result kernel convolutions noting segmentation label ﬁeld input patch thus interpret spatial position convolutional output layer providing model likelihood distribution label position probability labels simply taking product approach thus description cnns suggests simple architecture corresponding single stack several convolutional layers. conﬁguration commonly implemented architecture computer vision literature. however could imagine architectures might appropriate task hand. work explore variety architectures using concatenation feature maps diﬀerent layers anoperation composing cnns. operation allows construct architectures multiple computational paths serve diﬀerent purpose. describe types architectures explore work. two-pathway architecture architecture made streams pathway smaller receptive ﬁelds another larger receptive ﬁelds. refer streams local pathway global pathway respectively. motivation architectural choice would like prediction label pixel inﬂuenced aspects visual details region around pixel larger context i.e. roughly patch brain. full architecture along details illustrated figure refer architecture twopathcnn. allow concatenation hidden layers pathways layers local pathway kernels second layer. implies eﬀective receptive ﬁeld features layer pathway same global pathway’s parametrization directly ﬂexibly models features area. concatenation feature maps pathways output layer. disadvantage cnns described predict segmentation label separately other. unlike large number segmentation methods literature often propose joint model segmentation labels eﬀectively modeling direct dependencies between spatially close labels. approach deﬁne conditional random ﬁeld labels perform meanﬁeld message passing inference produce complete segmeninference joint segmentation methods typically computationally expensive simple feed-forward pass cnn. important aspect take account automatic brain tumor segmentation used day-to-day practice. here describe architectures exploit eﬃciency cnns also directly model dependencies adjacent labels segmentation. idea simple since we’d like ultimate prediction inﬂuenced model’s beliefs value nearby labels propose feed output probabilities ﬁrst additional inputs layers second cnn. again relying concatenation convolutional layers. case simply concatenate output layer ﬁrst layers second cnn. moreover two-pathway structure cnns. eﬀectively corresponds cascade cnns thus refer models cascaded architectures. input concatenation architecture provide ﬁrst cnn’s output directly input second cnn. thus simply treated additional image channels input patch. details illustrated figure refer model inputcascadecnn. architecture move layer local pathway perform concatenation ﬁrst hidden layer second cnn. details illustrated figure refer model localcascadecnn. last architecture move second perform concatenation right output layer. architecture interesting similar computations made pass mean-ﬁeld inference whose pairwise potential functions weights output kernels. view output ﬁrst ﬁrst iteration mean-ﬁeld output second would second iteration. diﬀerence regular mean-ﬁeld however allows output position inﬂuenced previous value convolutional kernels ﬁrst second cnn. details illustrated figure refer model mfcascadecnn. training gradient descent. interpreting output convolutional network model distribution segmentation labels natural training criteria maximize probability labels training equivalently minimize figure two-pathway architecture ﬁgure shows input patch going paths convolutional operations. feature-maps local global paths shown yellow orange respectively. convolutional layers used produce feature-maps indicated dashed lines ﬁgure. green embodies whole model later architectures used indicate twopathcnn. this follow stochastic gradient descent approach repeatedly selecting labels random subset patches within brain computing average negative log-probabilities mini-batch patches performing gradient descent step cnns parameters performing updates based small subset patches allows avoid process whole brain update providing reliable enough updates learning. practice implement approach creating dataset mini-batches smaller brain image patches paired corresponding center segmentation label target. improve optimization implemented so-called momentum strategy shown successful past idea momentum temporally averaged gradient order damp optimization velocity stands cnns parameters iteration gradient loss function integrated velocity initialized zero learning rate momentum coeﬃcient. deﬁne schedule momentum momentum coeﬃcient gradually increased training. experiments initial momentum coeﬃcient ﬁnal value two-phase training. brain tumor segmentation highly data imbalanced problem healthy voxels comprise total voxels. remaining pathological voxels belongs necrosis edema non-enhanced enhanced tumor selecting patches true distribution would cause model overwhelmed healthy patches causing problem training models. instead initially construct patches dataset labels equiprobable. call ﬁrst training phase. then second phase account un-balanced nature data re-train output layer representative distribution labels. best worlds capacity used balanced account diversity classes output probabilities calibrated correctly regularization. successful cnns tend models capacity making vulnerable overﬁtting setting like clearly enough training examples. accordingly found regularization important obtaining good results. here regularization took several forms. first layers bounded absolute value kernel weights applied regularization prevent overﬁtting. done adding regularization terms negative log-probability coeﬃcients regularization terms respectively). aﬀect parameters model diﬀerent ways encourages sparsity encourages small values. also used validation early stopping i.e. stop training validation performance stopped improving. validation also used tune hyper-parameters model. reader shall note hyper-parameters model includes using and/or coeﬃcients selected grid search range parameters. chosen hyper-parameters ones model performed best validation set. moreover used dropout recent regularization method works stochastically adding noise computation hidden layers cnn. done multiplying hidden input unit certain probability independently unit training update. encourages neural network learn cascaded architecture using pre-output concatenation architecture properties similar learning using limited number mean-ﬁeld inference iterations features useful since unit cannot assume units layer won’t masked well co-adapt behavior. test time units instead multiplied minus probability masked. details srivastava considering large number parameters model might think even regularization strategy training brains brats prevent overﬁtting. shown results section model generalizes well thus overﬁt. reason fact brain comes slices thus model approximately images train shall also mention nature images brains similar patient another. since variety images much lower realimage datasets cifar imagenet fewer number training samples thus needed. cascaded architectures. train cascaded architecture start training twopathcnn phase stochastic gradient descent procedure described previously. then parameters twopathcnn include cascaded architecture move training remaining parameters using similar procedure. noticed however spatial size ﬁrst cnn’s output layer second match must feed ﬁrst much larger input. thus training second must performed larger patches. example inputcascadecnn input size ﬁrst model size results output size case outputs ﬁrst concatenated input channels second cnn. implementation based pylearn library pylearn open-source machine learning library specializing deep learning algorithms. also supports gpus greatly accelerate execution deep learning algorithms. since cnn’s able learn useful features scratch applied minimal pre-processing. employed pre-processing tustison winner brats challenge pre-processing follows three steps. first highest lowest intensities removed. then apply nitk bias correction modalities. data normalized within input channel subtracting channel’s mean dividing channel’s standard deviation. post-processing simple method based connected components implemented remove blobs might appear predictions bright corners brains close skull. grid search cross-validation validation chosen hyper-parameters ones model performed best validation set. pooling always stride keep per-pixel accuracy full image prediction. observed practice pooling global path improve accuracy. also found adding additional layers architectures increasing capacity model adding additional feature maps convolutional blocks provide meaningful performance improvement. biases initialized zero except softmax layer initialized label frequencies. kernels randomly initialized training takes minutes epoch twopathcnn model nvidia titan black card. test time code order exploit computational speed. moreover convolutional nature output layer allows accelerate computations test time. done feeding input full image individual patches. therefore convolutions layers extended obtain label probabilities entire image. implementation able produce segmentation seconds brain titan black card twopathcnn model. turns times faster extracted patch pixel processed individually entire brain. experiments carried real patient data obtained brain tumor segmentation challenge part miccai conference brats dataset comprised sub-datasets. training dataset contains patient subjects pixelaccurate ground truth test dataset contains leaderboard dataset contains patient subjects ground truth provided test leaderboard datasets. brains dataset orientation. brain exists modalities namely flair coregistered. training brains come groundtruth segmentation labels provided namely non-tumor necrosis edema non-enhancing tumor enhancing tumor. figure shows example data well ground truth. total model iterates million examples tumorous patches goes million healthy patches. mentioned ﬁrst phase training distribution examples introduced model classes uniform. please note could brats dataset problems system performing evaluation quality labeled data. reasons figure ﬁrst four images left right show modalities used input channels various models ﬁfth image shows ground truth labels edema enhanced tumor necrosis non-enhanced tumor. represents model predictions represents ground truth labels. also note subset voxels predicted positives negatives tumor region question. similarly online evaluation system also provides ranking every method submitted evaluation. includes methods brats challenge published well anonymized unpublished methods reference available. section report experimental results diﬀerent architectures. brats dataset removed ofﬁcial website time submitting manuscript brats website still showed final data brats released soon. furthermore even conducted experiment trained model dataset made predictions test dataset; however performance worse results mentioned paper. reasons decided focus brats data. mentioned section work slices fact volumes dataset posses isotropic resolution spacing third dimension consistent across data. explored information didn’t improve performance made method slow. quantitative evaluation models performance test achieved uploading segmentation results online brats evaluation system online system provides quantitative results follows tumor structures grouped diﬀerent tumor regions. mainly practical clinical applications. described menze tumor regions deﬁned mentioned previously unlike conventional cnns twopathcnn architecture pathways local path focusing details global path focused context. better understand joint training global local pathways beneﬁts performance report results pathway well results averaging outputs pathway trained separately. method also deals unbalanced nature problem training phases discussed section impact phase training report results without refer model consisting local path localpathcnn model consisting global path globalpathcnn model averaging outputs local global paths averagecnn two-pathway architecture twopathcnn. second training phase noted appending architecture name. since second phase training substantial eﬀect always improves performance report results globalpathcnn averagecnn second phase. table presents quantitative results variations. table contains results twopathcnn training phases common single path training phases globalpathcnn* single path model following global pathway architecture output average trained single-pathway models without much surprise single path training phase ranked last lowest scores almost every region. using second training phase gave signiﬁcant boost model rank went also table shows joint training local global paths yields better performance compared pathway trained separately outputs averaged. likely explanation joint training local global paths model allows pathways co-adapt. fact averagecnn* performs worse localpathcnn* fact globalpathcnn* performs badly. performing method uncascaded models twopathcnn* rank also cases results less accurate enhancing region core complete regions. main reasons that. first borders usually diﬀused clear enhanced tumor nonenhanced tissues. creates problems user labeling figure randomly selected ﬁlters ﬁrst layer model. left right ﬁgure shows visualization features ﬁrst layer global local path respectively. features local path include edge detectors global path contains localized features. ground truth well model. second reason model learns sees ground truth. since labels created diﬀerent people since borders clear user slightly diﬀerent interpretation borders enhanced tumor sometimes overly thick enhanced tumor ground truth. figure shows representation level features local global paths. seen ﬁgure features local path include edge detectors ones global path localized features. unfortunately visualizing learned mid/high level features still much open research problem. however study impact features predictions visualizing segmentation results diﬀerent models. segmentation results subjects validation produced diﬀerent variations basic model viewed figure shown ﬁgure two-phase training procedure allows model learn realistic distribution labels thus removes false positives produced model trains training phase. moreover pathways model simultaneously learn global contextual features well local detailed features. gives advantage correcting labels global scale well recognizing details tumor local scale yielding better segmentation oppose single path architecture results smoother boundaries. joint training convolutional pathways training phases achieves better results. discuss experiments three cascaded architectures namely inputcascadecnn localcascadecnn mfcascadecnn. table provides quantitative results architecture. figure also provides visual examples segmentation generated architecture. figure visual results performing model inputcascadecnn* coronal sagittal views. subjects figure every sub-ﬁgure represents sagital view bottom represents coronal view. color codes follows edema enhanced tumor necrosis non-enhanced tumor. neurons softmax output layer directly connected previous outputs within receptive ﬁeld parameters likely learn center pixel label similar label surroundings. figure shows segmentation results brains sagittal coronal views. inputcascadecnn* model used produce results. seen ﬁgure although segmentation performed axial view output consistent coronal sagittal views. although subjects figure figure validation model trained segmentation results subjects give good estimate models performance test however clarity visualise models performance subjects brats- testst. results shown figure saggital axial views. better understand process inputcascadecnn* learns features present figure progression model making predictions every epochs subject validation set. figure progression learning inputcascadecnn*. stream ﬁgures ﬁrst left right show learning process ﬁrst phase. model learns better features better distinguish boundaries tumor sub-classes. made possible uniform label distribution patches ﬁrst phase training makes model believe classes equiprobable causes false positives. drawback alleviated training second phase distribution closer true distribution labels. color codes follows edema enhanced tumor necrosis non-enhanced tumor. table performance twopathcnn model variations. second phase training noted appending architecture name. ‘rank’ column represents ranking method online score board time submission. figure visual results architectures axial view. sub-ﬁgure left right shows modality conventional path conventional training phases twopathcnn model. second left right shows ground truth localcascadecnn model mfcascadecnn model inputcascadecnn. color codes follows edema enhanced tumor necrosis non-enhanced tumor. table performance cascaded architectures. reported results second phase training. ‘rank’ column shows ranking method online score board time submission. table shows implemented architectures compare currently published state-of-the-art methods mentioned table shows inputcascadecnn* performs tustison winner brats challenge ranked ﬁrst table. results brats- leaderboard presented table shows method outperforms approaches dataset. also compare performing method table state-of-the-art methods brats- label test mentioned seen table method performs methods tumor core category gets competitive results categories. mention tustison’s method takes minutes compute predictions brain reported inputcascadecnn* takes minutes thanks fully convolutional architecture implementation times faster winner challenge. twopathcnn* performance close state-of-the-art. however prediction time seconds times faster tustison’s method. methods table meier reza processing times minutes respectively. recently subbanna published competitive results brats dataset reporting dice measures complete core enhancing tumor regions. since report speciﬁcity sensitivity measures completely fair comparison method possible. however mentioned method takes minutes process subject times slower method. regarding methods using cnns urban used average convolutional networks dice measures complete core enhancing tumor regions brats test dataset prediction time minute model makes total minutes. again since report speciﬁcity sensitivity measures make full comparison. however based dice scores twopathcnn* similar perplease note results mentioned table table methods competing brats challenge static table provided since then methods added score board reference available. figure visual segmentation results performing model inputcascadecnn* examples brats test dataset saggital axial views. color codes follows edema enhanced tumor necrosis non-enhanced tumor. formance taking seconds four times faster. inputcascadecnn* better equal accuracy processing time. report results brats test dataset. however method similar localpathcnn which according experiments worse performance. using best performing method took part brats challenge. brats training dataset comprises subjects high grade subjects grade gliomas. subjects mixed high grade gliomas testing. every participating group hours receiving test subjects process submit segmentation results online evaluation system. brats’ contains training data ground truth rest training brains generated voted average segmented results performing methods brats’ brats’. automatically generated ground truths reﬁned manually user. distribution intensity values dataset variable subject another used fold cross validation training. test time voted average models made make prediction subject test dataset. results challenge presented figure semi-automatic methods participating challenge highlighted grey. please note since results publicly available refrain disclosing name participants. ﬁgure semi-automatic methods highlighted gray. seen ﬁgure method ranks either ﬁrst second complete tumor tumor core categories gets competitive results active tumor category. method also less outliers approaches. paper presented automatic brain tumor segmentation method based deep convolutional neural networks. considered diﬀerent architectures investigated impact performance. results brats online evaluation system conﬁrms best model managed improve currently published state-of-the-art method accuracy speed presented miccai figure brats’ challenge results using inputcascadecnn*. dice scores negative hausdorﬀ distances presented three tumor categories. since results challenge publicly available unable disclose name participants. semi-automatic methods highlighted gray. sub-ﬁgure methods ranked based mean value. mean presented green median outliers blue. high performance achieved help novel two-pathway architecture well modeling local label dependencies stacking cnn’s. training based phase procedure we’ve found allows train cnns eﬃciently distribution labels unbalanced. thanks convolutional nature models using eﬃcient implementation resulting segmentation system fast. time needed segment entire brain architectures varies seconds minutes making practical segmentation methods.", "year": 2015}