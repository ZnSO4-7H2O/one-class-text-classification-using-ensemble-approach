{"title": "A New Learning Paradigm for Random Vector Functional-Link Network: RVFL+", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "In school, a teacher plays an important role in various classroom teaching patterns. Likewise to this human learning activity, the learning using privileged information (LUPI) paradigm provides additional information generated by the teacher to 'teach' learning algorithms during the training stage. Therefore, this novel learning paradigm is a typical Teacher-Student Interaction mechanism. This paper is the first to present a random vector functional link network based on the LUPI paradigm, called RVFL+. Rather than simply combining two existing approaches, the newly-derived RVFL+ fills the gap between neural networks and the LUPI paradigm, which offers an alternative way to train RVFL networks. Moreover, the proposed RVFL+ can perform in conjunction with the kernel trick for highly complicated nonlinear feature learning, which is termed KRVFL+. Furthermore, the statistical property of the proposed RVFL+ is investigated, and we derive a sharp and high-quality generalization error bound based on the Rademacher complexity. Competitive experimental results on 14 real-world datasets illustrate the great effectiveness and efficiency of the novel RVFL+ and KRVFL+, which can achieve better generalization performance than state-of-the-art algorithms.", "text": "derive algorithm privileged information termed svm+ leverages strength lupi paradigm. thorough theoretical analysis svm+ illustrated previous work lupi paradigm focus aspects solving lupi based algorithms efﬁciently incorporating lupi paradigm various learning algorithms. paper focuses later. newly-derived rfvl+ however simpler optimization constraints svm+. result calculate closed-form solution rfvl+ naturally tackles former. optimization perspective formulation svm+ typical quadratic programming problem general problem solved optimization toolboxes however unnatural inconvenience train learning algorithm optimization toolboxes real-world applications. reason necessary derive efﬁcient approach solve pechyony presented smo-style optimization approach svm+. derived fast algorithms linear kernel svm+ respectively. addition solving svm+ efﬁciently lupi paradigm incorporated various learning algorithms feyereisl presented novel structured object localization uses attributes segmentation masks object privileged information. fouad provided generalized matrix approach based lupi paradigm. order tackle face veriﬁcation person re-identiﬁcation problems better used depth information rgb-d images privileged information derive novel distance metric learning algorithm. existing work conﬁrmed great effectiveness generality various lupi-based learning algorithms. nowadays neural network popular learning algorithms wave deep learning current deep learning methods neural networks including denoising auto-encoders convolutional neural networks deep belief networks long short-term memory etc. neural network methods achieved greatly successes various real-world applications including image classiﬁcation segmentation speech recognition natural language processing etc. therefore interesting combine neural networks lupi paradigm leverage strengths neural networks lupi paradigm. goal paper tackle open problem construct bridge link lupi paradigm neural networks. abstract—in school teacher plays important role various classroom teaching patterns. likewise human learning activity learning using privileged information paradigm provides additional information generated teacher ’teach’ learning algorithms training stage. therefore learning paradigm typical teacher-student interaction mechanism. paper ﬁrst present random vector functional link network based lupi paradigm called rvfl+. rather simply combining existing approaches newly-derived rvfl+ ﬁlls neural networks lupi paradigm offers alternative train rvfl networks. moreover proposed rvfl+ perform conjunction kernel trick highly complicated nonlinear feature learning termed krvfl+. furthermore statistical property proposed rvfl+ investigated derive sharp highquality generalization error bound based rademacher complexity. competitive experimental results real-world datasets illustrate great effectiveness efﬁciency novel rvfl+ krvfl+ achieve better generalization performance state-of-the-art algorithms. paradigm termed learning using privileged information aimed enhancing generalization performance learning algorithms. generally speaking classical supervised learning paradigm training data test data must come distribution. although learning paradigm training data also considered unbiased representation test data lupi provides additional information training data training stage called privileged information. lupi paradigm training containing privileged information train learning algorithm privileged information available test stage. note learning paradigm analogous human learning process. class teacher provide important helpful information course students information provided teacher help students acquire knowledge better. therefore teacher plays essential role human leaning process. lupi paradigm resembling classroom teaching model achieve better generalization performance traditional learning paradigm. paper derive novel random vector functional link network privileged information called rvfl+. random vector functional link network classical single layer feedforward neural network overcomes limitations slfns including slow convergence over-ﬁtting trapping local minimum. although rvfl achieved good generalization performance real-world tasks order improve effectiveness provide lupi paradigm rvfl. different existing variants rvfl rfvl+ open door towards alternative traditional learning paradigm rvfl real-world tasks. words rather using labeled training data training stage rvfl+ makes labeled training data also additional privileged information train rvfl interprets essential difference learning paradigms. moreover following kernel ridge regression derive kernel-based rvfl+ called krvfl+ order handle highly complicated nonlinear relationships high-dimensional data. krvfl+ major advantages rvfl+. hand random afﬁne transform leading unpredictability eliminated krvfl+. instead original privileged features mapped reproducing kernel hilbert space hand krvfl+ longer considers number enhancement nodes factor affect generalization ability. result performance krvfl+ terms effectiveness stability signiﬁcantly improved real-world tasks. furthermore investigate statistical property newly-derived rvfl+. provide tight generalization error bound based rademacher complexity rvfl+. generalization error bound beneﬁts advantageous properties rademacher complexity. rademacher complexity commonly-used powerful tool measure richness class real-valued functions terms inputs thus better capture properties distribution generates date. rvfl+ weights biases input layer enhancement nodes generated randomly ﬁxed output weights calculated moore-penrose pseudo-inverse ridge regression therefore rvfl+ considered ’special’ linear prediction. rademacher complexity ideal choice analysis type algorithms provide high-quality generalization error bound terms inputs. contrasts previous work provide tight general test error bound novel bound also applies various forms rvfl including newlyderived krvfl+. derive novel random vector functional link network privileged information called rvfl+. best knowledge rvfl+ ﬁrst bridge neural networks lupi paradigm. different existing variants rvfl newlyderived rvfl+ provides alternative paradigm train previous work focus aspects deriving efﬁcient solver lupi-based approaches combining lupi paradigm various learning algorithms. paper focus latter. however optimization perspective proposed rvfl+ sampler constraints svm+. result obtain closed-form solution rvfl+ naturally tackles former. paper also provide theoretical guarantee proposed rvfl+. according rademacher complexity derive tight input-dependent test error bound rvfl+ generalization error bound also applies various rvfls including proposed krvfl+. remainder paper organized follows. brief related work rvfl section section introduce newly-derived rvfl+ krvfl+. study statistical property rvfl+ provide novel tight generalization error bound based rademacher complexity section section conduct several experiments real-world datasets evaluate proposed rvfl+ krvfl+. paper concludes section three decades randomization based algorithms including random projection random forests bagging stochastic conﬁguration networks random vector functional link networks etc. play important roles machine learning community. refer great surveys randomized neural networks. rvfl presented ﬁrst popular single layer feedforward neural networks universal approximation ability great generalization performance. many researchers investigated numerous variants rvfl various domains three decades. chen presented novel algorithms functional-link network order calculate efﬁciently optimal weights update weights on-theﬂy receptively. chen presented novel single-hidden layer neural network structure rapidly calculate optimal weights. rvfl presented patra nonlinear dynamic systems. addition early studies rvfl gained attention researchers recent years. scardapane presented distributed rvfl algorithm order improve efﬁciency rvfl. scardapane derived rvfl full bayesian inference robust data modeling. used rvfl forecast short-term electricity load. built effective spatiotemporal user-deﬁned parameter. reasonable initial strategy overcome limitation rvfl random weights biases uniformly chosen respectively obtain optimal solution. nonlinear activation function sigmoid tanh etc. calculate directly output weights moore-penrose pseudo-inverse ridge regression shown respectively. model structure rvfl simple rvfl work well tasks? giryes give possible theoretical explanation open problem. provide explanation ﬁrst giryes reveal essence training learning algorithms. generally speaking angles samples different classes larger samples within class therefore role training learning algorithms ’the angles points different classes penalized angles points class’ moreover data highly complicated model architecture quick difﬁcult tune parameters leaning process needs extremely high computational cost. tackle problem randomization ideal choice learning algorithms leading cheaper computational cost. great random initialization allows learning algorithm universal training. revisit rvfl. rvfl uses hybrid strategy train entire network. rvfl random initial parameters input layer enhancement nodes handle well input samples distinguishable angles turned output weights deal remaining data. bartlett illustrates feedforward neural networks small empirical error smaller norm weights implies greater generalization performance. optimization perspective note basic idea rvfl network minimize training error output weights simultaneously emphasizes combination least square loss function tikhonov regularization. consequence rvfl achieve great generalization performance. order incorporate lupi paradigm following dual version ridge regression approach formulate rvfl model based kernel-based rvfl algorithm forecast distribution temperature. following basic idea rvfl chen proposed broad learning system dealing high-volume time-variety data provides alternative design architecture learning algorithms data era. rvfl network classical single layer feedforward neural network architecture rvfl shown figure rvfl initializes randomly weights biases input layer enhancement nodes parameters ﬁxed need tuned training stage. output weights solid lines figure calculated moore-penrose pseudoinverse ridge regression moreover direct link input layer output layer effective simple regularization technique preventing rvfl networks overﬁtting. learning paradigm train rvfl network. given additional privileged information training process training data becomes {|xi training original feature general privileged feature belongs privileged feature space different original feature space according ridge regression also impose additional term order avoid singularity guarantee stability rvfl+. result achieve aftermost closed-form solution rvfl+ regularization coefﬁcient. likewise also enhanced layer output vector corresponding privileged feature calculated fashion. correcting function privileged feature space output weight vector correcting function. note rvfl+ minimizes objective function therefore original features also privileged information determine meanwhile separating hyperplane rvfl+ training stage. moreover contrasts primal form svm+ correcting function rvfl+ either positive negative. words rvfl+ consider group constraints result number constraints rvfl+ least less svm+ binary classiﬁcation results rvfl+ much milder optimization constraints svm+. algorithm random vector functional link networks privileged information rvfl+ input training data {|xi nonlinear activation function number enhancement nodes user-speciﬁed coefﬁcients output output weight vector initialize randomly weights biases input layer enhancement nodes uniform distribution within respectively then generated weights biases ﬁxed; lagrange multipliers svm+. since sets lagrange multipliers need considered time makes optimization become difﬁcult addition constraint rvfl+ also eliminates ϕiyi svm+. result rvfl+ much simpler optimization constraints svm+ achieve closed-form solution. multiclass classiﬁcation adopt one-vs.-all strategy determine predicted label multiclass test output function classiﬁcation. k-th output nodes. predicted label test sample determined section derive kernel based random vector functional-link network privileged information major advantages rvfl+. krvfl+ longer considers number enhancement nodes krvfl+ maps input data reproducing kernel hilbert space order construct mercer kernel. hand krvfl+ much robust rvfl+. since krvfl+ perform random afﬁne transformation input layer enhancement nodes enhanced layer output matrix ﬁxed using kernel tricks. consequence according illustrate general mathematical formula generalization error bound based rademacher complexity following theorem. theorem generalization error bound empirical error bound respectively family functions. lipschitz continuous loss function bounded probability least samples theorem note generalization error bounded rademacher complexity. therefore according give rademacher complexity proposed rvfl+ serves bound generalization error rvfl+. following lemma helpful bounding rademacher complexity. lemma function σ-strongly convex respect itself rademacher complexity measure number family functions. convex duality show rademacher complexity bounded number inputs follows. theorem deﬁne subset dual space enhanced layer output vector space. -strongly convex respect itself. then rademacher complexity bounded algorithm kernel based random vector functional link networks privileged information krvfl+ input training data {|xi mercer kernel function user-speciﬁed coefﬁcients output output weight vector wkernel. calculate linear kernel functions corresection investigate statistical property rvfl+ consider binary classiﬁcation simplicity. following rademacher complexity provide tight generalization error bound rvfl+. assume sets considered paper measurable. first give following fact necessary condition rademacher complexity. novel generalization error bound dependent rademacher complexity thus give deﬁnition rademacher complexity. deﬁnition given i.i.d. samples family functions theorem lipschitz continuous loss function lipschitz constant rvfl+ probability least inputs length generalization risk minimization rvfl+ bounded section conduct several experiments evaluate proposed rvfl+ krvfl+ real-world datasets including binary classiﬁcation dataset multi-class classiﬁcation datasets regression datasets. trials algorithm carried average results reported. simulations carried matlab environment running machine inter core ram. dataset mnist+ dataset popular testbed used verifying performance algorithms based lupi. mnist+ handwritten digit recognition dataset consisting images digits training samples validation samples test samples mnist+. original -by- gray-scale images mnist resized -by- gray-scale images order increase challenge. mnist+ sample training test contains -dimensional attributes used main features. moreover additional privileged information mnist+ -dimensional texture features based data holistic description image. refer interested readers details. parameters selection rvfl+ determine empirically hyper-parameters well nonlinear activation function validation set. first select nonlinear activation function. table reports performance rvfl+ different activation functions terms accuracy. shown table rvfl+ triangular basis function outperforms others thus triangular basis function activation function case. however found exist general rule choosing activation function. therefore need determine activation function rvfl+ different tasks. user-deﬁned parameters chosen random search within also selected interval fashion. figure show parts results selecting parameters ignore numerous ones worse performance. shown figure respectively rvfl+ achieve best performance. therefore determine following comparison. additionally number enhancement nodes sufﬁcient large rvfl+ achieve good generalization performance. therefore balancing effectiveness efﬁciency determined experiments. furthermore found ’suitable’ improve around accuracy rvfl+ thus empirically determine optimal different tasks. experiment positive factor empirically space limitations omit procedures selecting kernel functions krvfl+. simulations commonly-used gaussian kernel function general mercer kernel kernel function always linear kernel function. user-speciﬁed kernel parameter well determined validation set. seen figure sufﬁcient large krvfl+ insensitive parameter. therefore krvfl+ experiments. figure illustrates performance krvfl+ different parameters found interval respectively rvfl+ achieve best performance case. approaches space limitations omit procedure selecting hyper-parameters. empirically determine user-speciﬁed parameters validation pick best performance. experimental results discussion compare state-of-the-art approaches mnist+ terms accuracy time including gsmo-svm+ cvx-svm+ mat-svm+ fast svm+ rvfl experimental results shown table krvfl+ best performance terms effectiveness efﬁciency. although proposed rvfl+ slightly worse kernel-based svm+ algorithm much better linear svm+. contrast experimental results illustrates beneﬁts lupi-based approaches accrue little additional computational cost. section compare fast svm+ matsvm+ rvfl real-world multi-class classiﬁcation datasets machine learning repository cover large range multi-class classiﬁcation tasks. previous work veriﬁed fast svm+ outperforms state-of-the-art lupi-based algorithms. avoid duplication algorithms included comparison. statistics classiﬁcation datasets illustrated table including number training test data attributes classes. white noise training test samples. original training data without white noise used privileged information lupi-based algorithms. normalization pre-process samples. two-fold cross validation shuttle ﬁve-fold abalone/red wine quality/white wine quality ten-fold rest datasets. found rvfl+ rvfl achieve best performance using sigmoid function nonlinear activation function experiment. space limitations report ﬁnal experimental results comparisons omit procedure selecting hyper-parameters pick best performance. experimental results reported table achieve best performance terms accuracy learning speed among comparisons. rvfl+ outperforms comparisons including krvfl+ white wine quality dataset. additionally likewise experiment performance algorithms lupi paradigm including krvfl+ rvfl+ fast svm+ mat-svm+ better remarkably rvfl. performance measured commonly-used root mean square error experimental results shown table experimental results illustrate advantage learning paradigm. although rvfl+ krvfl+ achieve sightly better performance slump small size datasets proposed algorithms distinct advantage size datasets scmd scmd. section compare rvfl real-world regression datasets cover different categories statistics regression datasets summarized table samples also added white noise original training data without white noise used privileged information. normalization pre-process data. two-fold cross validation scmd/scmd ten-fold rest datasets. also found rvfl+ rvfl achieve best performance using sigmoid function nonlinear activation function experiment. also omit procedure selecting user-speciﬁed parameters. neural networks lupi paradigm leverage beneﬁts both. signiﬁcantly rvfl+ offers learning paradigm rvfl network additional privileged information used ’teach’ network training stage. therefore note working mechanism newly-derived rvfl+ analogy teacher-student interaction human learning process. result rvfl+ achieve better generalization performance rvfl real-world tasks. addition optimization perspective rvfl+ less simpler optimization constraints svm+ results rvfl+ obtain closed-form solution. moreover rvfl+ perform conjunction kernel trick deﬁned krvfl+. novel krvfl+ powerful enough handle complicated nonlinear relationships high-dimensional input data. furthermore explore statistical property proposed rvfl+ establish tight generalization error bound based rademacher complexity rvfl+. competitive experimental results diverse real-world datasets conﬁrm efﬁciency effectiveness rvfl+ krvfl+ achieve better generalization performance state-ofthe-art algorithms. author would like thank prof. philip chen university macau valuable discussions random vector functional link network kernel ridge regression. author also would like thank prof. ajay joneja hong kong university science technology providing high-performance workstation constructive suggestions research. fouad tino raychaudhury schneider incorporating privileged information metric learning ieee transactions neural networks learning systems vol. distance metric learning using privileged information face veriﬁcation person re-identiﬁcation ieee transactions neural networks learning systems vol. igelnik y.-h. stochastic choice basis functions adaptive function approximation functional-link ieee transactions neural networks vol. zhang suganthan comprehensive evaluation random vector functional link networks information sciences vol. k.-k. h.-x. h.-d. yang kernel-based random vector learning spatiotemporal dynamic functional-link network fast processes ieee transactions systems cybernetics systems chen rapid learning dynamic stepwise updating algorithm neural networks application timeseries prediction ieee transactions systems cybernetics part vol. patra chatterji panda identiﬁcation nonlinear dynamic systems using functional link artiﬁcial neural networks ieee transactions systems cybernetics part vol. chen broad learning system effective efﬁcient incremental learning system without need deep architecture ieee transactions neural networks learning systems bartlett sample complexity pattern classiﬁcation neural networks size weights important size network ieee transactions information theory vol. suykens vandewalle least squares support vector machine classiﬁers neural processing letters vol. kakade sridharan tewari complexity linear prediction risk bounds margin bounds regularization advances neural information processing systems lecun bottou bengio haffner gradient-based learning applied document recognition proceedings ieee vol. c.-c. chang c.-j. libsvm library support vector machines transactions intelligent systems technology vol. software available http//www.csie.ntu. edu.tw/∼cjlin/libsvm.", "year": 2017}