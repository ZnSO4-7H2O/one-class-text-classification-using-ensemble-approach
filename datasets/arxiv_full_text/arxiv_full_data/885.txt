{"title": "Survey of Expressivity in Deep Neural Networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We survey results on neural network expressivity described in \"On the Expressive Power of Deep Neural Networks\". The paper motivates and develops three natural measures of expressiveness, which all display an exponential dependence on the depth of the network. In fact, all of these measures are related to a fourth quantity, trajectory length. This quantity grows exponentially in the depth of the network, and is responsible for the depth sensitivity observed. These results translate to consequences for networks during and after training. They suggest that parameters earlier in a network have greater influence on its expressive power -- in particular, given a layer, its influence on expressivity is determined by the remaining depth of the network after that layer. This is verified with experiments on MNIST and CIFAR-10. We also explore the effect of training on the input-output map, and find that it trades off between the stability and expressivity.", "text": "survey results neural network expressivity described paper motivates develops three natural measures expressiveness display exponential dependence depth network. fact measures related fourth quantity trajectory length. quantity grows exponentially depth network responsible depth sensitivity observed. results translate consequences networks training. suggest parameters earlier network greater inﬂuence expressive power particular given layer inﬂuence expressivity determined remaining depth network layer. veriﬁed experiments mnist cifar-. also explore effect training input-output trades stability expressivity. survey summarize results expressivity deep neural networks neural network expressivity looks architecture network affects properties resulting function. fundamental step better understanding neural networks much prior work area. many existing results rely comparing achievable functions particular network architecture compelling results also highlight limitations much existing work expressivity unrealistic assumptions sometimes made architectural shape e.g. exponentially large width networks often compared ability approximate speciﬁc function which isolation cannot result general conclusion. overcome this start analyzing expressiveness setting general hardcoded functions immediately related practice networks random initialization. mean conclusions independent speciﬁc weight settings understanding behavior random initialization provides natural baseline compare effects training trained networks summarize sections companion paper companion paper propagation riemannian curvature random networks studied developing mean ﬁeld theory approach quantitatively supports conjecture deep networks disentangle curved manifolds input space. results networks random initialization examine effect depth width network architecture expressive power random initialization three natural measures functional richness number transitions activation patterns dichotomies. precisely fully connected networks input dimension depth width studied weights bias randomly initialized gray ﬁrst layer only partitioning plane regions. center pane shows activation boundaries ﬁrst layers. inside every ﬁrst layer region second layer activation boundaries form different hyperplane arrangement. right pane shows activation boundaries ﬁrst three layers different hyperplane arrangements inside ﬁrst second layer regions. ﬁnal convex regions correspond different activation patterns network i.e. different linear functions. detail measures expressivity transitions counting neuron transitions introduced indirectly linear regions provides tractable method estimate non-linearity computed function. activation patterns transitions single neuron extended outputs neurons layers leading deﬁnition network activation pattern also measure non-linearity. network activation patterns directly show network partitions input space connections theory hyperplane arrangements figure dichotomies heterogeneity generic class functions particular architecture also measured counting number dichotomies seen ﬁxed inputs. measure ‘statistically dual’ sweeping input cases. paper shows three measures grow exponentially depth network width. connection trajectory length fact underlying connection three measures another quantity trajectory length curve input space changes length propagates network. proved trajectory length input grows exponentially depth network width theorem bound growth trajectory length hard tanh random neural network dimensional trajectory input space. deﬁne image also veriﬁed empirically theoretical intuition provided direct proportionality transitions activation patterns dichotomies trajectory length conﬁrmed experiments figure exponential growth trajectory length depth random deep network hard-tanh nonlinearities. circular trajectory chosen random vectors. image trajectory taken layer network length measured. trajectory length layer terms network width weight variance determine growth rate. average ratio trajectory’s length layer relative length layer solid line shows simulated data dashed lines show upper lower bounds growth rate function layer width weight variance figure number transitions linear trajectory length. compare empirical number sign changes length trajectory images trajectory different layers hard-tanh network. repeat comparison variety network architectures different network width weight variance paper explores effect training measures expressivity. importantly note exponential depth dependence demonstrated start training makes resulting function sensitive perturbations desired feature trained network. weights initialized large transitions training process figure training acts stabilize input-output decreasing trajectory length large. left pane plots growth trajectory length circular interpolation mnist datapoints propagated network different train steps. indicates start training purple training. interestingly supporting observation remaining depth ﬁrst layer appears increase trajectory length contrast later layers suggesting primarily used data. right pane shows identical plot interpolation random points also display decreasing trajectory length slower rate. note output layer plotted artiﬁcial scaling length normalization. network similar plot observed number transitions initialized figure training increases expressivity input-output small. left pane plots growth trajectory length circular interpolation mnist datapoints propagated network different train steps. indicates start training purple training. training process increases trajectory length likely increase expressivity input-output enable greater accuracy. right pane shows identical plot interpolation random points also displays increasing trajectory length slower rate. note output layer plotted artiﬁcial scaling length normalization. network initialized network initialized small however also potential adversely affect performance function initialization might offer enough expressiveness target. case training process monotonically increases trajectory length number transitions summary paper concludes training trades achieving enough expressiveness simultaneously trying maintain stability. expanding trajectory length suggests effect parameter choices earlier earlier layers ampliﬁed later layers. combining exponential increase dichotomies depth suggests expressive power parameters thus layers related remaining depth network layer. paper demonstrates practice experiments mnist cifar- figure demonstration expressive power remaining depth mnist. plot train test accuracy achieved training exactly layer fully connected neural mnist. different lines generated varying hidden layer chosen train. layers kept frozen random initialization. references maithra raghu poole kleinberg surya ganguli jascha sohl-dickstein. expressive power deep neural networks. arxiv e-prints june https//arxiv.org/abs/.. kurt hornik maxwell stinchcombe halbert white. multilayer feedforward networks universal james martens arkadev chattopadhya toni pitassi richard zemel. representational efﬁciency restricted boltzmann machines. advances neural information processing systems pages monica bianchini franco scarselli. complexity neural network classiﬁers comparison shallow deep architectures. neural networks learning systems ieee transactions poole subhaneil lahiri maithra raghu jascha sohl-dickstein surya ganguli. exponential expressivity deep neural networks transient chaos. advances neural information processing systems pages", "year": 2016}