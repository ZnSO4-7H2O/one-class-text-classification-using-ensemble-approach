{"title": "Feature Engineering for Predictive Modeling using Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Feature engineering is a crucial step in the process of predictive modeling. It involves the transformation of given feature space, typically using mathematical functions, with the objective of reducing the modeling error for a given target. However, there is no well-defined basis for performing effective feature engineering. It involves domain knowledge, intuition, and most of all, a lengthy process of trial and error. The human attention involved in overseeing this process significantly influences the cost of model generation. We present a new framework to automate feature engineering. It is based on performance driven exploration of a transformation graph, which systematically and compactly enumerates the space of given options. A highly efficient exploration strategy is derived through reinforcement learning on past examples.", "text": "hour given timestamp feature helps capture certain trends peak versus non-peak demand. note valuable features derived composition multiple simpler functions. perhaps central task improving predictive modeling performance documented detailed account performers various kaggle competitions practice orchestrated data scientist using hunch intuition domain knowledge based continuously observing reacting model performance trial error. result often timeconsuming prone bias error. inherent dependence human decision making colloquially referred art/science making difﬁcult automate. existing approaches automate either computationally expensive evaluation-centric and/or lack capability discover complex features. present novel approach automate based reinforcement learning involves training agent examples learn effective strategy exploring available choices given budget. learning application exploration strategy performed feature engineering crucial step process predictive modeling. involves transformation given feature space typically using mathematical functions objective reducing modeling error given target. however well-deﬁned basis performing effective feature engineering. involves domain knowledge intuition lengthy process trial error. human attention involved overseeing process signiﬁcantly inﬂuences cost model generation. present framework automate feature engineering. based performance driven exploration transformation graph systematically compactly enumerates space given options. highly efﬁcient exploration strategy derived reinforcement learning past examples. predictive analytics widely used support decision making across variety domains including fraud detection marketing drug discovery advertising risk management amongst several others. predictive models constructed using supervised learning algorithms classiﬁcation regression models trained historical data predict future outcomes. underlying representation data crucial learning algorithm work effectively. cases appropriate transformation data essential prerequisite step model construction. instance figure depicts different representations points belonging classiﬁcation problem dataset. left instances corresponding classes present alternating small clusters. machine learning algorithms hard draw reasonable classiﬁer representation separates classes hand feature replaced sine seen image right makes classes reasonably separable classiﬁers. task process altering feature representation predictive modeling problem better training algorithm called feature engineering sine function instance transformation used perform consider schema dataset forecasting hourly bike rental demand figure deriving several features dramatically reduces modeling error. instance extracting transformation graph directed acyclic graph representing relationships different transformed versions data. best knowledge ﬁrst work learns performance-guided strategy effective feature transformation historical instances. also work space provides adaptive budget constrained solution. finally output features compositions well-deﬁned mathematical functions make human readable usable insights predictive analytics problem illustrated figure given supervised learning dataset ficus performs beam search space possible features constructing features applying constructor functions ficus’s search better features guided heuristic measures based information gain decision tree surrogate measures performance. constrast approach optimizes prediction performance criterion directly rather surrogate criteria require constructor functions. note ficus general number less recent approaches propose fctree uses decision tree partition data using original constructed features splitting points ficus fctree uses surrogate tree-based information-theoretic criteria guide search opposed true prediction performance. fctree capable generating simple features capable composing transformations i.e. search smaller space approach. also propose weight update mechanism helps identify good transformations dataset used frequently. deep feature synthesis component data science machine relies applying transformations features performing feature selection model hyper-parameter optimization combined augmented dataset. similar approach adopted button machine call category expansion-reduction approach. approach suffers performance performance scalability bottleneck performing feature selection large number features explicitly generated simultaneous application transforms. spite expansion explicit expansion feature space consider composition transformations. feadis relies combination random feature generation feature selection. adds constructed features greedily requires many expensive performance evaluations. related work explorekit expands feature space explicitly. employs learning rank newly constructed features evaluating promising ones. approach scalable expand-select type still limited explicit expansion feature space hence time-consuming. instance reported results obtained running days moderately sized datasets. complex nature method consider compositions transformations. refer approach evolution-centric. cognito introduces notion tree-like exploration transform space; present simple handcrafted heuristics traversal strategies breadth-ﬁrst depth-ﬁrst search capture several factors adapting budget constraints. paper generalizes concepts introduced there. proposes learning based method predict likely useful transformation feature. considers features independent other; demonstrated work classiﬁcation allow composition transformations. plausible approaches hyper-parameter optimization transformation choice could parameter black-box optimization strategies bayesian optimization ones modelfeature-selection best knowledge approaches employed solving employ genetic algorithm determine suitable transformation given data limited single transformations. certain methods perform level indirectly.a recent survey topic appears found dimensionality reduction methods principal component analysis non-linear variants mapping input dataset lower-dimensional space fewer features.such methods also known embedding methods kernel methods support vector machines class learning algorithms kernel functions implicitly input feature space higher-dimensional space. multi-layer neural networks allow useful features learned automatically minimize training loss function. deep learning methods made remarkable successes various data video image speech manual tedious. however deep learning methods require massive amounts data avoid overﬁtting suitable problems instances small medium sizes quite common. additionally deep learning mostly successful video image speech natural language data whereas general numerical types data encompasses wide variety domains need technique domain model independent works generally irrespective scale data. also features learned deep network always easily explained limiting application domains healthcare contrary features generated algorithm compositions well-understood mathematical functions analyzed domain expert. automation challenging computationally well terms decision-making. first number possible features constructed unbounded since transformations composed i.e. applied repeatedly features generated previous transformations. order conﬁrm whether feature provides value requires training validation model upon including feature. expensive step infeasible perform respect newly constructed feature. evolution-centric approaches described related work section operate manner take days complete even moderately-sized datasets. unfortunately reusability results evaluation trial another. hand expansion-reduction approach performs fewer training-validation attempts ﬁrst explicitly applying transformations followed feature selection large pool features. presents scalability speed bottleneck itself; practice restricts number features considered. cases lack performance oriented search. insights proposed framework performs systematic enumeration space choices given dataset transformation graph. nodes represent different versions given datasets obtained application transformation functions transformation applied dataset applies function possible features produces multiple additional features followed optional feature selection training-evaluation. therefore batches creation features transformation function. lies somewhat middle evolution-centric expansion-reduction approaches. provides computation advantage also logical unit measuring performance various transforms used composing different functions performance-oriented manner. translates problem ﬁnding node transformation graph highest cross-validation performance exploring graph little possible. also allows composition transformation functions. transformations based performance given dataset even based past experience; whether explore different transformations exploit combinations ones shown promise thus dataset hard articulate notions rules basis decisions; hence recognize factors involved learn strategy function factors order perform exploration automatically. reinforcement learning examples variety datasets optimal strategy. based transformation graph. resultant strategy policy maps instance transformation graph action applying transformation particular node graph. notation problem description consider predictive modeling task consisting features fm}; target vector pair speciﬁed dataset nature whether categorical continuous describes classiﬁcation regression problem respectively. applicable choice learning algorithm measure performance signify cross-validation performance model constructed given data using algorithm performance measure additionally consider transformation functions disposal tk}. application transformation features suggests application corresponding function valid input feature subsets applicable instance square transformation applied features eight numerical categorical features produce eight output features square∀fi extends k-ary functions work input features. derived feature recognized entire derived features denoted operator feature sets union feature sets preserving order. generally transformations features; hand feature selection operator transformation algebraic notation removes features. note operations speciﬁed feature exchangeably written corresponding dataset implied operation applied corresponding feature set. also binary implied target common across operands result. goal feature engineering stated follows. given features target features feature maximize modeling accuracy given algorithm measure graph exploration budget constraint emphasized exhaustive exploration transformation graph option given massive potential size. instance transformations height complete graph contains million nodes; exhaustive search would imply many model training testing iterations. hand known property allows deterministically verify optimal solution subset trials. hence focus work performance driven exploration policy maximizes chances improvement accuracy within limited time budget. exploration transformation graph begins single node grows node time current state graph. beginning reasonable perform exploration environment i.e. stumble upon transforms signal improvement. time desirable reduce amount exploration focus exploitation. algorithm outlines general methodology exploration. step estimated reward possible move used rank options actions available given state transformation graph gi∀i bmax overallocated budget number steps. note algorithm allows different exploration strategies left deﬁnition function deﬁnes relative importance different steps step. parameters function suggest depends various aspects graph point remaining budget speciﬁcally attributes action characterized. below brieﬂy discuss factors inﬂuence exploration choice step. factors compared across choices node-transformation pairs figure example transformation graph start node corresponds given dataset; hierarchical nodes circular. nodes rectangular. example three transformations square well feature selection operator transformation graph given dataset ﬁnite transformations directed acyclic graph node corresponds either dataset derived transformation path. every node’s dataset contains target number rows. nodes divided three categories start root node corresponding given dataset; hierarchical nodes incoming node parent node connecting edge corresponds transform i.e. nodes result dataset similarly edges correspond either transforms operations children type type nodes respectively. direction edge represents application transform source target dataset height transformation graph refers maximum distance root node. transformation graph illustrated figure node transformation graph candidate solution problem equation also complete transformation graph must contain node solution problem certain combination transforms including feature selection. operator signiﬁes nodes graph signiﬁes hierarchical nodes. also signiﬁes transformation application created child; alternatively node parents complete transformation graph unbounded nonempty transformation set. constrained complete transformation graph transformations hierarchical nodes nodes seen datetime features; string features? simple graph traversal strategies handcrafted. strategy essentially translates design reward estimation function line cognito breadth-ﬁrst depth-ﬁrst strategy perhaps described. simplistic strategies work suitably speciﬁc circumstances seems hard handcraft uniﬁed strategy works well various circumstances. instead turn machine learning learn complex strategy several historical runs. discussed hierarchical organization choices transformation graph general algorithm explore graph budget allowance. heart algorithm function estimate reward action state. design reward estimation function determines strategy exploration. strategies could handcrafted; however section learn optimal strategy examples several datasets transformation graph exploration. behavioral nature problem perceived continuous decision making interacting environment discrete steps observing reward notion ﬁnal optimization target modeled problem. interested learning action-utility function satisfy expected reward function algorithm absence explicit model environment employ q-learning function approximation large number states infeasible learn state-action transitions explicitly. consider graph exploration process markov decision process state step combination components transformation graph node additions remaining budge step i.e. bratio entire states hand action step pair existing tree node transformation i.e. gi∀λ) signiﬁes application transform exiting nodes graph. entire actions policy determines action taken given state. note objective learn optimal policy learning action-value function elaborate later section. formulation uniquely identiﬁes state. considering remaining budget factor state helps address runtime exploration versus exploitation trade-off given dataset. note runtime explore/exploit trade-off identical commonly referred trade-off training context selecting actions balance reward getting stuck local optimum. step occurrence action results node hence dataset model trained tested accuracy obtained. step attribute immediate scalar reward weight vector action vector state characteristics described previous section remaining budget ratio. therefore approximate q-functions linear combinations characteristics state mdp. note that heuristic rule strategy used subset state characteristics self-conceived manner. however based approach here select entire characteristics appropriate weights characteristics hence approach generalizes handcrafted approaches. method reduces space coefﬁcients learnt factor makes faster learn weights. important note q-function still independent action factors actually average immediate reward transform present dataset. hence equation based approximation still distinguishes various actions based performance transformation graph exploration far; however learn bias different transformations general based feature types refer type strategy experiments efﬁciency somewhat inferior strategy strategy learned equation refer training training examples maximum budget bmax dataset random order. used discount factor learning rate parameter weight vectors size initialized training example steps drawn randomly probability current policy probability used following transformation functions general square square root product zscore min-max-normalization timebinning aggregation temporal window aggregate spatial aggregation spatio temporal aggregation k-term frequency difference division sigmoid binningu binningd nominalexpansion tanh. comparison tested impact publicly available datasets variety domains various sizes. report accuracy base dataset; routine bmax expansion-reduction implementation transformations ﬁrst applied separately original columns followed feature selection routine; random randomly applying transform function random feature adding result original dataset measuring performance; repeated times ﬁnally consider features whose cases showed improvement performance along original features train model tree-heur implementation cognito’s global search heuristic nodes. used random forest default weka parameters learning algorithm comparisons gave strongest baseline average. -fold cross validation using random stratiﬁed sampling used. results representative captured table seen outperforms others cases tied cognito global search. technique reduces error datasets presented table reference time taken took bikeshare dataset minutes seconds nodes single thread .ghz processor. times taken random cognito similar datasets expand-reduce took times time different datasets. traversal policy comparison figure average datasets rl-based strategies times efﬁcient handcrafted strategy ﬁnding optimal dataset given graph transformations bounded height hmax also figure tells takes data train efﬁcient demonstrating learning general bias transformations conditioned data types makes exploration efﬁcient. dataset ucirvine higgs boson kaggle amazon employee ucirvine pimaindian ucirvine spectf libsvm svmguide ucirvine german credit kaggle bikeshare ucirvine housing boston ucirvine airfoil openml ap-omentum-ovary ucirvine lymphography ucirvine ionosphere openml openml openml openml openml openml openml openml openml openml openml openml openml openml ucirvine credit default ucirvine messidor features wine quality ucirvine wine quality white ucirvine spambase ucirvine table comparing performance base dataset expansion-reduction style random tree heuristic using datasets. performance fscore classiﬁcation regression exploration cost higher depth. also using feature selection transform improves ﬁnal gain performance measured datasets finally different models lead different optimal features engineered dataset even similar improvements performance. paper presented novel technique efﬁciently perform feature engineering supervised learning problems. cornerstone framework transformation graph enumerates space feature options rl-based performance-driven exploration available choices valuable features. models produced using proposed technique considerably reduce error rate across variety datasets relatively small computational budget. methodology potentially save data analyst hours weeks worth time. direction improve efﬁciency system complex non-linear modeling state variables. additionally extending described framework aspects predictive modeling missing value imputation model selection potential interest well. since optimal features depend model type joint optimization particularly interesting. internal system comparisons additionally performed experimentation test tune internals system. figure shows maximum accuracy node found height constrained different numbers using bmax nodes; hmax signiﬁes base dataset. majority datasets maxima hmax hmax hmax tiny fraction shows deterioration interpreted unsuccessful", "year": 2017}