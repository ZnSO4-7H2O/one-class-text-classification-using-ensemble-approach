{"title": "Pillar Networks++: Distributed non-parametric deep and wide networks", "tag": ["cs.CV", "cs.NE", "stat.CO", "stat.ML"], "abstract": "In recent work, it was shown that combining multi-kernel based support vector machines (SVMs) can lead to near state-of-the-art performance on an action recognition dataset (HMDB-51 dataset). This was 0.4\\% lower than frameworks that used hand-crafted features in addition to the deep convolutional feature extractors. In the present work, we show that combining distributed Gaussian Processes with multi-stream deep convolutional neural networks (CNN) alleviate the need to augment a neural network with hand-crafted features. In contrast to prior work, we treat each deep neural convolutional network as an expert wherein the individual predictions (and their respective uncertainties) are combined into a Product of Experts (PoE) framework.", "text": "plenty work utilising variety network architectures factorising optical-ﬂow based features. example inception network uses convolutions inception block estimate crosschannel corrections followed estimation cross-spatial cross-channel correlations. residual network hand learns residuals inputs instead learning unreferenced functions frameworks proven useful many action recognition datasets show promise videos varying signal-to-noise ratio viewing angles etc. improve upon existing technology combining inception networks resnets using gaussian process classiﬁer combined product-of-expert framework yield best knowledge state-of-the-art performance hmdb data-set bayesian setting pillar networks provide mean predictions also uncertainty associated prediction. notably work forwards following contributions introduce pillar networks++ allow independent multi-stream deep neural networks enabling horizontal scalability section describe dataset network architectures nonparametric bayesian setup utilise four-stream pillar network activity recognition. refer readers original network architectures technical details. utilising classiﬁcation methodologies like adaboost gradient boosting random forests etc. provide accuracies range dataset either optic-ﬂow based features. recent work shown combining multi-kernel based support vector machines lead near state-of-the-art performance action recognition dataset lower frameworks used hand-crafted features addition deep convolutional feature extractors. present work show combining distributed gaussian processes multi-stream deep convolutional neural networks alleviate need augment neural network hand-crafted features. contrast prior work treat deep neural convolutional network expert wherein individual predictions combined product experts framework. recognizing actions video stream requires aggregation temporal well spatial features video streams unlike still images short long temporal correlations attributes single frame convolutional neural networks fail discover. therefore ﬁrst hurdle reach human-level performance designing feature extractors learn latent temporal structure. nonetheless much progress devising novel neural network architecture since work another problem large compute storage memory requirement analysing moderately sized video snippets. requires relatively larger computing resource train ultra deep neural networks learn subtleties temporal correlations given varying lighting camera angles pose etc. also difﬁcult utilise standard image augmentation techniques video stream. additionally features video stream evolve dynamics across several orders time-scales. nonetheless action recognition problem reached sufﬁcient maturity using two-stream deep convolutional neural networks framework framework utilises deep convolutional neural network extract static features well motion cues deconstructing optic-ﬂow given video clip. notably hmdb dataset action classiﬁcation dataset comprises video clips divided action classes. although much larger ucf-sports dataset exists action classes hmdb proven challenging. video ﬁlmed using variety viewpoints occlusions camera motions video quality etc. anointing challenges video-based prediction problems. second motivation behind using dataset lies fact hmdb storage compute requirement fulﬁlled modern workstation gpus alleviating deployment expensive cloud-based compute resources. inception layer architecture described video divided segments short sub-segment randomly selected segment preliminary prediction produced snippet. later combined form video-level prediction. inception batch normalisation network utilised spatial optic-ﬂow stream. feature size inception network ﬁxed details network pretraining construction etc. please refer utilise network architecture proposed authors leverage recurrent networks convolutions temporally constructed feature matrices shown fig. instantiation truncate network yield features different features feed lstm network. spatial stream network takes images input resnet- feature extractor; resnet- spatial-stream convnet pre-trained imagenet dataset. temporal stream stacks optical images using pretraining protocol suggested feature size resnet network ﬁxed details network pre-training construction etc. please refer gaussian processes emerged ﬁltering theory non-parametric bayesian statistics work done geostatistics simply collection random variables joint gaussian distribution prior hyperprior where kernel function parameterized parameter observation model; latent function evaluated i.e. features. denotes class input features denote hyper-parameters. product experts neural network subdivide training sub-sets different could trained giving deep networks trained ﬁrst part training. assume independent marginal likelihood product expert becomes implements different non-linear transformation utilise learn deep different features. utilising distributed-gp architecture enables parcellate feature tensors computable chunks input gaussian process classiﬁer. architectural choice therefore enables scale horizontally plugging variety networks requirement. used architecture video based classiﬁcation wide range problems apply methodology speech processing natural-languageprocessing ultra deep convolutional networks inﬂuential variety problems image classiﬁcation natural language processing recently work combining inception network residual network resulting network builds advantages offered either network isolation future would useful different features extracted inception module resnet module combination both. this wide variety hand-crafted features also augmented inputs distributed gps; initial experiments using features show indeed case. input data also augmented using difference optic warps done also second stage training i.e. classiﬁers work fewer examples deep learning network requires. would useful pillar networks perform immensely large datasets youtube-m data-set additionally recently published kinetics human action video dataset deepmind equally attractive pre-training pillar networks dataset ﬁnegrained training hmdb- invariably increase accuracy current network architecture. bayesian product-of-gps would suffer problem increase number experts. because precision experts adds leads overconﬁdent predictions especially absence data. unpublished work utilised generalised product experts bayesian committee machine increase ﬁdelity predictions. would reported subsequent publication along results robust bayesian committee machine includes product-of-gps special cases inference limited experiments laplace approximation inference distributed framework. alternative inference methodology multiclass classiﬁcation include expectation propagation variational approximations experience variational optimisation dynamical probabilistic graphical models merit using free-energy minimization simply lower computational overhead. indeed comes done reduce computational expenditure notice unlike inducing inputs variational parameters distributed require optimisation additional parameters. finally product-of-gp-experts instantiated predicts function test point used videos hmdb training dataset; split seven sub-sets videos. select videos randomly chosen category sub-set non-overlapping. based seven sub-sets seven trained different features different networks resnet-lstm total twenty-eight generated. features optical extracted last connected layer dimension inception network resnet network. fusion performed vertically horizontally accuracies individual different fusion combinations split- shown table fusion- represents results fusion seven feature; fusion- show fusion result flow using different networks; fusion-all shows result fusion gps. additionally results support-vector-machine network fusion using multi-kernel-learning listed last three rows average result three splits displayed table here make contributions build recently proposed pillar networks combine deep convolutional neural networks nonparametric bayesian models wherein possibility trained less amount data demonstrate utility model averaging takes uncertainty around mean predictions account. combining different methodologies allow supersede current state-ofthe-art video classiﬁcation especially action recognition. utilised hmdb- dataset instead former proven difﬁcult deep networks heterogeneity image quality camera angles etc. well-known videos contain extensive long-range temporal structure; using different networks capture subtleties temporal structure absolute requirement. since network figure pillar network++ framework pillar represents either single ultra-deep neural network feature tensors learnt automatically input data. action recognition factorize static dynamic features using resnet inception network. using features last layer train seven gaussian processes network combined product-of-experts formalism. hierarchy fused give prediction action types. methods two-stream rank pooling convolutional two-stream temporal-inception temporal segment network ts-lstm pillar networks++ pillar networks++ pillar networks svm-mkl st-multiplier network hand-crafted pillar networks++ problems underestimation variability posterior density inability describe multi-modal densities inaccuracy presence multiple equilibrium points. said problems also shared state-of-the-art mcmc samplers dynamical systems ﬂexibility utilising gpus methods prove computationally efﬁcient especially streaming data. thus scope future work apply inference methodologies compare vanilla laplace approximations utilised here.", "year": 2017}