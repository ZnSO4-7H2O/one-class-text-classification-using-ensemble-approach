{"title": "Efficient K-Shot Learning with Regularized Deep Networks", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Feature representations from pre-trained deep neural networks have been known to exhibit excellent generalization and utility across a variety of related tasks. Fine-tuning is by far the simplest and most widely used approach that seeks to exploit and adapt these feature representations to novel tasks with limited data. Despite the effectiveness of fine-tuning, itis often sub-optimal and requires very careful optimization to prevent severe over-fitting to small datasets. The problem of sub-optimality and over-fitting, is due in part to the large number of parameters used in a typical deep convolutional neural network. To address these problems, we propose a simple yet effective regularization method for fine-tuning pre-trained deep networks for the task of k-shot learning. To prevent overfitting, our key strategy is to cluster the model parameters while ensuring intra-cluster similarity and inter-cluster diversity of the parameters, effectively regularizing the dimensionality of the parameter search space. In particular, we identify groups of neurons within each layer of a deep network that shares similar activation patterns. When the network is to be fine-tuned for a classification task using only k examples, we propagate a single gradient to all of the neuron parameters that belong to the same group. The grouping of neurons is non-trivial as neuron activations depend on the distribution of the input data. To efficiently search for optimal groupings conditioned on the input data, we propose a reinforcement learning search strategy using recurrent networks to learn the optimal group assignments for each network layer. Experimental results show that our method can be easily applied to several popular convolutional neural networks and improve upon other state-of-the-art fine-tuning based k-shot learning strategies by more than10%", "text": "fine-tuning methods seek overcome limitation leveraging networks pre-trained large scale data. starting networks carefully adapting parameters enabled deep neural networks still effective learning samples. procedure affords advantages enables exploit good feature representations learned large scale data efﬁcient process often involving quick iterations small scale scales linearly large number k-shot learning tasks applicable existing pre-trained networks without need searching optimal architectures training scratch. unfortunately ﬁne-tuning unstable especially amount training data small. large deep neural networks typically comprised many redundant parameters parameters within layer highly correlation other. instance consider ﬁlters shown fig. ﬁrst layer lenet learned mnist dataset. number ﬁlters similar ﬁlters i.e. ﬁlters functionally play role tend produce similar activations. presence large number correlated ﬁlters potentially lead over-ﬁtting especially learning small sample regime. stabilize ﬁne-tuning process propose simple effective procedure regularize ﬁne-tuning based kshot learning approaches. idea approach identify redundancies parameters constrain updates ﬁne-tuning. achieved ﬁrst clustering parameters layer network multiple groups based similarity activations speciﬁc k-shot learning task. parameters group feature representations pre-trained deep neural networks known exhibit excellent generalization utility across variety related tasks. fine-tuning simplest widely used approach seeks exploit adapt feature representations novel tasks limited data. despite effectiveness ﬁne-tuning often sub-optimal requires careful optimization prevent severe over-ﬁtting small datasets. problem sub-optimality over-ﬁtting part large number parameters used typical deep convolutional neural network. address problems propose simple effective regularization method ﬁne-tuning pre-trained deep networks task k-shot learning. prevent overﬁtting strategy cluster model parameters ensuring intra-cluster similarity intercluster diversity parameters effectively regularizing dimensionality parameter search space. particular identify groups neurons within layer deep network share similar activation patterns. network ﬁne-tuned classiﬁcation task using examples propagate single gradient neuron parameters belong group. grouping neurons non-trivial neuron activations depend distribution input data. efﬁciently search optimal groupings conditioned input data propose reinforcement learning search strategy using recurrent networks learn optimal group assignments network layer. experimental results show method easily applied several popular convolutional neural networks improve upon state-of-the-art ﬁne-tuning based k-shot learning strategies even deep neural networks continue exhibit excellent performance large scale data suffer severe over-ﬁtting learning sample complexity. growing complexity size networks main factor contributes effectiveness learning large scale data also reason failure generalize limited data. learning training samples k-shot learning important learning paradigm widely believed humans learn concepts copyright association advancement artiﬁcial intelligence rights reserved. share common update ensuring intra-group similarity inter-group diversity activations. grouping model parameters guiding ﬁne-tuning process supervisory signals approach able reduce capacity network mitigate over-ﬁtting improve effectiveness pre-trained networks k-shot learning. make following contributions paper grouping neuron activations layer-wise clustering parameters enforcing intra-group similarity inter-group orthogonality group activations hybrid loss function k-shot learning consisting cross-entropy loss well triplet loss among k-shot data later providing supervision optimizing model reinforcement learning based mechanism efﬁciently search optimal clustering parameters across layers model. proposed k-shot learning approach affords following advantages task agnostic approach k-shot learning rely taskspeciﬁc prior knowledge applicable network without change original network structure general purpose technique decomposing parameter space high capacity deep neural networks. demonstrate effectiveness approach experimentally evaluate across tasks one-shot domainadaption task matching images across three different domains k-shot transfer learning task. experimental results show proposed approach yields signiﬁcant performance improvements task agnostic ﬁne-tuning approaches small sample learning without need task speciﬁc prior knowledge. k-shot learning earliest work one-shot learning object categories proposed fei-fei authors developed bayesian learning framework premise previously learned classes inform prior model parameters class. among recent work powerful generative models developed compose characters dictionary parts strokes generative models shown great promise datasets limited intra-class variation. siamese networks used automatically learn feature representations objects class closer together. santoro proposed memory-augmented neural networks external content based memory. wang hebert propose regression approach classiﬁers trained small datasets classiﬁers trained large datasets. vinyals proposed matching networks learns non-parameteric k-nearest neighbor classiﬁer end-to-end learning weights nearest neighbors provided lstm. ravi larochelle proposed lstm-based meta-learner uses state represent learning updates parameters classiﬁer k-shot learning. hariharan girshick suggest novel squared gradient magnitude regularization technique techniques hallucinate additional training examples small data classes. approaches state-of-the-art performance k-shot learning problems often utilize speciﬁc architectures designed problems. contrast explore general method reuse existing networks ﬁne-tuning k-shot learning. domain adaptation methods seek adapt pretrained model trained domain another domain proposed adaptation method feature augmentation creating feature vectors source component target component shared component. support vector machine trained augmented feature vector. used feature representation pre-trained network like alexnet trained imagenet -way classiﬁcation dataset authors replace source domain classiﬁcation layer domain-adaptive classiﬁcation layer takes activations existing network’s layers input features. also interested adapting model learned large scale data source domain model target domain examples. however unlike approaches propose task adaptive regularization approach improves adaptability exiting pre-trained networks target domains limited training samples. focus paper task k-shot learning ﬁne-tuning existing pre-trained network. consider setting pre-trained network learned source domain large amounts data k-shot target domain consists samples. avoid pitfalls overﬁtting training examples propose following strategy. ﬁrst search similar activations identify redundant ﬁlters group source domain. identiﬁng redundant parameters pre-trained network ﬁne-tuned group-wise backpropagaion target domain regularize network. proposed layer-wise grouping method model ﬁne-tune group-wise backpropagation effectively make ﬁne-tuning k-shot samples stable. however proposed grouping method signiﬁcant hyper-parameter number groups. deciding number groups layer non-trivial task optimal number groups different layer. suggest hyperparameter search method based reinforcement learning explore optimal group numbers. describe three sub-components involved approach grouping neurons activations model ﬁne-tuning k-shot learning reinforcement learning based policy searching optimal grouping parameters. similar other. insight update parameters group shared gradient learning regularize network. shared update computed average gradient ﬁlters group i.e. |gk| gradient demonstrate feasibility backpropagation average gradient domain adaptation transfer learning experiments described later. loss functions sample complexity typical k-shot learning results extremely noisy gradient updates k-shot entity. provide supervisory signals learning process introduce triplet loss network optimization objective. triplet loss similar introduced schroff triplet loss serves twin purposes providing supervisory signals increase separation k-shot entities well reduce noise gradient signal averaging larger number loss terms where output network input indices samples belonging class index sample belonging different class distance denotes margin maximizing loss. distance euclidean distance total variation regression classiﬁcation tasks respectively. note triplet loss reduces margin loss one-shot learning. margin loss deﬁned addition classiﬁcation loss described above important ensure intra-group activations similar other inter-group activations orthogonal other. augment k-shot learning loss function criterion training. activation ﬁlter l-th layer intra-group similarity loss deﬁned similar activations conditioned training images. would like group correlated ﬁlters means regularizing network. fig. illustrates example convolutional ﬁlters correlated activations since ﬁlters similar patterns outputs similar. consider fully connect layer neural network illustrated given batch data input pass data element network compute activations layer output nonlinear activation function i-th neuron layer compare activation another activation input data measure correlation neurons. example similar output patterns batch image data whereas different output patterns. implies good candidates grouping. proposed approach clustering algorithm group similar neurons based activations k-shot training data particular k-means clustering group neurons number clusters layer learned reinforcement learning procedure described later. backpropagation groups redundant parameter groups layer identiﬁed effective regularization method required ﬁne-tuning prevent over-ﬁtting. restrain redundant parameters overﬁting consider updating parameters group gradient gradients redundant weights group would expected hyper-parameter search reinforcement learning performance proposed approach critically dependent number clusters weights layer grouped into. manually selecting number clusters lead sub-optimal performance exhaustive search prohibitively expensive. problem exacerbated number layers network increases. common methods determining hyper parameters brute force search grid search random search. brute force search guaranteed optimal solution time consuming usually intractable. grid search used method hyper-parameter selection still limited granularity search potentially computationally expensive. hand surprisingly bergstra bengio suggests random search effective grid search. recently hansen proposed reinforcement learning approach determining hyperparameters. building upon this zoph proposed neural network architecture optimal hyper-parameter neural architecture reinforcement learning. work adopt similar approach determine optimal number clusters layer network k-shot learning. pose hyper-parameter search problem reinforcement learning problem locally optimal layerwise group size entire network. figure shows reinforcement learning problem environment pre-trained network wish ﬁne-tune k-shot learning. intuitively policy network implicitly learns relation different groupings layer weights performance network. model problem ﬁxed horizon episodic reinforcement learning problem actions equal affect ﬁnal outcome. represent sequence actions action l-th layer predicting number clusters l-th layer. deﬁne state vector number groups layer. agent’s policy network long short-term memory hochreier schmidhuber shown fig. learned policy gradient method. time horizon lstm equal number layers pre-trained network. output lstm consists fully connected layer followed softmax layer predict probabilities action l-th layer. input policy network l-th layer rna+ vector created figure adopt deep reinforcement learning framework search clustering hyper-parameters number groups. furthermore adopt lstm policy network determine numbers clusters layer. adopt policy gradient method learn agent’s policy maximizes expected accuracy proposed ﬁne-tuning process parameter clustering since cumulative reward nondifferentiable. deﬁne agent’s reward returned environment accuracy ﬁne-tuned model validation valid action invalid action accuracy ﬁne-tuned network parameters clustered calculated validation set. episode agent predicts list actions corresponding number groups layers network. parameters layer pre-network clustered number groups determined action. pre-trained network ﬁne-tuned k-shot data convergence validation accuracy network recorded reward agent. agent’s policy network updated backpropagating gradients computed loss episodes repeated policy network’s predictions inch closer optimal number parameter clusters layer turn resulting gradual increase accuracy ﬁne-tuning process. estimate optimal clustering network’s parameters policy network’s parameters optimized maximize expected reward computed future episodes current state. proposed approach fine-tuning standard approach updating pre-trained network k-shot data cross-entropy loss fine-tuning+triplet loss updating pre-trained network k-shot data cross-entropy loss triplet loss proposed orthogonal grouping parameters cross-entropy loss manual hyperparameter search gna+triplet loss proposed orthogonal grouping parameters cross-entropy triplet loss manual hyperparameter search gna+triplet loss+greedy proposed orthogonal grouping parameters cross-entropy triplet loss greedy hyperparameter selection gna+triplet loss+rl proposed orthogonal grouping parameters cross-entropy triplet loss based hyperparameter search. domain adaptation task consider ofﬁce dataset introduced consisting collection images three distinct domains amazon dslr webcam. dataset consists objects commonly encountered ofﬁce settings keyboards cabinets laptops etc. follow experimental protocol used consider domain adaptation between amazon webcam images. experiment conducted objects also present imagenet dataset. pre-trained network resnet- architecture trained imagenet dataset. action space experiment number possible clusters layer equivalently action space number possible groups layer. action space number ﬁlters. minimum number groups one. maximum number groups number weights. work deﬁne actions reduce size action space speed search process. however general action space also continuous like ....nf}. source images class clustering parameters ﬁne-tune model one-shot examples image class. performance proposed approach compared baselines table figure shows progression reinforcement learning based hyperparameter search k-shot learning accuracy. late fusion daume compared baselines. late fusion daume decaf- features works also apply method resnet- features fair comparison method. ﬁne-tuning learning-rate changed iteration. tried random runs randomly selected different dataset average performance. note clustering hyper-parameter search reinforcement learning able efﬁciently search hyper-parameter space better parameter groupings compared manual greedy search. manual baseline initialize number groups layers compute accuracy network. where reward episode number episodes. denotes probability history actions given policy-deﬁning weights complete algorithm presented algorithm usefulness proposed method veriﬁed experiments tasks domain adaptation transfer learning. tasks show approach used learn model number examples. present results multiple baseline variants method late fusion decaf- late fusion resnet- daume decaf- daume resnet- resnet- fine-tuning resnet- fine-tuning margin loss resnet- resnet- margin loss margin loss+greedy resnet- resnet- margin loss+rl method fine-tuning fine-tuning margin loss fine-tuning triplet loss gna+margin loss gna+triplet loss gna+triplet loss+greedy gna+triplet loss+rl task different pre-trained network task pre-trained network resnet- architecture trained classes cifar- dataset k-shot learning task classes cifar- dataset. transfer learning setting select classes different target classes source classes. action space experiment number possible clusters layer consider different kshot settings another k-shot data chosen randomly target training ﬁne-tuning evaluate entire target test set. performance proposed approach compared baselines table one-shot learning well -shot learning. proposed margin loss improves accuracies grouping method well ﬁne-tuning. accuracies grouping methods higher ﬁne-tuning result. thus proposed method search outperforms baseline ﬁne-tuning approach -shot learning -shot learning. effect sample size experiment interested comparing performance proposed approach vary difﬁculty k-shot learning paradigm. consider different ranges number samples category. table presents results clustering standard ﬁne-tuning withclustering vary unsurprisingly performance decreases greater uncertainty lowered one-shot learning. observe consistent improvement performance clustering approach comparison standard ﬁne-tuning procedure. effect clustering across layers commonly believed deep convolutional neural networks highly redundant ﬁlters initial layers only. indeed case applying clustering method layers initial layers helpful. compute accuracy network doubling halving number groups layer. action results higher accuracy selected. repeat process update number groups iteratively process described above. greedy baseline number groups ﬁrst layer compute accuracy original network. accuracy greater before number groups doubled otherwise number groups previous number move next layer. repeat procedure last layer. transfer learning domain adaptation experiment classes showed proposed method outperforms baseline approaches. apply grouping method task source classes different target classes. consider task transfer learning k-shot learntest hypothesis perform clustering increasing number layers starting initial layers network. experiment considered pre-trained resnet- network trained categories cifar- dataset used categories k-shot learning task. results table surprisingly conﬁrm hypothesis. found layers deep network consist redundant ﬁlters k-shot learning task. fact applying method layers network resulted best performance. experiment suggests large convolutional neural networks could potentially consist redundant parameters even higher layers necessitating search entire hyper-parameter space parameter groupings. motivates need efﬁcient techniques search hyper-parameter space like proposed paper. paper proposed regularization method ﬁne-tuning pre-trained network k-shot learning. idea approach effectively reduce dimensionality network parameter space clustering weights layer ensuring intra-group similarity inter-group orthogonality. provide additional supervision k-shot learning problem introduce triplet loss maximize separation k-shot samples. lastly introduced reinforcement learning based approach efﬁciently search hyper-parameters clustering approach. experimental results demonstrate proposed regularization technique signiﬁcantly improve performance ﬁne-tuning based k-shot learning approaches.", "year": 2017}