{"title": "A Neural Network Decision Tree for Learning Concepts from EEG Data", "tag": ["cs.NE", "cs.AI"], "abstract": "To learn the multi-class conceptions from the electroencephalogram (EEG) data we developed a neural network decision tree (DT), that performs the linear tests, and a new training algorithm. We found that the known methods fail inducting the classification models when the data are presented by the features some of them are irrelevant, and the classes are heavily overlapped. To train the DT, our algorithm exploits a bottom up search of the features that provide the best classification accuracy of the linear tests. We applied the developed algorithm to induce the DT from the large EEG dataset consisted of 65 patients belonging to 16 age groups. In these recordings each EEG segment was represented by 72 calculated features. The DT correctly classified 80.8% of the training and 80.1% of the testing examples. Correspondingly it correctly classified 89.2% and 87.7% of the EEG recordings.", "text": "nimia-sc nato advanced study institute neural networks instrumentation measurement related industrial applications study cases crema italy october abstract learn multi-class conceptions electroencephalogram data developed neural network decision tree performs linear tests training algorithm. found known methods fail inducting classification models data presented features irrelevant classes heavily overlapped. train algorithm exploits bottom search features provide best classification accuracy linear tests. applied developed algorithm induce large dataset consisted patients belonging groups. recordings segment represented calculated features. correctly classified training testing examples. correspondingly correctly classified recordings. learning multi-class conceptions electroencephalogram data important biomedical problems general learning conceptions also known induction classification models able improve clinical interpretation eegs complex problem. firstly eegs strong non-stationary signals whose statistics vary width range values secondly characteristics eegs depend human individuality activity well. obtain circumstances acceptable classification accuracy models must induced large data. reason learning time becomes crucial. decision tree methods successfully used learning multi-class concepts data. methods assume input features attributes characterize training examples must correlated goal. however input features irrelevant correlated goal. know using relevant features only induced model able classify unseen examples included training set. induced model able classify examples successfully generalization ability well. general methods inducting require computational time grows proportionally size training datasets. computational time also increased training examples non-linearly separable. note also that humans find classification models ease-to-understand. particular consists linear combinations input features easyto-read experts usually consist nodes types. decision node containing test leaf node assigned appropriate class. branch represents possible outcome test. example comes root follows branches leaf node reached. name class leaf resulting classification. node test input features. multivariate nodes test features. multivariate much shorter tests single feature. addition reducing size improves generalization ability i.e. ability classify unseen examples. learn concepts presented numeral attributes appropriate data authors suggested multivariate threshold logical units called also perceptions. multivariate named also oblique ones able classify linear separable examples. using algorithms suggested oblique also learn classify non-linear separable examples. general algorithms require computational time grows proportionally numbers training examples input features classes. nevertheless computational time needs induce multi-class models large datasets becomes crucial especially number training examples tens thousands. note oblique algorithms exploiting ideas cart relief require computational time grows proportionally squared number training examples large datasets dataset time expected huge reason consider induction algorithms. paper firstly introduce neural network based construction oblique secondly introduce induction method algorithm able select relevant features. algorithm performs acceptable time large datasets whose examples linearly separable. thirdly describe analyze realworld task classification. finally evaluate ability neural network learn multi-class concept data. section firstly describe algorithm inducting linear machine discuss detail training procedure known pocket algorithm shortly thermal training algorithm. finally discuss feature selection algorithms. realize idea algorithm cycles training given number epochs. epoch algorithm counts current length sequence examples classified correctly well accuracy training examples. correspondence inequality assigns training example j-th class class example actually belongs. training algorithm consists following steps algorithm finds best weights time grows proportionally numbers training examples input variables classes epochs. number epochs must given enough large order achieve acceptable classification accuracy example case number epochs maximal equal number training examples. note also best classification accuracy achieved amount correction equal training examples linearly separable classification accuracy unpredictable. cases behavior destabilized learning first case misclassified example dividing hyperplane removing error hypeplane must substantially readjusted. relatively large adjustments destabilize training procedure. second case misclassified example lies close dividing hyperplane weights converged. improve convergence training algorithm thermal procedure suggested firstly procedure decreases attention large errors using next correction linear machine linear discriminant functions calculated order assign example classes internal node tests linear combination input variables introduce feature vector discriminant function linear test node next form winner take one. train weight vectors discriminant functions updated example misclassifies. learning rule increases weights class example actually belongs decreases weights class erroneously assigns example done using next error correction rule amount correction. training examples linearly separable procedure trains desirable finite number steps examples linearly separable training procedure cannot provide predictable classification accuracy. case training procedures suggested discuss below. train examples linearly separable gallant suggested using pocket algorithm algorithm seeks weights multivariate test minimizes number classification errors. pocket algorithm uses error correction rule update weights corresponding discriminant functions normal training algorithm saves pocket best weight vectors happen. addition gallant suggested ratchet modification pocket algorithm. idea behind algorithm replace weight current current correctly classified training examples used modified algorithm finds optimal weights training time given enough. note classification accuracy resulting linear test depends order features included test. algorithm order including features determined classification accuracy. know accuracy depends initial weights well sequence training examples selected randomly. subsequently linear test built non-optimally i.e. test include less features needs obtain best classification accuracy. chance selecting non-optimal linear test expected high algorithm compares tests differed feature only. section describe training procedure able select relevant features tests effectively. section firstly discuss computational performance pocket algorithm used inducting data. describe neural network based structure discuss advantages scheme. data need classify split several classes extremely overlapped other. centers classes close other. circumstance worsens performance dramatically. found experimentally failed learning data. shortly discuss possible reasons. pocket thermal algorithms know update weights linear tests misclassified example. updating weights initiates setting size example sequence least implementation algorithm. happens next examples? next example correctly classified algorithm evaluate accuracy training examples. next example also correctly classified algorithm evaluate accuracy training set. calculation time grows quickly beginning steps especially number training examples large. misclassification example causes updating weights setting therefore next example algorithm evaluate accuracy updated training examples. happens often training examples heavily overlapped linear separable. conclude reasons training algorithm able induce large data acceptable time. training data consist examples heavily overlapped algorithm requires extensive computational time. given constant. handle parameter training magnitudes weight vectors summed. value decreased current weight adjustment increased previous adjustment parameter given constants. reducing enables spend time training small value needs refine location dividing hyperplane. however note experimental results authors implemented thermal procedure draw training time comparable time training obtain accurate understandable must eliminate features able contribute classification accuracy nodes tree. features irrelevant corrupted noise correlated features often cause over-fitted. section discuss sequential feature selection algorithms based greedy heuristic used eliminate irrelevant features. selection performed nodes learn that know allows avoid over-fitting effectively known methods feature pre-processing precede training. algorithm exploits bottom search method starts features iteratively adds features provide improvement quality linear test. algorithm continues features specified stopping criterion met. process best linear test minimum number features saved. general algorithm includes next steps. search must stop features involved test. case required calculate linear tests number steps. clearly number features well number examples large computational time needs reaching point unacceptable. stop search point reduce computational time authors suggested next heuristic stopping criterion. observed accuracy best test decreased chance finding better test features slight. depicted three dividing surfaces corresponding classes. first surface superposition linear tests i.e. tests summarized weights equaled because tests give positive outputs examples class correspondingly second third surfaces example belongs class causes outputs equal correspondingly correctly assign example class approximate dividing surfaces suggest exploit two-layer feed-forward neural networks consisting tlus. easy approximated neural network consists input hidden output layers fig. fig. depicted three hidden neurons perform linear tests whose weights hidden neurons connected output neurons weights equal respectively. general case classes neural network consists hidden neurons fi/j output neurons output neuron connected hidden neurons partitioned groups. first group consists hidden neurons fi/k group consists hidden neurons fk/i since outputs hidden neurons described equation weights output neuron connected hidden neurons fi/k fk/i equal correspondingly. connectivity hidden neurons performing linear tests depends significance features next section discuss training algorithm able select relevant features. idea behind algorithm inducting individually train tlus group order linearly approximate desirable dividing surfaces. realizes linear test individually trained classify examples classes. classes therefore needed variants training subsets train compose number tlus number tlus therefore equal classification accuracy test calculated. section firstly describe algorithm training weights linear tests. secondly describe detail algorithm able select relevant features training linear tests. within neural network introduced above hidden neurons perform linear tests. methods used training linear tests depend distortions noise training data. case real data distortion noise approximately distributed skew gauss distribution function. experimentally found skewness distribution function weak. reason suggest standard least square error technique. technique typically used linear discriminant functions training examples linear separable. general technique effectively used training weights multivariate functions well tlus. shortly describe training procedure introduce training matrix target vector number training examples number variables linear multivariate test uses. note number input variables. number examples must exceed number variables i.e. matrix contains training examples belong i-th j-th classes elements target vector marked examples belonging classes respectively. output linear test described linear test assigns example i-th class otherwise example assigned j-th class. minimal training error achieved procedure fits weight vector linear test that training euclidian norm. thus linear separable data procedure yield desirable weight vector minimizes residual error case classification accuracy linear test using weights depends level distortions noise training data. feature selection algorithm based bottom search discussed section algorithm searches features cause largest increasing classification accuracy linear test. tests algorithm compares differed feature. addition firstly remind heavily non-stationary signal spectral parameters vary time. demonstrate this calculated principal components recordings made patients belonging different classes. depicted space principle components segments belonging four time intervals values calculated intervals vary time considerably. variability allow distinguishing segments belonging different classes using principle components. secondly must remind eegs reflect individual activity patients. activity chaotic character significantly increases group variance class. analyze effect individual activity used next statistical technique. introduce variance classes group variance i-th class accuracy becomes higher accuracy current test candidate-test replaces test number features liner test includes increased algorithm repeated stopping criterion met. stopping criterion cases firstly linear test includes given number features secondly features tested. note step algorithm compares linear tests different features. increases chance search best linear test considerably. result unique features formed. using features classifies examples well. enlarge chance finding best solution tests multiply trained given number attempts time different sequence features. note fitting weights tlus used training examples. classification accuracy tlus evaluated training examples. number attempts varied features maximal number features linear test include equal used data recorded standard channels patients belonging different groups. following segment recordings represented calculated features. first based features power spectral densities calculated -second interval frequency bands sub-delta delta theta alpha beta beta densities calculated channels well total sum. second features relative absolute power densities variances first variables. data finally normalized. eeg-viewer manually deleted artifacts recordings assigned normal segments classes accordance ages patients. note cleaning average rate outlying segments exceed total segments equal experiment induce multi-class model large dataset. would like model order determine maturation patients. index maturation used clinical diagnosis pathologies since desirable model must patient-depended induce large dataset. learning multi-class concept data used neural network described sections training used examples segments. rest segments used testing trained classes neural network includes linear tests classifiers. training errors classifiers vary depicted fig. note trained classifier includes different features. number features classifiers varies depicted trained neural network correctly classified training testing examples represent recordings. summing segments belong recording conclude trained correctly classified recordings training testing examples respectively. fig. depicted summed outputs trained calculated testing segments patients respectively. value decreased proportionally group variances increased proportionally variance classes. clear group variance grows proportionally individual activity patients belong group. therefore conclude measure significance feature fig. depicts values features calculated training set. relevant feature less relevant addition interval sigma feature larger corresponding interval feature however feature allow distinguish classes properly interval sigma large. words values expected almost classes. clearly summed outputs interpret distribution segments classes. case provide probabilistic interpretation making decisions example assigned patients classes probabilities respectively fig. compare induction algorithm applied standard neural networks data mining techniques data. firstly tried train feed-forward neural network consisting hidden output layers fast back-propagation method levenberg-marquardt matlab provides. input output layers included units neurons respectively. secondly trained independently binary classifiers distinguish class others. thirdly trained binary decision tree consisted linear classifies. however well discussed section failed induction classification models. learning multi-class conceptions data represented irrelevant features developed neural network training algorithm. found known approaches failed induction classification models data whose examples strongly overlapped. neural network developed consists tlus perform linear multivariate tests. algorithm inducting trains tlus groups order linearly approximate desirable dividing surfaces. individually trained classify examples classes. trained tlus deal class collected neural network number groups corresponds number classes. tlus group superposed order linearly approximate dividing surface corresponding classes. training algorithm exploits bottom search select features provides best classification accuracy linear tests. evaluating feature significance algorithm uses unseen examples used training weights linear tests. applied developed algorithm induce large dataset consists recordings belonging different groups. recordings segment represented calculated features. training used examples segments. rest segments used testing trained finally trained neural network correctly classified training testing examples. corresponds correct classification recordings. conclude method developed inducting neural network performed large data successfully. hope method maybe used induce classification models large data represented irrelevant features overlapping classes. authors grateful frank pasemann joachim schult theorilabor enlightening discussions joachim frenzel burghart scheidt pediatric clinic university jena making available recordings. galicki witte dörschel doering eiselt grießbach. common optimization adaptive preprocessing units neural network learning period application pattern recognition. neural networks anderson devulapalli stolz. determining mental state signals using neural networks. scientific programming special issue applications analysis riddington ifeachor allen hudson mapps. fuzzy expert system interpretation. e.c. ifeachor k.g. rosen proc. int. conf. neural networks expert systems medicine healthcare university plymouth yang parekh honavar. comparison performance variants single-layer perceptron algorithms nonseparable data. neural parallel scientific computation", "year": 2005}