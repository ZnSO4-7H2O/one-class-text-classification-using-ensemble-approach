{"title": "Belief Propagation in Conditional RBMs for Structured Prediction", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Restricted Boltzmann machines~(RBMs) and conditional RBMs~(CRBMs) are popular models for a wide range of applications. In previous work, learning on such models has been dominated by contrastive divergence~(CD) and its variants. Belief propagation~(BP) algorithms are believed to be slow for structured prediction on conditional RBMs~(e.g., Mnih et al. [2011]), and not as good as CD when applied in learning~(e.g., Larochelle et al. [2012]). In this work, we present a matrix-based implementation of belief propagation algorithms on CRBMs, which is easily scalable to tens of thousands of visible and hidden units. We demonstrate that, in both maximum likelihood and max-margin learning, training conditional RBMs with BP as the inference routine can provide significantly better results than current state-of-the-art CD methods on structured prediction problems. We also include practical guidelines on training CRBMs with BP, and some insights on the interaction of learning and inference algorithms for CRBMs.", "text": "restricted boltzmann machines conditional rbms popular models wide range applications. previous work learning models dominated contrastive divergence variants. belief propagation algorithms believed slow structured prediction conditional rbms good applied learning work present matrix-based implementation belief propagation algorithms crbms easily scalable tens thousands visible hidden units. demonstrate that maximum likelihood maxmargin learning training conditional rbms inference routine provide signiﬁcantly better results current state-of-the-art methods structured prediction problems. also include practical guidelines training crbms insights interaction learning inference algorithms crbms. restricted boltzmann machine two-layer latent variable model uses layer hidden units model distribution visible units rbms widely used building blocks deep generative models deep belief networks deep boltzmann machines intractability partition function maximum likelihood estimation rbms usually learned using contrastive divergence algorithm approximates gradient log-partition function using k-step gibbs sampler speed convergence markov chain critical trick cd-k initialize state markov chain training instance. although shown cd-k follow gradient objective function works well many practical applications important variant cd-k persistent uses persistent markov chain learning markov chain reset parameter updates. learning rate usually small model changes slightly parameter updates long-run persistent chain usually provides better approximation target distribution limited step chain cd-k. conditional discriminative extension include observed features crbm used deep probabilistic model supervised learning also provides stand-alone solution wide range problems classiﬁcation human motion capture collaborative ﬁltering structured prediction structured prediction crbm need make explicit assumptions structure output variables especially useful many applications structure outputs challenging describe image denoising image segmentation hidden units encode higher-order correlations visible units play role high-order potentials improve statistical eﬃciency. contrast success methods rbms noted cd-k well suited learning conditional rbms particular appropriate learning conditional models observed features greatly aﬀect model potentials. means need separate persistent chain every training instance costly large datasets. make things worse revisit training instance stochastic gradient descent model parameters changed substantially making persistent chain instance target distribution. also given observed features crbms tend peaked rbms purely generative setting. methods make slow progress diﬃcult sampling procedure explore peaked multi-modal distributions. also observed important trick cd-k initializes markov chain using training data work well crbms structured prediction contrast starting gibbs chain random state provides better results. approximate mean ﬁeld belief propagation employed inference routines learning well making predictions crbm learned although loopy usually provides better approximation marginals found slow crbms structured prediction considered practical problems visible hidden nodes ineﬃciency prevents being widely applied conditional rbms structured prediction crbms thousands visible hidden units. importantly pervasive opinion belief propagation work well rbm-based models especially learning work present eﬃcient implementation belief propagation algorithms conditional rbms. takes advantage bipartite graph structure scalable tens thousands visible units hidden units. algorithm uses compact representation depends matrix product element-wise operations typically highly optimized modern high-performance computing architectures. demonstrate that conditional setting learning rbm-based models belief propagation variants provide much better results state-of-the-art methods. also show marginal structured provide improvements max-margin learning crbms include practical guidelines training crbms insights interaction learning message-passing algorithms crbms. organize rest paper follows. section discusses connections related work. review model conditional rbms section discuss learning algorithms section section provide eﬃcient inference procedure. report experimental results section conclude paper section mnih proposed cd-percloss algorithm conditional rbms uses cd-like stochastic search procedure minimize perceptron loss training data. given observed features training instance cd-percloss starts gibbs chain using logistic regression component crbm. yang trained crbms using latent structured objective used greedy search joint maximum posteriori inference hidden visible units. also feasible apply mean-ﬁeld approximation partition function learning rbms crbms although eﬃcient conceptually problematic sense eﬀectively maximizes upper bound log-likelihood learning. addition uses unimodal proposal approximate multi-modal distribution lead unsatisfactory results. although belief propagation variants long used learn conditional random ﬁelds hidden variables mainly applied sparsely connected graphs believed ineﬀective slow dense graphs like crbms recent works impose particular assumptions type edge potentials provide eﬃcient inference algorithms fully connected crfs. example edge potentials deﬁned linear combination gaussian kernels. work however propose speed general belief propagation conditional rbms without potential function restrictions. note marginalizing hidden variables log-linear model becomes non-linear model capture high-order correlations among visible units. property essentially important many applications crbms structured output structured prediction visible units typically represent output variables observed represent input features hidden units facilitate modeling output variables given observed features. make predictions choice ingives prediction pair obtains prediction simply discarding component. intuitively joint prediction over-conﬁdent since deterministically assigns sum-product mean ﬁeld methods provide pseudo-marginals substitutes intractable expectations deterministic gradient estimates advantage larger learning rate used. tends give accurate estimate marginals reported slow crbms impractical problems large output dimension hidden layer sizes structured prediction importantly observed belief propagation usually gives unsatisfactory results learning vanilla rbms. mainly parameters’ magnitude gradually increases learning; model eventually undergoes phase transition diﬃculty converging converge provide meaningful gradient direction update model leaning becomes stuck. however crbms appear behave quite differently operating high signal regime provided informative observation improves convergence behaviour surprising since loopy widely accepted useful learning conditional models addition given training instances learning crbm actually performed diﬀerent rbms corresponding diﬀerent features particular phase learning trouble converging training instances still make progress long converges majority instances. demonstrate behavior experiments. decomposable contrast lssvm mssvm marginalizes uncertainty hidden variables signiﬁcantly outperform lssvm uncertainty large experimentally mssvm improves performance max-margin crbms likely usually non-trivial uncertainty hidden units. explicitly takes account uncertainty hidden units marginalizing out. general predictions intractable crbms must approximate inference methods mean ﬁeld belief propagation. hxxn +bh) logistic function applied element-wise manner. positive part gradient calculated exactly. negative part arises derivatives logpartition function intractable calculate. gradients log-likelihood w.r.t. parameters analogous found appendix cd-k initializes gibbs chain instance performs k-step gibbs sampling then empirical moment used substitute intractable expectation works well rbms gives unsatisfactory results crbms. practice conditional distributions strongly inﬂuenced observed features usually peaked generative rbms. usually diﬃcult markov chain steps explore peaked multimodal distributions. uses long-run persistent markov chain improve convergence suitable crbms discussed section ﬁrst review standard message-passing form rbms. dense graphical models like rbms reduce amount calculation always pre-compute product incoming messages nodes reuse perform updates outgoing messages. sum-product write ﬁxed-point update rule message sent hidden unit visible unit summarize matrix-based sum-product mixed-product algorithm well known asynchronous message updates usually converge much faster synchronous updates algorithm although messages sent parallel hidden units visible units bipartite graph structure ensures actually asynchronous updates helps convergence practice. method also related messagepassing algorithms designed binary networks binary ldpc codes parametrize message single real number using hyperbolic tangent transform. algorithm specially designed rbm-based models signiﬁcantly speeds taking advantage structure using matrix operations. practice matrix implementation runs orders magnitude faster standard implementation belief propagation e.g. factor graph package libdai used assessments visible hidden units iterations matlab implementation takes seconds laptop intel core libdai iterslower. mainly matrix operations highly optimized modern computer architectures e.g. performed parallel instruction pipeline pointers need dereferenced. table average test error image denoising noisy minist. denotes percentage incorrectly labeled pixels among pixels. changed denotes percentage errors among pixels changed noise process. percloss algorithm uses -step gibbs sampling stochastic search process. methods epochs training. contrast mle-mf mlebp mssvm lssvm epochs provide deterministic gradient estimate larger learning rates applied. early stopping based validation error also used methods. test learned models methods mle-mf mean-ﬁeld predictions; learned model mle-bp sum-product predictions; mssvm mixed-product lssvm max-product results table shows percentage incorrectly labeled pixels noisy mnist diﬀerent methods. denotes errors among pixels main measurement. also report changed errors among pixels changed noise/occlusion process. mle-bp works best provides relative improvement cdobtain clean image test types structured prediction tasks experiment. ﬁrst task image denoising denoted noisy mnist noisy image obtained ﬂipping either entries second task image completion denoted occluded mnist occluded image obtained setting random patch within image patch size either image represents object silhouette. dataset divided three subsets examples training validation testing. test image denoising image completion tasks. noisy image noisy caltech obtained ﬂipping pixels clean occluded image occluded caltech model following structured crbm model hidden units giving million parameters model. learning algorithms applied learn crbm model. logistic regression method viewed learning crbm non-zero. algorithms train several crbms using state-of-the-art methods including cd-percloss. also train models optimize likelihood using mean ﬁeld sum-product finally train mssvm crbms using mixed-product lssvm crbms using max-product ﬁxed learnpercloss datasets diﬀerent noise levels. table shows results occluded mnist. mle-bp provides relative improvement cd-percloss datasets respectively. cd-k gives unsatisfactory results cases. mssvm performs worse mle-bp better methods table image completion task viewed diﬃcult changed pixels. however training crbm mlebp gives good results; last rows images figure table demonstrate results caltech silhouettes; setting mle-bp mssvm perform best image denoising image completion respectively. learning continues magnitudes parameters gradually increase becomes harder converge quickly. simple eﬀective strategy number iterations epoch figure illustration convergence behavior using strategy training. convergence tolerance model undergoes change convergence behaviour around epoch still make progress converges majority training instances. damping better. although message damping improve convergence always requires iterations message-passing eﬀectively slows progress learning crbms. approximate inference algorithms used learning test matched means inference method number iterations etc. same. otherwise unsatisfactory results. learning crbms vanilla rbms quite diﬀerent practice. literature suggests vanilla rbms also methods work better mle-bp. contrast past work argue belief propagation excellent choice learning inference rbm-based models conditional setting. present matrix-based expression updates crbms scalable tens thousands visible hidden units. implementation takes advantage bipartite graphical structure uses compact representation messages beliefs. since uses matrix product elementwise operations highly suited acceleration. demonstrate learning crbms sumproduct mixed-product provide signiﬁcantly better results stateof-the-art methods structured prediction problems. future directions include gpu-based implementation applying method deep probabilistic models deep boltzmann machines.", "year": 2017}