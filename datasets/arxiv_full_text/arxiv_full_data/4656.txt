{"title": "MCTS Based on Simple Regret", "tag": ["cs.AI", "cs.LG"], "abstract": "UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS) in games and Markov decision processes, is based on UCB, a sampling policy for the Multi-armed Bandit problem (MAB) that minimizes the cumulative regret. However, search differs from MAB in that in MCTS it is usually only the final \"arm pull\" (the actual move selection) that collects a reward, rather than all \"arm pulls\". Therefore, it makes more sense to minimize the simple regret, as opposed to the cumulative regret. We begin by introducing policies for multi-armed bandits with lower finite-time and asymptotic simple regret than UCB, using it to develop a two-stage scheme (SR+CR) for MCTS which outperforms UCT empirically.  Optimizing the sampling process is itself a metareasoning problem, a solution of which can use value of information (VOI) techniques. Although the theory of VOI for search exists, applying it to MCTS is non-trivial, as typical myopic assumptions fail. Lacking a complete working VOI theory for MCTS, we nevertheless propose a sampling scheme that is \"aware\" of VOI, achieving an algorithm that in empirical evaluation outperforms both UCT and the other proposed algorithms.", "text": "policy. action discovered usually beneﬁcial sample action exploitation thus meaningless search problems. finding good ﬁrst action closer pure exploration variant seen selection problem selection problem much better minimize simple regret. however simple cumulative regret cannot minimized simultaneously; moreover shows many cases smaller cumulative regret greater simple regret. begin background deﬁnitions related work. sampling schemes introduced shown better bounds simple regret sets ﬁrst contribution paper. results applied sampling trees combining proposed sampling schemes ﬁrst step rollout rest rollout. additional sampling scheme based metareasoning principles also suggested another contribution paper. finally performance proposed sampling schemes evaluated sets bernoulli arms randomly generated -level trees sailing domain showing proposed schemes improved performance. monte-carlo tree search initially suggested scheme ﬁnding approximately optimal policies markov decision processes deﬁned states actions transition distribution reward function initial state optional goal state several mcts schemes explore performing rollouts— trajectories current state state termination condition satisﬁed state-of-the algorithm monte carlo tree search games markov decision processes based sampling policy multi-armed bandit problem minimizes cumulative regret. however search differs mcts usually ﬁnal pull collects reward rather pulls. therefore makes sense minimize simple regret opposed cumulative regret. begin introducing policies multiarmed bandits lower ﬁnite-time asymptotic simple regret using develop two-stage scheme mcts outperforms empirically. optimizing sampling process metareasoning problem solution value information techniques. although theory search exists applying mcts non-trivial typical myopic assumptions fail. lacking complete working theory mcts nevertheless propose sampling scheme aware achieving algorithm empirical evaluation outperforms proposed algorithms. monte-carlo tree search especially version based formula appears numerous search applications although methods shown successful empirically authors appear using formula because shown successful past because good trading exploration exploitation. latter statement correct multi-armed bandit method argue inappropriate search. problem work; rather simple reconsideration basic principles result schemes outperform uct. core issue adversarial search search games nature optimizing behavior uncertainty goal typically either good strategy even best ﬁrst action copyright association advancement artiﬁcial intelligence rights reserved. method general intractable necessitating simplifying assumptions. however using standard metareasoning myopic assumption samples would selected though sample taken action chosen serious problems. even basic selection problem exhibits non-concave utility function results premature stopping standard myopic algorithms. fact value information single measurement frequently less time-cost even though true multiple measurements. applying selection problem mcts situation exacerbated. utility action usually bounded thus many cases single sample insufﬁcient change current best action regardless outcome. result frequently zero myopic value information single sample. analysis sampling sets supersampling polynomially decreasing upper bounds simple regret. bounds suggest schemes achieve lower simple regret uniform sampling; indeed conﬁrmed experiments. random reward unknown stationary distribution encountered. reward usually bounded cumulative setting encountered rewards collected agent. scheme shown near-optimal respect deﬁnition scheme pulls maximizes upper conﬁdence bound reward collect reward last pull. deﬁnition simple regret sampling policy multi-armed bandit problem expected difference best true expected reward true expected reward greatest sample mean maxi strategies minimize simple regret called pure exploration strategies upper bound simple regret uniform sampling exponentially decreasing number samples proposition best known respective upper bound simple regret polynomially decreasing number samples theorems however empirically appears yield lower simple regret uniform sampling. metareasoning completely different scheme control sampling principles bounded rationality metareasoning provided formal description rational metareasoning case studies applications several problem domains. search myopic sub-tree independence assumptions maintains current best move root ﬁnds expected gain ﬁnding another move better current best cost search actions also factored ideally optimal sampling scheme used selecting sample root node elsewhere developed using metareasoning. however task daunting following reasons sampling trees mentioned above extension mcts applies step rollout. root node sampling mcts usually aimed ﬁnding ﬁrst move perform. search re-started either scratch using previously collected information observing actual outcome opponent’s move move shown best choice high conﬁdence value information additional samples best move low. therefore able better optimizing simple regret rather cumulative regret root node. nodes deeper search tree different matter. order support optimal move choice root ε-greedy based solely sampling greatest sample mean higher probability rest arms ignores information sample means arms. hand distributes samples accordance sample means order minimize cumulative regret chooses current best often. intuitively better scheme simple regret minimization would distribute samples similar would sample current best less often. achieved replacing equation faster growing sublinear function example beneﬁcial many cases precise estimate value state search tree nodes. internal nodes optimizing simple regret answer cumulative regret optimization mark. lacking complete metareasoning sampling would indicate optimal sample root nodes internal nodes suggested improvement thus combines different sampling schemes ﬁrst step rest rollout deﬁnition sr+cr mcts sampling scheme selects action current root node according scheme suitable minimizing simple regret greedy ucb√· selects actions according approximately minimizes cumulative regret pseudocode two-stage rollout undiscounted algorithm firstaction selects ﬁrst step rollout nextaction selects steps rest rollout reward statistic selected action updated sample reward back-propagated towards current root. expect two-stage sampling schemes outperform signiﬁcantly less sensitive tuning exploration factor ucb. since contradiction need larger value ﬁrst step smaller value rest rollout resolved. fact sampling scheme uses steps larger value ﬁrst step rest steps also outperform uct. voi-aware sampling improvement achieved computing estimating value information rollouts choosing rollouts maximize voi. however indicated above actually computing infeasible. instead suggest following scheme based following features value information change occurs. distribution results generated rollouts known features could easily computed. however case mcts applications. therefore estimate bounds feature values current samples based myopic assumption algorithm sample actions bounds feature values equations derived follows. gain switching current best action another action bounded current expectation value current second-best action case sample minus current expectation sampling action. probability another action found best bounded exponential function difference expectations true value actions becomes known. effect individual sample sample mean inversely proportional current number samples hence current number samples denominator. estimates used voi-aware sampling scheme follows sample action maximum estimated voi. judged estimates crude used stopping criteria used sampling leaving issue future research. although scheme appears complicated amenable formal analysis early experiments approach demonstrate signiﬁcantly lower simple regret. results empirically veriﬁed multi-armed bandit instances search trees sailing domain deﬁned cases experiments showed lower average simple regret greedy ucb√· sets sr+cr scheme trees. simple regret multi-armed bandits figure presents comparison mcts sampling schemes multi-armed bandits. figure shows search tree corresponding problem instance. returns random reward drawn bernoulli distribution. search figure shows regret number samples averaged experiments randomly generated -greedy ucb√· dominate instances arms. either whole range. larger number samples advantage ucb√· -greedy becomes signiﬁcant. monte carlo tree search second experiments performed randomly generated -level max-max trees crafted deliberately deceive uniform sampling necessitating adaptive sampling scheme uct. switch nodes children anti-symmetric values would cause uniform sampling scheme incorrectly give value number samples shown trees root degree exploration factor default value rewards range algorithms ex-greedy+uct hibit similar relative performance either ucb√·+uct result lowest regret ucb√·+uct dominates everywhere except number samples small. advantage -greedy+uct ucb√·+uct grows number arms. sailing domain figures show results experiments sailing domain. figure shows regret number samples computed range values figure shows median cost figure minimum -greedy+uct costs. always worse either ucb√·+uct sensitive value median cost much higher minimum cost uct. signiﬁcantly less prominent. figure shows regret exploration factor different numbers samples. ucb√·+uct always better -greedy+uct better expect small range values exploration factor. voi-aware mcts finally voi-aware sampling scheme empirically compared sampling schemes again experiments performed randomly generated trees structure shown figure figure shows results arms. voi+uct uct-based monte-carlo tree search shown effective ﬁnding good actions mdps adversarial games. improvement sampling scheme thus interest numerous search applications. argue although already efﬁcient better sampling scheme considered metareasoning perspective value information mcts sr+cr scheme presented paper differs mainly ﬁrst step rollout attempt minimize ‘simple’ selection regret rather cumulative regret. theoretical analysis empirical evaluation provide evidence better general performance proposed scheme. plan rollouts. ideally using control sampling ab-initio even better theory still speed. instead suggest voi-aware sampling scheme based crude probability value estimates despite simplicity already shows marked improvement minimizing regret. however application theory rational metareasoning monte carlo tree search open problem solid theoretical model empirically efﬁcient estimates need developed. finding better sampling scheme non-root nodes well root node also possible. although cumulative regret reasonably well there optimal meta-reasoning principles imply optimal scheme nodes must asymmetrical finally applying methods complex deployed applications already mcts challenge addressed future work. particular extremely successful computer proposed scheme evaluated domain. non-trivial since programs typically non-pure versions extended domain-speciﬁc knowledge. example pachi typically re-uses information rollouts generated earlier moves thereby violating underlying assumption information used selecting current move. early experiments shown voi-aware scheme apears dominate uct. nevertheless also possible adapt voi-aware schemes take account expected re-use samples another topic future research. research partially supported israel science foundation grant lynne william frankel center computer sciences paul ivanier center robotics research production management.", "year": 2012}