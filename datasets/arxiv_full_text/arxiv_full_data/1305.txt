{"title": "Dropout improves Recurrent Neural Networks for Handwriting Recognition", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Recurrent neural networks (RNNs) with Long Short-Term memory cells currently hold the best known results in unconstrained handwriting recognition. We show that their performance can be greatly improved using dropout - a recently proposed regularization method for deep architectures. While previous works showed that dropout gave superior performance in the context of convolutional networks, it had never been applied to RNNs. In our approach, dropout is carefully used in the network so that it does not affect the recurrent connections, hence the power of RNNs in modeling sequence is preserved. Extensive experiments on a broad range of handwritten databases confirm the effectiveness of dropout on deep architectures even when the network mainly consists of recurrent and shared connections.", "text": "abstract—recurrent neural networks long short-term memory cells currently hold best known results unconstrained handwriting recognition. show performance greatly improved using dropout recently proposed regularization method deep architectures. previous works showed dropout gave superior performance context convolutional networks never applied rnns. approach dropout carefully used network affect recurrent connections hence power rnns modeling sequences preserved. extensive experiments broad range handwritten databases conﬁrm effectiveness dropout deep architectures even network mainly consists recurrent shared connections. unconstrained ofﬂine handwriting recognition problem recognizing long sequences text image text available. constraint setting text written given language. usually pre-processing module used extract image snippets contains single word line recognizer. handwriting recognizer therefore charge recognizing single line text time. generally recognizer able detect correlation characters sequence information local context presumably provides better performance. readers referred extensive review handwriting recognition systems. early works typically hidden markov model hmm-neural network hybrid system recognizer. however hidden states hmms follow ﬁrst-order markov chain hence cannot handle longterm dependencies sequences. moreover time step hmms select hidden state hence hidden states typically carry bits information dynamics recurrent neural networks limitations shown effective sequence modeling. recurrent connections rnns principle store representations past input events form activations allowing model long sequences complex structures. rnns inherently deep time many layers make training parameters difﬁcult optimization problem. burden exploding vanishing gradient reason lack practical applications rnns recently lately advance designing rnns proposed namely long short-term memory cells. lstm carefully designed recurrent neurons gave superior performance wide range sequence modeling problems. fact rnns enhanced lstm cells several important contests currently hold best known results handwriting recognition. emerging deep learning movement dropout used effectively prevent deep neural networks lots parameters overﬁtting. shown effective deep convolutional networks feed-forward networks best knowledge never applied rnns. moreover dropout typically applied fully-connected layers even convolutional networks work show dropout also used rnns certain layers necessarily fully-connected. choice applying dropout carefully made affect recurrent connections therefore without reducing ability rnns model long sequences. impressive performance dropout extensions technique proposed including dropconnect maxout networks approximate approach fast training dropout theoretical generalization bound dropout also derived. work consider original idea dropout section presents architecture designed handwriting recognition. dropout adapted architecture described section iii. experimental results given analyzed section last section dedicated conclusions. recognition system considered work depicted fig. input image divided blocks size four lstm layers scan input different directions indicated corresponding arrows. output lstm layer separately convolutional layers features ﬁlter size convolutional layer applied without overlaping biases. seen subsampling step trainable weights rather deterministic subsampling function. activations convolutional layers summed element-wise squashed hyperbolic tangent function. process repeated twice different ﬁlter sizes numbers features top-most layer fully-connected instead convolutional. ﬁnal activations summed vertically softmax layer. output softmax processed connectionist temporal classiﬁcation multidirectional lstm layers lstm cells carefully designed recurrent neurons multiplicative gates store information long periods forget needed. four lstm layers applied parallel particular scaning direction. network possibility exploit available context. elegant approach computing negative log-likelihood sequences whole architecture trainable without explicitly align input image corresponding target sequence. fact architecture featured winning entry arabic handwriting recognition competition openhart used optical model recognition system. paper improve performance optical model using dropout described next section. originally proposed dropout involves randomly removing hidden units neural network training keeping testing. formally consider layer units d-dimensional vector activations. dropout probability applied layer activations dropped htrain element-wise product binary mask vector size element drawn independently bernoulli testing units retained activations weighted htest dropout involves hyper-parameter common value fig. dropout applied feed-forward connections rnns. recurrent connections kept untouched. depicts recurrent layer inputs output layer comprise full shared connections. network unrolled time steps clearly show recurrent connections. recurrent connections order conserve ability rnns model sequences. idea illustrated fig. dropout applied feed-forward connections recurrent connections. construction dropout seen combine high-level features learned recurrent layers. practically implemeted dropout separated layer whose output identical input except dropped locations implementation dropout used stage deep architecture providing ﬂexibility designing network. another appealing method similar dropout dropconnect drops connections instead hidden units values. however dropconnect designed fullyconnected layers makes sense drop entries weight matrix. convolutional layers however weights shared actual weights. dropconnect applied convolutional layer weights sample different models training. contrast approach drops input convolutional layers. since number inputs typically much greater number weights convolutional layers dropout approach samples bigger pool models presumably gives superior performance. neural network smaller dropout rate typical value might slow convergence lead higher error rate. paper architecture covolutional layers recurrent layers. network signiﬁcantly deep still typical dropout rate yielding superior performance. improvement attributed keep recurrent connections untouched applying dropout. note previous works dropout seem favor rectiﬁed linear units tanh sigmoid network nonlinearity since provides better covergence rate. experiments however relu give good performance lstm cells hence keep tanh lstm cells sigmoid gates. three handwriting datasets used evaluate system rimes openhart containing handwritten french english arabic text respectively. split databases disjoint subsets train validate evaluate models. size selected datasets given table images used experiments consist either isolated words isolated lines scanned recall network architecture presented section designed resolution. assess performance system measure character error rate word error rate computed normalizing total edit distance every pair target recognized sequences characters simply classiﬁcation error rate case isolated word recognition normalized edit distance sequences words case line recognition. optical models trained online stochastic gradient descent ﬁxed learning rate objective function negative log-likelihood computed ctc. weights initialized sampling gaussian distribution zero mean standard deviation simple early stopping strategy employed regularization methods dropout used. dropout enabled always dropout probability since features layer dropout sample great number networks. moreover since inputs layer smaller sizes lower layers subsampling dropout layer take much time training. previous work suggests dropout helpful size model relatively network suffers overﬁtting. control size network change number hidden features recurrent layers. baseline architecture features topmost layer vary among parameters kept ﬁxed network trained without dropout. model highest performance validation selected evaluated corresponding test set. results given table seen dropout works well rimes signiﬁcantly improves performance regardless number topmost hidden units. openhart dropout also helps units hurts performance units likely model units underﬁtted. fig. depicts convergence curves various architectures trained three datasets dropout disabled enabled. experiments convergence curves show dropout effective preventing overﬁtting. dropout disabled rnns clearly suffer overﬁtting validation dataset increases certain number iterations. dropout enabled networks better regularized achieve higher performance validation end. especially openhart since training validation sets much larger rimes hidden units inadequate training takes long time converge. units dropout seems overﬁtted. however dropout enabled units give good performance. dropout multiple layers explore possibilities using dropout also layers topmost lstm layer. architecture lstm layers hence tried applying dropout topmost three lstm layers. normally dropout applied layer double number lstm units layer. keep number active hidden units using dropout baseline hidden units active. remind baseline architecture consists lstm layers units would correspond architecture units dropout applied every layer. since free parameters networks concentrate layers doubling last lstm layer almost doubles number free parameters. therefore also several experiments keep last lstm layer units dropout. besides order avoid favouring models trained dropout greater capacity also test architectures without dropout. lower layers generally obtain higher performance. however observed overﬁtting rimes features lowest lstm layers. makes sense rimes smallest three datasets. dropout decrease almost relative basis. found dropout lstm layers generally helpful however training time signiﬁcantly longer term number epochs convergence time epoch. note results presented table directly compared state-of-the-art results previously published databases since rnns output unconstrained sequences characters. complete system large vocabulary handwriting text recognition includes lexicon language model greatly decrease error rate inducing lexical constraints rescoring hypotheses produced optical model. order compare approach existing results trained best rnns database without dropout lines text. whitespaces annotations also considered targets training. concretely build hybrid hmm/rnn model. one-state label transition outgoing transition probability. emission probabilities obtained transforming posterior probabilities given rnns pseudo-likelihood. speciﬁcally posteriors divided priors scaled factor state i.e. character blank whitespace input. priors estimated training set. include lexical contraints decoding phase finite-state transducer decoding graph inject predictions. method create compatible outputs described whitespaces treated optional word separator lexicon. also represented composed lexicon language model image pseudo-likelihoods given language model optical scaling factor balancing importance optical model language model word insertion penalty. parameters along prior scaling factor tuned independently database validation set. applied -gram language model trained brown wellington corpora. passages corpus appearing validation evaluation sets removed prior training. limited vocabulary frequent words. resulting model perplexity rate validation rimes used vocabulary made words training set. built -gram language model modiﬁed kneser-ney discounting training annotations. language model perplexity rate evaluation set. openhart selected words vocabulary containing words training set. trained gram language model training annotations interpolated kneser-ney smoothing. language model perplexity rate evaluation set. results presented tables ﬁrst rows present error rates rnns alone without lexical constraint. seen dropout gives relative improvement. third rows present error rates adding lexical constraints without dropout. case valid sequences characters outputed relative improvement systems without lexical constraints dropout lexical constraints enabled dropout achieves relative improvement relative improvement wer. using single model closed vocabulary systems outperform best published results databases. note line table system presented adopts open-vocabulary approach order better understand behaviour dropout training rnns analyzed distribution network weights intermediate activations. table shows norm weights lstm gates cells topmost lstm layer weights topmost lstm layer softmax layer noticeable classiﬁcation weights smaller dropout enabled. regularization method dropout seems similar regularization effects weight decay. nice difference hyper-parameter dropout much less tricky tune weight decay. hand lstm weights tend higher dropout analysis intermediate activations shows distribution lstm activations wider spread. side effect partly explained hypothesis dropout encourages units emit stronger activations. since units randomly dropped training stronger activations might make units independently helpful given complex contexts hidden activations. furthermore checked lstm activations saturated effect dropout. keeping unsaturated activations particularly important training since ensures error gradient propagated learn long-term dependencies. presented dropout work recurrent convolutional layers deep network architecture. word recognition networks dropout topmost layer signiﬁcantly reduces performance improved dropout applied multiple lstm layers. experiments complete line recognition also showed dropout always improved error rates whether rnns used isolation constrained lexicon language model. report best known results rimes openhart databases. extensive experiments also provide evidence dropout behaves similarly weight decay dropout hyper-parameter much easier tune weight decay. noted although experiments conducted handwritten datasets described technique limited handwriting recognition applied well application rnns. work partially funded french grand emprunt-investissements d’avenir program pacte project partly achieved part quaero program funded oseo french state agency innovation. marti bunke using statistical language model improve performance hmm-based cursive handwriting recognition systems hidden markov models. river edge world scientiﬁc publishing inc. available http//dl.acm.org/citation.cfm?id=. marukatat artires gallinari dorizzi sentence recognition hybrid neuro-markovian modeling international conference document analysis recognition hochreiter vanishing gradient problem learning recurrent neural nets problem solutions international journal uncertainty fuzziness knowledge-based vol. menasri louradour a.-l. bianne-bernard kermorvant french handwriting recognition system rimesicdar competition document recognition retrieval conference nion menasri louradour sibade retornaz p.-y. m´etaireau kermorvant handwritten information extraction historical census documents international conference document analysis recognition bluche louradour knibbe moysset benzeghiba kermorvant arabic handwritten text recognition system openhart evaluation international workshop document analysis systems accepted. deng abdel-hamid deep convolutional neural network using heterogeneous pooling trading acoustic invariance phonetic confusion international conference acoustics speech signal processing dahl sainath hinton improving deep neural networks lvcsr using rectiﬁed linear units dropout international conference acoustics speech signal processing wang understanding dropout strategy analyzing effectiveness lvcsr international conference acoustics speech signal processing neural networks using dropconnect international conference machine learning goodfellow warde-farley mirza courville bengio maxout networks international conference machine learning graves fernandez gomez schmidhuber connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks international conference machine learning graves schmidhuber ofﬂine handwriting recognition multidimensional recurrent neural networks advances neural information processing systems koller schuurmans bengio bottou koller schuurmans bengio bottou eds. press available http//dblp.uni-trier.de/rec/bibtex/conf/nips/gravess marti bunke iam-database english sentence database ofﬂine handwriting recognition international journal document analysis recognition vol. available http//dx.doi.org/./s hinton dahl dropout simple effective improve neural networks advances neural information processing systems available http//videolectures.net/ nips hinton networks/ messina kermorvant surgenerative finite state transducer n-gram out-of-vocabulary word recognition iapr workshop document analysis systems accepted. espana-boquera castro-bleda gorbe-moya zamoramartinez improving ofﬂine handwritten text recognition hybrid hmm/ann models ieee transactions pattern analysis machine intelligence vol. graves liwicki fern´andez bertolami bunke schmidhuber novel connectionist system unconstrained handwriting recognition. ieee transactions pattern analysis machine intelligence vol. dreuw doetsch plahl hierarchical hybrid mlp/hmm rather features discriminatively trained gaussian comparison ofﬂine handwriting recognition international conference image processing kozielski doetsch hamdani multilingual offline handwriting recognition real-world images international workshop document analysis systems tours loire valley france apr.", "year": 2013}