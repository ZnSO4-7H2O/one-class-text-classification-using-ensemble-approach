{"title": "Durkheim Project Data Analysis Report", "tag": ["cs.AI", "cs.CL", "cs.LG", "I.2.1; I.2.2; I.2.7; J.4"], "abstract": "This report describes the suicidality prediction models created under the DARPA DCAPS program in association with the Durkheim Project [http://durkheimproject.org/]. The models were built primarily from unstructured text (free-format clinician notes) for several hundred patient records obtained from the Veterans Health Administration (VHA). The models were constructed using a genetic programming algorithm applied to bag-of-words and bag-of-phrases datasets. The influence of additional structured data was explored but was found to be minor. Given the small dataset size, classification between cohorts was high fidelity (98%). Cross-validation suggests these models are reasonably predictive, with an accuracy of 50% to 69% on five rotating folds, with ensemble averages of 58% to 67%. One particularly noteworthy result is that word-pairs can dramatically improve classification accuracy; but this is the case only when one of the words in the pair is already known to have a high predictive value. By contrast, the set of all possible word-pairs does not improve on a simple bag-of-words model.", "text": "dcaps program. models built primarily unstructured text several hundred patient records obtained veterans health administration models constructed using genetic programming algorithm applied bag-of-words bag-of-phrases datasets. inﬂuence additional structured data explored found minor. given small dataset size classiﬁcation cohorts high ﬁdelity cross-validation suggests models reasonably predictive accuracy rotating folds ensemble averages particularly noteworthy result word-pairs dramatically improve classiﬁcation accuracy; case words pair already known high predictive value. contrast possible word-pairs improve simple bag-of-words model. central goal durkheim project build classiﬁer suicide ideation prediction suicide risk based free-text clinician notes contained medical records obtained veterans health administration dartmouth-hitchcock medical center intended classiﬁer clinician determining suicide risk prospective patients. such able digest patient data assign risk level green/yellow/red suggesting likelihood suicidal ideation. order understand build classiﬁer extensive analysis medical records patients performed. patient records divided three cohorts. consist control group patients suicide cohort patients psychiatric cohort patients medical records consist primarily free-text notes entered clinician well additional structured data inclusion structured data training makes slight improvement overall score clinician notes include nurse doctor notes ranging mundane procedures ’patient received inﬂuenza vaccine order’ descriptions serious procedures ’ultrasound abdominal aorta done on...’ number semiautomatic script-generated tables ’issue date status last fill outpatient medications reﬁlls expiration’ well psychologically charged entries ’little interest pleasure things’. notes discuss psychological state including screenings depression alcoholism appear three cohorts. presume common delve deeper last cohorts. aside kind quick cursory review validate general form records deeper review examination performed. data analysis performed using supervised training genetic programming system build models datasets. models constructed converting free-text records ’bag words’ simple numerical count often given word appears context certain patient record. given model identiﬁes words taken combination serve predictors suicide. nature genetic programming system used build many different models depending initial random seed. thus data analysis consisted primarily generating ensemble averages models trained dataset. model validation performed using -fold cross-validation setting aside dataset testing training remaining /ths. model accuracy used score total fraction correct answers. data analysis focused building binary classiﬁer distinguish group done several reasons. important reason simply largest groups terms total word-count thus presented greatest amount data work with. equally important reason though clinical perception groups hard even impossible distinguish. contrast control group consists patients obtaining non-psychiatric medical care thus almost completely devoid references psychological state. such principle easy distinguish simply lacks vocabulary. results binary classiﬁers trained distinguish groups well group group also presented. many words appearing models emotionally charged psychologically signiﬁcant ’ptsd’ ’weapons’ ’overdose’. taken individually words meaningful clinically ordinary. thus question arises phrases words part thus phrase ’negative assessment ptsd’ carries different meaning ’positive assessment’ thus potentially useful feature classifying suicidal patients. suggests ’bag-of-phrases’ approach accurate bag-of-words model indeed found case. particular models built using certain word-pairs signiﬁcantly better scores single-word models best scores overall. besides word pairs trigrams -grams also explored offer improvements hypothesized training datasets small noticeable effect these. improvement word-pairs seen initial selection ’cut’ made word-pair used words pair already correlates well cohort. without using word-pairs improve score fact lowers easier over-train case. chance) selectively-chosen word-pairs individual model scores ranged ensemble average authors appears remarkable achievement given small size dataset fragmentary nature clinician notes. remainder document structured provide mode detailed review model building validation process size content clinician notes various results obtained. model building consists several stages. initial stage converts free-text data words. simply count word frequency nothing more count often given word used particular patient’s medical report. bag-of-words models completely ignore sort linguistic structure original text well ignoring punctuation structural markup typically thousand different words found depending cohort examined. words spell-checked stemmed include many typographical errors well large number abbreviations hospitals clinics departments tests procedures orders. next stage consists ’feature selection’. rather training discriminator directly full word counts reduced several thousand words judged signiﬁcant predicting outcome. done several ways. possible remove words occur less dozen times. although intent remove noise data possible perhaps signiﬁcant indicators lost well; thus data analysis includes experiments adjusting cut. another possible count word stems consolidate counts singular plural forms noun consolidate past present future tenses verbs. important choose words whose counts correlate well patient grouping. done computing ’mutual information’ group word-count frequency. thousand words highest selected used ﬁnal model-building stage. feature selection important step model building counter-intuitive effect ﬁnal model often case limiting number features used build model results better accurate model. machinelearning algorithms often focus irrelevant differences classifying groups differences irrelevant fail predictive value. greater number features given learning algorithm likely irrelevant differences; limiting input significant features helps prevent over-training. model building performed using poses/moses machine learning system. system builds candidate representative models ’representations’ data uses evolutionary algorithms discover effective representation. example representation many trained current data shown table example representation built dataset. understood follows moderate_t. takes value ’true’ word ’moderate’ occurs times text exclamation mark indicates condition hold prescribe_t. means word ’prescribe’ occur times. boolean operators ’and’ ’or’ serve conjoin conditions thus saying that word ’moderate’ appears least twice word ’prescribe’ appear words ’concern’ ’evidence’ ’increasing’ ’restricted’ appear least once word ’albuterol’ appears least twice patient classiﬁed belonging group note that approximately twenty-ﬁve thousand unique words appearing data really rather small subset. ﬁnal classiﬁer consists representation many ranging hundred depending parameter settings. predictions representative used cast vote; ﬁnal determination follows tally votes. process ensemble averaging eliminates considerable variation accuracy model next. determine accuracy performance classiﬁer standard k-fold crossvalidation techniques used style validation dataset divided parts. four parts used train model accuracy model measured ﬁfth part. repeats process time leaving different ﬁfth dataset used evaluation. average sessions given overall accuracy. almost data analysis reported done training classiﬁer maximize accuracy minimize false-positive false-negative rates. appropriate approach datasets balanced size here. alternatives maximizing accuracy would maximizing f-score f-score maximizing recall rate precision. none alternatives seem particularly suited dataset; lead unexpected imbalanced effects. example seen later appears considerably easier pick patients suicide risk mixed population pick associated patient note records covering span year. records generated many reasons upon hospital clinic intake patient care notes examination results; results; consultation notes; notes referrals including imaging; outpatient notes; surgery treatment notes; pharmacy notes; ongoing therapy notes; telephone follow-up notes; addenda corrections. thus single patient visit single generate dozen records. dataset tokenized words converting punctuation white-space using white-space word separators. exceptions wordphrases included hyphens underscores; punctuation simply removed create single run-on word. differences capitalization ignored converting words upper-case. normalization dataset found consist nearly million words; precisely words total. distributed across three groups follows number words record fairly uniform across three cohorts. record lengths limited characters record; clear longer records truncated mid-sentence mid-word. appears technical interoperability difﬁculties data processing systems. unique words dataset occurred least once occurred twice more. rough sketch distribution given table many words appear typos miss-spellings common words abbreviations medical terms fair number acronyms including abbreviated names clinics hospital departments procedures orders prescriptions. however also many non-misspelled words appear text abandonment abortive aborted abuser abuses abyss academy accuse achievable achieves acquainted. note many words emotionally meaningful words. whether infrequentlyused serve indicators psychological state unclear. experiments lowfrequency words removed dataset model building reported below. rate clear ’active vocabulary’ frequently used words fairly small. attempt made extract word stems correct exclude ’obvious’ miss-spellings. whether would enhance diminish ability categorize clear priori. inclusion exclusion criteria based vocabulary applied. many different cuts based word-counts mutual information explored detailed below. feature selection stage applied prior model building also effectively removes majority words consideration based purely predictive utility word morphology spelling lexical meaning usage. frequently occurring words shown table function words removed dataset thus appear table. good reason this function words known strong indicators psychological state particular writing suicides known make greater function words pronouns average. probability word obtained taking number times word occurs dividing total word count. here denotes logarithm base-. thus occurs times fraction time. overall word distribution appears obey zipf-mandelbrot quadratic fall-off tail. curved quickly falling tail commonly case natural-language texts. distribution shown word-pairs also explored predictive power well. word pairs constructed considering adjacent words well pairs word apart thus example \"big balloon\" generates three word pairs \"big_red\" \"red_balloon\" big_balloon. ﬁrst pairs particularly meaningful last semantic units. last big_balloon would captured conﬁned oneself adjacent words. eliding middle words semantically signiﬁcant pairs discovered. word pairs equally interesting. semantically meaningful word pairs high mutual information them. mutual information pair words deﬁned here probability seeing word pair divided total number word pairs. probabilities probabilities seeing word pair whose ﬁrst word last word respectively. general scores typically range slightly less zero; true dataset. general word pairs high form lexical units conveying meaning semantic content. collocations often forming idioms phrases. examples word pairs taken dataset include ulterior_motives hlthy_lvng vocalizes_intelligibely gin_tonics roast_beef marathon_runner governmental_entities. contrast lower scores less meaningful. typically boundary meaningful meaningless word pairs occurs around examples dataset include hungry_had had_sweat interact_in word rank distribution dataset. ’normalized word count’ frequency word appears dataset. ’rank’ order word sorted frequency. green line indicates zipf-mandelbrot given blue line quadratic given word distribution english-language texts much ﬂatter shown here. word-pairs incorporated ranking curve also ﬂattens becomes less steep. rx_ibuprofen anything_himself. zero degenerate random nonsense morphine_you recovery_are his_how yes_with pairs words appear next one-another purely coincidence linguistic construction. thus mutual information used exclude low-mi word pairs consideration model building. results models built variety different cuts presented below. word pairs ranked along individual words; overall shape distribution change much; similar shown ﬁgure considerably ﬂatter loosing quadratic fall-off frequency words. various stages feature selection model building validation themselves rather complex require care perform properly. none stages ’pre-determined’ ’automatic’; instead adjustable parameters requires deliberate choice parameters overall conﬁguration. since accuracy ﬁnal classiﬁers depends various parameters settings data processing stages important understand applied. sections immediately provide details describing stages. followed presentation results obtained stages applied. prior performing training dataset bin-counts created. binning helps make relatively sparse data lumping together similar word-counts category ’bin’. serves simplify data boost performance training step. performed counting often word occurs given patient assigning ’occurs twice less four times’. ﬁxed bins different patient records seen contain different numbers words them. natural sizes obtained ﬁrst determining probability distribution given word determining average number times occurs standard deviation average numbers provide natural size bin. example given average number times word occurs patient record that given patient given word occurs average less average; case bins total. another possibility three bins given patient word occur average number times well-below average well average bins serve ’smooth’ consolidate word counts make granular ’ﬁlter high-frequency noise’ data. general less data fewer bins used thus keeping bins fairly full. bins considered; seen later sections bins work best dataset. result binning boolean-valued features. example term ’ptsd’ occurs average times patient record two-bin system would create feature word either true false given patient record. example standard deviation word three-bin system would include features word standard deviation average; true false given patient record. values shown referred ’thresholds’ boundaries bins. thus specifying thresholds results bins. number thresholds parameter speciﬁed; varying parameter results models varying accuracy. number thresholds used word counts thus setting thresholds= speciﬁes bins used words. example given thousand distinct words two-bin system would create thousand features three-bin system would result twice many thousand boolean-valued features. four-bin system would result three times many features clear increasing number thresholds vastly increases dimensionality feature space. binning building model dataset converted collection true/false assignments static feature-selection stage. done reduce size dataset tens thousands features thousand. goal reduction simply improve run-time memory usage model-building stage. given overall dataset consists hundred records seem reasonable hundred features would sufﬁce provide predictive value; indeed ﬁnal models consist dozens words. however runtime speed next stage model-building strongly affected number features given deemed safer side giving many features choose rather this simple efﬁcient feature selection algorithm sufﬁces. algorithm used choose features highest mutual information desired patient classiﬁcation. mutual information deﬁned before except here variable taken classiﬁcation patient belonging group another variable taken denote whether given feature true false. thus certain feature true whenever patient belongs group expect large; likewise anti-correlate large. continue previous example mutual information content computed word ’ptsd’. ranks thousand accepted valid feature worth exploring during training stage. words occur equally often group another score thus selected. data analysis presented below highest-ranked features selected. represents anywhere total number features depending number bins chosen particular datasets examined. particularly strong reason choosing opposed correlation measure tf-idf. strong mathematical foundation rooted maximum entropy principles. discriminate rare words; word occurs infrequently still correlates well patient grouping reasonable score thus eminently suitable classiﬁer. case particular choice feature selection algorithm little impact model building. technically difﬁcult intensive stage processing creation models data. step performed meta-optimizing semantic evolutionary search system. system searches large representations shown table locates accurately training data. moses algorithm consists nested loops representation-building genetic-algorithm search. system starts creating program tree nodes leaves tree free vary full range input variables well vary boolean operators ﬁxed choice nodes leaves resulting tree scored input training data well ﬁts; clearly choices better others. node leaf settings explored using genetic evolutionary search algorithm combining hillclimbing genetic cross-over. improvements found process begun again time different usually complex program tree. step repeated either perfect score reached time-limits exceeded. generation candidate program trees involves second ’dynamic’ featureselection stage. candidate tree created older high-scoring tree decorating additional candidate features. rather creating candidate program tree several thousand features convergence improved working features information already tree working features likely improve current high-scoring tree. form feature-selection hereinafter referred ’dynamic feature selection’ selected features depend program tree well dataset different chosen program tree. training effective even small number dynamically selected features best results achieved less hundred technique highly effective little aside improving scores working smaller number features dramatically reduces training time. sify patients make predictions value dependent variable based input variables tested test patients held training group seen accuracy representations test data considerably variable. priori knowing representation performs ’the best’ test data. overcome variability ensemble created representation ensemble getting vote determine ﬁnal classiﬁcation. inputs presented representation representation making prediction majority vote taken determine ﬁnal classiﬁcation. ensemble referred model effectively distilled compressed version training data. theoretical validity using model classiﬁcation founded belief model captures something essential words used text. reasonable belief given industry experience bag-of-words classiﬁers. practical validity model tested several ways; k-fold cross-validation used here. order test validity models k-fold cross-validation performed input dataset split subsets subset containing patient records assigned round-robin selection. training performed using subsets input model built accuracy model evaluated subset held process repeated times obtain models different accuracy test results. test results averaged together obtain estimate overall system accuracy. model trained full data-set accuracy resulting model blind data expected similar cross-validated accuracy. effects choosing different values explored later section. cross-validation four different statistics gathered number truepositives false-positives true-negatives false-negatives models built binary classiﬁers ’positive’ refers membership cohort suicide cohort. thus case false-positives incorrectly classiﬁed suicidal whereas false-negatives patients whose suicide foreseen. four statistics presented form two-by-two table termed ’confusion matrix’. example matrix shown table order clinically useful system probably best that system erred ﬁnding many false positives rather failing detect suicidal patient different result variables capture idea different ways ’recall’ ’precision’ ’accuracy’ f-score f-score. ’recall’ addresses question were true positives identiﬁed ’precision’ opposite were false positives minimized accuracy fand different ways blending together obtain reasonable composite scores. presuming high recall clinically desirable classify patients f-score here stands ’true-positive’ above. quantities vary system perfect score quantities would equal classiﬁcation done random chance recall accuracy would equal precision would fractional size positive group data analysis concerned groups equal size desired quantities note possible classiﬁer simultaneously scores measures others. follows concept ensemble used related rather distinct ways. ﬁrst sense already discussed above model consists ensemble representations; representation gets vote determine ﬁnal classiﬁcation model makes. construction nature individual representations remain rather opaque effect ﬁnal classiﬁcation indirect. order gain insight individual representations combine form ensemble restriction made follows limit model holds single representation. thus follows ensemble overt behavior overtly explicitly presented. distribution classiﬁcations made representation average behavior variance explicitly presented. since model holds representation ensemble referred ensemble models. however wants revert intended purpose ensemble improve accuracy combining multiple representations model performing classiﬁcation majority vote. case accuracy model presumably depend number representations within exploration accuracy depends size ensemble given ﬁnal sections. summarize ensembles ensemble representations comprising model ensemble models. classiﬁer incorrect it’s assignment course. follows presumed classiﬁers binary group ranges values {posneg} denoting patient belong group. excluded middle assumed patients divided training test classiﬁer trained training directly measured evaluated test set. spos sneg sets patients test positive negative belonging group given classier gives following counts true positives models making ensemble size set. essence ensemble average expectation value. note ensemble average real-valued quantity ranging interval poses inference command uses ensemble average perform classiﬁcation reports average ’conﬁdence’ inference. speciﬁcally number different data analysis experiments performed. include effect tuning adjustable parameters machine-learning system exploration ensemble averages examination words appeared actual models effect data cuts predictive value word-pairs trigrams -grams. running classiﬁer once given parameters results single model created. precise model accuracy depends training parameters run-time number features selected number representations comprising model variables. cases resulting model training data well. case typical shown table model evaluated test accuracy measures course sharply lower. essence model over-ﬁt train set. results best-ﬁt model test shown table confusion matrix form table training set. model predictions shown columns expected results rows. training records classiﬁed -fold cross validation. results shown indicate model created training data well excelling measures. expected training set. data shown classiﬁer distinguishes groups trained bag-of-words dataset. features pre-selected features dynamically selected word-count thresholds used. practice parameters almost effect results essentially parameter settings result similar measures. results shown model trained pre-selected features dynamically narrowed features run. input features created partitioning word-counts levels threshold word-count average. model selected maximize accuracy rather recall score; however appears best score explored. model consists representations resembling shown table given representations positive negative keywords extracted. positive keywords appear target group control group. negative keywords reverse appear frequently control group target group. positive keywords distinguishing groups shown table negative keywords table note also fair number keywords apper typographic errors otherwise relatively rare. easily explained rare words appear relatively records thus presence gives immediate mechanism identify records. unfortunately also means keywords also poor predictive value fact word mis-spelled particular patient record unlikely future classifying patients. however also counter-productive exclude keywords becuse seem obviously relevant. example ’albuterol’ sometimes appears among postive keywords; superﬁcially asthma medication thus non-predictive irrelevant. however also well-known associated suicide risk. telling noise data examining keywords easy task. role infrequent words explored later section. adult antipsychotic appoint bars cahnge cancers consistantly despondent disorder drusen frightening icdcm intermittant lipitor monfri nalcohol nobody private punctum regained reorder restricted shave spare spell standards straightened strange street stvhcs subsalicylate swabs tach telemetry temazepam transfusions travels turmoil tuscon twave ultimately uncooperative undergone unresectable urinated valley videos visualization vtach watches whip worthlessness wtih younger list positive keywords distinguish groups model requires words appear frequently group group note appearance fair number emotionally laden words. models result particular word-list; differences different models discussed later section. afraid aggravating alignment aloh alot arouse blockers bronze crying demonstrating dent directory disheveled docusate effected epidermal fear geropsychiatry mealtime neut nocturia notably obesity outstanding practicing prefilled preoccupied presbyopia psychiatrically quadrants ranges residential ruminates specimen splitting stain struggles student style styles subluxation supervisor supervisors supply symptoms teacher teaspoonful teeth tempopormand tfts topical trazodone ucinations unchanged unhappy unique unmarried unpleasant unsp upcoming usefulness verified virtue visa visit voiding volume walkin warning warrant wellgroomed willing wounded xpatient year list negative keywords distinguish groups model requires words appear less frequently group group given group psych patient group surprising many words seem psychiatric signiﬁcance. space possible models dataset astronomically large cannot exhaustively searched. moses/poses system uses pseudo-random number generator explore different parts search space genetic algorithm part search also representation construction. resulting ﬁnal model thus depends initial random number seed; well model scores well. clear well score individual model trusted priori argument always extend good larger dataset. mitigate uncertainty ensemble average used. case average large number models built different initial random number seed used. much follows ensemble averages used. cases distinct models built. ﬁgure illustrates works shows bar-graph accuracy scores different models created parameters dataset differing initial random number seed. ﬁgure shows bell curve data. later section looks model differences greater detail. classiﬁer performance depends strongly choice training set. obtain idea training inﬂuences accuracy scores several different experiments performed summarized table cases total different training/test partitions created performing random draw allows average accuracy obtained across different test sets well standard deviation distribution. data presented bag-of-word-pairs model presented greater detail later sections. overall suggestion table maximizing size training making small size test averaging many partitions best strategy. remainder analysis split averaged round-robin partitions used -fold cross validation. time-consuming experiments determine optimal settings training parameters. important sensitive number bins chosen word-counts number dynamic features. size static feature list seems little bearing ultimate score reasonably large; static feature seems sufﬁcient. total training time seem matter much sufﬁciently long. increasing training time cause system build ever-more complex models attempting attain perfect score training set. complex models appear score better test appear score worse either. chart shows distribution model accuracy scores models built distinguish cohort cohort bag-of-words dataset. accuracy scores model assigned wide; thus bars graph. models trained single-word bag-of-words dataset word-count thresholding bins dynamically chosen features initial feature size ﬁtted curve gaussian mean standard deviation graph suggests ’typical’ accuracy single model although models score exceptionally well including models accuracy better. clear distribution fact gaussian; possible log-normal distribution would provide better note log-normal distribution would centered location note chart shown ﬁgure compared models. although shows best bag-of-words model outperformed bag-of-phrases models. average accuracy standard deviation different training/test partitions. partition allocates patients training test set. three different experiments shown exactly partitions initialize learner different random seeds. experiment explores accuracy completely different partitions. rows labeled explore effects reducing size training set. used ’focus’ distinctive parts dataset much image processing used sharpen image. larger datasets seems less need ’focus’; whether real effect artifact unclear. datasets small largest dataset three times size smallest one. order evaluate effect parameter tuning ensemble averages models used described above. parameters mean standard deviation accuracy distribution computed. these function parameters shown table ﬁgure shows three typical distributions table already shown ﬁgure infrequently occurring words appear play important predictive role. ’infrequent’ meant words appear less fourth patient records possibly two. quite remarkable result manifests several ways data. raises questions artifact working sparse data possible suicidal patients present variety ways common symptoms? section explores infrequent words inﬂuence model construction model accuracy. using ensemble averages question arises similar different individual models? differ little lot? measuring precise differences difﬁcult fact actual representations boolean program trees. however general indication generating keyword lists comparing models. table shows effect mean accuracy tuning classiﬁer parameters. entries table models built dataset bag-of-words dataset distinguishes groups table shows mean accuracy standard deviation -fold validation models. models trained features pre-selected. number features dynamically selected run-time indicated ﬁrst column. thresholds used word-counts bins thresholds. threshold used always mean word count. thresholds used standard deviation mean word count. three thresholds uses mean standard deviation below. histograms -feature case shown ﬁgure seems using threshold usually always best. dependence number dynamical features somewhat uneven. ﬁrst three columns graphed below. example accuracy score distributions three different parameter settings. three bar-graphs built dataset bag-of-words dataset distinguishes groups three sets models trained features pre-selected features dynamically selected run-time. difference whether word-counts binned bins thresholds. threshold used always mean word count. thresholds used standard deviation mean word count. three thresholds uses mean standard deviation below. observe almost classiﬁcation effect derived using threshold adding improves classiﬁcation slightly. observe threshold located mean appears important; used classiﬁcation suffers. always case; parameter settings fewer dynamic-runtime features used situation reversed even number thresholds work better number. case adding thresholds always improves score; sometimes leads over-training instead evident table representations. model generated using exactly parameters differing initial random number seed thus true ensembles. asks many words shared representations? many shared representations? many shared half them? answer questions words shared representations shared representations half representations. rather surprisingly counts depend much number different models look models number words shared common stays less same. shown ﬁgure percentages graphed models case many words acting synonyms another literal sense meaning rather used similar contexts. perhaps common words indicative patient records them others others. perhaps different situation patient record words also many others. case would enough pick words build model different models picked different words inter-changeable models superﬁcially deeply different. cluster analysis would need performed determine this. collection words rank words chosen model? hinted ﬁgure clearly seen infrequently-used words vital distinguishing patient groups. indeed would appear distinguishing words fairly small counts exceptions. observe rare words used model building tens thousands words appear less times text; these less hundred selected model. however dependence rare words model building indicates system keying attributes shared handfuls patients. clear artifact small dataset size whether different patients showing distinct non-overlapping ’symptoms’. recall patients total discriminated models. thus word appears times total entire text word select patients happening patients presenting speciﬁc way? records sparse perhaps patients would present simply observed noted? words suicidal patients present classes distinct behavior patterns? commonality suicidal behavior particular evident data. different measure importance infrequent words obtained excluding model building creating models word lists include words occur times text. superﬁcially seems like wise idea. word appears patient record found training phase impossible word also appear test patient records. thus cannot contribute accuracy model test positive negative cohorts missing word; graph shows fraction representations share words common. thus highest ranked word used largest number representations next highest ranked word used next greatest number representations models representations used unique words among them sharing many them. models used unique words models used unique words. thus creating models cause words employed diminishing rate. smooth line labeled ’zipf mandelbrot’ rough data given formula result phenomenological. graph reproduces ﬁgure high-lighting words used construct models green. total words highlighted green taken model collection. clear that exceptions words used distinguish patient groups words infrequently used. lower right corresponds words appear amongst patient texts. next green corresponds words appear twice thus graph makes clear words appear small number times vital distinguishing patient groups. note that although green crosses appear dominate lower right graph partly illusion green crosses total whereas thousands crosses lower right. thus although words appear twice text vital model building tiny fraction actually used. table shows ensemble averages accuracy infrequent words dataset. thus labeled indicates results words appearing fewer times dataset. results usual ensemble models. models trained parameters statically selected features dynamically selected features thresholds indicated. parameter choice results highest score cuts made shown ﬁgure ﬁgure table cutting words appear cutting none all. predictive value. word appears patient records unlikely locations test-set thus might also believe words little predictive value. perhaps accuracy increased cutting dataset discarding words appear fewer times dataset. much case. results shown table cutting rare words decreases model accuracy. modest even words large impact scores cutting essentially wipes predictive accuracy model almost completely. common issue arises machine learning applied sentiment analysis positive negative keywords negated text inverting meaning. example keyword unhappy occur sentence unhappy. another issue semantic meaning conﬁned single words associated word pairs collocations idioms. looking merely word collocation imply less reﬁned meaning possibly completely different meaning altogether thus might expect greater predictive value arising using neighboring word pairs even perhaps entire phrases. indeed case demonstrated section. dataset. simply including possible word pairs improve model accuracy. reason well-known including word-pairs number candidate features might data enlarges much larger number. statistical chance means correlate strongly training even though actually predictive. discarding word pairs mutual information score obvious make; also contemplate discarding infrequent word pairs although experience single words suggests good idea. alternative discarding word pairs consider word pairs involve word previously identiﬁed ’predictively signiﬁcant’ word already occurs single-word model tables word pairs constructed words ’clinically interesting’ provide larger window notes occurring patient record. sense approach inspired central idea corpus linguistics order better understand meaning word best view context used. inspired idea reasonable contemplate using three-word phrases -word phrases construct bag-of-phrases. follows n-grams referred ’corpus n-grams’ indicating constructed ’clinically interesting’ words. draw distinction these n-grams scores. sort approach known provide positive beneﬁt classiﬁcation. create list ’signiﬁcant words’ ensemble models trained group group dataset. noted previously caption ﬁgure ensemble results unique words. corpus n-grams selected considering n-grams contained words. follows n-grams constructed adjacent words also adjacent words ’holes’ them. reason properly take account multi-word noun verb modiﬁers. thus example phrase horrible frightening experience composed semantically interesting units horrible_experience frightening_experience ﬁrst would captured limited oneself solely adjacent words creating pairs. likewise constructing -grams three adjacent words considered also possibilities picking three words string four consecutive words. creating -grams possibilities picking words consecutive words considered. bag-of-n-grams constructed also includes n-grams shorter thus bag-of-pairs also includes single words bag-of-trigrams also includes pairs single words thus model building pair trigram used results better model using individual word. idea allowing holes n-gram construction partly emulate action syntactic parser would able identify meaningful semantic relationships adjective-noun even subject-verb. place syntactic parsing high mutualinformation phrases help identify meaningful phrases ways even superior given fractured badly-structured non-grammatical content notes. unfortunately even approach insufﬁcient deal long-range correlations words text. example given occurs note part semi-automated system ptsd screening whose full content appears chart compares three different bag-of-phrases models highest performing bag-of-words model. observe bag-of-phrases models outperform best bag-of-words model. results parameter tuning shown table note have ever experience frightening horrible upsetting that past month nightmares thought want note appears control-group patient; presence semi-automatically generated notes adds classiﬁcation challenge. clear extract kind information; challenge similar ways anaphora resolution perhaps techniques area could applied. corpus-linguistics-inspired approach considering word phrases contain words previously identiﬁed ’signiﬁcant’ works well. illustrated ﬁgure results four different experiments shown best bag-of-words result best corpus-pairs result best corpus-trigram result best corpus -gram result. particularly noteworthy bag-ofphrases models perform better best bag-of-words model. these outstanding word-pairs results. creating corpus-pairs requires previously computed list ’signiﬁcant words’. creating list time-consuming since requires training ensemble extracting words training again pairs. thus natural simpler ways obtaining list ’signiﬁcant words’ good. fact simple single-word feature selection sufﬁcient create graphed ﬁgure datasets subjected cuts words n-grams occurred fewer times n-grams less chart graphs table contents. bars showing standard deviation plotted comparison word-pair ensemble accuracy results sets corpus word-pairs created different sources ’signiﬁcant words’. ensemble ’signiﬁcant words’ consist words taken ensemble models. featureselection ’signiﬁcant words’ consist words selected maximizing mutual information word counts cohort ensembles trained parameters reported above static features dynamic features threshold group group dataset. list ’signiﬁcant words’ every good obtained ensemble maybe even little better shown table make methods comparable simple mutual-information-maximizing feature selection step performed select words number words obtained ensemble. feature selection runs seconds whereas training ensemble models take hours. distribution corpus-pairs all-pairs dramatically different shown ﬁgure selecting corpus pairs thousands highest-mi pairs discarded well low-mi pairs well. perhaps possible replicate corpuspairs results applying simple all-pairs dataset merely discarding low-mi pairs? seem case shown table table compares bag-of-words model several different all-pairs models different cuts applied. including pairs improve score bag-of-words. cutting low-mi pairs score improved somewhat effect dramatic; certainly strong decision corpus-pairs. original bag-of-phrases results shown tables ﬁgure made employing arbitrary ’intuitive’ cuts number words mutual information. later experiments effect cutting rare words shows negative effect documented table perhaps mistake rare words rare word-pairs using bag-of-pairs model? doesn’t seem table shows counter-intuitive result. experiment rare words pairs cut; altered. none results approach best accuracy table thus somehow word-pairs come play failing rare words phrases makes things worse distribution corpus-pairs word pairs ranked decreasing order mutual information. distributions rather dramatically different; corpus-pairs distribution zipﬁan segment lacking all-pairs distribution. relative rank pair obtained drawing horizontal line across curves corpus-pairs eliminated high-mutual-information pairs well low-mutual information pairs. ensemble averages accuracy comparing different cuts bag-of-single-words accuracy. simply including word pairs improve quality model. discarding pairs mutual information scores does however positive effect. best location entirely clear though. ensembles trained pre-selected features dynamically selected features count threshold. study effect varying mutual information corpus pairs. models trained parameters four entries labeled pairs mi>x cuts rare words rare pairs show results different cuts. entry labeled best pairs reproduces table namely three cuts besides also cuts words appear fewer times cuts phrases appear fewer times. comparison different word phrase cuts accuracy. models built corpus-pairs dataset reject pairs less trained parameters highest scoring ensemble also rejects words word-pairs occur fewer times. entries explore effect cutting fewer words and/or pairs; none particularly well quite approaching best result. moses immune effect over-training longer training times result better train dataset resulting models perform poorly test dataset. essence longer training times allow system quirks training present test set. minimum amount training done correlation train test scores disappears; even vague hint anti-correlation shown ﬁgure optimum training times explored table training times measured terms number evaluations scoring function single comparison model training table counts evaluation. highest score marked bold second-highest score italic highest score entire table occurs training time evaluations dynamical features high scores occur fewer training evaluations performed. exceptions occur number dynamical features extremely small suggests model builder starved features point must iterate many trials ﬁnding appropriate features. table shows ensemble averages accuracy comparing static dynamic feature selection. usual ensemble consists models; threshold used. dynamic feature-selection results identical reported table obtained statically selecting features dynamically selecting indicated number those. contrast static selection process chooses indicated number features initially makes dynamic selection all. scatter-plots show test train scores models. clear essentially correlation model scored well training poorly test vice versa. left ﬁgure dynamical features right ﬁgure clear x-axis labels using dynamical features helps improve training score lot; however test-score degrades noted table concept ensemble replaces signiﬁcant random variation accuracy single representation trustworthy average accuracy across multiple representations. imply accuracy ensemble model equal average accuracy representations ensemble. multiple representations allowed vote ﬁnal classiﬁcation accuracy classiﬁer usually increases. section explores accuracy model depends number representations voting model. results reported fashion before except model contains representations instead representation. essence ensemble ensembles although model consist representations still explore average accuracy taken models. data presented graphed typical cross-sections shown additional insight gained examining representations voted individual patients. shown ﬁgure given patient receive anywhere votes. vote ’for’ indicates patient belongs group vote ’against’ indicates patient belongs group thus receiving less vote classiﬁed group receiving classiﬁed group graph shows fraction votes received versus known priori patient cohort membership. ideally accurate classiﬁer would always give votes group members always less votes group members. fact classiﬁer sometimes wrong readily apparent graph. voting ensemble results. model consists n=’model size’ representations majority vote determining model classiﬁes. mean standard deviation obtained averaging different models built varying initial random number seed machine learning system. note standard deviation model comparable increases score improves variation shrinks sharply. models word-pair models usual word-pair cuts word pairs considered words score-correlated single words run-time dynamical feature count total training evaluations performed. parameters less identical discussed much paper case correspond highest score seen. left best result models containing single representation i.e. poses trained times parameters varying initial random seed. average accuracy error bars show variation among models scored high mid-upper next point shows results model containing representations. different random seeds used create representations. placed model ’vote’ likely classiﬁcation process repeated times average accuracy models error bars show variation among models best scoring model worst-scoring rightmost point model holding representations. average accuracy best scoring worst-scoring notice best scores always pegged ensemble seems merely trim away outliers scores. ﬁgure shows model comprising representations voted classify individual patients. vote counts divided bins; number patients receiving number votes shown. thus tall green left indicates patients group received fewer votes; patients correctly classiﬁed voting. contrast left indicates patients group received fewer votes; patients misclassiﬁed voting. indeed bars left mark green bars right mark indicate misclassiﬁed patients. rest paper shows performance classiﬁer test using -fold validation. different models created different random seeds show nearly identical vote distribution. mirror image green bars. particular interest classiﬁer overall quite conﬁdent classiﬁcation group patients seen tall right-hand side graph. given patient group classiﬁer correctly classify patient good accuracy high conﬁdence. case group suicide cohort here classiﬁer clearly less accurate tentative assignment. seen left-most green tall rightmost green small might hoped. essence classiﬁer good recognizing psychiatric patients; suicidal patients much. current datasets balanced number patients; suicide risk small general population. classiﬁcation system deployed large scale would need able cope this pull proverbial needle haystack. thus future durkheim project work seems appropriate optimize recall rather accuracy. recall rate classiﬁer measures well classiﬁer able identify true-positives possibly expense high false-positive rate. core presumption would rather ’safe sorry’ over-asses suicide risk miss true-positives. general-population classiﬁers seems best approach would maximize recall rate clamping false-positive rate reasonable level. another alternative would maximize f-score weighted harmonic mean recall precision model. dataset words worthlessness appear often group groups. word despondent appears group highly elevated counts words agitation group. contrast words remarkable absence words crying aggravating absent nearly absent group appear primarily group difference psychological coping abilities strategies groups although also reﬂect small sample size. vein obesity appears half often group group perhaps eating disorder coping? without fuller context standard approach corpus linguistics hard tell. given observations word counts promising avenue future research would explore corpus linguistics-inspired approach. rather creating bag-of-words core idea would create reﬁned bag-of-phrases phrases constructed nearest neighbors perhaps derived from incorporating syntactic information part-of-speech tags dependency tags dependency parse even semantic information wordnet lexical tags. separate challenge dataset presence question-answer constructions answer relevant evaluating psychological state whereas question worded psychologically signiﬁcant words would confuse bag-ofwords/bag-of-phrases classiﬁer. techniques anaphora resolution algorithms perhaps tricks question-answering systems might applicable disambiguate intended meaning. training classiﬁers distinguish three groups patients straightforward task. given relatively small dataset size also easy train classiﬁers over-ﬁt perform well training sometimes achieving perfect score scoring rather poorly test set. accuracies obtained ensemble averages models trained best parameter choices individual model accuracies rising high finding best models arduous task. evaluate ensemble models -fold cross-validation requires total models trained; take days wall-clock time individual models require anywhere minutes decent fraction hour train. order obtain good several training parameters must explored thresholding word-counts bins runtime dynamical feature-selection size. parameters must tuned individually different data sets; adjusted best bring view dataset sharp focus. interesting result word-pairs used build accurate models single words alone. however order work well number data cuts must applied word pairs mutual information scores discarded; infrequently occurring pairs words discarded important word-pairs don’t contain ’signiﬁcant’ words discarded well. work partially funded darpa grant n--c- iarpa grant npc. author would like thank chris poulin goertzel encouraging patient work carried out. system described part durkheim project http//durkheimproject.org/.", "year": 2013}