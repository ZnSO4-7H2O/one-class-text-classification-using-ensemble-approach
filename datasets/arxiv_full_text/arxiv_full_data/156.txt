{"title": "Causal Regularization", "tag": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "abstract": "In application domains such as healthcare, we want accurate predictive models that are also causally interpretable. In pursuit of such models, we propose a causal regularizer to steer predictive models towards causally-interpretable solutions and theoretically study its properties. In a large-scale analysis of Electronic Health Records (EHR), our causally-regularized model outperforms its L1-regularized counterpart in causal accuracy and is competitive in predictive performance. We perform non-linear causality analysis by causally regularizing a special neural network architecture. We also show that the proposed causal regularizer can be used together with neural representation learning algorithms to yield up to 20% improvement over multilayer perceptron in detecting multivariate causation, a situation common in healthcare, where many causal factors should occur simultaneously to have an effect on the target variable.", "text": "application domains healthcare want accurate predictive models also causally interpretable. pursuit models propose causal regularizer steer predictive models towards causally-interpretable solutions theoretically study properties. large-scale analysis electronic health records causally-regularized model outperforms l-regularized counterpart causal accuracy competitive predictive performance. perform non-linear causality analysis causally regularizing special neural network architecture. also show proposed causal regularizer used together neural representation learning algorithms yield improvement multilayer perceptron detecting multivariate causation situation common healthcare many causal factors occur simultaneously eﬀect target variable. domains healthcare genomics social science high demand data analysis reveals causal relationships independent target variables. example doctors want models accurately predict status patients also want identify factors improve distinction prediction causation times subject debate statistics machine learning machine learning focused mostly prediction tasks many scientiﬁc domains pure prediction without considering underlying causal mechanisms considered unscientiﬁc work propose causal regularizer balances causal interpretability predictive power. counterfactual causality framework random variable causes another variable denoted experimental testing would proven change distribution also competing explanations observed correlation confounding need reconciled assessing likelihood true. causal analytic methods used prioritize warrants testing clinical trials among diversity hypotheses primary evidence controlled trials feasible desireable healthcare particular common ensemble many causal factors needs occur simultaneously eﬀect target variable phenomenon call multivariate causation. scalable methods needed explore exponential combinations independent variables diﬀerent transformations order detect multivariate causal relationships. methods discovering causal relationships among multiple variables observational data largely based principle given causal relationships among multiple variables leaves well-deﬁned marks joint distribution variables. however methods used causal variable selection process becomes sensitive small changes figure proposed causal regularizer achieves signiﬁcantly higher causality score computed using ground truth causality. compute score codes ranked list reported three algorithms. causal regularizer also competitive predictive performance section details. main idea design causal regularizer control complexity statistical models time favor causal explanations. compared step procedure causal variable selection followed multivariate regression/classiﬁcation proposed approach performs joint causal variable selection prediction thus avoiding statistically sensitive thresholding causality scores causal variable selection step. allows dependencies cannot explained causation still included model relaxing variable selection procedure. technical contributions follows causality detectors construct causal regularizer guide predictive models towards learning causal relationships independent target variables. theoretically quantify impact accuracy causality detector causal accuracy regularized models. applied proposed algorithms clinical predictive modeling problems using large datasets heart failure onset prediction another mortality prediction using publicly available mimic dataset. altogether analyzed collective inﬂuence independent variables heart failure validated results clinical expert manually review ﬁndings blind setup. shown figure proposed causally-regularized algorithm signiﬁcantly outperforms baseline algorithms causality detection performance. show similar boost causality score detected multivariate causal hypotheses. finally show proposed algorithms also competitive predictive performance datasets. preliminaries causality detection begin description pairwise causal analysis single independent variable target variable based independence mechanisms assumption extend pairwise causality detector perform multivariate causality analysis next section. proposed causal figure common causal anti-causal structures observed hidden variables. algorithmic independence assumption sample joint distribution case train classiﬁer distinguish cases based features joint distribution. regularizer constructed using causality detection algorithm review based methods state causality detection algorithms helpful baseline algorithms experiments. interested ﬁnding causal models causes causes confounded based joint distribution however pairwise causality analysis infeasible arbitrary joint distributions. thus need resort additional assumptions nature causal relationships. recently several algorithms proposed distinguish cause eﬀect based natural assumption steps process generates data independent other references therein. work follow describe causality detection approach. next subsections describe novel causal regularizer designed based causality detection approach application non-linear causality analysis multivariate causal hypothesis generation. conceptual description independence cause mechanism. states processes generation cause mapping cause eﬀect independent. case assume probabilities generated independent higher-level distribution functions. thus assumptions functional form causal relationships variables interest. conforms scientiﬁc idea uniformitarianism which putting roughly states laws nature apply objects similarly. described deterministic probabilistic sense; work mainly uses probabilistic interpretation. used generate samples distributions agree possible graphical models including observed variables unobserved variable shown figure requiring probability functions factorization joint distribution independent other. hidden variables represent observed variables critical design regularizer next subsection. chalupka developed analytical likelihood ratio test decides causal anticausal cases however taking account confounded cases analytically diﬃcult. nevertheless possible generate samples scenarios figure train classiﬁer learn choose likelihood causal structure given samples joint idea causality detectors described rest section. mathematical description causality detection algorithm. formally suppose variables dimensionality variable observe sample size denoted {}ni observations common target variable denote samples. sample interested determining binary label determines whether causes not. fact interested function approximation problem learning mapping chalupka oﬀer means construct empirical joint distribution train supervised neural network mapping function lopez-paz learn representation followed training however rare true causal labels training causal detector. rather generate synthetic datasets represent scenarios figure based assumption. overall procedure generate samples distributions possible scenarios figure need select distributions impose minimum number restriction data synthetically-generated distributions statistics similar possible true data interest. example datasets independent variables counts number disease codes patients’ records thus sample mixture appropriate distributions count data zipf poisson uniform bernoulli distributions. hidden variable response variable sampled dirichlet bernoulli distributions respectively. details sampling training procedures provided appendix algorithm there. given causality detector section propose causal regularizer linear models demonstrate section using non-linear deep neural networks regularized causal regularizer learn non-linear causal relationships independent target variables. finally show causal regularizer eﬃciently explore space multivariate causal hypotheses extract meaningful candidates causality analysis. causal regularizer using causality detection methods previous section causal variable selection makes variable selection process becomes sensitive small changes joint distribution variables exclude many causal variables noise selection bias data. ideally holds access true joint distributions could discriminate causal non-causal variables perfect accuracy two-step procedure would suﬃcient. observational datasets usually accurate representation true probabilistic generative process measurement error selection bias perturb causality scores generated neural network causality detector. example consider two-step analysis process ﬁrst ﬁnding variables cause list variables performing sparse multivariate regression selected variables prioritize selected variables. procedure sensitive causality detection algorithm might give soft scores variables respectively. soft-scores interpreted probability variable cause two-step procedure include regression model however could possibly contribute predictive performance presence variables multivariate regression. words hard cut-oﬀ purpose two-step causal variable selection regression pose question what best cut-oﬀ threshold? instead propose causally regularized regression approach trade-oﬀ performed smoothly regularization parameter. select variables causal high probability also signiﬁcantly predictive. ﬁrst term multivariate analysis term whereas regularizer constructed using bivariate causality score independent variable target variable create problem design causal regularizer implicitly included variables hidden variables analysis allow regularizer used multivariate regression. rest observed independent variables considered hidden variables bivariate causality analysis allows proper regularization. proposed causal regularizer also decomposable regularizer makes analysis theoretical properties easier interplay causation prediction studied recently references therein. particular notion causal regularizer previously recognized possible however speciﬁc causal regularizer never developed evaluated. notice using score causal-anticausal-only classiﬁer without including confounding cases e.g. cannot properly regularize multivariate model logistic regression. moreover major novelty proposed causal regularizer joint causal variable selection prediction idea cannot. noise variable zero mean random variable variance distribution satisﬁes regularity conditions theorem assume causes correlation unobserved confounder. access imperfect causality detector without loss generality cleanness results study orthonormal design setting result l-norm l-norm based causally regularized regression respectively. asymptotically following results compared factor scaling causal coeﬃcient non-causal coeﬃcient nominator increasing chance correct causality detection. perfect causality detector guarantees causal interpretability magnitude outweights predictive advantage causal detector random show non-informative causality detector makes causal regularization equivalent standard regularization. finally limit large penalization coeﬃcient obtain non-linear modeling. linear model assumes strength impact independent variable target variable ﬁxed. however according probabilistic view causation strength causation change subject subject. thus need non-linear extensions logistic regression regularizerd causal regularizer steered towards causal. address problem seek neural network architectures represent impact independent variable single coeﬃcient regularize coeﬃcients causal regularizer. particular propose following non-linear generalized linear model embedding matrix rq×m maps input lower dimensional representation space symbol denotes element-wise product. logistic sigmoid function maps real values interval. term acts skip connection initialized result logistic regression. embedding allows dealing large discrete concepts initialized techniques skip-gram glove vector computed using multilayer perceptron model non-linear extension logistic regression suitable causal regularization. reorder equations write right hand side regression coeﬃcient change every input. coordinate regression coeﬃcient calculated α)ei denotes column embedding matrix figure scenarios using proposed causal regularizer proposed architecture applying causal regularizer allows identiﬁcation causal relationships non-linear settings causality coeﬃcient change subject subject. causal regularizer allows explore highdimensional multivariate combinations variables identify plausible hypotheses. here generates causal regularization coeﬃcients hypotheses regularizer encourages coordinates causal. multivariate causal hypothesis generation. application proposed causal regularizer conjunction deep representation learning eﬃciently extract multivariate causal hypotheses data. figure shows example causal hypothesis generation hypotheses generated mlp. assume representation learning network k-dimensional output denotes range output example sigmoid relu activation functions. goal force dimension causal thus coordinate used multivariate causal hypothesis. particular minimizing following objective function approach train anti-causality detector based design regularizer based score. then shown figure combine neural network regularize coeﬃcients last layer predicts labels weights lower layers regularized using regularizer make generated causal hypotheses simple interpretable. learning process steps first causality detector network trained synthetic dataset causal anti-causal scenarios labeled respectively. select non-linearity logistic sigmoid function thus beta distribution generating synthetic data training causality classiﬁer. second phase coeﬃcients ﬁxed train causality detector described section logistic regression regularizerd causal regularizer step procedure causal variable selection logistic regression discussed section rest parameters train network select batches ﬁxed-size examples. size batches indicate number samples available causality detector. select number large enough error rate causality detector becomes lower evaluate proposed causal regularizer section terms predictive causal performance. next compare quality codes identiﬁed causes heart failure identiﬁed diﬀerent approaches. finally evaluate performance multivariate causal hypothesis generation qualitatively analyzing extracted hypotheses. defer evaluation causality detection algorithms appendix main contributions work. table lists acronyms symbols techniques used experiments improve presentation. data sutter health heart failure dataset consists electronic health records middle-aged adults collected sutter health study heart failure. encounter records medication orders procedure orders problem lists extracted visit records consisting diagnosis medication procedure codes. denote codes given visit sequence predict patient diagnosed heart failure identify causes increase heart failure risk. cases selected approximately controls selected case case/control selection criteria fully described appendix cases index dates denote date diagnosed controls index dates corresponding cases. extract diagnosis codes medication codes procedure codes -month window index date. total number unique medical codes dataset. figure comparison variable selection logistic regression causal regularizers datsets accuracy measures. note stability variable selection logcause penalization coeﬃcient varies. figure predictive gain nonlincause mimic datset. gain visible fewer features used analysis input become expressive themselves. select variables descending order variance. average causality score computed using ground truth causality labels generated hypotheses. compute score hypotheses reported algorithms. runtime proposed algorithms number input variables change. mimic dataset publicly available dataset consisting medical records intensive care unit patients years. public query extract binary mortality labels patients. goal codes patients’ last visit predict mortality outcome. dataset includes patients deceased totoal diﬀerent medical codes used dataset. patients create feature vector n|c| counting number codes observed records patient. given large variations number codes logarithmically count data bins. ﬁnal data form patient’s label; heart failure mortality outcome heart failure mimic datasets respectively. training details. generate synthetic datasets training causality detector neural networks generate many batches data training parameter tuning purposes required. training parameter tuning models section perform common %/%/% training/validation/test splits. full details training procedure neural networks given appendix collection diagnoses especially causal heart failure heart failure manifest complication dissection aorta. dissection aorta present abdominal pain happen traumatic injuries involve burn unspeciﬁed degree multiple sites trunk occurring together. neoplasms kidney lead paraneoplastic systemic eﬀects lead heart failure. furthermore concurrent severe infections tuberculosis also increase risk heart failure. predictive performance evaluation table shows test accuracy heart failure mortality prediction heart failure mimic datasets respectively. algorithm times report mean standard deviation performance measures. proposed causal regularizer hurt predictive performance whereas two-step procedure signiﬁcantly reduces accuracy. interesting phenomenon shown figure relative robustness performance respect value penalization parameter compared regularization case. robustness comes surprise causal regularizer assigns small penalization coeﬃcients causal variables discussed section high values penalization force coeﬃcients become zero figures show sparsity results. predictive robustness causal regularizer also partially attributed invariant prediction peters property causal models. robustness fact causal regularizer might match true generative process dataset better regularizer model less pressure increase penalization parameter. demonstrate predictive gain nonlincause figure furthermore impact changing regularization parameter number selected variables visualized figures appendix causality detection performance evaluation risk factors heart failure well-studied medical literature making heart failure condition ideal case study causality. evaluate causality detection performance algorithms generate inﬂuential factors method. clinical expert label factor causal not-causal potentially causal assign scores them respectively. prevent bias expert label single list unique codes three lists list scores individual lists. figure shows average causality score algorithm based labels provided medical expert. expected regularized logistic regression performs poorly susceptible impact confounded variables. performance causally regularized logistic regression superior step procedure suggests picking factors causal highly predictive leads better causality score. result figure together predictive results table conﬁrm causal regularizer eﬃciently used ﬁnding causal variables highly predictive target quantity. qualitative advantages regularized approach also seen results table appendix marked disease codes potentially increase risk heart failure predicted causality score lower two-step procedure would eliminated evaluating multivariate causal hypotheses evaluate performance proposed causal hypothesis generation case causal regularization. generate lists hypotheses using algorithms medical expert label hypothesis causal non-causal possibly causal corresponding scores results figure shows causal regularizer increase causality score hypotheses also provide qualitative analysis causal hypotheses generated algorithm picking several hypotheses showing clinically meaningful. three examples multivariate causal hypotheses generated causal regularizer description clinical meaning shown table addressed problem exploring high-dimensional causal hypothesis space applications healthcare. designed causal regularizer maximally steers predictive models towards causally explainable models. proposed causal regularizer based causality detector increase computational complexity regularizer seamlessly integrated neural network perform non-linear causality analysis. also demonstrated application proposed causal regularizer generating multivariate causal hypotheses. finally demonstrated usefulness causal regularizer detecting risk factors heart failure using electronic health records dataset. authors would like thank frederick eberhardt helpful discussions. mohammad taha bahadori acknowledges previous discussions david kale micheal hankin concept causal regularizer. work supported national science foundation award iis- research partnership children’s healthcare atlanta georgia institute technology i-smile project google faculty award sutter health samsung scholarship. krzysztof chalupka’s work supported grant", "year": 2017}