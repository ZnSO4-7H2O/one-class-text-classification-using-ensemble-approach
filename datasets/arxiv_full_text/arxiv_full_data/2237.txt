{"title": "Coresets for Dependency Networks", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Many applications infer the structure of a probabilistic graphical model from data to elucidate the relationships between variables. But how can we train graphical models on a massive data set? In this paper, we show how to construct coresets -compressed data sets which can be used as proxy for the original data and have provably bounded worst case error- for Gaussian dependency networks (DNs), i.e., cyclic directed graphical models over Gaussians, where the parents of each variable are its Markov blanket. Specifically, we prove that Gaussian DNs admit coresets of size independent of the size of the data set. Unfortunately, this does not extend to DNs over members of the exponential family in general. As we will prove, Poisson DNs do not admit small coresets. Despite this worst-case result, we will provide an argument why our coreset construction for DNs can still work well in practice on count data. To corroborate our theoretical results, we empirically evaluated the resulting Core DNs on real data sets. The results", "text": "alejandro molina department dortmund germany alexander munteanu department dortmund germany kristian kersting dept. centre cogsci darmstadt germany many applications infer structure probabilistic graphical model data elucidate relationships variables. train graphical models massive data set? paper show construct coresets—compressed data sets used proxy original data provably bounded worst case error—for gaussian dependency networks i.e. cyclic directed graphical models gaussians parents variable markov blanket. speciﬁcally prove gaussian admit coresets size independent size data set. unfortunately extend members exponential family general. prove poisson admit small coresets. despite worst-case result provide argument coreset construction still work well practice count data. corroborate theoretical results empirically evaluated resulting core real data sets. results demonstrate signiﬁcant gains naive sub-sampling even case count data. artiﬁcial intelligence machine learning achieved considerable successes recent years ever-growing number disciplines rely them. data ubiquitous great value understanding data building e.g. probabilistic graphical models elucidate relationships variables. data however scalability become crucial useful machine learning approach. paper consider problem training graphical models particular dependency networks heckerman massive data sets. cyclic directed graphical models parents variable markov blanket proven successful various tasks collaborative ﬁltering heckerman phylogenetic analysis carlson genetic analysis dobra phatak network inference sequencing data allen trafﬁc well topic modeling hadiji speciﬁcally show dependency networks gaussians—arguably prominent type distribution statistical machine learning—admit coresets size independent size data set. coresets weighted subsets data guarantee models ﬁtting also provide good original data studied clustering badoiu feldman lucic classiﬁcation har-peled har-peled reddi regression drineas dasgupta geppert smallest enclosing ball problem badoiu clarkson feldman agarwal sharathkumar refer phillips recent extensive literature overview. contribution continues line research generalizes coresets probabilistic graphical modeling. unfortunately coreset result extend dependency networks members exponential family general. prove dependency networks poisson random variables allen hadiji admit coresets every single input point important model needs appear coreset. important negative result since count data—the primary target poisson distributions—is center many scientiﬁc endeavors citation counts page counts counts procedures medicine count births deaths census counts words document count gamma rays physics. here modeling event number times certain test yields particular result provide idea number potentially invasive procedures need performed patient. thus elucidating relationships variables yield great insights massive count data. therefore despite worst-case result provide argument coreset construction dependency networks still work well practice count data. corroborate theoretical results empirically evaluated resulting core dependency networks several real data sets. results demonstrate signiﬁcant gains naive sub-sampling even count data. proceed follows. review dependency networks prove gaussian admit sublinear size coresets discuss possibility generalize result count data. concluding illustrate theoretical results empirically. existing machine learning literature graphical models dedicated binary multinominal certain classes continuous random variables. undirected models markov random fields ising potts models found applications various ﬁelds robotics computer vision statistical physics among others. whereas mrfs allow cycles structures directed models bayesian networks required acyclic directed relationships among random variables. dependency networks —the focus present paper—combine concepts directed undirected worlds heckerman speciﬁcally like directed arcs allow networks cycles bi-directional arcs akin mrfs. makes quite appealing many applications build multivariate models univariate distributions allen yang hadiji still permitting efﬁcient structure learning using local estimtatiors gradient tree boosting. generally data fully observed learning done locally level conditional probability distributions variable mixing directed indirected needed. based local distributions samples joint distribution obtained gibbs sampling. indeed gibbs sampling neglects question consistent joint probability distribution instead makes formally denote random vector instantiation. dependency network pair directed possibly cyclic graph node corresponds random variable directed edges edge models dependency variables i.e. edge variables conditionally independent given variables x\\ij indexed network. refer nodes edge pointing parents denoted conditional probability distributions associated variable here highlights fact mean functional form dependent parents. often refer simply construction local conditional probability distribution similar bayesian network case. however case graph necessarily acyclic x\\i) typically inﬁnite range hence cannot represented using ﬁnite table probability values. finally full joint distribution simply deﬁned product local distributions note however guarantee existence consistent joint distribution i.e. joint distribution conditionals. bengio however recently proven existence consistent distribution given evidence known closed form long unordered gibbs sampler converges. argued learning dependency networks amounts determining conditional probability distributions given training instances representing rows data matrix rn×d variables. assuming pai) parametrized generalized linear model mccullagh nelder amounts estimating parameters associated variable since completely determines local distributions pai) possibly depend variables network dependencies deﬁne structure network. view training ﬁtting glms data allows develop core dependency networks sample coreset train certain members family sampled corest. figure illustration dependency networks using poissons. number goals scored soccer games follows poisson distribution. plot shows distribution home goals season german bundesliga home team. home team scored average goals game. example structure poisson conditional distribution count variable given neighbors poisson distribution. similar bayesian network poisson directed however also contains cycles. introduce formal framework need towards design coresets learning dependency networks. useful structural property based objective functions concept ε-subspace embedding. construct sampling matrix forms ε-subspace embedding constant probabilty following orthonormal basis columnspace basis obtained singular value decomposition data matrix. rank rank deﬁne leverage scores ui∗/u ui∗/ρ sampling size parameter o/ε) sample input points one-by-one probability min{ reweight contribution loss function /qi. note that squares loss corresponds deﬁning diagonal matrix probability otherwise. also note expected number samples o/ε) also holds constant probability markov’s inequality. moreover give intuition works note ﬁxed signiﬁcantly stronger property forming ε-subspace embedding according deﬁnition follows matrix approximation bound given rudelson vershynin drineas lemma input matrix rank sampling matrix constructed stated sampling size parameter o/ε). forms ε-subspace embedding columnspace constant probability. question arises whether better o/ε). show reduction coupon collectors theorem lower bound matching upper bound dependency hard instance dm×d orthonormal matrix stacked times. leverage scores equal scaled canonical basis implying uniform sampling distribution probability basis vector. rank preserving sample must comprise least them. exactly coupon collectors theorem coupons lower bound motwani raghavan fact sampling without replacement change this since reduction holds arbitrary large creating sufﬁcient multiple copies element simulate sampling replacement tropp know constant probability randomness construction algorithm satisﬁes ε-subspace embedding property given input matrix structural property show actually coreset gaussian linear regression models dependency networks. consider gaussian dependency network i.e. collection gaussian linear regression models theorem given ε-subspace embedding columnspace constructed above ε-coreset loss function. proof arbitrary consider afﬁne deﬁned clearly extends argument dimensions inserting entry position leaving entries original order. note moreover coreset affect inference within gdns. recently shown gaussian linear regression models entire multivariate normal distribution parameter space approximately preserved ε-subspace embeddings geppert generalizes above. implies coreset yields useful pointwise approximation markov chain monte carlo inference random walks like pseudo-gibbs sampler heckerman naturally following question arises coresets exist dependency networks exponential family general? unfortunately answer indeed coreset simpler problem poisson regression implies result poisson dns. show formally reduction communication complexity problem known indexing. proof reduce indexing problem known one-way randomized communication complexity jayram alice given vector produces every points denote unit roots plane i.e. vertices regular n-polygon radius note whose task guess chooses query also points within distance construction consequently hyperplane. thus note bound given complexity restricting data structure sampling based coreset assuming every data point expressed bits means still lower bound implicit original early works drineas explicitly formalized later langberg schulman clarkson woodruff crucial understand inherent property norm function thus holds arbitrary data. poisson contrast shown loss function come properties scratch. constructed worst case scenario basically every single input point important model needs appear coreset. usually case statistical models data assumed generated i.i.d. generating distribution model assumptions. consider instance data reduction gaussian linear regression leverage score sampling uniform sampling. shown given data follows model assumptions gaussian distribution approaches behave similarly. another leverage scores quite uniform. presence outliers generated heavier tails tdistributions leverage scores increasingly outperform uniform sampling though standard model count data suffers inherent limitation equidispersed data since exp. count data however often overdispersed especially large counts. unobserved variables problem speciﬁc heterogeneity contagion-effects. log-normal poisson model known inferior data specifically follows poisson model turns powerful modeling effects captured simple poisson model. wide applications instance econometric elasticity problems. review log-normal poisson model count data winkelmann follows constant independent controls amount overdispersion. taking limit arrive simple model since distribution tends deterministic dirac delta distribution puts mass inference might log-normal poisson model directly zhou performed maximum likelihood estimation simple poisson model. latter provides consistent estimator long log-linear mean function correctly speciﬁed even higher moments possess limitations inherent simple poisson model winkelmann summing review count modeling perspective learn preserving loglinear mean function poisson model crucial towards consistency estimator. moreover modeling counts log-normal model gives intuition leverage score sampling capture underlying linear model accurately log-normal poisson model follows log-normal distribution. thus holds figure performance gaussian cdns mnist poisson cnds trafﬁc dataset -fold cross-validated. shown negative pseudo likelihood squared error loss well training time y-axis different proportions data sampled please note jump x-axis cdns quickly approach predictive performance full dataset uniform sampling perform well cdns. moreover cdns orders magnitude faster full dataset scale similar uniform sampling. also supported vertical lines. denote mean performances axes. still missing piece argumentation. previous section used coreset construction ε-subspace embedding columnspace whole data including dependent variable i.e. face problems. first implicitly given data explicitly available. second vector derived setting might different instances. fortunately shown complicated arguments drineas sufﬁcient good approximation sampling done obliviously dependent variable. intuition comes fact loss point subspace expressed projection onto subspace spanned residual projection. good approximation subspace implicitly approximates projection ﬁxed vector applied residual vector orthogonal projection. solves ﬁrst problem since necessary subspace embedding second issue addressed increasing sample size factor boosting error probability taking union bound. table comparison empirical relative error best results dataset bold. gaussian poisson cdns recover model well fraction training data. uniformly sampled behind sample size drops. intention corroborate theoretical results investigating empirically following questions performance cdns compare access full training data uniform sample training data set? empirical error behave according sample sizes? coresets affect structure recovered implemented python calling experiments linux machine benchmarks mnist trafﬁc data considered datasets. ﬁrst experiment used mnist data handwritten labeled digits. employed training consisting images pixels total measurements trained gaussian second data considered contains trafﬁc count measurements selected roads around city cologne germany consists timestamped measurements taken sensors total measurements. dataset trained poisson dns. dataset performed fold cross-validation training full using data leverage score sampling coresets uniform samples different sample sizes. compared predictions made time taken train them. predictions mnist dataset clipped predictions range dns. trafﬁc dataset computed predictions every measurement rounded largest integer less equal fig. summarizes results. cdns outperform trained full data orders magnitude faster. compared uniform sampling coresets competitive. actually seen trafﬁc dataset cdns predictive power optimal model using full data. line mahoney observed coresets implicitly introduce regularization lead robust output. table summarizes empirical relative errors trained data. cdns clearly recover original model fraction training data. overall answers afﬁrmatively. figure elucidating relationships random variables. shown dependency structures gaussian poisson cdns nips different learning sampling sizes using edges show thresholded positive coefﬁcients glms. colors edges represent modularity. cdns elucidate relationships among words make semantically sense approach structure learned using full dataset. quantitative assessment tab. fig. illustrates results qualitatively. shows three cdns sampling sizes gaussians transformation poissons cdns capture well gist nips corpus. table conﬁrms quantitatively. shows frobenius norms cdns capture gist better naive i.e. uniform sampling. answers afﬁrmatively. inspired question train graphical models massive dataset studied coresets estimating dependency networks established ﬁrst rigorous guarantees obtaining compressed ε-approximations gaussian large data sets. proved worstcase impossibility results coresets poisson dns. review log-normal poisson modeling counts provided deep insights coreset construction still performs well count data practice. table frobenius norm difference adjacency matrices recovered trained full data trained uniform subsample resp. coresets training data. best results statiscal type bold. cdns recover structure better udns. experimental results demonstrate resulting core dependency networks achieve signiﬁcant gains naive sub-sampling even case count data making possible learn models much larger datasets using hardware. cdns provide several interesting avenues future work. conditional independence assumption opens door explore hybrid multivariate models variable potentially come different family link function massive data sets. used hint independencies among variables multivariate setting making useful many large data applications. generally results pave establish coresets deep models using close connection dependency networks deep generative stochastic networks bengio sum-product networks poon domingos molina well statistical models build multivariate distributions univariate ones yang acknowledgements work supported deutsche forschungsgemeinschaft within collaborative research center providing information resource-constrained analysis projects", "year": 2017}