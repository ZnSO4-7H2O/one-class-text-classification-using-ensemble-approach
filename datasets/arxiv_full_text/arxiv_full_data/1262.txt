{"title": "Routing Networks: Adaptive Selection of Non-linear Functions for  Multi-Task Learning", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network - for example a fully-connected or a convolutional layer. Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached. In this way the routing network dynamically composes different function blocks for each input. We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks. We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a significant improvement in accuracy, with sharper convergence. In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we obtain cross-stitch performance levels with an 85% reduction in training time.", "text": "multi-task learning neural networks leverages commonalities tasks improve performance often suffers task interference reduces beneﬁts transfer. address issue introduce routing network paradigm novel neural network training algorithm. routing network kind self-organizing neural network consisting components router function blocks. function block neural network example fully-connected convolutional layer. given input router makes routing decision choosing function block apply passing output back router recursively terminating ﬁxed recursion depth reached. routing network dynamically composes different function blocks input. employ collaborative multi-agent reinforcement learning approach jointly train router function blocks. evaluate model cross-stitch networks shared-layer baselines multi-task settings mnist mini-imagenet cifar- datasets. experiments demonstrate signiﬁcant improvement accuracy sharper convergence. addition routing networks nearly constant per-task training cost cross-stitch networks scale linearly number tasks. cifar obtain cross-stitch performance levels reduction training time. multi-task learning paradigm multiple tasks must learned simultaneously. tasks typically separate prediction problems data distribution. early formulation problem describes goal improving generalization performance leveraging domain-speciﬁc information contained training signals related tasks. means model must leverage commonalities tasks minimizing interference paper propose architecture problems called routing network consists trainable components router function blocks. given input router selects function block applies input passes result back router recursively ﬁxed recursion depth. router needs fewer iterations decide take pass action leaves current state unchanged. intuitively architecture allows network dynamically self-organize response input sharing function blocks different tasks positive transfer possible using separate blocks prevent negative transfer. architecture general allowing many possible router implementations. example router condition decision current activation task label other. also condition depth ﬁltering function module choices allow layering. addition condition decision instance historically decided instances encourage re-use existing functions improved compression. function blocks simple fully-connected neural network layers whole networks long dimensionality function block allows composition previous function block choice. needn’t even type layer. neural network part network routed adding layers function blocks making architecture applicable wide range problems. routers make sequence hard decisions differentiable reinforcement learning train them. discuss training algorithm section modeled problem create separate agent task task agent learns policy routing instances task function blocks. evaluate created routed version convnet used three image classiﬁcation datasets adapted learning multi-task mnist dataset created mini-imagenet data split introduced cifar- label superclasses treated different tasks. conduct extensive experiments comparing cross-stitch networks popular strategy joint training layer sharing described results indicate signiﬁcant improvement accuracy strong baselines speedup convergence often orders magnitude improvement training time cross-stitch networks. work multi-task deep learning traditionally includes signiﬁcant hand design neural network architectures attempting right task-speciﬁc shared parameters. example many architectures share low-level features like learned shallow layers deep convolutional networks word embeddings across tasks task-speciﬁc architectures later layers. contrast routing networks learn fully dynamic compositional model adjust structure differently task. routing networks share common goal techniques automated selective transfer learning using attention learning gating mechanisms representations latter papers experiments performed tasks time. consider tasks experiments compare directly work also related mixtures experts architectures well modern attention based sparse variants. gating network typical mixtures experts model takes input chooses appropriate weighting output expert network. generally implemented soft mixture decision opposed hard routing decision allowing choice differentiable. although sparse layer-wise variant presented save computational burden proposed end-to-end differentiable model approximation doesn’t model important effects exploration exploitation tradeoffs despite impact system. mixtures experts recently considered transfer learning setting however decision process modelled autoencoder-reconstructionerror-based heuristic scaled large number tasks. dynamic representations work also related single task multi-task models learn generate weights optimal neural network models powerful trouble scaling deep models large number parameters without tricks simplify formulation. contrast demonstrate routing networks applied create dynamic network architectures architectures like convnets routing layers. work extends emerging line recent research focused automated architecture search. work goal reduce burden practitioner automatically learning black algorithms search optimal architectures hyperparameters. include techniques based reinforcement learning evolutionary algorithms approximate random simulations adaptive growth best knowledge ﬁrst apply idea multitask learning. technique learn construct general class architectures without also related work literature minimizing computation cost single-task problems conditional routing. include decisions trained reinforce learning actor-critic methods approach differs however introduction several novel elements. speciﬁcally work explores multi-task learning setting uses multi-agent reinforcement learning training algorithm structured recursive decision process. large body related work focuses continual learning tasks presented network time potentially long period time. interesting recent paper setting also uses notion routes uses evolutionary algorithms instead fernando routing network novel artiﬁcial neural network formulation high-level idea task speciﬁc routing cognitive function well founded biological studies theories human brain routing network consists components router function blocks neural network layer. router function selects among function blocks given input. routing process iteratively applying router select sequence function blocks composed applied input vector. process illustrated figure input routing network instance classiﬁed representation vector dimension integer task identiﬁer. router given depth depth recursion selects among function block choices available depth picking indicated dashed line. applied input produce output activation. router chooses function block available depth finally router chooses function block last layer function block produces classiﬁcation algorithm gives routing procedure detail. algorithm takes input vector task label maximum recursion depth iterates times choosing function block iteration applying produce output representation vector. special pass action skips next iteration. experiments don’t require task label case pass dummy value. simplicity assume algorithm access router function function blocks don’t include explicitly input. router decision function router ass} maps current representation task label current depth index function block route next ordered function block. routing network invocations depth function blocks routing network depth select distinct trainable functions neural network represented routing network adding copies layers routing network function blocks. group function blocks network layer constrain router pick layer function blocks depth layer blocks depth number function blocks differs layer layer original network router accommodate example maintaining separate decision function depth. forward pass network applying algorithm sample store trace sequence visited states sequence actions taken sequence immediate action rewards action ﬁnal reward inal. last output network’s prediction ﬁnal reward inal prediction correct; not. compute loss prediction ground truth backpropagate along function blocks selected route train parameters. view routing problem following way. states triples representation vector integer task label depth actions function block choices ass} number function blocks. given state router makes decision action take. non-pass actions state updated process continues. pass action produces representation vector increments depth train router policy using variety algorithms settings describe detail next section. regardless algorithm applied router function blocks trained jointly. instance route instance network produce prediction along record trace states actions taken well immediate reward action last function block chosen record ﬁnal reward depends prediction true label train selected function blocks using sgd/backprop. example figure means computing gradients computed trace train router using algorithm. high-level procedure summarized algorithm illustrated figure keep presentation uncluttered assume training algorithm access router function function blocks loss function speciﬁc hyper-parameters discount rate needed training don’t include explicitly input. network’s performance. classiﬁcation problems focused paper prediction correct otherwise. domains regression domains negative loss could used. experimented immediate reward encourages router fewer function blocks possible. since number function blocks per-layer needed maximize performance known ahead time wanted whether could achieve comparable accuracy reducing number function blocks ever chosen router allowing reduce size network training. experimented rewards multiplied hyper-parameter average number times block chosen router historically average historical probability router choosing block. found signiﬁcant difference approaches average probability experiments. evaluated effect ﬁnal performance report results figure appendix. generally small value works best relatively little sensitivity choice range. figure task-based routing. value task input consisting value partial evaluation previous function block task label task. routing agent; dispatching agent. train router evaluate single-agent multi-agent strategies. figure shows three variations consider. figure single agent makes routing decision. trained using either policy-gradient q-learning experiments. figure shows multi-agent approach. ﬁxed number agents hard rule assigns input instance agent responsible routing experiments create agent task input task label index agent responsible routing instance. figure shows multi-agent approach additional agent denoted called dispatching agent learns assign input agent instead using ﬁxed rule. multi-agent scenarios additionally experiment marl algorithm called weighted policy learner experiment storing policy table form approximator. tabular representation invocation depth dimension function block column dimension entries containing probability choosing given function block given depth. approximator representation consist either passed depth vector mlps decision/depth. q-learning policy gradient algorithms applicable tabular approximation function policy representations. reinforce train approximation function tabular representations. q-learning table stores q-values entries. vanilla q-learning train tabular representation train approximators minimize norm temporal difference error. implementing router decision policy using multiple agents turns routing problem stochastic game multi-agent extension mdp. stochastic games multiple agents interact environment expected return given policy change without action agent’s part. view incompatible agents need compete blocks train since negative transfer make collaboration unattractive compatible agents gain sharing function blocks. agent’s optimal policies correspond game’s nash equilibrium routing networks environment non-stationary since function blocks trained well router policy. makes training considerably difﬁcult singleagent setting. experimented single-agent policy gradient methods reinforce less well adapted changing environment changes agent’s behavior degrade performance setting. marl algorithm speciﬁcally designed address problem also shown converge non-stationary environments weighted policy learner algorithm shown algorithm algorithm designed dampen oscillation push agents converge quickly. done scaling gradient expected return action according probability taking action intuitively effect slowing learning rate policy moving away nash equilibrium strategy increasing approaches one. full algorithm shown algorithm assumed historical average return action initialized start training. function simplex-projection projects updated policy values make valid probability distribution. details including convergence proofs examples giving intuition behind algorithm found longer explanation algorithm found section appendix. wpl-update algorithm deﬁned tabular setting. future work adapt work function approximators. described training router function blocks performed independently computing loss. also experimented adding gradients router choices function blocks produce input. found advantage leave thorough investigation future work. experiment three datasets multi-task versions mnist mini-imagenet introduced cifar- treat superclasses tasks. binary mnist-mtl dataset task differentiate instances given class non-instances. create tasks instances positive class remaining negative classes total instances task training test samples task min-mtl smaller version imagenet easier train reasonable time periods. mini-imagenet randomly choose labels create tasks disjoint random subsets labels chosen these. label training instances testing instances training testing instances task. tasks total training instances. finally cifar- coarse labels instances. follow existing work creating task coarse labels include instances corresponding labels. tasks total instances task; training testing. results reported test averaged runs. data summarized table datasets interesting characteristics challenge learning different ways. cifar-mtl natural dataset whose tasks correspond human categories. min-mtl randomly generated less task coherence. makes positive transfer difﬁcult achieve negative transfer problem. mnist-mtl simple difﬁcult property instance appear different labels different tasks causing interference. example digits task appears positive label digits task appears negative label. experiments conducted convnet architecture appeared recently model convolutional layers consisting convolution ﬁlters followed batch normalization relu. convolutional layers followed fully connected layers hidden units each. routed version network routes fully connected layers routed layer supply randomly initialized function block task dataset. neural approximators router agents always layer mlps hidden dimension state encoded input approximator concatenating -hot representation encoding concat). parameter sweep best learning rate value algorithm dataset. cifar-mtl min-mtl mnist-mtl. learning rate initialized annealed dividing every epochs. tried regular well adam kingma chose resulted marginally better performance. simpleconvnet batch normalization layers dropout. experiment dedicate special pass action allow agents skip layers during training leaves current state unchanged detailed description pass action provided appendix section ﬁrst experiment shown figure compare different training algorithms cifarmtl. compare algorithms marlwpl; single agent reinforce learner separate approximation function layer; agent-per-task reinforce learner maintains separate approximation function layer; agent-per-task learner separate approximation function layer; agent-per-task learner separate table layer. best performer algorithm outperforms nearest competitor tabular q-learning algorithm works better similar vanilla trouble learning; multiple agents works better single agent; tabular versions task depth make predictions work better approximation versions representation vector addition predict next action. next experiment compares best performing algorithm routing approaches including already introduced reinforce single agent algorithms route full-connected layers simpleconvnet using layering approach discussed earlier. make next comparison clear rename marlwpl routingall-fc figure reﬂect fact routes fully connected layers simpleconvnet rename reinforce single agent routing-all-fc single agent. compare several approaches. approach routing-all-fc-recurrent/+pass setup routing-allfc constrain router pick layer function blocks depth etc. allowed choose function block layers another approach soft-mixture-fc soft version router architecture. soft version uses function blocks routed version replaces hard selection trained softmax attention also compare single agent architecture shown called routing-all-fc single agent dispatched architecture shown figure called routing-all-fc dispatched. neither approached performance per-task agents. best performer large margin routing-all-fc fully routed algorithm. next compare routing-all-fc different domains cross-stitch networks misra challenging baselines task speciﬁc--fc task speciﬁc-all-fc described below. cross-stitch networks misra kind linear-combination model multi-task learning. maintain model task shared input layer cross stitch connection layers allow sharing tasks. instead selecting single function block next layer route cross-stitch network routes function blocks simultaneously input function block layer given linear combination activations computed layer activations vl−j. experiments cross-stitch layer routed layers simpleconvnet. additional compare similar soft routing version soft-mixture-fc figure soft-routing uses softmax normalize weights used combine activations previous layers shares parameters given layer task-speciﬁc--fc baseline separate last fully connected layer task shares rest layers tasks. task speciﬁc-all-fc baseline separate fully connected layers task. baseline architectures allow considerable sharing parameters also grant network private parameters task avoid interference. however unlike routing networks choice parameters shared tasks parameters task-private made statically architecture independent task. results shown figures case routing routing-all-fc performs consistently better cross-stitch networks baselines. cifar-mtl routing beats cross-stitch networks next closest baseline task-speciﬁc--fc min-mtl routing beats cross-stitch networks nearest baseline taskspeciﬁc--fc surmise results better cifar-mtl task instances common whereas min-mtl tasks randomly constructed making sharing less proﬁtable. mnist-mtl random baseline experimented several learning rates unable cross-stitch networks train well here. routing nets beats cross-stitch networks nearest baseline soft version also trouble learning dataset. experiments routing makes signiﬁcant difference cross-stitch networks baselines conclude dynamic policy learns function blocks compose per-task basis yields better accuracy sharper convergence simple static sharing baselines soft attention approach. addition router training much faster. cifar-mtl example training time stable compute cluster reduced roughly hours improvement. conducted scaling experiments compare training computation routing networks cross-stitch networks trained function blocks. results shown appendix figure routing networks consistently perform better cross-stitch networks baselines across problems. adding function blocks apparent effect computation involved training routing networks dataset given size. hand cross-stitch networks soft routing policy scales computation linearly number function blocks. soft policy backpropagates function blocks hard routing policy backpropagates selected block hard policy much easily scale many task learning scenarios require many diverse types functional primitives. explore multi-agent approach seems better single-agent manually compared policy dynamics several cifar-mtl examples. experiments collaboration reward might encourage less diversity agent choices. cases examined found single agent often chose function blocks depth routed tasks those. suspect simply little signal available agent early random stages bias established decisions suffer lack diversity. routing network hand learns policy which unlike baseline static models partitions network quite differently task also achieves considerable diversity choices seen figure ﬁgure shows routing decisions made whole mnist dataset. task labeled decisions three routed layers shown below. believe routing network separate policies task less sensitive bias function blocks agent learns independently works assigned task. better understand agent interaction created several views policy dynamics. first figure chart policy time ﬁrst decision. rectangle labeled left represents evolution agent’s policy task. task horizontal axis number samples task vertical axis actions vertical slice shows probability distribution actions seen many samples task darker shades indicating higher probability. picture that beginning task agents high entropy. samples processed agent develops several candidate function blocks task eventually agents converge close probability particular block. language games agents pure strategy routing. next view dynamics pick particular function block plot probability agent choosing block time. horizontal axis time vertical axis probability choosing block colored curve corresponds different task agent. considerable oscillation time agents pink green emerge victors block assign close probability choosing routing respective tasks. interesting eventual winners pink green emerge earlier well strongly interested block noticed pattern analysis blocks speculate agents want block pulled away early nash equilibrium agents train block away. finally figure show routing mnist-mtl. tasks layer represents routing decision. conventional wisdom networks beneﬁt sharing early using ﬁrst layers common representations diverging later accommodate differences tasks. setup baselines. interesting network learns own. agents converged strategy ﬁrst uses function blocks compresses expands clear optimal strategy certainly give improvement static baselines. presented general architecture routing multi-agent router training algorithm performs signiﬁcantly better cross-stitch networks baselines single-agent approaches. paradigm easily applied state-of-the-art network allow learn dynamically adjust representations. described section routing networks state space learned grows exponentially depth routing making challenging scale routing deeper networks entirety. would interesting hierarchical techniques here. successful experiments used multi-agent architecture agent task trained weighted policy learner algorithm currently approach tabular investigating ways adapt neural approximators. also tried routing networks online setting training sequence tasks shot learning. handle iterative addition tasks routing agent overﬁt shot examples training function modules slow learning rate. results mixed useful setting plan return problem. references sherief abdallah victor lesser. learning task allocation game. proceedings ﬁfth international joint conference autonomous agents multiagent systems http//dl.acm.org/citation.cfm?id=. andrew barto sridhar mahadevan. recent advances hierarchical reinforcement learning. discrete event dynamic systems http//link.springer. com/article/./a. emmanuel bengio pierre-luc bacon joelle pineau doina precup. conditional computation neural networks faster models. corr abs/. http//arxiv.org/ abs/.. chrisantha fernando dylan banarse charles blundell yori zwols david andrei rusu alexander pritzel daan wierstra. pathnet evolution channels gradient descent super neural networks. corr abs/. http//arxiv.org/abs/. risto miikkulainen jason liang elliot meyerson aditya rawal fink olivier francon bala raju arshak navruzyan nigel duffy babak hodjat. evolving deep neural networks. arxiv preprint arxiv. janarthanan rajendran prasanna balaraman ravindran mitesh khapra. adaapt attend adapt transfer attentative deep architecture adaptive policy transfer multiple sources domain. iclr abs/. http//arxiv.org/abs/ matthew riemer aditya vempaty flavio calmon fenno heath richard hull elham khabiri. correcting forecasts multifactor neural attention. international conference machine learning noam shazeer azalia mirhoseini krzysztof maziarz andy davis quoc geoffrey hinton jeff dean. outrageously large neural networks sparsely-gated mixture-of-experts layer. iclr andrea stocco christian lebiere john anderson. conditional routing information cortex model basal ganglias role cognitive coordination. psychological review marijn stollenga jonathan masci faustino gomez juergen schmidhuber. deep networks internal selective attention feedback connections. ghahramani welling cortes lawrence weinberger advances neural information processing systems curran associates inc. oriol vinyals charles blundell timothy lillicrap koray kavukcuoglu daan wierstra. matching networks shot learning. corr abs/. http//arxiv. org/abs/.. olga wichrowska niru maheswaranathan matthew hoffman sergio gomez colmenarejo misha denil nando freitas jascha sohl-dickstein. learned optimizers scale generalize. arxiv preprint arxiv. figure comparison per-task training cost cross-stitch routing networks. function block task normalize training time epoch dividing number tasks isolate effect adding function blocks computation. routing networks resulting sets function blocks applied repeatedly. might constraints prevalent dimensionality input output dimensions need match. applied simpleconvnet architecture used throughout paper means layers middle transformation applied arbitrary number times. case routing network becomes fully recurrent pass action applicable. allows network shorten recursion depth. epoch reinforce approx qlearning approx qlearning table marl-wpl table routing-all-fc routing-all-fc recursive routing-all-fc dispatched soft mixture-all-fc routing-all-fc single agent routing-all-fc task speciﬁc-all-fc task speciﬁc--fc cross stitch-all-fc routing-all-fc task speciﬁc-all-fc task speciﬁc-fc cross-stitch-all-fc routing-all-fc task speciﬁc-all-fc task speciﬁc-fc soft mixture-all-fc cross-stitch-all-fc policy representation dominant representation variations described ﬁrst policy stored table. since table needs store values different layers routing network size layers× actions. second represented either vector mlp’s hidden layer dimension layer routing network. case input representation vector concatenated one-hot representation task identiﬁer. policy input describes parts state used decision routing action. tabular policies task used index agent responsible handling task. agent uses depth index table. approximation-based policies variations. single agent case depth used index approximation function takes input concat). multi-agent case task label used index agent depth used index corresponding approximation function depth given concat) input. dispatched case dispatcher given concat) predicts agent index. agent uses depth approximation function depth given concat) input. algorithm multi-agent policy gradient algorithm designed help dampen policy oscillation encourage convergence. slowly scaling learning rate agent gradient change agents policy. determines gradient change using difference immediate reward historical average reward action taken. depending sign gradient algorithm scenarios. gradient positive scaled time gradient remains positive cause increase slowing learning. gradient negative scaled gradient remains negative time cause decrease eventually slowing learning again. slowing learning gradient changes dampens policy oscillation helps drive policies towards convergence.", "year": 2017}