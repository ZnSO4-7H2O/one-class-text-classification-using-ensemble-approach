{"title": "Reservoir computing for spatiotemporal signal classification without  trained output weights", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "Reservoir computing is a recently introduced machine learning paradigm that has been shown to be well-suited for the processing of spatiotemporal data. Rather than training the network node connections and weights via backpropagation in traditional recurrent neural networks, reservoirs instead have fixed connections and weights among the `hidden layer' nodes, and traditionally only the weights to the output layer of neurons are trained using linear regression. We claim that for signal classification tasks one may forgo the weight training step entirely and instead use a simple supervised clustering method based upon principal components of norms of reservoir states. The proposed method is mathematically analyzed and explored through numerical experiments on real-world data. The examples demonstrate that the proposed may outperform the traditional trained output weight approach in terms of classification accuracy and sensitivity to reservoir parameters.", "text": "reservoir computing recently introduced machine learning paradigm shown well-suited processing spatiotemporal data. rather training network node connections weights backpropagation traditional recurrent neural networks reservoirs instead ﬁxed connections weights among ‘hidden layer’ nodes traditionally weights output layer neurons trained using linear regression. claim signal classiﬁcation tasks forgo weight training step entirely instead simple supervised clustering method based upon principal components norms reservoir states. proposed method mathematically analyzed explored numerical experiments real-world data. examples demonstrate proposed outperform traditional trained output weight approach terms classiﬁcation accuracy sensitivity reservoir parameters. reservoir computing recently developed bio-inspired machine learning paradigm processing spatiotemporal data language neural networks reservoir collection hidden layer nodes nonlinear recurrent dynamics nodes sparsely connected ﬁxed weights trained speciﬁc data. weights ﬁxed using reservoir requires simple initialization step opposed traditional recurrent neural networks whose weights connections must learned tedious backpropagation training step property ﬁxing reservoir connections weights many beneﬁts including ease initialization along ability quickly adapt data applications. reservoirs like recurrent neural networks based premise state reservoir particular time depend current value input signal along recent inputs reservoir states. eﬀective method computation reservoir input data suﬃciently highdimensional space. desirable reservoir operate edge chaos’ dissimilar inputs suﬃciently separated reservoir node states inputs small perturbation-like diﬀerences stray apart. reservoir dynamics demonstrate long short-term memory individual point-wise errors signal corrupt entire reservoir response. types reservoirs emerged literature include echo state networks timedelay reservoirs uses randomly sparsely connected nodes randomly assigned ﬁxed weights uses cyclic topology node provides data exactly node ﬁxed non-random weights output layer tdr-type reservoirs traditionally linear output weights trained labeled dataset using least squares ridge regression. method easy training phase computationally cheap testing phase. however sensitive reservoir parameters dataset characteristics prone overﬁtting. training ∗this work cleared public release wright patterson force base public aﬀairs case number research simple supervised clustering method based principal components proposed classiﬁcation tasks using esns tdrs. method used based upon comparing norm reservoir response test signal principal components norms reservoir states classes labeled training data. clustering method slightly higher computational complexity using trained output weights classify input signals however achieve higher classiﬁcation accuracy less sensitive reservoir type size feedback strength. present rigorous analysis clustering method including theoreoms characterizing upper bound diﬀerence reservoir responses input signals upper bound terms input signals reservoir type user-generated parameters. moreover explore diﬀerence performance methods numerical simulations performed using real-world dataset esns tdrs various reservoir parameters. every simulation clustering approach outperforms trained output weights terms accuracy time required classify test signals. following notation used work. collection signals element collection denoted training sets partitioned classes. collection indices signals class. class vector norm given matrix spectral radius i.e. largest absolute value eigenvalue standard ‘big meaning exists m|g| reservoir computing models classiﬁcation suppose input signal length possibly application multiplexing mask states denoted vectors vector entry vectors denote state reservoir node time dynamics architectures described following models ease notation suppose reservoir type nodes. topology vector weights input signal feeding nodes matrix wres rn×n determines ﬁxed connections weights among nodes. node feeds node weighted entry wres model. virtual nodes corresponding physical node parameter input gain attenuation value. notice node values simply passed along reservoir unchanged except physical node. models reservoirs shown figure input multiplied vector concatenated form multiplexed input. purpose multiplexing mask tdrs several-fold. non-constant mask helps increase dimensionality reservoir yielding richer dynamics furthermore since inputs passed reservoir head node mask allows several virtual nodes process values single input vector once random esns design additional beneﬁt helps ‘slow down’ tdrs many physically implemented optical devices would process data much faster sample outputs approaches interpreting reservoir outputs supervised classiﬁcation tasks described subsections below. ﬁrst describes traditional approach using trained output weights second describes method clustering reservoir node states. wout rk×n time interest reservoir states close appropriate ‘indicator’ vector. choose collection times interest denote reservoir nodes time driven element training dataset using either equation collection output weights time computational cost determining class pattern using ‘testing’ phase algorithm determined follows. assume matrices wout given include derivation cost evaluation. drive reservoir nodes test pattern requires multiplications using dynamics equation multiplications using dynamics equation although reservoir node values times interest must drive reservoir using full times. vector requires multiplications underlying idea training method similar inputs reservoir produce similar outputs even non-linear high-dimensional processing applied. assumption feasible could classify data using clustering method without trained output weights. therefore propose following method using principal components norms reservoir responses perform class concatenate vectors form matrices r|ω|×|ck|. since input training patterns belong class columns exhibit similar characteristics. suppose r|ω|×r rithm determined follows. assume matrices uku∗ r|ω|×|ω| precomputed multiplications using multiplications using tdr. compute vector step requires multiplications compute values {dk} step requires complexity esn-type reservoirs complexity clustering method proposed algorithm accurate small variations input signals lead bounded diﬀerences reservoir states large discrepancies inputs mapped farther apart. conﬁdently approach must characterize reservoir responses similar inputs. several studies reservoir performance based type reservoir architecture chosen parameters well characteristics input data performed evidence combinations aforementioned factors seriously degrade performance however metrics used reservoir computing literature tend experimentally investigated. explore well reservoir response separates classes separation ratio point-wise separation class separation used. measure well reservoir separate inputs distinct classes distances disparate classes large keeping similar inputs close. similarly measure eﬀectively reservoir process particular dataset researchers universal approximation property kernel quality reservoir capacity echo state property measures properties concern representation inputs within reservoir response reconstructability input signal reservoir states. robustness noise generalization rank lyapunov coeﬃcient considered. although reservoir dynamics simple descriptions rigorous treatment behavior proven diﬃcult results far. proposition distance reservoir states given time bounded terms reservoir states previous timestep spectral radius reservoir weights. although mathematically proven proposition covers randomly connected esns incrementing timestep activation functions form tanh. theorem bounds distance output vectors determined using linear read-out weights terms reservoir parameters behavior input signals. theorems below prove upper bounds distances reservoir responses diﬀerent inputs terms reservoir parameters behavior inputs esns tdrs generality results given readability ﬁrst introduce notation. denote input time corresponding reservoir states δijt diﬀerence input signals time εijt distance corresponding node states time suppose sup{δijt bounded pair nonlinear activation function lipschitz continuous optimal lipschitz constant finally denote vector whose theorem suppose reservoir node states determined using dynamics equation spectral radius wres distance reservoir nodes time corresponding input signals satisﬁes theorems show input signals small pointwise discrepancies well-chosen reservoir parameters associated reservoir state norms cluster well. however theorems guarantee distinct inputs mapped dissimilar reservoir node state norms. this turn separation ratio introduced explored completeness include here modiﬁed algorithm algorithm algorithm operator reservoir responses themselves algorithm consider norms reservoir responses. handwritten digits classiﬁed using trained linear output weights algorithm using principal components method algorithm data used united states postal service database obtained sample images shown figure image data split classes images each representing digits simulation presented below images class randomly selected form training remaining images used test set. although nearby pixel behavior preserved horizontal direction transforming image column vector correlations still present reservoir response long short term memory property. randomly chosen density scaled largest eigenvalue mask used reservoirs tdr-type reservoirs nodes again parameter appearing equation ranges inputs multiplexed mask length randomly taking values reservoir sampled every time-steps. therefore simulation images class randomly selected form training dataset however training dataset selection used pair nonlinear activation function chosen throughout. trained linear output weights regularization parameters used. figure plots maximum separation ratio equation attained reservoirs algorithms selections parameters notice reservoirs separate data particularly well norms reservoir responses used algorithm tend slightly better separated vector responses used algorithm plots figure show average classiﬁcation accuracy bottom plots give time required classify images test using reservoir types algorithm algorithm parameters results clustering approach presented clustering approach always achieves higher classiﬁcation accuracy trained linear output weights takes seconds longer classify images. notice clustering approach fairly robust choice reservoir parameters trained linear output weights sensitive inversely related separation ratio given figure clustering approach algorithm also applied input dataset without using reservoir. trials average accuracy clustering method applied input data smaller average accuracy attained algorithm using tdr. suggests clustering method well-suited problem processing data reservoir improves accuracy parameter choices since reservoir preserves spatial correlations well. figure displays inequalities presented theorems measuring discrepancy reservoir activations time similar inputs. input signals randomly selected class ‘s’. values shown ﬁgure found dividing right hand side inequality giving figure comparison performance proposed method using algorithm method trained linear output weights using algorithm reservoirs various parameters reservoir size ﬁgures present classiﬁcation accuracy test bottom ﬁgures present total time required seconds classify entire test images. plots work theoretically experimentally explored method classify spatiotemporal patterns using principal components norms reservoir states training set. proposed method compared traditional method using trained linear output weights types reservoir topologies using several parameter selections. numerical experiments proposed method achieved better classiﬁcation accuracy test took longer complete computations. proposed method loses information since considers norms reservoir state vectors leads robustness respect reservoir type size well parameter choice. basic implementation methods used fundamental principles could compared. sophisticated implementations could used future work improve speed accuracy methods. adaptations could include selecting better training sets introducing subclasses reduce intra-class variation improve class separation using optimally designed masks tdrs reﬁning reservoir connections weights improving selection parameters subsequent solving trained output weights.", "year": 2016}