{"title": "Ordering-Based Search: A Simple and Effective Algorithm for Learning  Bayesian Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "One of the basic tasks for Bayesian networks (BNs) is that of learning a network structure from data. The BN-learning problem is NP-hard, so the standard solution is heuristic search. Many approaches have been proposed for this task, but only a very small number outperform the baseline of greedy hill-climbing with tabu lists; moreover, many of the proposed algorithms are quite complex and hard to implement. In this paper, we propose a very simple and easy-to-implement method for addressing this task. Our approach is based on the well-known fact that the best network (of bounded in-degree) consistent with a given node ordering can be found very efficiently. We therefore propose a search not over the space of structures, but over the space of orderings, selecting for each ordering the best network consistent with it. This search space is much smaller, makes more global search steps, has a lower branching factor, and avoids costly acyclicity checks. We present results for this algorithm on both synthetic and real data sets, evaluating both the score of the network found and in the running time. We show that ordering-based search outperforms the standard baseline, and is competitive with recent algorithms that are much harder to implement.", "text": "basic tasks bayesian networks learning network structure data. bn-learning problem nphard standard solution heuristic search. many approaches proposed task small number outperform baseline greedy hill-climbing tabu lists; moreover many proposed algorithms quite complex hard implement. paper propose simple easy-toimplement method addressing task. approach based well-known fact best network consistent given node ordering found eciently. therefore propose search space structures space orderings selecting ordering best network consistent search space much smaller makes global search steps lower branching factor avoids costly acyclicity checks. present results algorithm synthetic real data sets evaluating score network found running time. show orderingbased search outperforms standard baseline competitive recent algorithms much harder implement. much work done problem learning structure bayesian network data task formulated nding network structure maximizes scoring function dened relative although several denitions score proposed commonly used bayesian score specically variant unfortustandard methodology addressing problem perform heuristic search space. many algorithms proposed along lines varying formulation search space algorithm used search space. however proven surprisingly hard beat simple highly eective baseline greedy hill-climbing search space network structures modied tabu list random restarts. recently several powerful algorithms proposed although approaches shown provide improvements baseline tend fairly complex hard implement. paper dene simple eective algorithm nding high-scoring network structure. approach based fundamental observation that given ordering variables network nding highest-scoring network consistent np-hard. indeed bound indegree node task accomplished time itself observation limited determining appropriate ordering dicult problem usually requiring signicant domain knowledge. however observation leads obvious idea conducting search space orderings rather specic network structures. dene score ordering score best network consistent dene local search operators ipping pair adjacent nodes ordering traverse space orderings greedy hill-climbing search tabu list ranconsider problem analyzing distribution random variables takes values domain val. simplicity focus case variables discrete-valued approach extends easily continuous case. input fully observed data complete assignment variables goal network structure good predictor data. common approach task dene optimization problem. dene scoring function score evaluates dierent networks relative data need solve combinatorial optimization problem nding network achieves highest score. remainder discussion take training omit mentioning explicitly. several scoring functions proposed; common bic/mdl score score details scores relevant discussion. purpose assume properties shared scores others common use. first score decomposable i.e. scores associated individual families second property reduction score sucient statistics associated individual families. discrete case statistics simply frequency counts instantiations within family val). task hard combinatorial problem. several specic instantiations shown np-hard even maximum number parents node intuition behind result that global acyclicity constraint choice parent node imposes constraints possible parent sets nodes. method circumventing problem postulate pre-determined ordering restrict graph consistent ordering constraint ensures consistent structures acyclic rendering choice parent restarts ordering maximizes score. idea using orderings provide global view structures also used larranaga proposed approach similar ours using genetic algorithm search structures. algorithm quite complex advantages practice unclear. dierent setting friedman koller used mcmc space orderings bayesian model averaging structure discovery. ordering search space turns useful properties. first signicantly smaller space network structures network structures versus orderings. second step search makes global modication current hypothesis thereby better avoiding local minima. third branching factor search space rather reducing cost evaluating candidate successors step. finally acyclicity issue given ordering avoid need perform acyclicity checks candidate successors potentially costly operation large networks. main disadvantage ordering-based search need compute advance large sufcient statistics variable possible parent set. cost particularly high number data instances large. reduce cost using ad-tree data structure moore pruning space possible parents node using method friedman note that data large also reduce computational burden learning randomly-sampled subset. experimented ordering-based search comparing standard baseline greedy-tabu search structures. used synthetic data generated known network real data domain gene expression. show that domains large number variables method less likely trapped local minima therefore often signicantly higher-scoring structures. moreover generally solution much faster structure-based search also performed partial comparison experimental results moore wong showing simpler method networks comparable quality often using less computation. overall results suggest simple somewhat obvious method surprisingly eective approach model selection bayesian network learning. dierent nodes independent. observation used early algorithms structure learning searched network consistent pre-determined ordering unfortunately coming good ordering requires signicant amount domain knowledge commonly available many practical applications. therefore recent algorithms structure learning make assumption search general space network structures. common solution nding highscoring network variant local search space networks using operators edge addition deletion reversal. decomposability property sucient statistics allow operators evaluated eciently. typically algorithm performs greedy hill-climbing search occasional random restarts address problem local maxima. important improvement also avoids local maxima tabu list prevents algorithm undoing operators performed recently. despite simplicity approach despite extensive attempts better methods baseline proven hard beat. paper propose simple approach based observation nding best network consistent given ordering much easier general case. restrict network in-degree number parents node bound note assumption commonly used structure learning algorithms help reduce fragmentation data resulting over-tting training set. given ordering dene possible parent sets node thus optimal parent node time fmax maximal number possible families node. formulation fmax decisions dierent nodes constrain other selected families provides optimal network consistent perform search using simple successful approach greedy local hill-climbing random restarts tabu list. state space ordering several possible sets operators space simple worked well experiments simple swap operator perform search considering candidate successors current ordering. compare delta-scores successor orderings obtained swaps dierence score current take gives highest delta-score. tabu list used prevent algorithm reversing swap executed recently search. continue process local maximum reached. importantly case bayesian network search gain considerable savings caching computations. consider operator takes ordering another possible parent sets variable xij+ remain unchanged variables preceding thus operator need recompute optimal parent sets variables rather moreover take step going operators swapping xij+ swapping xij+ operators moreover delta-score relative search states change. thus move need re-evaluate delta-score newly-created operators. overall computed delta-scores cost evaluating delta-scores chosen successor fmax. steps search requires time linear fmax. addition initialize search must initially compute score hence sucient statistics every possible family every reduce cost several ways. first ecient ad-tree data structure moore pre-compute sucient statistics relevant families advance. structure allows compute sucient statistics even large data sets practice computation time longer linear number records data set. rank possible families node according score computed using sucient statistics. best scoring family consistent searching list selecting family consistent reduce cost pruning possible parent sets node thereby reducing cost step fmax fmax. specifically score score eliminate family consideration ordering legal choice parent algorithm always pick pruning signicant eect practice; example icu-alarm network observed cases fmax whereas pruning procedure sound guaranteed never remove optimal parent consideration. however reduces costs incurred search itself; initially must still pre-compute scores possible parent sets. reduce costs using heuristic pruning procedure necessarily sound based sparse candidate algorithm friedman heuristic reduces search space pre-selecting small candidate parents variable consider parent sets selected among candidate set. implementation select variable xed-size candidate parents variables strongly correlated pre-selection reduces number families must score number possible parent sets considered search. evaluated algorithm ordering-search variety data sets synthetic real generated synthetic data sets bayesian networks icu-alarm network diabetes network also used discretized version real world yeast gene expression data sets large subset stress response data small subset rosetta yeast knockout data also three data sets used moore wong allowing direct comparison results. obtain baseline performance implemented standard benchmark algorithm greedy hill-climbing space graphs random restarts tabu list implementation ad-trees computing sucient statistics candidate parents node selected sparse candidate algorithm. optimized baseline search much possible compared state-of-the-art systems convinced fair benchmark. search algorithms parameters aect performance. selected size tabu list number random moves restart using systematic search picking best parameters algorithm. starting point algorithms also chosen according performance search dag-search best results given starting empty network whereas ordering-search chose random starting point. number moves without improvement restarting selected size tabu list. number candidate parents node chosen depending size data number nodes original network. data generated network selected maximal in-degree actual maximum in-degree network. real data bounds applied dag-search ordering-search. finally runs used score uniform prior equivalent sample size experiments performed pentium gigabytes ram. compare searches recorded best scoring network found computation time network found. experiment least times averaged results. graphical results found figure precise numerical values table terms computation time dag-search always produces result ordering-search. initial cost precomputing family scores. however soon computation over ordering-search converges almost instantaneously. overall appears ordering-search figure score datapoint versus computation time. ordering-search represented thick line dag-search thin line. known score original network represented dashed line. minimal vertical axis scale used except dierence searches large. time seconds. table comparison results achieved ordering search number pair) search best result bold. score log-likelihood test data given datapoint. time convergence seconds. mostly converges dag-search small constant factor afterwards. terms maximum score obtained algorithm ordering-search performs least well better dag-search. main factors aect diculty search. first number variables determines size search space variables harder search. second main factor number records many records data statistical signal fainter hard algorithms right direction search. results experiments show roughly fewer variables records algorithms exact best value runs experiment. although determined certainty appears likely algorithms optimal networks. number variables gets larger ordering-search outperforms dag-search. believe improved performance reduced size ordering search space fact ordering-search takes much larger steps search space avoiding many local maxima. small data sets ordering-search also gives better results. believe ordering-search gains fact takes larger steps search fact search steps faster. size data sets increases cost precomputing family scores increases whereas statistical signal gets stronger searching greedily tends work well. consequence dierence performance algorithms decreases. largest data note dag-search outperforms ordering-search terms time convergence cost preprocessing step. finally stress data largest number variables smallest number instances expected search space orderings much ecient. conclude search easy algorithms exact score solution found optimum. search gets harder dagsearch longer able optimal solution ordering-search better scoring network. search algorithms trying optimize score hence measuring score best network found fair evaluation. however also tested whether dierences score translate better generalization performance unseen data. therefore measured log-likelihood model found disjoint test generated distribution. synthetic data simply generated data network. real data used -fold cross-validation averaged results folds. results found table obtain similar results experiment score. networks small data sets log-likelihood search space orderings higher space directed graphs. result satisfying surprising known reasonable surrogate generalization performance. finally compare results data sets alarm edsgc letter published results or-search algorithm moore wong hard compare scores obtained algorithms moore wong report prior used experiments score). nevertheless algorithms seem give similar best scoring networks seems dier much point. terms computation time orderingsearch appears much faster shown table note computation time reported or-search wall clock time whereas cpu-time measure ordering-search. although measures same dierence cannot account fact ordering-search order magnitude faster. conclusion safe results or-search much faster machine paper describe simple easy-toimplement algorithm learning bayesian networks data. method based searching space orderings rather standard space network structures. note search spaces also proposed space network equivalence classes space skeletons involve fairly complex operators computationally expensive hard implement. results show method outperforms state-of-the-art methods nding higherscoring networks computation time. several possible extensions work. clever approach prune space candidate families node would also interesting combine approach state-of-the-art structure learning methods data perturbation approach elidan finally method also applied straightforward task structure learning incomplete data using structural approach friedman e-step compute expected sucient statistics thereby expected scores apply ordering-search described. however computation expected sucient statistics requires inference therefore costly; would interesting construct heuristics avoid full recomputation every model modication step. acknowledgements. thank elidan useful comments providing rosetta stress data sets. work funded darpa’s epca program sub-contract international.", "year": 2012}