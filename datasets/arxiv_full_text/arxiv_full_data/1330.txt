{"title": "Training Deep Neural Networks on Noisy Labels with Bootstrapping", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overfitting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the- art results, and can also benefit from unlabeled face images with no modification to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection.", "text": "current state-of-the-art deep learning systems visual object recognition detection purely supervised training regularization dropout avoid overﬁtting. performance depends critically amount labeled examples current practice labels assumed unambiguous accurate. however assumption often hold; e.g. recognition class labels missing; detection objects image localized; general labeling subjective. work propose generic handle noisy incomplete labeling augmenting prediction objective notion consistency. consider prediction consistent prediction made given similar percepts notion similarity deep network features computed input data. experiments demonstrate approach yields substantial robustness label noise several datasets. mnist handwritten digits show model robust label corruption. toronto face database show model handles well case subjective labels emotion recognition achieving state-of-theart results also beneﬁt unlabeled face images modiﬁcation method. ilsvrc detection challenge data show approach extends deep networks high resolution images structured outputs results improved scalable detection. currently predominant systems visual object recognition detection purely supervised training regularization dropout avoid overﬁtting. systems account missing labels subjective labeling inexhaustivelyannotated images. however assumption often hold especially large datasets high-resolution images complex scenes. example recognition class labels missing; detection objects image localized; subjective tasks facial emotion recognition humans even agree class label. training sets deep networks become larger problem missing noisy labels becomes acute argue fundamental problem scaling vision. work propose simple approach hande noisy incomplete labeling weaklysupervised deep learning augmenting usual prediction objective notion perceptual consistency. consider prediction consistent prediction made given similar percepts notion similarity incorporates features learned deep network. categories general structured outputs. provides learner justiﬁcation disagree perceptually-inconsistent training label effectively re-label data training. accurate labels lead better model allows label clean-up learner bootstraps way. course much skepticism labels carries risk ending delusional agent important balance trade-off prediction learner’s perceptual consistency. experiments demonstrate approach yields substantial robustness several types label noise several datasets. mnist handwritten digits show model robust label corruption. toronto face database show model handles well case subjective labels emotion recognition achieving state-of-the-art results also beneﬁt unlabeled face images modiﬁcation method. ilsvrc detection challenge data show approach improves single-shot person detection using multibox network also improves performance full -way detection using multibox region proposal deep post-classiﬁcation. literature semi-supervised weakly-supervised learning vast survey) section focus previous papers inspired work papers weaklysemi-supervised deep learning. notion bootstrapping self-training learning agent proposed word-sense disambiguation unlabeled examples small list seed example sentences labels. algorithm proceeds building initial classiﬁer using seed examples iteratively classifying unlabeled examples extracting seed rules classiﬁer using expanded training data repeating steps convergence. algorithm analyzed abney recently co-training similarlymotivated used pair classiﬁers separate views data iteratively learn generate additional training labels. whitney sarkar proposed bootstrapping labeled training examples graph-based label propagation. brodley friedl developed statistical methods identifying mislabeled training data. rosenberg also trained object detection system weakly-supervised manner using self-training demonstrated proposed model achieved comparable performance models trained much larger labels. however approach works wrapper around existing detection system whereas work integrate consistency objective bootstrapping training deep network itself. work shares similar motivation earlier works instead explicitly generating training labels adding examples training outer loop incorporate consistency objective directly model. addition consider case learning unlabeled examples also noisy labels inexhaustively-annotated examples. mnih hinton developed deep neural networks improved labeling aerial images robust loss functions handle label omission registration errors. work shares similar motivation robustness noisy labels rather formulating loss functions speciﬁc types noise generic consistency objective loss achieve robustness. minimum entropy regularization proposed performs semisupervised learning augmenting cross-entropy loss term encouraging classiﬁer make predictions high conﬁdence unlabeled examples. notable approach training unlabeled examples require generative model beneﬁcial training high-resolution images sensory data. take similar approach sidestepping difﬁculty fully-generative models high-dimensional sensory data. however extend beyond shallow models deep networks structured output prediction. never ending language learning never ending image learning lifelong-learning systems language image understanding respectively. continuously bootstrap using cycle data collection propagation labels newly collected data self-improvement training data. work complementary efforts focuses building robustness noisy missing labels model weakly-supervised deep learning. larochelle bengio developed classiﬁcation uses hybrid generative discriminative training objective. deep boltmann machines also trained semi-supervised manner labels connected layer. recently multi-prediction training generative stochastic networks improved performance simpliﬁed training deep generative models enabling training backpropagation much like standard deep supervised networks. however fully-generative unsupervised training high-dimensional sensory data e.g. imagenet images still behind supervised methods terms performance work follow generative approach directly. instead work focuses beneﬁt unlabeled weakly-labeled examples minimal modiﬁcation existing deep supervised networks. demonstrate increased robustness label noise performance improvements unlabeled data minimal engineering effort. recently problem deep learning noisy labels begun receive attention. also followed idea minimum entropy regularization proposed generating pseudolabels training targets unlabeled data showed improved performance mnist labeled examples. sukhbaatar fergus developed deep learning techniques handling noisy labels learning model noise distribution top-down bottom-up fashion. work push extending beyond class labels structured outputs achieve state-of-the-art scalable detection performance ilsvrc despite fact method require explicitly modeling noise distribution. section describe approaches section uses reconstruction error consistency objective explicitly models noise distribution matrix mapping model predictions training labels. reconstruction loss added promote top-down consistency model predictions observations allows model discover pattern noise data. method presented section uses convex combination training labels current model’s predictions generate training targets thereby avoids directly modeling noise distribution. property well-suited case structured outputs modeling dense interactions among pairs output units neither practical useful. approaches compared empirically section section show apply bootstrapping approach structured outputs using multibox region proposal network handle case inexhaustive structured output labeling single-shot person detection class-agnostic region proposal. observed noisy multinomial labels. standard softmax regresses onto without taking account noisy missing labels. addition optimizing conditional log-likelihood regularization term encouraging class prediction perceptually consistent. label noise distribution posterior true labels deﬁned above. perform discriminative training gradient ascent however purely discriminative training incorporate perceptual consistency explicit incentive model treat true label; viewed another hidden layer multinomial constraint resulting information bottleneck. unpublished work hinton mnih developed restricted boltzmann machine variant hidden multinomial output unit observed noisy label unit described above. associated energy function written bipartite structure conditionally independent given energy function leads similar form posterior marginalizing hidden multinomial unit. probability distribution arising given exp) partition function. model trained generative objective e.g. approximate gradient ascent contrastive divergence generative training naturally provides notion consistency observations predictions model learns draw sample observations conditional however fully-generative training complicated fact exact likelihood gradient intractable computing partition function practice mcmc used. generative training complicated cases features nonbinary. avoid complications make approach rapidly applicable existing deep networks using rectiﬁed linear activations trainable exact gradient descent propose analogous autoencoder version. section develop simple consistency objective require explicit noise distribution reconstruction term. idea dynamically update targets prediction objective based current state model. resulting targets convex combination noisy training label current prediction model. intuitively learner improves time predictions trusted more. mitigates damage incorrect labeling incorrect labels likely eventually highly inconsistent stimuli predicted label model. paying less heed inconsistent labels learner develop coherent model improves ability evaluate consistency noisy labels. refer approach bootstrapping sense pulling oneself one’s bootstraps also inspiration work yarowsky also referred bootstrapping. concretely cross-entropy objective before generate regression targets mini-batch based current state model. empirically evaluated types bootstrapping. soft bootstrapping uses predicted class probabilities directly generate regression targets batch follows fact shown resulting objective equivalent softmax regression minimum entropy regularization previously studied intuitively minimum entropy regularization encourages model high conﬁdence predicting labels used mini-batch stochastic gradient descent leads em-like algorithm e-step estimate true conﬁdence targets convex combination training labels model predictions; m-step update model parameters better predict generated targets. noisy labels also occur structured output prediction problems object detection. current state-of-the-art object detection systems train images annotated bounding labels relevant objects image class label box. however expensive exhaustively annotate image commonly-appearing categories data prone missing annotations. section modify training objective multibox network object detection incorporate notion perceptual consistency loss. multibox approach ground-truth bounding boxes clustered resulting centroids used priors predicting object location. deep neural network trained predict groundtruth object image residual groundtruth bounding best-matching bounding prior. network also outputs logistic conﬁdence score prior indicating model’s belief whether object appears corresponding location. multibox gives proposals conﬁdence scores enables efﬁcient runtime-quality tradeoffs detection thresholding top-scoring proposals within budget. thus attractive target quality improvements pursue section. denote conﬁdence score training targets predicted conﬁdence scores objective multibox written following cross-entropy loss note object locations class labels case sections object location inexhaustive annotation model pays large cost correctly predicting training naively noisy labels leads perverse learning situations following objects category appear training data labeled. reduce loss conﬁdence prediction layer must learn distinguish objects exactly contrary objective visual invariance category-preserving differences. incorporate notion perceptual consistency loss follow approach case multi-class classiﬁcation augment regression targets using model’s current state. hard case estimates obtained thresholding bootstrap variants multibox objective unlabeled positives pose less problem penalties large down-scaled factor ﬁrst term second term. mitigating penalties missing positives data approach allows model learn predict high conﬁdence even objects location often unlabeled. perform experiments three image understanding tasks mnist handwritten digits recognition toroto faces database facial emotion recognition ilsvrc detection. tasks train deep neural network proposed consistency objective. ﬁgures bootstraprecon refers training described section using reconstruction consistency objective. bootstrap-soft bootstrap-hard refer method described sections section train using reconstruction-based objective mnist handwritten digits varying degrees noise labels. speciﬁcally used ﬁxed random permutation labels visualized ﬁgure perform control experiments varying probability applying label permutation training example. models trained mini-batch architecture neural network rectiﬁed linear units. used weight decay found worked best bootstrap-hard bootstrap-soft bootstrap-recon. initialize identity matrix. network trained proposed consistency objective initialized network layers baseline prediction-only model. also possible initialize scratch using approach found pre-training could larger quickly converge good result. intuitively similar initial collection seed rules original bootstrapping algorithm ﬁne-tuning training phase network weights updated backpropagating gradients layers. figure shows bootstrapping method provides signiﬁcant beneﬁt case permuted labels. bootstrap-recon method performs best bootstrap-hard nearly well. bootstrap-soft method provides beneﬁt high-noise regime slightly better baseline overall. figure left digit recognition accuracy versus percent corrupted labels. middle visualization noise pattern. white entry column label mapped label probability training. right visualization learned bootstrap-recon model figure also shows bootstrap-recon effectively learns noise distribution parameters intuitively loss reconstruction term provides learner basis predictions disagree training labels since must able reconstructed bootstrap-recon learning non-identity allows ﬂexibly vary better reconstruct without incurring penalty prediction error. however interesting note bootstrap-hard achieves nearly equivalent performance without explicitly parameterizing noise distribution. useful reconstruction challenging many cases drawn complicated high-dimensional distribution bootstrap-hard trivial implement existing deep supervised networks. section present results emotion recognition. toronto faces database images emotion labels. experiments ﬁrst extracted spatial-pyramidpooled omp- features described -dimensional features. trained network predict -of- emotion labels image. figure summarizes results. case mnist bootstrap-recon bootstrap-hard perform best signiﬁcantly outperforming softmax baseline bootstrap-soft provides modest improvement. signiﬁcant off-diagonal weight learned bootstrap-recon suggesting model learns hedge emotion prediction training spreading probability mass predicted class commonly-confused classes afraid surprised. strongest diagonals happy column fact happy common expression perhaps happy expressions large visual diversity. method improves emotion recognition performance knowledge state-of-the-art. performance improvement three bootstrap methods suggests approach useful mistaken labels also semi-supervised learning learning weak labels emotion categories. section apply method detecting persons using multibox network built inception architecture proposed ﬁrst pre-trained multibox classagnostic localization using full ilsvrc training since several thousand images labeled persons ilsvrc ﬁne-tuned person images only. important point comparison top-k bootstrapping heuristic introduced multibox training person detection presence missing annotations. approach top-k largest conﬁdence predictions dropped loss setting used words gradient coming top-k conﬁdent location predictions. fact viewed form bootstrapping top-k conﬁdent locations modify targets become predictions themselves. work achieve similar better performance general applied multibox discriminative models. precision-recall curves ﬁgure show proposed bootstrapping improves substantially prediction-only baseline. high-precision curve approaches introduced paper perform better top-k heuristic slightly better high-recall. table proposed bootstrap-hard bootstrap-soft top-k heuristic signiﬁcantly improve average precision compared prediction-only baseline. recallp recall achievable precision. section apply method case object detection large-scale imagenet data. proposed method applied ways ﬁrst multibox network region proposal second classiﬁer network predicts labels cropped image region. follow approach combine image crops multibox region proposals deep network context features input classiﬁer proposed region. paper developed novel training methods weakly-supervised deep learning demonstrated effectiveness approach multi-class prediction structured output prediction several datasets. method exceedingly simple applied little engineering effort existing networks trained using purely-supervised objective. improvements show even simple methods suggest moving beyond purely-supervised deep learning worthy research attention. addition achieving better performance data already have results suggest performance gains achieved collecting data cheaper price since image annotation need exhasutive mistaken labels harmful performance. future work promising consider learning time-dependent policy tuning scaling factor prediction perceptual consistency objectives also extend approach case situated agent. another promising direction augment large-scale training detection unlabeled weakly-labeled images beneﬁt proposed perceptual consistency objective. coates adam andrew importance encoding versus training sparse coding vector quantization. proceedings international conference machine learning hinton geoffrey srivastava nitish krizhevsky alex sutskever ilya salakhutdinov ruslan improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv. nigam kamal ghani rayid. analyzing effectiveness applicability co-training. proceedings ninth international conference information knowledge management ouyang wanli ping zeng xingyu tian yonglong hongsheng yang shuo wang xiong yuanjun qian chen deepid-net multi-stage deformable deep convolutional neural networks object detection. arxiv preprint arxiv. rifai salah bengio yoshua courville aaron vincent pascal mirza mehdi. disentangling factors variation facial expression recognition. computer vision–eccv springer russakovsky olga deng krause jonathan satheesh sanjeev sean huang zhiheng karpathy andrej khosla aditya bernstein michael berg alexander fei-fei imagenet large scale visual recognition challenge szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. going deeper convolutions. arxiv preprint arxiv. whitney sarkar anoop. bootstrapping graph propagation. proceedings annual meeting association computational linguistics long papers-volume association computational linguistics", "year": 2014}