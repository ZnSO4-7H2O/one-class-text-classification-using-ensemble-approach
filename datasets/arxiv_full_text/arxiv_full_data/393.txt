{"title": "Deep Predictive Coding Networks for Video Prediction and Unsupervised  Learning", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "q-bio.NC"], "abstract": "While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (\"PredNet\") architecture that is inspired by the concept of \"predictive coding\" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.", "text": "great strides made using deep learning algorithms solve supervised learning tasks problem unsupervised learning leveraging unlabeled examples learn structure domain remains difﬁcult unsolved challenge. here explore prediction future frames video sequence unsupervised learning rule learning structure visual world. describe predictive neural network architecture inspired concept predictive coding neuroscience literature. networks learn predict future frames video sequence layer network making local predictions forwarding deviations predictions subsequent network layers. show networks able robustly learn predict movement synthetic objects networks learn internal representations useful decoding latent object parameters support object recognition fewer training views. also show networks scale complex natural image streams capturing aspects egocentric movement movement objects visual scene representation learned setting useful estimating steering angle. altogether results suggest prediction represents powerful framework unsupervised learning allowing implicit learning object scene structure. many successful current deep learning architectures vision rely supervised learning large sets labeled training images. performance networks undoubtedly impressive reliance large numbers training examples limits utility deep learning many domains datasets available. furthermore need large numbers labeled examples stands odds human visual learning views object often needed enable robust recognition object across wide range different views lightings contexts. development representation facilitates abilities especially unsupervised largely unsolved problem. addition computer vision models typically trained using static images real world visual objects rarely experienced disjoint snapshots. instead visual world alive movement driven self-motion viewer movement objects within scene. many suggested temporal experience objects move undergo transformations serve important signal learning structure objects instance wiskott sejnowski proposed slow feature analysis framework exploiting temporal structure video streams approach attempts build feature representations extract slowly-varying parameters object identity parameters produce fast changes image movement object. approaches rely temporal coherence arguably yielded representations powerful learned supervised methods nonetheless point potential learning useful representations video here explore another potential principle exploiting video unsupervised learning prediction future image frames insight order able predict visual world change time agent must least implicit model object structure possible transformations objects undergo. designed neural network architecture informally call prednet attempts continually predict appearance future video frames using deep recurrent convolutional network bottom-up topconnections. work builds previous work next-frame video prediction take particular inspiration concept predictive coding neuroscience literature predictive coding posits brain continually making predictions incoming sensory stimuli top-down connections convey predictions compared actual observations generate error signal. error signal propagated back hierarchy eventually leading update predictions. demonstrate effectiveness model synthetic sequences access underlying generative model investigate model learns well natural videos. consistent idea prediction requires knowledge object structure networks successfully learn internal representations well-suited subsequent recognition decoding latent object parameters also architecture scale effectively natural image sequences training using car-mounted camera videos. network able successfully learn predict movement camera movement objects camera’s view. supporting notion prediction unsupervised learning rule model’s learned representation setting supports decoding current steering angle. figure predictive coding network left illustration information within layers. layer consists representation neurons output layer-speciﬁc prediction time step compared target produce error term propagated laterally vertically network. right module operations case video sequences. prednet architecture diagrammed figure network consists series repeating stacked modules attempt make local predictions input module subtracted actual input passed along next layer. brieﬂy module network consists four basic parts input convolutional layer recurrent representation layer prediction layer error representation representation layer recurrent convolutional network generates prediction layer input next frame. network takes difference outputs error representation split separate rectiﬁed positive negative error populations. error passed forward convolutional layer become input next layer recurrent prediction layer receives copy error signal along top-down input representation layer next level network organization network ﬁrst time step operation right side network equivalent standard deep convolutional network. meanwhile left side network equivalent generative deconvolutional network local recurrence stage. architecture described inspired originally proposed formulated modern deep learning framework trained end-to-end using gradient descent loss function implicitly embedded network ﬁring rates error neurons. work also shares motivation deep predictive coding networks chalasani principe however framework based upon sparse coding linear dynamical system greedy layer-wise training whereas rooted convolutional recurrent neural networks trained backprop. architecture general respect kinds data models focus image sequence data. consider sequence images target lowest layer actual sequence itself i.e. followed rectiﬁed computed convolution error units layer below linear unit activation max-pooling. representation neurons speciﬁcally convolutional lstm units setting ﬁrst spatially upsampled pooling present feedforward path. predictions stack followed relu non-linearity. made convolution lowest layer also passed saturating non-linearity maximum pixel value satlu min. finally error response calculated difference split relu-activated positive negative prediction errors concatenated along feature dimension. discussed although explicit model separate error populations analogous existence on-center off-surround off-center on-surround neurons early visual system. full update rules listed equations model trained minimize weighted activity error units. explicitly training loss formalized equation weighting factors time layer number units layer. error units consisting subtraction followed relu activation loss layer equivalent error. although explored here error unit implementations potentially even probabilistic adversarial could also used. order unit model updated must also speciﬁed implementation described algorithm updating states occurs passes top-down pass states computed forward pass calculate predictions errors higher level targets. last detail note initialized zero which convolutional nature network means initial prediction spatially uniform. gain understanding representations learned proposed framework ﬁrst trained prednet models using synthetic images access underlying generative stimulus model latent parameters. created sequences rendered faces rotating degrees freedom along roll axes. faces start random orientation rotate random constant velocity total frames. different face sampled sequence. images processed grayscale values normalized pixels size. used sequences training validation testing. predictions generated prednet model shown figure model able accumulate information time make accurate predictions future frames. since representation neurons initialized zero prediction ﬁrst time step uniform. second time step motion information prediction blurry reconstruction ﬁrst time step. iterations model adapts underlying dynamics generate predictions closely match incoming frame. choosing hyperparameters model performed random search chose model lowest error frame prediction averaged time steps validation set. given selection criteria best performing models tended loss solely concentrated lowest layer case model shown. using equal loss layer considerably degraded predictions enforcing moderate loss upper layers magnitude smaller lowest layer slightly worse predictions illustrated figure appendix. cases time loss weight zero ﬁrst time step time steps after. remaining hyperparameters model shown layers ﬁlter sizes convolutions max-pooling stride number channels layer units model weights optimized using adam algorithm quantitative evaluation generative models difﬁcult unsolved problem report prediction error terms meansquared error structural similarity index measure ssim designed correlated perceptual judgments ranges larger score indicating greater similarity. compare prednet trivial solution copying last frame well control model shares overall architecture training scheme prednet sends forward layer-wise activations rather errors model thus takes form traditional encoder-decoder pair encoder lateral skip connections convolutional lstm decoder. performance models rotating faces dataset summarized table scores calculated average predictions ﬁrst frame. report results prednet model trained loss lowest layer denoted prednet well model trained weight upper layers denoted prednet lall. prednet models outperformed baselines measures model slightly outperforming lall expected evaluating pixel-level predictions. synthetic sequences chosen initial training order better understand learned different layers model speciﬁcally respect underlying generative model rotating faces generated using facegen software package internally generates face meshes principal component analysis face space derived corpus face scans. thus latent parameters image sequences used consist initial roll angles roll velocities principal component values control identity face. understand information contained trained models decoded latent parameters representation neurons different layers using ridge regression. states taken earliest possible informative time steps which notation second third steps respectively static dynamic parameters. regression trained using sequences validation testing. baseline comparison information implicitly embedded network architecture compare decoding accuracies untrained network random initial weights. note randomly initialized case still expect above-chance decoding performance given past theoretical empirical work random networks latent variable decoding accuracies roll velocities initial angle ﬁrst shown left panel figure several interesting patterns. first trained models learn representation generally permits better linear decoding underlying latent factors randomly initialized model striking difference terms rotation speed second notable difference lall versions occurs ﬁrst principle component model trained loss layers higher decoding accuracy model trained loss lowest layer. figure information contained prednet representation rotating faces sequences. left decoding latent variables using ridge regression angular velocity θpan angle ﬁrst principal component face αroll roll angular velocity). right orientation-invariant classiﬁcation static faces. latent variable decoding analysis suggests model learns representation generalize well tasks explicitly trained. investigate further assessed models classiﬁcation task single static images. created dataset previously unseen facegen faces angles equally spaced roll angles equally spaced therefore orientations identity tested cross-validated fashion. linear decode face identity model’s representation random subset orientations tested remaining angles. size training ranging orientations face different random splits generated results averaged splits. static face classiﬁcation task compare prednets standard autoencoder variant ladder network models constructed number layers channel sizes prednets well similar alternating convolution/max-pooling upsampling/convolution scheme. networks autoencoders trained reconstruction loss dataset consisting individual frames sequences used train prednets. ladder network denoising autoencoder lateral skip connections must also choose noise parameter well relative weights layer total cost. tested noise levels ranging increments loss weights either evenly distributed across layers solely concentrated pixel layer bottom layer upper layers shown model performed best classiﬁcation consisted noise pixel weighting. lastly architecture ladder network lateral top-down streams combined combinator function. inspired learnable improved results consistent comparing prednet used purely convolutional combinator. given distributed representation networks decoded concatenation feature representations layers except pixel layer. prednets representation units used features extracted processing input frame. face classiﬁcation accuracies using representations learned lall prednets standard autoencoder ladder network variant shown right panel figure prednets compare favorably models sizes training suggesting learn representation relatively tolerant object transformations. similar decoding accuracy ﬁrst principle component prednet lall model actually outperformed variant. altogether results suggest predictive training prednet viable alternative models trained traditional reconstructive denoising loss relative layer loss weightings important particular task hand. next sought test prednet architecture complex real-world sequences. testbed chose car-mounted camera videos since videos span across wide range settings characterized rich temporal dynamics including self-motion vehicle motion objects scene models trained using videos kitti dataset captured roof-mounted camera driving around urban environment germany. sequences frames sampled city residential road categories recording sessions used training used validation. frames center-cropped downsampled pixels. total training consisted roughly frames. random hyperparameter search model selection based validation resulted layer model convolutions layer channel sizes models trained adam using loss either solely computed lowest layer weight lowest layer upper layers adam parameters initially default values learning rate decreasing factor halfway training. assess network indeed learned robust representation tested caltech pedestrian dataset consists videos dashboard-mounted camera vehicle driving around angeles. testing sequences made match frame rate kitti dataset cropped pixels. quantitative evaluation performed entire caltech test partition split sequences frames. sample prednet predictions caltech pedestrian dataset shown figure example videos found https//coxlab.github.io/prednet/. model able make fairly accurate predictions wide range scenarios. sequence fig. passing opposite direction model perfect able predict trajectory well ground leaves behind. similarly sequence model able predict motion vehicle completing left turn. sequences illustrate prednet judge movement predicts appearance shadows stationary vehicle approach. model makes reasonable predictions even difﬁcult scenarios camera-mounted vehicle turning. sequence model predicts position tree vehicle turns onto road. turning sequences also illustrate model’s ability ﬁll-in able extrapolate tree textures unseen regions come view. additional control show sequence bottom fig. input temporally scrambled. case model generates blurry frames mostly resemble previous frame. finally although prednet shown trained predict frame ahead also possible predict multiple frames future feeding back predictions inputs recursively iterating. explore appendix quantitatively prednet models outperformed cnn-lstm encoderdecoder. ensure difference performance simply choice hyperparameters trained models four sets hyperparameters sampled initial random search number layers ﬁlter sizes number ﬁlters layer. four additional sets prednet best performance average error reduction ssim figure prednet predictions car-cam videos. ﬁrst rows contain ground truth second rows contain predictions. sequence line temporally scrambled. model trained kitti dataset sequences shown caltech pedestrian dataset. respectively compared cnn-lstm encoder-decoder. details well thorough investigation systematically simpliﬁed models continuum prednet cnn-lstm encoder-decoder found appendix brieﬂy elementwise subtraction operation prednet seems beneﬁcial nonlinearity positive/negative splitting also adds modest improvements. finally experiments measure beneﬁts component model also directly compare recent work similar car-cam setting reporting results pixel grayscale car-cam dataset released brabandere prednet model outperforms model brabandere details found appendix also appendix present results human.m dataset reported finn without re-optimizing hyperparameters test implicit encoding latent parameters car-cam setting used internal representation prednet estimate car’s steering angle used dataset released comma.ai consisting videos totaling hours mostly highway driving. ﬁrst trained networks next-frame prediction linear fully-connected layer learned representation estimate steering angle using loss. concatenate representation layers ﬁrst spatially average pool lower layers match spatial size upper layer order reduce dimensionality. steering angle estimation results using representation time step shown figure given labeled training examples simple linear readout prednet representation explains variance steering angle outperforms cnn-lstm enc.-dec. labeled training examples prednet point reference model designed predict steering angle albeit single frame instead multiple frames achieve trained end-to-end using labeled training examples. details analysis found appendix interestingly task prednet lall model actually underperformed model slightly underperformed cnn-lstm enc.-dec suggesting parameter affect representation learned different values preferable different tasks. nonetheless readout lall model still explained substantial proportion steering angle variance strongly outperformed random initial weights. overall analysis demonstrates representation learned prediction particularly prednet model appropriate hyperparameters contain useful information underlying latent parameters. figure steering angle estimation accuracy comma.ai dataset left example steering angle curve model estimations segment test set. decoding performed using fully-connected readout prednet representation trained labeled training examples. prednet representation trained next-frame prediction comma.ai training set. right mean-squared error steering angle estimation. above demonstrated predictive coding inspired architecture able predict future frames synthetic natural image sequences. importantly shown learning predict object scene move future frame confers advantages decoding latent parameters give rise object’s appearance improve recognition performance. generally argue prediction serve powerful unsupervised learning signal since accurately predicting future frames requires least implicit model objects make scene allowed move. developing deeper understanding nature representations learned networks extending architecture instance allowing sampling important future directions. would like thank rasmus berg palm fruitful discussions early brainstorming. would also like thank developers keras work supported iarpa national science foundation center brains minds machines mariusz bojarski davide testa daniel dworakowski bernhard firner beat flepp prasoon goyal lawrence jackel mathew monfort muller jiakai zhang zhang jake zhao karol zieba. learning self-driving cars. corr dileep george jeff hawkins. hierarchical bayesian model invariant pattern recognition visual cortex. proceedings international joint conference neural networks. ieee catalin ionescu dragos papava vlad olaru cristian sminchisescu. human.m large scale datasets predictive methods human sensing natural environments. ieee transactions pattern analysis machine intelligence nicolas pinto david doukhan james dicarlo david cox. high-throughput screening approach discovering good forms biologically inspired visual representation. plos comput biol marc’aurelio ranzato arthur szlam joan bruna micha¨el mathieu ronan collobert sumit chopra. video modeling baseline generative models natural videos. corr andrew saxe maneesh bhand zhenghao chen pang bipin suresh andrew random weights unsupervised feature learning. workshop deep learning unsupervised feature learning xingjian zhourong chen wang dit-yan yeung wai-kin wong wang-chun woo. convolutional lstm network machine learning approach precipitation nowcasting. corr spratling. unsupervised learning generative discriminative weights encoding elementary image components predictive coding model cortical function. neural computation christopher summerﬁeld tobias egner matthew greene etienne koechlin jennifer mangels hirsch. predictive codes forthcoming perception frontal cortex. science table contains results additional variations prednet cnn-lstm encoder-decoder evaluated caltech pedestrian dataset trained kitti. evaluate models terms pixel prediction thus using prednet model trained loss lowest layer base model. addition mean-squared error structural similarity index measure include calculations peak signal-to-noise ratio model evaluate original hyperparameters well four additional sets hyperparameters randomly sampled initial random search explanation additional control models cnn-lstm encoder-decoder model except number ﬁlters doubled. controls total number ﬁlters model compared prednet since prednet ﬁlters produce layer integrated model’s feedforward response. cnn-lstm enc.-dec. cnn-lstm encoder-decoder model except error passed lowest layer. remaining layers pass activations training loss taken lowest layer variation allows determine prediction subtraction operation upper layers essentially unconstrained learnable case aids model’s performance. cnn-lstm enc.-dec. cnn-lstm encoder-decoder model except activations split positive negative populations passed layers network. isolates effect additional nonlinearity introduced procedure. table quantitative evaluation additional controls next-frame prediction caltech pedestrian dataset training kitti. first number indicates score original hyperparameters. number parenthesis indicates score averaged total different hyperparameters. ssim equalizing number ﬁlters cnn-lstm encoder-decoder cannot account performance difference prednet actually leads overﬁtting decrease performance. passing error lowest layer cnn-lstm enc.-dec. improves performance still match prednet errors passed layers. finally splitting activations positive negative populations cnn-lstm enc.-dec. help prednet linear error activation performs slightly worse original split version. together results suggest prednet’s error passing operation lead improvements next-frame prediction performance. main comparison text control model isolates effects unique components prednet directly compare published models. report results pixel grayscale car-cam dataset human.m dataset compare concurrently developed models brabandere finn respectively. comparisons model hyperparameters prednet model trained kitti train scratch datasets. modiﬁcation make train using loss instead effective loss since models train loss report results using l-based metrics psnr finn keep original prednet model intact directly optimize using actual predicted frames. measure next-frame prediction performance inputting frames frames respectively car-cam human.m datasets consistent published works. also include results using feedforward multi-scale network similar model mathieu human.m reported finn dataset similar kitti model outperforms model proposed brabandere human.m model outperforms model similar underperforms finn although note perform hyperparameter optimization. figure extrapolation sequences generated feeding prednet predictions back model. left orange line normal predictions; right generated recursively using predictions input. first ground truth sequences. second generated frames original model trained solely predict third model ﬁne-tuned extrapolation. models presented originally trained predict frame ahead made predict multiple frames treating predictions actual input recursively iterating. examples process shown figure prednet model. although next frame predictions reasonably accurate model naturally breaks extrapolating future. surprising since predictions unavoidably different statistics natural images model trained handle additionally train model process predictions model better able extrapolate. third every sequence shows output original prednet ﬁne-tuned extrapolation. starting trained weights model trained loss time steps actual frame inputted ﬁrst model’s predictions used input network last ﬁrst time steps training loss calculated activations usual last calculated directly mean absolute error respect ground truth frames. despite eventual blurriness ﬁne-tuned model captures structure extrapolations tenth time step. instance ﬁrst sequence model estimates general shape upcoming shadow despite minimal information last seen frame. second sequence model able extrapolate motion moving right. reader encouraged visit https//coxlab.github.io/prednet/ view predictions video form. quantitatively model’s predictions stay well trivial solution copying last seen frame illustrated increases fairly linearly time steps even though model trained prediction. figure show steering angle estimation accuracy comma.ai dataset using representation learned prednet model function number frames inputted model. prednet’s representation layers concatenated fully-connected readout using mse. level number training examples average cross-validation splits. serve points reference include results static models. ﬁrst model autoencoder trained single frame reconstruction appropriately matching hyperparameters. fully-connected layer autoencoder’s representation estimate steering angle fashion prednet. second model default model posted comma.ai code layer cnn. model trained end-to-end estimate steering angle given current frame input loss. addition examples trained version using frames comma dataset models ﬁnal weights chosen minimum validation error training. given relatively small number videos dataset compared average duration video used video validation testing chosen random continuous chunk discarded frames chosen segments training set. illustrated figure prednet’s performance gets better time might expect model able accumulate information. interestingly performs reasonably well time step regime orthogonal training procedure prednet dynamics. altogether results point usefulness model learning underlying latent parameters. figures compare next-frame predictions prednet lall model trained prediction loss layers prednet model trained loss lowest layer. ﬁrst glance difference predictions seem fairly minor indeed terms lall model underperformed version respectively rotating faces caltech pedestrian datasets. upon careful inspection however apparent lall predictions lack ﬁner details predictions blurry regions high variance. instance rotating faces facial features less deﬁned caltech details approaching shadows cars less precise. figure next-frame predictions prednet lall model rotating faces dataset comparison version. error lall−l visualization shows pixel error smaller model lall model. green regions correspond better corresponds lall better. figure next-frame predictions prednet lall model caltech pedestrian dataset comparison version. error lall visualization shows pixel error smaller model lall model. green regions correspond better corresponds lall better.", "year": 2016}