{"title": "Predicting Runtime Distributions using Deep Neural Networks", "tag": ["cs.AI", "cs.LG"], "abstract": "Many state-of-the-art algorithms for solving hard combinatorial problems include elements of stochasticity that lead to high variations in runtime, even for a fixed problem instance, across runs with different pseudo-random number seeds. Knowledge about the runtime distributions (RTDs) of algorithms on given problem instances can be exploited in various meta-algorithmic procedures, such as algorithm selection, portfolios, and randomized restarts. Previous work has shown that machine learning can be used to individually predict mean, median and variance of RTDs. To establish a new state-of-the-art in predicting RTDs, we demonstrate that the parameters of an RTD should be learned jointly and that neural networks can do this well by directly optimizing the likelihood of an RTD given runtime observations. In an empirical study involving four algorithms for SAT solving and AI planning, we show that our neural networks predict the true RTDs of unseen instances better than previous methods. As an exemplary application of RTD predictions, we show that our RTD models also yield good predictions of running these algorithms in parallel.", "text": "st¨utzle trivial measure algorithm’s empirical instance running many times completion instances would like solve course practical. instead would like estimate instance without running algorithm rich history artiﬁcial intelligence shows runtime algorithms solving hard combinatorial problems indeed predicted certain degree runtime predictions enabled wide range meta-algorithmic procedures algorithm selection model-based algorithm conﬁguration generating hard benchmarks gaining insights instance hardness algorithm performance creating cheap-to-evaluate surrogate benchmarks given method predicting rtds randomized algorithms applications could extended additional dimension. indeed predictions rtds already enabled dynamic algorithm portfolios adaptive restart strategies predictions runtime parallelized algorithms many applications possible given effective methods prediction. advance underlying foundation applications paper focus better methods predicting rtds. speciﬁcally contributions follows propose principled pipeline build predictors problem instances based observed runtimes training instances. compare different ways predicting rtds demonstrate neural networks jointly predict parameters various parametric rtds yielding predictions superior previous approaches predict rtd’s parameters independently. many state-of-the-art algorithms solving hard combinatorial problems include elements stochasticity lead high variations runtime even ﬁxed problem instance across runs different pseudo-random number seeds. knowledge runtime distributions algorithms given problem instances exploited various meta-algorithmic procedures algorithm selection portfolios randomized restarts. previous work shown machine learning used individually predict mean median variance rtds. establish state-of-the-art predicting rtds demonstrate parameters learned jointly neural networks well directly optimizing likelihood given runtime observations. empirical study involving four algorithms solving planning show neural networks predict true rtds unseen instances better previous methods. exemplary application predictions show models also yield good predictions running algorithms parallel. algorithms solving hard combinatorial problems often rely random choices decisions improve performances. example randomization helps escape local optima enforces stronger exploration diversiﬁes search strategy relying heuristic information. particular local search algorithms randomized structured search also beneﬁt randomization runtimes randomized algorithms hard combinatorial problems well-known vary substantially often orders magnitude even running algorithm multiple times instance hence central object interest analysis randomized algorithm instance runtime distribution contrast single scalar deterministic algorithms. knowing rtds important many practical applications computing optimal restart strategies optimal algorithm portfolios speedups obtained executing multiple independent runs randomized algorithms ﬁnally discuss different ways predicting rtds including approach jointly learning parameters using nns. considered parametric continuous probability distributions widely studied describe rtds combinatorial problem solvers baseline consider normal distribution widespread throughout sciences; assumptions gaussian observation noise also underlie many machine learning methods gaussian processes. since runtimes hard combinatorial solvers often vary exponential scale much better empirical rtds typically achieved lognormal distribution; distribution attained logarithm runtimes gaussiandistributed shown empirical rtds well previous work another popular parametric family literature rtds exponential distribution tends describe rtds many well-behaved stochastic local search algorithms well unique family property probability ﬁnding solution next time interval remains constant time. distribution like lognormal distribution therefore model relatively long rich history predicting algorithm runtimes mentioned introduction focuses predicting mean runtimes exceptions. hutter predicted single distribution parameter exponential arbelaez truchet o’sullivan predicted parameters log-normal shifted exponential rtds independent models. contrast jointly predict multiple parameters work closely related gagliolo schmidhuber proposed learn distribution time left algorithm solves problem based features describing algorithm’s current state problem solved; used predictions dynamically assign time slots algorithms. contrast predict rtds unseen problem instances. ﬁnal related ﬁeld study predicting distributions uses non-parametric estimators. recently eggensperger used quantile regression forests non-parametric model enables sampling predicted runtime distributions. also recently ambrogioni showed used nonparametric estimator arbitrary conditional distributions. contrast that focus parametric approach using predict distribution parameters non-gaussian distributed target values. existing methods predicting runtime unseen instances base predictions instance features numerically characterize problem instances. particular context algorithm selection instance features proposed many domains hard-combinatorial problems propositional satisﬁability planning mixed integer programming answer programming avoid manual step feature construction loreggia proposed directly text format instance input neural network obtain numerical representation instance. since approach performed worse manually constructed features work traditional features framework work type features. since instance results measure quality parametric family rtds given instance averaging negative log-likelihoods instances. obtain comparable likelihoods across instances normalize multiplying likelihoods maximal observed runtime instance alternative goodness statistical test kolmogorov-smirnov test. ks-statistic based maximal distance empirical distribution cumulative distribution function reference distribution. aggregate test results across instances count number times ks-test rejected null-hypothesis measured drawn reference rtd. selected parametric family distributions last part pipeline predictor efﬁciently obtain rtds instances. formally problem predictive model maps instance features parameters selected family. following brieﬂy discuss traditional regression models used problem optimizes wrong loss function. show obtain better predictions neural networks introduce practical distnet task. when done easy instances weighted heavily rtds look differ scale factor instance times harder easier instance times larger normalization removes bias towards ﬁtting easy instances better. empirically studying variety alternative parametric families also found inverse gaussian distribution tends rtds well. like lognormal exponential distribution ﬂexible distribution also model long tails well. setting parameter close inﬁnity also made resemble normal distribution. measure well parametric distribution parameters empirical runtime observations likelihood parameters given observations equal probability observations distribution parameters consequently estimating parameters given empirical maximum-likelihood numerical reasons common machine learning negative log-likelihood loss function minimized straightforward approach predicting parametric rtds based standard regression models rtd’s parameters training instance train regression model data points θπ∈πtrain directly maps instance features parameters. approach used based gaussian processes random forests variants extend approaches problem predicting multiple parameters rtds governed parameters ﬁtting independent regression models ﬁtting multi-output model outputs. since random forests shown perform well standard runtime prediction tasks experimented variant also variant using multi-output random forest implementation scikit-learn however note multi-output variant although principle powerful independent models optimize loss function equation care about. also note variants require ﬁtting rtds training instance making approach inapplicable e.g. access million instances. show neural networks used solve problems. neural networks recently shown achieve stateof-the-art performance many supervised machine learning problems large data sets became available e.g. image classiﬁcation segmentation speech processing natural language processing. thorough introduction refer interested reader goodfellow bengio courville here apply prediction. background neural networks. approximate arbitrary functions deﬁning mapping weights learnt training approximate function. work fully-connected feedforward network described acyclic graph connects nonlinear transformations chain layer layer. example hidden layers predicts input written denoting trainable network weights nonlinear transformation applied weighted outputs j-th layer. last activation function special case constrain outputs positive. neural networks predicting rtds. figure shows general architecture joint prediction multiple parameters. input neuron instance feature output neuron distribution parameter assume know best-ﬁtting distribution family previous step pipeline. contrast train networks directly minimize negative log-likelihood predicted distribution parameters given observed runtimes. formally given observed runtimes instance features minimize following loss function end-to-end fashion here denotes values distribution parameters obtained output layer given instantiation nn’s weights. optimization process targets exactly loss function interest allows effectively predict distribution parameters jointly. since predicted combinations judged directly resulting negative log-likelihood optimization process driven combinations work well together above). end-to-end optimization process also general removes need ﬁtting training instance thereby enables using arbitrary algorithm performance data ﬁtting model. network architecture training procedure hyperparameter settings. call resulting neural network following considerations distnet. preprocess runtime data t{...k} normalized feature mean standard deviation weights neural network typically initialized random samples normal distribution mean standard deviation training distnet considered following aspects ﬁrst networks tended overﬁt training data training data small network large. therefore recommend sufﬁciently large number instances runtime observations chose fairly small neural network hidden layers neurons. believe better performance could achieved larger deeper networks training instances available. considered runtime observation individual data sample. hence increased number training samples number instances times number observations k—the training size random forest equals number instances. loss function large gradients slightly suboptimal parameters lead likelihoods close zero therefore used fairly small initial learning rate used gradient clipping exponentially decaying learning rate. experiments study following research questions parametric families considered best describe empirical rtds algorithms instances? automated machine learning tools autonet help process autonet support distributions; contribute feature future. experimental setup focus well-studied algorithms evaluated different problem instances different domains saps-cv-var based dynamic local search solver saps instances randomly generated varying clausevariable ratio clasp-k based tree-based cdcl solver clasp experiments clasp randomized using randomly-selected split variable probability instances randomly generated unsatisﬁable instances. sizes instance sets shown table gather training data algorithm different seeds instance. runs performed compute cluster nodes equipped intel xeon memory running centos used opensource neural network library keras neural networks scikit-learn implementation scipy ﬁtting distributions. figure empirical cdfs observed running default conﬁguration algorithms times. blue indicates satisﬁable instances indicates unsatisﬁable instances. table results ﬁtted distributions average negative log-likelihood across instances percentage rejected distributions according ks-test scenario report result top- distributions ranked negative log-likelihood. metrics smaller numbers better. different algorithms’ rtds show different characteristics. saps-cv-var rtds similar short right tails. lpg-zenotravel rtds long right tail short left tail. contrast rtds probsatsat long right tail left tail. clasp-k rtds nearly symmetric. table shows quantitative evaluation different families considered overall ﬁtted distributions closely resembled true empirical rtds rejection rate test best ﬁtting distribution hence instances best distributions statistically signiﬁcantly different observed ones. surprisingly different parametric families performed best scenario showing overall good performance followed performed worst cases best probsat-sat. lpg-zenotravel ks-test showed statistically signiﬁcant differences best ﬁtting distribution since cdfs start small shift therefore cannot approximated perfectly distributions. still distributions achieved good negative log-likelihood values. clasp-k distributions except achieved good results. predicting rtds next turn empirical evaluation models comparing predictive negative log-likelihood obtained distnet multi-output random forest ﬁtting table averaged negative log-likelihood achieved predicting rtds unseen instances. report average across -fold cross-validation ﬁrst line dataset performance training data second line performance test data. scenario picked best-ﬁtting families highlight best predictions. multiple independent distribution parameter distnet used settings described table limit training take epochs whichever less. gold standard report negative log-likelihood obtained maximum likelihood empirical rtd. table shows negative log-likelihood achieved using -fold cross-validation i.e. split instances disjoint sets train models subset measure test performance left subset. report average performance train test data across splits best ﬁtting distributions according table overall results show possible predict parameters unseen instances distnet performed best. three four scenarios models achieved negative log-likelihood close gold standard ﬁtting rtds observed data. also scenarios distribution families similarly easy predict figure estimated runtime running algorithms parallel. compare true runtime based collected data estimated runtime obtained ﬁtted distributions based distnet. models. rf-based model observed slight overﬁtting saps-cv-var lpg-zenotravel clasp-k. distnet observed smallest data sets probsat-sat clasp-k. saps-cv-var multi-output yielded poor predictions distribution; believe fact ranges distribution parameters obtained ﬁtting distributions vary greatly. affects loss function uses data happen ﬁtting individually underlines importance optimizing right loss function distnet. probsat-sat models basically needed single parameter single parameter although parameters important probsat-sat since almost constant. thus case difference irfs noise distnet could proﬁt joint model. overall distnet clearly yielded robust results. achieved best test predictions cases sometimes substantial improvements baselines performed slightly worse smallest probsatsat benchmark distnet parallel prediction finally evaluate distnet typical application predicting estimated runtime simple parallelization scheme. setting algorithm trivially parallelized running multiple independent copies different random seeds parallel. randomization runs solve instance faster others point runs terminated. knowing advance many here approximate runtime parallel runs drawing samples sequential algorithm’s times obtain expected parallel runtime taking average best samples across simulated parallel runs. simulate running copies algorithm parallel compare expected parallel runtime based empirical rtds based predicted rtds distnet. aggregate results used average estimated runtime across problem instances figure shows expected runtime depending number parallel runs. probsat-sat clasp-k obtained estimated runtimes close true distribution. lpg-zenotravel ks-test already indicated perfectly-ﬁtting distribution hence estimated runtimes parallel algorithm runs obtained ﬁtting distributions line true parallel runtimes. nevertheless distnet perfectly matched gold standard ﬁtted distribution. finally saps-cv-var distnet overestimated left tail slightly causing also underestimate runtime parallel portfolio. nevertheless overall trend predicted correctly. overall experiments conclude predictions obtained distnet suitable estimate runtime parallel algorithm runs. paper showed neural networks used jointly learn distribution parameters predict runtime distributions contrast previous random forest models train model individual runtime observations removing need rtds training instances. importantly neural network distnet directly optimizes loss function interest end-to-end fashion obtains better predictive performance previously-used random forests models directly optimize loss function. extend work increase amount available training data would also consider censored observations loss function neural network proposed gagliolo schmidhuber furthermore studied problems either satisﬁable unsatisﬁable instances. practice face instances types follow different family. expect using mixture models learn different distribution families could alleviate problem. paper brieﬂy illustrated suitability predictions simple application note methodology allows better predictions general therefore pave improving many exciting applications currently rely mean predictions only e.g. algorithm selection modelbased algorithm conﬁguration authors acknowledge funding emmy noether grant eggensperger additionally acknowledges funding state graduate funding program baden-w¨urttemberg. furthermore authors acknowledge support state baden-w¨urttemberg bwhpc grant inst fugg.", "year": 2017}