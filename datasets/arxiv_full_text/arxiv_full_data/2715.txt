{"title": "Multi-Generator Generative Adversarial Nets", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We propose a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapse and delivering state-of-the-art results. A minimax formulation is able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture GAN (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators' distributions and the empirical data distribution is minimal, whilst the JSD among generators' distributions is maximal, hence effectively avoiding the mode collapse. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by generators.", "text": "propose paper approach train generative adversarial nets mixture generators overcome mode collapsing problem. main intuition employ multiple generators instead using single original gan. idea simple proven extremely effective covering diverse data modes easily overcoming mode collapsing problem delivering state-of-the-art results. minimax formulation able establish among classiﬁer discriminator generators similar spirit gan. generators create samples intended come distribution training data whilst discriminator determines whether samples true data generated generators classiﬁer speciﬁes generator sample comes from. distinguishing feature internal samples created multiple generators randomly selected ﬁnal output similar mechanism probabilistic mixture model. term method mixture generative adversarial nets develop theoretical analysis prove that equilibrium jensen-shannon divergence mixture generators’ distributions empirical data distribution minimal whilst among generators’ distributions maximal hence effectively avoiding mode collapsing problem. utilizing parameter sharing proposed model adds minimal computational cost standard thus also efﬁciently scale large-scale datasets. conduct extensive experiments synthetic data natural image databases demonstrate superior performance mgan achieving state-of-the-art inception scores latest baselines generating diverse appealing recognizable objects different resolutions specializing capturing different types objects generators. generative adversarial nets recent novel class deep generative models successfully applied large variety applications image video generation image inpainting semantic segmentation image-to-image translation text-to-image synthesis name game theory metaphor model consists discriminator generator playing two-player minimax game wherein generator aims generate samples resemble training data whilst discriminator tries distinguish narrated training however challenging easily trapped mode collapsing problem generator concentrates producing samples lying modes instead whole data space include modifying discriminator’s objective modifying generator’s objective employing additional discriminators yield useful gradient signals generators common theme variants generators shown equilibrium able recover data distribution convergence remains elusive practice. experiments conducted datasets narrow-domain datasets lsun celeba knowledge warde-farley bengio nguyen perform quantitative evaluation models trained much diverse datasets stl- imagenet given current limitations training single-generator gans recent attempts made following multi-generator approach. tolstikhin apply boosting techniques train mixture generators sequentially training adding generators mixture. however sequentially training many generators computational expensive. moreover approach built implicit assumption single-generator generate good images modes reweighing training data incrementally training generators result mixture covers whole data space. assumption true practice since current single-generator gans trained diverse datasets imagenet tend generate images unrecognizable objects. arora train mixture generators discriminators optimize minimax game reward function weighted average reward function pair generator discriminator. model computationally expensive lacks mechanism enforce divergence among generators. ghosh train many generators using multi-class discriminator that addition detecting whether data sample fake predicts generator produces sample. objective function model punishes generators generating samples detected fake directly encourage generators specialize generating different types data. propose paper novel approach train mixture generators. unlike aforementioned multi-generator gans proposed model simultaneously trains generators objective mixture induced distributions would approximate data distribution whilst encouraging specialize different data modes. result novel adversarial architecture formulated minimax game among three parties classiﬁer discriminator generators. generators create samples intended come distribution training data whilst discriminator determines whether samples true data generated generators classiﬁer speciﬁes generator sample comes from. term proposed model mixture generative adversarial nets provide analysis model optimized towards minimizing jensen-shannon divergence mixture distributions induced generators data distribution maximizing among generators. empirically proposed model trained efﬁciently utilizing parameter sharing among generators classiﬁer discriminator. addition simultaneously training many generators enforcing among generators helps focus modes data space learn better. trained cifar- generator learned specialize generating samples different class horse ship bird airplane. overall models trained cifar- stl- imagenet datasets successfully generated diverse recognizable objects achieved state-of-the-art inception scores model trained cifar- even outperformed gans trained semi-supervised fashion short main contributions novel adversarial model efﬁciently train mixture generators enforcing among generators; theoretical analysis objective function optimized towards minimizing mixture generators’ distributions real data distribution maximizing among generators; comprehensive evaluation performance method synthetic real-world large-scale datasets diverse natural scenes. drawn data distribution pdata drawn prior distribution mapping induces generator distribution pmodel data space. alternatively optimizes using stochastic gradient-based learning. result optimization order reversed causing minimax formulation become maximin. therefore incentivized every single likely classiﬁed true data leading mode collapsing problem. another commonly asserted cause generating less diverse samples that optimal point minimizing equivalent minimizing data model distributions empirically proven prefer generate samples around modes whilst ignoring modes present main contribution novel approach effectively tackle mode collapse gan. idea mixture many distributions rather single standard approximate data distribution simultaneously enlarge divergence distributions cover different data modes. analogy game among generators discriminator classiﬁer formulated. generator maps thus inducing single distribution pgk; generators altogether induce mixture distributions namely pmodel data space. index drawn multinomial distribution mult coefﬁcients mixture; sample used output. here predeﬁned instead learning. discriminator aims distinguish sample training samples. classiﬁer performs multi-class classiﬁcation classify samples labeled indices corresponding generators. term whole process model mixture generative adversarial nets fig. illustrates general architecture proposed mgan components parameterized neural networks. parameters together except input layer whilst share parameters except output layer. parameter sharing scheme enables networks leverage common information features low-level layers close data layer hence helps train model effectively. addition also minimizes number parameters adds minimal complexity standard thus whole process still efﬁcient. formally play following multi-player minimax optimization game probability generated diversity hyper-parameter. ﬁrst terms show interaction generators discriminator standard gan. last term recognized standard softmax loss multi-classiﬁcation setting aims maximize entropy classiﬁer. represents interaction generators classiﬁer encourages generator produce data separable produced generators. strength interaction controlled similar proposed network trained alternatively updating refer appendix pseudo-code algorithms parameter learning proposed mgan. assuming enough capacity show equilibrium point minimax problem mixture induced data distribution minimal i.e. pdata pmodel among generators maximal i.e. arbitrary generators almost never produce data. follows present mathematical statement sketch proofs. refer appendix full derivations. proposition ﬁxed generators mixture weights optimal solution theorem shows progressing towards equilibrium equivalently minimizing pmodel) maximizing jsdπ next theorem further clarify equilibrium point speciﬁc case wherein data distribution form thm. explicitly offers optimal solution speciﬁc case wherein real data generated mixture distribution whose components well-separated. reveals mixture components well-separated setting number generators number mixtures data maximizing divergence generated components exactly recover mixture components using generated components hence strongly supporting motivation developing mgan. practice parameterized neural networks optimized parameter space rather function space. generators share objective function efﬁciently update weights using backpropagation passes. empirically minimizes objective value parameter w.r.t thm. simplify computational graph assume generator sampled number times minibatch. addition adopt non-saturating heuristic proposed train maximizing instead minimizing recent attempts address mode collapse modifying discriminator include minibatch discrimination unrolled denoising feature matching idea minibatch discrimination allow discriminator detect samples noticeably similar generated samples. although method generate visually appealing samples computationally expensive thus normally used last hidden layer discriminator. unrolled improves learning unrolling computational graph include additional optimization steps discriminator. could effectively reduce mode collapsing problem unrolling step expensive rendering unscalable large-scale datasets. augments objective function generator denoising autoencoder minimizes reconstruction error activations penultimate layer discriminator. idea gradient signals guide generator towards producing samples whose activations close manifold real data activations. surprisingly effective avoiding mode collapse involvement deep adds considerable computational cost model. alternative approach train additional discriminators. dgan employs discriminators minimize kullback-leibler reverse divergences thus placing fair distribution across data modes. method avoid mode collapsing problem certain extent still could outperform dfm. another work uses many discriminators boost learning generator authors state method robust mode collapse provide experimental results support claim. another direction train multiple generators. so-called mix+gan related model mixture idea different. based min-max theorem mix+gan trains mixture multiple generators discriminators different parameters play mixed strategies min-max game. total reward game computed weighted averaging rewards pairs generator discriminator. lack parameter sharing renders method computationally expensive train. moreover mechanism enforce divergence among generators ours. attempts made train mixture gans similar spirit boosting algorithms. wang propose additive procedure incrementally train gans subset training data badly modeled previous generators. discriminator expected classify samples subset real high conﬁdence i.e. high subset chosen include larger predeﬁned threshold. tolstikhin however show heuristic fails address mode collapsing problem. thus propose adagan introduce robust reweighing scheme prepare training data next gan. adagan boosting-inspired gans general based assumption singlegenerator learn generate impressive images modes dogs cats fails cover modes giraffe. therefore removing images dogs cats training data train next create better mixture. assumption true practice current single-generator gans trained diverse data sets imagenet tend generate images unrecognizable objects. closely related mad-gan trains many generators uses multi-class classiﬁer discriminator. work strategies proposed address mode collapse augmenting generator’s objective function user-deﬁned similarity based function encourage different generators generate diverse samples modifying discriminator’s objective functions push different generators towards different identiﬁable modes separating samples generator. approach different that rather modifying discriminator additional classiﬁer discriminates samples produced generator others multi-class classiﬁcation setting. nicely results optimization problem maximizes among generators thus naturally enforcing generate diverse samples effectively avoiding mode collapse. section conduct experiments synthetic data real-world large-scale datasets. using synthetic data visualize examine evaluate learning behaviors proposed mgan whilst using real-world datasets quantitatively demonstrate efﬁcacy scalability addressing mode collapse much larger wider data space. fair comparison experimental settings identical previous work hence quote results latest state-of-the-art gan-based models compare ours. tensorflow implement model release code publication. experiments shared parameters among generators layers except weights input ﬁrst hidden layer; shared parameters discriminator classiﬁer layers except weights penultimate layer output; adam optimizer learning rate ﬁrst-order momentum minibatch size samples training discriminators; relu activations generators; leaky relu slope discriminator classiﬁer; weights randomly initialized gaussian distribution zero biases. refer appendix detailed model architectures additional experimental results. ﬁrst experiment following reuse experimental design proposed investigate well mgan explore capture multiple data modes. training data sampled mixture isotropic gaussian distributions covariance matrix means arranged circle zero centroid radius purpose using small variance create density regions separate modes. fig. shows evolution samples generated model baselines time. seen regular generates data collapsing single mode hovering around valid modes data distribution thus reﬂecting mode collapse expected. time unrolledgan dgan mgan distribute data around mixture components hence demonstrating abilities successfully learn multimodal data case. proposed model however converges much faster since successfully explores neatly covers modes early step whilst baselines produce samples cycling around till last steps. mgan captures data modes precisely unrolledgan dgan since mode unrolledgan generates data concentrate several points around mode’s centroid thus seems produce fewer samples whose samples fairly spread entire mode exceed boundary whilst dgan still generates many points scattered adjacent modes. next quantitatively compare quality generated data. since know true distribution pdata case employ measures namely symmetric kullback-leibler divergence wasserstein distance. measures compute distance normalized histograms points generated model true pdata. figs. clearly demonstrate superiority approach unrolledgan dgan w.r.t distances notably wasserstein distances dgan’s true distribution almost reduce zero time symmetric metric signiﬁcantly better dgan. ﬁgures also show stability mgan dgan training much less ﬂuctuating compared unrolledgan lastly perform experiments different numbers generators. mgan models generators successfully explore modes models generators generate fewer points scattered adjacent modes. also examine behavior diversity coefﬁcient training -generator model different values without force generated samples cluster around mode. force weak generated data cluster near different modes. force strong causes generators collapse generating increasingly tight clusters. generators successfully cover modes. please refer appendix experimental details. datasets. widely-adopted datasets cifar- stl- imagenet cifar- contains training images classes airplane automobile bird deer frog horse ship truck. stl- subsampled imagenet diverse dataset cifar- containing images. imagenet presents largest diverse consisting million images classes. order facilitate fair comparison baselines follow procedure resize stl- imagenet images respectively. evaluation protocols. quantitative evaluation adopt inception score proposed computes conditional label distribution image estimated reference inception model metric rewards good varied samples found well-correlated human judgment code provided compute inception scores partitions randomly generated samples. qualitative demonstration image quality obtained proposed model show samples generated mixture well samples produced generator. samples randomly drawn rather cherry-picked. model architectures. generator discriminator architectures closely follow dcgan’s design difference apply batch normalization layers networks except output layer. regarding classiﬁer empirically proposed mgan achieves best performance classiﬁer shares parameters layers discriminator except output layer. reason parameter sharing scheme would allow classiﬁer discriminator leverage common features representations learned every layer thus helps improve speed training progress. parameters tied model learns slowly eventually yields lower performance. training observe percentage active neurons chronically declined possible cause batch normalization center gradually shifted negative range thus deactivating relu units generator networks. ad-hoc solution problem offset zero layers generator networks. rationale feature relu gates open highest inputs minibatch across locations generators close rest. also experiment activation functions generator networks. first leaky relu obtain similar results using relu. maxout units achieves good inception scores generates unrecognizable samples. finally selu fail train model. hyperparameters. three hyperparameters model number generators coefﬁcient controlling diversity minibatch size. minibatch size generator total number samples training generators train models generators generators corresponding minibatch sizes each models generators performs better. imagenet additional setting generators minibatch size each. batch samples small updating sufﬁcient statistics batch-norm layer thus drop batch-norm input layer generator. -generator model however obtain considerably better results -generator one. therefore follows report results models generators. diversity coefﬁcient observe signiﬁcant difference inception scores varying value quality generated images declines high. generated samples generator vary vary less become less realistic high. reasonable range ﬁnally cifar- imagenet stl-. inception results. report inception scores obtained mgan baselines tab. worthy note models trained completely unsupervised manner without label information included fair comparison; dcgan’s dgan’s results available models trained resolution. overall proposed model outperforms baselines large margins achieves state-of-the-art performance datasets. moreover would highlight mgan obtains score cifar- even better models trained labels improved ac-gan addition train model original resolution stl- achieve score .±.. suggests mgan successfully trained higher resolution images achieve higher inception score. image generation. next present samples randomly generated proposed model trained datasets qualitative assessment. fig. shows cifar- images containing wide range objects airplanes cars trucks ships birds horses dogs. similarly generated images fig. include cars ships airplanes many types animals wider range different themes underwater mountain forest. images generated imagenet diverse recognizable objects lady birds human living room slippers name few. fig. shows several cherry-picked stl- images demonstrate mgan capable generating visually appealing images complicated details. however many samples still incomplete unrealistic shown fig. leaving plenty room improvement. finally investigate samples generated generator well evolution samples numbers training epochs. fig. shows images generated generators mgan trained cifar- epoch training. samples correspond different generator. generators start specialize generating different types objects early epoch become consistent generator ﬂying objects generator full pictures cats dogs generator portraits cats dogs generator ships generator trucks generator horses. generator seems generate images frog animals bush. generator however collapses epoch possible explanation behavior images different object classes tend different themes. lastly wang noticed causes non-convergence gans generators discriminators constantly vary; generators consecutive epochs training generate signiﬁcantly different images. experiment demonstrates effect force preventing generators moving around data space. presented novel adversarial model address mode collapse gans. idea approximate data distribution using mixture multiple distributions wherein distribution captures subset data modes separately others. achieve goal propose minimax game discriminator classiﬁer many generators formulate optimization problem minimizes pdata pmodel i.e. mixture distributions induced generators whilst maximizes among generator distributions. helps model generate diverse images better cover data modes thus effectively avoids mode collapse. term proposed model mixture generative adversarial network mgan efﬁciently trained sharing parameters discriminator classiﬁer among generators thus model scalable evaluated real-world largescale datasets. comprehensive experiments synthetic data cifar- stl- imagenet databases demonstrate following capabilities model achieving state-of-the-art inception scores; generating diverse appealing recognizable objects different resolutions; specializing capturing different types objects generators. references mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. adam coates andrew honglak lee. analysis single-layer networks unsupervised feature learning. proceedings fourteenth international conference artiﬁcial intelligence statistics vincent dumoulin ishmael belghazi poole alex lamb martin arjovsky olivier mastropietro aaron courville. adversarially learned inference. arxiv preprint arxiv. goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems vinod nair geoffrey hinton. rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet large scale visual recognition challenge. international journal computer vision salimans goodfellow wojciech zaremba vicki cheung alec radford chen. improved techniques training gans. advances neural information processing systems christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition ilya tolstikhin sylvain gelly olivier bousquet carl-johann simon-gabriel bernhard sch¨olkopf. adagan boosting generative models. arxiv preprint arxiv. fisher seff yinda zhang shuran song thomas funkhouser jianxiong xiao. lsun construction large-scale image dataset using deep learning humans loop. arxiv preprint arxiv. proposed method generators deep convolutional neural networks parameterized networks share parameters layers except input layers. input layer generator parameterized mapping fθgk maps sampled noise ﬁrst hidden layer activation shared layers parameterized mapping maps ﬁrst hidden layer generated data. pseudo-code sampling mixture described alg. classiﬁer classiﬁer also deep convolutional neural networks parameterized θcd. share parameters layers except last layer. pseudo-code alternatively learning using stochastic gradient descend described alg. sample minibatch generated data points current mixture. update ∇θcd sample minibatch generated data points current mixture. update mixture generators ascending along gradient ∇θglg. proof. optimal proved prop. section shows similar proof optimal assuming optimized functional space calculate functional derivatives with respect true data sampled mixture gaussian distributions covariance matrix means arranged circle zero centroid radius simple architecture generators fully connected hidden layers classiﬁer discriminator shared hidden layer. hidden layers contain number relu units. input layer generators contains noise units sampled isotropic multivariate gaussian distribution batch normalization layer. refer tab. speciﬁcations network hyperparameters. shared short parameter sharing among generators classiﬁer discriminator. feature maps last layer means separate fully connected layers applied penultimate layer outputs logits another outputs logit. effect number generators generated samples. fig. shows samples produced mgans different numbers generators trained synthetic data epochs. model generator behaves similarly standard expected. models generators successfully cover modes ones generators draw fewer points scattered adjacent modes. finally model generators also covers modes wherein generators share mode generator hovering around another mode. operation fully connected fully connected fully connected fully connected fully connected number generators batch size real data batch size generator number iterations leaky relu slope learning rate regularization constants weight bias initialization effect generated samples. examine behavior diversity coefﬁcient fig. compares samples produced mgan generators epochs training different values without force generated samples cluster around mode. generated data clusters near different modes. force strong causes generators collapse generating increasingly tight clusters. generators successfully cover modes. fixing batch normalization center. training observe percentage active neurons deﬁne relu units positive activation least samples minibatch chronically declined. fig. shows percentage active neurons generators trained cifar- declined consistently layer layer therefore quality generated images reaching peak level started declining. possible cause batch normalization center gradually shifted negative range shown histogram fig. also observe problem dcgan. ad-hoc solution problem i.e. offset zero layers generator networks. rationale feature relu gates open highest inputs minibatch across locations generators close rest. therefore batch normalization keep relu units alive even inputs otherwise negative introduces form competition encourages generators specialize different features. measure signiﬁcantly improves performance totally solve dying relus problem. late training input generators’ relu units became right-skewed causing relu gates open less less often. figure samples generated mgan models trained synthetic data different values diversity coefﬁcient generated data blue data samples gaussians red. experiment settings. experiments three large-scale natural scene datasets closely followed network architecture training procedure dcgan. speciﬁcations models trained cifar- stl- stl- imagenet datasets described tabs. respectively. short batch normalization center short whether learn batch normalization’s center zero. shared short parameter sharing among generators classiﬁer discriminator. feature maps last layer means separate fully connected layers applied penultimate layer outputs logits another outputs logit. finally figs. respectively enlarged version figs. main manuscript. uniform fully connected transposed convolution transposed convolution transposed convolution convolution convolution convolution fully connected number generators batch size real data batch size generator number iterations leaky relu slope learning rate regularization constants weight bias initialization uniform fully connected transposed convolution transposed convolution transposed convolution transposed convolution convolution convolution convolution convolution fully connected number generators batch size real data batch size generator number iterations leaky relu slope learning rate regularization constants weight bias initialization uniform fully connected transposed convolution transposed convolution transposed convolution transposed convolution transposed convolution convolution convolution convolution convolution convolution fully connected number generators batch size real data batch size generator number iterations leaky relu slope learning rate regularization constants weight bias initialization uniform fully connected transposed convolution transposed convolution transposed convolution convolution convolution convolution fully connected number generators batch size real data batch size generator number iterations leaky relu slope learning rate regularization constants weight bias initialization", "year": 2017}