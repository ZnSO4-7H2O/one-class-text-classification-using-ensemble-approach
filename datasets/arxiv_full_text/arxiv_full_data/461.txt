{"title": "Online Segment to Segment Neural Transduction", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "We introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training, and during decoding beam search is employed to find the best alignment path together with the predicted output sequence. Our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixed-length hidden states before producing any output. It is different from previous attentive models in that, instead of treating the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation. Experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder-decoders.", "text": "encoder-decoder maparadigm proved successful sequence chine output architecture appealing makes possible tackle problem sequence-to-sequence mapping training large neural network end-to-end fashion. however difﬁcult ﬁxed-length vector memorize necessary information input sequence especially long sequences. often large encoding needs employed order capture longest sequences invariably wastes capacity computation short sequences. attention mechanism bahdanau goes address issue still requires full input seen output produced. paper propose architecture tackle limitations vanilla encoder-decoder model segment transduclearns generate tion model align simultaneously. model inspired word alignment model proposed statistical machine translation impose monotone restriction alignments incorporate recurrent dependencies input enable rich locally non-monotone alignments captured. similar sequence transduction model graves propose alignment distributions parameterised separately making introduce online neural sequence sequence model learns alternate encoding decoding segments input read. independently tracking encoding decoding representations algorithm permits exact polynomial marginalization latent segmentation training decoding beam search employed best alignment path together predicted output sequence. model tackles bottleneck vanilla encoder-decoders read memorize entire input sequence ﬁxedlength hidden states producing outdifferent previous attentive put. models that instead treating attention weights output deterministic function model assigns attention weights sequential latent variable marginalized permits online generation. experiments abstractive sentence summarization morphological inﬂection show signiﬁcant performance gains baseline encoder-decoders. problem mapping sequence animportance challenge natural language processing. common applications include machine translation abstractive sentence summarisation. traditionally type problem tackled combination hand-crafted features alignment models segmentation heuristics language models tuned separately. outperforms attention-based benchmark. qualitative analysis shows alignments found model highly intuitive demonstrates model learns read ahead required number tokens producing output. model introduces latent segmentation determines correspondences tokens input sequence output sequence. aligned hidden states encoder decoder used predict next output token calculate transition probability alignment. carefully design input output rnns independently update respective hidden states. enables derive exact dynamic programme marginalize hidden segmentation training efﬁcient beam search generate online best alignment path together output sequence during decoding. unlike previous recurrent segmentation models capture dependencies input segmentation model able capture unbounded dependencies input output sequences still permitting polynomial inference. attentive models treat attention weights output deterministic function model assigns attention weights sequential latent variable marginalized out. model general could incorporated rnnbased encoder-decoder architecture neural turing machines memory networks stack-based networks enabling models process data online. conduct experiments different transduction tasks abstractive sentence summarisation morphological inﬂection generation evaluate proposed algorithms online setting input encoded unidirectional lstm whole input available encoded bidirectional network. experimental results demonstrate effectiveness ssnt consistently output performs baseline encoder-decoder approach requiring signiﬁcantly smaller hidden layers thus showing segmentation model able learn break large transduction task series smaller encodings decodings. bidirectional encodings used segmentation model figure illustrates model graphically. path left node right-most column graph corresponds alignment. constrain alignments monotone i.e. forward downward transitions permitted point grid. constraint enables model learn perform online generation. additionally model learns align input output segments means learn local reorderings memorizing phrases. another possible constraint alignments would ensure entire input sequence consumed last output word emitted i.e. valid alignment paths bottom right corner grid. however enforce constraint setup. probability contributed alignment obtained accumulating probability word predictions point path transition probability points. transition probabilities word output probabilities modeled neural networks described detail following sub-sections. input sentence encoded recurrent neural network particular lstm encoder either unidirectional bidirectional lstm. unidirectional encoder used model able read input generate output symbols online. hidden state vectors computed figure illustrates model structure. note hidden states input output decoders kept independent permit tractable inference output distributions conditionally dependent both. alignments constrained monotone treat transition timestep sequence shift emit operations. specifically input position decision shift emit made model; operation emit next output word generated; otherwise model shift next input word. multinomial distribution alternative parameterising alignments shift/emit parameterisation place upper limit jump size multinomial distribution would biases model towards shorter jump sizes multinomial model would learn. describe methods modelling alignment transition probability. ﬁrst approach independent input output words. parameterise alignment distribution terms shift emit operations geometric distribution explicitly calculate every conditional probability instead approach problem using dynamicprogramming algorithm similar forwardbackward algorithm hmms figure structure model. denote input output sequences respectively. points e.g. grid represent alignment column concatenation hidden states used predict search algorithm based dynamic programming main idea create path probability matrix cell recursively taking probable path could lead cell. present greedy search algorithm algorithm also implemented beam search tracks best partial sequences position notation refers backpointers stores words predicted denotes output vocabulary jmax maximum length output sequences model allowed predict. evaluate effectiveness model representative natural language processing tasks sentence compression morphological inﬂection. primary evaluation assess whether proposed architecture able outperform baseline encoder-decoder model overcoming encoding bottleneck. benchmark results attention model order determine whether alternative alignment strategy able provide similar beneﬁts processing input online. sentence summarisation task generating condensed version sentence preserving meaning. abstractive sentence summarisation summaries generated given vocabulary without constraint copying words input sentence. rush compiled data task annotated gigaword data sentence-summary pairs obtained pairing headline article ﬁrst sentence. rush splits .m/k/k training validation testing. previous work dataset rush proposed attention-based model feed-forward neural networks chopra proposed attention-based recurrent encoder-decoder similar baselines. table rouge scores sentence summarisation test set. seqseq refers vanilla encoder-decoder attention denotes attentionbased model. ssnt denotes model alignment transition probability modelled geometric distribution. ssnt+ refers model transition probability modelled using neural networks. preﬁxes unibidenote using unidirectional bidirectional encoder lstms respectively. ﬁltered pairs validation sample pairs training. training smaller used previous work serves purpose evaluating algorithm sequence sequence attention-based approaches identical data conditions. following previous work report results randomly sampled test sentence-summary pairs. quality generated summaries evaluated three versions rouge different match lengths namely rouge- rouge- rouge-l training adam optimization initial learning rate mini-batch size number hidden units model baseline models dropout applied input lstms. hyperparameters optimised grid search table displays rouge-f scores models test together baseline modincluding attention-based model. models achieve signiﬁcantly better results vanilla encoder-decoder outperform attention-based model. fact ssnt+ performs better line expectations neural network-parameterised alignment model expressive modelled geometric distribution. make comparison experimented different sizes hidden units adding layers baseline encoder-decoder. table lists conﬁgurations different models corresponding perplexities validation set. vanilla encoder-decoder tends better results adding hidden units stacking layers. limitation compressing information ﬁxed-size vector. larger vectors deeper structure order memorize information. contrast model well smaller networks. fact even layer hidden units model works much better vanilla encoder-decoder layers hidden units layer. morphological inﬂection generation task predicting inﬂected form given lexical item based morphological attribute. transformation base form inﬂected form usually includes concatenating preﬁx sufﬁx substituting characters. example inﬂected form german stem abgang abg¨angen experiments dataset faruqui dataset originally created durrett denero wiktionary german nouns german verbs spanish verbs finnish noun adjective finnish verbs expanded nicolai adding dutch verbs french verbs number inﬂection types language ranges number base forms i.e. number instances dataset ranges predeﬁned split test sets rest data training. model trained separately type inﬂection setting factored model described faruqui model trained predict character sequence inﬂected form given stem. output evaluated accuracies string matching. experiments task hidden units lstms apply dropout input output lstms. adam optimisation initial learning rate decoding beam search employed beam size unissnt+ bissnt+ vanilla encoder-decoder attention-based models. model best previous average result denoted adaptedseqseq also included comparison. bissnt+ model outperforms vanilla encoder-decoder large margin almost matches state-of-the-art result task. mentioned earlier characteristic datasets stems corresponding inﬂected forms mostly overlap. compare vanilla encoder-decoder model better copying ﬁnding correspondences preﬁx stem sufﬁx segments. results bissnt+ individual dataset. models nicolai models tackle task feature engineering. ftnd adapted vanilla encoderdecoder feeding i-th character encoded string extra input i-th position decoder. considered special case model forcing ﬁxed diagonal alignment input output sequences. model achieves comparable results models datasets. notably outperforms models finnish noun adjective verbs datasets whose stems inﬂected forms longest. figure presents visualisations segment alignments generated model sample instances tasks. model able learn correct correspondences segments input output sequences. instance alignment follows nearly diagonal path example figure input output sequences identical. figure learns preﬁx ‘ge’ start sequence replace ‘en’ copying ‘zock’. observe model robust long phrasal mappings. shown figure mapping ‘the wall street journal asia asian edition us-based business daily’ ‘wall street journal asia’ demonstrates model learns ignore phrasal modiﬁers containing additional information. also examples word reordering e.g. phrase ‘industrial production france’ reordered ‘france industrial output’ model’s predicted output. seminal work inspired alignment model proposed machine translation. contrast work predicting target word additionally condition previously generated words enabled recurrent neural models. means model also functions conditional language model. therefore applied directly traditional models combined language model noisy channel order effective. additionally instead training likely alignments iteration model trained direct gradient descent marginalizing alignments. latent variables employed neural network-based models sequence labelling tasks past. examples include connectionist temporal classiﬁcation speech recognition recent segmental applications handwriting recognition part-of-speech tagging. weighted ﬁnite-state transducers also augmented encode input sequences bidirectional lstms permitting exact inference possible output strings. models shown achieve appealing performance different applications common limitations terms modelling depossible pendencies labels. ctcs model explicit dependencies. srnns model shares property sequence transduction model graves able model unbounded dependencies output tokens output rnn. property makes possible apply model tasks like summarisation machine translation require tokens output sequence modelled highly dependently. graves models joint distribution outputs alignments inserting null symbols output sequence. training model uses dynamic programming marginalize permutations null symbols beam search employed decoding. contrast model deﬁnes separate latent alignment variable adds ﬂexibility alignment distribution deﬁned alignments constrained without redeﬁning dynamic program. addition marginalizing training decoding algorithm also makes dynamic programming allowing either beam small beam sizes. work also related attentionbased models ﬁrst introduced machine translation luong proposed alternative attention mechanisms global method attends words input sentence local points parts input words. another variation theme pointer networks outputs pointers elements variablelength input predicted attention distribution. jaitly propose online sequence sequence model attention conditions ﬁxed-sized blocks input sequence emits output tokens corresponding block. model trained alignment information generate supervised segmentations. although model shares idea joint training aligning attention-based models design fundamental differences advantages. attention-based models treat attention weights output deterministic function model attention weights correspond hidden variable marginalized using dynamic programming. further model’s inherent online nature permits ﬂexibility capacity chose much input encode decoding segment. proposed novel segment segment neural transduction model tackles limitations vanilla encoder-decoders read memorize entire input sequence ﬁxed-length context vector producing output. introducing latent segmentation determines correspondences tokens input output sequences model learns generate align jointly. training hidden alignment marginalized using dynamic programming decoding best alignment path generated alongside predicted output sequence. employing unidirectional lstm encoder model capable online generation. experiments representative natural language processing tasks abstractive sentence summarisation morphological inﬂection generation showed model signiﬁcantly outperforms encoderdecoder baselines requiring much smaller hidden layers. future work would like incorporate attention-based models framework enable models process data online. thank chris dyer karl moritz hermann edward grefenstette tom´aˇs kˇocisk´y gabor melis yishu miao many others helpful comments. ﬁrst author funded epsrc.", "year": 2016}