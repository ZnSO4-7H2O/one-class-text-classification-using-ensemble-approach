{"title": "Non-Deterministic Policies in Markovian Decision Processes", "tag": ["cs.AI", "cs.LG"], "abstract": "Markovian processes have long been used to model stochastic environments. Reinforcement learning has emerged as a framework to solve sequential planning and decision-making problems in such environments. In recent years, attempts were made to apply methods from reinforcement learning to construct decision support systems for action selection in Markovian environments. Although conventional methods in reinforcement learning have proved to be useful in problems concerning sequential decision-making, they cannot be applied in their current form to decision support systems, such as those in medical domains, as they suggest policies that are often highly prescriptive and leave little room for the users input. Without the ability to provide flexible guidelines, it is unlikely that these methods can gain ground with users of such systems. This paper introduces the new concept of non-deterministic policies to allow more flexibility in the users decision-making process, while constraining decisions to remain near optimal solutions. We provide two algorithms to compute non-deterministic policies in discrete domains. We study the output and running time of these method on a set of synthetic and real-world problems. In an experiment with human subjects, we show that humans assisted by hints based on non-deterministic policies outperform both human-only and computer-only agents in a web navigation task.", "text": "markovian processes long used model stochastic environments. reinforcement learning emerged framework solve sequential planning decision-making problems environments. recent years attempts made apply methods reinforcement learning construct decision support systems action selection markovian environments. although conventional methods reinforcement learning proved useful problems concerning sequential decision-making cannot applied current form decision support systems medical domains suggest policies often highly prescriptive leave little room user’s input. without ability provide ﬂexible guidelines unlikely methods gain ground users systems. paper introduces concept non-deterministic policies allow ﬂexibility user’s decision-making process constraining decisions remain near optimal solutions. provide algorithms compute non-deterministic policies discrete domains. study output running time method synthetic real-world problems. experiment human subjects show humans assisted hints based non-deterministic policies outperform human-only computer-only agents navigation task. planning decision-making well studied community. intelligent agents designed developed interact with variety environments. usually involves sensing environment making decision using intelligent inference mechanism performing action environment often times process involves level learning along decision-making process make agent eﬃcient performing intended goal. reinforcement learning branch tries develop computational approach solving problem learning interaction. process learning do—how situations actions—so maximize numerical reward signal many methods developed solve problem diﬀerent types environments diﬀerent types agents. however work focused autonomous agents robots software agents. controllers thus designed issue single action time-step executed acting agent. past years methods developed community started used sequential decision support systems many systems human makes ﬁnal decision. usability acceptance issues thus become important cases. methods therefore require level adaptation used decision support systems. adaptations main contribution paper. medical domains among cases needs adaptation. although framework correctly models sequential decision-making complex medical scenarios including long-term treatment design standard methods cannot applied medical settings current form lack ﬂexibility suggestions. requirements course speciﬁc medical domains instance might needed aircraft controller provides suggestions pilot. important diﬀerence decision support system classical problem stems fact decision support system acting agent often human being course his/her decision process. therefore assumption controller send clear commanding signal acting agent appropriate. accurate assume aspect decision-making process inﬂuenced user system. view decision process particularly relevant diﬀerent situations. first many practical cases exact model system. instead noisy model built ﬁnite number interactions environment. leads type uncertainty usually referred extrinsic uncertainty. algorithms ignore uncertainty assume model perfect. however look closely performance optimal action based imperfect model might statistically diﬀerent next best action. bayesian approaches looked problem providing conﬁdence measure agent’s performance cases acting agent human being conﬁdence measures provide user complete actions might optimal enough evidence diﬀerentiate. user his/her expertise make ﬁnal decision. methods guarantee suggestions provided system statistically meaningful plausible. hand even complete knowledge system identify optimal action might still actions roughly equal performance. point decision near-optimal options could left acting agent—namely human using decision support system. could many advantages ranging better user experience increased robustness ﬂexibly. among near-optimal solutions user select based domain knowledge preferences captured system. instance medical diagnosis system suggests treatments providing physician several options might useful ﬁnal decision could made based knowledge patient’s medical status preferences regarding side eﬀects. throughout paper address latter issue combination theoretical empirical investigations. introduce concept non-deterministic policies capture decision-making process intended decision support systems. policies involve suggesting actions non-deterministic choice made user. apply formulation solve problem ﬁnding near-optimal policies provide ﬂexible suggestions user. particular investigate suggest several actions acting agent providing performance guarantees worst-case analysis. section introduces necessary technical background material. section deﬁnes concept non-deterministic policies related concepts. section addresses problem providing choice acting agent keeping near-optimality guarantees performance worst-case scenario. propose algorithms solve problems provide approximation techniques speed computation larger domains. methods introduced paper general enough apply decision support system observable markovian environment. empirical investigations focus primarily sequential decision-making problems clinical domains system provide suggestions best treatment options patients. decisions provided sequence treatment phases. systems speciﬁcally interesting often times diﬀerent treatment options seem provide slightly diﬀerent results. therefore providing physician several suggestions would beneﬁcial improving usability system performance ﬁnal decision. markov decision process model system dynamics sequential decision problems involves probabilistic uncertainty future states system mdps used model interactions agent observable markovian environment. system assumed state given time. agent observes state performs action accordingly. system makes transition next state agent receives reward. current state system action taken agent receive reward drawn model. focus homogeneous processes which again reward distribution change time. reward time denoted have ﬁnite horizon case taken horizon limit discount factor however inﬁnite horizon case discount factor less return ﬁnite value. return process depends stochastic transitions rewards well actions taken agent. often times transition structure contains loop non-zero probability. transition graph modeled directed acyclic graph class mdps interesting includes multi-step decision-making ﬁnite horizons found medical domains. agent interacts environment takes actions according policy. value function policy deﬁned expectation return given agent acts according policy value function used primary measure performance much literature. however ideas take risk variance return account measure optimality common criteria though assume agent trying policy maximizes value function. policy referred optimal policy. much literature focused ﬁnding optimal policy. many methods developed policy optimization. optimal policy solve bellman optimality equation choose actions. bellman optimality equation formulated simple linear program represents initial distribution states. solution problem optimal value function. notice represented matrix form equation. known linear programs solved polynomial time however solving might become impractical large state spaces. therefore often times methods based dynamic programming preferred linear programming solution. begin section considering problem decision-making sequential decision support systems. recently mdps emerged useful frameworks optimizing action choices context medical decision support systems given adequate model many methods used good action-selection policy. policy usually deterministic stochastic function. policies types face substantial barrier terms gaining acceptance medical community highly prescriptive leave little room doctor’s input. problems course speciﬁc medical domain present application actions executed human. cases preferable provide several equivalently following sections discuss scenarios non-deterministic policies useful. show used implement robust decision support systems statistical guarantees performance. even cases complete knowledge dynamics planning problem hand accurately calculate actions’ utilities might desirable provide user optimal choice action time step. domains diﬀerence utility actions substantial. medical decision-making instance diﬀerence medically signiﬁcant based given state variables. cases seems natural user decide actions using his/her expertise domain. results injection domain knowledge decision-making process thus making robust practical. decisions based facts known user incorporated automated planning system. also based preferences might change case case. instance doctor several recommendations treat patient maximize chance remission decide medication apply considering also patient’s medical record preferences regarding side eﬀects medical expenses. idea providing choice user accompanied reasonable guarantees performance ﬁnal decision regardless choice made user. notion near-optimality enforced make sure actions never best possible option. guarantees enforced providing worst-case analysis decision process. many practical cases complete knowledge system hand. instead trajectories collected system according speciﬁc policy. cases given chance choose policy cases access data ﬁxed policy. medical trials particular data usually collected according randomized policy ﬁxed ahead time consultation clinical researchers. given sample trajectories either build model domain directly estimate utility diﬀerent actions however models estimates always accurate observe ﬁnite amount data. many cases data sparse incomplete uniquely identify best option. diﬀerence performance measure diﬀerent actions statistically signiﬁcant. cases might useful user decide ﬁnal choice actions enough evidence diﬀerentiate. comes assumption user identify best choice among recommended. task therefore provide user small actions almost surely include optimal one. paper focus problem providing ﬂexible policies nearoptimal performance. using non-deterministic policies handling model uncertainty remains interesting future work. often times beneﬁcial provide user decision support system near-optimal solutions. mdps would suggest near-optimal actions user user make decision among proposed actions. notion near-optimality therefore possible policies consistent proposed actions. matter action chosen among proposed options state ﬁnal performance close optimal policy. constraint suggests worst-case analysis decision-making process. therefore guarantee performance action selection consistent non-deterministic policy putting near-optimality constraint worst-case selection actions user. paper working constraints particular property policy satisfy policy includes satisfy refer constraints monotonic. constraint \u0001-optimality discussed next section. instead solving using inequality constraint applying guarantees non-deterministic policy \u0001-optimal still augmentable according hence name conservative. non-deterministic policy adding actions violates nearsearch \u0001-optimal policies optimality constraint worst-case performance. non-augmentable locally maximal size. means although policy might largest among \u0001-optimal policies cannot actions without removing actions hence locally maximal reference. remainder section focus problem searching space non-augmentable \u0001-optimal policies maximize criteria. speciﬁcally non-deterministic policies give acting agent options staying within acceptable sub-optimal margin. present example clariﬁes concepts introduced far. simplify presentation example assume deterministic transitions. however concepts apply well probabilistic mdp. figure shows example mdp. labels arcs show action names corresponding rewards shown parentheses. figure includes possible non-augmentable \u0001-optimal policies. although policies figure \u0001-optimal union \u0001-optimal. fact adding option states removes possibility adding options formalize problem ﬁnding \u0001-optimal non-deterministic policy terms optimization problem. several optimization criteria formulated still complying \u0001-optimality constraint. augmentable \u0001-optimal policies biggest overall size provides options agent still keeping \u0001-optimal guarantees. algorithms proposed later sections optimization criterion. notice solution optimization problem non-augmentable according \u0001-optimal constraint maximizes overall size policy. uncertainty optimal action state. variance estimation value function along z-test conﬁdence level comparisons probability wrong order comparing actions according values. value true model empirical estimate based dataset minimize uncertainty non-deterministic policy following sections provide algorithms solve ﬁrst optimization problem mentioned above aims maximize size policy. focus criterion seems appropriate medical decision support systems desirable acceptability system policies provide much choice possible acting agent. developing algorithms address optimization criteria remains interesting open problem. exact computational complexity ﬁnding maximal \u0001-optimal policy known. problem certainly value non-deterministic policy polynomial time solving evaluation linear program. suspect problem np-complete reduction known np-complete problem. order largest \u0001-optimal policy present algorithms. ﬁrst present mixed integer program formulation problem present search algorithm uses monotonic property \u0001-optimal constraint. method useful general theoretical formulation problem search algorithm potential extensions heuristics. second constraints ensures least action selected state. third ensures state-action pairs chosen policy bellman constraint holds otherwise constant vmax makes constraint trivial. notice solution problem maximizes result non-augmentable. proof. first notice solution \u0001-optimal ﬁrst constraints value function. show non-augmentable counter argument suppose could state-action pair solution still staying subuse solver solve problem. note however make monotonic nature constraints. general purpose solver could searching space possible non-deterministic policies would alternatively develop heuristic search algorithm maximal \u0001-optimal policy. make monotonic property \u0001-optimal policies narrow search. start computing conservative policy. augment arrive non-augmentable policy. also make fact policy \u0001-optimal neither policy includes thus search tree point. algorithm presented table one-sided recursive depth-ﬁrst-search algorithm searches space plausible non-deterministic policies maximize function maximum size \u0001-optimal policy minus size conservative policy time solve original time calculate function although worst-case running time still exponential number state-action pairs run-time much less search space suﬃciently search improved using heuristics order state-action pairs prune search. also start search policy rather conservative policy. potentially useful constraints problem. guarantee non-augmentable policies. fact adding action order values might change. transition structure contains loop non-zero probability heuristic produce optimal result cutting search time. proof. prove this ﬁrst notice sort topological sort. therefore arrange states levels state make transitions states future level. easy adding actions state non-deterministic policy change worst-case value past levels. eﬀect q-values current level future level. given non-augmentable \u0001-optimal policy generated full search sequence augmentations generated policy. permutation sequence would create policy intermediate polices \u0001-optimal. rearrange sequence actions reverse order level. point mentioned above q-value actions point added change target policy realized. therefore actions q-values minimum value must policy otherwise them conﬂicts target policy non-augmentable. since actions certain q-value must added order. therefore target policy realized rule transition structure might partial evaluation augmented policy approximate value adding actions possibly backups rather using original q-values. oﬀers possibility trading-oﬀ computation time better solutions. evaluate framework proposed algorithms ﬁrst test search formulations mdps created randomly test search algorithm real-world treatment design scenario. finally conduct experiment computer-aided navigation task human subjects assess usefulness non-deterministic policies assisting human decision-making. ﬁrst experiment study non-deterministic policies change value algorithms compare terms running time. begin generated random mdps states actions. transitions deterministic rewards random values except states reward actions; method implemented matlab cplex. figure shows solution deﬁned particular randomly generated mdp. size non-deterministic policy increases performance threshold relaxed. even small values several actions included policy state. course result q-values close other. property typical many medical scenarios diﬀerent treatments provide slightly diﬀerent results. compare running time solver search algorithm constructed random mdps described state-action pairs. figure shows running time averaged diﬀerent random mdps states assuming expected algorithms running time exponential number state-action pairs running time search algorithm bigger constant factor smaller exponent base results faster asymptotic running time. even exponential running time still search algorithm solve problems hundred state-action pairs. suﬃcient many practical domains including real-world medical decision scenarios shown next section. value measure running time algorithms trials. figure shows average running time algorithms diﬀerent values expected search algorithm deeper search tree optimality threshold relaxed running time thus increase. running time method hand remains relativity constant exhaustively searches space possible non-deterministic policies. results representative relative behaviour approaches range problems. demonstrate non-deterministic policies used presented medical domain tested full search algorithm constructed medical decisionmaking task involving real patient data. data collected part large multi-step randomized clinical trial designed investigate comparative eﬀectiveness diﬀerent treatments provided sequentially patients suﬀering depression goal treatment plan maximizes chance remission. dataset includes large number measured outcomes. current experiment focus numerical score called quick inventory depressive symptomatology used study assess levels depression purposes experiment discretize qids scores uniformly quartiles assume this along treatment step completely describe patient’s state. note underlying transition graph treated study limited four steps treatment action choices change steps. actions total. reward given patient achieves remission reward given otherwise. transition reward models estimated empirically medical database using frequentist approach. table shows non-deterministic policy obtained state second step trial computed using search algorithm assuming diﬀerent values although problem tractable formulation full search space \u0001-optimal policies still possible. table also shows running time algorithm expected increases relax threshold here heuristics. however underlying transition graph could heuristic discussed previous section policies even faster. practice doctor full table guideline using smaller values he/she wants rely decision support system larger values relying his/her assessments. believe particular presentation non-deterministic policies could used accepted clinicians excessively prescriptive keeps physician patient decision cycle. contrast traditional notion policies reinforcement learning often leaves place physician’s intervention. finally conduct experiment assess usefulness non-deterministic policies human subjects. ideally would like conduct experiments medical settings physicians studies costly diﬃcult conduct given require participation many medical professionals. therefore study non-deterministic policies easier domain constructing web-based game played computer human game deﬁned follows. user given target word asked navigate around pages wikipedia visit pages contain target word. user click word page. system uses google search wiki website clicked word keyword current page randomly chooses eight search results moves page. process mimics hyperlink structure user given attempts asked reach many pages target word possible. similar game used another work infer semantic distances concepts game however designed computer model provide results similar human player thus enable assess eﬀectiveness computer-aided decisions non-deterministic policies. construct task version wikipedia structured manageable version wikipedia intended schools. test approach also need build model task. done using empirical data follows. first latent dirichlet allocation using gibbs sampling divide pages wikipedia topics. topic corresponds state mdp. algorithm identiﬁes topic keywords occur often pages topic. deﬁne sets keywords action randomly navigate around wiki using protocol described collect transitions. observed data build transition reward model speciﬁc choices parameter number states actions made best policy provided model comparable performance human player. using amazon mechanical turk consider three experimental conone experiment given target computer chooses word ditions task. keywords comes optimal policy model. another experiment human subjects choose click word without help. finally test domain human users computer highlights hints words come non-deterministic policy record time taken process number times target word observed table summarizes average outcomes experiment four target words also include p-value t-test comparing results human agents without hints. computer score averaged runs. ﬁrst three target words performance computer agent close human user observe providing hints user results statistically signiﬁcant increase number hits. fact computer-aided human outperforms computer human agents. shows non-deterministic policies provide means inject human domain knowledge computer models ﬁnal outcome superior decision-making solely performed party. last word computer model working poorly judging rate. thus surprising hints provide much help human agent case also observe general speedup time taken agent choose click words shows usefulness non-deterministic policies accelerating human subjects’ decision-making process. paper introduces concept non-deterministic policies potential decision support systems based markovian processes. context investigate assumption decision-making system return single optimal action relaxed instead return near-optimal actions. non-deterministic policies inherently diﬀerent stochastic policies. stochastic policies assume randomized action selection strategy speciﬁc probabilities whereas non-deterministic policies impose constraint. thus bestcase worst-case analysis non-deterministic policies highlight diﬀerent scenarios human user. beneﬁts non-deterministic policies sequential decision-making two-fold. first several actions diﬀerence performance negligible report actions near-optimal options. instance medical setting diﬀerence outcome treatment options might medically case beneﬁcial provide near-optimal options. signiﬁcant. makes system robust user-friendly. medical decision-making process instance physician make ﬁnal decision among near-optimal options based side eﬀects burden patient’s preferences expense criteria captured model used decision support system. constraint however make sure regardless ﬁnal choice actions performance executed policy always bounded near optimal. framework property maintained \u0001-optimality guarantee worst-case scenario. another potential non-deterministic action sets markovian decision processes capture uncertainties optimality actions. often times amount data models constructed suﬃcient clearly identify single optimal action. forced chose action optimal might high chance making wrong decision. however given chance provide possibly-optimal actions ensure include promising options cutting obviously ones. setting task trim action much possible providing guarantee optimal action still among possible options. solve ﬁrst problem paper introduces algorithms ﬂexible nearoptimal policies. first derive exact solution formulation maximal \u0001-optimal policy. solution however computationally expensive scale large domains. describe search algorithm solve problem less computational cost. algorithm fast enough applied real world medical domains. also show heuristics search algorithm solution structures even faster. heuristic search also provide approximate solutions general case. another scale problem larger domains approximate solution program relaxing constraints. relax constraints allow non-integral solutions penalize objective values away study approximation methods remains interesting direction future work. idea non-deterministic policies introduces wide range problems research topics. section discuss idea near optimal non-deterministic policies address problem ﬁnding largest action set. mentioned optimization criteria might useful decision support systems. include maximizing decision margin alternatively minimizing uncertainty wrong selection. formalizing problems formulation incorporating heuristic search might prove useful. evidenced human interaction experiments non-deterministic policies substantially improve outcome planning decision-making tasks human user assisted robust computer-generated plan. allowing several suggestions step provides eﬀective incorporating domain knowledge human side decision-making process. medical domains physician’s domain knowledge often hard capture computer model collaborative model decision-making non-deterministic policies could oﬀer powerful framework selecting eﬀective clinically acceptable treatment strategies. authors wish thank john rush susan murphy doina precup helpful discussions regarding work. funding provided national institutes health nserc discovery grant program.", "year": 2014}