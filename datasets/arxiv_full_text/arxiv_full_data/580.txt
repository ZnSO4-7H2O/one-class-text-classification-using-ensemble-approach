{"title": "A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional  Neural Networks for Sentence Classification", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically important task of sentence classification (kim 2014, kalchbrenner 2014, johnson 2014). However, these models require practitioners to specify an exact model architecture and set accompanying hyperparameters, including the filter region size, regularization parameters, and so on. It is currently unknown how sensitive model performance is to changes in these configurations for the task of sentence classification. We thus conduct a sensitivity analysis of one-layer CNNs to explore the effect of architecture components on model performance; our aim is to distinguish between important and comparatively inconsequential design decisions for sentence classification. We focus on one-layer CNNs (to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance, which makes it a modern standard baseline method akin to Support Vector Machine (SVMs) and logistic regression. We derive practical advice from our extensive empirical results for those interested in getting the most out of CNNs for sentence classification in real world settings.", "text": "comprising sentence vector forming matrix used input models need complex realize strong results example proposed simple one-layer achieved state-of-the-art results across several datasets. strong results achieved comparatively simple architecture suggest serve drop-in replacement well-established baseline models logistic regression. complex deep learning models text classiﬁcation undoubtedly continue developed deploying technologies practice likely attracted simpler variants afford fast training prediction times. unfortunately downside cnn-based models even simple ones require practitioners specify exact model architecture used accompanying hyperparameters. uninitiated making decisions seem like something black many free parameters model. especially true compared e.g. logistic regression. furthermore practice exploring space possible conﬁgurations model extremely expensive reasons training models relatively slow even using gpus. example sst- dataset takes hour -fold cross validation using similar conﬁguration described space possible model architectures hyperparameter settings vast. indeed simple architecture consider requires minimum specifying input word vector representations; ﬁlter region size; number feature maps; activation function; pooling strategy; regularization terms convolutional neural networks recently achieved remarkably strong performance practically important task sentence classiﬁcation however models require practitioners specify exact model architecture accompanying hyperparameters including ﬁlter region size regularization parameters currently unknown sensitive model performance changes conﬁgurations task sentence classiﬁcation. thus conduct sensitivity analysis one-layer cnns explore effect architecture components model performance; distinguish important comparatively inconsequential design decisions sentence classiﬁcation. focus one-layer cnns comparative simplicity strong empirical performance makes modern standard baseline method akin support vector machine logistic regression. derive practical advice extensive empirical results interested getting cnns sentence classiﬁcation real world settings. introduction convolutional neural networks recently shown achieve impressive results practically important task sentence categorization cnns capitalize distributed representations words ﬁrst converting tokens tuning parameters simply feasible especially parameter estimation computationally intensive. emerging research begun explore hyperparameter optimization methods including random search bayesian optimization however sophisticated search methods still require knowing hyperparameters worth exploring begin furthermore believe time bayesian optimization methods integrated deployed real-world systems. work identify empirically settings practitioners expend effort tuning either inconsequential respect performance seem ‘best’ setting independent speciﬁc dataset provide reasonable range hyperparameter. take inspiration previous empirical analyses neural models coates breuel investigated factors unsupervised feature learning hyperparameter settings stochastic gradient descent respectively. report results large number experiments exploring different conﬁgurations cnns nine sentence classiﬁcation datasets. previous work area reports mean accuracies calculated cross-validation. substantial variance performance cnns even folds model conﬁguration held constant. therefore experiments perform replications cross-validation report accuracy/area curve score means ranges these. deep neural learning methods well established machine learning especially successful image speech processing tasks. recently methods begun overtake traditional sparse linear models proposed architecture multiple convolution layers positing latent dense low-dimensional word vectors inputs. deﬁned one-layer architecture performed comparably. model uses pre-trained word vectors inputs treated static non-static. former approach word vectors treated ﬁxed inputs latter ‘tuned’ speciﬁc task. elsewhere johnson zhang proposed similar model swapped high dimensional ‘one-hot’ vector representations words inputs. focus classiﬁcation longer texts rather sentences relative simplicity kim’s architecture largely proposed johnson zhang modulo word vectors coupled observed strong empirical performance makes strong contender supplant existing text classiﬁcation baselines logistic regression. practice faced making several model architecture decisions setting various hyperparameters. present little empirical data available guide decisions; addressing here. architecture begin tokenized sentence convert sentence matrix rows word vector representations token. might e.g. outputs trained wordvec glove models. denote dimensionality word vectors length given sentence dimensionality sentence matrix following collobert weston effectively treat sentence matrix ‘image’ perform convolution linear ﬁlters. text applications inherent sequential structure data. rows represent discrete symbols reasonable ﬁlters widths equal dimensionality suppose ﬁlter parameterized weight matrix region size contain parameters estimated. denote sentence matrix rs×d represent sub-matrix output sequence rs−h+ convolution operator obtained repeatedly applying ﬁlter sub-matrices dimensionality feature generated ﬁlter vary function sentence length ﬁlter region size. pooling function thus applied feature induce ﬁxed-length vector. common strategy -max pooling extracts scalar feature map. together outputs generated ﬁlter concatenated ﬁxed-length ‘top-level’ feature vector softmax function generate ﬁnal classiﬁcation. softmax layer apply ‘dropout’ means regularization. entails randomly setting values weight vector also impose norm constraint i.e. linearly scale norm vector pre-speciﬁed threshold exceeds this. fig. provides schematic illustrating model architecture described. reasonable training objective minimized categorical cross-entropy loss. parameters estimated include weight vector ﬁlter bias term activation function weight vector softmax function. ‘non-static’ approach also tunes nine sentence classiﬁcation datasets all; seven also used brieﬂy sentence polarity dataset sst- stanford sentiment treebank make input representations consistent across tasks train test sentences contrast wherein models trained phrases sentences. sst- derived sst- pared classes. train test models sentences excluding phrases. subj subjectivity dataset trec question classiﬁcation customer dataset review dataset mpqa opinion polarity dataset additionally opinosis dataset comprises sentences extracted user reviews given topic e.g. sound quality ipod nano. topics topic contains approximately sentences irony contains sentences reddit labeled ironic dataset imbalanced thus training under-sampled negative instances make classes sizes equal. dataset report area curve rather accuracy imbalanced. provide point reference results ﬁrst report performance achieved using sentence classiﬁcation. baseline used linear kernel exploiting unibi-gram features. used averaged word vectors calculated words comprising sentence features used rbf-kernel classiﬁer operating dense feature space. figure illustration architecture sentence classiﬁcation. depict three ﬁlter region sizes ﬁlters. filters perform convolutions sentence matrix generate feature maps; -max pooling performed i.e. largest number feature recorded. thus univariate feature vector generated maps features concatenated form feature vector penultimate layer. ﬁnal softmax layer receives feature vector input uses classify sentence; assume binary classiﬁcation hence depict possible output states. also experimented combining uni-gram bi-gram word vector features linear kernel svm. kept frequent ngrams datasets tuned hyperparameters nested cross-fold validation optimizing accuracy consistency used pre-processing steps data described previous work report means -folds datasets table notably even naively incorporating wordvec embeddings feature vectors usually improves results. ﬁrst consider performance baseline conﬁguration. speciﬁcally start architectural decisions hyperparameters used previous work described table contextualize variance performance attributable various architecture decisions hyperparameter settings critical assess variance strictly parameter estimation procedure. prior work unfortunately reported variance despite highly stochastic learning procedure. variance attributable estimation random dropout random weight parameter initialization. holding variables table accuracy achieved different feature sets. bowsvm unibi-gram features. wvsvm naive wordvec-based representation i.e. average word vector sentence. bowwvsvm concatenates vectors average wordvec representations. constant show mean performance calculated -fold cross validation exhibits relatively high variance repeated runs. replicated experiments times dataset replication -fold wherein folds ﬁxed. recorded average performance replication report mean minimum maximum average accuracy values observed replications provides sense variance might observe without changes model. static non-static methods. experiments used preprocessing steps data used adadelta update rule minibatch size randomly selected training data validation early stopping. fig. provides density plots mean accuracy -fold replications methods datasets. presentation clarity ﬁgure exclude sst- irony datasets performance substantially lower note pre-processed/split datasets differently original work ensure consistency present analysis; thus results directly comparable prior work. emphasize improve state-of-the-art rather explore sensitivity cnns respect design decisions. table baseline conﬁguration. ‘feature maps’ refers number feature maps ﬁlter region size. ‘relu’ refers rectiﬁed linear unit commonly used activation function cnns. chitecture decisions hyperparameter settings. hold settings constant vary component interest. every conﬁguration consider replicate experiment times replication constitutes -fold report average means associated ranges achieved replicated runs. performed experiments using ‘static’ ‘non-static’ word vectors. latter uniformly outperformed former report results ‘non-static’ variant. effect input word vectors nice property sentence classiﬁcation models start distributed representations words inputs ﬂexibility architectures afford swap different pre-trained word vectors during model initialization. therefore ﬁrst explore sensitivity cnns sentence classiﬁcation respect input representations used. speciﬁcally replaced wordvec glove representations. google wordvec uses local context window model trained billion table performance using non-static wordvec-cnn non-static glove-cnn non-static glove+wordvec respectively. cell reports mean summary performance measures calculated multiple runs -fold cross-validation. format tables involving replications words google news glove model based global wordword co-occurrence statistics used glove model trained corpus billion tokens data. wordvec glove induce -dimensional word vectors. report results achieved using glove representations table report non-static glove results also experimented concatenating wordvec glove representations thus creating -dimensional word vectors used input cnn. pre-trained vectors always available speciﬁc words cases randomly initialized corresponding subvectors. results reported ﬁnal column table relative performance achieved using glove versus wordvec depends dataset unfortunately simply concatenating representations necessarily seem helpful. practically results suggest experimenting different pre-trained word vectors tasks. also experimented using long sparse one-hot vectors input word representations spirit johnson zhang strategy word encoded one-hot vector dimensionality equal vocabulary size. though representation combined one-layer achieves good results document classiﬁcation still unknown whether useful sentence classiﬁcation. keep settings basic conﬁguration one-hot vector ﬁxed training. compared using embeddings input found one-hot approach perform poorly sentence classiﬁcation tasks. believe one-hot suitable sentence classiﬁcation small modestly sized training dataset likely sparsity sentences perhaps brief provide enough information highdimensional encoding. alternative one-hot architectures might appropriate scenario. example johnson zhang propose semi-supervised variant ﬁrst learns embeddings small text regions unlabeled data integrates supervised cnn. emphasize training data plentiful learning embeddings scratch indeed best. ﬁrst explore effect ﬁlter region size using region size number feature maps region size consider region sizes record means ranges replications -fold each. report results table fig. interested trend accuracy alter region size optimal region size larger. also explored effect combining different ﬁlter region sizes keeping number feature maps region size ﬁxed found combining several ﬁlters region sizes close optimal single region size improve performance adding region sizes optimal range hurt performance. example using single ﬁlter size observe optimal single region size dataset therefore combined several different ﬁlter region sizes close optimal range compared approaches region sizes outside range. table using sets near best single region size produce best results. difference especially pronounced comparing baseline setting note even using single good ﬁlter region size results better performance combining different sizes best performing strategy simply many feature maps region size equal i.e. single best region size. however note cases using multiple different nearoptimal region sizes performs best. provide another illustrative empirical result using several region sizes trec dataset table performance single region size best single ﬁlter region sizes trec explore region size around values compare using multiple region sizes away ‘optimal’ values. worse however result still shows combination region sizes near optimal single best region size outperforms using multiple region sizes optimal single region size. furthermore single good region size outperforms combining several suboptimal region sizes light observations believe advisable ﬁrst perform coarse line-search single ﬁlter region size ‘best’ size dataset consideration explore combination several region sizes nearby single best size including combining different region sizes copies optimal sizes. ‘best’ number feature maps ﬁlter region size depends dataset. however would seem increasing number maps beyond yields best marginal returns often hurts performance another salient practical point takes longer time train model number feature maps increased. practice evidence suggests perhaps searching range note range provided possible standard trick faced similar sentence classiﬁcation problem; course possible cases feature maps beneﬁcial evidence suggests expending effort explore probably worth practice consider whether best observed value falls near border range searched over; probably worth exploring beyond border suggested effect activation function consider seven different activation functions convolution layer including relu hyperbolic tangent sigmoid function softplus function cube function tanh cube function ‘iden’ denote identity function means using activation function. report results achieved using different activation functions non-static table datasets best activation function iden relu tanh. softplus function outperformedd dataset sigmoid cube tanh cube consistently performed worse alternative activation functions. thus report results here. performance tanh function zero centering property relu merits non-saturating form compared sigmoid observed accelerate convergence interesting result applying activation function sometimes helps. indicates datasets linear transformation enough capture correlation word embedding output label. however multiple hidden layers iden less suitable non-linear activation functions. practically respect choice activation function one-layer cnns results suggest experimenting relu tanh perhaps also iden. next investigated effect pooling strategy pooling region size. ﬁxed ﬁlter region sizes number feature maps baseline conﬁguration thus changing pooling strategy pooling region size. baseline conﬁguration performed pooling globally feature maps inducing feature vector length ﬁlter. however pooling also performed small equal sized local regions rather entire feature small local region feature generate single number pooling numbers concatenated form feature vector feature map. following step pooling concatenate feature vectors together form single feature vector classiﬁcation layer. experimented local region sizes found -max pooling outperformed local pooling conﬁgurations. result held across datasets. also considered k-max pooling strategy similar maximum values extracted entire feature relative order values preserved. explored found -max pooling fared best consistently outperforming k-max pooling. next considered taking average rather regions held rest architecture constant. experimented local average pooling region sizes found average pooling uniformly performed worse pooling least trec datasets. substantially worse performance slow running time observed average pooling complete experiments datasets. analysis pooling strategies shows pooling consistently performs better alternative strategies task sentence classiﬁcation. location predictive contexts matter certain n-grams sentence predictive entire sentence considered jointly. effect regularization common regularization strategies cnns dropout norm constraints; explore effect here. ‘dropout’ applied input penultimate layer. experimented varying dropout rate ﬁxing norm constraint baseline conﬁguration. results non-static shown fig. designated baseline. also report accuracy achieved remove dropout norm constraint denoted ‘none’. norm imposed weight vectors parametrize softmax function. recall norm weight vector linearly scaled constraint exceeds threshold smaller implies stronger regularization. show relative effect varying non-static figure ﬁxed dropout rate baseline figures non-zero dropout rates help points depending datasets. imposing norm constraint generally improve performance much even adversely effects performance least dataset also explored dropout rate effect increasing number feature maps. increase number feature maps ﬁlter size norm constraint effect dropout rate shown fig. effect dropout rate almost number feature maps help much. observe dataset sst- dropout rate actually helps referring fig. number feature maps larger hurts performance possibly overﬁtting reasonable case dropout would mitigate effect. also experimented applying dropout convolution layer still setting norm constraint classiﬁcation layer keeping settings exactly same. means randomly elements sentence matrix training probability multiplied sentence matrix test time. effect dropout rate convolution layer shown fig. dropout convolution layer helps little large dropout rate dramatically hurts performance. summarize contrary existing literature found dropout little beneﬁcial effect performance. attribute observation fact one-layer smaller number parameters multi-layer deep learning models. anpossible explanation using word embeddings helps prevent overﬁtting however advocating completely foregoing regularization. practically suggest setting dropout rate small value using relatively large norm constraint increasing number feature maps whether features might help. increasing number feature maps seems degrade performance probably worth increasing dropout rate. conducted extensive experimental analysis cnns sentence classiﬁcation. conclude summarizing main ﬁndings deriving practical guidance researchers practitioners looking deploy cnns real-world sentence classiﬁcation scenarios. prior work tended report mean performance datasets achieved models. overlooks variance solely stochastic inference procedure used. substantial holding everything constant variance exclusively stochastic inference procedure mean accuracy range points. range achieved irony dataset even greater points replication performed future work ranges/variances reported prevent potentially spurious conclusions regarding relative model performance. that even tuning task hand choice input word vector representation impact performance however different representations perform better different tasks. least sentence classiﬁcation seem perform better using one-hot vectors directly. note however that case sufﬁciently large amount training data recent semi-supervised model proposed johnson zhang improve performance compared simpler version model considered speciﬁc advice practitioners drawing upon empirical results provide following guidance regarding architecture hyperparameters practitioners looking deploy cnns sentence classiﬁcation tasks. consider starting basic conﬁguration described table using non-static wordvec glove rather one-hot vectors. however training dataset size large worthwhile explore using one-hot vectors. alternatively access large unlabeled in-domain data might also option. line-search single ﬁlter region size ‘best’ single region size. reasonable range might however datasets long sentences like worth exploring larger ﬁlter region sizes. ‘best’ region size identiﬁed worth exploring combining multiple ﬁlters using regions sizes near single best size given empirically multiple ‘good’ region sizes always outperformed using single best region size. alter number feature maps ﬁlter region size explored small dropout rate large norm constraint. note increasing number feature maps increase running time trade-off consider. also attention whether best value found near border range best value near worth trying larger values. consider different activation functions possible relu tanh best overall candidates. might also worth trying activation function one-layer cnn. assessing performance model imperative consider variance. therefore replications cross-fold validation procedure performed variances ranges considered. course suggestions applicable datasets comprising sentences similar properties considered work. examples counter ﬁndings here. nonetheless believe suggestions likely provide reasonable starting point researchers practitioners looking apply simple one-layer real world sentence classiﬁcation tasks. emphasize selected simple one-layer light observed strong empirical performance positions standard baseline model akin bag-of-words logistic regression. approach thus considered prior implementation sophisticated models. attempted provide practical empirically informed guidance help data science practitioners best conﬁguration simple model. recognize manual grid search hyperparameters sub-optimal note suggestions also inform hyperparameter ranges explore random search bayesian optimization frameworks. work supported part army research ofﬁce foundation science technology portugal work also made possible support texas advanced computer center austin. references yoshua bengio r´ejean ducharme pascal vincent christian janvin. neural probabilistic language model. journal machine learning research james bergstra daniel yamins david daniel cox. making science model search hyperparameter optimization hundreds dimensions vision architectures. y-lan boureau francis bach yann lecun jean ponce. learning mid-level features recognition. computer vision pattern recognition ieee conference pages ieee. y-lan boureau nicolas roux francis bach jean ponce yann lecun. locals multi-way local pooling image recognition. computer vision ieee international conference pages ieee. danqi chen christopher manning. fast accurate dependency parser using neural networks. proceedings conference empirical methods natural language processing volume pages adam coates andrew honglak lee. analysis single-layer innetworks unsupervised feature learning. ternational conference artiﬁcial intelligence statistics pages ronan collobert jason weston. uniﬁed architecture natural language processing deep neural networks proceedings inmultitask learning. ternational conference machine learning pages acm. ronan collobert jason weston l´eon bottou michael karlen koray kavukcuoglu pavel kuksa. natural language processing scratch. journal machine learning research charles dugas yoshua bengio franc¸ois b´elisle claude nadeau ren´e garcia. incorporating second-order functional knowledge better option pricing. advances neural information processing systems pages kavita ganesan chengxiang zhai jiawei han. opinosis graphbased approach abstractive summarization proceedings highly redundant opinions. international conference computational linguistics pages association computational linguistics. geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutdinov. improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv.. minqing bing liu. mining summarizing customer reviews. proceedings tenth sigkdd international conference knowledge discovery data mining pages acm. tong zhang. semi-supervised convolutional neural networks text categorization region advances neural information embedding. processing systems pages kalchbrenner edward grefenstette phil blunsom. convolutional neural network modelling sentences. proceedings annual meeting association computational linguistics pages baltimore maryland june. association computational linguistics. ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural advances neural information networks. processing systems pages tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases advances neural compositionality. information processing systems pages pedregosa varoquaux gramfort michel thirion grisel blondel prettenhofer weiss dubourg vanderplas passos cournapeau brucher perrot duchesnay. scikit-learn machine learning python. journal machine learning research jeffrey pennington richard socher christopher manning. glove global vectors word representation. proceedings empiricial methods natural language processing richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. proceedings conference empirical methods natural language processing volume page citeseer. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research byron wallace laura kertz kook choe eugene charniak. humans require context infer ironic intent proceedings annual meeting association computational linguistics pages peng wang jiaming chenglin heng zhang fangyuan wang hongwei hao. semantic clustering convolutional neural network short text categorizaproceedings annual meettion. association computational linguistics international joint conference natural language processing", "year": 2015}