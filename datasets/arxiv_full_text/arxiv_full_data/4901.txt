{"title": "Dynamic Key-Value Memory Networks for Knowledge Tracing", "tag": ["cs.AI", "cs.LG"], "abstract": "Knowledge Tracing (KT) is a task of tracing evolving knowledge state of students with respect to one or more concepts as they engage in a sequence of learning activities. One important purpose of KT is to personalize the practice sequence to help students learn knowledge concepts efficiently. However, existing methods such as Bayesian Knowledge Tracing and Deep Knowledge Tracing either model knowledge state for each predefined concept separately or fail to pinpoint exactly which concepts a student is good at or unfamiliar with. To solve these problems, this work introduces a new model called Dynamic Key-Value Memory Networks (DKVMN) that can exploit the relationships between underlying concepts and directly output a student's mastery level of each concept. Unlike standard memory-augmented neural networks that facilitate a single memory matrix or two static memory matrices, our model has one static matrix called key, which stores the knowledge concepts and the other dynamic matrix called value, which stores and updates the mastery levels of corresponding concepts. Experiments show that our model consistently outperforms the state-of-the-art model in a range of KT datasets. Moreover, the DKVMN model can automatically discover underlying concepts of exercises typically performed by human annotations and depict the changing knowledge state of a student.", "text": "knowledge tracing task tracing evolving knowledge state students respect concepts engage sequence learning activities. important purpose personalize practice sequence help students learn knowledge concepts eﬃciently. however existing methods bayesian knowledge tracing deep knowledge tracing either model knowledge state predeﬁned concept separately fail pinpoint exactly concepts student good unfamiliar with. solve problems work introduces model called dynamic key-value memory networks exploit relationships underlying concepts directly output student’s mastery level concept. unlike standard memory-augmented neural networks facilitate single memory matrix static memory matrices model static matrix called stores knowledge concepts dynamic matrix called value stores updates mastery levels corresponding concepts. experiments show model consistently outperforms state-ofthe-art model range datasets. moreover dkvmn model automatically discover underlying concepts exercises typically performed human annotations depict changing knowledge state student. advent massive open online courses intelligent tutoring systems students appropriate guidance acquire relevant knowledge process solving exercises. exercise posted student must apply concepts solve exercise. example student attempts solve exercise apply concept integer addition; student attempts solve apply concepts integer addition decimal addition. probability student answer exercise correctly based student’s knowledge state stands depth robustness underlying concepts student mastered. goal knowledge tracing trace knowledge state students based past exercise performance. essential task online learning platforms. tutors give proper hints tailor sequence practice exercises based personal strengths weaknesses students. students made aware learning progress devote energy lessfamiliar concepts learn eﬃciently. although eﬀectively modeling knowledge students high educational impact using numerical simulations represent human learning process inherently difﬁcult usually formulated supervised sequence learning problem given student’s past exercise interactions {xx...xt−} predict probability student answer exercise correctly i.e. input tuple containing exercise student attempts timestamp correctness student’s answer model observed variables student’s knowledge state {ss...st−} underlying concepts existing methods bayesian knowledge tracing deep knowledge tracing model knowledge state students either concept speciﬁc manner summarized hidden vector shown figure student’s knowledge state analyzed diﬀerent concept states figure model diﬀerences among model. concept speciﬁc. uses summarized hidden vector model knowledge state. model maintains concept state concept simultaneously concept states constitute knowledge state student. hidden markov model update posterior distribution binary concept state. therefore cannot capture relationship diﬀerent concepts. moreover keep bayesian inference tractable uses discrete random variables simple transition models describe evolvement concept state. result although output student’s mastery level predeﬁned concepts lacks ability extract undeﬁned concepts model complex concept state transitions. besides solving problem bayesian perspective deep learning method named exploits variant recurrent neural networks called long short-term memory lstm assumes highdimensional continuous representation underlying to-state transitions stronger representational power bkt. human-labeled annotation required. however summarizes student’s knowledge state concepts hidden state makes difﬁcult trace much student mastered certain concept pinpoint concepts student good unfamiliar present work introduces model called dynamic key-value memory networks combines best worlds ability exploit relationship between concepts ability trace concept state. dkvmn model automatically learn correlation input exercises underlying concepts maintain concept state concept. timestamp related concept states updated. instance figure exercise comes model ﬁnds requires application concept read corresponding concept states predict whether student answer exercise correctly. student completes exercise model update concept states. concept states constitute addition unlike standard memory-augmented neural networks facilitate single memory matrix variation static memory matrices model static matrix called stores concept representations dynamic matrix called value stores updates student’s understanding concept. terms static dynamic matrices respectively analogous immutable mutable objects keys values dictionary data structure meanwhile training process analogous object creation. network static memory matrices suitable solving task learning static process. learning builds upon shaped previous knowledge human memory model single dynamic matrix maps exercise correct answer exercise incorrect answer diﬀerent concept states match cognition. experiments show dkvmn model outperforms mann model single memory matrix stateof-the-art model. highly constrained structured model models concept-speciﬁc performance i.e. individual instantiation made concept assumes knowledge state binary variable. many following variations raised integrating personalization study exercise diversity information bayesian framework. exploits utility lstm break restriction skill separation binary state assumption. lstm uses hidden states kind summary past sequence inputs parameters shared diﬀerent time steps. experiments showed outperforms previous bayesian models large margin terms prediction accuracy. study ﬁrst attempt integrate deep learning models achieved signiﬁcant success areas including computer vision natural language processing inspired computer architecture particular neural network module called external memory proposed enhance ability network capture long-term dependencies solve algorithmic problems mann progress various areas question answering natural language transduction algorithm inference one-shot learning despite powerful lstm storing past performance students mann still deﬁciencies applied task. mann content read lies space content write. however tasks like input prediction exercises student receives correctness student’s answer diﬀerent types. therefore embed exercise response jointly attention make sense. furthermore mann cannot explicitly model underlying concepts input exercises. knowledge state particular concept dispersed cannot traced. solve problems dkvmn model uses keyvalue pairs rather single matrix memory structure. instead attending reading writing memory matrix mann dkvmn model attends input component immutable reads writes corresponding value component. sponse tuple here also comes distinct exercise tags binary value. assume latent concepts underlying student’s mastery levels stored concept i.e. concept states changes time. value matrix dkvmn traces knowledge student reading writing value matrix using correlation weight computed input exercise matrix. model details elaborated following sections. typical external memory module contains parts memory matrix stores information controller communicates environment reads writes memory. reading writing operations achieved additional attention mechanisms. previous works similar compute read weight. input cosine similarity inner product input memory slot computed goes softmax positive strength obtain read weight softmax]) softmax write process attention mechanism focusing content location proposed facilitate locations memory. addition pure content-based memory writer named least recently used access module raised write either least recently used memory location recently used memory location. owing recurrence introduced read write operations mann special kind well. however mann diﬀerent conventional rnns like lstm used three aspects. first traditional models single hidden state vector encode temporal information whereas mann uses external memory matrix increase storage capacity second state-to-state transition traditional rnns unstructured global whereas mann uses read write operations encourage local state transitions third number parameters traditional rnns tied size hidden states mann increasing number memory slots increase number parameters outcome computationally eﬃcient. section ﬁrst introduce exploit existing mann model solve problem. show deﬁciencies mann describe dkvmn model. description below denote vectors bold small letters matrices bold capital letters. number memory locations vector size location. timestamp input mann joint embedding comes distinct exercise tags binary value indicating whether student answered exercise correctly. embedding vector used compute read weight implementation choose cosine similarity attention mechanism compute lrua mechanism compute details attention mechanisms shown appendix. intuition mann student answers exercise stored memory response written previously used memory locations exercise arrives student gets diﬀerent response written least recently used memory locations. figure architecture model drawn timestamp purple components describe read process green components describe write process. blue components dkvmn model denote attention process compute corresponding weight. student working exercise. writing student’s knowledge growth value component memory erased ﬁrst information added step inspired input forget gates lstms. row-vector therefore elements memory location reset zero weight location erase element one. memory vector left unchanged either weight erase signal zero. calculated read content treated summary student’s mastery level exercise. given exercise diﬃculty concatenate read content input exercise embedding pass fully connected layer tanh activation summary vector contains student’s mastery level prior diﬃculty exercise student answers question model update value matrix according correctness student’s answer. joint embedding written value part memory correlation weight used read process. table test results datasets. standard bkt. bkt+ best-reported result variations. result using lstm. mann baseline using single memory matrix. dkvmn model. overall model architecture shown figure training embedding matrices well parameters initial value jointly learned minimizing standard cross entropy loss true label prediction accuracy ﬁrst evaluated comparing dkvmn model methods four datasets namely synthetic dataset three real-world datasets collected online learning platforms. then comparative experiments diﬀerent dimensions states performed dkvmn model exploration. finally ability model veriﬁed discover concepts automatically depict knowledge state students. synthetic- dataset simulates virtual students answering exercises training testing dataset. exercise drawn hidden concepts diﬀerent levels diﬃculty. assistments dataset gathered assistments online tutoring platform. owing duplicated record issues updated version released previous results dataset longer reliable. experiments paper conducted using updated skill-builder dataset. records without skill names discarded preprocessing. thus number records experiments smaller total students answer exercises along distinct exercise tags. statics statics college-level engineering statics course trials students exercises tags experiments concatenation problem name step name used exercise tag; thus maximum number exercise tags maximum number average records student. input exercise data presented neural networks using one-hot input vectors. speciﬁcally diﬀerent exercises exist total exercise memory part length vector whose entries zero except entry one. similarly combined input value matrix component length vector entry one. learn initial value value matrix training process. slot memory concept embedding ﬁxed testing process. meanwhile initial value value memory initial state concept represents initial diﬃculty concept. assistmentshttps//sites.google.com/site/ assistmentsdata/home/assistment---data/ skill-builder-data-- assistmentshttps//sites. google.com/site/assistmentsdata/home/ -assistments-skill-builder-data staticshttps//pslcdatashop.web.cmu.edu/ datasetinfo?datasetid= table comparison dkvmn four datasets diﬀerent numbers state dimensions memory size size represent state dimension memory size number parameters respectively. choose state dimensions dkvmn. dkvmn change memory size state dimension report best test corresponding memory size. likewise compare number parameters models. figure validation training dkvmn datasets. blue line represents model line represents dkvmn model. dotted line represents training line upper triangles represents validation auc. datasets sequences held testing except synthetic dataset training testing datasets size. total training split form validation used select optimal model architecture hyperparameters perform early stopping parameters initialized randomly gaussian distribution zero mean standard deviation initial learning rate case case number students exercise tags total answers dataset varied learning rate annealed every epochs epoch reached. used lstm implementation. standard mann implemented using cosine similarity reading attention mechanism lrua writing attention mechanism. stochastic gradient descent momentum norm clipping used train mann dkvmn experiments. consistently momentum norm clipping threshold given input sequences different lengths sequences length null symbol used short sequence ﬁxed size cases hyperparameters tuned using ﬁvefold cross validation. test area curve computed using model highest validation among epochs. repeated training times diﬀerent initializations reported average test along standard deviation. measured evaluate prediction accuracy dataset. represents score achievable random guessing. high score accounts high prediction performance. results test datasets shown table compare dkvmn model mann baseline state-of-the-art standard model possible optimal variations interesting observation implemented lstm achieves better original papers reason implementations norm clipping early stopping improve overﬁtting problem lstm. results directly obtained recent works synthetic- dataset dkvmn model achieves average test simulation exercise treated distinct skill label. mann produces average produces value better reported original paper variant model achieve respectively prediction results dkvmn assistments achieve improvement mann respectively dataset preprocessed diﬀerently results comparable. assistments dataset test dkvmn better mann figure concept discovery results synthetic- dataset memory size left heat x-axis represents exercise y-axis represents correlation weight exercise latent concepts generated dkvmn model. ground-true concept labeled exercise. right exercise clustering graph node number represents exercise. exercises ground-truth concept clustered together. figure concept discovery results asssistments dataset. exercises clustered concepts. exercises concept labeled color left picture also block right table. classic regard statics maximum number exercise tags minimum number answers classical gains cooperating forgetting skill discovery latent abilities obtains implemented leads better mann produces average however dkvmn model achieves outperforming previous models. summary dkvmn performs better methods across datasets particularly statics dataset whose number distinct exercises large. result demonstrates dkvmn model student’s knowledge well number exercises large. dkvmn achieve better prediction accuracy student exercise performance also requires considerably fewer parameters model large external memory capacity. table compares dkvmn model model using lstm traversing different hyperparameters. table reveals dkvmn state dimensions achieve better prediction accuracy high state dimensions. instance statics dataset reaches maximum test dimension states equals using million parameters. meanwhile dkvmn achieve test state dimensions using thousand parameters. idation dkvmn increases smoothly. however epoch proceeds training increases continuously validation increases ﬁrst several epochs begins decrease. dkvmn model power discover underlying patterns concepts exercises using correlation weight traditionally annotated experts. correlation weight exercise concept implies strength inner relationship. compared conditional inﬂuence approach computes dependencies exercises deﬁnes threshold cluster exercises model directly assigns exercises concepts. predeﬁned threshold required. result model discover concepts exercises end-to-end manner. exercise usually associated single concept. case assign exercise concept largest correlation weight value. experiments model intelligently learn sparse weight among concepts discovered concepts reveal compelling result. concept accessed exercises shown x-axis heat figure exercises concept labeled squares color. left heat figure shows correlation weight distinct exercises latent concepts figure example student’s changing knowledge state concepts. concepts marked diﬀerent colors left side. answering exercises student masters second third fourth concepts fails understand ﬁfth concept. column represents correlation weight exercise latent concepts. exercise weight sparse exactly value approximates others approximate clustering exercise concept maximum weight value graph shown right part figure reveals perfect clustering latent concepts. adjusted mutual information clustering result ground truth moreover memory size larger ground truth e.g. model also exercise clusters appropriate concept exercise. additional results described appendix. assistments dataset ground truth concept used exercise. however name exercise obtained shown right part figure exercise followed name. resulting cluster graph figure drawn using t-sne projecting multi-dimensional correlation weights points. exercises grouped clusters exercises cluster labeled color. clustering graph reveals many reasonable results. related exercises close another cluster. example ﬁrst cluster ordering fractions ordering integers ordering positive decimals ordering real numbers clustered together exposes concept elementary arithmetic. dkvmn also used depict changing knowledge state students. depicting knowledge state especially concept state helpful users online learning platforms. students possess concept states concepts pinpoint strengths weaknesses motivated learning gaps independently. student’s changing knowledge state obtained read process using following steps. figure shows example depicting student’s changing concept states. ﬁrst column represents initial state concept student answers exercise state diﬀers concept concept. owing model’s ability discover concepts exercise time student answers exercise concept state discovered concept increase decrease. example student answers ﬁrst three exercises correctly concept states second ﬁfth concepts increase; student answers fourth exercise incorrectly concept state third concept decreases. answering exercises student shown mastered second third fourth concepts failed understand ﬁfth concept. work proposes sequence learning model called dkvmn tackle problem. model implemented online learning platforms improve study eﬃciency students. dkvmn outperforms state-of-the-art also trace student’s understanding concept time main drawback dkt. compared standard manns keyvalue pair allows dkvmn discover underlying concepts input exercise trace student’s knowledge state concepts. future work incorporate content information exercise concept embeddings improve representations. also investigate hierarchical key-value memory networks structure encode hierarchical relationship concepts. memory size synthetic- dataset exercises still clustered categories. heat figure describes correlation weight vectors fall several concepts. clustering exercise concept maximum weight value adjusted mutual information clustering result ground truth additionally figure concept discovery results synthetic- dataset memory size heat x-axis represents exercise y-axis represents correlation weight exercise latent concepts generated dkvmn model. exercise clustering graph using t-sne node number represents exercise. exercises groundtruth concept clustered together. chen wang wang xiao zhang zhang. mxnet ﬂexible eﬃcient machine learning library heterogeneous distributed systems. neural information processing systems workshop machine learning systems accurate student modeling contextual estimation slip guess probabilities bayesian knowledge tracing. international conference intelligent tutoring systems pages springer zhao karklin inwegen ekanadham beck estimating student proﬁciency deep learning panacea. neural information processing systems workshop machine learning education", "year": 2016}