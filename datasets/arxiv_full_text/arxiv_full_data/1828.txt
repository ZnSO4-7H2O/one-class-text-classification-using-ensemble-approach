{"title": "Efficient Correlated Topic Modeling with Topic Embedding", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "Correlated topic modeling has been limited to small model and problem sizes due to their high computational cost and poor scaling. In this paper, we propose a new model which learns compact topic embeddings and captures topic correlations through the closeness between the topic vectors. Our method enables efficient inference in the low-dimensional embedding space, reducing previous cubic or quadratic time complexity to linear w.r.t the topic size. We further speedup variational inference with a fast sampler to exploit sparsity of topic occurrence. Extensive experiments show that our approach is capable of handling model and data scales which are several orders of magnitude larger than existing correlation results, without sacrificing modeling quality by providing competitive or superior performance in document classification and retrieval.", "text": "poor scaling large data. instance direct modeling pairwise correlations non-conjugacy logistic-normal prior impose inference complexity number latent topics signicantly demanding compared scales linearly. recent work improved modeling inference model scale still limited less latent topics. stands stark contrast recent industrial-scale models handle millions topics billions documents capturing longtail semantics supporting industrial applications rich extraction task expected beer addressed expressive correlation models. therefore highly desirable develop ecient correlated topic models great representational power highly scalable inference practical deployment. paper develop model extracts correlation structures latent topics sharing comparable expressiveness costly model keeping ecient simple lda. propose learn distributed representation latent topic characterize correlatedness topics closeness respective topic vectors embedding space. compared previous pairwise correlation modeling topic embedding scheme parsimonious less parameters estimate exible enable richer analysis visualization. figure illustrates correlation paerns topics inferred model million nytimes news articles clear dependency structures among large collection topics grasp semantics massive text corpus. derive ecient variational inference procedure combined fast sparsity-aware sampler stochastic tackling non-conjugacies. embedding based correlation modeling enables inference low-dimensional vector space resulting linear complexity w.r.t topic size lightweight lda. allows discover latent topics correlations near million articles several orders magnitude larger prior work work diers recent research combines topic models word embeddings capturing word dependencies instead focus modeling dependencies latent topic space exhibit uncertainty inferentially challenging. best knowledge work incorporate distributed representation learning topic correlation modeling oering intuitive geometric interpretation theoretical bayesian modeling advantages. demonstrate ecacy method extensive experiments various large text corpora. approach shows greatly improved eciency previous correlated topic models scales well much simpler lda. achieved without sacricing modeling power—the proposed model extracts high-quality topics correlations obtaining competitive abstract correlated topic modeling limited small model problem sizes high computational cost poor scaling. paper propose model learns compact topic embeddings captures topic correlations closeness topic vectors. method enables ecient inference low-dimensional embedding space reducing previous cubic quadratic time complexity linear w.r.t topic size. speedup variational inference fast sampler exploit sparsity topic occurrence. extensive experiments show approach capable handling model data scales several orders magnitude larger existing correlation results without sacricing modeling quality providing competitive superior performance document classication retrieval. introduction large ever-growing document collections provide great opportunities pose compelling challenges infer rich semantic structures underlying data data management utilization. topic models particularly latent dirichlet allocation model popular statistical frameworks identify latent semantics text corpora. drawback derives conjugate dirichlet prior models topic occurrence independently fails capture rich topical correlations nance). eective modeling pervasive correlation paerns essential structural topic navigation improved document representation accurate prediction correlated topic model extends using logistic-normal prior explicitly models correlation paerns gaussian covariance matrix. despite enhanced expressiveness resulting richer representations practical applications correlated topic modeling unfortunately limited high model complexity permission make digital hard copies part work personal classroom granted without provided copies made distributed prot commercial advantage copies bear notice full citation page. copyrights components work owned others must honored. abstracting credit permied. copy otherwise republish post servers redistribute lists requires prior specic permission and/or fee. request permissions permissionsacm.org. kdd’ august halifax canada. acm. ----//.... figure visualization correlated topics nytimes news corpus. point cloud shows topic embeddings point represents latent topic. smaller distance indicates stronger correlation. show four sets topics nearby embedding space respectively. topic characterized words according word distribution. edge indicates correlation topics strength threshold. rest paper organized follows section briey reviews related work; section presents proposed topic embedding model; section shows extensive experimental section concludes paper. related work correlated topic modeling topic models represent document mixture latent topics. among popular topic models model assumes conjugate dirichlet prior topic mixing proportions easier inference. simplicity scalability extracted broad interest industrial applications dirichlet prior however incapable capturing dependencies between topics. classic model provides elegant extension replacing dirichlet prior logistic-normal prior models pairwise topic correlations gaussian covariance matrix. however enriched extraction comes computational cost. number parameters covariance matrix grows square number topics parameter estimation full-rank matrix inaccurate high-dimensional space. importantly frequent matrix inversion operations inference lead time complexity signicantly restricted model data scales. address this chen derives scalable gibbs sampling algorithm based data augmentation. ough bringing inference cost document computation still expensive practical real-world massive tasks. puhividhya reformulates correlation prior independent factor models faster inference. however similar many approaches problem scale still limited thousands documents hundreds topics. contrast scale correlated topic modeling industrial level deployment reducing complexity level linear topic size providing rich extraction costly model. note recent scalable extensions alias methods orthogonal approach applied inference speedup. consider future work. another line topic models organizes latent topics hierarchy also captures topic dependencies. however hierarchy structure either pre-dened inferred data using bayesian nonparametric methods known computationally demanding proposed model exible without sacricing scalability. distributed representation learning growing interest distributed representation learns compact vectors words entities network nodes others. induced vectors expected capture semantic relatedness target items successfully used various applications. compared work induces embeddings observed units learn distributed representations latent topics poses unique challenge inference. previous work also induces compact topic manifold visualizing large document collections. work distinct leverage learned topic vectors document vector similar distances vectors semantically correlated topics themselves nearby space thus tend assign similar probability mass topics. figure part schematically illustrates embedding based correlation modeling. thus avoid expensive modeling pairwise topic correlation matrix enabled perform inference low-dimensional embedding space leading signicant reduction model inference complexity. exploit intrinsic sparsity topic occurrence develop stochastic variational inference fast sparsity-aware sampling enable high scalability. derive inference algorithm section contrast word representation learning word tokens observed embeddings induced directly word collocation paerns topics hidden text posing additional inferential challenge. resort generative framework conventional topic models associating word distribution topic. also take account uncertainty topic correlations exibility. addition intuitive geometric interpretation embedding based correlation scheme full bayesian treatment also endows connection classic model oering theoretical insights approach. present model structure next section. model structure establish notations. {wd}d collection documents. document contains words {wdn}nd vocabulary size assume topics underlying corpus. discussed above topic want learn compact distributed representation dimensionality rk×m denote topic vector collection common choice word embedding methods vector inner product measuring closeness embedding vectors. addition topic embeddings also induce document vectors vector space. denote embedding document conveniently compute anity topic nearby document topic thus semantically correlated topic naturally similar distance document ukad small. emerging line approaches incorporates word embeddings conventional topic models capturing word dependencies improving topic coherence. work diers since interested topic level aiming capturing topic dependencies learned topic embeddings. topic embedding model section proposes topic embedding model correlated topic modeling. give overview approach present model structure detail. derive ecient variational algorithm inference. overview develop expressive topic model discovers latent topics underlying correlation structures. despite added representational power want keep model parsimonious ecient order scale large text data. discussed captures correlations topic pairs gaussian covariance matrix imposing parameter size inference cost. contrast adopt modeling scheme drawing inspiration recent work distributed representations word embeddings learn low-dimensional word vectors shown eective encoding word semantic relatedness. induce continuous distributed representations latent topics word embeddings expect topics relevant semantics close embedding space. contiguity embedding space enables capture topical co-occurrence paerns conveniently—we embed documents vector space characterize document’s topic proportions distances topics. smaller distance indicates larger topic weight. triangle inequality distance metric intuitively express uncertainty anity modeling actual topic weights gaussian variable centered anity vector following characterizes uncertainty degree pre-specied simplicity. logisticnormal models project topic weights probability simplex obtain topic distribution somax sample topic word document. conventional topic models topic associated multinomial distribution word vocabulary observed word drawn respective word distribution indicated topic assignment. puing everything together generative process proposed model summarized algorithm theoretically appealing property method intrinsic connection conventional logistic-normal models model. marginalize document embedding variable obtain recovering pairwise topic correlation matrix rank constraint element closeness respective topic embeddings coherent geometric intuitions. covariance decomposition used context sparse gaussian processes ecient approximation gaussian reparameterization dierentiation reduced variance. relate low-dimensional embedding learning low-rank covariance decomposition estimation. low-dimensional representations latent topics enable parsimonious correlation modeling parameter complexity ecient terms topic number moreover allowed perform ecient inference embedding space inference cost linear huge advance compared previous cubic complexity vanilla quadratic recent improved version derive inference algorithm next section. inference posterior inference parameter estimation analytically tractable coupling latent variables nonconjugate logistic-normal prior. makes learning dicult especially context scaling unprecedentedly large data model sizes. develop stochastic variational method involves compact topic vectors cheap infer includes fast sampling strategy tackles non-conjugacy exploits intrinsic sparsity document topic occurrence topical words. pppp optimize coordinate ascent interleaving update variational parameters iteration. employ stochastic variational inference optimizes parameters stochastic gradients estimated data minibatchs. space limitations describe computation rules gradients stochastically estimated quantities used update variational parameters after scaled learning rate. please refer supplementary material detailed derivations. omied subscript variational covariance matrix independent intuitively optimal variational topic embeddings centers variational document embeddings scaled respective document topic weights transformed variational covariance matrix. involves variational expectations logistic transformation analytic form. construct fast monto carlo estimator approximation. particularly employ reparameterization trick assuming diagonal covariance matrix commonly used previous work denotes vector standard deviations resulting following sampling procedure indicator vector element rest practice usually sucient eective inference. second equation applies hard topic sample mentioned above reduces time complexity original standard computation equation) term second). term depends topic document embeddings encode topic correlations document’s topic weights. derivative w.r.t variational parameter computed collection variational means topic embeddings ˜uk· that low-dimensional topic document vector representations inferring topic correlations cost grows linearly w.r.t topic size. complexity remaining terms well respective derivatives w.r.t variational parameters complexity details). summary cost updating document finally optimal solution variational topic word distribution given algorithm summarization. summarize variational inference algorithm analyzed above time complexity variational method inferring topic embeddings cost document computing updating ond) maintaining overall complexity document thus ond) linear model size comparable model greatly improving previous correlation methods cubic quadratic complexity. variational inference algorithm endows rich independence structures variational parameters allowing straightforward parallel computing. implementation updates variational topic vector means {µk}k imposes complexity updating covariance requires similarly cost optimizing respectively. note shared across documents digamma function; variational means document’s topic weights variational word weights respectively. direct computation complexity becomes prohibitive presence many latent topics. address this exploit aspects intrinsic sparsity modeling ough whole corpus cover large diverse topics single document corpus usually small number them. thus maintain entries making complexity term right-hand side topics total; topic typically characterized words large vocabulary thus variational word weight vector maintaining entries sparse treatment helps enhance interpretability learned topics allows cheap computation average cost second term. sparsity-aware updates resulting complexity topics brought great speedup original cost. entries selected using min-heap data structure whose computational cost amortized across words document imposing computation word. cost nding entries similarly amortized across documents words becomes insignicant. updating remaining variational parameters frequently involve computation variational expectations thus crucial speedup operation. employ sparse approximation sampling single indicator hard sparse distribution estimate expectations. note sampling operation cheap complexity computing above. shown shortly sparse computation signicantly reduce running cost. ough stochastic expectation approximation commonly used tackling intractability instead apply technique fast estimation tractable expectations. variational topic embeddings {µk} topic word distributions {λk} document embeddings {γd} data minibatch computed parallel across multiple cores. experiments demonstrate ecacy approach extensive experiments. evaluate extraction quality tasks document classication retrieval model achieves similar beer performance existing correlated topic models signicantly improving simple lda. scalability approach scales comparably handles massive problem sizes orders-of-magnitude larger previously reported correlation results. alitatively model reveals meaningful topic correlation structures. setup datasets. three public corpora provided repository evaluation newsgroups collection news documents partitioned evenly across dierent newsgroups. article associated category label serving ground truth tasks document classication retrieval; nytimes widely-used large corpus york times news articles; pubmed large pubmed abstracts. detailed statistics datasets listed table removed standard list stop words performed stemming. nytimes pubmed kept frequent words vocabulary selected documents uniformly random test sets respectively. newsgroups followed standard training/test spliing performed widely-used pre-processing removing indicative meta text headers footers document classication forced based semantics plain text. latent dirichlet allocation uses conjugate dirichlet priors thus scales linearly w.r.t topic size fails capture topic correlations. inference based stochastic variational algorithm evaluating scalability leverage sparsity assumptions model speeding correlated topic model employs standard logistic-normal prior captures pairwise topic correlations. model uses stochastic variational inference developed scalable sparse gibbs sampler inference time complexity using distributed inference machines method discovers topics millions documents knowledge largest automatically learned topic correlation structures far. parameter setting. roughout experiments embedding dimension sparseness parameters found modeling quality robust parameters. following common practice hyperparameters baselines using similar hyper-parameter seings. document classication evaluate performance document classication based learned document representations. evaluate newsgroups dataset ground truth class labels available. compare proposed model ctm. multi-class classier trained based topic distributions training documents proposed model classier takes document embedding vectors input. generally accurate modeling topic correlations enables beer document modeling representations resulting improved document classication accuracy. figure shows classication accuracy number topics varies. proposed model performs best cases indicating method discover high-quality latent topics correlations. model signicantly outperforms treats latent topics independently validating importance topic correlation accurate text semantic modeling. compared method achieves beer competitive accuracy varies indicates model though orders-of-magnitude faster sacrice modeling power compared complicated computationally demanding model. document retrieval evaluate topic modeling quality measuring performance document retrieval newsgroups dataset. retrieved document relevant query document class label. document similarity measured inner product topic distributions model inner product document embedding vectors. figure shows retrieval results varying number topics test query documents retrieve similar documents training results averaged possible queries. observe similar paerns document classication task. model obtains competitive performance capture topic correlations greatly improve lda. validates goal proposed method lower modeling complexity time accurate powerful previous complicated correlation models. addition ecient model inference learning approach based compact document embedding vectors also enables faster document retrieval compared conventional topic models based topic distribution vectors achieves similar beer level performance conventional complicated correlated topic model want approach tackle large problem sizes impossible existing correlation methods scale eciently lightweight practical deployment. table compares total running time model training dierent sized datasets models. common practice determine convergence training dierence test per-word log-likelihoods consecutive iterations smaller threshold. small dataset like newsgroups small model approaches nish training reasonable time. however increasing number documents latent topics vanilla model inference complexity) scalable version s-ctm inference complexity) quickly becomes impractical limiting deployment real-world scale tasks. proposed topic embedding method contrast scales linearly topic size capable handling topics documents problem size several orders magnitude larger previously reported largest results notably even added model power increased extraction performance compared model imposes negligible additional training time showing strong potential method practical deployment real-world large-scale applications does. figure panel shows convergence curves nytimes training goes. using similar time model converges beer point does s-ctm much slower failing arrive convergence within time frame. figure convergence nytimes topics. middle total training time newsgroups. right runtime inference iteration minibatch nytimes articles result points s-ctm large omitted fail nish iteration within hours. figure portion topic correlation graph learned newsgroups. node denotes latent topic whose semantic meaning characterized words according topic’s word distribution. font size word proportional word weight. topics correlation strength threshold connected edges. thickness edges proportional correlation strengths. figure middle panel measures total training time varying number topics. small newsgroups dataset since larger data sctm models usually slow converge reasonable time. training time increases quickly topics used. s-ctm works well small data model scale shown above incapable tackling larger problems. contrast approach scales eciently simpler model. figure right panel evaluates runtime inference iteration minibatch documents. topic size grows large number s-ctm fail nish iteration hours. model contrast keeps scalable considerably speeds s-ctm. figure visualizes topic correlation graph inferred newsgroups dataset. many topics strongly correlated exhibit clear correlation structure. instance topics right upper region mainly astronomy interrelated closely connections information security topics shown lower part weak. figure shows topic embeddings correlations pubmed dataset. related topics close embedding space revealing diverse substructures themes collection. model discovers meaningful structures providing insights semantics underlying large text corpora facilitating understanding large collection topics. conclusions developed correlated topic model induces distributed vector representations latent topics characterizes correlations closeness topic vectors embedding jianfei chen kaiwei wenguang chen. warplda simple ecient algorithm latent dirichlet allocation. vldb. jianfei chen wang zheng zhang. scalable inference logistic-normal topic models. nips. prasoon goyal zhiting xiaodan liang chenyu wang eric xing. nonparametric variational auto-encoders hierarchical representation learning. arxiv preprint arxiv. yuezhang ronghuo zheng tian tian zhiting rahul iyer katia sycara. joint embedding hierarchical categories entities concept categorization dataless classication. coling. latent dirichlet allocation. arxiv preprint arxiv. john paisley chong wang david blei discrete innite logistic normal distribution. bayesian analysis wang xuemin zhao zhenlong lifeng wang zhihui liubin wang yang ching zeng. peacock learning long-tail topic features industrial applications. tist stochastic variational deep kernel learning. nips. jinhui yuan qirong jinliang zheng eric xing tie-yan wei-ying lightlda topic models modest computer clusters. www. space. modeling scheme along sparsity-aware sampling inference enables highly ecient model training linear time complexity terms model size. approach scales unprecedentedly large data models achieving strong performance document classication retrieval. proposed correlation method generally applicable context modeling word dependencies improved topical coherence. interesting speedup model inference variational neural bayes techniques amortized variational updates across data examples. note model particularly suitable incorporate neural inference networks that replacing per-document variational embedding distributions documents compact document embeddings directly. also interested combining generative topic models advanced deep text generative approaches improved text modeling. sparsity-aware topic sampling. direct computation complexity becomes prohibitive presence many latent topics. address this exploit aspects intrinsic sparsity modeling ough whole corpus cover large diverse topics single document corpus usually small number them. thus maintain entries making complexity term right-hand side topics total; topic typically characterized words large vocabulary thus variational word weight vector maintaining entries sparse treatment helps enhance interpretability learned topics allows cheap computation average cost second term. sparsity-aware updates resulting complexity topics brought great speedup original cost. entries selected using min-heap data structure whose computational cost amortized across words document imposing computation word. cost nding entries similarly amortized across documents words becomes insignicant. besides updating remaining variational parameters frequently involve computation variational expectations thus crucial speedup operation. employ sparse approximation sampling single indicator ˜zdn hard sparse distribution estimate expectations. note sampling operation cheap complexity computing above. shown shortly sparse computation signicantly reduce running cost. optimize topic isolate terms contain", "year": 2017}