{"title": "Complex Embeddings for Simple Link Prediction", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.", "text": "th´eo trouillon johannes welbl sebastian riedel eric gaussier guillaume bouchard xerox research centre europe chemin maupertuis meylan france universit´e grenoble alpes avenue centrale saint martin d’h`eres france university college london gower london united kingdom statistical relational learning link prediction problem automatically understand structure large knowledge bases. previous studies propose solve problem latent factorization. however make complex valued embeddings. composition complex embeddings handle large variety binary relations among symmetric antisymmetric relations. compared state-of-the-art models neural tensor network holographic embeddings approach based complex embeddings arguably simpler uses hermitian product complex counterpart standard product real vectors. approach scalable large datasets remains linear space time consistently outperforming alternative approaches standard link prediction benchmarks. web-scale knowledge bases provide structured representation world knowledge projects dbpedia freebase google knowledge vault enable wide range applications recommender systems question answering automated personal agents. incompleteness stimulated express data directed graph labeled edges nodes natural redundancies among recorded relations often make possible missing entries example relation countryofbirth recorded entities easily inferred relation cityofbirth known. goal link prediction automatic discovery regularities. however many relations non-deterministic combination facts isbornin islocatedin always imply fact hasnationality. hence required handle facts involving relations entities probabilistic fashion. increasingly popular method state link prediction task binary tensor completion problem slice adjacency matrix relation type knowledge graph. completion based low-rank factorization embeddings popularized netﬂix challenge partially observed matrix tensor decomposed product embedding matrices much smaller rank resulting ﬁxed-dimensional vector representations entity relation database. given fact subject linked object relation score recovered multi-linear product embedding vectors binary relations exhibit various types pathierarchies compositions like fatherof terns partial/total olderthan strict/non-strict relations like issimilarto. described bordes relational model able learn combinations properties namely reﬂexivity/irreﬂexivity symmetry/antisymmetry transitivity linear time memory order scale size present keep growth. products embeddings scale well naturally handle symmetry reﬂexivity relations; using appropriate loss function even enables transitivity however dealing antisymmetric relations almost always implied explosion number parameters making models prone overﬁtting. finding best ratio expressiveness parameter space size keystone embedding models. work argue standard product embeddings effective composition function provided uses right representation. instead using embeddings containing real numbers discuss demonstrate capabilities complex embeddings. using complex vectors i.e. vectors entries product often called hermitian product involves conjugate-transpose vectors. consequence product symmetric more facts antisymmetric relations receive different scores depending ordering entities involved. thus complex vectors effectively capture antisymmetric relations retaining efﬁciency beneﬁts product linearity space time complexity. remainder paper organized follows. ﬁrst justify intuition using complex embeddings square matrix case single relation entities. formulation extended stacked square matrices third-order tensor represent multiple relations. describe experiments large scale public benchmark empirically show representation leads simpler faster algorithms also gives systematic accuracy improvement current state-of-the-art alternatives. give clear comparison respect existing approaches using real numbers also present equivalent reformulation model involves real embeddings. help practitioners implementing method without requiring complex numbers software implementation. understanding factorization complex space leads better theoretical understanding class matrices actually approximated products embeddings. so-called normal matrices left right embeddings share unitary basis. modelling relations entities relation entities represented binary value subject relation object. probability given logistic inverse link function goal generic structure leads ﬂexible approximation common relations real world kbs. standard matrix factorization approximates matrix product functionally independent matrices rank matrix. within formulation assumed entities appearing subjects different entities appearing objects. means entity different embedding vectors depending whether appears subject object relation. extensively studied type model closely related singular value decomposition well case matrix rectangular. however many link prediction problems entity appear subject object. seems natural learn joint embeddings entities entails sharing embeddings left right factors proposed several authors solve link prediction problem order embedding subjects objects researchers generalised notion products scoring functions also known composition functions combine embeddings speciﬁc ways. brieﬂy recall several examples scoring functions table well extension proposed paper. often used approximate real symmetric matrices covariance matrices kernel functions distance similarity matrices. cases eigenvalues eigenvectors live real space orthogonal table scoring functions state-of-the-art latent factor models given fact along relation parameters time space complexity. embeddings subject object model except model additional latent dimension model. denote respectively fourier transform inverse element-wise product vectors. work however explicitly interested problems matrices thus relations represent also antisymmetric. case eigenvalue decomposition possible real space; exists decomposition complex space embeddings composed real vector component imaginary vector component complex numbers product also called hermitian product sesquilinear form deﬁned complex-valued vectors i.e. corresponding real imaginary parts vector denoting square root crucial operation take conjugate ﬁrst vector iim. simple justify hermitian product composing complex vectors provides valid topological norm induced vectorial space. example implies case bilinear form many complex vectors even complex eigenvectors cn×n inversion eigendecomposition equation leads computational issues. fortunately mathematicians deﬁned appropriate class matrices prevents inverting eigenvector matrix consider space normal matrices i.e. complex matrices spectral theorem normal matrices states matrix normal unitarily diagonalizable hierarchical relations isolder) well orthogonal matrices many matrices useful represent binary relations assignment matrices represent bipartite graphs. however matrices expressed purely real equation requires scores purely real. simply keep real part decomposition eigenvalues necessarily positive real; factorization useful rows used vectorial representations entities corresponding rows columns relation matrix indeed given entity subject embedding vector complex conjugate object embedding vector. link prediction problem relation matrix unknown goal recover entirely noisy observations. enable model learnable i.e. generalize unobserved links regularity assumptions needed. since deal binary relations assume sign-rank. sign-rank sign matrix smallest rank real matrix sign-pattern scoring function typically based factorization observed relations denotes parameters corresponding model. whole unknown assume observe true false facts {yrso}r∈ω }|ω| corresponding partially observed adjacency matrices different relations observed triples. goal probabilities entries yrso true false targeted unobserved triples depending scoring function used predict entries tensor obtain different models. examples scoring functions given table model scoring function easily check function antisymmetric purely imaginary symmetric real. interestingly separating real imaginary part relation embedding obtain decomposition relation matrix symmetric matrix antisymmetric matrix relation embeddings naturally weights latent dimension symmetric real part antisymmetric imaginary part indeed meaning reeo symmetric imeo antisymmetric. enables accurately describe theoretically justiﬁed fact signrank natural complexity measure sign matrices linked learnability empirically conﬁrmed wide success factorization models although twice sounds actually good upper bound. indeed sign-rank often much lower rank example rank identity matrix rank± permutation columns matrix corresponds relation marriedto relation known hard factorize model express rank imposing low-rank ﬁrst values diag non-zero. directly cn×k ck×k. individual relation scores entities predicted following product embeddings previous section focused modeling single type relation; extend model multiple types relations. allocating embedding relation sharing entity embeddings across relations. relations entities present want recover matrices scores relations given entities log-odd probability fact true order evaluate proposal conducted experiments synthetic real datasets. synthetic dataset based relations either symmetric antisymmetric whereas real datasets comprise different types relations found different standard kbs. refer model complex complex embeddings. assess ability proposal accurately model symmetry antisymmetry randomly generated relations entities. relation entirely symmetric completely antisymmetric. dataset corresponds tensor. figure shows part randomly generated tensor symmetric slice antisymmetric slice decomposed training validation test sets. diagonal unobserved relevant experiment. train contains observed triples whereas validation test sets contain triples each. figure shows best cross-validated average precision different factorization models ranks ranging models trained using stochastic gradient descent mini-batches adagrad tuning learning rate minimizing negative log-likelihood logistic model regularization parameters considered model model corresponds embeddings describe full algorithm appendix validated expected distmult able model antisymmetry predicts symmetric relations correctly. although transe symmetric model performs poorly practice particularly antisymmetric relation. rescal large number parameters quickly overﬁts rank grows. canonical polyadic decomposition fails figure parts training validation test sets generated experiment symmetric antisymmetric relation. pixels positive triples blue negatives green missing ones. plots symmetric slice ﬁrst entities. bottom plots antisymmetric slice ﬁrst entities. next evaluate performance model datasets. subset freebase curated general facts whereas subset wordnet database featuring lexical relations between words. original training validation test splits provided bordes table summarizes metadata datasets. datasets contain positive triples. bordes generated negatives using local closed world assumption. triple randomly change either subject object random form negative example. negative sampling performed runtime batch training positive examples. evaluation measure quality ranking test triple among possible subject object substitutions ∀s∀o mean reciprocal rank hits standard evaluation measures datasets come ﬂavours ﬁltered ﬁltered metrics figure average precision factorization rank ranging different state models combined symmetry antisymmetry experiment. top-left symmetric relation only. top-right antisymmetric relation only. bottom overall since ranking measures used previous studies generally preferred pairwise ranking loss task chose negative log-likelihood logistic model continuous surrogate sign-rank shown learn compact representations several important relations especially transitive relations preliminary work tried losses indeed loglikelihood yielded better results ranking loss especially fbk. report ﬁltered ﬁltered hits table evaluated models. furthermore chose transe distmult hole baselines since best performing models datasets best knowledge also compare model emphasize empirically importance learning unique embeddings entities. experimental fairness reimplemented methods within framework complex model using theano however describes lexical semantic hierarchies concepts contains many antisymmetric relations hypernymy hyponymy part indeed distmult transe models outperformed complex hole respective ﬁltered scores table shows ﬁltered test models considered relation conﬁrming advantage model antisymmetric relations losing nothing others. projections relation embeddings provided appendix visually corroborate results. much pronounced complex model largely outperforms hole ﬁltered hits compared hole. attribute simplicity model different loss function. supported relatively small compared distmult model fact interpreted complex number version distmult. datasets transe complex distmult transe relation name hypernym hyponym member meronym member holonym instance hypernym instance hyponym part part member domain topic synset domain topic member domain usage synset domain usage member domain region synset domain region derivationally related form similar verb group also largely left behind. illustrates power simple product ﬁrst case importance learning unique entity embeddings second. performs poorly small number relations magniﬁes subject/object difference. reported results hyper-parameters following values regularization parameter initial learning rate number negatives generated positive training triple. also tried varying batch size impact settled batches epoch. best ranks generally cases scores always close models. number negative samples positive sample also large inﬂuence ﬁltered much datasets regularization important found initial learning rate important much think also explain large improvement model provides dataset compared previously published results distmult results also better previously reported along log-likelihood objective. seems general adagrad relatively insensitive initial learning rate perhaps causing overconﬁdence ability tune step size online consequently leading less efforts selecting initial step size. investigated inﬂuence number negatives generated positive training sample. previous experiment computational limitations number negatives training sample validated among possible numbers want explore whether increasing numbers could lead better results. focused best validated obtained previous experiment. vary figure shows inﬂuence number generated negatives positive training triple performance model fbk. generating negatives clearly improves results ﬁltered negative triples decreasing negatives. model also converges fewer epochs compensates partially additional training time epoch negatives. grows linearly number negatives increases making good trade-off accuracy training time. original multi-linear distmult model symmetric subject object every relation achieves good performance presumably simplicity. transe model bordes also embeds entities relations space imposes geometrical structural bias model subject entity vector close object entity vector translated relation vector. recent novel handle antisymmetry holographic embeddings model hole circular correlation used combining entity embeddings measuring covariance embeddings different dimension shifts. generally suggests composition functions classical tensor product helpful allow richer interaction embeddings. however asymmetry composition function hole stems asymmetry circular correlation operation whereas inherited complex inner product described simple approach matrix tensor factorization link prediction data uses vectors complex values retains mathematical deﬁnition product. class normal matrices natural binary relations using real part allows efﬁcient approximation learnable relation. results standard benchmarks show modiﬁcations needed improve state-of-the-art. several directions work extended. obvious merge approach known extensions tensor factorization order further improve predictive performance. example pairwise embeddings together complex numbers might lead improved results many situations involve non-compositionality. another direction would develop intelligent negative sampling procedure generate informative negatives respect positive sample sampled. would reduce number negatives required reach good performance thus accelerating training time. figure inﬂuence number negative triples generated positive training example ﬁltered test training time convergence complex model times given relative training time negative triple generated positive training sample early spectral theory linear algebra complex numbers used matrix factorization mathematicians mostly focused bi-linear forms eigen-decomposition complex domain taught today linear algebra courses came years later similarly existing approaches tensor factorization based decompositions real domain canonical polyadic decomposition methods effective many applications different modes tensor different types entities. link prediction problem antisymmetry relations quickly seen problem asymmetric extensions tensors studied mostly either considering independent embeddings considering relations matrices instead vectors rescal model direct extensions based uni-bitrigram latent factors triple data well low-rank relation matrix pairwise interaction models also considered improve prediction performances. example universal schema approach factorizes unfolding tensor welbl extend also pairs. neural tensor network model socher combine linear transformations multiple bilinear forms subject object embeddings jointly feed nonlinear neural layer. non-linearity multiple ways including interactions embeddings gives advantage expressiveness models auer sren bizer christian kobilarov georgi lehmann jens ives zachary. dbpedia nucleus open data. intl semantic conference busan korea springer bergstra james breuleux olivier bastien fr´ed´eric lamblin pascal pascanu razvan desjardins guillaume turian joseph warde-farley david bengio yoshua. theano math expression proceedings python scientiﬁc compiler. computing conference june oral presentation. bollacker kurt evans colin paritosh praveen sturge taylor jamie. freebase collaboratively created graph database structuring human knowledge. sigmod proceedings sigmod international conference management data bordes antoine usunier nicolas garcia-duran alberto weston jason yakhnenko oksana. translating embeddings modeling multi-relational data. advances neural information processing systems bouchard guillaume singh sameer trouillon th´eo. approximate reasoning capabilities low-rank vector spaces. aaai spring syposium knowledge representation reasoning integrating symbolic neural approaches shaohua zhang wei. knowledge vault web-scale approach probabilistic knowledge fusion. proceedings sigkdd international conference knowledge discovery data mining jenatton rodolphe bordes antoine roux nicolas obozinski guillaume. latent factor model highly multi-relational data. advances neural information processing systems nickel maximilian tresp volker kriegel hanspeter. three-way model collective learning multi-relational data. international conference machine learning nickel maximilian jiang xueyan tresp volker. reducing rank relational factorization models including observable patterns. advances neural information processing systems nickel maximilian rosasco lorenzo poggio tomaso holographic embeddings knowledge graphs. proceedings thirtieth aaai conference artiﬁcial intelligence riedel sebastian limin mccallum andrew marlin benjamin relation extraction matrix human lanfactorization universal schemas. guage technologies conference north american chapter association computational linguistics proceedings socher richard chen danqi manning christopher andrew. reasoning neural tensor networks knowledge base completion. advances neural information processing systems sutskever ilya. modelling relational data using bayesian clustered tensor factorization. advances neural information processing systems volume welbl johannes bouchard guillaume riedel sebastian. factorization machine framework testing bigram embeddings knowledgebase completion. arxiv. yang bishan wen-tau xiaodong jianfeng deng embedding entities relations learning inference knowledge bases. international conference learning representations algorithm complex model input training validation learning rate embedding dim. regularization factor negative ratio batch size iter early stopping randn randn ..|ω|/b algorithm describes formulation scoring function. contains positive triples generate negatives positive train triple corrupting either subject object positive triple described bordes used principal component analysis visualize embeddings relations wordnet dataset plotted four ﬁrst components best distmult complex model’s embeddings figure complex model simply concatenated real imaginary parts embedding. relations describe hierarchies hierarchic thus antisymmetric. relation dataset. relations hypernym hyponym part example member domain topic. since distmult unable model antisymmetry correctly represent nature pair opposite relations direction relations. loosely speaking hypernym hyponym pair nature sharing semantics direction entity generalizes semantics other. makes distmult reprensenting opposite figure plots ﬁrst second third fourth components relations embeddings using pca. left distmult embeddings. right complex embeddings. opposite relations clustered together distmult correctly separated complex. relations close embeddings figure shows. especially striking third fourth principal component conversely complex manages oppose spatially opposite relations.", "year": 2016}