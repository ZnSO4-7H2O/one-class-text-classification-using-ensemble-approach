{"title": "Extreme State Aggregation Beyond MDPs", "tag": ["cs.AI", "cs.LG"], "abstract": "We consider a Reinforcement Learning setup where an agent interacts with an environment in observation-reward-action cycles without any (esp.\\ MDP) assumptions on the environment. State aggregation and more generally feature reinforcement learning is concerned with mapping histories/raw-states to reduced/aggregated states. The idea behind both is that the resulting reduced process (approximately) forms a small stationary finite-state MDP, which can then be efficiently solved or learnt. We considerably generalize existing aggregation results by showing that even if the reduced process is not an MDP, the (q-)value functions and (optimal) policies of an associated MDP with same state-space size solve the original problem, as long as the solution can approximately be represented as a function of the reduced states. This implies an upper bound on the required state space size that holds uniformly for all RL problems. It may also explain why RL algorithms designed for MDPs sometimes perform well beyond MDPs.", "text": "consider reinforcement learning setup agent interacts environment observation-reward-action cycles without assumptions environment. state aggregation generally feature reinforcement learning concerned mapping histories/raw-states reduced/aggregated states. idea behind resulting reduced process forms small stationary ﬁnite-state eﬃciently solved learnt. considerably generalize existing aggregation results showing even reduced process value functions policies associated state-space size solve original problem long solution approximately represented function reduced states. implies upper bound required state space size holds uniformly problems. also explain algorithms designed mdps sometimes perform well beyond mdps. introduction feature markov decision processes exact aggregation approximate aggregation general approximate aggregation results extreme aggregation reinforcement learning feature reinforcement learning miscellaneous discussion references list notation reinforcement learning agent takes actions environment observes consequences rewarded them. well-understood eﬃciently solvable eﬃciently learnable case environment ﬁnite-state stationary markov decision process unfortunately interesting real-world problems neither ﬁnitestate stationary markov. dealing mismatch somehow transform real-world problem small feature reinforcement learning u-tree deal case arbitrary unknown environments state aggregation assumes environment large known stationary former maps histories states latter groups states aggregated states. follow approach terminology since arguably general subsumes cases original process k-order pomdp others thinking terms histories also naturally stiﬂes temptation naive frequency estimate finally history state terminologically somewhat neater state aggregated state. importantly consider maps histories states reduced process ﬁrst seems defeat original purpose namely reducing well-understood eﬃciently solvable problem class namely small mdps. main novel contribution paper show still associated ﬁnite-state stationary whose solution solves original problem long solution still represented indeed provide upper bound required state space size holds uniformly interesting theoretical insights a-priori clear whether could utilized design algorithms. also show learn experience sketch overall learning algorithm regret/pac analysis based main theorems brieﬂy discuss relax conditions main theorems permuting actions conclude outlook future work open problems list notation found appendix section formally describes setup consists agentenvironment framework maps observation-reward-action histories states. arrangement called feature short φmdp. upper-case letters probability value policy original process lower-case letters probability value policy mdp. agent-environment setup start standard agentenvironment setup agent interacts environment agent choose actions environment provides observations real-valued rewards agent. happens cycles time observing receiving reward agent takes action based history next cycle starts. agent’s objective maximize long-term reward. avoid integrals densities assume spaces ﬁnite. huge really restrictive. indeed φmdp framework speciﬁcally developed huge observation spaces. generalization continuous routine furthermore assume ﬁnite smallish restrictive. potential extensions continuous discussed section agent environment viewed pair interlocking functions indicates mappings general stochastic. make assumption environment parts environment assumed ﬁxed dependencies suppressed. convenience since optimal policies chosen deterministic consider deterministic policies only. value functions optimal policies history bellman equations. measure performance policy terms -expected γ-discounted reward called value policy history maximum policies always exists unique case argmax denotes optimal policies denotes representative whole optimal policies. despite history-based write recursive bellman equations values unlike classical state-space cousins self-consistency equations r.h.s. refers longer history always diﬀerent history l.h.s precludes learning algorithm based estimating frequency state/history visits. still recursions convenient mathematical development. histories states space histories huge unwieldy history ever repeats. standard ways dealing deﬁne similarity metric histories aggregate histories pursue latter feature reduces histories states w.l.g. assume surjective. also assume state space ﬁnite; indeed interested small corresponds indeed equivalent partitioning histories ∈s}. classical state aggregation usually uses partitioning view notation convenient here. state supposed summarize relevant information history lower bounds size pass complete history ora...onrn ‘reduced’ history sra...snrn. traditionally ‘relevant’ means future predictable alone technically reduced history forms markov decision process. precisely condition paper intends lift elsewhere quantiﬁer shall mean values involved variables consistent constraint φ=st. assumed stationary i.e. independent another condition lifted later. condition essentially stochastic bisimulation condition generalized histories somewhat restrictive regarding rewards condition reward distribution constrains expectation only. could easily rectiﬁed besides point paper. bisimulation metric approximate version measures deviation mdp. many problems reduced stationary mdps full-information games chess static opponent already markov classical physics approximately nd-order markov i.i.d. processes bandits counting suﬃcient statistics pomdp planning problem belief vector markov. markov decision processes used continue uppercase letters general process lower-case letters mdps current state action successor state reward. consider stationary ﬁnite-state stationary deterministic policy section given general diﬀerent case p-expected γ-discounted reward called value policy given bellman equations notation. equations often assume imply technically sas′r′ diﬀerent variables variables history ora...otrtatot+rt+...anrn. less prone confusion hao′r′. show reduces solution equations yields values optimal policy original process surprising history-based versions classical state-aggregation results state prove here since notation setup somewhat diﬀerent proof ideas fragments reused later. following theorems show reduces stationary essentially coincide policy assumed constant within partition allows eﬃciently solve time polynomial solving/learning instead note well-deﬁned since surjective standard proof considers m-horizon truncated induction besides adaptation histories proof slight variation avoids truncation limit. style useful later. explain steps detail here since variations utilize later. show q∗±γδ following exactly steps replaced using instead using bellman optimality equations instead bellman equations also before implies hence hence finally latter implies argmaxaq∗ argmaxaq∗ section prepares main technical contribution paper next section. quantity relate original reduced bellman equations form stochastic inverse whose choice analysis deferred section dispersion probability probability distribution ﬁnite histories state-action pair viewed stochastic inverse assigns non-zero probability formal constraints pose implicitly requires surjective i.e. always made true deﬁning note taken histories any/mixed length. general somewhat weird distribution since assigns probabilities past future observations given current state action. interpretation choice need concern except later want learn ﬁnite-state stationary built feature dispersion probability environment p-probability observing state-reward pair state-action pair deﬁned b-average histories consistent pφ-probability observing given history action r.h.s. ﬁrst line merely shorthand second line. note sas′r′ ﬁxed appear ranges histories lengths. easy probability distribution markov deﬁnition. deﬁnition coincides deﬁned general depending arbitrary state distribution induced general non-markov. note stationary satisfying need following lemmas inequalities trivially bound diﬀerences terms diﬀerences |v−v maxa|q−q|. following lemma shows reverse holds expectation i.e. |q−hqib| γ|v−v expectation dropped constant formally deﬁne section contains main technical contribution paper. show histories aggregated modeled even true aggregated process actually mdp. necessary condition successful aggregation course quantities interest namely value functions policies represented functions aggregated states. results section roughly show necessary condition signiﬁcantly weaker requirement also suﬃcient. result also holds approximate aggregation i.e. approximate conditions lead approximate reductions. also lift stationarity assumption. theorem shows φ-uniform obtain similar somewhat weaker results. proof latter involves extra complications present three proofs. indeed whether arguably desirable bound holds open problem example. consider process observations transition matrix oo′rar′ reward function i.e. example right special form too′ actionindependent markov process deterministic reward function read diagram. observation space consider reduction proof. proofs start routine warning care order recycling similar proofs. theorem relies assumption lucky proof theorem worked without knowing advance. work harder. primarily interested optimal policy correctly represent value indirect interest. φ-uniform represented φ-uniformity condition theorem dropped conclusion fail following example shows. counter example. states actions {αβ} formally deﬁned left depicted right policy highest value therefore aggregate states -state mdp. value stationary distribution ρ⊤=ρ⊤t particular since aggregated state stationary policies action. leads hence ∀sh. despite constant shows condition theorem cannot dropped. note constant. continuity policy reversal also holds indeed example works immediately follows theorem since case continuity argument might allow establish bound small theorem i&iii mostly carried theorem a-priori implausible theorem carries hand proofs theorems suﬃciently diﬀerent analogy argument weak. pair inequalities implies lower bounds expectation therefore must constant equal φ−-partition. high probability cannot much smaller weren’t probability qualiﬁer could apply lemma establish probability events could invalidate argument. discussion. open problem would main result proof absent content theorem statements imply aggregate histories much wish long optimal value function policy still approximately representable functions aggregated states. whether reduced process markov immaterial. surrogate ε-optimal policy work including state aggregation formulated terms mdps i.e. original process already mdp. call original mdp. could interpret whole history state formally makes every normally observations identiﬁed states i.e. case etc. depends states =ot). since results hold clearly hold maps states aggregated states. results section showed histories aggregated modeled even true aggregated process mdp. restrictions value functions policies could still represented functions aggregated states. section theory allows represent process small ﬁnite-state mdp. holds extreme aggregation based open problem aggregate even better consider maps history optimal value discretized ﬁnite ε-grid optimal action last inequality assumed theorem trivial since policy ε′-optimal). deﬁned derivation similar. theorem follows considerations paragraphs theorem. discussion. valid question course whether theorem interesting theoretical insight/curiosity practical use. depends knew would readily available detour pointless. theorem reaches relevance following observation start suﬃciently rich class maps contains least approximately representing learning algorithm favors theorems tell need worry whether not; simply use/learn instead. theorem tells allows extreme aggregation beyond mdps. behavior policy behavior policy agent general non-stationary learning often stochastic ensure exploration diﬀerent policy considered note sequence policies ππ... learnt used time nothing single non-stationary policy πt∀tht indeed includes case policy learning. choice interaction agent environment stochastically generates history followed action joint probability subscripts and/or indicate dependence and/or natural choice would condition stat. show work problem. several useful distributions follows close required condition crucially diﬀerent. histories lengths limited histories length easy miss diﬀerence compact notation. technically probability measure inﬁnite sequences short inﬁnite histories starting i.e. probability w-weighted time-average second also introduced shorthand pφb. stationary i.e. independent choice since weights estimation easy. note general cannot estimate non-stationary since sample available estimation still possible. assume observed choose discussion. limit shows standard frequency estimation converge true weak conditions. samples conditionally i.i.d. ‘weak conditions’ satisﬁed. large numbers hence holds beyond i.i.d. case e.g. stationary ergodic processes. condition every state-action pair visited non-vanishing relative frequency signiﬁcantly relaxed. stationarity also necessary indeed often hold non-stationary environment nonstationary behavior policy idea learn starts class maps compares diﬀerent selects appropriate given experience far. several criteria based well reduces devised theoretically experimentally investigated theorems show demanding approximately overly restrictive. theorem suggests relax condition much substantial aggregation possible provided rich enough. deals case unknown ﬁrst discuss learning unrealistic case exact aggregation inﬁnite sample size serves useful guide work generalization realistic signiﬁcantly complex case approximate aggregation based ﬁnite sample size. finally discuss family recent algorithms appear nearly right properties purpose. section collection ideas outlook towards algorithms exploiting motivating usefulness insights obtained previous sections. search exact based inﬁnite sample size. since concerned comparing diﬀerent subscribe quantities necessary. consider unrealistic case inﬁnite sample size search exact reductions call reduction exact even hence needed estimable hand p=pφ determined determine solution always satisﬁes reduced bellman equations exactly even reductions e.g. single state reduced problem suﬃcient judge quality alternative assuming assume known also allows determine etc. follows applies stochastic planning well. coarsening reﬁning reductions coarsen i.e. merge partitions simplest case merge states one. general consider coarsening coarser reduction also call reﬁnement example u-trees kd-trees used expanding leaf corresponds splitting state. φdbn binary feature vector removing component corresponds pairwise combining states states means better reduction since leads optimal qvalue policy does parsimonious constant ψ-partitions coarsening using leads suboptimal solutions. enriching order transitive ‘very’ partial order. maps incomparable neither reﬁnement other. enrich order follows maps ×sψ′ reﬁnes both. deﬁne ≺ψ′. extended order still total. remaining incomparable cases case ≺φ≻ψ′ possible coincide. secondary criterion based relative complexity could decide case e.g. |sψ|<|sψ′|. case ≻φ≺ψ′ inferior class closed cartesian product favored relative order less important. search assume contains least exact reduction. ≺×minimal elements exactly maximally coarse exact closed arbitrary coarsening unique minimizer also closed cartesian product holds implies exhaustive search ≺×-minimum give exact minimal number states theorem tells optimal value policy also original process irrespective whether markov not. conditions theorem cannot veriﬁed practice theorem justiﬁes search procedure based ignores markov structure search approximate based ﬁnite sample size. principle approach previous paragraph sound needs generalized various ways used real sample size ﬁnite means access approximations estimation criterion exact equality done anyway since real-word problems seldom allow exact reductions. chosen come statistical guarantees; e.g. kolmogorov-smirnov tests used suitable requires eﬀort large also requires appropriate regularization i.e. penalizing complex ensure need proper exploration strategies finally want eﬃcient search procedure rather exhaustive search. heuristic require strong assumptions last point raised general solutions family uses basic setup used also here. authors consider countable class assumed contain least consider average reward rather discounting analyze regret requires assumption mixing rate ‘diameter’ mdp. prove total regret grows depending algorithm. algorithms analyses rely ucrl exploration algorithm ﬁnite-state mdps. going proofs appears condition removed used instead modulo analysis ucrl itself. proofs bounds ucrl exploit s′r′ conditioned i.i.d. true markov general. asymptotic versions remain valid ‘weak conditions’ alluded stronger assumptions guarantee good convergence rates regret analysis ucrl remain valid too. formally hoeﬀding’s inequality i.i.d. need replaced comparable bounds weaker conditions e.g. azuma’s inequality martingales. serious argument above. uses average reward theorems discounted reward. often possible adapt algorithms proofs come regret bounds average reward bounds discounted reward vice versa. would done ﬁrst either version combining merl ucrlγ average reward versions bounds derived paper. action permutation instead policy condition. rename actions without changing underlying problem bijection deﬁne clearly results also hold replaced everywhere particular general little use. things become interesting allow bijection historydependent since results hold even non-stationarity allows devise a;h) =constant policy interest. example achieved permutation swaps action arbitrary ﬁxed action leaves actions unchanged since a;h) constant φ-uniformity condition theorems becomes vacuous. transformation theoretical interest becomes practically useful somehow learn function without knowledge particular could also allow non-bijective merge actions q-value. summary. results show algorithms ﬁnite-state mdps utilized even problems arbitrary history dependence historyto-state reductions/aggregations induce also neither stationary mdp. condition placed reduction quantities interest values policies approximately represented. considerably generalizes previous work feature reinforcement learning state aggregation allows extreme state aggregations beyond mdps. obtained results also explain algorithms designed mdps sometimes perform well beyond mdps. approximate φ-uniformity condition theorem weak compared bisimilarity uniformity theorem even weaker open problem whether analogue theorem also holds theorem beyond trick a-dependent a-independent section vectorize unfortunately leads state-space size exponential solution based pair linear rests open problem other/better ways dealing actions? extreme aggregations a-dependent possible? small discrete action spaces typical many board games exact conditions met. continuous action spaces robotics simply discretize action space introducing another ε-error action-continuous versions results would nicer. except theorem interesting generalization replace exact approximate φ-uniformity conditions bisimulation conditions classical state aggregation results reward transition probabilities. would interesting derive explicit weaker conditions still imply conditions values.", "year": 2014}