{"title": "Neural Machine Translation with Recurrent Attention Modeling", "tag": ["cs.NE", "cs.CL"], "abstract": "Knowing which words have been attended to in previous time steps while generating a translation is a rich source of information for predicting what words will be attended to in the future. We improve upon the attention model of Bahdanau et al. (2014) by explicitly modeling the relationship between previous and subsequent attention levels for each word using one recurrent network per input word. This architecture easily captures informative features, such as fertility and regularities in relative distortion. In experiments, we show our parameterization of attention improves translation quality.", "text": "knowing words attended previous time steps generating translation rich source information predicting words attended future. improve upon attention model bahdanau explicitly modeling relationship previous subsequent attention levels word using recurrent network input word. architecture easily captures informative features fertility regularities relative distortion. experiments show parameterization attention improves translation quality. contrast earlier approaches neural machine translation used ﬁxed vector representation input attention mechanisms provide evolving view input sentence output generated although attention intuitively appealing concept proven practice existing models attention content-based addressing made limited historical attention masks. however lessons better word alignment priors latent variable translation models suggests value modeling attention independent content. need models deal variable numbers predictions across variable lengths. work sought address problem models either rely explicitly engineered features resort indirect modeling previous attention decisions looking content-based states generated models coverage rather coverage together ordering patterns contrast propose model sequences attention levels word looking ﬁxed window previous alignment decisions. enables learn long range information coverage constraints deal fact input sentences varying sizes. paper propose explicitly model dependence attentions among target words. generating target word summarize attention history source word. resultant summary vector concatenated context vectors provide representation able capture attention history. attention current target word determined based concatenated representation. alternatively viewpoint memory networks framework model seen augmenting static encoding memory dynamic memory depends preceding source word attentions. method improves plain attentive neural models demonstrated data sets. also seen memory addressing reading process. content based addressing used weights αij. decoder reads memory weighted average vectors. combined predict j-th target word. implementation concatenate layer predict target word major limitation attention time step directly dependent other. however machine translation next word attend highly depends previous steps neighboring words likely selected next time step. attention mechanism fails capture important characteristics encoding lstm expensive. following attach dynamic memory vector original static memory keep track many times word attended whether neighboring words selected previous time steps information together used predict next word select. dynamic memory source word attach dynamic memory vector keep track history attention maps. ˜αij vector length centers position vector keeps track attention maps status around word directly models condition probability target sequence given source sequence tokens source sequence target sequence respectively. sutskever bahdanau slightly different choosing encoder decoder network. choose rnnsearch model baseline model. make several modiﬁcations rnnsearch model empirically modiﬁcation lead better results. bidirectional lstms encode source sentences. given source sentence embed words vectors embedding matrix vector i-th word wsxi. annotations word summarizing information neighboring words using bidirectional lstms forward backward annotation concatenated annotation word annotations source words form context conditioned generate target sentence. also seen memory vectors encode information source sequences. contains europarl common crawl news commentary corpus. remove sentence pairs german english similar million sentence pairs preprocessing. newstest validation newstest newstest test. experiments conﬁguration exclude sentences longer words training. vocabulary size english-german chineseenglish. words appear vocabulary replaced unk. rnnsearch model model word embedding size lstm dimension size dynamic memory vector size following initialize parameters uniformly within range plain train model batch size rescale gradient whenever norm greater initial learning rate english-german start halve learning rate every epoch training epochs. train model total epochs. chinese-english start halve learning rate every epochs training epochs. train model total epochs. investigate effect window size report results i.e. windows size model shown fig. call vector dynamic memory decoding step memory updated static. assumed keep track history attention status around word concatenate addressing attention weight vector calculated note used dynamic memory addressing process actual memory read include dij. also tried fully connected layer convolutional layer. empirically lstm works best. spotted three days later walker trapped quarry drei tage sp¨ater wurde einem spazierg¨anger steinbruch ihrer misslichen lage entdeckt wurde drei tage sp¨ater einem hund entdeckt drei tage sp¨ater wurde einem hund steinbruch gefangen entdeckt metropolitan transportation commission francisco area ofﬁcials congress could simply deal bankrupt highway trust fund raising taxes metropolitan transportation commission f¨ur gebiet francisco erkl¨arten beamte kongress k¨onne problem bankrotten highway trust fund einfach durch erh¨ohung kraftstoffsteuer lsen metropolitan francisco area sagen ofﬁzielle vertreter kongresses ganz einfach konkurs highway durch steuererh¨ohungen metropolitan transportation commission francisco area sagen beamte dass kongress durch steuererh¨ohungen ganz einfach konkurs highway trust fund umgehen k¨onnte same. table adding dependency improves rnnsearch model newstest newstest denote test test respectively. using window size coverage property considered improve much. following replace token probable target words bleu score data sets respectively. compare results related works including uses four layer lstm local attention mechanism uses character based encoding model outperform best bleu score respectively. table shows english-german translation examples. model dependent attention pick right part translate better better translation quality. improvement obvious chineseenglish. even considering coverage property improves using window size improves using replacement trick improves bleu score improvement signiﬁcant english-german data english german share lots words chinese english don’t. paper proposed attention mechanism explicitly takes attention history consideration generating attention map. work motivated intuition attention based next word attend highly oriol vinyals łukasz kaiser terry slav petrov ilya sutskever geoffrey hinton. grammar foreign language. advances neural information processing systems pages dependent previous steps. recurrent neural network summarize preceding attentions could impact attention current decoding attention. experiments data sets show method outperforms previous independent attentive models. also using larger context attention window would result better performance. future directions work staticdynamic memory view would like explore extending model fully dynamic memory model directly update representations source words using attention history generate target word.", "year": 2016}