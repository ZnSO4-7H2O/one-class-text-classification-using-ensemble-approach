{"title": "The Google Similarity Distance", "tag": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG", "I.2.4; I.2.7"], "abstract": "Words and phrases acquire meaning from the way they are used in society, from their relative semantics to other words and phrases. For computers the equivalent of `society' is `database,' and the equivalent of `use' is `way to search the database.' We present a new theory of similarity between words and phrases based on information distance and Kolmogorov complexity. To fix thoughts we use the world-wide-web as database, and Google as search engine. The method is also applicable to other search engines and databases. This theory is then applied to construct a method to automatically extract similarity, the Google similarity distance, of words and phrases from the world-wide-web using Google page counts. The world-wide-web is the largest database on earth, and the context information entered by millions of independent users averages out to provide automatic semantics of useful quality. We give applications in hierarchical clustering, classification, and language translation. We give examples to distinguish between colors and numbers, cluster names of paintings by 17th century Dutch masters and names of books by English novelists, the ability to understand emergencies, and primes, and we demonstrate the ability to do a simple automatic English-Spanish translation. Finally, we use the WordNet database as an objective baseline against which to judge the performance of our method. We conduct a massive randomized trial in binary classification using support vector machines to learn categories based on our Google distance, resulting in an a mean agreement of 87% with the expert crafted WordNet categories.", "text": "abstract— words phrases acquire meaning used society relative semantics words phrases. computers equivalent ‘society’ ‘database’ equivalent ‘use’ ‘way search database.’ present theory similarity words phrases based information distance kolmogorov complexity. thoughts world-wide-web database google search engine. method also applicable search engines databases. theory applied construct method automatically extract similarity google similarity distance words phrases world-wide-web using google page counts. world-wide-web largest database earth context information entered millions independent users averages provide automatic semantics useful quality. give applications hierarchical clustering classiﬁcation language translation. give examples distinguish colors numbers cluster names paintings century dutch masters names books english novelists ability understand emergencies primes demonstrate ability simple automatic english-spanish translation. finally wordnet database objective baseline judge performance method. conduct massive randomized trial binary classiﬁcation using support vector machines learn categories based google distance resulting mean agreement expert crafted wordnet categories. index terms— accuracy comparison wordnet categories automatic classiﬁcation clustering automatic meaning discovery using google automatic relative semantics automatic translation dissimilarity semantic distance google search google distribution page counts google code kolmogorov complexity normalized compression distance normalized information distance normalized google distance meaning words phrases extracted parameter-free data-mining universal similarity metric material paper presented part ieee itsoc information theory workshop coding complexity aug. sept. rotorua zealand ieee intn’l symp. information theory seattle wash. august manuscript received april ﬁnal revision june rudi cilibrasi supported part netherlands bsik/bricks project project centre mathematics computer science amsterdam netherlands. address kruislaan amsterdam netherlands. email rudi.cilibrasicwi.nl. paul vitanyi’s work done part author sabbatical leave national australia sydney laboratory unsw. afﬁliated centre mathematics computer science university amsterdam amsterdam netherlands. supported part project resq ist-- programmme pascal netherlands bsik/bricks project. address kruislaan amsterdam netherlands. email paul.vitanyicwi.nl. represented literal object itself. objects also given name like four-letter genome mouse text peace tolstoy. also objects cannot given literally name acquire meaning contexts background common knowledge humankind like home red. make computers intelligent would like represent meaning computer-digestable form. long-term laborintensive efforts like project wordnet project establish semantic relations common objects precisely names objects. idea create semantic vast proportions rudimentary intelligence knowledge real world spontaneously emerge. comes great cost designing structures capable manipulating knowledge entering high quality contents structures knowledgeable human experts. efforts longrunning large scale overall information entered minute compared available world-wide-web. rise world-wide-web enticed millions users type trillions characters create billions pages average quality contents. sheer mass information almost every conceivable topic makes likely extremes cancel majority average meaningful low-quality approximate sense. devise general method amorphous low-grade knowledge available free world-wide-web typed local users aiming personal gratiﬁcation diverse objectives globally achieving effectively largest semantic electronic database world. moreover database available using search engine return aggregate page-count estimates large range searchqueries like google. previously others developed compression-based method establish universal similarity metric among objects given ﬁnite binary strings widely reported objects genomes music pieces midi format computer programs ruby pictures simple bitmap formats time sequences heart rhythm data. method feature-free sense doesn’t analyze ﬁles looking particular features; rather analyzes features simultaneously determines similarity every pair objects according dominant shared feature. crucial point method analyzes objects themselves. precludes comparison abstract notions objects don’t lend direct analysis like emotions colors socrates plato mike bonanno albert einstein. previous method compares objects particularly suited obtain knowledge irrespective common beliefs similarities develop method uses name object obtains knowledge similarity objects quantiﬁed relative google semantics tapping available information generated multitudes users. reminded words d.h. rumsfeld trained know awful going world punching mouse relatively modest cost paper google semantics word phrase consists pages returned query concerned. storage assumptions reﬁned propose. contrast many references cited there google counts identify lexicosyntactic patterns data. again theory feature analysis execution different ours cannot meaningfully compared. essentially method automatically extracts semantic relations arbitrary objects manner feature-free search-engine used computationally feasible. seems direction altogether. theory propose rather intricate resulting method simple enough. give example time experiment google search horse returned hits. number hits search term rider searching pages horse rider occur gave hits google indexed pages. using numbers main formula derive below yields normalized google distance terms horse rider follows sequel paper argue normed semantic distance terms question usually cognitive space invoked usage terms world-wide-web ﬁltered google. vastness diversity taken related current terms society. calculation google indexed onehalf number pages instructive probabilities used search terms didn’t change signiﬁcantly doubling pages number hits horse equal rider equal horse rider equal computed situation line contention relative frequencies pages containing search terms gives objective information semantic relations search terms. case google probabilities search terms computed stabilize growing google database. great deal work cognitive psychology linguistics computer science using word frequencies text corpora develop measures word similarity word association partially surveyed going back least successful latent semantic analysis applied various forms great number applications. discuss relation present approach appendix vii. many previous approaches extracting corollations text documents based text corpora many order magnitudes smaller local main thrust develop theory semantic distance pair objects based background contents consisting database documents. example latter pages constituting world-wide-web. similarity relations pairs objects distilled documents using number documents objects occur singly jointly google semantics word phrase consists pages returned query concerned. note mean terms different meaning semantics opposites like true false often similar semantics. thus discover associations terms suggesting likely relationship. grows google semantics become less primitive. theoretical underpinning based theory kolmogorov complexity terms coding compression. allows express prove properties absolute relations objects cannot even expressed approaches. theory application particular formula express bilateral semantic relations equivalent earlier theory application formula area. current paper next step decade cumulative research area main thread using related approach ﬁrst start technical introduction outlining notions underpinning approach kolmogorov complexity information distance compression-based similarity metric give technical description google distribution normalized google distance universality notions possible principle methods entire world-wide-web determine semantic similarity terms know method uses entire computationally entire aims method. validate method therefore cannot compare performance existing methods. proposal task. validate method following theoretical analysis anecdotical evidence plethora applications systematic massive comparison accuracy classiﬁcation application compared uncontroversial body knowledge wordnet database. section give theoretic underpinning method prove universality. section present plethora clustering classiﬁcation experiments validate universality robustness accuracy proposal. section test repetitive automatic performance uncontroversial semantic knowledge present results massive randomized classiﬁcation trial conducted gauge accuracy method expert knowledge implemented decades wordnet database. preliminary publication work archives widely reported discussed example actual experimental data downloaded method implemented easy-to-use software tool available available all. application theory develop method justiﬁed vastness world-wide-web assumption mass information diverse frequencies pages returned google queries averages semantic information distill valid semantic distance query subjects. appears method starts scratch feature-free uses search engine supply contents automatically generates relative semantics words phrases. possible drawback method relies accuracy returned counts. noted returned google counts inaccurate especially uses boolean operator search terms time writing. operator less problematic operator. furthermore google apparently estimates number hits based samples number indexed pages changes rapidly. compensate latter effect inserted normalizing mechanism complearn software. generally though search engines peculiar ways counting number hits large part matter long reasonable conditions hold counts reported. linguists judge accuracy google counts trustworthy enough shown searches rare two-word phrases correlated well frequency found traditional corpora well human judgments whether phrases natural. thus google simplest means information. note however single google query takes fraction second google restricts every address maximum queries day—although cooperative enough extend quotum noncommercial purposes. experimental evidence provided shows combination google method yields reasonable results gauged common sense expert knowledge wordnet data base. reviewer suggested downscaling method testing smaller text corpora. seem useful. clearly perfomance deteriorate decreasing data base size. thought experiment using extreme case single page consisting single term sufﬁces. practically addressing issue begging question. instead section theoretically analyze basis much theory explored paper kolmogorov complexity. introduction details textbook give intuition notation. assume ﬁxed reference universal programming system. system general computer language like lisp ruby also ﬁxed reference universal turing machine given standard enumeration turing machines. latter choice advantage formally simple hence easy theoretically manipulate. choice makes difference principle theory invariant changes among universal programming systems provided stick particular choice. consider universal programming systems associated programs preﬁx code—as case standard computer languages. kolmogorov complexity string length bits shortest computer program ﬁxed reference computing system produces output. choice computing system changes value additive ﬁxed constant. since goes inﬁnity additive ﬁxed constant ignorable quantity consider large think kolmogorov complexity view length bits ultimate compressed version recovered general decompression program. compressing using compressor gzip results length |xg| |x|. using better compressor bzip results usually |xb| |xg|; using still better compressor like ppmz results |xp| |xb|. kolmogorov complexity gives lower bound ultimate value every existing compressor compressors possible known less equal length compressed version gives ultimate value length compressed version task designing better better compressors approach lower bound closely possible. considered following notion given strings length shortest binary program reference universal computing system program computes output input also output input called information distance denoted that negligible logarithmic additive term consider large class admissible distances distances nonnegative symmetric computable sense every distance preﬁx program that given strings binary length equal distance then constant depends minorizes additive constant. call information distance universal family computable distances since former minorizes every member latter family additive constant. strings close according computable distance least close according distance since every feature compare strings quantiﬁed terms distance every distance viewed expressing quantiﬁcation much particular feature strings common information distance determines distance strings minorizing dominant feature similar. means that consider strings information distance every pair based minorizing different dominating feature. small strings differ information distance large compared sizes strings different. however large strings differ information distance similar. therefore information distance suitable express true similarity. must deﬁne relative information distance need normalize information distance. approach ﬁrst proposed context genomics-based phylogeny improved here. normalized information distance values inherits universality information distance sense minorizes vanishing additive term every possible normalized computable distance identify computable normalized distances computable similarities according features discovers every pair strings feature similar expresses similarity scale considering strings feature strings similar different different pairs strings. deﬁned pression programs approximate kolmogorov complexities compression algorithm deﬁnes computable function strings lengths compressed versions strings. therefore number bits compressed version string upper bound kolmogorov complexity string additive constant depending compressor string question. thus compressor denote length compressed version string arrive normalized compression distance convenience replaced pair formula concatenation transition raises several tricky problems example approximates approximates need concern here. thus actually family compression functions parameterized given data compressor limiting case denotes number bits shortest code decompressed general purpose computable decompressor. every text corpus particular user combined frequency extractor deﬁnes relative frequencies words phrases usage. world-wide-web google setting millions users text corpora distribution. sequel show google distribution universal individual users distributions. number pages currently indexed google approaching every common search term occurs millions pages. number vast number authors generating pages enormous probabilities google search terms conceived frequencies page counts returned google divided number pages indexed google approximate actual relative frequencies search terms actually used society. based premise theory develop paper states relations represented normalized google distance approximately capture assumed true semantic relations governing search terms. formula uses probabilities search terms extracted text corpus question. world-wide-web google method used text corpora like king james version bible oxford english dictionary frequency count extractors world-wide-web yahoo frequency count extractor. cases obtains text corpus frequency extractor biased semantics search terms. obtain true relative frequencies words phrases society major problem applied linguistic research. requires analyzing representative random samples sufﬁcient sizes. question sample randomly representatively continuous source debate. contention large diverse text corpus google able extractor relative page counts approximate true societal wordphrases usage starts supported current real linguistics research singleton google search terms denoted sequel singleton search terms doubleton search terms pages indexed google cardinality denoted time writing assume priori pages equi-probable probability returned google subset called event. every search term usable google deﬁnes singleton google event pages contain occurrence returned google search uniform mass probability function. probability event |x|/m similarly doubleton google event pages returned google search pages containing search term search term probability event y|/m also deﬁne boolean combinations event probability equal cardinality divided event obtained basic events corresponding basic search terms ﬁnitely many applications boolean operations probability |e|/m google event consisting pages containing occurrences search term thus embodies every possible sense direct context occurs web. constitutes google semantics term. remark course possible parts direct contextual material link pages occur thereby supply additional context. approach indirect context ignored. nonetheless indirect context important future reﬁnements method take account. event consists possible direct knowledge regarding therefore natural consider code words events coding background knowledge. however cannot probability events directly determine preﬁx code rather underlying information content implied probability. reason events overlap hence summed probability exceeds kraft inequality prevents corresponding code-word lengths. solution normalize probability google events deﬁne probability mass function google search terms singleton doubleton terms. singleton terms doubletons consisting pair counting singleton doubleton summation. note means every pair pages counted three times since every page indexed google contains least occurrence search term hand pages contain average certain constant search terms. therefore deﬁne lm/n y|/n. then p{xy}⊆s g-distribution changes time different samplings distribution. imagine holds sense instantaneous snapshot. real situation approximation this. given google machinery absolute probabilities allow deﬁne associated preﬁx code-word lengths singletons doubletons. google code deﬁned contrast strings complexity represents length compressed version using compressor search term google code length represents shortest expected preﬁx-code word length associated google event expectation taken google distribution sense google distribution compressor google semantics associated search terms. associated called normalized google distance deﬁned rewritten right-hand expression denotes number pages containing denotes number pages containing reported google. approximation using preﬁx code-word lengths generated google distribution deﬁning compressor approximating length kolmogorov code using background knowledge viewed google conditional information. practice page counts returned google frequencies choose right-hand side term apparent increasing decrease everything gets closer together decreasing increase everything gets apart. experiments suggest every reasonable value used normalizing factor results seem general insensitive choice. software parameter adjusted appropriate often following main properties always nonnegative every every pair symmetric. however metric satisfy every before denote pages containing occurrences example choose then satisfy triangle inequality ngd+ngd example choose then yields violates triangle inequality scale-invariant following sense assume number pages indexed google grows number pages containing given search term goes ﬁxed fraction number pages containing given conjunction search terms. means doubles -frequencies. give objective semantic relation search terms needs become stable number grows unboundedly. central notion application compression learning notion universal distribution consider effective enumeration probability mass functions domain list ﬁnite countably inﬁnite. deﬁnition probability mass function occurring universal every constant pi=u every depend indexes functional mappings elements list universal immediately follows every preﬁx code-word length source word associated minorizes preﬁx codeword length associated satisfying every following consider partitions pages subset partition together probability mass function search terms. example consider list authors producing pages consider pages produced author partition. author metaphor convenience. author list produce pages denote |ωi|. identify author pages produces. since knowledge authors consider every possible partion equivalence classes s··· deﬁning realizable authors consider partition search term usable google deﬁnes event pages produced author contain search term similarly pages produced returned google searching pages containing search term search term remark understand consider codelengths involved google database changes time. reasonable expect total number pages well total number search terms google database continue grow time. period total probability mass carved increasingly smaller pieces search terms. maximum singleton doubleton codelengths within google database grow. universality property google distribution implies google distribution’s code length almost particular search terms exceed best codelength among individual authors size grow slowly codelength particular search term time. thus coding space suboptimal google distribution’s code ever-smaller piece total coding space. giprobability concentrated pairs search terms holds least proof preﬁx code-word lengths associated satisfy gi+log n/ni n/ni. substituting n/ni middle term obtain markov’s inequality says following probability mass function; nonnegative function p-expected value pi{p author consider conditional probability mass functions singleton search terms iexpected value g′/g′ since probability event doubleton search terms greater event based either constituent search terms probability singleton event conditioned singleton event least large unconditional probability event g′/g′ search terms satisfy condition moreover probabilities satisfy together follows px{gi taking uniform distribution search terms assigns probability them. deﬁnition universality probability mass function list individual google probability mass functions choose function freely choose search term search terms then obtain cigi yields required contradiction normalized google distance ngdi author deﬁned according substituted google distances ngdi viewed individual semantic distances according bias author individual semantics subsumed general google semantics following sense normalized google distance universal family individual normalized google distances sense small least individual normalized google distance high probability. hence google semantics evoked society certain sense captures biases knowledge individual authors. theorem show that every inequality remark interpret observe case large respect moreover n/ni large respect approximately max{g estimate case reasonable assumptions. without loss generality assume number pages returned query log). thus approximately uniform expectation n/|a| divided expectation equals number authors producing pages. uniform expectation n/|s| divided expectation equals number google search terms use. thus approximately number search terms exceeds number authors goes expectation. log. substitute gi−log min-term numerator max-term denominator. noting gi-probabilities independent total gi-probability substitutions justiﬁed least therefore google normalized distance minorizes every normalized compression distance based particular user’s generated probabilities search terms high probability error term typical cases ignorable. http//www.complearn.org tool used earlier papers construct trees representing hierarchical clusters objects unsupervised way. however normalized google distance instead normalized compression distance method works ﬁrst calculating distance matrix whose entries pairswise terms input list. calculate best-matching unrooted ternary tree using novel quartet-method style heuristic based randomized hill-climbing using ﬁtness objective function candidate trees. brieﬂy explain method does; explanation given objects points space provided distance measure associated distance matrix entries pairwise distances objects. regardless original space distance measure always possible conﬁgure objects ndimensional euclidean space associated distances identical original ones resulting identical distance matrix. distance matrix contains pairwise distance relations according chosen measure form. format information easily usable since cognitive capabilities rapidly fail. distance matrix reduced form information representing original data need reduce information even order achieve cognitively acceptable format like data clusters. extract hierarchy clusters distance matrix determine dendrogram agrees distance matrix according ﬁdelity measure. allows extract information data clustering method take strongest link case true ignore others; instead tree represents relations distance matrix little distortion possible. particular examples give below clustering examples depicted ﬁdelity close meaning relations distance matrix faithfully represented tree. objects clustered search terms consisting names colors numbers tricky words. program automatically organized colors towards side tree numbers towards other figure arranges terms meaning color number nothing else farthest reach color side number side respectively. puts general terms black white zero towards center thus indicating ambiguous interpretation. also things exactly colors numbers also towards center like word small. authors know exist experiments create type semantic distance automatically using google similar search engines. thus baseline compare against; rather current experiment baseline evaluate behavior future systems. example figure names ﬁfteen paintings steen rembrandt entered. full name single google search term experiment painting title names used; associated painters given below. know comparable experiments baseline judge performance; type contents clustering made possible existence search engines. painters paintings used follows clustering given figure provide feeling ﬁgures involved give associated matrix figure value figure gives ﬁdelity tree representation pairwise distances matrix perfect possible. details question arises expect this. names artistic objects distinct? distinguishing feature subject matter title style? experiments objects belonging cultural heritage clearly woman importance midsummer night’s dream modest proposal gulliver’s travels julius caesar lady windermere’s love’s labours lost romeo juliet salome tale battle books picture dorian gray subject matter. stress point used julius caesar shakespeare. term occurs overwhelmingly contexts styles. collection objects used semantic distance towards objects given formula singled semantics julius caesar relevant experiment. term co-occurrence speciﬁc context author discussion swamped uses common english term particular form distances pairwise. using book titles common words like horse rider author supposing exist swamping effect presumably arise. system gets confused artists? subjects like music sculpture? experiments representative performed current software. cherrypick best outcomes. example experiments three english writers different selections four works each always yielded tree could draw convex hull around works author without overlap. interestingly similar experiment russian authors gave worse results. readers experiments satisfy curiosity using publicly available software tool http//clo.complearn.org/ also used depicted experiments. experiment take long time hours googling network trafﬁc tree reconstruction layout. don’t wait check result later. augment google method adding trainable component learning system. support vector machine trainable component. method used paper refer exposition libsvm software experiments. setting binary classiﬁcation problem examples represented search terms. require human expert provide list least training words consisting least positive examples negative examples illustrate contemplated concept class. expert also provides anchor words half related concept consideration. then anchor words convert training words -dimensional training vectors entry deﬁned figure trained using list emergencies positive examples list almost emergencies negative examples. ﬁgure self-explanatory. accuracy test figure method learns distinguish prime numbers non-prime numbers example. accuracy test example illustrates several common features method distinguish strictly deductive techniques. another potential application method natural language translation. suppose given system tries infer translation-vocabulary among english spanish. assume system already determined words appear different matched sentences permutation associating english spanish words undetermined. setting arise real situations english spanish different rules wordordering. outset assume pre-existing vocabulary eight english words matched spanish translation. infer correct permutation mapping unknown words using pre-existing vocabulary basis? start forming matrix using additional english words translation known figure iv-e. label columns translation-known english words rows translation-unknown english words. entries matrix english words labeling columns rows. constitutes english basis matrix. next consider known spanish words corresponding known english words. form matrix known spanish words labeling columns order known english words. label rows matrix choosing many possible permutations unknown spanish words. permutation form matrix spanish words compute pairwise correlation sequence values values given english word basis matrix. choose permutation highest positive correlation. positive correlation report failure extend vocabulary. example computer inferred correct permutation testing words figure wordnet semantic concordance english. focusses meaning words dividing categories. follows. category want learn concept termed electrical represents anything pertain electronics. negative examples constituted simply everything else. category represents typical expansion node wordnet hierarchy. experiment accuracy test turns electrical terms unambiguous easy learn classify method. information wordnet database entered decades human experts precise. database academic venture publicly accessible. hence good baseline judge accuracy method indirect manner. cannot directly compare semantic distance objects indirectly judge accurate using basis learning algorithm. particular investigated well semantic categories learned using approach agree corresponding wordnet categories. details structure wordnet refer ofﬁcial wordnet documentation available online. considered randomly selected semantic categories wordnet database. category executed following sequence. first trained labeled training samples. positive examples randomly drawn wordnet database category question. negative examples randomly drawn dictionary. latter examples false negatives consider probability negligible. experiment used total anchors three randomly drawn wordnet database category question three drawn dictionary. subsequently every example converted -dimensional vectors using entry vector anchor example concerned trained resulting labeled vectors. kernel-width errorcomparison made project project commercial venture cycorp tries create artiﬁcial common sense. cyc’s knowledge base consists hundreds microtheories hundreds thousands terms well million hand-crafted assertions written formal language called cycl cycl enhanced variety ﬁrst-order predicate logic. knowledge base created course decades paid human experts. therefore extremely high quality. google hand almost completely unstructured offers primitive query capability nearly ﬂexible enough represent formal deduction. lacks expressiveness google makes size; google already indexed eight billion pages shows signs slowing down. basis assumption latent semantic analysis cognitive similarity words reﬂected co-occur small subsamples language. particular implemented constructing matrix rows labeled documents involved columns labeled attributes entries number times column attribute occurs document. entries processed taking logarithm entry dividing number documents attribute occurred normalizing function. results sparse high-dimensional matrix main feature reduce dimensionality matrix projecting adequate subspace lower dimension using singular value decomposition orthogonal matrices diagonal matrix. diagonal elements satisfy closest matrix dimension rank terms so-called frobenius norm obtained setting using corresponds using important dimensions. attribute taken correspond column vector similarity attributes usually taken cosine vectors. compare proposed method documents could pages entries matrix frequencies search terms page. converted obtain vectors search term. subsequently cosine vectors gives similarity terms. used plethora applications ranging data base query systems synonymy answering systems toefl tests. comparing performance method problematic several reasons. first numerical quantity measuring semantic distance pairs terms cannot directly cost parameters automatically determined using ﬁve-fold cross validation. finally testing well learned classiﬁer performed using examples balanced ensemble positive negative examples obtained converted -dimensional vectors manner training examples. results accuracy score correctly classiﬁed test examples. experiments. actual data available histogram agreement accuracies shown figure average method turns agree well wordnet semantic concordance made human experts. mean accuracies agreements variance gives standard deviation thus rare agreement less total number google searches involved randomized automatic trial upper bounded considerable savings resulted fact re-use certain google counts. every term computing -dimensional vector computed respect anchors requires counts anchors needs computed experiment count term computed once count joint occurrence term anchors computed case. altogether gives total every experiment google searches entire trial. conceivable scores instead used construction -dimensional vectors work competetively. something simple like number words used common dictionary deﬁnition begging question unlikely successful. abbroach compression literal objects compared number alternative approaches like euclidean distance frequency vectors blocks. alternatives gave results completely unacceptable. current setting conceive euclidean vectors word frequencies pages corresponding search term. apart fact google support automatical analysis pages reported search term would computationally infeasible analyze millions pages involved. thus compared since quite different epistimologies. indirect comparison could given using method basis particular application comparing accuracies. however application terms using google computationally question matrix would rows even google would report frequencies occurrences pages identify pages properly. would need retrieve entire google data base many terabytes. moreover noted section google search takes signiﬁcant amount time cannot automatically make certain number day. alternative interpretation considering single document makes matrix vector appears defeat process altogether. summarizing basic idea method similar spirit. novel selected terms large document collection whereas involves matrix operations closed collection limited size hence possible apply context. basics google search http//www.google.com/help/basics.html. l.g. kraft device quantizing grouping coding amplitude modulated pulses. master’s thesis dept. electrical engineering m.i.t. cambridge mass. j.h. badger chen kwong kearney zhang information-based sequence distance application whole mitochondrial genome phylogeny bioinformatics chen vitanyi. similarity metric costa santos bernardes p.m.b. vitanyi antunes clustering fetal heart rate tracings compression proc. ieee symp. computer-based medical systems rudi cilibrasi received b.s. honors california institute technology programmed computers decades academia industry various companies silicon valley including microsoft diverse areas machine learning data compression process control vlsi design computer graphics computer security networking protocols. student centre mathematics computer science netherlands expects receive soon circle ideas paper representative. helped create ﬁrst publicly downloadable normalized compression/google distance software maintaining http//www.complearn.org now. home page http//www.cwi.nl/∼cilibrar/ paul m.b. ´anyi fellow centre mathematics computer science amsterdam professor computer science university amsterdam. serves editorial boards distributed computing information processing letters theory computing systems parallel processing letters international journal foundations computer science journal computer systems sciences elsewhere. worked cellular automata computational complexity distributed parallel computing machine learning prediction physics computation kolmogorov complexity quantum computing. together ming pioneered applications kolmogorov complexity co-authored introduction kolmogorov complexity applications springer-verlag york parts translated chinese russian japanese. home page http//www.cwi.nl/∼paulv/", "year": 2004}