{"title": "Efficient Independence-Based MAP Approach for Robust Markov Networks  Structure Discovery", "tag": ["cs.AI", "cs.CV"], "abstract": "This work introduces the IB-score, a family of independence-based score functions for robust learning of Markov networks independence structures. Markov networks are a widely used graphical representation of probability distributions, with many applications in several fields of science. The main advantage of the IB-score is the possibility of computing it without the need of estimation of the numerical parameters, an NP-hard problem, usually solved through an approximate, data-intensive, iterative optimization. We derive a formal expression for the IB-score from first principles, mainly maximum a posteriori and conditional independence properties, and exemplify several instantiations of it, resulting in two novel algorithms for structure learning: IBMAP-HC and IBMAP-TS. Experimental results over both artificial and real world data show these algorithms achieve important error reductions in the learnt structures when compared with the state-of-the-art independence-based structure learning algorithm GSMN, achieving increments of more than 50% in the amount of independencies they encode correctly, and in some cases, learning correctly over 90% of the edges that GSMN learnt incorrectly. Theoretical analysis shows IBMAP-HC proceeds efficiently, achieving these improvements in a time polynomial to the number of random variables in the domain.", "text": "facundo bromberg federico schl¨uter information systems department national technological university facultad regional mendoza rodriguez mendoza argentina work introduces ib-score family independence-based score functions robust learning markov networks independence structures. markov networks widely used graphical representation probability distributions many applications several ﬁelds science. main advantage ib-score possibility computing without need estimation numerical parameters np-hard problem usually solved approximate data-intensive iterative optimization. derive formal expression ib-score ﬁrst principles mainly maximum posteriori conditional independence properties exemplify several instantiations resulting novel algorithms structure learning ibmap-hc ibmap-ts. experimental results artiﬁcial real world data show algorithms achieve important error reductions learnt structures compared state-of-the-art independencebased structure learning algorithm gsmn achieving increments amount independencies encode correctly cases learning correctly edges gsmn learnt incorrectly. theoretical analysis shows ibmap-hc proceeds eﬃciently achieving improvements time polynomial number random variables domain. present work presents novel approach problem learning independence structure markov network data taking maximum-a-posteriori bayesian perspective independence-based approach independence-based algorithms learn structure performing succession statistical tests independence among diﬀerent groups random variables. appealing important reasons amenable proof correctness eﬃcient reaching time complexities worst case number random variables domain. runtime overwhelming improvement score-based approaches maximum-likelihood super-exponential runtime unfortunately holds theory correctness algorithms i.e. guarantee produce correct structure compromised independence decision statistical tests unreliable almost certainty tests performed data. moreover discuss detail following sections tests reliable require data size exponential number variables n.to address important concerns present work focuses algorithms improving quality independence-based approaches maintaining manegeable runtime. main contribution ib-score result hybrid approach independence score based approaches improvements both. hand generalizing previous works e.g. improves quality existing independence-based algorithms taking bayesian approach modelling explicitly posterior distribution structures given data. other computed eﬃciently contrary existing scores require estimating model parameters np-hard computation usually approximated data-intensive iterative algorithm. achieved ﬁrst modelling uncertainty independencies random variables expanding posterior independencies operation results expression posterior structures dependent posterior independence assertions computed eﬃciently proceed next section discuss detail many concepts algorithms including thorough literature review. following section formalizes derives ib-score ﬁrst-principles. section introduces algorithms ibmaphc ibmap-ts exemplifying diﬀerent possible optimization searches diﬀerent possible instantiations ib-score. then section present experimental results conﬁrm robustness ibmap-hc ibmapts polynomial runtime ibmap-hc. conclude present summary possible directions future works section section describe motivate detail comparing various existing approaches learning independence structure data expanding deﬁciencies approaches described above contribution addresses deﬁciencies successfully. innovation rate computing digital storage capacity increasing rapidly time passes producing important increase data available digital format. response community data mining machine learning constantly producing novel improved algorithms extraction knowledge information implicit data. without doubt well-established probabilistic theory powerful modeling tool contributing algorithms specially data uncertain probabilistic graphical models family multi-variate eﬃcient probability distributions that codifying implicitly conditional independences among random variables domain produce sometimes exponential reductions storage requirements timecomplexity statistical inference eﬃciencies reason restoration noisy images texture classiﬁcation image segmentation; genetic research disease diagnosis spatial data mining geography transport ecology many others graphical model probabilistic model joint random variables. consists numerical parameters graph encodes —compactly— independences among random variables domain. edges represent explicitly possible probabilistic conditional independences among variables thus called sometimes independence structure simply structure domain. interested problem learning graphical model data consists learning independence structure numerical parameters illustrated fig. statistical modeling process assume data available sampling unknown underlying probability distribution learning consists producing structure parameters best data hope model matches underlying distribution. figure outline problem disposal dataset assumed sampling unknown underlying probability distribution learning consists analyzing input dataset produce model distribution learning consists analyzing input dataset produce model best hope matches underlying model. predominant types graphical models bayesian networks markov networks directed acyclic graph undirected graph. families diﬀer independencies encode. sets independences —and therefore probability distributions— representable graphs moreover sets independences —and therefore distributions— representable others case distributions faithfully representable undirected graphs called graph-isomorph details). common practice follow work assume underlying distribution graph-isomorph. follows capitals denote random variables bold capitals denote sets random variables. denote random variables problem domain cardinality i.e. work assume variables discrete. denote input dataset cardinality said before consists numerical parameters graph encodes conditional independences among random variables contained notation denote conditional independence predicate triplet true whenever independent given variables graph consists nodes edges nodes encode probabilistic conditional independences among random variables fig. shows example encoding independences follows shown pearl underlying model graph-isomorph encoding equivalent reading independences vertex-separation i.e. variables independent according graph given variables disconnected sub-graph resulting removing edges incident variable words intercepts every path illustrate consider variables variables dependent given still path variable thus independent given mentioned strength graphical models lies sometimes exponential reduction storage capacity time complexity statistical inference. storage statistical inference exponential representation multi-variate joint probability distribution consists multidimensional table columns exponentially many tuples consisting complete assignments variables column store probability assignment. fig. illustrates table joint probability system binary variables consisting tuples conﬁguration. exponential reduction storage inference occurs certain conditional independences allow decomposition joint probability product factor potential functions quantiﬁed parameters dependent subset instance well-known fact variables function variables exponential number variables decomposition requires polynomial number smaller tables. moreover shown many cases number variables factors bounded small number resulting exponential reduction storage marginalization. continue example decomposition binary variables results tables tuples each illustrated fig. important diﬀerence lies properties factor functions. factor functions normalized thus order obtain fully quantiﬁed probabilistic model exponential computation normalization constant possible assignments variables required. instead allow factorization joint distribution conditional probability distributions i.e. normalized factors thus learned eﬃciently data historically literature presented broad approaches learning scorebased algorithms independence-based algorithms. scorebased algorithms exempliﬁed bacchus mccallum acid campos perform search space possible graphs structure maximum score. algorithms diﬀer approach taken explore space graphs undirected graphs nodes) functional form theoretical justiﬁcation score. examples scores maximum likelihood data given model minimum description length model pseudolikelihood given model last approach pseudo-likelihood introduces approximate expression computing likelihood require normalization model compute score structure visited search necessary estimate parameters operation performed eﬃciently costly operation consists optimization instance case likelihood optimization would space parameters ﬁxed structure later show approach although requires search space structures compute eﬃciently score step require estimation parameters. bromberg margaritis bromberg ability learn independence structure eﬃciently require neither search structures estimation parameters step execution need estimate parameters structure learned). instead algorithms take rather direct approach constructing independence structure inquiring conditional independences hold input data. inquiry performed practice statistical tests pearson’s test recently bayesian test continuous gaussian data partial correlation test among others. step algorithms propose triplet inquire data whether independence dependence holds triplet. triplet proposed step depends independence information algorithm point triplets proposed corresponding independence value. structure consistent independence information algorithms proceed discarting step structures inconsistent independence learned structure discarded. algorithms exist require number tests polynomial number variables domain which together fact statistical tests executed running time proportional number data points dataset result total running time polynomial eﬃciency another important advantage algorithms that assumptions possible prove correctness algorithms i.e. return structure underlying model. thorough example proof refer reader bromberg unfortunately statistical independence tests always reliable. independence-based algorithms oblivious fact evident design discards structures based single test. test wrong underlying structure discarded. problem underestimated because quality statistical independence tests degrades exponentially number variables involved test. example recommends pearson’s independence test deemed unreliable cells tests contingency table expected count less data points. since contingency table conditional independence test d-dimensional number cells thus number data points required grows exponentially size test. words ﬁxed size dataset quality test degrades exponentially size conditioning set. problem addressed bromberg margaritis using argumentation. approach modeled problem inconsistent knowledge base consisting pearl’s axioms conditional independence rules triplets corresponding assignments independence values fact predicates. underlying model graph-isomorph independencies must satisfy axioms koller friedman details). independencies queried directly underlying model sampled dataset. independencies unreliable thus violate rules. argumentation inference procedure designed work inconsistent knowledge bases. bromberg margaritis used framework infer independencies sometimes resulting inferred values diﬀerent measured values. taking inferred value correct obtained important improvements quality reliability statistical independence tests thus independence structures discovered. present alternative approach unreliability problem. approach inspired work margaritis bromberg designed algorithm instead discarding structures based outcomes statistical tests maintains distribution structures conditioned data i.e. posterior distribution structures. learn structure algorithm takes maximum-a-posteriori approach. next section present ib-score expression computing efﬁciently posterior structure based outcomes statistical independence tests following sections score diﬀerent algorithms conduct search maximum. resulting algorithms hybrids score independence based algorithms. score-based proceed maximizing score i.e. posterior structures given data call ib-score; independence-based ib-score computed statistical tests independence however contrary previous score-based approaches computation score require estimation numerical parameters therefore eﬃcient. discussed previous section independence-based algorithms advantage requiring neither search structures interleaved estimation parameters estimation iteration search unfortunately advantages compensated major robustness problem vast majority independence-based algorithms rely blindly correctness tests perform risk discarting true underlying structure test incorrect. problem exacerbated fact reliability statistical independence tests degrades exponentially size conditioning test overcome robustness problem take bayesian approach inspired extended approach models problem structure learning distribution structure given data i.e. posterior distribution structures. model structures inconsistent outcome test discarded probability reduced. approach taken combination score independence based algorithms structure learning. score-based search structure whose posterior probability maximal take maximum-a-posteriori approach independence-based expression obtained computing posterior probability based outcome statistical independence tests explained detail following section. later section present practical algorithms conducting maximization ibmap-hc ibmap-ts section present ib-score computationally feasible expression posterior proceed re-expressing posterior structure terms posterior particular independence assertions call closure then approximating assumptions obtain eﬃciently computable expression posterior call ib-score. example second example considers independence-based algorithm exists proof correctness; usual assumptions faithfulness correctness independence assertions. proof provided correct independence values independence inquiry structure output algorithm consistent values. possible structure independencies independencies determine conform closure. examples correct algorithms gsmn gsimn algorithms proof proof proceeds incorporating steps information independence assertions contained closure posterior first model random variables uncertainty independence assertions obtained unreliable statistical independence tests. formally uncertainty independence assertion denoting triplet independent true formalized random variable taking values {true false} second independence assertion incorporate posterior using total probability namely abbreviations true false respectively. simplify expression ﬁrst collapsing uni-dimensional summations single multidimensional summation abbreviating obtaining deﬁnition closure structure determined independence assertions i.e. probability given assertions must equal moreover independence value assertions clear cannot structure words probability given ﬂipped assertions must results left factor terms summation except term containing signments consistent closure denote assignments approximation implies belief independence triplet closure aﬀected knowledge triplet closure independent true practice. certainly false triplets related pearl axioms independence case determine other i.e. thus joint probability easily computable triplets related pearl axioms forced approximation method computing joint several independence assertions developed conclude section note made single approximation namely random variables independence assertions mutually independent. expression found computed eﬃciently runtime complexity proportional complexities statistical tests performed computing probability factor. bromberg margaritis argued computational cost performing statistical test data triplet proportional number data points dataset i.e. times total number variables involved test i.e. |z|. worst case complexity ib-score would full speciﬁcation search requires speciﬁcation search mechanism closure chosen structures. present follows approaches ibmap-hc ibmap-ts diﬀer search mechanism choice closure motivation choices two-fold. first serve realistic examples closures given section second implementations ib-score robust experimental conclusions. discuss detail approaches. given domain variables propose hill climbing local search mechanism ﬁnding best structure space undirected structures size starting structure search proceeds ﬁnding structure maximum ib-score among structure edge-ﬂip away edge-ﬂip pair variables consists removing edge edge exists adding otherwise. amount neighbors thus equals number pairs variables algorithm continues recursively structures ﬂip-away smaller ib-score i.e. algorithm reaches local maxima. starting structure chose structure output gsmn algorithm hill climbing search seen perturbation output gsmn hope local maxima proximity gsmn better quality. experimental results conﬁrm fact case. could done course structures output structure learning algorithm although expect improvements independencebased algorithms. experiments show results gsmn only. substraction consequent redundant neither blanket made explicit later convenience. also unless explicitly stated mention markov blanket refers minimal markov blanket boundary. let’s ﬁrst discuss computational complexity ib-score using markov blanket closure. according complexity obtain note variable variable either edge cases cannot happen. thus pair either condition l.h.s. union true condition r.h.s. true independence assertion added pair variables. therefore also note trick however reducing complexity neighbor’s ib-score order magnitude. diﬀerence neighbor currently visited structure exactly edge cases edge removed edge added blanket would change thus conditionant independence assertions closure containing either independence assertions would remain exactly thus incremental computation ib-score would require statistical tests resulting time complexity ib-score time complexity hill climbing step section introduce another algorithm ibmap-ts learning independence structure using independence-based approach section algorithm implements maximization tree search uniform cost algorithm using algorithm-based closure described brieﬂy example resulting algorithm eﬃcient requiring exponential number statistical tests structure maximum ib-score. however believe description presentation experimental results later helpful ways. first closure presented generic sense easily instantiated independence-based algorithm besides gsmn algorithm used here. also although algorithm exponential time quality improvements obtained experimentation help reinforce hypothesis ibmap approach improves quality structures learned. let’s start formalizing process followed independence-based algorithm. mentioned section independence-based algorithm proceeds follows iteration proposes triplet performs statistical independence test data obtain independence assertion tsit superscript included later convenience denotes independence value considered indicated statistical independence test. triplet selected iteration depends tsit sequence triplets proposed independence values tsit statistical tests assigned iteration independence-based structure learning algorithm thus summarized follows algorithms proceed independence assertions done suﬃcient determining structure. deﬁnition closure thus closure. call closure obtained algorithm-based closure. closure search? iteration propose considering besides independence assertion tsit alternative value ¬tsit words propose distrust statistical test. interestingly change aﬀect algorithm. would simply continue statistical test would given value. moreover would also structure eventually assertions found execution would thus closure well. clearly structure found ﬁnalize notice bifurcation splits sequence independences recursively split next iteration. modeled binary tree node corresponding independence assertion whose children correspond assignments triplet obtained applying assertions path root parent. exception root node represents dummy empty sequence node. path root leave determines structure determines closure structure. closure used compute ib-score structure multiplying posterior implemented tested ibmap-ts closure constructed using gsmn algorithm uniform cost strategy search tree. chose gsmn demonstrate approach well established independence-based markov networks structure learning algorithm. algorithm uses algorithm learning markov blanket every variable uses corollary build structure. results comparing quality structures learned gsmn versus ibmap-ts uniform cost gsmn based closure shown experimental results section. important advantage uniform cost solutions optimal. improve exponential runtime tested heuristics success avoiding exponential runtimes. thus limit ﬁndings uniform cost. fig. shows example search tree small system three random variables node annotated triplet independence assertion triplet posterior probability independence assertion local cost partial computation ib-score ﬁnally partial path-cost minus product node checkmark lowest path-cost thus next line expanded uniform-cost. describe several experiments testing eﬀectiveness ib-score improving quality independence structures discovered novel independence-based algorithms ibmap-hc ibmap-ts experiments also corroborate experiments benchmark sampled datasets. real world datasets allow assessment performance algorithms realistic settings disadvantage lacking solution network thus resulting approximate measures quality. used publicly available benchmark datasets obtained repositories machine learning datasets artiﬁcial datasets although limited scope results sampled known networks allowing systematic controlled study performance algorithms. using gibbs sampler data sampled several randomly generated undirected graphical models randomly generated graph parameters. considered models diﬀerent number variables diﬀerent number neighbors node given graphs generated connecting randomly uniformly nodes nodes. achieved connecting node ﬁrst nodes random permutation generated graphs parameters generated randomly following procedure described detail bromberg grow polynomially size largest triplet test depends connectivity networks kept ﬁxed instead elements predict behavior depends solely landscape score function. thus report experiments measure complexity. measured quality types errors output network model considered correct edges independences hamming distance. case artiﬁcial datasets comparison done true known underlying network whereas real datasets comparison done data itself. discuss quantities detail edge hamming distance graphs equal number nodes represents number edges exist exist vice versa. another measures minimum number edge substitutions required change graph other. measure error structure output structure learning algorithm measure edges hamming distance simply solution network formally deﬁne following indicator function evaluating existence edge variables given probability distributions variables deﬁne independences hamming distance number matches comparison independence assertions holds denotes possible triplets checked many triplets independent distributions normalized unfortunately size exponential. thus compute follows given fully speciﬁed distribution. instead dependending type dataset artiﬁcial real given independence structure sample respectively. independencies however measured independence structures datasets cases measure independence hamming distance. let’s consider cases. estimate error structures output algorithms artiﬁcial datasets measured independence hamming distance underlying true network querying independences directly structures using vertex-separation. formally denote result experiments real datasets underlying structure unknown. thus conducted experiments learning structure smaller datasets sizes input dataset compared independencies output structure ibmap-ts) gsmn competitor. report ratio errors network output ibmap-hc error network ggsm output gsmn. similarly network output ibmap-ts report ratio three diﬀerent types errors edge hamming distance results possible ratios three three ratios allows quick comparison algorithms involves ratio equal means error structures output ratio lower means reduction error structures output algorithms ratio greater means reduction quality algorithms. ﬁrst experiments demonstrate ibmap algorithms ibmap-hc ibmap-ts successful improving quality artiﬁcially generated datasets comparing edge errors well independence errors gsmn. ibmap-ts algorithm impractical larger domains considered scenarios underlying models size comparing errors ibmap-hc ibmap-ts gsmn larger models size comparing ibmap-hc gsmn assess algorithms different conditions reliability connectivity datasets increasing number datapoints sampled networks increasing node degrees increase statistical signiﬁcance datasets sampled pair them subsample size obtained randomly selecting datapoints. table shows edge errors reporting mean values standard deviations edge errors gsmn ibmap-ts ibmap-hc respectively diﬀerent conditions connectivity reliability. last columns show corresponding ratios indicating bold statistically signiﬁcant improvements underlined statistically signiﬁcant reductions errors shown table similar structure results show cases proposed algorithms present quality improvements gsmn. datasets connectivities hamming distances better equal ibmap-ts ibmap-hc cases table almost cases table cases improvement drastic meaning approximately wrong edges wrong edges ggsm pour results datasets connectivity explained recalling approximation made namely conditional independence assertions mutually independent. independence expected hold assertions involving common variables stronger dependence larger overlaps. tendency clearer results shown table plotted figure table shows mean value standard deviation edge hamming distance left group columns right group columns. again bold signiﬁes increase quality ibmap-hc quality gsmn. observe case edge hamming distance increases number bold ratios decreases number underlined ratios increases figure average datasets error bars represents standard deviation. smaller value important improvement values greater represent reduction quality. decrease decrease slower increases. although small pour results edge errors ratios never greater cases corresponding improvement independence error. instance case independence ratio value smaller statistical signiﬁcance. cases four cases show improvement case showing increase error. remaining cases edge errors show signiﬁcant improvements reaching many cases large ratios smaller proportions wrong edges gsmn ibmap-hc respectively conclude section present results number hill climbs conducted ibmap-hc. measured quantity runs algorithms datasets sampled graphs increasing size connectivities three diﬀerent reliability conditions figure shows four plots curves each dataset size give reader sense runtime report runtime hardest scenario. java virtual machine running athlon processor bits case took days reduction hours implementing simple cache avoiding computation repeated tests. conclude discussion experiments compare quality performances ibmaphc gsmn benchmark datasets. datasets underlying true network unknown thus restricted independence hamming distances measured dataset. table experiments results edge independence hamming distances several datasets increasing number data points standard deviations shown parenthesis. again quality improvements bold quality reductions underlined. results experiments shown table figure case table figure case. ﬁgures numbers xaxis represent indexes dataset corresponding table. tables also show last columns values results show improvements cases again shown bold tables bars lower ﬁgures. quality reductions underlined tables. figure experimental results real datasets. table shows average standard deviation subsets subsets again smaller values mean greater improvements. conclusion ibmap-hc algorithm provides possibility solving problem complex systems getting signiﬁcant quality improvements gsmn. ib-score seems good eﬃcient likelihood function. work several future research possibilities arise motivated pursue including continuing look practical algorithm lower computational cost experience approximate optimization algorithms local beam metropolis hastings continue analysis quality measures propose eﬃcient reliable scoring functions perform thorough comparison novel scoring functions existing scoring functions likelihood data given complete model work funded postdoctoral fellowship conicet argentina; scholarship program teachers national technological university ministry science technology productive innovation national agency scientiﬁc technological promotion foncyt; argentina. prove counter-positive minimality must variable removing blanket becomes dependent i.e. also since deﬁnition boundary holds lemma follows auxiliary lemma letting", "year": 2011}