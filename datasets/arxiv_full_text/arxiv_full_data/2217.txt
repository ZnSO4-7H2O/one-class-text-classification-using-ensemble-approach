{"title": "An Effective Training Method For Deep Convolutional Neural Network", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this paper, we propose the nonlinearity generation method to speed up and stabilize the training of deep convolutional neural networks. The proposed method modifies a family of activation functions as nonlinearity generators (NGs). NGs make the activation functions linear symmetric for their inputs to lower model capacity, and automatically introduce nonlinearity to enhance the capacity of the model during training. The proposed method can be considered an unusual form of regularization: the model parameters are obtained by training a relatively low-capacity model, that is relatively easy to optimize at the beginning, with only a few iterations, and these parameters are reused for the initialization of a higher-capacity model. We derive the upper and lower bounds of variance of the weight variation, and show that the initial symmetric structure of NGs helps stabilize training. We evaluate the proposed method on different frameworks of convolutional neural networks over two object recognition benchmark tasks (CIFAR-10 and CIFAR-100). Experimental results showed that the proposed method allows us to (1) speed up the convergence of training, (2) allow for less careful weight initialization, (3) improve or at least maintain the performance of the model at negligible extra computational cost, and (4) easily train a very deep model.", "text": "paper propose nonlinearity generation method speed stabilize training deep convolutional neural networks. proposed method modifies family activation functions nonlinearity generators make activation functions linear symmetric inputs lower model capacity automatically introduce nonlinearity enhance capacity model training. proposed method considered unusual form regularization model parameters obtained training relatively low-capacity model relatively easy optimize beginning iterations parameters reused initialization higher-capacity model. derive upper lower bounds variance weight variation show initial symmetric structure helps stabilize training. evaluate proposed method different frameworks convolutional neural networks object recognition benchmark tasks experimental results showed proposed method allows speed convergence training allow less careful weight initialization improve least maintain performance model negligible extra computational cost easily train deep model. convolutional neural networks enabled rapid development variety applications particularly ones related visual recognition tasks major reasons this building powerful models development efficient robust strategies. hand recent deep learning models becoming increasingly complex owing increasing depth width decreasing strides hand better generalization performance obtained using various regularization techniques designing models varying structure nonlinear activation functions however aforementioned techniques follow underlying hypothesis model highly non-linear many inputs directly fall nonlinear parts activation functions. although highly nonlinear structure improves model capacity leads difficulties training. training problems recently partly addressed carefully constructed initializations batch normalization however methods lead loss efficiency training deep neural networks. even though resnets outperform plain cnns still require warming trick train deep network thus challenges training completely solved. counterpart deep neural networks linear acti-vation functions relatively model capacity relatively easy optimize beginning training show. thus need strike maintain balance between difficulties training model ca-pacity. regard possibility combining ad-vantages lowcapacity high-capacity models motivating example multigrid method used accelerate convergence differential equations using hierarchy discretization. main idea underlying multigrid method speed computation using coarse grid interpolating correction computed fine grid. model capacity considered coarse approximation problem high-capacity model corresponds fine approximation. similarly unsupervised pre-training pre-training shallow networks correspond models relatively capacity coarsely training data. pre-trained parameters transferred complex model carefully dataset. makes sense combine advantages models different capacity first restricting capacity coarsely problem endowing model greater capacity training procedure hence enabling gradually perform better. note symmetric structures activation functions also make training procedure stable show. therefore modify family activation functions recified linear unit leaky relu parametric relu introducing trainable parameter call nonlinearity generator make activation functions linear symmetric inputs initial stage. introduce nonlinearity training procedure endowing model greater capacity. introduced parameter easily incorporated backpropagation framework. proposed method allows speed convergence training allow less careful parameter initializations improve least maintain performance convolutional neural network negligible extra computational cost easily train deep model. activation function relu lrelu prelu input activation function node trainable parameter controlling linearity generator given input distribution. note different node call equation element-wise version. also consider channel-wise variant different nodes channel share increases introduces greater nonlinearity increases probability inputs activation functions fall nonlinear parts. smaller minimum input inputs linear area making linear symmetric activation function inputs. figure compares shapes different activation functions versions given preprocessed input image proper weight initialization property guarantee model almost linear initial stage. show analysis capacity model relatively making training relatively easy. thus difficulties training alleviated initial iterations. easily optimized using back propagation algorithm. consider element-wise version example; derivative using gradient descent determine degree nonlinearity layer based gradient information training process endowing model greater capacity. since experiments shown performances channel-wise element-wise versions comparable former introduces small number extra parameters. first example show strong nonlinearity improves capacities models. used simple without activation functions baseline test dataset cifar-. model convolutional layers dense layer without activation functions softmax layer. convolutional layer contained three kernels loss function cross entropy. used stochastic gradient descend optimizer train model. initial learning rate divided loss longer decreased value. batch size adopt momentum simple data augmentation msra weight initialization added ng-relus different untrainable values convolutional layers make comparisons baseline model. finaly initially made trainable test training performance. table shows maximum training accuracies different models none means baseline model without activation functions. increased maximum training accuracy. trainable mean value increased training thus performance example shows nonlinearity changes model capacity. next experimentally show less nonlinear relatively easy train. goal explore critical depth depth beyond model converge. test models plain cnns initially made trainable parameter relus different parameters untrainable. tested critical depth. model structure without dataset cifar. used training strategy except xavier initialization used weight initialization experiments. table shows results. increased critical depth decreased indicating increasing difficulty training. discussed previously greater value corresponds higher model capacity thus models lower capacities relatively easy train. recall properties initially makes model almost linear automatically enhance model capacity during training. thus considered unusual form regularization training procedure parameters early stages obtained training relatively low-capacity model iterations parameters reused initialization higher-capacity model. inputs linear area using proper value model almost linear initial stage; training contained updated based gradient information hence endowing model greater capacity. extracted -layer plain changed training procedure. figure shows value layer different epochs. training proceeded became increasingly oscillatory increasing capacity model. advantage strategy model less likely overfit early stage training procedure restrict parameters particular regions first following expanded gradually seek better parameters. seen figure although training accuracies resnets relus lrelus prelus increased steadily test accuracy oscillated. comparison test accuracies resnets ng-relus ng-lrelus ng-prelus stable. many researchers shown approximately symmetric structure activation functions speed learning because alleviates mean shift problem. mean shift correction pushes off-diagonal block fisher information matrix close zero rendering solver closer natural gradient descent subsection show variance weight variation influence training well. following theorem. theorem given fully-connected neural network layers theorem shows variance weight variation layer closely related expectation activations gradients asymmetric structure activation function cause mean shift problem mean shift caused previous layer acts bias next layer. units correlated higher mean shift shift activation expectations turn affect expectation gradient. theorem mean shift problem makes upper lower bounds variance different layers unstable thus raise instability variation weights different layers. unstable variation weights would result unstable selu-msra selu-xavier selu-orthogonal ng-relu-msra ng-relu-xavier ng-relu-orthogonal ng-lrelu-msra ng-lrelu-xavier ng-lrelu-orthogonal ng-prelu-msra ng-prelu-xavier ng-prelu-orthogonal selu-msra selu-xavier selu-orthogonal ng-relu-msra ng-relu-xavier ng-relu-orthogonal ng-lrelu-msra ng-lrelu-xavier ng-lrelu-orthogonal ng-prelu-msra ng-prelu-xavier ng-prelu-orthogonal weight variance training hampers information flow raising training problems train deep model. stabilize training procedure want keep bounds stable. proposed symmetric respect original point linear area pull close zero making training procedure stable. figure shows comparisons weight variance -layer plain cnns layer resnets. compared relu weight variance using ng-relu stable supports analysis above. tested method models reported appendix). gtx-ti. focused fair performance comparisons models state-of-the-art results. thus used strategies training. modified activation functions model i.e. relu lrelu prelu ng-relu ng-lrelu ng-prelu respectively compare model performances counterparts scaled exponential linear units datasets cifar- cifar- used simple data augmentation method cifar- cifar-. experiments. plain cnns initial learning rate divided training procedure test error longer decreased. resnets initial learning rate divided epochs terminated training epochs. models used batch size weight decay evaluated method plain cnns resnets cifar-. test models -layer plain cnns -layer resnets different activation functions i.e. relus lrelus prelus versions selus. three parameter initialization methods—xavier initialization msra initialization orthogonal initialization —were used test behavior models. fig. shows training accuracy comparisons different models. models method insensitive different initializations models not. addition models converged much faster counterparts cases. resnets methods still robust initializations converged much faster counterparts other models relatively sensitive initializations. tested compared various models including -layer -layer plain cnns resnets cifar- cifar datasets. msra initialization used. note plain cnns relus lrelus prelus selus converge depths layers whereas models still converged. table shows test accuracy comparisons. although method first trained models relatively capacity still resulted better least comparable performances compared counterparts selus cases. method used improve performance. used cifar- test dataset tested performance plain cnns using relus ng-relus shown table test accuracy using ngrelus highest experiments. please appendix comparisons plain cnns also used -layer wider resnet structure shown appendix test performance. final test accuracy better -layer pre-activation resnet although introduces training parameters extra computational cost small. -layer plain ng-relus took seconds average epoch whereas relus prelus take seconds average. -layer resnet ng-relus take seconds average epoch whereas relus prelus took seconds seconds average epoch. explored critical depths plain cnns ngs. model depth gradually increased integral multiples layers test dataset cifar-. table shows results three weight initializations. critical depths method much greater. paper proposed nonlinearity generation method begins training relatively low-capacity model gradually improves model capacity. proposed training method modifies family activation functions introducing trainable parameter make activation functions linear symmetric inputs makes model relatively capacity easy optimize beginning. nonlinearity introduced automatically training procedure endow model greater capacity. introduced parameters easily incorporated training. proposed method considered unusual form regularization training procedure parameters early stages obtained training relatively low-capacity model iterations reused initialization higher-capacity model. derived upper lower bounds variance weight variation showed symmetric structure helps stabilize training. experiments showed proposed method speeds convergence training allows less careful initialization improves least maintains performance cnns negligible extra computational cost used improve performance. finally train deep model proposed method easily.", "year": 2017}