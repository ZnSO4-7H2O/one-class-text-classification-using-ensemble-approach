{"title": "Hierarchical Reinforcement Learning: Approximating Optimal Discounted  TSP Using Local Policies", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this work, we provide theoretical guarantees for reward decomposition in deterministic MDPs. Reward decomposition is a special case of Hierarchical Reinforcement Learning, that allows one to learn many policies in parallel and combine them into a composite solution. Our approach builds on mapping this problem into a Reward Discounted Traveling Salesman Problem, and then deriving approximate solutions for it. In particular, we focus on approximate solutions that are local, i.e., solutions that only observe information about the current state. Local policies are easy to implement and do not require substantial computational resources as they do not perform planning. While local deterministic policies, like Nearest Neighbor, are being used in practice for hierarchical reinforcement learning, we propose three stochastic policies that guarantee better performance than any deterministic policy.", "text": "function policy making possible generalize across different states. famous examples td-gammon deep network introduced deep neural network approximate value function leading high performance solving backgammon video games. different approach deals long horizons using policy network search among game outcomes efﬁciently leading super-human performance playing chess poker however utilizing approach possible simulate environment model-based still open problem long-standing approach dealing long horizons introduce hierarchy problem survey). focus options framework two-level hierarchy formulation options learned achieve subgoals policy options selects among options accomplish ﬁnal goal task. recently demonstrated learning selection rule among pre-deﬁned options using delivers promising results challenging environments like minecraft atari studies shown possible learn options jointly policy-over-options end-to-end work focus speciﬁc type hierarchy reward function decomposition dates back works studied among different research groups recently formulation option learns maximize local reward function ﬁnal goal option trained separately provides value function option policy options uses local values select among options. option responsible solving simple task options learned parallel across different machines. higher level policy trained using smdp algorithms different research groups suggested using pre-deﬁned rules select among options. work provide theoretical guarantees reward decomposition deterministic mdps. reward decomposition special case hierarchical reinforcement learning allows learn many policies parallel combine composite solution. approach builds mapping problem reward discounted traveling salesman problem deriving approximate solutions particular focus approximate solutions local i.e. solutions observe information current state. local policies easy implement require substantial computational resources perform planning. local deterministic policies like nearest neighbor used practice hierarchical reinforcement learning propose three stochastic policies guarantee better performance deterministic policy. unique characteristics human problem solving ability represent world different granularities. plan trip ﬁrst choose destinations want visit decide destination. hierarchical reasoning enables complexities world around simple plans computationally tractable reason. nevertheless successful reinforcement learning algorithms still performing planning abstraction level. provides general framework optimizing decisions dynamic environments. however scaling real-world problems suffers curses dimensionality; coping exponentially large state spaces action spaces long horizons. approach deals large state spaces introducing function approximation value example choosing option maximal value function choosing action maximizes option value functions using pre-deﬁned rules derive policies learning options learning fully decentralized. although many cases reconstruct options original would defeat entire purpose using options. work concentrate local rules select among available options. even speciﬁcally consider mdps {mi}n deterministic dynamics share components reward. given options reward optimal policy collecting reward interested deriving optimal policy collecting rewards i.e. solving ri}. setting optimal policy derived solving smdp whose actions speciﬁcally focus collectible rewards special type reward common navigation domains like minecraft deepmindlab vizdoom challenge dealing collectible rewards state space changes time collect reward since combinations remaining items considered state space grows exponentially number rewards. here show solving smdp considerations equivalent solving reward discounted traveling salesman problem similar classical traveling salesman problem computing optimal solution rd-tsp np-hard furthermore np-hard approximate solution value least optimal discounted return polynomial time. brute force approach solving rd-tsp requires evaluating possible tours connecting rewards. also adapt bellman–held–karp dynamic programming algorithm solve rd-tsp scheme identical tabular q-learning smdp still requires exponential time. blum proposed polynomial time planning algorithm rd-tsp computes policy collects least fraction optimal discounted return later improved planning algorithms need know entire smdp order compute approximately optimal policies. contrast work focus deriving analyzing policies local information make decisions; local policies simpler implement efﬁcient need learn reinforcement learning community already using simple local approximation algorithms rd-tsp. hope research provide important theoretical support comparing local heuristics addition introduce reasonable local heuristics. speciﬁcally prove worst-case guarantees reward collected algorithms relative reward optimal rd-tsp tour. also prove bounds maximum relative reward local algorithms collect. experiments compare performance local algorithms. particular main contributions follows. results establish impossibility results local policies showing deterministic local policy guarantee reward larger opt/n every stochastic policy guarantee reward larger opt/ every mdp. impossibility results imply nearest neighbor algorithm iteratively collects closest reward optimal constant factor amongst deterministic local policies. positive side propose three simple stochastic policies outperform best combines random depth first search guarantees performance least achieves least general case. combining jumping random reward sorting rewards distance slightly worse guarantee. simple modiﬁcation ﬁrst jump random reward continues there already improves guarantee o/n). deﬁne problem explicitly starting general transfer framework deﬁnition speciﬁc transfer learning setting collectible reward decomposition deﬁnition deﬁnition given mdps {mi}n deﬁnition describes general transfer learning problem similar transfer framework assumes mdps sharing reward signal. interested transfer learning i.e. using quantities learned mdps {mi}n speciﬁcally model-free given optimal options value functions interested zero-shot transfer i.e. deriving policies solving without learning deﬁnition reward decomposition reward represents collectible rewards reward signal {ri}n represents collectible prize i.e. particular state action otherwise. addition reward collected once. deterministic dynamics deterministic transition matrix i.e. action exactly value equals values equal zero. property deﬁnition requires decomposition previous rewards property requires local reward collectible prize. limiting generality models satisfy properties investigated theory simulation given value functions local policies optimal shortest path reward reward given following option state addition length shortest path denoted given value function since γdij notice state optimal policy always follow shortest path rewards. this assume exists policy following shortest path state next reward-state then improve taking shortest path contradicting optimality last observation implies optimal policy composition local options {oi}n property deﬁnition requires deterministic dynamics. property perhaps limiting three again appears numerous domains including many maze navigation problems arcade learning environment games like chess given deterministic transition matrix optimal policy make decisions states contain rewards. words policy arrived reward state decided reward-state follow optimal policies reaches collectible reward decomposition optimal policy derived smdp denoted state space contains initial state reward states {si}n action space replaced options {oi}n corresponds following optimal policy reaching state addition action space transition matrix deterministic since finally reward signal discount factor remain same. general optimal policies smdps guaranteed optimal original smdp includes options regular actions related study mann analyzed landmark options speciﬁc type options plan reach state deterministic mdp. landmark options related reward decomposition collectible rewards also represent policies plan reach speciﬁc state mdp. given landmark options mann analyzed errors searching optimal solution smdp instead searching original deﬁnition given policies value functions optimal errors equal zero. thus solution smdp guaranteed optimal mdp. addition analysis mann provides bounds dealing sub-optimal options nondeterministic dynamics help extend analysis cases future work. finally optimal policy derived solving rd-tsp this look graph includes initial state reward states. deﬁne length edge graph i.e. value following option state path graph deﬁned indices {it}n t=and length summarize modeling approach allows deal curse dimensionality three different ways. first option learned function approximation techniques e.g. deal high dimensional inputs like vision text. second formulating problem smdp reduces state space include reward states effectively reduces planning horizon third rd-tsp formulation derive approximate solutions dealing exponentially large state spaces emerge modeling one-time events like collectible rewards. basic idea proof ﬁrst reward larger reward collects. next propose simple easy implement stochastic adjustment vanilla algorithm better upper bound call r-nn algorithm starts collecting rewards random continues executing improvement seem small observation stochasticity improves performance guarantees local policies essential work. following sections derive sophisticated randomized algorithms better performance guarantees. previous subsection heuristic guarantees performance least next show impossibility result deterministic local policies indicating policy guarantee makes optimal policies. theorem deterministic local policy d-local exists graph nodes discount factor that proof sketch. consider family graphs consists star central vertex leaves distance starting vertex central vertex reward leaf. graph family corresponds different subset leaves connect edges length central vertex local policy cannot distinguish among rewards therefore choice graphs start deﬁning local policy. policies analyze local policies. local policy mapping inputs current state history containing previous steps taken policy; particular encodes collectible states already visited discounted return reward current state i.e. {vi}n whose output distribution options. notice local policy full information graph formally deﬁnition local policy πlocal mapping start analysis natural heuristics famous algorithm. context problem policy selecting option highest estimated value exactly like shall abuse notation slightly name algorithm value; confusion arise. general graphs know deterministic policies. observation least motivated component stochastic algorithms. theorem discounted reward traveling salesman graph nodes discount factor stated theorem analysis conducted three steps. ﬁrst steps assume achieved value collecting rewards segment length ﬁrst step considers case second step remove requirement analyze performance nn-rdfs worst value third step considers value collected completes proof. second third steps loose logarithmic factors. since segment length collects value least opt/ second guessing good enough approximation theorem instance rd-tsp rewards proof. step assume collects sopt rewards segment length dmin dmax shortest longest distances reward sopt respectively. triangle inequality dmax dmin assume show rdfs lemma path length less previous subsection deterministic local policies could guarantee showed optimal policies small stochastic adjustment improve guarantees. observations motivated look better local policies broader class stochastic local policies. begin providing better impossibility result policies theorem theorem stochastic local policy s-local exists graph nodes discount factor policy achieves lower bound propose analyze stochastic policies substantially improve deterministic upper bound. policies satisfy occam’s razor principle i.e. policies better guarantees also complicated require computational resources. describe nn-rdfs policy best performing local policy able derive. policy performs probability local policy call rdfs probability rdfs starts random node continues performing edges shorter chosen random specify later. runs edges shorter rdfs continues performing guess guarantees probability true value factor guess work approximation true value factor approximations degrade bounds factor log. step finally consider general case collect value segment length larger notice value collects rewards follow ﬁrst segments length tour means exists least segment length collects least value. combining analysis previous step proof complete. describe nn-ra policy similar spirit nn-rdfs policy performs probability local policy call probability starts random node sorts nodes increasing order distance visits nodes order. algorithm simple implement require guessing parameters however comes cost worse worst case bound. performance guarantees nn-ra method given theorem analysis follows steps proof nn-rdfs algorithm. emphasize here pruning parameter used analysis purposes part algorithm. consequently logarithmic factor performance bound theorem contrast theorem theorem discounted reward traveling salesman graph nodes discount factor last inequality follows triangle inequality. step assume gets value rewards collects segment length recall nn-rdfs policy either probability rdfs probability picking single reward closest starting point gets least value opt. otherwise probability rdfs starts rewards picked then analysis step sets rdfs collects value collected section evaluate compare performance deterministic stochastic local policies measuring reward achieved algorithm different rd-tsp instances function number rewards place rewards collect almost within constant discount always place initial state origin i.e. deﬁne denotes short distance. next describe scenarios considered evaluation. graph types generate nmaps different graphs report reward achieved algorithm average nmaps graphs worst-case algorithms stochastic report average results i.e. graph algorithm nalg times report average score. finally provide visualization different graphs tours taken different algorithms helps understanding numerical results. random cities. vanilla rewards randomly distributed plane known algorithm yields tour longer optimal average used similar input compare algorithms rd-tsp speciﬁcally inspecting figure algorithm performs best average worst case. observation suggests rewards distributed random selecting nearest reward reasonable thing addition nnrdfs performs best among stochastic policies hand policy performs worst among stochastic policies. happens sorting rewards distances introduces undesired zig-zag behavior collecting rewards equal distance line. graph demonstrates scenario greedy algorithms like r-nn likely fail. rewards located three different groups; contains rewards. group rewards located cluster left origin group located cluster right origin closer group group also located right rewards placed increasing distances i-th reward located inspecting results r-nn indeed perform worst. understand this consider tour algorithm takes. goes group stochastic tours depend choice belongs group figure evaluation deterministic stochastic local policies different rd-tsps. cumulative discounted reward policy reported average worst case scenarios. collect group left right perform relatively same. belongs group ﬁrst collect rewards left ascending order come back collect remaining rewards right performing relatively same. however group nn-rdfs nn-ra visit group going r-nn tempted group going random clusters. graph demonstrates advantage stochastic policies. ﬁrst randomly place cluster centers circle radius distance circle) draw reward ﬁrst draw cluster center uniformly draw scenario motivated maze navigation problems collectible rewards located rooms rooms fewer rewards collect. inspecting results nn-rdfs r-nn perform best particular worst case scenario. reason picks nearest reward value comes rewards collected cluster. hand stochastic algorithms visit larger clusters ﬁrst higher probability achieve higher value circle. graph origin radii circle equal distances. here nn-rdfs performs best among policies since collects rewards closer ﬁrst. greedy algorithms hand tempted collect rewards take outer circles results lower values. rural urban. here rewards sampled mixture normal distributions. half rewards located city i.e. position gaussian random variable small standard deviation s.t. n)). half located village i.e. position gaussian random variable s.t. graph worst case scenario stochastic policies perform much better happens mistakenly choosing rewards take remote places rural area stochastic algorithms remain near city high probability collect rewards. algorithms qualitatively compare algorithm stochastic algorithms rdfs graphs present rewards displayed grid using gray dots. graph type present single graph sampled appropriate distribution. display tours taken different algorithms corresponds single algorithm. stochastic algorithms present best worst tours among different runs finally better interpretability display ﬁrst rewards tour policy collects value unless mentioned otherwise. discussion. random cities inspecting worst tours stochastic tours longer tour distance ﬁrst reward addition observe zig-zag behavior tour nn-ra collecting rewards equal distanced causes perform worst scenario. best tours exhibit similar behavior case located closer line recall graph rewards located three different groups; containing rewards. ordered left right group cluster rewards located left origin distance group also cluster located right slightly shorter distance origin group group also located right rewards placed increasing distances i-th reward located visualization purposes added small variance locations rewards groups rescaled axes. vertical lines rewards represent groups cropped graph ﬁrst rewards group observed. finally chose ﬁrst half tour displayed ﬁrst groups visited tour. examining best tours ﬁrst visits group tempted going right group harms performance. hand stochastic algorithms staying closer origin collect rewards groups ﬁrst. case worst case tours algorithms perform relatively same. random clusters here ﬁrst visits cluster nearest origin. nearest cluster necessarily largest practice collects rewards cluster traverses remaining clusters result lower performance. different approach tackle challenges multitask learning optimize options parallel policy options method achieves goal local sarsa algorithm similar function learned locally option however local functions learnt on-policy respect policy options argmaxa instead learned off-policy learning. russell zimdars showed policy options updated parallel local sarsa updates local sarsa algorithm promised converge optimal value function. work provided theoretical guarantees reward decomposition deterministic mdps allows learn many policies parallel combine composite solution efﬁciently safely. particular focused approximate solutions local therefore easy implement require many computational resources. local deterministic policies like nearest neighbor used practice hierarchical reinforcement learning. study provides important theoretical guarantee reward collected three policies well impossibility results local policy. policies outperform worst case scenario; evaluated average worst case scenarios suggesting better one. hand since stochastic algorithms selecting ﬁrst reward random high probability reach larger clusters achieve higher performance. circles scenario distance adjacent rewards circle longer distance adjacent rewards consecutive circles. examining tours indeed r-nn taking tours lead outer circles. hand rdfs staying closer origin. local behavior beneﬁcial rdfs achieves best performance scenario. however performs well best case performance much worse algorithms worst case. hence average performance worst scenario. rural urban graph large rural area city located near origin hard visualize. improve visualization here chose ﬁrst half tour displayed. since half rewards belong city choosing ensures tour reaching city ﬁrst segment tour displayed. looking best tours taking longest tour reaches city stochastic algorithms reach earlier. stochastic algorithms collect many rewards traversing short distances therefore perform much better scenario. related work pre-deﬁned rules option selection used several studies. karlsson suggested policy chooses greedily respect local qvalues argmaxa humphrys suggested choose option highest local q-value argmaxaiqi greedy combination local policies optimized separately necessarily perform well. barreto considered transfer framework similar focus collectible reward decomposition instead proposed framework rewards linear reward features similar suggested using pre-deﬁned rule option selection addition authors provided performance guarantees using form additive error bounds provide impossibility results. contrast prove multiplicative performance guarantees well three stochastic policies. also proved ﬁrst time impossibility results local option selection methods. barreto andre munos remi schaul silver david. successor features transfer reinforcement learning. advances neural information processing systems beattie charles leibo joel teplyashin denis ward wainwright marcus k¨uttler heinrich lefrancq andrew green simon vald´es v´ıctor sadik amir deepmind lab. arxiv preprint arxiv. bellemare marc naddaf yavar veness joel bowling michael. arcade learning environment evaluation platform general agents. journal artiﬁcial intelligence research blum avrim chawla shuchi karger david lane terran meyerson adam minkoff maria. approximation algorithms orienteering discounted-reward tsp. siam journal computing mann timothy arthur mannor shie precup doina. approximate value iteration temporally extended actions. journal artiﬁcial intelligence research mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg human-level control deep reinforcement learning. nature moravˇc´ık matej schmid martin burch neil lis`y viliam morrill dustin bard nolan davis trevor waugh kevin johanson michael bowling michael. deepstack expert-level artiﬁcial intelligence heads-up no-limit poker. science junhyuk xiaoxiao honglak lewis richard singh satinder. action-conditional video prediction using deep networks atari games. advances neural information processing systems junhyuk singh satinder honglak kohli pushmeet. zero-shot task generalization multi-task deep reinforcement learning. proceedings international conference machine learning higgins irina arka rusu andrei matthey loic burgess christopher pritzel alexander botvinick matthew blundell charles lerchner alexander. darla improving zeroshot transfer reinforcement learning. proceedings international conference machine learning kempka michał wydmuch marek runc grzegorz toczek jakub ja´skowski wojciech. vizdoom doom-based research platform visual reinforcement learning. computational intelligence games ieee conference ieee kulkarni tejas narasimhan karthik saeedi ardavan tenenbaum josh. hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. advances neural information processing systems silver david huang maddison chris guez arthur sifre laurent driessche george schrittwieser julian antonoglou ioannis panneershelvam veda lanctot marc mastering game deep neural networks tree search. nature silver david hubert thomas schrittwieser julian antonoglou ioannis matthew guez arthur lanctot marc sifre laurent kumaran dharshan graepel thore mastering chess shogi self-play general reinforcement learning algorithm. arxiv preprint arxiv. bapst victor pascanu razvan heess nicolas quan john kirkpatrick james czarnecki wojciech hadsell raia. distral robust multitask reinforcement learning. advances neural information processing systems seijen harm fatemi mehdi romoff joshua laroche romain barnes tavian tsang jeffrey. hybrid reward architecture reinforcement learning. advances neural information processing systems vezhnevets alexander sasha osindero simon schaul heess nicolas jaderberg silver david kavukcuoglu koray. feudal networks hierarchical reinforcement learning. proceedings international conference machine learning andrew chi-chin. probabilistic computations toward proceedings uniﬁed measure complexity. annual symposium foundations computer science ieee computer society proof. consider family graphs consists star central vertex leaves. starting vertex central vertex reward leaf. length edge chosen graph corresponds subset leaves pairwise connect form clique. since hand local policy central vertex cannot distinguish among rewards therefore every graph picks ﬁrst reward distribution. policy continues choose rewards distribution hits ﬁrst reward argue formally every s-local policy small expected reward graph yao’s principle consider expected reward d-local policy uniform distribution probability d-local picks ﬁrst n-size clique. assuming ﬁrst vertex vertex probability clique second vertex clique deﬁned similarly. d-local picks vertex clique reward however time d-local misses clique collects single reward suffers discount neglecting rewards collected hits clique total value d-local proof. consider family graphs consists star central vertex leaves. starting vertex central vertex reward leaf. length edge chosen s.t. graph family corresponds different subset leaves connect edges length leaves connected central vertex.) therefore choice graphs follows that given policy exists graph adjacent rewards visited last. finally since last inequality follows triangle inequality. step assume gets value rewards collects segment length recall nn-ra policy either probability probability picking single reward closest starting point gets least value opt. otherwise probability starts rewards picked then analysis step sets value collected follows r-nn analyze performance guarantees r-nn method. analysis conducted steps. ﬁrst step assume achieved value collecting rewards consider case second step considers general case analyzes performance nn-random worst value emphasize unlike previous algorithms assume time collects rewards segment length theorem discounted reward traveling salesman graph nodes proof. step assume collects rewards. deﬁne fractional power affect asymptotics result) denote {cj} obtained pruning edges longer deﬁne large contains rewards. observe since least large exists. lemma assume large component path covered starting reaches large component. length number rewards collected note proof. step assume collects sopt rewards segment length dmin dmax shortest longest distances reward sopt respectively. triangle inequality dmax dmin assume threshold below denote {cj} sopt created deleting edges longer among vertices sopt. lemma assume starts vertex component |cj| since diameter collects ﬁrst vertices within total distance collects least |cj| rewards traveling total distance shortest longest distances sopt respectively. triangle inequality dmax dmin therefore constant sopt. taking expectation probability ﬁrst random pick follows similar analysis nn-rdfs assume collects value rewards collects segment length recall r-nn either probability random pick probability followed picking single reward closets starting point gets least value opt. notice need assume anything length tour takes collect rewards follows that log) proof. preﬁx ends reward length distance reward reward since reward neighbor distance reward thus solution recurrence lemma visits r-nn large large exists unvisited reward distance shorter lemma lemma imply following corollary. corollary assume path reward large reward large connected component. denote length number rewards rk+kθ. following lemma concludes analysis step. lemma preﬁx r-nn length number segments r-nn connect rewards large contain internally rewards small ccs. number rewards r-nn collects segment. lemma guarantees r-nn collects rewards traversing distance next notice chance belongs large larger finally similar nn-rdfs assume value greater constant fraction i.e. n/α. means must collected ﬁrst n/α+ rewards traversing distance denote fraction rewards sopt. denote dmin dmax this recall traversing distance achieved less n/α+. since already traversed achieve less n/α+ remaining rewards thus contraction assumption achieved n/α. exact solutions rd-tsp present variation held-karp algorithm rdtsp. note similar denotes length tour visiting cities last however formulation required deﬁnition additional recursive quantity accounts value function shortest path. using notation observe held-karp identical tabular q-learning smdp since held-karp known exponential complexity follows solving using smdp algorithms also exponential complexity. provide exact polynomial time solutions based dynamic programming simple geometries like line star. note solutions cannot derived general geometries. given rd-tsp instance rewards located single line easy optimal policy collects time either nearest reward right left current location. thus time rewards already collected continuous interval ﬁrst uncollected reward left origin denoted ﬁrst uncollected reward right origin denoted action take next either collect collect since able classify state space polynomial size contains states optimal policy describe dynamic programming scheme ﬁnds optimal policy. algorithm computes table maximum value collecting rewards starting deﬁned analogously starting algorithm ﬁrst initializes entries either entries correspond cases rewards left agent collected. iterates counter number rewards left collect. value deﬁne combinations partitioning rewards right left agent. increasing value entry take largest among value collect rewards appropriately discounted value collect analogously. optimal value starting position note algorithm computes value function; policy merely track argmax maximization step. consider rd-tsp instance rewards located d-star i.e. rewards connected central connection point lines rewards along line. denote rewards line ordered origin line focus case agent starts origin. easy optimal policy collects time uncollected reward nearest origin along lines. thus time rewards already collected continuous intervals origin ﬁrst uncollected reward along line denoted {i}d action take next collect nearest uncollected rewards. follows state optimal agent uniquely deﬁned tuple current location agent. observe therefore possible states optimal policy could since able classify state space polynomial size contains states optimal policy describe dynamic programming scheme ﬁnds optimal policy. algorithm computes table maximum value collecting rewards starting algorithm ﬁrst initializes entries except exactly entry. entries correspond cases rewards collected except line segment iterates counter number rewards left collect. value deﬁne combinations partitioning rewards among lines. increasing value algorithm optimal solution rd-tsp line. rewards denoted left right. denote distance reward reward denote maximum value collecting rewards starting reward similarly denote maximum value collecting rewards starting leftmost reward collected deﬁne algorithm optimal solution rd-tsp d-star. denote amount rewards collect rewards line denote along line center star line. denote distance reward line reward line ﬁrst uncollected reward along line denoted maximum value collecting remaining rewards mdnd starting reward deﬁned {i}d rewards collected line deﬁne", "year": 2018}