{"title": "Smart Augmentation - Learning an Optimal Data Augmentation Strategy", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "A recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks(DNN). There are many techniques to address this, including data augmentation, dropout, and transfer learning. In this paper, we introduce an additional method which we call Smart Augmentation and we show how to use it to increase the accuracy and reduce overfitting on a target network. Smart Augmentation works by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss. This allows us to learn augmentations that minimize the error of that network.  Smart Augmentation has shown the potential to increase accuracy by demonstrably significant measures on all datasets tested. In addition, it has shown potential to achieve similar or improved performance levels with significantly smaller network sizes in a number of tested cases.", "text": "augmentation serves type regularization reducing chance overﬁtting extracting general information database passing network. classify augmentation methods different types. ﬁrst unsupervised augmentation. type augmentation data expansion task done regardless label sample. example adding different kind noise rotating ﬂipping data. kinds data augmentations usually difﬁcult implement. challenging kinds data expansion mixing different samples label feature space order generate sample label. generated sample recognizable valid data sample also sample representative speciﬁc class. since label data used generate sample kind augmentation viewed type supervised augmentation. many deep learning frameworks generate augmented data. example keras built method randomly rotate scale images training methods improve performance used blindly. example mnist adds rotation network unable distinguish properly handwritten digits. likewise system uses deep learning classify interpret road signs become incapable discerning left right arrows training augmented indiscriminate ﬂipping images. sophisticated types augmentation selectively blending images adding directional lighting rely expert knowledge. besides intuition experience universal method determine speciﬁc augmentation strategy improve results training. since training deep neural nets time-consuming process means limited number augmentation strategies likely attempted deployment model. blending several samples dataset order highlight mutual information trivial task practice. samples mixed together many mixed problem data augmentation using blending techniques. abstract—a recurring problem faced training neural networks typically enough data maximize generalization capability deep neural networks. many techniques address this including data augmentation dropout transfer learning. paper introduce additional method call smart augmentation show increase accuracy reduce overﬁtting target network. smart augmentation works creating network learns generate augmented data training process target network reduces networks loss. allows learn augmentations minimize error network. smart augmentation shown potential increase accuracy demonstrably signiﬁcant measures datasets tested. addition shown potential achieve similar improved performance levels signiﬁcantly smaller network sizes number tested cases. ﬁrst probably important task access enough labeled samples data. enough quality labeled data generate overﬁtting means network highly biased data seen training therefore able generalize learned model samples. discussion much diversity training data mixing different datasets affect model generalization. mixing several datasets might good solution always feasible lack accessibility. approaches solving problem using different regularization techniques. recent years different regularization approaches proposed successfully tested deep neural network models. droptechnique batch normalization wellknown regularization methods used avoid overﬁtting training deep models. another technique addressing problem called augmentation. data augmentation process supplementing dataset similar data created information dataset. augmentation deep learning ubiquitous dealing images often includes application rotation translation blurring modiﬁcations existing images allow network better generalize augmentation typically performed trial error types augmentation performed limited imagination time experience researcher. often choice augmentation strategy important type network architecture used convolutional neural networks became norm computer vision research features handcrafted. handcrafting features went style shown convolutional neural networks could learn best features given task suggest since generate best features speciﬁc pattern recognition tasks might able give best feature space order merge several samples speciﬁc class generate sample label. idea generate merged data produces best results speciﬁc target network intelligent blending features samples. manual augmentation techniques rotating ﬂipping adding different kinds noise data samples described depth attempt measure performance gain given speciﬁc augmentation techniques. also provide list recommended data augmentation methods. srivastava introduced dropout technique aiming reduce overﬁtting especially cases enough data. dropout works temporarily removing unit artiﬁcial neural network connections unit. jaderberg devised image blending strategy part paper synthetic data artiﬁcial neural networks natural scene text recognition used call natural data blending image layers blended randomly sampled crop image training dataset. note signiﬁcant increase accuracy using synthetic images image layers blended together random process. another related technique training adversarial examples. goodfellow note that although augmentation usually done goal creating images similar possible natural images expects testing need case. demonstrate training adversarial examples increase generalization capacity network helping expose overcome ﬂaws decision function generative adversarial neural networks powerful unsupervised learning technique uses min-max strategy wherein ’counterfeiter’ network attempts generate images look enough like images within dataset ’fool’ second network second network learns detect counterfeits. process continues synthetic data nearly indistinguishable would expect real data look like. generative adversarial neural networks also used generate images augment datasets strategy employed shrivastav another method increasing generalization capacity neural network called transfer learning. transfer learning want take knowledge learned network transfer another case convolutional neural networks used technique reduce overﬁtting small datasets common trained weights large network trained speciﬁc task starting point training network perform well another task. batch normalization introduced another powerful technique. discovered upon realization normalization need performed input layer also achieved intermediate layers. like regularization methods smart augmentation attempts address issue limited training data improve regularization reduce overﬁtting. method attempt produce augmentations appear natural. instead network learns combine images ways improve regularization. unlike address manual augmentation network attempt learn simple transformations. unlike approach image blending arbitrarily randomly blend images. smart augmentation used conjunction regularization techniques including dropout traditional augmentation. goal smart augmentation learn best augmentation strategy given class input data. learning merge samples class. merged sample used train target network. loss target network used inform augmenter time. result generating data target network. process often includes letting network come unusual unexpected highly performant augmentation strategies. training phase networks network generates data; network network perform desired task main goal train network speciﬁc task enough representative samples given dataset. another network generate samples. network accepts several inputs class generates output approximates data class. done minimizing loss function accepts image input. output network mage selected sample class input. constraint network input output network shape type. example samples p-channel image network output single pchannel image. network either implemented single network multiple networks ﬁgure using network advantage networks learn class-speciﬁc augmentations suitable classes work well given class. network neural network generative model difference network inﬂuenced network back propagation step network accepts multiple samples input simultaneously instead time. causes data generated network converge best choices train network speciﬁc task time controlled loss function ensures outputs similar members class. overall loss function training function whose output transformation function could epoch-dependent function i.e. function could change epoch number. training process error back-propagates network network tunes network generate best augmentations network training ﬁnished network model network used test process. joint information data samples exploited reduce overﬁtting increase accuracy target network training. proposed method uses network learn best sample blending speciﬁc problem. output network used input network idea network learn best data augmentation train network network accepts several samples class dataset generates sample class sample reduce training loss network ﬁgure output network designed gender classiﬁcation. image left merged image two. image represents sample class male appear dataset still identifying features class. notice ﬁgure image created open mouth open eyes images. quality face image produced network matter. ability help network better generalize. approach applicable classiﬁcation tasks also applications approach selective blending sample features improves performance. observations show approach reduce overﬁtting increase accuracy. following sections evaluate several implementations smart augmentation technique various datasets show improve accuracy prevent overﬁtting. also show smart augmentation train small network perform well much larger network produces state results. evaluate method chose datasets characteristics would allow examine performance algorithm speciﬁc types data. since goal paper measure impact proposed technique attempt provide comparison techniques work well databases. comparison refer gender datasets places dataset. ﬁrst dataset composed faces database total frontal faces male female subjects. data split subject exclusive training validation testing sets training validation testing. face images reduced grayscale pixel values normalized compare traditional augmentation smart augmentation examine effect traditional augmentation smart augmentation created augmented version every combination ﬂipping blurring rotation resulted larger training images. test validation sets unaltered data split subject exclusive training validation testing sets training validation testing. face images reduced pixel values normalized feret second dataset feret dataset. converted feret grayscale reduced size image pixel values normalized data split subject exclusive training validation testing sets training validation testing. color feret version collected december august made freely available intent promoting development face recognition algorithms. images labeled gender pose name. fig. image left created learned combination images right. type image transformation helped increase accuracy network image produced ideal approximation face instead contains features helped network better generalize concept gender task trained for. although feret contains large number high-quality images different poses varying face obstructions certain similarities quality background pose lighting make easy modern machine learning methods correctly classify. experiments images feret gender labels exist. adience third dataset adience. converted adience grayscale images size normalized pixel values data split subject exclusive training validation testing sets training validation testing. restricted ﬁrst classes dataset pixel values normalized small dataset rescaled color channels used experiments without modiﬁcation except normalization pixel values analyze effectiveness smart augmentation performed experiments using datasets different parameters. brief overview experiments seen table experiments conducted motivation answering following questions listed below used three neural network architectures varied parameters connection mechanisms. experiments architectures combined various ways speciﬁed table network simple small convolutional neural network trained classiﬁer takes image input outputs class labels softmax layer. network illustrated ﬁgure network unmodiﬁed implementation described network large network takes image input outputs class labels softmax layer. network convolutional neural network takes images input outputs modiﬁed image. experiments seen table trained gender classiﬁcation using technique illustrated ﬁgure experiments smart augmentation train network gender classiﬁcation using speciﬁed database. ﬁrst images randomly selected class dataset. samples merged channels single sample. grayscale values ﬁrst image mapped channel grayscale values second image mapped channel reach number channels speciﬁed experiments table. channel image network network fully convolutional neural network accepts images input gives images size output single channel. additional grayscale image randomly selected class dataset loss function network calculated mean squared error randomly selected image output network output network target image network separate inputs. network typical deep neural network convolutional layers followed batch normalization max-pooling steps convolutional layer. fully connected layers placed network. ﬁrst layers units second dense layer made units output network using softmax. dense layer takes advantage drop-out technique order avoid overﬁtting. loss function network calculated categorical cross-entropy outputs targets. total loss whole model linear combination loss functions networks. approach designed train network generates samples reduce error network validation loss calculated network without considering network. allows compare validation loss without smart augmentation. experiments varied number input channels datasets used. speciﬁcally trained network scratch input channels single network channels network channels network shown table experiments. experiments evaluate different implementation smart augmentation containing separate network class. before ﬁrst images randomly selected class dataset. samples merged channels single sample.the grayscale values ﬁrst image mapped channel grayscale values second image mapped channel reach number channels speciﬁed experiments table before. since network important separate loss functions network illustrated ﬁgure important difference updated learning rate performing initial experiments noticed using learning rate dying relu problem stopped effective learning within ﬁrst epochs. network also sensitive variations batch size. goal experiments examine using multiple network impacts accuracy overﬁtting compared using network also wanted know differences trained manually augmented database experiments train network perform gender classiﬁcation without applying network training stage. experiments intended serve baseline comparison network learn without smart augmentation speciﬁc dataset measure improvement given smart augmentation. full implementation network shown ﬁgure before fully connected layers placed network. ﬁrst layers units second dense layer units dense layer takes advantage drop-out technique order avoid over-ﬁtting. previous experiments section used different face datasets. experiments examine suitability smart augmentation color scenes around world places dataset evaluate method data completely different topic. varied parameter global loss function could identify inﬂuence results. unlike previous experiments also retained color information. experiment utilized trained scratch classiﬁer chosen models performed well places dataset public competitions input network images output determined class softmax classiﬁer. experiment network identical respects used previous subsection except lower learning rate speciﬁed experiments table take color images places instead gender. experiments images randomly selected class dataset. samples merged channels single sample. values ﬁrst three channels image mapped channel ﬁrst three channels second image mapped channels reach number channels speciﬁed experiments table multiplied number color channels source images. channel image used network network fully convolutional neural network) accepts images input outputs image. additional image randomly selected class dataset. loss function network calculated mean squared error randomly selected image output network output network target image network separate inputs. network typical deep neural network convolutional layers followed batch normalization max-pooling steps convolutional layer. fully connected layers placed network. ﬁrst layers units second dense layer made units output network using softmax. dense layer takes advantage drop-out technique order avoid over-ﬁtting. loss function network calculated categorical cross-entropy outputs targets. total loss whole model linear combination loss functions networks. approach designed train network generates samples reduce error network validation loss calculated network without considering network allows compare validation loss without smart augmentation. experiments varied number input channels datasets used. speciﬁcally trained network scratch input channels network channels network channels network shown table experiments. experiments images randomly selected class dataset. samples merged channels single sample. values ﬁrst three channels image mapped channel ﬁrst three channels second image mapped channels reach number channels speciﬁed experiments table multiplied number color channels source images. channel image network network fully convolutional neural network accepts images input outputs single color image. additional image randomly selected class dataset. loss function network calculated mean squared error randomly selected image output network output network target image network separate inputs. network typical deep neural network convolutional layers followed batch normalization max-pooling steps convolutional layer. fully connected layers placed network. ﬁrst layers units second dense layer made units output network using softmax. dense layer takes advantage drop-out technique order avoid over-ﬁtting. loss function network calculated categorical cross-entropy outputs targets. dataset faces faces faces faces faces faces faces faces faces faces faces faces faces faces faces faces faces faces faces faces adience adience feret feret total loss whole model linear combination loss functions networks. approach designed train network generates samples reduce error network validation loss calculated network without considering network allows compare validation loss without smart augmentation. experiments varied number input channels datasets used. speciﬁcally trained network scratch input channels network channels network channels network shown table experiments. smart augmentation technique could prevent network overﬁtting training stage. smaller difference training loss validation loss caused smart augmentation technique shows approach helps network learn general features task. network also higher accuracy test trained smart augmentation. noticeable pattern vary number inputs network despite lack pattern signiﬁcant difference observed channels providing best results respectively. lower channels performed worst accuracies recall accuracy without network faces dataset. suspect much variation accuracy reported chance. since particular experiment images chosen randomly times images helpful mutual information present chance opposite also possible. interesting channels used network accuracy note traditional augmentation improved accuracy without smart augmentation gender classiﬁcation task. smart augmentation realize improvement accuracy fig. training validation losses experiments showing reductions overﬁtting using smart augmentation. smaller difference training loss validation loss caused smart augmentation technique shows approach helps network learn general features task. avoid confusion remind reader loss smart augmentation given means loss graphs combination losses networks whereas losses without smart augmentation experiments show approaches distinct network class tend slightly outperform networks network seems provide support initial idea network used class class-speciﬁc augmentations could efﬁciently learned. networks input channels excluded average increase accuracy smart augmentation used median accuracy going experiment smart augmentation performed worse using smart augmentation. seen table channel caused accuracy contrasted smart augmentation used. expected channel used mutual information effectively utilized. experiment shows importance always using least channels. previously discussed results places dataset used networks multiple network performed slightly better. also notice higher increase accuracy realized. demonstrates smart augmentation used traditional augmentation improve accuracy. cases examined smart augmentation performed better traditional augmentation. however since practical limits types traditional augmentation performed guarantee manual augmentation could better augmentation strategy. major concern since claim smart augmentation replace traditional augmentation. claim smart augmentation help regularization. finally found smart augmentation small network achieved better results obtained much larger network help enable practical implementations networks embedded systems consumer devices large size networks limit usefulness. future work include expanding smart augmentation learn sophisticated augmentation strategies performing experiments larger datasets larger numbers data classes. statistical study identify number channels give highest probability obtaining optimal results could also useful. research funded strategic partnership program science foundation ireland fotonation ltd. project /spp/i next generation imaging smartphone embedded platforms. work also supported irish research council employment based programme award. project ebppg// signiﬁcant results experiments comparison smart augmentation network trained alone. note small network trained alone accuracy compared small network trained smart augmentation accuracies ranging indicates smart augmentation cases allow much smaller network replace larger network. smart augmentation shown potential increase accuracy demonstrably signiﬁcant measures datasets tested. addition shown potential achieve similar improved performance levels signiﬁcantly smaller network sizes number tested cases. paper discussed regularization approach called smart augmentation automatically learn suitable augmentations process training deep neural network. focus learning augmentations take advantage mutual information within class. proposed solution tested progressively difﬁcult datasets starting highly constrained face database ending highly complex unconstrained database places. various experiments presented work demonstrate method appropriate wide range tasks demonstrates biased particular type image data. primary conclusion experiments demonstrate augmentation process automated speciﬁcally nontrivial cases samples certain class merged nonlinear ways resulting improved generalization target network. results indicate deep neural network used learn augmentation task time task learned. demonstrated smart augmentation used reduce overﬁtting training process reduce error testing. thirdly experiments demonstrated better accuracy could achieved smart augmentation traditional augmentation alone. found altering parameters loss function slightly impacts results experiments needed identify optimal parameters found. bazrafkan nedelcu filipczuk corcoran deep learning facial expression recognition step closer smartphone knows moods ieee international conference consumer electronics goodfellow bengio courville deep learning. press http//www.deeplearningbook.org. lemley bazrafkan corcoran deep learning consumer devices services pushing limits machine learning artiﬁcial intelligence computer vision. ieee consumer electronics magazine vol. zisserman synthetic data artiﬁcial neural networks natural scene text recognition corr vol. abs/. available http//arxiv.org/abs/. goodfellow shlens szegedy explaining harnessing adversarial examples arxiv preprint arxiv. goodfellow pouget-abadie mirza warde-farley ozair courville bengio generative adversarial nets advances neural information processing systems theano development team theano python framework fast computation mathematical expressions arxiv e-prints vol. abs/. available http//arxiv.org/abs/ lemley abdul-wahid banik andonie comparison recent machine learning techniques gender recognition facial images modern artiﬁcial intelligence cognitive science conference zhou lapedriza xiao torralba oliva learning deep features scene recognition using places database advances neural information processing systems phillips moon rizvi rauss feret evaluation methodology face-recognition algorithms ieee transactions pattern analysis machine intelligence vol. simonyan zisserman very deep convolutional networks large-scale image recognition arxiv preprint arxiv. sutskever martens dahl hinton importance initialization momentum deep learning. icml vol.", "year": 2017}