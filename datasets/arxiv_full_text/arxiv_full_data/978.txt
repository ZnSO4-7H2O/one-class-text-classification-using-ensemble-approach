{"title": "Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer  Free Energy", "tag": ["cond-mat.dis-nn", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Restricted Boltzmann machines are undirected neural networks which have been shown to be effective in many applications, including serving as initializations for training deep multi-layer neural networks. One of the main reasons for their success is the existence of efficient and practical stochastic algorithms, such as contrastive divergence, for unsupervised training. We propose an alternative deterministic iterative procedure based on an improved mean field method from statistical physics known as the Thouless-Anderson-Palmer approach. We demonstrate that our algorithm provides performance equal to, and sometimes superior to, persistent contrastive divergence, while also providing a clear and easy to evaluate objective function. We believe that this strategy can be easily generalized to other models as well as to more accurate higher-order approximations, paving the way for systematic improvements in training Boltzmann machines with hidden units.", "text": "restricted boltzmann machines undirected neural networks shown effective many applications including serving initializations training deep multi-layer neural networks. main reasons success existence efﬁcient practical stochastic algorithms contrastive divergence unsupervised training. propose alternative deterministic iterative procedure based improved mean ﬁeld method statistical physics known thouless-anderson-palmer approach. demonstrate algorithm provides performance equal sometimes superior persistent contrastive divergence also providing clear easy evaluate objective function. believe strategy easily generalized models well accurate higher-order approximations paving systematic improvements training boltzmann machines hidden units. restricted boltzmann machine type undirected neural network surprisingly many applications. model used problems diverse dimensionality reduction classiﬁcation collaborative ﬁltering feature learning topic modeling also quite remarkably shown generative rbms stacked multi-layer neural networks forming initialization discriminative deep belief nets deep architectures believed crucial learning high-order representations concepts. training procedure rbms written log-likelihood maximization exact implementation approach computationally intractable smallest models. however fast stochastic monte carlo methods speciﬁcally contrastive divergence persistent made large-scale training practical efﬁcient. methods popularized rbms even though entirely clear approximate methods work well paper propose alternative deterministic strategy training rbms neural networks hidden units general based so-called mean ﬁeld extended mean ﬁeld methods statistical mechanics. strategy used train neural networks number earlier works fact entirely visible networks adaptive cluster expansion mean ﬁeld methods lead spectacular results learning boltzmann machine representations however unlike fully visible models hidden units must taken account training procedure. welling hinton presented similar deterministic mean ﬁeld learning algorithm general boltzmann machines hidden units considering priori potentially efﬁcient extension tieleman tested method detail rbms found provided poor performance compared pcd. wake papers little inquiry made direction apparent consensus deterministic mean ﬁeld approach ineffective training. goal challenge consensus going beyond na¨ıve mean ﬁeld mere ﬁrst-order approximation introducing second- possibly third- order terms. principle even possible extend approach arbitrary order. using extended mean ﬁeld approximation commonly known thouless-anderson-palmer approach statistical physics training performance signiﬁcantly improved na¨ıve mean ﬁeld approximation even comparable pcd. clear easy evaluate objective function along extensible nature approximation paves systematic improvements learning efﬁciency. restricted boltzmann machine viewed layer undirected bipartite neural network speciﬁc case energy based model wherein layer visible units fully connected layer hidden units. denote binary visible hidden units indexed respectively energy given state {vi} {hj} given entries matrix specifying weights couplings visible hidden units biases external ﬁelds language statistical physics visible hidden units respectively. thus parameters {wij deﬁne model. joint probability distribution visible hidden units given gibbs-boltzmann measure normalization constant known partition function physics. given writing marginal terms important features model easily computed summed analytically since hidden units conditionally independent visible units owing rbm’s bipartite structure. however calculating computationally intractable since number possible states scales combinatorially number units model. complexity frustrates exact computation gradients log-likelihood needed order train parameters gradient ascent. monte carlo methods training rely observation simulated lower computational cost. nevertheless drawing independent samples ∂wij model order approximate derivative computationally expensive often approximate sampling algorithms used instead. here present physics-inspired tractable estimation free energy rbm. approximation based high temperature expansion free energy derived georges yedidia context spin glasses following pioneering works refer reader review topic. apply georges-yedidia expansion free energy start general energy based model possesses arbitrary couplings undifferentiated binary spins energy gibbs-boltzmann measure wijsisj. also restore role temperature usually energy based models multiplying energy functional boltzmann weight inverse temperature next apply legendre transform free energy standard procedure statistical physics ﬁrst writing free qisi. external ﬁeld eventually value order recover true free energy. legendre transform given function conjugate variable {mi} maximizing maximizing auxiliary ﬁeld function conjugate variables inverse function exactly equal operator refers average conﬁguration boltzmann since derivative measure conjugate variable fact equilibrium magnetization vector finally observe free energy also inverse lengendre transform legendre transform dependence product must carefully taken account. inﬁnite temperature spins decorrelate causing average value arbitrary product spins equal product local magnetizations; useful zeroth-order term corresponds entropy non-interacting spins constrained magnetizations values. taking expansion ﬁrst-order term recover standard na¨ıve mean ﬁeld theory. second-order term known onsager reaction term equations higher orders terms systematic corrections ﬁrst derived entropy contribution introduced denote magnetization visible hidden units equal viewed weak coupling expansion wij. recover estimate free energy must minimized respect arguments lastly writing stationary condition obtain self-consistency constraints magnetizations. instance second-order obtain following constraint visible magnetizations sigm logistic sigmoid function. similar constraint must satisﬁed hidden units well. clearly stationarity condition obtained order utilizes terms order within sigmoid argument consistency relations. whatever order approximation magnetizations solutions non-linear coupled equations cardinality number units model. finally provided deﬁne procedure efﬁciently derive value magnetizations satisfying constraints obtain extended mean ﬁeld approximation free energy denote emf. recalling log-likelihood shown tractable approximation obtained weak coupling expansion long solve coupled system equations magnetizations shown spirit iterative belief propagation propose self-consistency relations serve update rules magnetizations within iterative algorithm. fact convergence procedure rigorously demonstrated context random spin glasses expect convergence properties remain present even real data. iteration self-consistency relations hidden visible magnetizations written using time indexing follows careful application values minimizing thus providing value obtained running eqs. converge ﬁxed point. note present iteration second-order above third-order terms easily introduced procedure. using estimation iterative algorithm detailed previous section calculate possible estimate gradients log-likelihood used unsupervised training model substituting emf. note deterministic iteration propose estimating stark contrast stochastic sampling procedures utilized end. instance gradient ascent update weight approximated computed differentiating ﬁxed computing value derivatives ﬁxed points eqs. obtained iterative procedure. gradients respect visible hidden biases derived similarly. interestingly merely ﬁxed-point magnetizations visible hidden units priori training procedure sketched used order weak coupling expansion. training algorithm introduced shown perform poorly training recovered retaining ﬁrst-order expansion calculating emf. taking second-order expect training efﬁciency performance greatly improved fact including third-order term training algorithm easy including second-order fact particular structure model admit triangles corresponding factor graphs. although third-order term include distinct pairs units well coupled triplets units triplets excluded bipartite structure rbm. however coupled quadruplets contribute fourth-order term therefore fourthhigher-order approximations require much expensive computations though possible utilize adaptive procedures evaluate performance proposed deterministic training algorithm perform number numerical experiments separate datasets compare results pcd. ﬁrst mnist dataset labeled handwritten digit images dataset split training images test images. subsets contain approximately fraction digit classes image comprised pixels taking values range mnist dataset binarized setting non-zero pixels experiments exception experiment train version mnist dataset rescaled second pixel version caltech silhouette dataset constructed caltech image dataset silhouette dataset consists black regions primary foreground scene object white background. images labeled according object original picture unevenly represented object labels. dataset split training validation test sets. datasets models require visible units. following previous studies evaluating rbms datasets number hidden units experiments. training adopt mini-batch learning procedure gradient averaging training points batch mnist training points batch caltech silhouette. fig. estimates per-sample log-likelihood mnist test normalized total number units function number training epochs hidden unit rbm. results different training algorithms plotted different colors color code used panels. left panel pseudo log-likelihood estimate. difference algorithms contrastive divergence algorithms minimal. right panel log-likelihood estimate order. improvement clear. perhaps reasonably demonstrates advantage pcd. notice second-order approximation provides less noisy estimates lower computational cost. test learning algorithm presented section various settings. first compare implementations utilizing ﬁrst-order second-order third-order approximations higher orders considered greater complexity. next investigate training quality self-consistency relations magnetizations converged calculating derivatives instead iterated small ﬁxed number iterations approach similar cd-. furthermore also evaluate persistent version algorithm similar implementation magnetizations points dubbed fantasy particles updated maintained throughout training order estimate persistent procedure takes advantage fact rbm-deﬁned boltzmann measure changes slightly training epochs. convergence ﬁxed point magnetizations epoch therefore sped initializing converged state previous update. ﬁnal experiments consist persistent training algorithms using iterations magnetization self-consistency relations persistent training algorithm using iterations comparison. lastly evaluate training rescaled non-binarized mnist dataset experiment designed mimic pre-training second stacked deep belief net. setting training data second consists non-binary magnetizations derived hidden units ﬁrst operating true binary training data experiment iterations used estimate clamped term well free energy term computation log-likelihood gradients. comparison also train models using following prescriptions implemented tieleman given goal compare training approaches rather achieving best possible training across free parameters neither momentum adaptive learning rates included implementations tested. however employ weight decay regularization trainings keep weights small; necessity weak coupling expansion relies. comparing learning procedures plot free parameters training identically. results presented averages independent trainings standard deviations reported error bars. ﬁrst observation implementations training algorithms overly belabored. free parameters relevant procedures found equally well suited training algorithms. fact shown left panel fig. right inset fig. ascent pseudo log-likelihood training epochs similar training methods trainings. fig. fantasy particles generated hidden unit epochs training mnist dataset p-mf p-tap fantasy particles represent typical samples generated trained used generative prior handwritten numbers. samples generated p-tap similar subjective quality perhaps slightly preferable generated certainly preferable generated p-mf. fig. test classiﬁcation accuracy mnist caltech silhouette datasets using logistic regression hidden unit marginal probabilities function number epochs used train hidden unit rbm. baseline comparison classiﬁcation accuracy logistic regression performed directly data given black dashed line. here p-tap refers p-tap training algorithm performed rescaled non-binarized version mnist dataset. results different training algorithms displayed different colors color code used panels. pseudo log-likelihood training epochs caltech silhouette dataset. interestingly caltech silhouettes dataset seems persistent algorithms tested difﬁculties ascending pseudo-likelihood ﬁrst epochs training. contradicts common belief persistence yields accurate approximations likelihood gradients. complexity training classes unevenly represented training points might explain unexpected behavior. persistent fantasy particles converge similar non-informative blurs earliest training epochs many epochs required resolve particles distribution values informative pseudo log-likelihood. examining fantasy particles also gives idea performance generative model. fig. randomly chosen fantasy particles epoch training p-mf p-tap displayed. trained generates recognizable digits model seems trouble generating several digit classes fantasy particles extracted p-mf training poorer quality half drawn particles featuring non-identiﬁable digits. p-tap algorithm however appears provide qualitative improvements. digits visually discerned visible defects found particles. particles seem indicate indeed possible efﬁciently persistently train without converging ﬁxed point magnetizations. relevance log-likelihood training conﬁrmed right panel fig. observe ascend second-order log-likelihood even though explicitly constructed optimize objective. expected persistent algorithm iterations magnetizations achieves best maximization however p-tap iterations magnetizations achieves similar performance perhaps making preferable faster training algorithm desired. moreover note although p-tap demonstrates improvements respect p-mf p-tap yield signiﬁcantly better results p-tap. perhaps surprising since third order term expansion consists many terms second order smaller order {wij}. also evaluate training algorithms perspective supervised classiﬁcation. interpreted deterministic function mapping binary visible unit values real-valued hidden unit magnetizations. case hidden unit magnetizations represent contributions learned features. although supervised ﬁne-tuning weights implemented tested quality features learned different training algorithms usefulness classiﬁcation tasks. datasets logistic regression classiﬁer calibrated hidden units magnetizations mapped labeled training images using scikit-learn toolbox purposely avoid using sophisticated classiﬁcation algorithms order place emphasis quality training classiﬁcation method. fig. mnist classiﬁcation accuracy rbms trained p-tap algorithms roughly equivalent obtained using training training yields markedly poorer classiﬁcation accuracy. slight decrease performance along training epochs increase might emblematic over-ﬁtting non-persistent algorithms although decrease test log-likelihood observed. note classiﬁcation accuracy trained rescaled mnist data marginally better tested approaches implies training real-valued data successful. consequently even algorithm designed binary visible units used equally real visible variables treating magnetizations commonly done algorithm. finally caltech silhouettes dataset classiﬁcation task shown right panel fig. much difﬁcult priori. interestingly persistent algorithms yield better results task. however observe performance deterministic training least comparable pcd. presented method training rbms based extended mean ﬁeld approximation. although na¨ıve mean ﬁeld learning algorithm already designed rbms judged unsatisfactory shown extending beyond na¨ıve mean ﬁeld include terms second-order brings signiﬁcant improvements ﬁrst-order approach allows practical efﬁcient deterministic training performance comparable stochastic training algorithms. demo implementation algorithm provided online. extended mean ﬁeld theory also provides estimate log-likelihood easy evaluate thus enables practical monitoring progress unsupervised learning throughout training epochs. furthermore training real-valued magnetizations theoretically well-founded within presented approach shown successful experimentation. results pave many possible extensions. instance would quite straightforward apply kind expansion gauss-bernoulli rbms well multi-label rbms. extended mean ﬁeld approach might also used learn stacked rbms jointly rather separately done deep boltzmann machine deep belief network pre-training strategy shown promise fact approach generalized even non-restricted boltzmann machines hidden variables little difﬁculty. another interesting possibility would make higher-order terms series expansion using adaptive cluster methods used believe results show extended mean ﬁeld approach particular thouless-andersonpalmer good starting point theoretically analyze performance rbms deep belief networks. research leading results received funding european research council european union’s framework programme thank lenka zdeborov´a numerous discussion. hinton training products experts minimizing contrastive divergence neural computation vol. hinton salakhutdinov reducing dimensionality data neural networks science vol. galland limitations deterministic boltzmann machine learning network vol. kappen ortiz boltzmann machine learning using mean field theory linear response correction advances opper saad advanced mean ﬁeld methods theory practice. press bolthausen iterative construction solutions equations sherrington–kirkpatrick model communications hinton practical guide training restricted boltzmann machines computer vol. pedregosa scikit-learn machine learning python journal machine learning research vol.", "year": 2015}