{"title": "Observational Learning by Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Observational learning is a type of learning that occurs as a function of observing, retaining and possibly replicating or imitating the behaviour of another agent. It is a core mechanism appearing in various instances of social learning and has been found to be employed in several intelligent species, including humans. In this paper, we investigate to what extent the explicit modelling of other agents is necessary to achieve observational learning through machine learning. Especially, we argue that observational learning can emerge from pure Reinforcement Learning (RL), potentially coupled with memory. Through simple scenarios, we demonstrate that an RL agent can leverage the information provided by the observations of an other agent performing a task in a shared environment. The other agent is only observed through the effect of its actions on the environment and never explicitly modeled. Two key aspects are borrowed from observational learning: i) the observer behaviour needs to change as a result of viewing a 'teacher' (another agent) and ii) the observer needs to be motivated somehow to engage in making use of the other agent's behaviour. The later is naturally modeled by RL, by correlating the learning agent's reward with the teacher agent's behaviour.", "text": "observational learning type learning occurs function observing retaining possibly replicating imitating behaviour another agent. core mechanism appearing various instances social learning found employed several intelligent species including humans. paper investigate extent explicit modelling agents necessary achieve observational learning machine learning. especially argue observational learning emerge pure reinforcement learning potentially coupled memory. simple scenarios demonstrate agent leverage information provided observations agent performing task shared environment. agent observed effect actions environment never explicitly modeled. aspects borrowed observational learning observer behaviour needs change result viewing ’teacher’ observer needs motivated somehow engage making agent’s behaviour. later naturally modeled correlating learning agent’s reward teacher agent’s behaviour. humans evolved live societies major beneﬁt ability leverage knowledge parents ancestries peers understanding world rapidly develop skills deem crucial survival. learning done observing behaviour agents; emerges role modeling imitation observational learning. particularly interested observational learning paper deﬁne ability agent modify behavior acquire information effect observing another agent sharing environment. machine learning literature popular successful ways modeling goalmotivated learning agents reinforcement learning recent years combining increased representational power deep learning memory capabilities recurrent models lead string impressive successes ranging video-game playing navigation tasks robotics motivated part these want study observational learning naturally emerge deeprl agents empowered memory. main questions would want answer then coupled memory enough successfully tackle observational learning? agent learn ignore leverage teacher? signal enough emergence complex behaviour like imitation goal emulation information seeking? words want understand whether agents explicitly modeled learning agents combination perception memory motivation enough learn sole observation agents’ effects shared environment. worth noting similar questions investigated cognitive behaviour science community. work bandura proposed coined term ’observational learning’ social learning according observational learning differs imitative learning strictly require duplication behavior exhibited teacher/expert. heyes distinguished imitation non-imitative social learning following imitation occurs animals learn behavior observing conspeciﬁcs whereas non-imitative social learning occurs animals learn environment observing others meaning learn dynamics environment observing agents evolving environment. learning help teacher means idea machine learning neither. imitation learning long standing machine learning literature body work distinguish major ideas behaviour cloning regressing directly onto policy another agent/expert inverse trying infer reward function behaviour agents this conjunction techniques optimize inferred function. methods successfully applied variety tasks problematic aspect scenarios almost always need provide learning agent expert trajectories state-action space learner. otherwise explicit mapping learner teacher state space discovered previously argued/recognized somewhat restrictive unrealistic requirements. furthermore inverse explicitly model behaviour/trajectories coming another agent infer reward signal. order problem become tractable time need make structural assumptions reward like linearity given feature space smoothness assumptions might model closely true reward signal minor approximation errors easily ampliﬁed planning onto faulty signal given increased complexities propose study simpler natural alternative observational learning moving away traditional setup learning experts. address problems tackled imitation learning literature. merely arguing might scenarios level modelling required agent learn directly pure observations. context main contribution exhibit scenarios observational learning emerging standard deeprl algorithm combined memory. scenarios agent shares environment another agent better knowledge task solve. learner observes expert sole perception environment. rewarded performing task doesn’t receive incentive follow imitate interact expert. expert aware watched learner meant teach provide extra information learner neither. building tasks increasing difﬁculty show complex behaviours imitative non-imitative learning emerge without explicit modeling expert. addition provide theoretical insights explain behaviours possible. next section describe details experimental design. section provides general background theoretical foundations work. section provide details experimental results concluding section explained introduction primarily interested agent learn leverage behaviour expert based solely external reward ability observe consequences expert’s actions environment. learner notion agency teacher. additional agent simply part learner’s environment choose ignore presence expert deems signal unimportant. worth noting case studies presence teacher impact dynamics rewards agent. necessarily assumption formalized section ﬁrst question want answer whether teacher’s presence impact learner. look scenarios learner perfect information learn optimal behaviour learner partial information environment/task expert’s behaviour provide additional information providing demonstration desired behaviour. ﬁrst case expect difference learnt policies without expert. nevertheless adding teacher environment effectively expanding state space learning agent. thus policy student also needs learn ignore extra signal state space. second scenario however expert’s behaviour contains crucial information improving student’s policy. case ignoring expert student still complete task hand sub-optimally. real incentive learning agent attention teacher. nevertheless student’s reward signal still coming directly environment agent needs somehow correlate reward signal behaviour exhibited teacher. highly non-trivial association learning agent needs make. learn exploit order improve policy. agents reward structure good strategy learning agent would imitate possible expert’s behaviour. this principle much easier safer policy attempting randomly explore environment own. student would need solve local problem following expert would need worry global task global planning done expert. although might optimal kind behaviour transferable tasks and/or environments could potentially provide student better initial policy exploring unfamiliar environment. could lead principled/guided explore unknown environment major impact speed agent discovers areas interest especially sparse reward setting. finally interested showing student become autonomous still perform task optimally absence expert learning observations. indeed ﬁnal goal learning agent solve tasks still able reach goal learned optimal policies teacher. notation markov decison process tuple sapr states actions available agent transitional kernel giving probabily next states given current state action reward function discount factor. deﬁne policy maps states probability actions. given policy deﬁne value function expected cumulative discounted reward associated following policy introducing expert learner’s environment making visible observational state learner change learner’s mdp. resulting parameterized follows state space consists part state space directly inﬂuenced learner refer part state space controllable part state space part state space learner direct control over still part observational state includes useful information s¬c. case include observations corresponding presence teacher. given factorization transition dynamics naturally factorizes dimensions thus policy expert stationary introducing extra agent results another valid mdp. hand policy agent stationary induced transition kernel change every time switch expert’s policy. consider mdps share states actions transition dynamics discount factor differ reward function {mt|mt saprt consider uniformly sampling mdps unrolling episode given sampled choice. episode terminated re-sample repeat process start episode. procedure deﬁnes another sapr holds transitional dynamics shared across candidate mdps. interested policy performs well expectation across family mdps. teacher part environment isomorphic leading following distinguish mdps teacher including expert environment respective optimal policies ˜π∗. introducing expert environment. question would like answer whether possible better stationary policy placing student augmented agent access another agent’s behaviour observational state. another consequence moving augmentation state space results also expansion policy space consideration. since possible policies superset policies easy optimal policy augmented space least good optimal policy original agent leverage behaviour teacher order learn policy better deﬁned given setup take closer look particular case studies might occur imitation. consider case action-space expert. assume teacher’s policy better stationary policy deﬁned otherwise incentive deviate behaviour proposed cloning inferior. includes actions taken expert imitation trivially take place reading desired action actually known result observing experts’ actions mandatory achieve actual imitation purely observational setting though student would typically access directly actions performed expert. nevertheless also know deterministic environment observing effects one’s actions environment enough infer/encode action performed. learn mapping need ’remember’ keep track least consecutive observations teacher’s state. thus imitation emerge system able distill information observational state. agents need memory augmentation state space include several time steps. information seeking behaviour. identiﬁed learner’s policy become independent teacher task optimally performed learning agent optimal policy make without input teacher. denote assume agents starting position relaxed ﬁrst employing policy gets learning agent close enough teacher mimicking behaviour occur. assume learner always step behind teacher longer time difference negotiated memory component. additional assumption given optimal policy without expert same. formally thus identity task known optimal behaviour agent would switch optimal policies given context policy optimal sampled mdps results optimal behaviour provided context known. information distilled observation expert learner improve stationary policy deﬁned thus s.t. stationary policy augmented state space outperfom note principle take account computation series observations expert practically implemented stacking several recent observations current state space relying memory/recurrent model distill temporal information part hidden state infer current context. experiments choose widely used deeprl algorithm asynchronous advantage actor-critic algorithm learns policy value function given state observation approximations share intermediate representation diverge ﬁnal fully-connected layer. policy given softmax actions. setup closely follows including entropy regularization addition lstm layer incorporate memory. simple feed-forward recurrent version experiments. details refer reader original paper. scenarios consider series simple navigation tasks. ﬁrst start two-room layout place agents environment. possible goal locations considered corners rooms start episode sample uniformly location goal make information available teacher times. learning agent consider including occluding goal observational space. second version task student know goal activated point time. learn possible positions reward eight corners. ignores expert’s behaviour optimal stationary policy would visit locations minimum time possible. hand teacher access goal seek directly. observing behaviour learner could potentially disentangle corner active least narrow correct room information improve performance. provide student top-view representation environment. one-hot encoding element present environment channel layout position agent position teacher position goal implement four variations observational space lagt. figure level performance training measured number steps goal. curves learning agent alone environment. green curves learning agent shares environment teacher. blue curves learning scratch. bold coloured curves goal present light coloured curves goal occluded figure level performance training measured number steps goal. curves learning agent alone environment. green curves learning agent shares environment teacher. blue curves learning scratch bold coloured curves goal present light coloured curves goal occluded goal occluded agent better leveraging expert. experiment agent small convolutional layers followed fully connected layer. results displayed figures ﬁrst thing notice that perfect information learned policies without teacher asymptotic performance. moreover speed reached negatively impacted expansion state space. thus experiment conclude presence absence teacher impact learning process observed across multiple experiments second case student impoverished knowledge environment clearly spot difference performance student sharing environment teacher acting own. ﬁrst part training agent achieves similar performance point agent sharing environment teacher manages leverage extra signal signiﬁcantly improve policy. observed student able exploit expert’s behaviour beneﬁt want know transferable knowledge across different environments. test constructed levels adding additional room time thus smoothly increasing difﬁculty task. depiction levels found figure extending environment also naturally extend number possible goal locations. average number steps goal increases goal location occluded ’blind’ stationary policy quite expensive thus becomes crucial agent leverage expert. train levels curriculum fashion ﬁrst train level continue training level ﬁnally level mainly done speed training time also expect figure local view performance training levels curriculum. green curves learning agent shares environment teacher. blue curves learning scratch environment. bold coloured curves goal present light coloured curves goal occluded teacher’s presence signiﬁcant impact performance training quality policy. universally improving least matching performance lone agent agent learn importance expert level continue employ knowledge learning process next levels. speciﬁcation observational state learner maintained throughout curriculum with/without teacher with/without goal visible. results compiled figure reference included baseline learning levels starting random initialization curriculum. curriculum helps learning process goal visible occluded performance slightly better begin convergence achieved considerably faster. importantly presence expert consistently improves policy ’blind’ agent across levels. worth noting also last level goal occluded baseline lone agent always able complete task whereas agent leveraging teacher almost instantaneously complete task episode seen previous section using reward signal obtain different policies environment augmented presence expert. although occluding goal might seem like somewhat artiﬁcial task proxy impoverished imperfect information. aknowledge natural example partial observability agent local view environment. thus simulate environment taking local window observation centered around agent. rest setup remains same note changing nature problem quite drastically. especially true thinks setting teacher picture. scenario learner actively pursue expert hope beneﬁting presence. cannot expert learner cannot learn increased complexity start rooms level basically treat pre-training simple navigation skills local view. level learner learn navigate goal visit potential locations goal hidden observe beneﬁt expert present. size local window observation size room agent middle room whole room. always means problem keeping track agent difﬁcult ﬁrst setting. size window ensures expert seen good proportion observations. learning curves found figure nevertheless different story emerges look training level first observe goal hidden teacher present improvement policy scenario agent alone environment. consistent seen global view. potential beneﬁt ’blind’ stationary policy enough student begins ’pay’ attention teacher uses observations improve policy. carries levels furthermore last levels cases teacher present asymptotic performance agent matches slightly outperforms lone agent goal visible. remarkable seeing improvement policy even goal visible. partial observability trajectory expert still contains valuable information goal might step teacher narrows possible locations. fact ﬁnal performance independent goal’s presence absence state space suggests information agent learns exploit behaviour teacher. visual inspection ﬁnal policies fully supports intuition. behaviour emerges following agent seeks teacher tries close possible simply follows goal. potentially useful transferable strategy different environment. test idea expand environment rooms without learning previously trained agent successfully negotiate environment help teacher succeeds ﬁnding goals never seen before. nevertheless relying always teacher somewhat unsatisfactory. fact eliminate teacher learning agent quite lost. reach goals currently visible otherwise continue waiting agent appear even attempting leave spawning room. agent previously negotiated environment help agent retained particularities environment need order break dependence propose simple strategy mask presence teacher probability increases time. probability reaches agent becomes completely independent teacher. start masking probability steps decrease ﬁnally process successfully achieved agent tackle environment without teacher. paper illustrates case meta learning reinforcement learning. demonstrates observational learning emerge reinforcement learning combined memory. argued observational learning important learning mechanism ’cheaper’ alternative current approaches learning experts. particular approach require explicit modelling expert mapping experience agent’s state action space. shown experiments relying reward signal given environment coupled sheer presence another agent lead variety behaviours ranging smoothly imitation information seeking. also demonstrated that curriculum could autonomous agent solves tasks without presence teacher although learned observations teacher. initial work extended further settings. especially want test scenarios goal teacher learner strictly aligned. performed preliminary study negative correlation results provided appendix. study optimal teacher much motivation correlated student. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. international conference machine learning piotr mirowski razvan pascanu fabio viola hubert soyer andy ballard andrea banino misha denil ross goroshin laurent sifre koray kavukcuoglu learning navigate complex environments. proceedings international conference learning representations nathan ratliff j.andrew bagnell siddhartha srinivasa. imitation learning locomotion manipulation. proceedings ieee-ras international conference humanoid robots zico kolter pieter abbeel andrew hierarchical apprenticeship learning application quadruped locomotion. advances neural information processing systems stéphane ross geoffrey gordon andrew bagnell. reduction imitation learning structured prediction no-regret online learning. proceedings international conference statistics mark michael littman james macglashan fiery cushman joseph austerweil. showing versus doing teaching demonstration. advances neural information processing systems bilal piot matthieu geist olivier pietquin. boosted reward-regularized classiﬁcation apprenticeship learning. proceedings international conference autonomous agents multi-agent systems bilal piot matthieu geist olivier pietquin. learning demonstrations worth estimating reward function? proceedings european conference machine learning", "year": 2017}