{"title": "Online Open World Recognition", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "As we enter into the big data age and an avalanche of images have become readily available, recognition systems face the need to move from close, lab settings where the number of classes and training data are fixed, to dynamic scenarios where the number of categories to be recognized grows continuously over time, as well as new data providing useful information to update the system. Recent attempts, like the open world recognition framework, tried to inject dynamics into the system by detecting new unknown classes and adding them incrementally, while at the same time continuously updating the models for the known classes. incrementally adding new classes and detecting instances from unknown classes, while at the same time continuously updating the models for the known classes. In this paper we argue that to properly capture the intrinsic dynamic of open world recognition, it is necessary to add to these aspects (a) the incremental learning of the underlying metric, (b) the incremental estimate of confidence thresholds for the unknown classes, and (c) the use of local learning to precisely describe the space of classes. We extend three existing metric learning algorithms towards these goals by using online metric learning. Experimentally we validate our approach on two large-scale datasets in different learning scenarios. For all these scenarios our proposed methods outperform their non-online counterparts. We conclude that local and online learning is important to capture the full dynamics of open world recognition.", "text": "abstract. enter data avalanche images become readily available recognition systems face need move close settings number classes training data ﬁxed dynamic scenarios number categories recognized grows continuously time well data providing useful information update system. recent attempts like open world recognition framework bendale tried inject dynamics system incrementally adding classes detecting instances unknown classes time continuously updating models known classes. paper argue properly capture intrinsic dynamic open world recognition necessary aspects incremental learning underlying metric incremental estimate conﬁdence thresholds unknown classes local learning precisely describe space classes. extend three existing metric learning algorithms towards goals using online metric learning. experimentally validate approach large-scale datasets diﬀerent learning scenarios. scenarios proposed methods outperform non-online counterparts. conclude local online learning important capture full dynamics open world recognition. introduction open world recognition framework introduced bendale attempt move beyond dominant classiﬁcation methods assuming static setting number training images ﬁxed well number classes model handle. address intrinsically dynamic nature recognition unconstrained settings i.e. scenarios possible predict priori many objects which system recognize. true robots equipped cameras deployed hospitals public spaces automatic tagging systems deal dynamically growing datasets forth. open world recognition systems diﬀer standard static visual classiﬁcation algorithms three features ability incrementally update model known categories data arrives; ability learn fig. proposed online open world recognition workﬂow labeled data presented continuously model used predict using current classiﬁers compute accuracy ﬁnally update mahalanobis metric class centroids bandwidths novelty thresholds incrementally. resulting model able update continuously internal representation known classes well detecting adding system categories seen initially training without need retrain whole system scratch ability detect whether incoming image depicts known category something needs learned. requirement adding classes favours metric learning approaches svms several metric learning methods proposed presenting features still methods estimate used metric threshold novelty detection initial closed classes keep metric threshold ﬁxed problem evolves. conﬂicts deﬁnition open world recognition structure problem progressively revealed data observed optimal parameters likely change time. paper argue properly model dynamics challenging open world recognition scenario necessary learn online metric novelty threshold instances classes arrive rather estimating initial closed classes done objective similar online learning stream mining therefore learn classiﬁers online incrementally update model whenever data available time up-to-date predictions known classes unknown classes experimentally incremental metric learning approaches demonstrate continuously updating metric data classes arrive leads better performance closed accuracy open accuracy. furthermore introduce method incremental learn threshold novelty detection uses current internal conﬁdences classiﬁer known classes. continuously tuning rejection threshold shows better performance classes added classiﬁer compared ﬁxed threshold previously used third contribution introduce non-linear local metric learning approach adapts local complexity space respect classes. experimentally show especially beneﬁcial open world recognition setting since ﬂexible modeling border known classes unknown classes. ﬁndings general applicable large class algorithms. demonstrate proposing online incremental learning extensions three non-parametric methods nearest class mean classiﬁer previously used incremental adding novel classes nearest non-outlier classiﬁer extension proposed open world recognition; nearest ball classiﬁer local learning method incrementally adding balls used streaming context before. three algorithms experiments show proposed extensions lead sizable advantage. incremental learning. huge literature incremental learning various extensions however incremental svms suffer several drawbacks among important extremely expensive update eﬃcient implementations multi-class incremental learning permit addition classes well incremental classiﬁers kuzborskij proposed max-margin based approach incremental learning novel classes exploited prior knowledge previous classes method conservative behavior tending privilege older classes respect performance-wise. methods tree-based approaches showed success addressing scalability test-time large scale visual recognition challenges recently challenges become dominated deep learning methods again main drawback approaches need priori knowledge categories availability whole training data learning phase. open learning. open recognition considers incompleteness knowledge world learning classiﬁer possible lack knowledge classes testing scheirer formulated problem open recognition static one-vs-all setting balancing open space risk empirical error. setting extended introducing compact abating probability model. work oﬀers robust methods handle unseen classes. however relies decision scores scale. fragoso proposed scalable version modeling matching scores contextualized general recognition problem. scalable incremental method leverage classiﬁer recently adapted larger scale vision problems recent approaches combining metric learning random forests contrast linear classiﬁer nearest ball classiﬁer non-linear local classiﬁer. incremental learning method adapts problem adding balls classiﬁer used classiﬁcation data streams action recognition videos best knowledge applied metric learning open recognition setting paper. open world recognition. bendale boult extended notion open recognition include incremental scalable learning leading comprehensive problem called open world recognition address algorithm coupled module limiting open space risk model combinations transformed spaces resulting model nearest-non outlier described section section introduce online incremental metric learning extension three recent non-parametric classiﬁers. classiﬁers used within open world online learning template described algorithm predict label incoming sample. d-dimensional vectors rm×d acting regularizer improves computational eﬃciency. metric learning used best low-rank mahalanobis distance optimizing log-likelihood correct classiﬁcation training data-set metric learned large classes obtained distance function shown generalize classifying novel classes however novel instances used class mean vectors metric updated novel classes. contrast describe method learns incrementally class means metric fig. illustration diﬀerent learning settings. closed-set recognition whole space assigned speciﬁc class open recognition classes clear boundaries. local learning allows ﬂexible class boundaries useful open world recognition setting. classiﬁer designed predict whether instance unknown class known classes. accommodate novelty prediction next describe nearest non-outlier algorithm open world classiﬁcation scenario. nearest-non outlier method extension open world scenario adjusted deﬁne class boundaries instances beyond class boundaries assigned unknown class instead using multi-class probability deﬁned conﬁdence score class given incremental learning rejection. extend allow incremental learning metric automatically tuning class-rejection threshold formulate prediction conﬁdence similarly rbf-kernel function strictly bounded. using also reduces open space risk deﬁned since obeys abating property given function value decreases areas away observed training data. bandwidth parameter learned incrementally using expected value distances class means learning means metric resort incremental updates deﬁned eqs. known limitation class mean models limited ﬂexibility representation results linear classiﬁers. next section introduce local learning approach allows non-linear classiﬁcation. incremental metric learning. ball deﬁned center radius local class probability number samples within ball assigned class total number samples assigned ball. predicting class label example ball classiﬁer uses local class probability nearest learn balls follow uses distance training sequence observed training examples used incrementally build balls cover region feature space span. time step denote nearest ball training example updates intrinsic dimension space mean updated using correctly predicted samples radius updated using initial radius count number errors made within ball far. novelty detection. ball classiﬁer important local properties local class probability ball radius. latter could seen indicator local complexity feature space feature space locally smooth respect class labels radius likely large ball complex non-smooth feature space ball radius small. combine properties estimation prediction conﬁdence. combines local class probability kernel estimate local bandwidth twice radius ball intuitively assigns highest conﬁdence examples closer ball pure distribution. opposed global bandwidth local bandwidths deﬁned ball radii. desired conﬁdence level inversely proportional time number current classes bound becomes closer increasingly training examples less tight number classes increase. novelty prediction assign instance unknown formulation similar non-linear variant proposed albeit used ﬁxed number centroids class k-means determine centroids priori. contrast method learns number balls number balls class centroids ball incrementally. section validate online metric learning approaches three diﬀerent validation scenarios. show three proposed extensions online metric learning incremental updating thresholds local ball classiﬁer lead better predictions diﬀerent datasets. make available used features evaluation protocols data upon publication. testing. dataset densely sampled sift features clustered visual words provided though advanced features available combination dataset features allow fair comparison performance ncm-forests original methods. table comparison incremental learning ilsvrc’ dataset using features. bottom rows show proposed incremental metric learning approaches results taken incremental metric learning algorithms clearly outperform methods number classes increases places- second dataset consider recent places- dataset contains images diﬀerent scene types. dataset features training images class consistent real-world frequencies occurrence. dataset deep learning features training googlenet style convnet imagenet classes images using caﬀe subsequently process images places- dataset extract ﬁnal last dimensional layer image representation. best performing incremental methods comparison speciﬁcally initial metric ncm-forest svm-forest. compare three non-incremental baselines multi-class svms metric learning svm-forest online oncm onbc comparison. methods learned incrementally start shuﬄing data within batch learning. whitening features mean standard deviation calculated initial classes. performance measured using top- accuracy commonly used ilsvrc dataset. results shown table highlight ﬁndings. first observe among metric learning approaches variants fig. comparison results open world recognition ilsvrc’ dataset. proposed incremental/online algorithms oncm onno onbc clearly outperform incremental counterparts. approaches. second notice performance algorithms decreases number classes increases. expected classiﬁcation problem becomes harder number classes grows. still decrease deﬁnitely graceful metric learned incrementally oncm onbc. believe mainly incremental learning metric leads continuously adapting classes rather relying initial limited knowledge problem. open world performance measured considering unknown classes single category. allows calculate standard multi-class -top accuracy argmaxy∪unknown fig. surface plot proposed open world online metric learning methods. local learning onbc method clearly outperforms method number unknown test classes increases. compare proposed methods several baselines. first evaluate standard linear vset latter designed open-set recognition allow classify images unknown classes; note method able learn incrementally classes. also compare allow adjust towards classes incremental way. three methods designed assign images unknown class. train metric initial using metric incremental ball construction. assess performance open world recognition setting consider variables number known categories incremental learning number unknown categories testing. visualize results figure left show top- accuracy number known training classes grows case unknown classes. right show top- accuracy changes number unknown test classes increases ﬁxed number known classes main observation online approaches clearly outperform closed open world settings. lack rejecting images unknown classes yield almost random performance method. note oncb adapts classiﬁcation problems reject images unknown classes indeed prediction becomes easier number unknown known classes unbalanced. figure show surface plot diﬀerent range known classes unknown classes proposed online methods. experiment simulate online image stream prediction setting introduce novel evaluation protocol. believe realistic protocol permits fully represents dynamical behavior algorithm simultaneously updating testing phases. experimental setup consider follows algorithm consider stream incoming images. time learner practical reasons generate stream images frequent classes ilvrc’ places- classes being known unknown classes. ﬁnal number instances close open classes totally balanced. data stream generated follows stream divided stream-segments; ﬁrst segments introduce known unknown classes each; learner given images active class segment; introduced class dries segments; number images segment varies peak half-way; online accuracy recorded stream-segments. online harmonic top- accuracy. equally weights performance closed accuracy open accuracy. moreover method performs well accuracies poorly obtains harmonic mean favorable property. compare onno onbc variants using initial learned metric learned online learning phase stream-segments top-row ﬁgures show online harmonic accuracy incremental metric learning methods onno onbc clear beneﬁt metric-learning counterparts. becomes clearer images classes added later stream segments. moreover local learning classiﬁer adjust precisely added classes therefore outperforms linear classiﬁer. notice addition classes methods start gain performances learning already explored categories. finally signiﬁcant diﬀerence performance ilvrc’ places datasets using amount classes images likely powerful features used places dataset. bottom ﬁgures figure show mean conﬁdence values assigned closed open together mean estimated thresholds methods within streamsegment. order achieve good performances threshold rejecting image unknown class closed open conﬁdence. results observed open closed conﬁdence almost identical onno classiﬁer therefore ﬁnding good threshold value almost impossible. onbc method conﬁdence function estimated threshold depend local information open closed conﬁdences well apart. remark using ﬁxed threshold tuned initial lead good performances conﬁdence change time. paper addressed open world recognition problem proposed three extensions current formulation online metric learning incremental updating thresholds novelty detection local learning nearest ball classiﬁcation. evaluated eﬀect extensions three diﬀerent existing algorithms assessed eﬀects extensions three diﬀerent experimental scenarios large-scale incremental learning open world recognition online image stream prediction. last setting protocol evaluation online open world recognition believe mimics better out-of-the-lab applications. three scenarios proposed methods performed substantially better baselines showcasing importance fully embracing online learning open world recognition. future work focus studying suitability active learning scenario interaction module balance number true label requests performance query rate. another setting investigate bandit learners access labels making correct predictions.", "year": 2016}