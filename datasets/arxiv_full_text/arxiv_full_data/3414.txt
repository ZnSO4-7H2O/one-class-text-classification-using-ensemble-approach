{"title": "Supervised Learning of Labeled Pointcloud Differences via Cover-Tree  Entropy Reduction", "tag": ["cs.LG", "cs.CV", "stat.ML", "62H30, 60G55"], "abstract": "We introduce a new algorithm, called CDER, for supervised machine learning that merges the multi-scale geometric properties of Cover Trees with the information-theoretic properties of entropy. CDER applies to a training set of labeled pointclouds embedded in a common Euclidean space. If typical pointclouds corresponding to distinct labels tend to differ at any scale in any sub-region, CDER can identify these differences in (typically) linear time, creating a set of distributional coordinates which act as a feature extraction mechanism for supervised learning. We describe theoretical properties and implementation details of CDER, and illustrate its benefits on several synthetic examples.", "text": "abstract. introduce algorithm called cder supervised machine learning merges multi-scale geometric properties cover trees information-theoretic properties entropy. cder applies training labeled pointclouds embedded common euclidean space. typical pointclouds corresponding distinct labels tend diﬀer scale sub-region cder identify diﬀerences linear time creating distributional coordinates feature extraction mechanism supervised learning. describe theoretical properties implementation details cder illustrate beneﬁts several synthetic examples. authors partially supported contract number n--d jhu-apl subcontract number bendich harer also partially supported award bigdata harer also partially supported darpa modyl program number hr---. pieloch supported national science foundation graduate student fellowship program grant thank christopher tralie many helpful insights cover trees thank david porter michael williams jhu-apl many motivational discussions. propose supervised machine-learning method classiﬁcation object classiﬁed weighted pointcloud classiﬁcation task learn ﬁnite labels applied pointcloud. method fast data-driven multi-scale requires tuning. additionally details transparently geometric; suﬀer black interpretation diﬃculties arise many machine-learning methods. call cover-tree diﬀerencing entropy reduction high-level sketch cder follows. construct partial cover tree union labeled pointclouds given training search cover tree convex regions likely local minima entropy region build distributional coordinates dominant labels’ data. ordered sequence distributional coordinates allows determine likelihood label unlabeled test pointcloud. section explains notion distributional coordinates deﬁned pointclouds front-end supervised learning. section details cover-tree algorithm enhancements section explains simple approach entropy minimization labeled pointcloud context. section gives formal description cder. section illustrates cder several synthetic examples. remainder section establishes context notation problem. denote union pointclouds cloud collection pointcloud weights labels point inherited respective supervised learning task training data cloud collection purposes |λ|. denote label function acceptable pointclouds unequal sizes across also acceptable labeled sub-collections unequal sizes across labels pointwise weights assigned compensate sizes section structural hypothesis that labeled sub-collection pointclouds sampled underlying density function—or several density functions chosen random process—on label denote density function. eﬃciently regions label greater density labels. seek convex regions characteristic particular label unusually prominent there ignoring regions various density functions similar. figure analyze relative density labels regions using informationtheoretic notion entropy locating subregions entropy. cover trees method emphasizes regions large dense small-yet-remote regions construct function approximates near example call distributional coordinate figure single magenta pointcloud points single green pointcloud points. bottom union cloud collection consisting pointclouds label cder identiﬁes upper-right lower-left concentrations mostly ignoring background mutual concentration middle. label equally likely among test data. give training |λ|. total weight label alloted total weight moreover make assumption sample equally representative underlying density function regardless |xi|. pointcloud cardinality x|λi number training pointclouds label finally assume points equal lni|xi| denote labeled weighted pointcloud given could imagine supervised learning approach labeled pointclouds perform feature extraction; example ﬁrst deﬁning metric pointclouds applying nearest-neighbor clustering method. however metrics unstable noise slower-than-linear compute wasserstein metric section frames problem feature extraction common language distributional coordinates. start deﬁnition intuitive concept describe advantages disadvantages standard examples. approach advocate comes next section. working example euclidean plane. class labels magenta green. imagine magenta pointcloud contains large number points sampled large central blob points sampled much smaller blobs along horizontal axis. green pointcloud contains large number points sampled large central blob points sampled smaller diﬀerent blobs along horizontal axis. cover domain overlap boundaries commonly referred binning used successfully supervised-learning labeled pointclouds. hand binning obviously unstable consider gaussian mixtures. address instability could deﬁne gaussian mean covariance matrix obvious questions many gaussians used means covariances? data-driven approach used gaussian mixture central large distributional coordinate useless. perhaps innermost small gaussians less useful remaining distributional coordinates suﬃcient accurate classiﬁcation. goal replicate observation construct gaussian modes data-driven manner appropriate labeled distributional coordinates cover trees originally conceived accelerate nearestneighbors problem. recently used dictionary learning speeding analysis topology pointclouds pick applications. section gives basic deﬁnitions cover trees well fast algorithm construction; account detailed usually appears literature meant accompany publicly available code. sections cover trees deﬁne label-driven distributional coordinates cloud collection class labels. element called adult level cohort cover tree builds ﬁltration covering balls smaller smaller speciﬁcally cover tree ﬁltration following additional properties {a}; radius every ∪ai∈ab) point assigned guardian lies ball child vocabulary. cover trees generated using fast algorithm relying notion friends ﬁrst introduced rough algorithm familiar many practitioners widely published. enhance algorithm giving point weight label. describe method details construction crucial supervised-learning task article. extending maturation/reproduction metaphor adults children guardians figure changing level level radius ball shrinks children farther guardians become orphans. must decide whether orphans adopted adults level orphans emancipated adults level newly emancipated adults level comprise cohort level reduce confusion maintaining consistent metaphor diverge prior works’ notation places. adult called center elsewhere. guardian usually called parent. avoid word parent many distinct tree relations embedded algorithm. sort orphans ﬁrst orphan nearest per-label mean label greatest weight second orphan nearest per-label mean label second-greatest weight two-dimensional argsort without repetition. reason sorting become apparent section adult level child itself. friends initialized predecessor note ordered cohort within cohort ordered predecessor within predecessor block ordered proximity predecessor’s per-label means. algorithmic eﬃciency. suppose cover-tree ﬁltration stops level max. interested storage computational complexity algorithm terms ﬁxed ambient dimension desired output successor/predecessor tree children/guardian relationships level. thus storage friend lists issue might lead another algorithm. main question given adult level many adults within since adults level within another related idea doubling dimension. deﬁne maximum number points placed ball radius including center points distance number grows ambient dimension euclidean space independent scale; number works ball radius using points distance least number bounded number simplices edge length least ball radius since simplices minimum volume bound like upper bound expected size friends list constant independent level thus total storage size particular accounts lists type- type- type- friends. upper bound expected number successors adult level constant independent level size maximum expected size maximum expected size generally maximum expected size moreover larger size cohort shallower cover tree. expected order logd. interesting rapid growth ambient dimension counteracted logarithm term. experience applications eﬀectively linear terminates earlier estimate. estimate logd typically irrelevant; maximum level cover tree actually controlled ratio maximum minimum distances pointcloud. applications typically within several orders magnitude learning application section abort cover tree much earlier low-entropy regions found. quantify relative prominence various labels informationtheoretic notion entropy. entropy characterized several nice properties detailed formula smaller subset labels becomes likely. thus entropy good indicator region particularly prominent small subset labels. question remains locate low-entropy regions? proof expand power series around approximate truncating second-order term. level sets concentric ellipsoids given positive-deﬁnite symmetric matrix theorem suppose positive continuous functions deﬁned compact convex region suppose achieves local then radius average value greater average value proof mean value theorem integrals. putting together region ball radius region whose center near mean particular label. whether entropy ball non-decreasing radius; entropy become lower ball shrinks? true consistent hypothesis local maximum particular label occurs near center ball labels remain roughly constant. false subdivide region search around boundary original ball smaller regions entropy. region selection. describe heart cder algorithm wherein search adults cover tree data-driven method locate regions want build distributional coordinates. construction weighting distributional coordinates occurs section compare entropy. left cohort elder. right cohort several elders theorem smaller regions lower entropy build distributional coordinate using level level cover tree deﬁne subset adults still potential candidates distributional coordinates. {a}. break. cover tree constructed level-by-level section level adult denote children level union children elders less β−.) note figure then elif child level pass. region elif build distributional coordinate elif append ˆa+. smaller elif append successors ai—other itself—to ˆa+. annulus contain elif append successors ai—other itself—to ˆa+. annulus contain elif pass. region elif pass. region else append successors ˆa+. little information decide whether contains low-entropy region re-analyze everything next level. recall method section produces list ordered cohort within cohort ordered predecessor within predecessor block ordered proximity predecessor’s per-label means. therefore ordering tends nearer mean prominent label larger region hence distributional coordinates sorted granularity coarse ﬁne. scenarios detailed analysis clusters important eﬃcient classiﬁcation. therefore implement non-parsimonious version replacing pass operations append. machine learning applications parsimonious version faster cover tree stopped much earlier limit suggested section dominant labels among children pca/svd build gaussian model mass denote entropy diﬀerence caused erasing non-dominant labels. amplify gaussian model coeﬃcient selected region coordinate detected. perhaps chance selection process single region detected later level form several smaller regions. weight smaller regions weight larger region. region. else equal remote regions particularly distinctive. cover-tree construction remoteness measured size cover-tree ball. example suppose diﬀerent regions detected cder entropy weight conﬁguration points; gaussian coordinates translates one-another. suppose ﬁrst region distant rest pointcloud detected early level suppose second region surrounded points sampled standard normal distribution points sampled normal distribution mean points sampled normal distribution mean points sampled normal distribution mean points sampled normal distribution mean points sampled standard normal distribution points sampled normal distribution mean points sampled normal distribution mean points sampled normal distribution mean points sampled normal distribution mean produced weights vary orders magnitude. table shows conservative entropy-reduction process selecting regions generate distributional coordinates. labels indicated colors intentionally chosen colors seem show non-catastrophically black-and-white printouts. nonetheless recommend printing paper color least reading section computer screen pointcloud xtest mapped point many sophisticated methods possible simplicity simply given pointcloud xtest bigger euclidean norm magenta gaussian coordinates evaluated xtest simple comparison algorithm achieves accuracy -fold cross-validation cloud collection training/testing split. precisely cross-validation take percent point clouds turn cloud collection build distributional coordinates using cloud collection test results remaining percent point clouds. entire procedure repeated times. moreover relative masses distributional coordinates vary four orders magnitude sort comparison could dispose many interest speed preserving accuracy. note distributions contrived mass mean variance. elementary statistical tests would distinguish them; -moments skewness tests would necessary. blocks. example shows cder fairly robust background noise prevents strict separation. also demonstrates smoothness underlying distributions necessary good results. consider labels /magenta /green. cloud collection consists magenta pointclouds green pointclouds. magenta pointcloud generated sampling points uniformly unit square well extra well extra points green pointcloud generated sampling points uniformly unit square well extra points sampled well extra points sampled uniformly figures using simple comparison section algorithm achieves accuracy despite high background noise. deep field. example demonstrate gaussian mixtures ﬁxed points cder. also demonstrates algorithm handle unequal sample sizes weighting system section three labels. simplicity previous examples involved labels. two-label system low-entropy region exactly dominant label. however algorithm sensible number labels low-entropy regions dominated multiple labels. hence ensemble regions necessary distinguish pointclouds. cder algorithm detects shared regions perfectly achieving -fold cross-validation cloud collection training/testing split using simple comparison method section figure three-label task without unique dominance. upperleft original cloud collection. upper-right cover-tree balls used cder. lower-left gaussian covariances constructed cder. lower-right heatmap resulting distributional coordinates. ideally would like prove cder stable backward-stable sense formulate stability backward-stability cder must expressed numerical approximation formal function normed vector spaces. numerical algorithm obtained roughly sample equally apply cder generate weighted distributional coordinates. weighted distributional coordinates label distributional coordinates label stability result subject future work. succeeded proving either stability statement cder high cross-validation examples promising. paper introduced cder data-driven label-driven feature extraction method collections labeled pointclouds. cder fast terms theoretical complexity initial tests examples. require user choose tuning parameters geometric meaning output features transparently clear. section outlines future directions makes generalizing remarks. attentive reader section notice cover trees deﬁned arbitrary metric space indeed originally deﬁned construct distributional coordinates algorithm demands able quickly compute means sets points. fr´echet mean deﬁned metric space always fast algorithms computation generality could achieved deﬁnition cder make complexity statements cloud collections common euclidean space. examples section artiﬁcial simply intended help visualize novel fairly-technical cder algorithm emphasize properties. future work involve applications cder real data variety real also hope cder prove useful feature-extraction method topological data analysis since persistence diagrams thought point clouds plane. future work compare performance cder feature-extraction methods finally recall weighting discussion section used simplifying assumptions color/label equally likely point within single point cloud given equal weight. note cder easily adapted accommodate prior assumptions relative likelihoods labels even prior assumptions outlier status certain points cloud part bayesian learning process.", "year": 2017}