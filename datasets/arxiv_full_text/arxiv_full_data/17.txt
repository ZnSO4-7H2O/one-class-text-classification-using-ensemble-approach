{"title": "Domain Adaptive Neural Networks for Object Recognition", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "stat.ML"], "abstract": "We propose a simple neural network model to deal with the domain adaptation problem in object recognition. Our model incorporates the Maximum Mean Discrepancy (MMD) measure as a regularization in the supervised learning to reduce the distribution mismatch between the source and target domains in the latent space. From experiments, we demonstrate that the MMD regularization is an effective tool to provide good domain adaptation models on both SURF features and raw image pixels of a particular image data set. We also show that our proposed model, preceded by the denoising auto-encoder pretraining, achieves better performance than recent benchmark models on the same data sets. This work represents the first study of MMD measure in the context of neural networks.", "text": "abstract. propose simple neural network model deal domain adaptation problem object recognition. model incorporates maximum mean discrepancy measure regularization supervised learning reduce distribution mismatch source target domains latent space. experiments demonstrate regularization eﬀective tool provide good domain adaptation models surf features image pixels particular image data set. also show proposed model preceded denoising auto-encoder pretraining achieves better performance recent benchmark models data sets. work represents ﬁrst study measure context neural networks. learning-based computer vision probability distribution mismatch between training test samples essential problem overcome success real world scenarios. example suppose object recognizer learned training containing objects speciﬁc viewpoints backgrounds transformations. applied environment similar object category diﬀerent viewpoints backgrounds transformations condition. situation might happen lack labeled data representing target environment insuﬃcient knowledge regarding target condition. good recognition model setting guaranteed trained using traditional learning techniques. distribution respectively goal predict target labels information suﬃcient. recent years many solutions problem proposed computer vision applications natural language processing image recognition oﬃce data become standard image evaluate performance domain adaptation models. standard evaluation protocol data based using surf feature descriptor inputs model. however utilization descriptor usually needs careful engineering good discriminative features. furthermore bring complexity context real time feature extraction processes. therefore worthwhile build good models without using handcrafted feature descriptors. representation feature learning provides framework reduce dependency manual feature engineering examples considered representation learning principal component analysis independent component analysis sparse coding neural networks deep learning. deep learning greedy layer-wise unsupervised training known pretraining played important role success deep neural networks although representation learning-based techniques brought successes many applications methods address distribution mismatch well studied. work propose simple neural network model good domain adaptation performance image pixels. particularly utilize non-parametric probability distribution distance measure maximum mean discrepancy regularization embedded supervised backpropagation training. used reduce distribution mismatch hidden layer representations induced samples drawn diﬀerent domains. despite eﬀectiveness best knowledge context neural networks investigated yet. work therefore ﬁrst study neural networks. speciﬁcally investigate whether regularization indeed improve discriminative domain adaptation performance neural networks. section describe several tools related proposed method measure feed forward neural network denoising auto-encoder. reviews tools recent literature also included. domain adaptation transfer learning used reduce distribution mismatch source target domain. proposed pca-based model referred transfer component analysis used induce subspace data distributions diﬀerent domains closed other. long presented transfer sparse coding utilizes encoding stage match distributions sparse codes. work adopts idea incorporating learning algorithm similarly tsc. diﬀerence carry regularization respect supervised criterion unsupervised learning. expect regularization embedded supervised training induce better discriminative features. feed forward neural network used extensively solving many discrimative tasks past decades including object recognition tasks. standard ffnn structure consists three types layer input hidden output layers weighted inter-layer connections. ffnn training corresponds adjusting connection weights respect speciﬁc criterion. activation functions. work rectiﬁer function approximated softplus function log) softmax function argued biologically plausible logistic function importantly several experimental works proved rectiﬁer activation function improve performance neural network models furthermore softmax function induces probabilistic interpretation ffnn output. auto-encoder refers unsupervised neural network used learning eﬃcient codings. deep learning research known eﬀective technique pretraining deep neural networks terms structure auto-encoder similar standard feed-forward neural network except output layer equal number nodes input layer. objective auto-encoder reconstruct inputs means reconstruction loss function. denoising auto-encoder variant auto-encoder model captures robust representations reconstructing clean inputs given noisy counterparts qualitatively several types noise zero masking gaussian salt-and-pepper noises characterizes particular ﬁlters correspond ﬁrst hidden layer parameters daes considered better standard auto-encoders comparable restricted boltzmann machines context deep learning discriminative performance work consider pretraining stage proposed domain adaptive model. unlabeled images source target domains considered inputs pretraining. investigate effect without pretraining regarding domain adaptation performance. propose variant standard feed forward neural network refer domain adaptive neural network model incorporates measure regularization embedded supervised backpropagation training. using regularization train network parameters supervised criterion optimized hidden layer representations encouraged invariant across diﬀerent domains. jnns )]k)) loss function shown applied source data linear combination outputs activation regularization constant controlling importance contribution loss function. minimize need gradient jdann. computing gradie depends choice kernel function. choose gaussian kernel considered universal kernel kernel function main reason choosing gaussian kernel well studied proven make useful practice furthermore worth noting applied linear combination outputs non-linear activation function. means provides biased estimate respect actual distribution discrepancy hidden representations. however since rectiﬁer activation function close linear expect measure would able produce good approximation true distribution discrepancy. steps. firstly jnns minimized using mini-batched stochastic gradient descent respect update. mini-batched setting become standard practice neural network training establish compromise speed accuracy. then evaluated proposed method context object recognition several domain mismatches. ﬁrst compared dann baselines recent domain adaptation methods. results terms recognition accuracy represented mean standard deviation independent runs reported. last investigated eﬀect regularization measuring diﬀerence ﬁrst hidden layer activations domain another domain. experiments used oﬃce data contains images object classes three diﬀerent domains amazon webcam dslr. amazon images contain single centered object others images acquired unconstrained settings variations lighting background changes. used object classes following protocol designed gong ends instances total. number images amazon webcam dslr respectively webcam dslr known similar based rank domain measure examples oﬃce images seen figure dann model used experiments hidden layer i.e. shallow network hidden nodes. input layer dann either pixels surf features. output layer contains nodes corresponding classes. experiments used parameter setting supervised backpropagation learning speciﬁed table note employed dropout regularization introduced hinton regularization randomly omits hidden node training case certain probability. kernel following calculation median squared distance source samples. regularization constant suﬃciently large accommodate small values compared jnns three domains originated oﬃce data evaluation divided settings unsupervised adaptation semi-supervised adaptation. unsupervised adaptation corresponds setting labeled images source domain unlabeled images target domain training labels target domain incorporated. semisupervised adaptation incorporate labeled images target domain additional training images. first three images object category target domain selected. diﬀerently conducted initial work used labeled images source domain instead randomly sampled performance model compared svm-based baselines existing domain adaptation methods simple neural network follows l-svm model linear kernel applied original features. l-svm model l-svm preceded reduce feature dimensionality. geodesic flow kernel approach considering inﬁnite number intermediate subspaces source target domains followed k-nn classiﬁcation. transfer sparse coding technique based combination graph regularized sparse coding regularization logistic regression. single layer neural network structure parameter setting used dann without regularization. http//www.csie.ntu.edu.tw/~cjlin/liblinear used subspaces constructed http//learn.tsinghua.edu.cn//long.html basically algorithm without step ﬁrst investigated performance model standard image features provided gong brieﬂy image features acquired ﬁrst utilizing surf descriptor resized grayscaled images detect local scale-invariant interest points. followed encoding data points -bin histograms using codebook trained subset amazon images ﬁnal features normalized z-scored zero mean unit variance. conducted unsupervised setting evaluation results shown table found dann better performance approaches standard features. speciﬁcally dann performs well amazon particular domain pairs. case webcam-dslr shifts tested oﬃce dataset previous work surprisingly best model. despite eﬀectiveness longer feature extraction time than example neural network-based approaches less eﬃcient real world situation. also noted incorporates multiple intermediate subspaces fails surpass baselines several cases. indicates projection onto subspaces generated insuﬃcient reduce domain mismatch. table unsupervised setting performances oﬃce data domain pair using surf-based features inputs. column header starting second column indicates domain pair e.g. also conducted evaluation pixels oﬃce images. previous works oﬃce image mostly done using surf-based features. worth investigating performance oﬃce pixels directly since good models pixels preferable sense reducing needs handcrafted feature extractors. ﬁrst converted pixels oﬃce images values grayscaled pixels resized experiment unsupervised semi-supervised adaptation setting domain pairs. addition also investigated eﬀect pretraining precedes dann supervised training respect performance. pretraining slightly change step algorithm denoted models dann. examples pretrained weights depicted figure complete accuracy rates oﬃce pixels domain pairs presented table fig. visualization randomly chosen weights pretraining domain pairs oﬃce image set. white gray black pixels indicate high-positive close-to-zero zero high-negative values particular connection weight. zero-masking noise used destruction. clear dann always provides accuracy improvements domain pairs compared svm-based baselines model. words regularization indeed improves performance neural networks. compared also employs regularization unsupervised training stage dann performs better cases. however match dann performance webcam-dslr couples lower level mismatch couples. indicates utilization regularization supervised training might gain adaptation ability unsupervised training pairs diﬃcult mismatches solve. pretraining applied dann indeed improves performances couples domains. improvements quite signiﬁcant several cases especially webcam-dslr couples. general pretraining also produces stable models sense resulting lower standard deviations independent runs. furthermore combination pretraining dann performs best among methods experiments almost cases. sense qualitative analysis seen figure pretraining captures distinctive ﬁlters local blob detectors object parts detectors especially amazon images included. eﬀect somewhat consistent found initial work suggesting pretraining provides useful neural network representations. semi-supervised setting performance trend somewhat similar unsupervised setting. however performance discrepancies dann becomes smaller unsupervised setting. outcome also holds case pretraining. suggests regularization pretraining might less impactful labeled images target domain acquired. whether domain adaptation results shown table reasonable compared standard learning setting. refer standard setting in-domain setting training test samples come domain. in-domain performance considered reference indicates eﬀectiveness domain adaptation models dealing domain mismatch. investigated in-domain performances non-domain adaptive models described section i.e. l-svm pca+l-svm pixels oﬃce images. domain conducted -fold cross-validation. complete in-domain results terms mean standard deviation shown table general best in-domain model model training test images. comparison domain adaptation results highest in-domain accuracies better results domain mismatches amazon webcam used target sets work utilized measure regularization supervised back-propagation training. regularization encouraged hidden layer representation distributions similar other. demonstrated dann performs well oﬃce image especially image pixels inputs. furthermore dann preceded denoising auto-encoder pretraining better performance compared svm-based baselines oﬃce image almost domain pairs. despite eﬀectiveness regularization still many aspects improved. seen performance pixels main concern representation learning approach still good surf features. note good models perform well without preceding handcrafted feature extractors preferable reduce complexity. better model pixels might achieved using deeper neural network layers similar strategy since deep architectures brought successes many applications recent years initial work using standard deep neural network pretraining shown page limit suggested deeper representations always improve performance domain mismatch. addition study kernel choice computing regarding domain adaptation problem might worth addressing. assumed universal gaussian kernel function detect underlying distribution mismatches oﬃce data might true. better understanding relationship kernel function particular image mismatch e.g. background lighting aﬃne transformation changes would induce great impact ﬁeld research. baktashmotlagh harandi lovell salzmann. unsupervised domain adaptation domain invariant projection. proceedings international conference computer vision pages bengio. deep learning representations looking forward. statistical language speech processing volume lecture notes computer science pages springer borgwardt gretton rasch h.-p. kriegel sch¨olkopf smola. integrating structured biological data kernel maximum mean discrepancy. bioinformatics daum´e-iii. frustratingly easy domain adaptation. corr abs/. erhan bengio courville p.-a. manzagol vincent. unsupervised pre-training help deep learning? journal machine learning research vincent larochelle lajoie bengio p.-a. manzagol. stacked denoising autoencoders learning useful representations deep network local denoising criterion. journal machine learning research", "year": 2014}