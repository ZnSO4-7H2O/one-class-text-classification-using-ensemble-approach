{"title": "Semantics derived automatically from language corpora contain human-like  biases", "tag": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "abstract": "Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model---namely, the GloVe word embedding---trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the {\\em status quo} for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.", "text": "caliskan aylin joanna bryson arvind narayanan. semantics derived automatically language corpora contain human-like biases. science http//opus.bath.ac.uk// artiﬁcial intelligence machine learning period astounding growth. however concerns technologies used either without intention perpetuate prejudice unfairness unfortunately characterizes many human institutions. show ﬁrst time human-like semantic biases result application standard machine learning ordinary language—the sort language humans exposed every day. replicate spectrum standard human biases exposed implicit association test well-known psychological studies. replicate using widely used purely statistical machine-learning model—namely glove word embedding—trained corpus text web. results indicate language contains recoverable accurate imprints historic biases whether morally neutral towards insects ﬂowers problematic towards race gender even simply veridical reﬂecting status distribution gender respect careers ﬁrst names. regularities captured machine learning along rest semantics. addition empirical ﬁndings concerning language also contribute methods evaluating bias text word embedding association test word embedding factual association test results implications machine learning also ﬁelds psychology sociology human ethics since raise possibility mere exposure everyday language account biases replicate here. introduction astonished human-like capacities visible recent advances artiﬁcial intelligence comforted know source progress. machine learning exploiting universality computation able capture knowledge computation discovered transmitted humans human culture. however leading spectacular advances strategy undermines assumption machine neutrality. default assumption many computation deriving mathematics would pure neutral providing fairness beyond present human society. instead concerns machine prejudice coming fore—concerns historic biases prejudices reiﬁed machines. documented cases automated prejudice range online advertising criminal sentencing experts commentators recommend always applied transparently certainly without prejudice. code algorithm process applying must open public. transparency allow courts companies citizen watchdogs others understand monitor suggest improvements algorithms another recommendation diversity among developers address insensitive under-informed training machine learning algorithms third collaboration engineers domain experts knowledgeable historical inequalities show strategies might helpful even necessary could sufﬁcient. document machine prejudice derives fundamentally human culture possible eliminate strategies above. demonstrate ﬁrst time long suspected —that semantics meaning words necessarily reﬂects regularities latent culture know prejudiced. demonstrate showing standard widely used natural language processing tools share biases humans demonstrate psychological studies. tools language model built neutral automated parsing large corpora derived ordinary web; exposed language much like human would bias expected result whenever even unbiased algorithm used derive regularities data; bias regularities discovered. human learning also form computation. therefore ﬁnding data derived human culture deliver biases prejudice implications human sciences well. present null hypothesis explaining transmission prejudice humans. ﬁndings also implications addressing prejudice whether humans machines. fact rooted language makes prejudice difﬁcult address means impossible. argue prejudice must addressed component intelligent system learning culture. cannot entirely eliminated system rather must compensated for. article begin explaining meaning methods determine human understanding interpret machines. present results. replicate previously-documented biases prejudices attitudes towards ordinary objects animals humans. show prejudices reduce number interview invitations sent people racial association name associate women arts rather science mathematics retrieved standard language tools used ordinary products. also show veridical information proportions women particular categories proportion versus women particular name recovered using methods. present detailed account methods discussion implications work. meaning bias humans machines machine learning bias refers prior information necessary prerequisite intelligence bias problematic prior information derived precedents known harmful. purpose paper call harmful biases ‘prejudice’. show prejudice special case bias identiﬁable negative consequences therefore impossible eliminate purely algorithmically. rather prejudice requires deliberate action based knowledge society outstanding ethical challenges. demonstrate incorporates bias humans ﬁrst able document human bias. several methods below implicit association test first introduced greenwald demonstrates enormous differences response times subjects asked pair concepts similar contrast concepts different. follows reaction time paradigm means subjects encouraged work quickly possible response times quantiﬁed measure. example subjects much quicker told label insects unpleasant ﬂowers pleasant asked label objects reverse. fact pairing faster taken indicate task easy therefore subjects linked mind. ordinarily used pair categories ‘male’ ‘female’ attributes ‘violent’ ‘peaceful’. used describe account wide range implicit prejudices phenomena including stereotype threat method demonstrating bias prejudice text variant implicit association test applied widely-used semantic representation words termed word embeddings. derived representing textual context word found vector high-dimensional space. roughly word relationship words around summed across many occurrences text. measure distances vectors. thesis behind word embeddings words closer together vector compare words. many words multiple meanings makes pairwise measurements noisy. control this small baskets terms represent concept. present paper never invented basket words rather every case used words used psychological study replicating. note distances similarities word embeddings notoriously lack intuitive interpretation. poses problem results import depend attaching meaning distances. primary claim associations revealed relative nearness scores categories match human biases stereotypes strongly across many categories. thus associations word vectors could arisen chance instead reﬂect extant biases human culture. however several differences method iat. discuss future versions paper’s appendices particular critical presentation results. applies individual human subjects embeddings interest derived aggregate writings humans web. corpora generated uncontrolled fashion representative population sometimes used draw conclusions populations averaging individual results samples important insights racial bias gender stereotypes among others. tests word embeddings loosely analogous population-level iats. nevertheless difference precludes direct numerical comparison human biases measured algorithmic biases measured methods. particular allows rejecting null hypothesis p-value quantiﬁcation strength association effect size. obtained administering test statistically-signiﬁcant sample subjects word embeddings notion test subjects. roughly able measure mean association strength subjects collectively created corpora. observe variation subjects trials. report p-values effect sizes resulting multiple words category meaning numbers entirely different reported iats. results using techniques described methods section found every linguistic bias documented psychology looked for. sample think persuasive. cherry picked effect size—these uniformly high. rather chose illustrate assertion account variety implicit human biases purely language regularities fact part parcel meaning language. demonstrate showing measures replicate implicit bias also replicate prejudicial hiring practices return veridical information employment naming practices contemporary america. ensure impartiality approach using benchmarks keywords established well-known heavily cited works human sciences psychology sociology. state-of-the-art widely used word embedding namely glove made available pennington used glove’s standard semantic models trained standard corpora ordinary language found world wide web. also found similar results standard tools corpora also discuss future versions paper’s appendices. following lead result report sets target concepts attempting learn sets attribute words comparing apply method weat report probability observed similarity scores could arisen semantic association target concepts attribute. report effect size based number standard deviations separate sets target words terms association attribute words; precise details measure described methods section. baseline replication associations universally accepted ﬁrst results presented initial publication concerned biases found universal humans social concern. allows introduction clariﬁcation method validation relatively morally neutral topics. begin replicating inoffensive results purposes. flowers insects original finding greenwald report able demonstrate reaction times ﬂowers signiﬁcantly pleasant insects insects unpleasant ﬂowers. based reaction latencies participants results effect size considered large effect p-value statistical signiﬁcance. finding replicate ﬁnding looking semantic similarity glove stimuli using weat method. flowers likely insects closer pleasant unpleasant. applying method observe expected association effect size p-value statistical signiﬁcance. musical instruments weapons original finding similarly greenwald musical instruments signiﬁcantly pleasant weapons. based reaction latencies participants results effect size p-value finding replicate ﬁnding looking semantic nearness glove stimuli. musical instruments likely weapons closer pleasant unpleasant. applying method observe expected association effect size p-value stimuli stimuli found greenwald racial biases technique demonstrate machine learning absorbs prejudice easily biases. replicate original results racial prejudice also recent striking ﬁnding names alone enormous impact probability candidates called interview. replicating implicit associations valence original finding greenwald extreme impacts race indicated simply name. bundle names associated european american found signiﬁcantly easier associate pleasant unpleasant terms compared bundle african american names. subjects greenwald show european american names likely implicitly associated pleasant effect size p-value finding able replicate attitude towards races looking semantic nearness glove. forced slightly alter stimuli original african american names occur corpus sufﬁcient frequency. shown italics below. therefore also deleted number european american names chosen random balance number elements sets concepts. results european american names likely african american names closer pleasant unpleasant effect size p-value stimuli subset stimuli found greenwald names marked italics excluded replication. european american names adam chip harry josh roger alan frank justin ryan andrew fred jack matthew stephen brad greg paul todd brandon hank jonathan peter wilbur amanda courtney heather melanie sara amber crystal katie meredith shannon betsy donna kristin nancy stephanie bobbie-sue ellen lauren peggy sue-ellen colleen emily megan rachel wendy african american names alonzo jamel lerone percell theo alphonse jerome leroy rasaan torrance darnell lamar lionel rashaun tvree deion lamont malik terrence tyrone everol lavon marcellus terryl wardell aiesha lashelle nichelle shereen temeka ebony latisha shaniqua tameisha teretha jasmine latonya shanise tanisha lakisha latoya sharise tashika yolanda lashandra malika shavonn tawanda yvette replicating bertrand mullainathan r´esum´e study original finding bertrand mullainathan sent nearly identical r´esum´es advertisements change made r´esum´es names candidates. found european american candidates likely offered opportunity interviewed. finding perhaps unsurprisingly found signiﬁcant result names used bertrand mullainathan. before delete low-frequency names. also assumed semantic nearness pleasantness correlate invitation interview. different sets ‘pleasant/unpleasant’ stimuli original paper also revised shorter used recently found nosek sets attributes european american names likely african american names invited interviews using greenwald attributes effect size p-value using updated nosek attributes effect size p-value stimuli names stimuli found bertrand mullainathan ﬁrst pleasant unpleasant words above second nosek updated pleasantness love peace wonderful pleasure friend laughter happy. updated unpleasantness agony terrible horrible nasty evil awful failure. gender biases turn gender-related biases stereotypes. begin returning prejudice demonstrated turn matching biases data mine veridical information taken published u.s. government statistics. replicating implicit associations career family whether appropriate women careers matter signiﬁcant cultural dispute. historically consensus not; today means americans consider appropriate woman career man. similarly historical biases choose take domestic roles. study compare conducted online thus vastly larger subject pool. however since difﬁculty ensuring online subjects complete task attention also fewer keywords examined. able replicate results even reduced keyword sets. original finding interpretable subjects female names found associated family career words effect size p-value nosek finding found result females associated family males career effect size p-value stimuli stimuli found nosek male names john paul mike kevin steve greg jeff bill. female names joan lisa sarah diana kate donna. career words executive management professional corporation salary ofﬁce business career. family words home parents children family cousins marriage wedding relatives. math words math algebra geometry calculus equations computation numbers addition. arts words poetry dance literature novel symphony drama sculpture. male attributes male brother son. female attributes female woman girl sister hers daughter. replicating implicit associations arts sciences another laboratory study nosek found female terms less associated sciences male terms less associated arts. original finding associations observed effect size p-value nosek finding examining arts sciences attributes found female terms associated arts male terms science effect size p-value stimuli stimuli found nosek science words science technology physics chemistry einstein nasa experiment astronomy. arts words poetry shakespeare dance literature novel symphony drama. male attributes brother father uncle grandfather him. female attributes sister mother aunt grandmother daughter hers her. comparison real-world data occupational statistics suggested implicit gender-occupation biases linked gender gaps occupational participation however relationship complex mutually reinforcing. examine correlation gender association occupation words labor-force participation data. original data x-axis figure derived u.s. bureau labor statistics provides information occupational categories percentage women certain occupations categories. generated single word occupation names based available data calculated percentage women single word occupation names. finding applying wefat able word embeddings predict percentage women relevant occupations pearson’s correlation coefﬁcient p-value stimuli gender stimuli found nosek along occupation attributes derived labor statistics. careers technician accountant supervisor engineer worker educator clerk counselor inspector mechanic manager therapist administrator salesperson receptionist librarian advisor pharmacist janitor psychologist physician carpenter nurse investigator bartender specialist electrician ofﬁcer pathologist teacher lawyer planner practitioner plumber instructor surgeon veterinarian paramedic examiner chemist machinist appraiser nutritionist architect hairdresser baker programmer paralegal hygienist scientist. comparison real-world data androgynous names similarly looked veridical association gender androgynous names names sometimes used either gender. case recent information able census name gender statistics. perhaps name data correlation weaker occupation statistics still strikingly signiﬁcant. finding y-axis reﬂects calculation bias male female names applying wefat able predict percentage people name women pearson’s correlation coefﬁcient figure projection -dimensional vector space glove word embedding lines illustrate algebraic relationships related words pairs words differ gender pairs vectors whose vector difference roughly constant. similar algebraic relationships shown semantic relationships countries capital cities companies ceos simply different forms word. names kelly tracy jamie jackie jesse courtney lynn taylor leslie shannon stacey jessie shawn stacy casey bobby terry ashley eddie chris jody carey willie morgan robbie joan alexis kris frankie bobbie dale robin billie adrian jaime jean francis marion dana rene johnnie jordan carmen ollie dominique jimmie shelby. methods data training word embedding representation words points vector space. loosely embeddings satisfy property vectors close represent semantically similar words. word embeddings derive power discovery vector spaces around dimensions sufﬁce capture aspects similarity enabling computationally tractable representation words large corpora text starting wordvec family word embedding techniques gained popularity computational techniques generating word embeddings large training corpora text superior speed predictive performance various natural-language processing tasks famously word embeddings excel solving word analogy tasks algebraic relationships vectors capture syntactic semantic relationships words addition word embeddings found natural-language processing tasks search document classiﬁcation. also found cognitive science understanding human memory recall results paper state-of-the-art glove word embedding method which high level similarity pair vectors related probability words co-occur close text word embedding algorithms glove substantially amplify signal found simple co-occurrence probabilities using dimensionality reduction. pilot-work experiments along lines presented co-occurrence probabilities shown lead substantially weaker results rather train embedding ourselves pre-trained glove embeddings distributed authors. replicate effects found real applications extent possible using pre-trained embeddings minimizes choices available simpliﬁes reproducing results. pick largest four corpora glove authors provide trained embeddings common crawl corpus obtained large-scale crawl containing billion tokens tokens corpus case-sensitive million different ones corresponding -dimensional vector. large size corpus resulting model important since enables word vectors even relatively uncommon names. important limitation vectors multi-word phrases. expect similar results ones presented used corpora and/or embedding algorithms. example repeated weat wefat experiments presented using different pre-trained embedding wordvec google news corpus experiments observed statistically signiﬁcant effects high effect sizes. further found gender association strength occupation words highly correlated glove embedding wordvec embedding concurrent work bolukbasi compared embeddings using different measure gender bias occupation words also ﬁnding high correlation word embedding association test demonstrate quantify bias permutation test. borrowing terminology literature consider sets target words sets attribute words null hypothesis difference sets target words terms relative similarity sets attribute words. permutation test measures likelihood null hypothesis computing probability random permutation attribute words would produce observed difference sample means. re-iterate p-values effect sizes don’t interpretation iat. subjects experiments words people. measure differential association single pair target concepts attribute weat measure differential association sets target concepts attribute. word embedding factual association test understand demonstrate necessity human bias word embeddings also wish examine word embeddings capture empirical information world also embedded language. consider target concepts occupations real-valued factual property world associated concept percentage workers occupation women. we’d like test vectors corresponding concepts embed knowledge property algorithm extract predict property given vector. principle could algorithm work test association target concept attribute words analogous weat above. word discuss detail apply wefat cases. ﬁrst test occupation word vectors embed knowledge gender composition occupation real world. data released bureau labor statistics occupations categorized hierarchically occupation number workers percentage women given chief difﬁculty many occupation names multi-word terms whereas word vectors represent single words. strategy convert multi-word term single word represents superset category ﬁlter occupations possible. second application wefat test androgynous names embed knowledge often name given boys versus girls. picked popular names window gender frequency based u.s. census data. difﬁculty names also regular english words state-of-the-art word embeddings sophisticated enough handle words multiple senses meanings; usages lumped single vector. handle this algorithmically determine name-like vector eliminate vectors least name-like. discussion shown machine learning acquire prejudicial biases training data reﬂect historical injustice. entirely ﬁnding. recent line work fairness machine learning tries minimize avoid biases however unlike literature setting particular explicit decision-making task rather often unconscious consequences language. show ﬁrst time exploit language vast knowledge culture compiled inevitably inherit human-like prejudices. words learns enough properties language able understand produce also acquires cultural associations offensive objectionable harmful. much broader concerns intentional discrimination possibly harder address. distinction informs much rest section. implications understanding human prejudice simplicity strength results suggests null hypothesis explaining origins prejudicial behavior humans namely implicit transmission ingroup/outgroup identity information language. providing explicit institutional explanation individuals make decisions disadvantage group regards another must show unjust decision simple outcome unthinking reproduction statistical regularities absorbed language. similarly positing complex models prejudicial attitudes perpetuate generation next group another must check whether simply learning language sufﬁcient explain observed transmission prejudice. null hypotheses important necessarily expect true cases occam’s razor requires eliminate them least quantify ﬁndings prejudice comparison explainable language transmission alone. work lends credence highly parsimonious theory needed create prejudicial discrimination malice towards others preference one’s ingroup theory also supported recent results showing times conﬂict rather increase ingroup altruism decrease baseline altruism towards outgroup results also explain support empirical results education indicating reducing prejudice requires directed interventions facilitate decategorizing recategorizing outgroups simple contact members groups enough. needs speciﬁc bridging experiences facilitating construction identities develop skills work people across group boundaries. known time even newborn infants attend foremost speakers sharing mother’s dialect conjectured ingroup signaling even account origins music language shown language identiﬁes one’s group also group currently culturally dominant dominates particular regions culture. account koreans japanese people living countries associate ‘less pleasant’ african americans show european-american-oriented biases though strongly european americans dominance european-american orientation change american demography changes; indeed would interesting examine corpora consisting newspapers public language towns cities different demographic makeup particularly racial diversity also represented consistently public ofﬁces media. course neither work theory explaining origins prejudice justify prejudiced behavior. humans good using explicit knowledge better cooperate including choosing behave fairly. shown recently level implicit bias displayed subjects predict cooperative performance. words learned biases affect rate comprehension test stimuli construction artiﬁcial pairings affect deliberate choices treat others least laboratory setting. however demonstrated known case prejudicial decision making replicated biases latent language. therefore recommend continuing program research examining behavior correlate human subject performance iat. recommend using text processing tools check pilot predictions likely performance comparisons none currently known. consequences bias humans machines shown inherit substantially biases humans exhibit. however consequences bias different humans machine-learning systems. bias important increasingly given agency society tasks ranging predictive text search determining criminal sentences assigned courts. machines artifacts owned controlled humans operators. means learning shut completely product production operation frequently done create efﬁcient uniform experiences. approach opens potential downside enshrine imperfect procedure context routinely reexamined humans. artifacts could persist perpetuate biases society long time digital analogs robert moses’s racially motivated overpasses advantage least algorithms outcomes open inspection least make errors explicit therefore potentially subject monitoring correction. dependencies history uncovered well also pollute individual expectations public policy even law. natural intelligence learning artifacts pick upon correlations without considering sufﬁciently carefully whether causal relationship whether correlation caused unobserved factor possibly correctable injustice. effects bias applications better understand potential impact bias word embeddings consider applications found use. sentiment analysis classiﬁes text positive negative neutral. uses marketing quantify customer satisfaction ﬁnance predict market trends consider straw-man sentiment analysis technique based word embeddings calculate valence word based association designated positive negative words sentiment scores. consider applying technique movie reviews. results show european-american names positive valence african-american names state-of-the-art word embedding. means sentence containing european-american name higher sentiment score sentence name replaced african-american name. words tool display racial bias output based actor character names. picked example argument follows directly experiments names. results suggest imprints human racial prejudice conﬁned names also picked machine-learning models. besides bias known creep indirectly proxy thus would simplistic conclude problem withholding names inputs applications. next consider statistical machine translation unsurprisingly today’s systems reﬂect existing gender stereotypes. translations english many gender-neutral languages finnish estonian hungarian persian turkish lead gender-stereotyped sentences. example google translate converts turkish sentences genderless pronouns doktor. hems¸ire. english sentences doctor. nurse. test occupation words used results presented figure shows pronoun translated majority cases quarter cases; tellingly found gender association word vectors almost perfectly predicts pronoun appear translation. challenges addressing bias redresses transparent development technology improving diversity ethical training developers useful little address kind prejudicial bias expose here. unfortunately work points several additional reasons addressing bias machine learning harder might expect. first results suggest word embeddings don’t merely pick speciﬁc enumerable biases gender stereotypes rather entire spectrum human biases reﬂected language. fact show bias meaning. bias identical meaning impossible employ language meaningfully without incorporating human bias. term unacceptable bias prejudice paper. biases reveal aren’t particular application machine learning rather basic representation knowledge used possibly human cognition certainly expanding variety applications. second idea correcting even prejudiced biases also problematic. societal understanding prejudice constantly evolving along understanding humanity human rights also varies cultures. therefore hard impossible specify algorithmically prejudiced. give example monteith pettit using show people mental illnesses stigmatized compared people physical illnesses result also replicated word embeddings prejudice? determines whether corrected? third ﬁnally shown biases result extant well historic inequalities world. many contexts inequalities important know about. generally shared awareness real world important communication consider gender stereotypes occupations. using machine learning evaluate suitability applicants stereotypes would bad. task analyze historical infer women worked roles stereotypical associations would exactly information would wish utilize. gender associations found word embeddings names might exceedingly useful associations might lead prejudicial expectations concerning names occupations. remedies must tailored applications. within given context college admissions decide whether considerations fairness override usual focus predictive accuracy meaningful devoid context. simply eliminating bias eliminating information; eliminating prejudice takes thought. awareness better blindness reasons view approach debiasing word embeddings skepticism. view perception followed action debiasing alters ai’s perception world rather acts perception. gives incomplete understanding world. debiasing fairness blindness. place also important limits prejudice creep back proxies consider indirect bias paper). efforts ﬁght prejudice level initial representation necessarily hurt meaning accuracy hard adapt societal understanding fairness evolves. instead take inspiration fact humans express behavior different implicit biases human intelligence typiﬁed behavior integrating multiple forms memory evidence includes capacity recall one-shot exposure highly context-speciﬁc information form rules instructions. learn prejudice women used trapped homes careers gender doesn’t necessarily determine family role forth. built similar would possible prejudice absorbed machine learning much greater negative impact prejudice absorbed children. children also receive kinds instruction social examples part ordinary painstaking process child rearing. normally design architectures keep simple possible facilitate capacity debug maintain systems. however partially constructed automatically machine learning human culture also need analog human explicit memory deliberate actions trained programmed avoid expression prejudice. course approach doesn’t lend straightforward algorithmic formulation. instead requires long-term interdisciplinary research program includes cognitive scientists ethicists. concrete suggestion present choose corpora training machine learning little prejudice possible tools presented used identify these. another given vulnerability relying purely statistical information understanding operating within culture advisable consider complex architectures cognitive systems heterogeneous approaches representing knowledge intelligence allow exploit great strengths machine learning instructability symbolic systems. acknowledgements grateful following people lowe substantial assistance design signiﬁcance tests macfarlane pilot research part undergraduate dissertation solon barocas miles brundage excellent comments early version paper.", "year": 2016}