{"title": "Safety Verification of Deep Neural Networks", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Deep neural networks have achieved impressive experimental results in image classification, but can surprisingly be unstable with respect to adversarial perturbations, that is, minimal changes to the input image that cause the network to misclassify it. With potential applications including perception modules and end-to-end controllers for self-driving cars, this raises concerns about their safety. We develop a novel automated verification framework for feed-forward multi-layer neural networks based on Satisfiability Modulo Theory (SMT). We focus on safety of image classification decisions with respect to image manipulations, such as scratches or changes to camera angle or lighting conditions that would result in the same class being assigned by a human, and define safety for an individual decision in terms of invariance of the classification within a small neighbourhood of the original image. We enable exhaustive search of the region by employing discretisation, and propagate the analysis layer by layer. Our method works directly with the network code and, in contrast to existing methods, can guarantee that adversarial examples, if they exist, are found for the given region and family of manipulations. If found, adversarial examples can be shown to human testers and/or used to fine-tune the network. We implement the techniques using Z3 and evaluate them on state-of-the-art networks, including regularised and deep learning networks. We also compare against existing techniques to search for adversarial examples and estimate network robustness.", "text": "abstract. deep neural networks achieved impressive experimental results image classiﬁcation surprisingly unstable respect adversarial perturbations minimal changes input image cause network misclassify potential applications including perception modules end-to-end controllers self-driving cars raises concerns safety. develop novel automated veriﬁcation framework feed-forward multi-layer neural networks based satisﬁability modulo theory focus safety image classiﬁcation decisions respect image manipulations scratches changes camera angle lighting conditions would result class assigned human deﬁne safety individual decision terms invariance classiﬁcation within small neighbourhood original image. enable exhaustive search region employing discretisation propagate analysis layer layer. method works directly network code contrast existing methods guarantee adversarial examples exist found given region family manipulations. found adversarial examples shown human testers and/or used ﬁne-tune network. implement techniques using evaluate state-of-the-art networks including regularised deep learning networks. also compare existing techniques search adversarial examples estimate network robustness. deep neural networks achieved impressive experimental results image classiﬁcation matching cognitive ability humans complex tasks thousands classes. many applications envisaged including perception modules end-to-end controllers self-driving cars vector space images wish classify assume class labels models human perception capability neural network classiﬁer function approximates training examples {}i=..m. example perception module self-driving input image camera must correctly classify type object view irrespective aspects angle vision image imperfections. therefore though clearly include imperfections four pairs images figure arguably classiﬁed automobiles since appear human eye. classiﬁers employed vision tasks typically multi-layer networks propagate input image series linear non-linear operators. high-dimensional often millions dimensions non-linear potentially discontinuous even small network trained classify hand-written images digits real-valued parameters neurons ﬁrst layer. time networks trained ﬁnite data expected generalise previously unseen images. increase probability correctly classifying image regularisation techniques dropout typically used improves smoothness classiﬁers sense images close training point assigned class label. unfortunately observed deep neural networks including highly trained smooth networks optimised vision tasks unstable respect called adversarial perturbations. adversarial perturbations changes input image often imperceptible human cause network misclassify image. examples include artiﬁcially generated random perturbations also modiﬁcations camera images correspond resizing cropping change lighting conditions. devised without access training transferable sense example misclassiﬁed network also misclassiﬁed network diﬀerent architecture even trained diﬀerent data. figure gives adversarial perturbations automobile images misclassiﬁed bird frog airplane horse highly trained state-of-the-art network. obviously raises potential safety concerns applications autonomous driving calls automated veriﬁcation techniques verify correctness decisions. safety systems receiving increasing attention mention view potential cause harm safety-critical situations autonomous driving. typically decision making systems either solely based machine learning end-to-end controllers involves combination logic-based reasoning machine learning components image classiﬁer produces classiﬁcation speed limit stop sign serves input controller. recent trend towards explainable approaches learn assign classiﬁcation labels also additional explanations model take form justiﬁcation explanation cases safety decision reduced ensuring correct behaviour machine learning commain diﬃculty image classiﬁcation tasks play critical role perception modules autonomous driving controllers formal speciﬁcation usual sense ideally performance classiﬁer match perception ability class labels assigned human. traditionally correctness neural network classiﬁer expressed terms risk deﬁned probability misclassiﬁcation given image weighted respect input distribution images. similar robustness properties deep neural network classiﬁers compute average minimum distance misclassiﬁcation independent data point studied estimated using tools deepfool cleverhans however interested safety individual decision focus property classiﬁer invariant perturbations given point. notion also known pointwise robustness local adversarial robustness contributions. paper propose general framework automated veriﬁcation safety classiﬁcation decisions made feed-forward deep neural networks. although work concretely image classiﬁers techniques generalised settings. given image assume region around point incontrovertibly supports decision sense points region must class. region speciﬁed user given small diameter points whose salient features type. next assume family operations call manipulations specify modiﬁcations image classiﬁcation decision remain invariant region manipulations represent example camera imprecisions change camera angle replacement feature. deﬁne network decision safe input region respect manipulations applying manipulations result class change employ discretisation enable ﬁnite exhaustive search highdimensional region adversarial misclassiﬁcations. discretisation approach justiﬁed case image classiﬁers since typically represented vectors discrete pixels achieve scalability propagate analysis layer layer mapping region manipulations deeper layers. show propagation sound complete additional assumption minimality manipulations holds discretised settings. contrast existing approaches framework guarantee misclassiﬁcation found exists. since reduce veriﬁcation search adversarial examples achieve safety veriﬁcation falsiﬁcation implement techniques using tool called evaluate state-of-the-art networks including regularised deep learning networks. includes image classiﬁcation networks trained classifying hand-written images digits classes small colour images classes german traﬃc sign recognition benchmark classes colour images used well-known imagenet large-scale visual recognition challenge also perform comparison falsiﬁcation functionality mnist dataset methods focusing search strategies statistical robustness estimation. perturbed images figure found automatically using tool network trained cifar dataset. consider feed-forward multi-layer neural networks henceforth abbreviated neural networks. perceptrons neural network arranged disjoint layers perceptron layer connected next layer connection perceptrons layer. layer network associated nk-dimensional vector space dimension corresponds perceptron. write perceptrons layer |pk| number perceptrons layer formally neural network tuple layers layer input layer output layer sequential connections layers that except input output layers layer incoming connection outgoing connection activation functions dlk− non-input layer. layers input output network input input layer propagated layers successive application activation functions. activation point layer value corresponding function denoted φk)) perceptron write value activation input every activation layer deﬁne prek {αyk αxk} activations layer whose corresponding activation layer αxk. classiﬁcation decision made based activations output layer e.g. assigning class maxp∈pn αxn. simplicity denote class assigned input thus expresses inputs class. neural network classiﬁer represents function approximates function models human perception capability labelling images labels training examples {}i=..m. image classiﬁcation networks example convolutional networks contain many layers non-linear work high dimensions image classiﬁcation problems order millions. digital images represented tensors pixels pixel discrete value range training process determines real values weights used ﬁlters convolved activation functions. since diﬃcult approximate samples sparsely populated high-dimensional space increase probability classifying correctly previously unseen image various regularisation techniques dropout employed. improve smoothness classiﬁer sense points \u0001-close training point classify same. section deﬁne notion safety classiﬁcation decisions neural network based concept manipulation image essentially perturbations human observer would classify original image. safety deﬁned individual classiﬁcation decision parameterised class manipulations neighbouring region around given image. ensure ﬁniteness search region adversarial misclassiﬁcations introduce called ladders nondeterministically branching iterated application successive manipulations state conditions search exhaustive. safety robustness method assumes existence region around data point points region indistinguishable human therefore true class. region understood supporting classiﬁcation decision usually inferred type classiﬁcation problem. simplicity identify region diameter respect user-speciﬁed norm intuitively measures closeness point deﬁned network approximating human capability said robust exists point region input layer point minimal distance known adversarial example. deﬁnition safety classiﬁcation decision follows intuition except work layer layer therefore identify region subspace layer successively reﬁne regions deeper layers. justify choice based observation deep neural networks thought compute progressively powerful invariants depth increases. words gradually transform images representation classes separable linear classiﬁer. intuitively safety network point means classiﬁcation decision robust perturbations within region note that perturbation applied layer classiﬁcation decision based activation output layer deﬁnition region layer neural network safe input region written activations αxn. remark that unlike notions risk robustness work safety speciﬁc point account input distribution expectation measures considered section comparison. manipulations concept framework notion manipulation operator intuitively models image perturbations example angles scratches weather conditions idea classiﬁcation decisions region images close invariant manipulations. choice type manipulation dependent application user-deﬁned reﬂecting knowledge classiﬁcation problem model perturbations allowed. judicious choice families manipulations appropriate distance metrics particularly important. simplicity work operators activations vector space layer consider euclidean manhattan norms measure distance image perturbation techniques generalise norms discussed speciﬁcally applying manipulation activation result another activation values dimensions changed. therefore represent manipulation hyper-rectangle deﬁned activations layer ×p∈pk αyk) αyk)]. main challenge veriﬁcation fact region contains potentially uncountable number activations. approach relies discretisation order enable ﬁnite exploration region discover and/or rule adversarial perturbations. activation manipulations denote polyhedron includes hyper-rectangles result applying manipδ∈∆ rec). possible manipulations layer ensure region coverage deﬁne valid manipulation follows. deﬁnition given activation manipulations valid interior point αxk) i.e. αxk) belong boundary αxk). figure presents example valid manipulations two-dimensional space arrow represents manipulation dashed represents rectangle corresponding manipulation activation interior point space dashed boxes. since work discretised spaces reasonable assumption images introduce notion minimal manipulation. applying minimal manipulation suﬃces check misclassiﬁcation points allows exhaustive albeit impractical exploration region unit steps. activation hyper-rectangle former also hyper-rectangle latter. implied deﬁnition activation hyper-rectangle moreover write φk)) representing corresponding activation layer applying manipulation activation deﬁnition manipulation activation minimal exist manipulations intuitively minimal manipulation ﬁner manipulation results diﬀerent classiﬁcation. however possible diﬀerent classiﬁcations applying minimal manipulation i.e. possible αxn. hard minimality manipulation implies class change associated hyper-rectangle detected checking class points bounded variation recall apply manipulations layer check classiﬁcation decisions output layer. ensure ﬁnite exhaustive coverage region introduce continuity assumption mapping space output space adapted concept bounded variation given activation associated region deﬁne ladder activations containing ﬁnitely many possibly zero activations activations ladder arranged increasing order every activation αxtk appears successor αxt+k αxt+k manipulation greatest element successor outside region i.e. given ladder write activation preﬁx activation last greatest element figure gives diagrammatic explanation ladders. deﬁnition ladders total variation region neural network respect fig. examples ladders region starting activations αxk...αx form ladder consecutive activation results valid manipulation applied previous activation ﬁnal activation outside region complete ladder activations element manipulation exists ladder δk). intuitively complete ladder complete tree node represents activation branch node corresponds valid manipulation. root every path tree leading leaf ladder. moreover covering polyhedra activations cover region i.e. based above following deﬁnition safety respect manipulations. intuitively iteratively nondeterministically apply manipulations explore region safety means class change observed successive application manipulations. deﬁnition given neural network input manipulations safe input respect region manipulations written region -variation ladders complete covering. theorem means that minimality assumption manipulations exhaustive search complete covering ladder tree adversarial examples enable conclude network safe given point none found. though computing minimal manipulations practical discrete spaces iterating increasingly reﬁned manipulations able rule existence adversarial examples region. contrasts partial exploration according e.g. comparison section section propose novel framework automated veriﬁcation safety classiﬁcation decisions based search adversarial misclassiﬁcation within given region. distinctive distinctive features framework compared existing work guarantee misclassiﬁcation found exists; propagation analysis layer layer; working hidden layers addition input output layers. since reduce veriﬁcation search adversarial examples achieve safety veriﬁcation falsiﬁcation activation function dlk− also require mapping dlk− opposite direction represent manipulated activation layer aﬀects activations layer lk−. simply take inverse function order propagate safety regions point deeper layers assume existence functions activations regions impose following restrictions functions shown diagrammatically figure deﬁnition functions mapping activations regions size complexity deep neural network generally means determining whether given manipulations minimal intractable. partially counter this deﬁne reﬁnement relation safety manipulations consecutive layers sense reﬁnement manipulations reﬁned sequence manipulations therefore although cannot theoretically conﬁrm minimality reﬁned layer layer discrete settings process bounded unit step. moreover work gradually speciﬁc layer inwards adversarial example found ﬁnishing processing reaching output layer. relations safety notions labelled conditions needed. goal reﬁnements chain implications justify fact implies constraints deﬁnition fact implies follows theorem implication condition minimal theorem deﬁne notion reﬁnability manipulations layers. intuitively manipulation layer reﬁnable layer exists sequence manipulations layer implements manipulation layer lk−. deﬁnition manipulation reﬁnable layer exist activations valid manipulations given δk−k αxtk neural network input manipulations reﬁnement layer αyk− valid manipulations reﬁnable layer note adversarial example safety manipulations also adversarial example general safety however adversarial example layer needs checked adversarial example i.e. input layer. recall prek necessarily unique. equivalent checking emptiness pre∩ start analysis hidden layer speciﬁcation instead consider checking emptiness αxn}. summarise theory developed thus search-based recursive veriﬁcation procedure given below. method parameterised region around given point family manipulations manipulations speciﬁed user classiﬁcation problem hand alternatively selected automatically described section vector norm identify region also speciﬁed user vary layer. method start layer analysis propagated deeper layers terminates misclassiﬁcation found. adversarial example found manipulating hidden layer mapped back input layer section report adversarial example. implement algorithm utilising satisﬁability modulo theory solvers. problem decision problem logical formulas respect combinations background theories expressed classical ﬁrst-order logic equality. checking reﬁnement layer theory linear real arithmetic existential universal quantiﬁers veriﬁcation within layer theory without universal quantiﬁcation. details encoding approach taken compute regions manipulations included section enable practical veriﬁcation deep neural networks employ number heuristics described remainder section. feature decomposition discovery theorem provide ﬁnite verify safety neural network classiﬁcation decisions high-dimensionality region make computational approach impractical. therefore concept feature partition region features exploit independence lowdimensionality. allows work state-of-the-art networks hundreds even thousands dimensions. intuitively feature deﬁnes point high-dimensional space explicit salient feature e.g. red-coloured frame street sign figure formally layer feature function assigns small region activation space subspaces dlk. region lower dimension argued e.g. natural images natural data example natural images sound forms high-dimensional manifold embeds tangled manifolds represent features. feature manifolds usually lower dimension data manifold classiﬁcation algorithm separate tangled manifolds. assuming appearance features independent manipulate regardless manipulation order thus reduce problem size smaller problems size analysis activations hidden layers performed method provides opportunity discover features automatically. moreover deﬁning feature activation single region corresponding speciﬁc feature without loss generality although activation include multiple features independence relation features suggests existence total relation features. function essentially deﬁnes activation particular feature subject certain criteria explicit knowledge features also explored parallel. procedure checking points acts conducted either following pre-speciﬁed sequential order exhaustively searching possible orders section demonstrate single-path search according prominence features enable adversarial examples multi-path search examples whose distance original input image smaller. procedure summarised algorithm typically invoked given image input layer providing insight hidden layers available start layer network. selection regions automated described below. ﬁrst layer considered i.e. region deﬁned ﬁrst selecting subset dimsk dimensions whose activation values furthest away average activation value layer. intuitively knowledge represented activations explicit knowledge represented dimensions manipulations explicit knowledge likely result p∈pk αxk)/nk average activation value layer dimsk) ﬁrst dimsk dimensions greatest values |αxk avg| among dimensions deﬁne i.e. dimsk-polytope containing activation represents small span represents number spans. dimsk)} variables. function mapping dimsk) dimsk)} functions. manipulation activation manipulation changes subset dimensions span according directions given deﬁned collecting manipulations. based this deﬁne ladders complete covering. determining region according given functions automatically determine region satisfying deﬁnition using following approach. according function activation value perceptron computed activation values subset perceptrons pk−. vars perceptrons. selection dimensions dimsk) depends dimsk−) requiring that every dimsk−) least dimension dimsk) vars. dimsk) {arg omit details rewriting αyk− αyk− boolean expressions follow standard techniques. note expression includes variables αyk−. variables ﬁxed given ηk−. region always exists simple iterative procedure invoked gradually increase size region represented variables eventually satisfy expression. determining manipulation according values variables obtained satisﬁability yield deﬁnition manipulations using however obtained values span variables necessarily satisfy reﬁnement layer relation deﬁned deﬁnition therefore need adapt values variables while time retaining region could rewrite constraint deﬁnition formula solved solver. practice notice precise computations easily lead overly small spans turn result unacceptable amount computation needed verify relation reduce computational cost work weaker reﬁnable layer notion parameterised respect precision given activations dist represent distance. deﬁnition manipulation reﬁnable layer precision exists sequence activations valid manipulations δk−k given neural network dist αxtk input manipulations reﬁnement layer comparing deﬁnition deﬁnition replaces δk−k δk−k dist intuitively instead requiring manipulation reach activation δk−k precisely deﬁnition allows δk−k within hyper-rectangle rec. suitable values according approximate reﬁnement-by-layer relation variable represent maximal number manipulations layer used express manipulation layer value automatically adapted ensure satisﬁability following formula expresses constraints deﬁnition mapping back input layer manipulating hidden layers need back activation layer input layer obtain input image resulted misclassiﬁcation involves computation described next. check -variation region need compute diﬀn many points diﬀn given diﬀn otherwise. known need compute αyn. compute ﬁnding point using neural network predict value αyn. noted that although include point points class point suﬃcient purpose. computation relies solver encode functions piecewise linear functions taking corresponding inverse functions directly sigmoid functions. possible that point found solver means point corresponding point input layer. safely discard points. maxpooling function selects every dimensions maximal element computation maxpooling layer combined computation next layer ﬁnding following expression proposed framework implemented software tool called written python appendix details input parameters tool. solver employ python apis. neural networks built widely-used neural networks library keras deep learning package theano backend. validate experiments performed neural networks trained classiﬁcation based predeﬁned multi-dimensional surface well image classiﬁcation networks respectively representative types layers fully connected layers convolutional layers. also types layers e.g. relu layer pooling layer zero-padding layer dropout layer. ﬁrst three demonstrate single-path search functionality euclidean norm whereas fourth multipath search norms. two-dimensional point classiﬁcation network demonstrate exhaustive veriﬁcation facilitated framework consider neural network trained classifying points two-dimensional curve shown figure figure network three fully-connected hidden layers relu activation function. input layer perceptrons every hidden layer perceptrons output layer perceptrons. network trained points sampled provided two-dimensional space accuracy around point taking unit steps directions manipulation shown figure points point middle represents activation points represent activations resulting applying manipulations note that although class changes region manipulation able detect changes. therefore sions layer indices computes manipulation mapping back input layer function given figure note satisfy deﬁnition reﬁnement layer image classiﬁcation network mnist handwritten image dataset well-known mnist image dataset contains images size channel network trained source code given trained network medium size parameters accuracy stateof-the-art. layers within convolutional layers well layers relu dropout fully-connected layers softmax layer. images preprocessed make value pixel within bound given image start layer parameter dimensions computed according simple heuristic mentioned section satisfy deﬁnition deﬁnition region allow changes activation value selected dimension within includes manipulations change activation value subset dimensions incrementing decrementing value dimension experimental results show examples class change within dimensional changes layer comparing number pixels changed less dimensional changes. figure presents examples class changes layer also experiment images dimensional changes layer tool able check entire network reaching output layer claiming training network takes half hour ﬁnding adversarial example takes several minutes. image classiﬁcation network cifar- small image dataset work medium size neural network trained source code hours well-known cifar dataset. inputs network images size three channels. trained network real-valued paillustration type perturbations investigating consider images figure correspond parameter setting dimensions respectively layer manipulations change activation values dimensions. image obtained mapping back ﬁrst hidden layer represents point close boundary corresponding region. relation holds ﬁrst images fails last image classiﬁed truck. intuitively choice region identiﬁes subset dimensions extreme activations taking advantage analytical capability ﬁrst hidden layer. higher number selected dimensions implies larger region apply manipulations importantly suggests dramatic change knowledge represented activations moving boundary region. also work dimensions otherwise experimental parameters mnist. figure appendix gives pairs original images perturbed images found that manipulations lead human-recognisable modiﬁcations images perturbed images classiﬁed wrongly network. image ﬁnding adversarial example ranges seconds minutes. image classiﬁcation network imagenet dataset also conduct experiments large image classiﬁcation network trained popular imagenet dataset. images size three channels. network model -layer network called used team ilsvrc competition downloaded trained network realvalued parameters includes convolutional layers relu layers zero-padding layers dropout layers max-pooling layers fully-connected layers softmax layer. experimental parameters previous experiments except work dimensions. several additional pairs original perturbed images included figure appendix figure also give examples street sign images. image left reported unsafe second layer dimensional changes right reported safe dimensional changes layer appears complex manipulations involving dimensions needed case cause class change. evaluate gtsrb dataset classes. figure presents results multi-path search. ﬁrst case stop sign changed speed limit miles distance distance conﬁdence manipulated image second easy case speed limit miles changed speed limit miles distance distance conﬁdence manipulated image also right sign easily manipulated sign classiﬁed straight. fgsm calculates optimal attack linear approximation network cost whereas explores proportion dimensions feature space input hidden layers. jsma ﬁnds dimensions input layer manipulate according linear approximation model current output nominated target output. intuitively diﬀerence dlv’s manipulation jsma manipulates features discovered activations hidden layer jsma manipulates according partial derivatives depend parameters network. experiment randomly select image mnist dataset. figure shows intermediate ﬁnal images obtained running three approaches fgsm jsma dlv. fgsm single parameter greater represents greater perturbation along gradient cost function. given input example perturbed example returned test whether adversarial example checking misclassiﬁcation original image. gradually increase parameter last image witnessing class change images figure fgsm eﬃciently manipulate images requires relatively large manipulation misclassiﬁcation. jsma approach conduct experiment setting parameters parameter means consider adversarial examples changing pixels suﬃcient here. stated parameter allows maximum change every pixel ensure fewer pixels need changed. approach takes series manipulations gradually lead misclassiﬁcation images middle figure misclassiﬁed image distance distance original image. jsma adversarial examples smaller distance original image takes longer manipulate images. fgsm jsma follow speciﬁc heuristics deterministically explore space images. however cases heuristics omit better adversarial examples. experiment instead giving features speciﬁc order manipulating sequentially allow program nondeterministically choose features. currently done mcts theoretical guarantee convergence inﬁnite sampling. therefore high-dimensional space explored following many diﬀerent paths. taking manipulation fig. fgsm jsma fgsm jsma search single path multiple paths. original image perturbed deterministically fgsm ﬁnal image misclassiﬁed middle original image perturbed deterministically jsma show even numbered images produced jsma ﬁnal image misclassiﬁed bottom original image perturbed nondeterministically manipulation single pixel jsma working input layer ﬁnal image misclassiﬁed single pixel jsma working input layer able another perturbed image also classiﬁed smaller distance original image images last figure terms time taken adversarial example take longer jsma since searches many diﬀerent paths. experiment table gives comparison robustness evaluation three appraoches mnist dataset. fgsm vary input parameter according values select regions deﬁned section single path ﬁrst hidden layer. experiment parameterised varying maximal number dimensions changed i.e. dimsl input image adversarial example returned found manipulating fewer maximal number dimensions. maximal number reached report failure return last perturbed example. jsma experiment conducted letting take value setting test size images selected randomly. takes minutes manipulate input image mnist. jsma takes minutes image works classes running time similar dlv. fgsm works images fastest image. mention smaller distance leading misclassiﬁcation result lower rate transferability meaning misclassiﬁcation harder witness another model trained data-set. constraints approximating sigmoid using constraints solved solver approach works neurons similar idea presented contrast work layer layer obtain much greater scalability. since ﬁrst version paper appeared another constraint-based method proposed improves consider general correctness properties paper handle relu activation functions extending simplex method work piecewise linear relu functions cannot expressed using linear programming. necessitates search tree heuristic search proposed shown complete. approach demonstrated networks relu nodes encodes full network unclear whether scaled work practical deep neural networks example mnist network relu nodes. also handle continuous spaces directly without discretisation beneﬁts clear since argued linear behaviour high-dimensional spaces suﬃcient cause adversarial examples. computing perturbations also proposed based box-constrained optimisation approximate view non-convexity search space. work followed introduced much faster fgsm method employed compromise notation uses deterministic iterative manipulation \u0001signxj) image matrix representation hyper-parameter tuned diﬀerent manipulated images cross-entropy cost function neural network input class αxn. therefore approach test discrete points region input layer. therefore manipulations test lasso-type ladder tree satisfy covering property. instead working single image evolutionary algorithm employed population images. individual image current population manipulation mutation and/or crossover. mutations nondeterministic manipulations individual image also following lasso-type ladder tree covering. also mention uses several distortions jpeg compression thumbnail resizing random cropping test robustness trained network. distortions understood manipulations. attacks leverage speciﬁc properties model family guarantee misclassiﬁed image constraint region even image exists. notion robustness studied similarities deﬁnition safety except authors work values averaged input distribution diﬃcult estimate accurately high dimensions. optimisation without convergence guarantees result computing approximation minimal perturbation. pointwise robustness adopted corresponds general safety; also constraint solver represent full constraint system reduction convex problem verify approximation property. contrast work directly activations rather encoding activation functions method exhaustively searches complete ladder tree adversarial example iterative nondeterministic application manipulations. further deﬁnition manipulation ﬂexible since allows select subset dimensions subset diﬀerent region diameter computed respect diﬀerent norm. paper presents automated veriﬁcation framework checking safety deep neural networks based systematic exploration region around data point search adversarial manipulations given type propagating analysis deeper layers. though focus classiﬁcation task approach also generalises types networks. implemented approach using validated several state-of-the-art neural network classiﬁers realistic images. results encouraging adversarial examples found cases matter seconds working dimensions veriﬁcation process exponential number features prohibitive complexity larger images. performance scalability method signiﬁcantly improved parallelisation. would interesting notions regularity suggested permit symbolic approach whether abstraction reﬁnement framework formulated improve scalability computational performance.", "year": 2016}