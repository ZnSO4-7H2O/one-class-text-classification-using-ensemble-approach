{"title": "Training wide residual networks for deployment using a single bit for  each weight", "tag": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "abstract": "For fast and energy-efficient deployment of trained deep neural networks on resource-constrained embedded hardware, each learned weight parameter should ideally be represented and stored using a single bit. Error-rates usually increase when this requirement is imposed. Here, we report large improvements in error rates on multiple datasets, for deep convolutional neural networks deployed with 1-bit-per-weight. Using wide residual networks as our main baseline, our approach simplifies existing methods that binarize weights by applying the sign function in training; we apply scaling factors for each layer with constant unlearned values equal to the layer-specific standard deviations used for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with 1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve error rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We also considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test results of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error rates halve previously reported values, and are within about 1% of our error-rates for the same network with full-precision weights. For networks that overfit, we also show significant improvements in error rate by not learning batch normalization scale and offset parameters. This applies to both full precision and 1-bit-per-weight networks. Using a warm-restart learning-rate schedule, we found that training for 1-bit-per-weight is just as fast as full-precision networks, with better accuracy than standard schedules, and achieved about 98%-99% of peak performance in just 62 training epochs for CIFAR-10/100. For full training code and trained models in MATLAB, Keras and PyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/ .", "text": "mark mcdonnell computational learning systems laboratory school information technology mathematical sciences university south australia mawson lakes australia mark.mcdonnellunisa.edu.au fast energy-efﬁcient deployment trained deep neural networks resource-constrained embedded hardware learned weight parameter ideally represented stored using single bit. error-rates usually increase requirement imposed. here report large improvements error rates multiple datasets deep convolutional neural networks deployed -bit-per-weight. using wide residual networks main baseline approach simpliﬁes existing methods binarize weights applying sign function training; apply scaling factors layer constant unlearned values equal layer-speciﬁc standard deviations used initialization. cifar- cifar- imagenet models -bitper-weight requiring less parameter memory achieve error rates respectively. also considered mnist svhn imagenet achieving -bit-per-weight test results respectively. cifar error rates halve previously reported values within errorrates network full-precision weights. networks overﬁt also show signiﬁcant improvements error rate learning batch normalization scale offset parameters. applies full precision -bit-per-weight networks. using warm-restart learning-rate schedule found training -bit-per-weight fast full-precision networks better accuracy standard schedules achieved peak performance training epochs cifar-/. full training code trained models matlab keras pytorch https//github.com/mcdonnell-lab/-bit-per-weight/. fast parallel computing resources namely gpus integral resurgence deep neural networks ascendancy becoming state-of-the-art methodologies many computer vision tasks. however gpus expensive wasteful terms energy requirements. typically compute using single-precision ﬂoating point recognized providing precision needed deep neural networks. moreover training deployment require availability large amounts memory storage trained models operational ram. deep-learning methods become embedded resourceconstrained sensors devices intelligent systems ranging robotics internet-of-things self-driving cars reliance high-end computing resources need reduced. ∗this work conducted part hosted visit institute neural computation university california diego part sabbatical period consilium technology adelaide australia. paper report signiﬁcant reduction convolutional neural networks deployed using weights stored applied using standard precision networks deployed using weights represented single-bit each. process developing methods also obtained signiﬁcant improvements error-rates obtained full-precision versions cnns used. addition application custom hardware deploying deep networks networks deployed using -bit-per-weight previously shown enable signiﬁcant speedups regular gpus although possible using standard popular libraries. figure error-rate gaps using full-precision -bit-per-weight. points except black crosses data best results reported paper dataset. black points results full imagenet dataset comparison results rastegari notation corresponds network width form called deep residual network resnet developed used many accuracy records computer-vision benchmarks. comparison older cnns alexnet vggnet resnets achieve higher accuracy fewer learned parameters flops image processed. reducing number parameters resnets replace all-to-all layers vgg-like nets global-average-pooling layers learned parameters simultaneously training much deeper network previously. idea enabled deeper network trained effectively introduction so-called skip-connections many variations resnets since proposed. resnets offer virtue simplicity given motivation deployment custom hardware chosen primary focus. despite increased efﬁciency parameter usage similar cnns accuracy resnets still tends increase total number parameters; unlike cnns increased accuracy result either deeper wider networks achieving best accuracy speed possible deploying resnets similar networks hardware-constrained mobile devices require minimising total number bits transferred between memory processors given number parameters. motivated considerations recent attention directed towards compressing learned parameters reducing precision computations carried neural networks—see hubara detailed literature review. recently published strategies model compression include reducing precision weights deployed networks during training reducing number weights trained neural networks pruning quantizing compressing weights following training reducing precision computations performed forward-propagation inference modifying neural network architectures theoretical analysis various methods proved results convergence variety weight-binarization methods range strategies focused approach simultaneously contributes desirable attributes simplicity sense deployment trained models immediately follows training without extra processing; implementation convolution operations achieved without multipliers demonstrated rastegari state-of-the-art baseline. sought begin baseline full-precision deep variant close state-of-the-art error rates. time commencement state-of-the-art cifar- cifar- held wide residual networks starting point. subsequent approaches exceeded accuracy resnets offer superior simplicity conforms third strategy list. make minimal changes training -bit-per-weight. aimed ensure training -bit-per-weight could achieved minimal changes baseline training. simplicity desirable custom hardware. custom hardware implementations mind sought simplify design baseline network much possible without sacriﬁcing accuracy. although paper chieﬂy -bit-per-weight exceeded objectives fullprecision baseline network surpassed reported error rates cifar- cifar- using wide resnets achieved using convolutional layers; prior work demonstrated best wide resnet performance using layers. innovation achieves signiﬁcant error-rate drop cifar- cifar- wide resnets simply learn per-channel scale offset factors batch-normalization layers retaining remaining attributes layers. important done conjunction exchanging ordering ﬁnal weight layer global average pooling layer observed effect pronounced cifar- gaining around test-error rate. method advantageous networks overﬁt; overﬁtting issue imagenet removing learning batch-norm parameters detrimental. ﬁrst study aware consider error-rate -bit-per-weight compared full-precision weights changes full-precision accuracy across diverse range image classiﬁcation datasets approach surpasses large margin previously reported error rates cifar-/ networks constrained -bit-per-weight inference time. reason achieved lower error rates -bit case previously start superior baseline network previous studies namely wide resnets. however approach also results smaller error rate increases relative full-precision error rates previously training requires number epochs case full-precision weights. main innovation introduce simple ﬁxed scaling method convolutional layer permits activations gradients network minimum change standard deviation accordance principle underlying popular initialization methods combine warm-restart learning-rate method enables report close-to-baseline results -bit case fewer epochs training reported previously. follow approach courbariaux rastegari merolla good results using -bit-per-weight inference time training apply sign function real-valued weights purpose forward backward propagation update full-precision weights using gradients calculated using full-precision. however previously reported methods training using sign weights either need train many hundreds epochs computationallycostly normalization scaling channel layer changes minibatch training i.e. method rastegari obtained results using simple alternative approach describe. begin noting standard deviation sign weights convolutional layer kernels size close assuming mean zero. contrast standard deviation layer full-precision networks initialized method number input channels convolutional layer number convolutional layers inputs. applying sign function alone mismatch principled approach controlling gradient activation scaling deep network although batch-normalization still enable learning convergence empirically slow less effective. address problem training using sign weights initialization method full-precision weights updated also introduce layer-dependent scaling applied sign weights. scaling constant unlearned value equal method enables standard deviation forward-propagating information equal value would initially full-precision networks. implementation training multiply sign weights layer value. inference multiplication using scaling layer following weight layer weights network stored using deployed using hence custom hardware implementations would able perform model’s convolutions without multipliers signiﬁcant speedups also possible fact scale weights explicitly training important. although forward backward propagation equivalent scale input output feature maps convolutional layer also scales calculated gradients respect weights since calculated convolving input output feature maps. consequence learning dramatically slower unless layer-dependent learning rates introduced cancel scaling. approach similar method rastegari constant scaling method faster less complex. summary differences make comparison full-precision training follows. tensor convolutional weights i–th convolutional layer. weights processed following forward propagation backward propagation weight updates resnets ‘post-activation’ identity mapping approach residual connections. imagenet -layer design datasets mainly -layer network also report results layers. residual block includes convolutional layers preceded batch normalization rectiﬁed linear unit layers. rather train deep resnets wide residual networks although zagoruyko komodakis others reported /-layer networks result better test accuracy /-layer networks found cifar-/ layers typically produces best results possibly approach learning batch-norm scale offset parameters. baseline resnet design used experiments several differences comparison zagoruyko komodakis details articulated appendix mostly simplicity little impact accuracy. exception approach learning batch-norm parameters. trained models following aspects standard stochastic gradient descent methods used zagoruyko komodakis wide resnets. speciﬁcally cross-entropy loss minibatches size momentum cifar-/ svhn mnist overﬁtting evident wide resnets larger weight decay imagenet full imagenet weight decay apart experiments added simple extra approach called cutout standard ‘light’ data augmentation including randomly ﬂipping image horizontally probability cifar-/ imagenet. figure wide resnet architecture. design mostly standard pre-activation resnet ﬁrst convolutional layer ﬁrst residual blocks output channels. next blocks output channels widening parameter. ﬁnal convolutional layer convolutional layer gives output channels number classes. importantly ﬁnal convolutional layer followed batch-normalization prior global-average-pooling softmax blocks number channels double downsampling blocks reduce spatial dimension feature factor two. rectiﬁed-linearunit layer closest input optional included best learn scale offset subsequent layer. figure downsampling blocks wide resnet architecture. standard pre-activation resnet downsampling used convolutional layers number output channels increases. corresponding downsampling skip connections done residual block. unlike standard pre-activation resnets average pooling layer residual path downsampling. datasets plus svhn pixels sides crop patch random location resulting image. full imagenet scale crop image-preprocessing i.e. subtract mean initial layer network performs role whitening augment using color brightness. initialization method training cifar-/ svhn batch-norm layers learn scale offset factor instead initializating channels keeping values training. note also learn biases convolutional layers. usual approach setting moments batch-norm layers inference mode keep running average training. learning batch-normalization parameters found small beneﬁt calculating batch-normalization moments used inference training ﬁnished. simply form many minibatches possible full training-set data augmentation used training applied pass trained network averaging returned moments batch. network’s final weight layer convolutional layer signiﬁcant difference resnets zagoruyko komodakis exchange ordering global average pooling layer ﬁnal weight layer ﬁnal weight layer becomes convolutional layer many channels classes training set. convolutional layers follow ﬁnal layer batch-normalization layer; beneﬁts conjunction learning batch-normalization scale offset described discussion section. epochs repeats across twice many epochs. restricted attention maximum epochs using method total number epochs reducing learning rate maximum minimum epochs epochs. cifar-/ typically found could achieve test error rates epochs within error rates achievable epochs. literature experiments cifar- cifar- simple standard data augmentation consisting randomly ﬂipping training image left-right probability padding image sides pixels cropping version image random location. augmentation although minor modiﬁcation uniform random integers rather zero-padding. additionally experimented cutout involves randomly selecting patch training image remove. method shown combine state-of-the-art methods latest state-of-the-art results cifar-/ found better results using larger cutout patches cifar- reported devries taylor hence cifar- cifar- choose patches size following method devries taylor ensured pixels chosen included patch equally frequently throughout training ensuring chosen patch location near image border patch impacts image part patch inside image. differently devries taylor padding uniform random integers replace image pixel values location patches. apply cutout datasets. conducted experiments databases four databases images—cifar cifar- svhn imagenet—and full ilsvrc imagenet database well mnist details ﬁrst three datasets found many papers e.g. imagenet downsampled version imagenet training validation images cropped using annotated bounding boxes downsampled http//image-net.org/download-images. experiments carried single using matlab acceleration matconvnet cudnn. report results wide resnets wider baseline resnets terminology zagoruyko komodakis baseline channels layers ﬁrst spatial scale. notation form denote wide resnets convolutional layers channels ﬁrst spatial scale hence width full imagenet dataset -layer wide resnets channels ﬁrst spatial scale given standard resnet baseline width corresponds width dataset denote networks table lists top- error rates cifar-/; indicates cifar- indicates cifar-; superscript indicates standard crop augmentation; indicates cutout. table lists error rates svhn imagenet full imagenet; cutout datasets. top- top- results tabulated imagenet. imagenet provide results single-center-crop testing also multi-crop testing. latter decision test image obtained averaging softmax output passing network times corresponding crops obtained rescaling scales described random positions scale. full-precision imagenet error rates slightly lower expected wide resnet according results zagoruyko komodakis probably fact color augmentation. table shows comparison results original work wide resnets subsequent papers reduced error rates cifar- cifar- datasets. also show results knowledge imagenet. current state-of-the-art svhn without augmentation cutout augmentation full-precision result svhn little short even though used resnet less million parameters training epochs. table shows comparison results previous work trains models using sign weights training. additional results appear hubara activations quantized error rates much larger. inspection tables reveals baseline full-precision networks trained cutout surpass performance deeper wide resnets trained dropout. even without cutout network surpasses cifar- error rate reported essentially network zagoruyko komodakis also better previous wide resnet results cifar- imagenet. elaborated section improved accuracy approach learning batch-norm scale offset parameters. using cutout cifar-/ reduces error rates expected. comparison training wide resnets cifar- shown table effective cutout augmentation network reduce error rate using quarter weights. figure illustrates convergence overﬁtting trends cifar-/ wide resnets comparison cutout wide resnets. clearly even width- resnets error rate full precision weights -bit-per-weight small. also noticeable warm-restart method enables convergence good solutions epochs; training longer epochs reduces test error rates also observed network powerful enough model cifar-/ training sets well accuracy modelling power reduced -bit version particularly cifar. reduced modelling capacity single-bit weights consistent test-error rate performance -bit -bit cases. using cutout training longer gives improved error rates using cutout epochs sufﬁces peak performance. finally mnist wide resnet without data augmentation full-precision method achieved epoch training epochs whereas -bit method achieved epochs. comparison reported -bit-per-weight case courbariaux hubara figure convergence training. left marker shows error rates test training cycle warm-restart training method resnets right marker shows test error rate resnets without cutout. indicates cifar- indicates cifar-. cifar-/ full-precision wide resnets -bit-per-weight versions beneﬁt method learning batch-norm scale offset parameters. accuracy -bit-perweight case also beneﬁts warm-restart training schedule demonstrate inﬂuence aspects figure show cifar- test error rate changes training either methods used. cutout purpose ﬁgure. comparison learning-rate-schedule drops learning rate epochs. clear methods lower ﬁnal error rate around absolute learning batch-norm parameters. warm-restart method enables faster convergence full-precision case signiﬁcant reducing error rate. however -bit-per-weight case clear best results best warm-restart learn batch-norm parameters. expected smaller error rate result network using fullprecision -bit-per-weight test error rate full precision network gets smaller. indeed tables quantify error rate full-precision -bit-perweight cases tends grow error-rate full-precision network grows. illustrate trend approach plotted figure top- error rates top- error rate full precision case best performing networks datasets used. strong conclusions data made relative alternative methods ﬁrst study knowledge consider datasets. nevertheless also plotted figure error rate reported rastegari different networks using -bit weight method. reasons larger gaps points unclear clear better full-precision accuracy results smaller cases. challenge work derive theoretical bounds predict gap. magnitude changes full precision error rate dependent many factors including method used generate models -bit-per-weight. high-error rate cases loss function throughtraining much higher -bit case -bit case hence -bit-per-weight network able training well -bit one. whether loss precision weights mismatch gradients inherent propagating -bit weights updating full-precision weights training open question. latter case possible principled reﬁnements weight update method used reduce gap. however also interesting -layer networks applied cifar/ much smaller despite beneﬁts full precision case extra depth also warrants investigation. approach differs method rastegari reasons. first need calculate mean absolute weight values underlying full precision weights output channel layer following minibatch enables faster training. second need adjust gradient term corresponding appearance weight mean absolute value. found overall methods work equally effectively advantages faster training fewer overall parameters. note found method rastegari also works equally effectively per-layer basis rather per-channel. also note focus rastegari much case combines single-bit activations -bit-per-weight solely -bit-per-weight. remains tested scaling method compares case. also interesting understand whether batch-norm renders scaling sign weights robust different scalings whether networks batch-norm might sensitive precise method used. unusual design choice learning batch normalization parameters made cifar/ svhn mnist wide resnets overﬁtting evident datasets training typically loss function becomes close zero corresponding severe overﬁtting. inspired label-smoothing regularization aims reduce overconﬁdence following softmax layer hypothesized imposing control standard deviation inputs softmax might similar regularizing effect. removed ﬁnal all-to-all layer resnets replaced convolutional layer followed batch-normalization layer prior global average pooling layer. turn learning scale offset batch-normalization layer ensures batches ﬂowing softmax layer standard deviations grow throughout training tends increase entropy predictions following softmax equivalent lower conﬁdence observing success methods wide resnets observed learning batch-norm parameters layers also increased overﬁtting increased test error rates removed learning layers shown figure signiﬁcant beneﬁts approach full-precision bit-per-weight networks. table results surpass zagoruyko komodakis effectively wide resnet expected motivation found method appropriate datasets imagenet overﬁtting evident case learning batch normalization parameters signiﬁcantly reduces test error rates. compare approach squeezenet reported signiﬁcant memory savings trained model relative alexnet. squeezenet approach uses strategies achieve this replacing many kernels kernels; deep compression regarding squeezenet strategy note squeezenet all-convolutional network closely resembles resnets used here. experimented brieﬂy -bit-per-weight approach many all-convolutional variants—e.g. plain all-convolutional squeezenet mobilenet resnext found effectiveness relative full-precision baselines comparable variants. also observed many experiments total number learned parameters correlates well classiﬁcation accuracy. applied squeezenet variant cifar- found obtain accuracy resnets depth increase width squeezenet approximately number learned parameters resnet. conclude method therefore reduces model size baseline squeezenet architecture factor albeit accuracy gap. regarding squeezenet strategy squeezenet paper reports deep compression able reduce model size approximately factor accuracy loss. method reduces model size factor small accuracy loss gets larger full-precision accuracy gets smaller. would interesting explore whether deep compression might applied -bit-per-weight models focus methods minimally alter training leave investigation complex methods future work. regarding squeezenet performance best accuracy reported squeezenet paper imagenet top- error requiring model’s weights. single-bit-weight models achieve better top- error require model’s weights. paper focus reducing precision weights single-bit beneﬁts model compression enabling inference multiplications. also interesting desirable reduce computational load inference using trained model carrying layer computations numbers bits facilitating requires modifying non-linear activations network relus quantized relus extreme case binary step functions. full-precision calculations. expected combining methods reduced precision processing inevitably increase error rates. addressed extension forthcoming submission. work supported discovery project funded australian research council discussions visit hosting gert cauwenberghs hesham mostafa ucsd andr´e schaik runchun wang western sydney university gratefully zhang sun. delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation. proc. ieee international conference computer vision howard chen kalenichenko wang weyand andreetto adam. mobilenets efﬁcient convolutional neural networks mobile vision applications. corr abs/. http//arxiv.org/abs/.. hubara courbariaux soudry el-yaniv bengio. quantized neural networks training neural networks precision weights activations. corr abs/. http//arxiv.org/abs/.. mcdonnell wang schaik. reduced-memory training deployment deep residual networks stochastic binary quantization. abstract talk neuro inspired computational elements workshop held almaden march merolla appuswamy arthur esser modha. deep neural networks robust weight binarization non-linear distortions. corr abs/. http//arxiv.org/abs/.. rastegari ordonez redmon farhadi. xnor-net imagenet classiﬁcation using binary convolutional neural networks. corr abs/. http//arxiv. org/abs/.. russakovsky deng krause satheesh huang karpathy khosla bernstein berg fei-fei. imagenet large scale visual recognition challenge. international journal computer vision s---y. skip connections feature maps different sizes zero-padding increasing number channels option however residual pathway average pooling using kernel size stride downsampling instead typical lossy downsampling discards pixel values samples. literature reported various options optimal ordering usage placement relu layers residual networks. following zagoruyko komodakis precede convolutional layers combination followed relu. however different zagoruyko komodakis also insert immediately input layer ﬁrst convolutional layer. optional relu used unlike batch-normalization layers enable learning scale offset factors. ﬁrst layer enables avoid pre-processing inputs network since layer provides necessary normalization. optional relu included found learned offset ensures input ﬁrst relu never negative. accordance strategy simplicity weight layers thought block three operations sequence indicated figure conceptually batch-norm followed relu thought single layer consisting relu adaptively changes centre point positive slope channel relative mini-batch. also precede global average pooling layer layer relu point since nonlinear activation provided softmax layer. found including relu leads differences early training completion training. wide resnet zagoruyko komodakis speciﬁed ﬁrst convolutional layer always constant number output channels even number output channels layers increases. found need impose constraint instead always allow ﬁrst layer share number output channels blocks ﬁrst spatial scale. increase total number parameters small relative total number parameters since number input channels ﬁrst layer beneﬁt change increased simplicity network deﬁnition ensuring fewer change dimensionality residual pathway. interested understanding whether good results achieved single-bit weights consequence skip connections residual networks. therefore applied method plain all-convolutional networks identical residual networks except skip connections removed. initially training indicated much slower convergence found altering initial weights standard deviations proportional instead helped change made. change also applied equation summarized figure found convergence remained slower resnets small accuracy penalty comparison resnets epochs training. consistent ﬁndings resnets deeper layers showed signiﬁcant advantage plain all-convolutional networks. experiment others done support view method particular resnets.", "year": 2018}