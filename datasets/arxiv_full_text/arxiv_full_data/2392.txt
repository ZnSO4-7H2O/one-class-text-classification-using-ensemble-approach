{"title": "A Greedy Approximation of Bayesian Reinforcement Learning with Probably  Optimistic Transition Model", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Bayesian Reinforcement Learning (RL) is capable of not only incorporating domain knowledge, but also solving the exploration-exploitation dilemma in a natural way. As Bayesian RL is intractable except for special cases, previous work has proposed several approximation methods. However, these methods are usually too sensitive to parameter values, and finding an acceptable parameter setting is practically impossible in many applications. In this paper, we propose a new algorithm that greedily approximates Bayesian RL to achieve robustness in parameter space. We show that for a desired learning behavior, our proposed algorithm has a polynomial sample complexity that is lower than those of existing algorithms. We also demonstrate that the proposed algorithm naturally outperforms other existing algorithms when the prior distributions are not significantly misleading. On the other hand, the proposed algorithm cannot handle greatly misspecified priors as well as the other algorithms can. This is a natural consequence of the fact that the proposed algorithm is greedier than the other algorithms. Accordingly, we discuss a way to select an appropriate algorithm for different tasks based on the algorithms' greediness. We also introduce a new way of simplifying Bayesian planning, based on which future work would be able to derive new algorithms.", "text": "plan exploit possible future knowledge hence naturally trade exploring exploiting. however except limited environments full bayesian planning intractable. therefore general need adopt approximation techniques monte-carlo method myopic approach optimism face uncertainty principle computationally efficient approximate bayesian planning. intractability bayesian planning comes considering possible future beliefs myopic approach solves problem simply disregarding compensate myopic thinking approach usually employs optimism encourage agents explore uncertain aspects environments. several algorithms based approach shown guarantee polynomial sample complexity work surprisingly well practice however recent studies raised question parameter sensitivity algorithms. parameters type algorithm optimal testing algorithms’ performances wide range parameter values however usually cannot tune parameters hence useful algorithms work without thorough parameter optimization procedure also brunskill stated parameter tuning required number time steps algorithms optimal large. algorithms’ sample complexities large performances unacceptably poor reaching sample numbers. summary desirable fast algorithm less time steps poor behaviors maintains high level performance despite parameter choices. paper propose novel algorithm works wider range parameter values lower sample complexity previous algorithms. proposed algorithm keeps similar level overall computational cost existing fast algorithms. present algorithm first review bayesian approximation methods. then introduce effectively modify standard bayesian planning using information potentially correct mdps. addition discuss demonstrate proposed algorithm’s properties. agent’s goal maximize total returns solving sequential decision-making problems environment containing unknown aspects. environment represented markov decision process tuple states actions transition abstract bayesian reinforcement learning capable incorporating domain knowledge also solving explorationexploitation dilemma natural way. bayesian intractable except special cases previous work proposed several approximation methods. however methods usually sensitive parameter values finding acceptable parameter setting practically impossible many applications. paper propose algorithm greedily approximates bayesian achieve robustness parameter space. show desired learning behavior proposed algorithm polynomial sample complexity lower existing algorithms. also demonstrate proposed algorithm naturally outperforms existing algorithms prior distributions significantly misleading. hand proposed algorithm cannot handle greatly misspecified priors well algorithms can. natural consequence fact proposed algorithm greedier algorithms. accordingly discuss select appropriate algorithm different tasks based algorithms’ greediness. also introduce simplifying bayesian planning based future work would able derive algorithms. reinforcement learning successful technique used number real-world problems renders ability design adaptable agents work well uncertain environments consequences action obvious remaining challenge exploration-exploitation dilemma; agents need explore world order obtain knowledge must exploit current knowledge earn rewards. elegant solution dilemma bayesian agents possible next belief transition observed belief optimal policy corresponds actions maximize right hand side equation seen equation bayesian optimal agent chooses actions considers actions affect knowledge well. hence bayesian optimal agent naturally solves trade-off exploration better knowledge exploitation current knowledge. however computing bayesian value function equation usually possible. number possible belief states typically large full bayesian planning equation intractable cases therefore approximation techniques required. straightforward approach approximate bayesian planning monte carlo method. monte carlo method used deal curse dimensionality many problems approach used belief-state space bayesian sparse sampling direct application monte carlo method dynamic programing described bellman’s equation standard bayesian since common drawback monte carlo method computational time number algorithms developed based sparse sampling chiefly order expedite calculation time particular bayesian sparse sampling proposed bayesian setting modifying sparse sampling. worth understanding concept algorithm intuitions behind proposed method similarities. unlike original sparse sampling looks ahead possible scenarios towards certain degrees future bayesian sparse sampling utilizes information embedded agent’s belief order pick possible future scenarios sampled. looks ahead scenarios actions potentially optimal based belief that effectively allocates samples practice also works cases continuous discrete action spaces unlike original sparse sampling. myopic approach optimism algorithms using monte carlo method like sparse sampling guarantee near-optimal behavior theory. however sampling planning phase slows decision-making speed. computationally faster approximate bayesian planning myopic approach optimism face uncertainty principle. myopic approach agent explicitly consider effects actions future beliefs thus myopic. optimism face uncertainty principle compensates myopic thinking. myopic planning optimal current agent’s knowledge perfect agent takes actions favoring reduce epistemic uncertainty myopic thinking justified end. probability function reward function discount factor. agent takes action state triggers transition another state accordance transition probability function receiving reward based reward function discount factor accounts relative importance immediate rewards compared future rewards discounting future rewards. also obviates need think ahead toward infinite horizon. paper consider case discrete state space discrete action space maximize rewards received lifetime agent needs take action considering immediate consequences possible future repercussions words rewards dependent sequences actions rather single action accordingly agent’s performance described actions policy maps state space action space. agent’s performance expressed value policy next state transitioned current state. find optimal policy optimal value function instead comparing possible value functions convenient bellman’s optimality equation optimal policy corresponds actions state maximizes right hand side equation optimal policy action found solving equation simple algorithms value iteration policy iteration work focus model-based also assume unknown aspect environment transition probability function agent ought learn equation setting agent needs estimate based observations. maximum likelihood estimation straightforward this. however agent uses equation along estimated standard would become stuck sub-optimal policy agent intention explore state-action pairs order gain knowledge. bayesian reinforcement learning elegant solution exploration-exploitation dilemma bayesian approach explicitly accounts transitions agents’ beliefs. means bayesian planning agent recognizes transitions belief besides transitions environment’s states denote expected value transition probability based current belief algorithms guarantee optimality myopic approach perform well narrow range parameter values. section propose algorithm called probably optimistic transition perform better work wider range parameter choices compared existing algorithms. main reason existing algorithms generalize well large parameter values degree optimism becomes unreasonably high unless perfect parameter value assigned. intuition behind adaptively adjust degree optimism combining knowledge potentially true bayesian planning. standard optimistic approach agent expects maximum outcomes agent believes possible thinking ahead. hand lets agent preferable models agent actually able obtain high probability future. words agent uses probably optimistic transition models. posterior variance transition probability. right hand side equation represents possible number observing transition based potentially true mdp. seen decreasing parameter diminishes degree optimism last term right hand side penalizes nonoptimistic parameter setting balances degree. theoretical support equation discussed later. note greater time horizon replaces theoretical properties algorithm convenient consider computational effectiveness type algorithm different levels computation time action number actions required achieving optimal learning behavior former disadvantage seems first glance calculation time extra argument compared comparison bolt. however remember optimism tightly bounded. fixed discount factor convergence criteria means value iteration converges smaller number steps force agent favor uncertain states agent believe preferable outcomes range uncertainty algorithms guarantee near-optimal behavior. example r-max assumes unknown states maximum rewards assures pac-mdp behavior. hand several algorithms bellman’s equation exploration reward bonus defined reward plus exploration reward bonus algorithm uses version equation model based interval estimation exploration bonus mbie-eb ensures pac-mdp behavior prior information. order make prior information equation employed bonus posterior variance variance-based reward bonus vbrb guarantees pac-mdp behavior like r-max mbie-eb. however make sure behavior correct regarding true pac-mdp algorithms show overexploration despite prior knowledge preferable sense. bayesian exploration bonus uses equation algorithms ensure near-bayesian optimal policy without sampling. another algorithm bayesian optimistic local transitions uses modified transition probability model rather reforming reward function bolt employs corresponds transition model based belief modified certain number artificial observations number parameter bolt. instance independent dirichlet distribution stateaction pair known flat-dirichlet-multinomial bolt’s modified transition model written follows outputs equal otherwise returns bolt distinct advantage algorithms equation bolt degree optimism bounded probabilistic parameter choice; parameter takes place numerator denominator right hand side equation thus right hand side bounded this bolt less sensitive parameter values works well wider range parameter values still performance decreases parameter well tuned. finding algorithms’ optimal parameter values possible many practical situations therefore want algorithms work well wide range parameter values. however existway bayesian planning simplifies standard bayesian planning analogous following situation. cannot identify exact occurrence-probability nuclear accidents. thus future believe probability much higher currently believed. however would impractical assume believe accidents happen every nature even imagining scenario sequential accidents coincidentally occurred number days. sample complexity section show fdms holds optimism bayesian guarantees polynomial sample complexity near bayes-optimal behavior. first make relationship bayesian clear also reveal derive information regarding probably true using chebyshev’s inequality emma define maximum number belief updating transition bayesian planning. equation equal positive real number least transition model number less probability least roof. true transition probabilities least within belief space infer upper bounds values using chebyshev's inequality. based mean estimations posterior variance probability least -/λ. notice bound true mdp’s values used unlike order achieve pac-mdp behavior. then applying hoeffding's inequality upper bound above probability least maximum occurrence-number transitions bounded instance maximum number belief updating example illustrated figure seen final step proof lemma restrict number free parameters has. thus parameter algorithm allowed instead employing equation equation improve learning performance. paper equation denote optimal bayesian value function value function used respectively. define number value function updates. parameter least probability least -|s||a|c/λ figure simple example bayesian planning states indicated black taking actions. numbers along state transition arrows abstractly illustrate agent’s belief evolution transition probability states transition happen less times high probability example agent update belief events violating knowledge indicated red.) closer actual horizon bayes-optimal planning. thus agent take action could faster bolt practice. discuss sample complexity first introduce modification bayesian behavior. guarantees polynomial sample complexity agent behave nearly well modified version bayes-optimal learning. show shortly lower sample complexity pac-mdp near bayes-optimal algorithms. moreover modified version bayes-planning exploit additional current knowledge thus allow agents greedier exploration methods. modification bayes-optimal planning knowledge regarding probably true order illustrate pot’s property present simplifying bayesian planning relaying information likely correct mdp. characteristic call simplified bayesian planning probably upper bounded belief-based bayesian planning bayesian planning short. idea behind similar concept underling bayesian sparse sampling. bayesian sparse sampling limits possible future scenarios thought based myopic heuristics likely optimal actions bayesian planning accordance probability theory likely correct mdp. also unlike bayesian sparse sampling omit progression state-action space unreasonable belief evolutions. concretely bayesian planning agent consider belief evolutions sets events happen high probability. figure shows simple example. transition probability states upper bounded probability least agent observe transition less times according hoeffding's inequality. sample complexity bayesian optimal policy weaker concept standard bayesian optimal policy. discuss exact relationship types optimal polices paper leaving future work. section present performances existing algorithms -state chain environment standard benchmark problem literature. figure illustrates environment agent states agent choose actions ‘b’. action leads agent action lets agent stay hand action leads agent states. probability agent slips performs opposite action intended. rewards returning staying otherwise. even though optimal policy always select action setting encourages non-exploring agent settle taking action ‘b’. assume dynamics transitions among states completely unknown discount factor convergence criteria equal value iteration. make results comparable previously published results report algorithms’ performances showing cumulative rewards first steps. results section show algorithms’ performances three different situations according prior knowledge transition probabilities. focus situations prior knowledge varying degrees useful knowledge different magnitudes misleading knowledge. andv step value iteration represents arbitrary positive value. noticing second term right hand side equation reaches maximum equal rewrite inequality follows second line fact probability least transition probability. needs true preferable transition state-action pair inequality holds probability least -|s||a|/λ. turn true entire execution must true value function updates results upper probability bound -|s||a|c/λ. last line shown induction. sorg indicated number value function updates upper bounded |s||a| continue using notation paper. finally based discussions present pot’s sample complexity. denote value function described equation policy rather bayesian optimal policy. suppose agent stops updating belief |a|=θ/). parameter least least |s||a|c/δ. then follow policy -close bayesian optimal policy probability least less time steps previous algorithms practice order achieve desired behavior. example bolt greater equal also sample complexity pac-mdp algorithms usually much greater. course notice informative knowledge regarding dynamics. account situation constructed informative priors updating uniform prior containing small amounts information ideal observations multiple true probabilities prior’s sizes. similar create informative prior used figure figure respectively show runs’ average total rewards bolt different degree informative priors parameters. result bolt shown representative existing algorithms natural approach bayesian statistics. figure shows average total reward versus parameter value. total rewards shown figure average based runs made standard error negligible results indicate maintained higher level performance wider range parameters algorithms. exactly predicted previous sections. existing algorisms work well large parameter values values made agent much optimistic. hand behaved poorly small parameter values since values agent optimistic exploit current knowledge. contrast worked well even large small parameter values adaptively changed degree optimism based information true mdp. terms range parameter values agent achieved rewards turned best followed bolt vbrb worst. note parameters’ theoretical meanings differ algorithm hence comparing results scale figure abuse notation theoretical point view. however practical point view parameters’ theoretical meanings almost irrelevant here. notice peak curve figure corresponds theoretical values parameters. based practical standpoint treated parameters equally arbitrarily adjustable parameter without meaning pragmatically important comparison. table summarizes different algorithms’ performances optimal parameter settings. algorithms ordered table highest lowest performance. results based runs standard errors presented along average total rewards. parameter algorithm optimized adopted previous work. indeed estimated optimal parameter mbie-eb value reported kolter worked better others difference average total rewards tended ascribed value. algorithms table potential obtain several trials could assure near total rewards high probability discuss reason relationship among algorithms’ performances figure table together results misleading priors later understand pot’s great performance case prior knowledge time discuss performance setting). figure respectively indicates bolt effectively utilize informative priors. average total rewards went degree information increased almost reached optimal total reward importantly comparison figure figure tells unlike bolt earned least settings. also report optimal parameter settings achieved average total reward prior size bolt size implies would superior ability utilize small amount informative knowledge. putting together results thus could perform better existing algorithms prior knowledge either informative assigned agent. intuitively makes sense adaptively controls degree optimism greedier algorithms. finally discuss algorithms’ ability handle misspecified prior information. here misspecified prior defined uniform prior non-small amount information. prior considered misspecified true transition probabilities uniform concept used figure shows average total reward versus degree misspecified prior based runs. seen comparing figure figure table algorithms’ ranks terms ability handle misspecified priors opposite ranks non-misleading priors. example vbrb seems best situation worst settings. vbrb pac-mdp algorithm derived bayesian setting could avoid misled misspecified priors expense aiming bayesian optimality. hand bolt share goal approximate bayesian optimal behavior. hence misled incorrect knowledge similar extent worked better pac-mdp algorithm prior reasonable. turn difference performances bolt explained inequality greediness sample complexity. discussed previous section greedier lower sample complexity others. theoretical properties naturally explain results worked best prior reasonable worst prior misspecified. paper introduced algorithm called agent greedier existing algorithms perform well wide range parameter values. derived letting agent utilize bayesian optimal reasoning also information potentially true mdp. concretely agent adaptively changes degree optimism learns true potentially lies. larger optimal parameter value existing algorithms usually maintain much optimism explore. hand smaller optimal parameter value existing algorithms optimistic enough become stuck sub-optimal state. naturally solved issue letting agent adaptive degrees optimism. relaxed requirement placed optimism face uncertainty principle. consequence relaxing condition optimism guarantee standard bayesian behavior probably upper bounded belief-based bayesian behavior. unlike existing approximation methods bayesian bayesian myopic heuristics omit search state-action space. instead bayesian limits possible belief-evolutions thought ahead accordance probability theory. therefore bayesian optimal high probability assigned information largely misleading condition also holds true standard bayesian difference standard bayesian bayesian greedier standard bayesian exploits additional current knowledge limit explorations. concept alternative optimal behavior allowed lower sample complexity ability explore environments greedily previous algorithms. demonstrated points standard chain problem. predicted outperformed algorithms prior distribution greatly misspecified. case achieved highest average total reward optimal parameter setting also showed much lower parameter sensitivity compared others. hand limitation shown inability handle misspecified priors. also demonstrated exact drawback exists near bayes-optimal algorithms compared pac-mdp algorithm. disadvantage near bayes-optimal algorithms comes greater greediness pac-mdp said pot. course greedier behaviors distinct advantages also confirmed point experiment. therefore think selection algorithm pac-mdp standard bayesian optimal algorithms choice preferable greediness level. preference regarding degree greediness differ different tasks confidence levels prior knowledge. example small prior confident prior large bias ought choose greedier algorithm. sense introduction concept bayesian contribute giving robust algorithm varying parameter values also choice select algorithm level greediness. future work includes using concept bayesian derive algorithms. instance interesting existing algorithms like sparse sampling work modified concept bayesian another future work would test variety experiments. chain problem largely penalize overexploration find another advantage pot’s adaptive optimism problems disfavor over-exploration more. kearns mansour sparse sampling algorithm near-optimal planning large markov decision processes. proceedings sixteenth international joint conference artificial intelligence pages asmuth littman. approaching bayes-optimality using monte-carlo tree search. proceedings international conference automated planning scheduling brunskill. bayes-optimal reinforcement learning discrete uncertainty domains. proceedings international conference autonomous agents multiagent systems strehl littman. analysis model-based interval estimation markov decision processes. journal computer system sciences sorg singh lewis. variance-based rewards", "year": 2013}