{"title": "Learning from Simulated and Unsupervised Images through Adversarial  Training", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.", "text": "figure simulated+unsupervised learning. task learn model improves realism synthetic images simulator using unlabeled real data preserving annotation information. data however learning synthetic images problematic synthetic real image distributions synthetic data often realistic enough leading network learn details present synthetic images failing generalize well real images. solution closing improve simulator. however increasing realism often computationally expensive content modeling takes hard work even best rendering algorithms still fail model characteristics real images. lack realism cause models overﬁt ‘unrealistic’ details synthetic images. paper propose simulated+unsupervised learning goal improve realism synthetic images simulator using unlabeled real data. improved realism enables training better machine learning models large datasets without data collection human annotation effort. addition adding realism learning preserve annotation information training machine learning models e.g. gaze direction figure preserved. moreover since machine learning models sensitive artifacts synthetic data recent progress graphics become tractable train models synthetic images potentially avoiding need expensive annotations. however learning synthetic images achieve desired performance synthetic real image distributions. reduce propose simulated+unsupervised learning task learn model improve realism simulator’s output using unlabeled real data preserving annotation information simulator. develop method learning uses adversarial network similar generative adversarial networks synthetic images inputs instead random vectors. make several modiﬁcations standard algorithm preserve annotations avoid artifacts stabilize training ‘self-regularization’ term local adversarial loss updating discriminator using history reﬁned images. show enables generation highly realistic images demonstrate qualitatively user study. quantitatively evaluate generated images training models gaze estimation hand pose estimation. show signiﬁcant improvement using synthetic images achieve state-of-the-art results mpiigaze dataset without labeled real data. large labeled training datasets becoming increasingly important recent rise high capacity deep neural networks however labeling large datasets expensive timeconsuming. thus idea training synthetic instead real images become appealing annotations automatically available. human pose estimation kinect recently plethora tasks tackled using synthetic present qualitative quantitative user study experiments showing proposed framework signiﬁcantly improves realism simulator output. achieve state-of-the-art results without human annotation effort training deep neural networks reﬁned output images. framework learns networks competing losses. goal generator network random vector realistic image whereas goal discriminator distinguish generated real images. framework ﬁrst introduced goodfellow generate visually realistic images since then many improvements interesting applications proposed wang gupta structured learn surface normals combine style generate natural indoor propose recurrent generative scenes. model trained using adversarial training. recently proposed igan enables users change image interactively natural image manifold. cogan uses coupled gans learn joint distribution images multiple modalities withrequiring tuples corresponding images achieving weight-sharing constraint favors joint distribution solution. chen propose infogan information-theoretic extension allows learning meaningful representations. tuzel tackled image super-resolution face images gans. wand propose markovian efﬁcient texture synthesis. lotter adversarial loss lstm network visual sequence prediction. propose seqgan framework uses gans reinforcement learning. tackle pixel-level semantic transfer learning gans. style transfer also closely related work. many recent works explored related problems domain generative models pixelrnn predicts pixels sequentially softmax loss. generative networks focus generating images using random noise vector; thus contrast method generated images annotation information used training machine learning model. figure overview simgan. reﬁne output simulator reﬁner neural network minimizes combination local adversarial loss ‘selfregularization’ term. adversarial loss ‘fools’ discriminator network classiﬁes image real reﬁned. self-regularization term minimizes image difference synthetic reﬁned images. reﬁner network discriminator network updated alternately. learning generate images without artifacts. develop method learning term simgan reﬁnes synthetic images simulator using neural network call ‘reﬁner network’. figure gives overview method synthetic image generated black simulator reﬁned using reﬁner network. realism train reﬁner network using adversarial loss similar generative adversarial networks reﬁned images indistinguishable real ones using discriminative network. preserve annotations synthetic images complement adversarial loss self-regularization loss penalizes large changes synthetic reﬁned images. moreover propose fully convolutional neural network operates pixel level preserves global structure rather holistically modifying image content e.g. fully connected encoder network. framework requires training neural networks competing goals known unstable tends introduce artifacts avoid drifting introducing spurious artifacts attempting fool single stronger discriminator limit discriminator’s receptive ﬁeld local regions instead whole image resulting multiple local adversarial losses image. moreover introduce method improving stability training updating discriminator using history reﬁned images rather ones current reﬁner network. realism synthetic image need bridge distributions synthetic real images. ideal reﬁner make impossible classify given image real reﬁned high conﬁdence. need motivates adversarial discriminator network trained classify images real reﬁned parameters discriminator network. adversarial loss used training reﬁner network responsible ‘fooling’ network classifying reﬁned images real. following approach model two-player minimax game update reﬁner network discriminator network alternately. next describe intuition precisely. equivalent cross-entropy error class classiﬁcation problem probability input synthetic image real one. implement convnet whose last layer outputs probability sample reﬁned image. training network mini-batch consists randomly sampled reﬁned synthetic images xi’s real images yj’s. target labels cross˜ entropy loss layer every every ˜xi. mini-batch updated taking stochastic gradient descent step mini-batch loss gradient. minimizing loss function reﬁner forces discriminator fail classifying reﬁned images synthetic. addition generating realistic images reﬁner network preserve annotation information simulator. example gaze estimation learned transformation change gaze direction hand pose estimation location joints change. restriction essential ingredient enable training machine learning model uses reﬁned images simulator’s annotations. purpose propose using selfregularization loss minimizes per-pixel difference feature transform synthetic reﬁned images mapping image space feature space norm. feature transform identity various prediction tasks including gaze estimation text detection classiﬁcation images font recognition object detection hand pose estimation depth images scene recognition rgb-d semantic segmentation urban scenes human pose estimation gaidon show pre-training deep neural network synthetic data leads improved performance. work complementary approaches improve realism simulator using unlabeled real data. ganin lempitsky synthetic data domain adaptation setting learned features invariant domain shift synthetic real images. wang train stacked convolutional auto-encoder synthetic real data learn lower-level representations font detector convnet. zhang learn multichannel autoencoder reduce domain shift real synthetic data. contrast classical domain adaptation methods adapt features respect speciﬁc prediction task bridge image distributions adversarial training. approach allows generate realistic training images used train machine learning model potentially multiple tasks. johnson transfer style real images synthetic image co-segmenting identifying similar regions. approach requires users select matches image database. contrast propose end-to-end solution require user intervention inference time. goal simulated+unsupervised learning unlabeled real images learn reﬁner reﬁnes synthetic image function parameters. reﬁned image denoted requirement learning reﬁned image look like real image appearance preserving annotation information simulator. synthetic training image. ﬁrst part cost real adds realism synthetic images second part preserves annotation information. following sections expand formulation provide algorithm optimize algorithm adversarial training reﬁner network input sets synthetic images real images number steps number discriminator network updates step number generative network updates step image derivatives mean color channels learned transformation convolutional neural network. paper unless otherwise stated used identity feature transform. thus overall reﬁner loss function used implementation implement fully convolutional neural without striding pooling modifying synthetic image pixel level rather holistically modifying image content e.g. fully connected encoder network thus preserving global structure annotations. learn reﬁner discriminator parameters minimizing alternately. updating parameters keep ﬁxed updating summarize training procedure algorithm local adversarial loss another requirement reﬁner network learn model real image characteristics without introducing artifacts. train single strong discriminator network reﬁner network tends over-emphasize certain image features fool current discriminator network leading drifting producing artifacts. observation figure illustration local adversarial loss. discriminator network outputs probability map. adversarial loss function cross-entropy losses local patches. local patch sampled reﬁned image similar statistics real image patch. therefore rather deﬁning global discriminator network deﬁne discriminator network classiﬁes local image patches separately. division limreceptive ﬁeld hence capacity discriminator network also provides many samples image learning discriminator network. reﬁner network also improved multiple ‘realism loss’ values image. implementation design discriminator fully convolutional network outputs dimensional probability patches belonging fake class number local patches image. training reﬁner network cross-entropy loss values local patches illustrated figure another problem adversarial training discriminator network focuses latest reﬁned images. lack memory cause divergence adversarial training reﬁner network re-introducing artifacts discriminator forgotten about. reﬁned image generated reﬁner network time entire training procedure ‘fake’ image discriminator. hence discriminator able classify images fake. based observation introduce figure example output simgan unityeyes gaze estimation dataset real images mpiigaze reﬁner network label information mpiigaze dataset training time. reﬁnement results unityeye. skin texture iris region reﬁned synthetic images qualitatively signiﬁcantly similar real images synthetic images. examples included supplementary material. method improve stability adversarial training updating discriminator using history reﬁned images rather ones current minibatch. slightly modify algorithm buffer reﬁned images generated previous networks. size buffer mini-batch size used algorithm iteration discriminator training compute discriminator loss function sampling images current reﬁner network sampling additional images buffer update parameters keep size buffer ﬁxed. training iteration randomly replace samples buffer newly generated reﬁned images. procedure illustrated figure contrast approach salimans used running average model parameters stabilize training. note approaches complementary used together. evaluate method appearance-based gaze estimation wild mpiigaze dataset hand pose estimation hand pose dataset depth images fully convolutional reﬁner network resnet blocks experiments. gaze estimation ingredient many human computer interaction tasks. however estimating gaze direction image challenging especially image quality e.g. laptop mobile phone camera annotating images gaze direction vector challenging even humans. therefore generate large amounts annotated data several recent approaches train models large amounts synthetic data. here show training reﬁned synthetic images generated simgan signiﬁcantly outperforms state-of-the-art task. gaze estimation dataset consists synthetic images unityeyes simulator real images mpiigaze dataset samples shown figure mpiigaze challenging gaze estimation dataset captured extreme illumination conditions. unityeyes single generic rendering environment generate training data without dataset-speciﬁc targeting. qualitative results figure shows examples synthetic real reﬁned images gaze dataset. shown observe signiﬁcant qualitative improvement synthetic images simgan successfully captures skin texture sensor noise appearance iris region real images. note method preserves annotation information improving realism. self-regularization feature space synthetic real images signiﬁcant shift distribution pixel-wise difference restrictive. cases replace identity alternative feature transform. example figure mean channels color image reﬁnement. shown network trained using feature transform able generate realistic color images. note quantitative experiments still grayscale images gaze estimation better tackled grayscale added invariance ‘visual turing test’ quantitatively evaluate visual quality reﬁned images designed simple user study subjects asked classify images real reﬁned synthetic. subject shown random selection real images retable comparison gaze estimator trained synthetic data output simgan. results distance degrees ground truth. training output simgan outperforms training synthetic data ﬁned images random order asked label images either real reﬁned. subjects constantly shown examples real reﬁned images performing task. subjects found hard tell difference real images reﬁned images. aggregate analysis subjects chose correct label times trials meaning able reliably distinguish real images synthetic. table shows confusion matrix. contrast testing original synthetic images real images showed real synthetic images subject subjects chose correctly times trials signiﬁcantly better chance. quantitative results train simple convolutional neural network similar predict gaze direction loss. train unityeyes test mpiigaze. figure table compare performance gaze estimation trained synthetic data another trained reﬁned synthetic data output simgan. observe large improvement performance training simgan output absolute percentage improvement. also observe large improvement using training data refers training dataset. quantitative evaluation conﬁrms value qualitative improvements observed figure shows machine learning models generalize signiﬁcantly better using simgan. table shows comparison state-of-the-art. training reﬁned images outperforms state-of-the-art mpiigaze dataset relative improvement large improvement shows practical value method many tasks. figure quantitative results appearance-based gaze estimation mpiigaze dataset real images. plot shows cumulative curves function degree error compared ground truth gaze direction different numbers training examples data. method support vector regression adaptive linear regression alr) random forest multiview multiview k-nn unityeyes unityeyes synthetic images unityeyes reﬁned images error table comparison simgan state-of-the-art mpiigaze dataset real eyes. second column indicates whether methods trained real/synthetic data. error mean gaze estimation error degrees. training reﬁned images results degree improvement relative improvement compared state-of-the-art. preserving ground truth quantify ground truth gaze direction doesn’t change signiﬁcantly manually labeled ground truth pupil centers synthetic reﬁned images ﬁtting ellipse pupil. approximation gaze direction difﬁcult humans label accurately. absolute difference estimated pupil center synthetic corresponding reﬁned image quite small implementation details reﬁner network residual network resnet block consists convolutional layers containing feature maps. input image size convolved ﬁlters output feature maps. output passed resnet blocks. output last resnet block passed convolutional layer producing feature corresponding refigure example reﬁned test images hand pose dataset real images synthetic images corresponding reﬁned output images reﬁner network. major source noise real images non-smooth depth boundaries reﬁner networks learns model. discriminator network contains convolution layers max-pooling layers follows convx stride= feature maps= convx stride= feature maps= maxpoolx stride= convx stride= feature maps= convx stride= feature maps= convx stride= feature maps= softmax. adversarial network fully convolutional designed receptive ﬁeld last layer neurons similar. ﬁrst train network self-regularization loss steps steps. then update update twice i.e. algorithm gaze estimation network similar changes enable better exploit large synthetic dataset. grayscale image passed convolutional layers followed fully connected layers last encoding -dimensional gaze vector convx feature maps= convx feature maps= convx feature maps= maxfeature maps= poolx stride= convx maxpoolx stride= euclidean loss. networks trained constant learning rate batch size validation error converges. next evaluate method hand pose estimation depth images. hand pose dataset contains training frames testing frames captured kinect cameras frontal side views. depth frame labeled hand pose information used qualitative results figure shows example output simgan hand pose test set. main source noise real depth images depth discontinuity edges simgan able learn without requiring label information. quantitative results train fully convolutional hand pose estimator similar stacked hourglass real synthetic reﬁned synthetic images hand pose training evaluate model real images hand pose test set. train hand joints many state-of-the-art hand pose estimation methods customized pipelines consist several steps. single deep neural network analyze effect improving synthetic images avoid bias factors. figure table present quantitative results hand pose. training reﬁned synthetic data output simgan require labeling real images outperforms model trained real images supervision proposed method also outperforms training synthetic data. also observe large improvement number synthetic training examples increased corresponds training views. implementation details architecture gaze estimation except input image size ﬁlter size resnet blocks used. discriminative convx stride= feature maps= convx stride= feature maps= maxpoolx stride= convx stride= feature maps= convx stride= feature maps= convx stride= feature maps= figure quantitative results hand pose estimation hand pose test real depth images plot shows cumulative curves function distance ground truth keypoint locations different numbers training examples synthetic reﬁned images. first analyzed effect using history reﬁned images training. shown figure using history reﬁned images prevents severe artifacts observed training without history results increased gaze estimation error degrees without history comparison degrees history. next compare local global adversarial loss training. global adversarial loss uses fully connected layer discriminator network classifying whole image real reﬁned. local adversarial loss removes artifacts makes generated image signiﬁcantly realistic seen figure figure using history reﬁned images updating discriminator. synthetic images; result using history reﬁned images; result without using history reﬁned images observe obvious unrealistic artifacts especially around corners eyes. figure importance using local adversarial loss. example image generated standard ‘global’ adversarial loss whole image. noise around edge hand contains obvious unrealistic depth boundary artifacts. image generated local adversarial loss looks signiﬁcantly realistic. proposed simulated+unsupervised learning realism simulator preserving annotations synthetic images. described simgan method learning uses adversarial network demonstrated state-of-the-art results without labeled real data. future intend explore modeling noise distribution generate reﬁned image synthetic image investigate reﬁning videos rather single images.", "year": 2016}