{"title": "Theoretical Properties for Neural Networks with Weight Matrices of Low  Displacement Rank", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Recently low displacement rank (LDR) matrices, or so-called structured matrices, have been proposed to compress large-scale neural networks. Empirical results have shown that neural networks with weight matrices of LDR matrices, referred as LDR neural networks, can achieve significant reduction in space and computational complexity while retaining high accuracy. We formally study LDR matrices in deep learning. First, we prove the universal approximation property of LDR neural networks with a mild condition on the displacement operators. We then show that the error bounds of LDR neural networks are as efficient as general neural networks with both single-layer and multiple-layer structure. Finally, we propose back-propagation based training algorithm for general LDR neural networks.", "text": "recently displacement rank matrices so-called structured matrices proposed compress large-scale neural networks. empirical results shown neural networks weight matrices matrices referred neural networks achieve signiﬁcant reduction space computational complexity retaining high accuracy. formally study matrices deep learning. first prove universal approximation property neural networks mild condition displacement operators. show error bounds neural networks efﬁcient general neural networks single-layer multiple-layer structure. finally propose backpropagation based training algorithm general neural networks. neural networks especially large-scale deep neural networks made remarkable success various applications computer vision natural language processing etc. however large-scale neural networks memory-intensive computation-intensive thereby posing severe challenges deploying large-scale neural network models memory-constrained energy-constrained embedded devices. overcome limitations many studies approaches connection pruning rank approximation sparsity regularization etc. proposed reduce model size large-scale neural networks. construction neural networks among efforts displacement rank construction type structure-imposing technique network model reduction computational complexity reduction. regularizing weight matrices neural networks using format matrices composition multiple matrices strong structure naturally imposed construction neural networks. since matrix typically requires independent parameters exhibits fast matrix operation algorithms immense space network model computational complexity reduction enabled. pioneering work direction applied special types matrices circulant beneﬁts neural networks compared types network compression approaches construction shows several unique advantages. first unlike heuristic weight-pruning methods produce irregular pruned networks construction approach always guarantees strong structure trained network thereby avoiding storage space computation time overhead incurred complicated indexing process. second train scratch technique construction need extra re-training hence eliminating additional complexity training process. third reduction space complexity computational complexity using structured weight matrices signiﬁcant. different network compression approaches provide heuristic compression factor construction enable model reduction computational complexity reduction big-o complexity storage requirement reduced computational complexity reduced existence fast matrix-vector multiplication algorithm matrices. example applying structured matrices fully-connected layers alexnet using imagenet dataset storage requirement reduced incurring negligible degradation overall accuracy motivation work inherent structure-imposing characteristic convenient retraining-free training process unique capability simultaneous big-o complexity reduction storage computation construction promising approach achieve high compression ratio high speedup broad category network models. however since imposing structure weight matrices results substantial reduction weight storage cautious researchers need know whether neural networks construction referred neural networks consistently yield similar accuracy compared uncompressed networks. although already shown using construction still results accuracy minor degradation various datasets imagenet cifar etc. theoretical analysis provide mathematically solid proofs neural networks converge effectiveness uncompressed neural networks still necessary order promote wide application neural networks emerging larger-scale applications. technical preview contributions address necessity paper study provide solid theoretical foundation neural networks ability approximate arbitrary continuous function error bound function approximation applications shallow deep neural networks etc. speciﬁcally main contributions paper include prove universal approximation property neural networks states neural networks could approximate arbitrary continuous function arbitrary accuracy given enough parameters/neurons. words neural network effectiveness classical neural networks without compression. property serves theoretical foundation potential broad applications neural networks. develop universal training process neural networks computational complexity reduction compared backward propagation process classical neural networks. proposed algorithm generalization training process restricts structure weight matrices circulant matrices toeplitz matrices. outline paper outlined follows. section review related work topic. section presents necessary deﬁnitions properties matrix displacement neural networks. problem statement also presented section. section prove universal approximation property broad family neural networks. section addresses approximation potential limited amount neurons shallow neural networks deep neural networks respectively. proposed detailed procedure training general neural networks derived section section concludes article. universal approximation error bound analysis feedforward neural networks hidden layer proved separately universal approximation property guarantees given continuous function decision function error bound always exists single-hidden layer neural network approximates function within integrated error. however property specify number neurons needed construct neural network. practice must limit maximum amount neurons computational limit. moreover magnitude coefﬁcients neither large small. address issues general neural networks proved sufﬁcient approximate functions weights biases whose absolute values bounded constant extended result arbitrarily small bound. showed feedforward networks layer sigmoidal nonlinearities achieve integrated squared error order number neurons. recently several interesting results published approximation capabilities deep neural networks. shown exist certain functions approximated three-layer neural networks polynomial amount neurons two-layer neural networks require exponentially larger amount achieve error. shown exponential increase linear regions neural networks grow deeper. proved layers neural network achieve error bound continuous function parameters layer. matrices neural networks analyzed effectiveness replacing conventional weight matrices fully-connected layers circulant matrices reduce time complexity space complexity respectively. demonstrated signiﬁcant beneﬁts using toeplitz-like matrices tackle issue large space computation requirement neural networks training inference. experiments show matrices displacement rank offers superior tradeoffs accuracy time/space complexity. table pairs displacement operators associated structured matrices. represent unit-circulant matrix -unit-circulant matrix respectively vector denote vectors deﬁning vandermonde cauchy matrices matrix rank bounded value independent size matrix referred matrix displacement rank paper call matrices matrices. even full-rank matrix small displacement rank appropriate choice displacement operators figure illustrates series commonly used structured matrices including circulant matrix cauchy matrix toeplitz matrix hankel matrix vandermonde matrix table summarizes displacement ranks corresponding displacement operators. general procedure handling matrices generally takes three steps compression computation displacements decompression. compression means obtain low-rank displacement matrices decompression means converting results displacement computations answer original computational problem. particular displacement operator property power equals identity matrix following method decompress directly lemma a-potent matrix important characteristics structured matrices number independent variables. number independent parameters n-by-n structured matrix instead order indicates storage complexity potentially reduced besides computational complexity many matrix operations matrix-vector multiplication matrix inversion etc. signiﬁcantly reduced operating structured ones. deﬁnition analysis structured matrices generalized case n-by-m matrices e.g. block-circulant matrices application matrices neural networks would general n-by-m weight matrices. certain lemmas theorems lemma form square matrices needed derivation procedure paper. omit generalized form statements unless necessary. paper study viability applying matrices neural networks. without loss generality focus feed-forward neural network fully-connected layer similar network setup input layer hidden layer assumed fully connected weight matrix rn×kn displacement rank corresponding displacement operators domain input vector n-dimensional hypercube output layer contains neuron. neural network expressed activation function denotes j-th column weight matrix weight matrix low-rank displacement call neural network. matrix displacement techniques ensure neural network much lower space requirement higher computational speed comparing classical neural networks similar size. paper providing theoretical support accuracy function approximation using neural networks represents effectiveness neural networks compared original neural networks. given continuous function deﬁned study following tasks ﬁrst task handled section universal approximation property neural networks. states neural networks could approximate arbitrary continuous function arbitrarily well underpinning widespread applications. error bounds shallow deep neural networks derived section addition derived explicit back-propagation expressions neural networks section section ﬁrst prove theorem matrix displacements. based theorem prove universal approximation property neural networks utilizing matrices. theorem non-singular diagonalizable matrices satisfying denote resulting scalar matrix product bk−ej deﬁne order prove theorem need show exists vector shjλk nonsingular. order distinguish scalar multiplication matrix multiplication notation denote multiplication scalar value matrices whenever necessary. rewrite expression shjλk nonsingular diagonal entries nonzero. unless every index always choose appropriate vector resulting diagonal matrix nonsingular. next show former case possible using proof contradiction. assume column every must have assumption matrix different absolute values since absolute value fact suggests distinguished solutions equation contradicts fundamental theorem algebra. thus incorrect assume shjλk singular property proven given vector take following procedure matrix index j-th column equals main goal section show neural networks many types matrices approximate continuous functions arbitrarily well. particular going show toeplitz matrices circulant matrices speciﬁc cases matrices property. order need introduce following deﬁnition discriminatory function property. deﬁnition function called discriminatory zero measure measure satisﬁes following property weight matrix theorem continuous discriminatory function. continuous function deﬁned rn×n satisfying assumptions theorem exists function form equation weight matrix consists submatrices displacement rank denote continuous functions deﬁned linear subspace expressed form equation consists sub-matrices displacement rank want show dense continuous functions suppose hahn-banach theorem exists bounded linear functional since discriminatory function lemma conclude zero measure. result function deﬁned integral measure must zero input function last statement contradicts property hahn-banach theorem obtained based assumption neural network functions dense assumption true universal approximation property neural networks. reference work utilized circulant matrix toeplitz matrix weight representation deep neural networks. please note general case n-by-m weight matrices either general block-circulant matrices utilized padding extra columns rows zeroes needed circulant matrices topelitz matrices special form matrices thus could apply universal approximation property neural networks provide theoretical support circulant toeplitz matrices although circulant toeplitz matrices displacement rank instead property theorem still holds toeplitz matrix completely determined ﬁrst ﬁrst column therefore arrive following corollary. corollary continuous function arbitrarily approximated neural networks constructed toeplitz matrices circulant matrices universal approximation property proved naturally seek ways provide error bound estimates neural networks. able prove matrices deﬁned parameters corresponding structured neural network capable achieving integrated squared error order number parameters. result asymptotically equivalent barron’s aforementioned result general neural networks indicating essentially loss restricting matrices. proved following theorem theorem every function γcbr every sigmoidal function every probability measure every exists linear combination sigmoidal functions form estimates neural networks theorem every disk every function γcbr every sigmoidal function every normalized measure every exists neural network deﬁned weight matrix consists submatrices next theorem naturally extended result neural networks indicating neural networks also beneﬁt parameter reduction uses layers. precisely following statement proof. theorem better bounds without assumption neural network proved theorem binary step unit rectiﬁer linear unit construction general neural network attach dummy units expand weights associated unit vector matrix based theorem need factor original amount units asymptotic bounds relaxed accordingly. activation function rn×kn weight matrix input vector bias vector. according equation matrix operators satisfying conditions theorem essentially determined matrices rn×r rn×r practical want choose matrices fast multiplication method diagonal matrices permutation matrices banded matrices etc. space complexity rather traditional dense matrix. time complexity compared dense matrix. particularly structured matrix like toeplitz matrix space complexity toeplitz matrix deﬁned parameters. moreover matrix-vector multiplication accelerated using fast fourier transform resulting time complexity back-propagation computation layer done near-linear time. paper proven universal approximation property neural networks. addition also theoretically show error bounds neural networks least efﬁcient general unstructured neural network. besides also develop back-propagation based training algorithm universal neural networks. study provides theoretical foundation empirical success neural networks.", "year": 2017}