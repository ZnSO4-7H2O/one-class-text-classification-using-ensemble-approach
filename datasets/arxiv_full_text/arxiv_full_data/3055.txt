{"title": "CleanNet: Transfer Learning for Scalable Image Classifier Training with  Label Noise", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "In this paper, we study the problem of learning image classification models with label noise. Existing approaches depending on human supervision are generally not scalable as manually identifying correct or incorrect labels is timeconsuming, whereas approaches not relying on human supervision are scalable but less effective. To reduce the amount of human supervision for label noise cleaning, we introduce CleanNet, a joint neural embedding network, which only requires a fraction of the classes being manually verified to provide the knowledge of label noise that can be transferred to other classes. We further integrate CleanNet and conventional convolutional neural network classifier into one framework for image classification learning. We demonstrate the effectiveness of the proposed algorithm on both of the label noise detection task and the image classification on noisy data task on several large-scale datasets. Experimental results show that CleanNet can reduce label noise detection error rate on held-out classes where no human supervision available by 41.5% compared to current weakly supervised methods. It also achieves 47% of the performance gain of verifying all images with only 3.2% images verified on an image classification task.", "text": "paper study problem learning image classiﬁcation models label noise. existing approaches depending human supervision generally scalable manually identifying correct incorrect labels timeconsuming whereas approaches relying human supervision scalable less effective. reduce amount human supervision label noise cleaning introduce cleannet joint neural embedding network requires fraction classes manually veriﬁed provide knowledge label noise transferred classes. integrate cleannet conventional convolutional neural network classiﬁer framework image classiﬁcation learning. demonstrate effectiveness proposed algorithm label noise detection task image classiﬁcation noisy data task several large-scale datasets. experimental results show cleannet reduce label noise detection error rate held-out classes human supervision available compared current weakly supervised methods. also achieves performance gain verifying images images veriﬁed image classiﬁcation task. factors drive recent advances largescale image recognition massive collections labeled images like imagenet coco however normally expensive time-consuming collect largescale manually labeled datasets. practice fast development image recognition tasks widely used surrogate automatically collect noisy labeled data internet many studies shown label noise affect accuracy induced classiﬁers signiﬁcantly making desirable develop algorithms learning presence label noise. learning label noise categorized type supervision methods rely human supervision methods not. instance large-scale training data constructed using classiﬁers trained manually veriﬁed seed images remove label noise places studies learning convolutional neural networks noise also rely manual labeling estimate label confusion methods using human supervision exhibit disadvantage scalability require labeling effort every class. classiﬁcation tasks millions classes infeasible even manual annotation class. contrast methods without human supervision unsupervised outliers removal scalable often less effective heuristic. going existing approaches either classes none need manually veriﬁed. difﬁcult scalability effectiveness. work strive reconcile gap. observe ideas learning noisy data ﬁnding class prototypes effectively represent classes. methods learn manually veriﬁed seed images like methods assume majority correctness like belong category. inspired observation develop attention mechanism learns select representative seed images reference image collected class supervised information transfer learned knowledge classes without explicit human supervision transfer learning. effectively addresses scalability problem methods rely human supervision. thus introduce label cleaning network novel neural architecture designed setting. first develop reference encoder attention mechanism encode reference images class embedding vector represents class. second parallel reference embedding also build query embedding vector individual image impose matching constraint training require query embedding similar class embedding query relevant class. words model tell siﬁer noisy data. carried comprehensive experimentation evaluate method label noise detection image classiﬁcation three large datasets real-world label noise clothingm webvision food-n. food-n contains images collected internet food taxonomy added veriﬁcation label veriﬁes whether noisy class label correct image. experimental results show cleannet reduce label noise detection error rate held-out classes human supervision available compared current weakly supervised methods. also achieves performance gain verifying images images veriﬁed image classiﬁcation task. label noise reduction. method belongs category approaches address label noise demoting removing mislabeled instances training data. popular approaches unsupervised outlier removal uocl drae using approach label noise detection relies assumption outliers mislabeled. however outliers often well deﬁned therefore removing presents challenge another approach also need human supervision weakly supervised label noise reduction example thongkam proposed classiﬁcation ﬁltering method learns noisy data removes instances misclassiﬁed svm. weakly supervised methods often heuristic aware large dataset actually built methods. hand label noise reduction using human supervision widely studied dataset constructions. instance proposed manually labeling seed images training multilayer perceptrons remove mislabeled images. similarly places dataset constructed using alexnet trained manually veriﬁed seed images. however methods using human supervision exhibit disadvantage scalability require human supervision every class cleansed. direct neural network learning label noise apart removing label noise algorithms developed directly learning neural network label noise. several works proposed learning models case having label noise rely manual labeling estimate label confusion real-world label noise. azadi developed regularization method actively select image features training depends features pre-trained tasks hence less effective. zhuang proposed attention random sample groups compare standard classifigure cleannet architecture learning class embedding vector query embedding vector similarity matching constraint. exists class embedding classes. details component depicted fig. whether image mislabeled comparing query embedding class embedding. since class embeddings generated different reference sets represents different classes wish model adapt cleannet generalize classes without explicit human supervision. fig. illustrates end-to-end differentiable model. ﬁrst step work demonstrate cleannet effective tool label noise detection. simple thresholding based similarity reference query image lead good results compared existing methods. label noise detection useful training image classiﬁers noisy data also important values applications like image search result ﬁltering linking images knowledge graph entities. cleannet predicts relevance image noisy class label. therefore propose cleannet assign weights image samples according image-tolabel relevance guide training image classiﬁer. hand better classiﬁer provides discriminative convolutional image features learning cleannet refresh cleannet using newly trained classiﬁer. introduce uniﬁed learning scheme train cleannet image classiﬁer jointly. summarize contributions include novel neural architecture cleannet designed make label noise detection learning noisy data human supervision scalable transfer learning. also propose uniﬁed scheme training cleannet image clasﬁers thus less practical. hand training technique perspective rolnick suggested tuning learning rate batch size alleviate accuracy drop caused label noise. several works proposed using side information like knowledge graph beyond scope paper. transfer learning neural network. large body literature learning neural joint embeddings transfer learning tsai trained visual-semantic embeddings supervised unsupervised objectives using labeled unlabeled data improve robustness embeddings transfer learning. recently tzeng exploited adversarial objectives domain adaptation. inspired also incorporate unsupervised objectives work. focus learning image classiﬁer images label noise using transfer learning. specifically assume dataset images i.e. i-th image class label total number classes. note labels noisy means images’ labels incorrect. section present cleannet joint neural embedding network requires fraction classes manually veriﬁed provide knowledge label noise transferred classes. integrate cleannet conventional convolutional neural network system image classiﬁer training label noise. speciﬁcally introduce designs properties cleannet section section integrate cleannet framework image classiﬁer learning noisy data. cleannet overall architecture cleannet shown fig. consists parts reference encoder query encoder. reference encoder learn focus representative features noisy reference image collected speciﬁc class output classlevel embedding vector. since using images reference computationally expensive ﬁrst create representative subset extract visual feature vector image subset form representative feature vector i.e. denotes representative reference feature vector class ﬁrst random sampling subset images class extract features using pre-trained shown fig. second approach running kmeans extracted features images class cluster centroids k-means step ignored ﬁgures. since k-means approach shows slightly better result held-out choose experiments hereafter. select feature vectors form parallel reference encoder also develop query encoder denote query image labeled class query encoder maps query image feature query embedding impose matching constraint query embedding similar class embedding query relevant class words decide whether query mislabeled comparing query embedding vector class embedding vector. since class labels noisy mark query image class label manual veriﬁcation label. veriﬁcation label image deﬁned model learns matching constraint supervised information veriﬁcation labels query embedding similar class embedding query image truly belongs class label transfer different classes human veriﬁcation available. following present build reference encoder query encoder objectives learning matching constraint. reference encoder. architecture reference encoder depicted fig. maps reference feature first two-layer projects image feature hidden representation next learn attention mechanism encode representative features ﬁxed-length hidden normalized cosine similarity negative sample weight balancing positive negative samples margin work. case ignored loss function since supervised objective utilizes query images veriﬁcation label. hand images without veriﬁcation label also utilized learn matching constraint. similar introduce unsupervised self-reinforcing strategy applies pseudo-veriﬁcation images without veriﬁcation label. speciﬁc query treated relevant cos. queries initially treated relevant model learn push similarity queries reference sets; queries initially treated irrelevant ignored. total loss. summarize training objectives model learned minimizing total loss combining supervised unsupervised objectives selected hyper-parameter search indicates whether query image veriﬁcation label. work. training randomly sample images without veriﬁcation label queries fraction mini-batch note parameters attentional reference encoder query encoder tied across classes information learned classes human veriﬁcation labels transferred classes human veriﬁcation label. cleannet label noise detection. shown importance measured similarity context vector similar context vector learned training. driven matching constraint attention mechanism learns attention representative features classes. model learn supervised information i.e. manual veriﬁcation label adapt classes without explicit supervision. example attention mechanism shown fig. finally one-layer maps hidden representation class embedding query encoder. illustrated fig. adopt layer autoencoder query encoder incorporate autoencoder reconstruction error learning objectives. taking strategy proposed forces query embedding preserve semantic information classes including classes without veriﬁcation labels images without veriﬁcation label used training unsupervised objective. proven effective improving domain adaptation performance. reconstructed representation. learning objectives based matching constraint. supervised information human veriﬁcation labels query emsimilarity class embedding bedding maximized query relevant class otherwise similarity minimized adopt cosine similarity loss margin impose threshold selected cross-validation. observe threshold sensitive different classes cases therefore usually select uniform threshold classes veriﬁcation labels required classes cross-validation. cleannet learning classiﬁers cleannet predicts relevance image noisy class label comparing query embedding image class embedding represents class. distance embeddings used decide much attention data sample training image classiﬁer. speciﬁcally assign attention weights data samples based cosine similarity integrating cleannet image classiﬁer. learning image classiﬁer relies cleannet assign proper attention weights data samples. hand better classiﬁer provides discriminative features critical cleannet learning. therefore integrate cleannet cnn-based image classiﬁer framework end-to-end learning image classiﬁers label noise. overall architecture framework illustrated fig. structure cnn-based image classiﬁer split fully-connected layer convolutional layers used feature extraction. alternating training. adopt alternative training scheme learn proposed classiﬁcation system. step ﬁrst train classiﬁer noisy data sample weights step parameters convoluational layers copied feature extractor cleannet trained convergence. step classiﬁer ﬁne-tuned using sample weights proposed cleannet. similar alternating process continue till classiﬁer stops improving. iterations learning classiﬁer convoluational layers ﬁne-tune fullyconnected layers. food-n collect images google bing yelp tripadviser using food taxonomy avoid foodspotting.com original food collected. estimated noisy class label accuracy manually veriﬁcation label training testing label noise detection. release dataset community. image classiﬁcation evaluated food test set. clothingm clothingm public largescale dataset designed learning noisy data human supervision. consists images noisy class labels fashion classes. estimated accuracy supervised supervised methods learn binary classiﬁcation veriﬁcation labels class. consider neural networks data construction) label prop label spread also explored mlps layers -layer shows best results. unsupervised consider drae state unsupervised outlier removal. empirically drae shows better results one-class like unsupervised method weakly supervised methods require veriﬁcation labels. compare widely used classiﬁcation ﬁltering method train model noisy data predict top-k classes training image. image classiﬁed relevant class label class top-k predictions. otherwise classiﬁed mislabeled. selected validation set. table label noise detection terms average error rate classes cleannet* denotes results using image features extracted classiﬁers retrained data cleansed cleannet. class labels also three sets images size respectively correct class labels provided human labelers call clean sets. images overlap three clean sets noisy set. overlapped images verify whether noisy class label correct given human labels images hence obtained veriﬁcation labels images. process obtain veriﬁcation labels training validation respectively. state result image classiﬁcation clothingm reported webvision webvision contains noisy labeled images crawled flickr google using ilsvrc taxonomy conveniently verify noisy class labels using inception-resnet-v model pre-trained ilsvrc. noisy class label image veriﬁed relevant falls top- predictions. otherwise noisy class label veriﬁed mislabeled. randomly obtain pseudo-veriﬁcation labels class training. evaluating image classiﬁcation webvision validation ilsvrc validation set. ﬁrst evaluate cleannet task label noise detection. label noise detection problem viewed binary classiﬁcation problem class hence results comparisons reported average error rate classes. compare following categories existing baseline methods provide additional baselines naive baseline treats class labels correct average baseline simply averages reference features class embedding vector query feature query embedding vector. cleannet baselines depend extract image features. ﬁne-tune imagenet pre-trained resnet- models noisy data step alternating training scheme extract pool layer implementations label image features. prop label spread scikit-learn reimplemented drae experimentation. following evaluate cleannet label noise detection scenarios full supervision veriﬁcation labels classes available learning cleannet; transfer learning fraction classes contain veriﬁcation labels learning cleannet. full supervision. table report label noise detection results terms average error rate classes. cleannet gives error rate food-n clothingm. comparing food-n clothingm validate cleannet performs similar best supervised baseline. comparing classiﬁcation ﬁltering foodn clothingm results demonstrate effectiveness adding veriﬁcation labels human supervision label noise detection. cleannet* denotes results cleannet using image features extracted classiﬁers retrained data cleansed cleannet shows improvements however improvements become negligible iterations. transfer learning. choose food-n demonstrate label noise detection cleannet setting transfer learning veriﬁcation labels classes held cleannet. also consider uses figure label noise detection food-n transfer learning. veriﬁcation labels classes held learning cleannet whereas still uses veriﬁcation labels. note average error rate evaluated classes held cleannet figure selected examples cleannet results food-n clothingm. denotes cosine similarity predicted model using veriﬁcation labels classes. denotes cosine similarity transfer learning class names veriﬁcation labels shown bottom-left. veriﬁcation labels classiﬁcation ﬁltering needs veriﬁcation labels. evaluate results held-out classes demonstrate results classes withexplicit human supervision. results shown fig. first observe cleannet reduce label noise detection error rate held-out classes human supervision available relatively compared classiﬁcation ﬁltering. cleannet consistently outperforms classiﬁcation ﬁltering weakly-supervised baseline. also observe result cleannet classes held still comparable result based supervised learning subsection present experiments learning image classiﬁcation models label noise using proposed cleannet-based learning framework. experimentation section based resnet-. experiments food-n. table lists results food-n using veriﬁcation labels classes. observe performance smooth soft weighting without need thresholding outperforms hard weighting fig. presents results image classiﬁcation using proposed cleannet-based method veriﬁcation labels classes held out. held-out classes information needed cleaning noisy class labels transferred classes cleannet. observed still accuracy gain classes held out. validates labeling effort table image classiﬁcation food-n terms top- accuracy veriﬁcation labels classes available. none denotes classiﬁer without method label noise. figure image classiﬁcation food-n terms top- accuracy line shows results veriﬁcation labels classes held cleannet. blue dashed line shows baseline without using cleannet. fig. shows examples cleannet. cosine similarity score image class’s reference shown example. transfer learning cleannet assign reasonable scores images classes seen training. deﬁnition veriﬁcation labels every image classes classes selected group classes share common secondlevel hypernym wordnet random selected classes random selected classes classes experiments clothingm. clothingm consider state result reported also used resnet-. used part data clothingm noisy correct class labels estimate confusion among classes modeled information loss function. since compare noisy class label correct class label image verify whether noisy class label correct lose label confusion information thus numbers directly comparable. however labeling correct classes like clothingm scalable number classes having labeling workers select large number classes time-consuming unlikely accurate. table lists results image classiﬁcation using veriﬁcation labels classes. using cleannet signiﬁcantly improves accuracy noisy training data. also follow ﬁne-tune best model trained noisy clean training set. proposed method achieves comparable state reported beneﬁts extra label confusion information. experiments webvision. opposed food-n clothingm ﬁne-grained tasks webvision experiments sheds light general image classiﬁcation large scale. mentioned sec. pseudoveriﬁcation labels model-based obtain table image classiﬁcation webvision terms top- top- accuracy models trained webvision training tested webvision ilsvrc validation sets various veriﬁcation conditions. images. property allows explore select classes adding veriﬁcation labels compare upper bound scenario noisy class labels veriﬁed without cost. deﬁne veriﬁcation labels veriﬁcation conditions listed table table shows experimental results using cleannet soft weighting observe verifying every image improves top- accuracy ilsvrc validation set. images veriﬁed semantic- random- give performance gain every-image ilsvrc validation respectively. note include veriﬁcation labels class experiments using cleannet. results conﬁrm labeling fraction classes effective transfer learning cleannet. work highlighted difﬁculties scalability effectiveness human supervision label noise detection classiﬁcation learning noisy data. introduced cleannet transfer learning approach reconcile issue transferring supervised information transferring correctness labels classes without explicit human supervision. empirically evaluate proposed methods general ﬁne-grained image classiﬁcation datasets. results show cleannet outperforms methods using human supervision large margin small fraction classes manually veriﬁed. also matches existing methods require extensive human supervision sufﬁcient classes manually veriﬁed. believe work creates novel paradigm efﬁciently utilizes human supervision better address label noise large-scale image classiﬁcation tasks. would like thank chen bosco chiu yandong po-sen huang thoughtful feedback discussions. thanks also kelly huang arun sacheti helping development food-n dataset.", "year": 2017}