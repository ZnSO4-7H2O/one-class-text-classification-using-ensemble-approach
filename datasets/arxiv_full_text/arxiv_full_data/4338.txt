{"title": "Computational Cost Reduction in Learned Transform Classifications", "tag": ["cs.CV", "stat.ML"], "abstract": "We present a theoretical analysis and empirical evaluations of a novel set of techniques for computational cost reduction of classifiers that are based on learned transform and soft-threshold. By modifying optimization procedures for dictionary and classifier training, as well as the resulting dictionary entries, our techniques allow to reduce the bit precision and to replace each floating-point multiplication by a single integer bit shift. We also show how the optimization algorithms in some dictionary training methods can be modified to penalize higher-energy dictionaries. We applied our techniques with the classifier Learning Algorithm for Soft-Thresholding, testing on the datasets used in its original paper. Our results indicate it is feasible to use solely sums and bit shifts of integers to classify at test time with a limited reduction of the classification accuracy. These low power operations are a valuable trade off in FPGA implementations as they increase the classification throughput while decrease both energy consumption and manufacturing cost.", "text": "present theoretical analysis empirical evaluations novel techniques computational cost reduction classiﬁers based learned transform soft-threshold. modifying optimization procedures dictionary classiﬁer training well resulting dictionary entries techniques allow reduce precision replace ﬂoating-point multiplication single integer shift. also show optimization algorithms dictionary training methods modiﬁed penalize higher-energy dictionaries. applied techniques classiﬁer learning algorithm soft-thresholding testing datasets used original paper. results indicate feasible solely sums shifts integers classify test time limited reduction classiﬁcation accuracy. power operations valuable trade fpga implementations increase classiﬁcation throughput decrease energy consumption manufacturing cost. image classiﬁcation feature extraction important step specially domains training large dimensional space requires higher processing memory resource. recent trend feature extraction image classiﬁcation construction sparse features features consist representation signal overcomplete dictionary. dictionary learned speciﬁc input dataset classiﬁcation sparse features achieve results comparable state-of-the-art classiﬁcation algorithms however approach drawback test time sparse coding input test sample computationally intense impracticable embedded applications scarce computational power resources. recent approach drawback learn sparsifying transform target image dataset therefore learned classiﬁer architecture seen feedforward neural network hidden layer bias. test time approach reduces sparse coding input image simple matrix-vector multiplication followed soft-threshold eﬃciently realized hardware inherent parallel nature. nevertheless matrix-vector multiplications require ﬂoating-point operations high cost hardware specially fpga increases fabrication cost demands higher energy operate. exploring properties derive classiﬁers propose techniques reduce computational cost test time divide four main groups decrease dynamic range dictionary ﬁrst penalizing norm entries training phase zeroing entries absolute values smaller trained threshold; test images integer format sampled analog-to-digital converters instead scaled normalized version thus replace costly ﬂoating-point operations integer operations cheaper implement hardware aﬀect classiﬁcation accuracy; quantize integer valued test images thus decrease number bits needed represent them; quantize transform dictionary classiﬁer approximating entries nearest power thus replace multiplication simple shift. refer techniques xquant. study case xquant recent classiﬁcation algorithm named learning algorithm soft-thresholding classiﬁer learns sparse representation signals hyperplane vector classiﬁer time. tests datasets used paper introduces last results indicate techniques reduce computational cost substantially degrading classiﬁcation accuracy. moreover particular dataset tested techniques substantially increased classiﬁcation accuracy. reduce computational cost test time classiﬁers based learned transform. valuable application embedded systems power consumption critical computational power restricted. furthermore xquant dismiss necessity using dsps intense matrix-vector operations fpgas architectures image classiﬁcation lowering overall manufacturing cost embedded systems. even though simulations test techniques performed image classiﬁcation using last proposed techniques suﬃciently general applied diﬀerent problems diﬀerent classiﬁcation algorithms matrix-vector multiplications extract features extreme learning machine deep neural networks literature reducing computational cost classiﬁers vast thus present signiﬁcant trends. also worth noting quantization strategies reduce resource usage ffnn classiﬁers implemented fpga used past century success. example quantization scheme proposed eliminate multiplications test time. training parameters feedforward neural network approximate parameters power retrain network letting bias values change freely real domain bias participate multiplications. reduces multiplication single operation shift. problem approach still relies ﬂoating-point operations costly applications limited energy and/or small computational power. diﬀerent quantization strategies presented allow ﬁxed-point values training test time. works lack power reducing beneﬁts quantization schemes approaches network parameters powers probably unknown feature authors. authors start experiment quantization schemes allow higher computational cost reduction. quantize network parameters reduce multiplications simple sign changes small decrease classiﬁcation accuracy. also follow lead. quantization scheme drastic eliminates multiplications shifts test time substantially reduce learning capacity neural network. authors propose post-processing scheme approximate trained parameters input images approach allows convolutions estimated xnor bit-counting operations. nevertheless oversimpliﬁcation comes price higher degradation classiﬁcation accuracy compared original classiﬁer. approach diﬀers aforementioned many points. first easily adapted learning algorithm rely speciﬁc thus used diﬀerent network architectures diﬀerent amounts neurons. also xquant also applied training classiﬁer. second drops ﬂoating-point operations favor integer ones. avoids costly normalization denormalization techniques required ﬂoating-point operations. third optional strategy reduce dynamic range parameters training consequently reduce number bits necessary store them. strategy penalizes parameter values causes increase dynamic range forcing closer average. fourth xquant hurt much classiﬁcation accuracy approximation performed previously mentioned works. section brieﬂy review synthetical analytical sparse representation signals along threshold operation used sparse coding approach also review last measures number nonzero coeﬃcients. therefore signal synthesized linear combination nonzero columns dictionary also called synthesis operator. solution requires testing possible sparse vectors combination entries taken time. np-hard problem approximate solution obtained using norm instead norm i.e. norm. solution computed solving problem minimizing norm coeﬃcients among decompositions convex solved eﬃciently. solution suﬃciently sparse equal solution sparse coding transform another sparsifying signal dictionary linear transform maps signal sparse representation. example signals formed superposition sinusoids dense representation time domain sparse representation frequency domain. type signal fourier transform sparse coding transform. quite simply sparse transform sparse coeﬃcient vector. general transform well structured ﬁxed base learned speciﬁcally target problem represented training dataset. learned dictionary overcomplete dictionary learned signal dataset square invertible dictionary even dictionary without restrictions number atoms last signal corrupted additive white gaussian noise transform result coeﬃcient vector sparse. common making sparse apply threshold operation entries right transform entries lower speciﬁed threshold zero. among existing threshold operators soft-threshold that addition threshold operation subtracts remaining values threshold shrinking toward zero last algorithm based learned transform followed softthreshold described section diﬀerently original soft-threshold last uses soft-threshold version also sets zero negative values i.e. threshold also called sparsity parameter. threshold operator seen relu activation function produced good results deep neural network architectures chose last study case simplicity learning process sparsifying dictionary classiﬁer hyperplane. training cases rn×m labels sparsifying dictionary rn×n contains atoms classiﬁer hyperplane estimated using supervised optimization section introduce techniques simplifying testtime computations classiﬁers based learned transforms soft-threshold. start describing section dataset images apply proposed techniques validation. next present section main theoretical ﬁndings supporting xquant ﬁnally presented section ﬁrst datasets contain patches extracted textures presented figure belong brodatz dataset built datasets using following methodology first separate image half left half create training patches right half create test patches. patches subsets image containing pixels. next patch stack columns normalize resulting vector norm equals ﬁrst task consisted discriminating test patches images bark woodgrain second task consisted discriminating patches images pigskin pressedcl. future reference named ﬁrst task bark_woodgrain second task pigskin_pressedcl. third binary dataset built using subset cifar- image dataset dataset contains classes tiny images images training test set. image color channels stored vector positions. dataset used subset formed images labeled deer horse. ﬁrst multiclass dataset mnist dataset contains images handwritten digits size distributed images training images test set. images zero-mean norm equals theorem relative distance real scalar powerized version deﬁned upper bounded proof. distance powerized version. distance maximum middle point closest power show classiﬁcation accuracy test inﬂuenced small variations introduced entries model using datasets bark_woodgrain pigskin_pressedcl described section trained initial model atoms created versions using following steps. model built multiplying entries initial model random value chosen uniform distribution open interval next evaluated models test set. better estimate classiﬁcation accuracy model performed steps times diﬀerent initial models trained using diﬀerent initial values. results shown figure indicate clear trade-oﬀ classiﬁcation accuracy entries displaced corresponding entries original models worth noting theorem guarantees upper bound relative distance real scalar powerized version. therefore reasonable hypothesize classiﬁcation accuracy using powerized pair power worse using shown figure support hypothesis performed another simulation datasets bark_woodgrain pigskin_pressedcl. dataset trained models diﬀerent random versions training power test evaluated respective powerized versions set. regarding bark_woodgrain dataset original model accuracy powerized model accuracy pigskin_pressedcl original model accuracy powerized model accuracy theorem xint training formed integer valued vectors normalized version norm model trained classiﬁcation accuracy signals xint normalized signals exactly sparsity parameter xint xint xint. proof. xint respectively vector test normalized version also model trained therefore extracted features xint xint soft-thresholded feature xint xint maxxint xint). finally classiﬁcation xint maxxint xint) hypothesized forcing sparse would decrease dynamic range substantial decrease classiﬁcation accuracy. support hypothesis performed another simulation datasets bark_woodgrain pigskin_pressedcl. dataset trained model created versions hard-thresholding entries using threshold values linearly spaced subsequently divided element hard-thresholded dictionary lowest value |dt| diﬀerent finally evaluated resulting models test set. better estimate classiﬁcation accuracy performed steps models trained diﬀerent random versions training computed average. shown figure ﬁrst threshold diﬀerent zero already reduces precision less half original slightly decreasing classiﬁcation accuracy. also third threshold diﬀerent shown figure almost maintains classiﬁcation accuracy reducing dynamic range less half original. figure impact classiﬁcation accuracy hard threshold used reduce precision dictionary values shown average classiﬁcation accuracy test evaluated models atoms trained diﬀerent training sets. original results marked circle. datasets described section empirical evidence quantizing integer valued images test xint certain level decrease dynamic range xint thus reduce number bits necessary compute dxint cost slight classiﬁcation accuracy decrease. also hypothesized original integer valued signals unnecessarily quantized quantization level could decreased substantially worsening classiﬁcation accuracy. support hypothesis performed another simulation datasets bark_woodgrain pigskin_pressedcl. dataset averaged results thousand runs consisting models trained using diﬀerent training sets evaluated diﬀerent quantized versions test set. images test xint quantized using levels ranging results shown figure worth noting ﬁgure images datasets precision reduced limited decrease classiﬁcation accuracy. figure impact classiﬁcation accuracy images test quantized certain level. original results marked circle. note reducing precision test images bits substantially worsens classiﬁcation accuracy. results average classiﬁcation results test evaluated models atoms trained diﬀerent training sets. datasets described section proposed techniques technique signals representation rather normalized version technique powerize technique decrease dynamic range test xint quantizing integer valued test images xint. technique decrease dynamic range entries penalizing -norm training followed hard-thresholding using trained threshold level. strategy decrease dynamic range dictionary involves addition penalty norm entries minimization objective function last described motivation penalizing fact avoid solutions containing high-valued entries would require representation using bits. also note penalizing would seem reasonable terms providing sparse dictionaries would still allow higher entries would anyway require bits proper quantization. proposed optimization problem hence becomes controls penalization. section show proposed technique including penalization general constrained optimization algorithms followed included penalization diﬀerence convex optimization algorithm used last training using modiﬁed objective function apply hard-threshold entries zero values closer zero. assumption small values little contribution ﬁnal feature value thus zero without aﬀecting much classiﬁcation accuracy. threshold value test best unique absolute values powerized using technique number unique absolute values substantially reduced using technique computational burden test possible values greatly reduced. show include term objective function penalizes potential dictionaries whose entries larger energy values opposed lower-energy dictionaries. favoring vectors lower energies obtain dictionaries span narrower ranges values. development consider inclusion penalization gradient descent methods many optimization problems based experimental where vector containing dictionary terms vector classiﬁer parameters; cost function based training set; null vector; function representing scalar equality constraints. methods also include inequality constraints. iterative methods commonly used solve constrained optimization problems start initial value iterated generate supposedly convergence sequence satisfying consider method computing requires evaluating gradient dual function associated objective function constraints speciﬁcally lagrangian example dual function thus local maximum minimum objective function point satisﬁes constraints. problems lagrangian functions given respectively ﬁrst objective regarding solving modiﬁed problem compute gradient terms gradient show problem solves modiﬁed order solve comparing deﬁning gradient function equations show modify estimated gradient method order penalize range dictionary entries thus force solution narrower range. note gradient respect dictionaries altered. section evaluate techniques aﬀect accuracy last datasets used this performed many simulations using datasets presented section compared classiﬁcation accuracy/error classiﬁcation precision minimum number bits necessary perform classiﬁcation. present section parameters chose generate models last analysis results obtained comes section tested datasets ﬁxed parameter z_threshold assume unique values powerized version dpower i.e. applying technique number unique values dpower substantially lower ones necessary computational burden test valid thresholds low. also ﬁxed quantization parameter quanta choice parameter values empirically based previous simulations. parameters last used used direct reader understanding parameters values used last. large number parameter combinations technique technique simulations generate many diﬀerent models classiﬁcation accuracy/error classiﬁcation precision. select best model best combination parameters z_threshold quanta relied classiﬁcation accuracy separate data set. also created parameter control trade-oﬀ classiﬁcation accuracy classiﬁcation precision. used following steps model selection first used training train models used remaining estimate best combination parameters z_threshold quanta. models trained combinations parameters z_threshold quanta. also classiﬁcation results training using models best_acc best training accuracy create subset contains models results best_acc]. create subset mbits results rbits lowest_num_bits lowest number bits necessary computation rbits ﬁnally choose model mbest result rbest rbits. worth noting traditional rule thumb using dataset train test safe estimating true classiﬁcation accuracy classiﬁcation accuracy whole dataset higher nevertheless solely reserving part training selection best parameters values estimation true classiﬁcation accuracy opted conservative proportion train models. advantage lowering chance missing underrepresented training sample. moreover last step model selection algorithm selects model produces sparsest signal representation leads models generalize better section original results ones classiﬁcation test using model built original last algorithm. conversely proposed results ones obtained classiﬁcation test using best model rbest built dataset. best model rbest selected using methodology presented section show results simulations binary tasks figure shown bottom figures techniques substantially decrease original classiﬁcation accuracy. time techniques considerably reduce number bits necessary perform multiplication shown figures note original results figures lower ones presented diﬀerently work used completely disjoint training test sets allow better estimation true classiﬁcation accuracy. table contains results simulations tasks mnist cifar-. original results obtained large datasets slightly higher classiﬁcation error ones reported hypothesize caused random nature last larger datasets optimized small portion data called mini-batch randomly sampled training set. moreover trained using training used negatively aﬀect generalization power dictionary classiﬁer. note techniques resulted increase classiﬁcation error mnist cifar- tasks. nevertheless techniques reduced number bits necessary classiﬁcation test time. again dynamic range reduction highly valuable applications fpga. figure comparison results using original last algorithm proposed techniques. regarding classiﬁcation test time ﬁgures show dataset trade-oﬀ necessary number bits classiﬁcation accuracy approach reduces necessary number bits almost half original formulation cost slight classiﬁcation accuracy decrease. datasets described section results presented section indicate feasibility using integer operations place ﬂoating-point ones shifts instead multiplications slight classiﬁcation accuracy decrease. substitutions reduce computational cost classiﬁcation test time fpgas important embedded applications power consumption critical. moreover techniques reduce almost half number bits necessary perform expensive operation classiﬁcation matrix-vector multiplication result application technique technique also worth noting techniques developed reduce computational cost classiﬁcation expected accuracy reduction within acceptable limits. nevertheless classiﬁcation accuracies bark_woodgrain dataset using techniques substantially outperforms accuracies using original model shown figure higher accuracies unexpected. regarding original models noted classiﬁcation accuracies training using dictionaries least atoms. models probably overﬁtted training making fail generalize data. powerize technique introduces perturbation entries hypothesize table comparison original proposed results regarding classiﬁcation error number bits necessary compute matrix-vector multiplication sparse representation. paper presented techniques reduction computations test time classiﬁers based learned transform softthreshold. basically techniques adjust threshold classiﬁer signals represented integer instead normalized version ﬂoating-point; reduce multiplications simple shifts approximating entries dictionary classiﬁer vector nearest power increase sparsity dictionary applying hardthresholding entries. simulations using datasets used original paper introduces last results indicate techniques substantially reduce computation load small cost classiﬁcation accuracy. moreover datasets tested substantial increase accuracy classiﬁer. proposed optimization techniques valuable applications power consumption critical. work partially supported scholarship coordination improvement higher education personnel thank dept. utep allowing access nsfsupported cluster used simulations described also gumataotao assistance thank fawzi source code last help details. also thank borries fruitful cooperation discussions.", "year": 2015}