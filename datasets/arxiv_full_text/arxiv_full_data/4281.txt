{"title": "Solving the G-problems in less than 500 iterations: Improved efficient  constrained optimization by surrogate modeling and adaptive parameter control", "tag": ["math.OC", "cs.NE", "stat.ML"], "abstract": "Constrained optimization of high-dimensional numerical problems plays an important role in many scientific and industrial applications. Function evaluations in many industrial applications are severely limited and no analytical information about objective function and constraint functions is available. For such expensive black-box optimization tasks, the constraint optimization algorithm COBRA was proposed, making use of RBF surrogate modeling for both the objective and the constraint functions. COBRA has shown remarkable success in solving reliably complex benchmark problems in less than 500 function evaluations. Unfortunately, COBRA requires careful adjustment of parameters in order to do so.  In this work we present a new self-adjusting algorithm SACOBRA, which is based on COBRA and capable to achieve high-quality results with very few function evaluations and no parameter tuning. It is shown with the help of performance profiles on a set of benchmark problems (G-problems, MOPTA08) that SACOBRA consistently outperforms any COBRA algorithm with fixed parameter setting. We analyze the importance of the several new elements in SACOBRA and find that each element of SACOBRA plays a role to boost up the overall optimization performance. We discuss the reasons behind and get in this way a better understanding of high-quality RBF surrogate modeling.", "text": "constrained optimization high-dimensional numerical problems plays important role many scientiﬁc industrial applications. function evaluations many industrial applications severely limited analytical information objective function constraint functions available. expensive black-box optimization tasks constraint optimization algorithm cobra proposed making surrogate modeling objective constraint functions. cobra shown remarkable success solving reliably complex benchmark problems less function evaluations. unfortunately cobra requires careful adjustment parameters order work present self-adjusting algorithm sacobra based cobra capable achieve high-quality results function evaluations parameter tuning. shown help performance proﬁles benchmark problems sacobra consistently outperforms cobra algorithm ﬁxed parameter setting. analyze importance several elements sacobra element sacobra plays role boost overall optimization performance. discuss reasons behind better understanding high-quality surrogate modeling. real-world optimization problems often subject constraints restricting feasible region smaller subset search space. goal constraint optimizers avoid infeasible solutions stay feasible region order converge optimum. however search constraint black-box optimization diﬃcult since usually a-priori knowledge feasible region ﬁtness landscape. problem even turns harder limited number function evaluations allowed search. however industry good solutions requested restricted time frames. example well-known benchmark mopta past diﬀerent strategies proposed handle constraints. repair methods guide infeasible solutions feasible area. penalty functions give negative bias objective function value constraints violated. many constraint handling methods available scientiﬁc literature often demand large number function evaluations little work devoted eﬃcient constraint optimization possible solution regard surrogate models objective constraint functions. real function might expensive evaluate evaluations surrogate functions usually cheap. example approach solver cobra proposed regis outperforms many algorithms large number benchmark functions. previous work studied reimplementation cobra enhanced repair mechanism reported strengths weaknesses. although good results obtained problem required tedious manual tuning many parameters cobra. paper follow unifying path present sacobra extension cobra starts settings problems adjusts necessary parameters internally. adaptive parameter control according terminology introduced eiben present extensive tests sacobra algorithms well-known popular benchmark literature so-called g-problem g-function benchmark introduced michalewicz schoenauer floudas pardalos provides constrained optimization problems wide range diﬀerent conditions. frequently used approach handle constraints incorporate static dynamic penalty terms order stay feasible region penalty functions helpful solving constrained problems main drawback often require additional parameters balancing ﬁtness penalty terms. tessema propose interesting adaptive penalty method need parameter tuning. feasible solution preference methods always prefer feasible solutions infeasible solutions. little information infeasible solutions risk getting stuck local optima. improves method introducing diversity mechanism. stochastic ranking similar successful improvement certain probability infeasible solution ranked behind according ﬁtness value among feasible solutions. stochastic ranking shown good results g-problems. however requires usually large number function evaluations thus well suited eﬃcient optimization. repair algorithms transform infeasible solutions feasible ones work chootinan chen shows good results g-problems requires large number function evaluations well. recent years multi-objective optimization techniques attracted increasing attention solving constrained optimization problems. general idea treat constraints objective functions optimized conjunction ﬁtness function. coello coello montes pareto dominance-based tournament selection genetic algorithm similarly venkatraman propose two-phase second phase formulated bi-objective optimization problem uses non-dominated ranking. jiao novel selection strategy based biobjective optimization improved reliability large number benchmark functions. emmerich kriging models approximating constraints multi-objective optimization scheme. ﬁeld model-assisted optimization algorithms constrained problems support vector machines used poloczek kramer make svms classiﬁer predicting feasibility solutions achieve slight improvements. powell proposes cobyla direct search method models objective constraints using linear approximation. recently regis developed cobra eﬃcient solver makes radial basis function interpolation model objective constraint functions outperforms algorithms terms required function evaluations large number benchmark functions. tenne armﬁeld present adaptive topology network tackle highly multimodal functions. consider unconstrained optimization. optimization algorithms need parameter respect speciﬁc optimization problem order show good performance. eiben introduced terminology parameter settings evolutionary algorithms distinguish parameter tuning parameter control parameter control subdivided predeﬁned control schemes control feedback optimization control parameters part evolvable chromosome several papers deal adaptive self-adaptive parameter control unconstrained constrained optimization suganthan propose self-adaptive diﬀerential evolution algorithm. brest propose another self-adaptive algorithm. handle constraints whereas zhang describe constraint-handling mechanism compare later results de-implementation deoptimr based works farmani wright propose self-adaptive ﬁtness formulation test g-problems. show comparable results stochastic ranking require many function evaluations well. coello coello tessema propose self-adaptive penalty approaches. survey self-adaptive penalty approaches given area eﬃcient constrained optimization optimization severely limited budget less function evaluations attracting attention recent years regis proposed besides already mentioned cobra approach trustregion evolutionary algorithm uses surrogates well exhibits high-quality results many g-functions less function evaluations. jiao propose self-adaptive selection method combine informative feasible infeasible solutions formulate multi-objective problem. algorithm solve g-functions really fast less evaluations others solved less evaluations remaining g-functions require evaluations solved. zahara show similar results g-functions investigate best knowledge currently approach solve g-problems less evaluations. tenne armﬁeld present interesting approach approximating rbfs optimize highly multimodal functions less evaluations results unconstrained functions competitive terms precision. sec. present constrained optimization problem methods surrogate modeling technique cobrar algorithm sacobra algorithm. sec. perform thorough experimental study analytical test functions real-world benchmark function mopta automotive domain. analyze help data proﬁles impact various sacobra elements overall performance. results discussed sec. paper always consider minimization problems. maximization problems transformed minimization without loss generality. problems equality constraints transformed inequalities ﬁrst cobra algorithm incorporates optimization auxiliary functions regression models search space. although numerous regression models available employ interpolating models since outperform models terms eﬃciency paper notation regis models require quality. here euclidean norm linear polynomial variables coeﬃcients cubic form alternative cubic rbfs gaussian rbfs e−r/ introduce additional width parameter reason necessary provide independent points initial design. usually case linearly independent points provided. matrix inversion done eﬃciently using singular value decomposition similar algorithms. linear polynomial serves purpose alleviate simple linear functions otherwise approximated superimposing many rbfs complicated way. polynomials higher order used well. consider option additional direct squares replaced models fast train even high dimensions. often provide good approximation accuracy even training points given. makes ideally suited surrogate models high-dimensional optimization problems large number constraints. figure inﬂuence large output ranges. left fitting original function cubic model. right fitting plog-transformed function rbf-model transforming back original modeled exactly linear rbf-model contains linear tail well would expect ﬁrst sight perfect surrogate model. fig. shows case large perfect weaker extremely extrapolation reason behavior large values lead computationally singular coeﬃcient matrices cubic coeﬃcients tend many orders magnitude larger coeﬃcients linear part. either linear equation solver stop error produces result large rmse demonstrated right plot fig. solver sets linear tail model zero order avoid numerical instabilities. model thus attempts approximate linear function superposition cubic rbfs. bound fail model extrapolate beyond green sample points. small values interval around minimum quickly grows large values original function model using green sample points shown fig. left plot oscillating behavior function. results large rmse reason model tries avoid large slopes. instead ﬁtted model similar spline function. therefore useful remedy apply logarithmic transform puts output smaller range results smaller slopes. regis shoemaker deﬁne function model perfectly plog-transformed function. afterward transform plog− back original space back-transform takes care large slopes. result much smaller approximation error rmse original space right-hand side fig. shows. apply plog-transform functions steep slopes surrogateassisted optimization sacobra. functions constant slope experiments shown nonlinear nature plog approximation plog less accurate. cobra algorithm developed regis solve constrained optimization tasks severely limited budgets. main idea cobra costly optimization surrogate models reimplemented algorithm small modiﬁcations. give short review algorithm cobra-r following. cobra-r starts generating initial population ninit points build ﬁrst surrogate models minimum number points ninit usually larger choice ninit gives better results. surrogate functions true functions approximated surrogate models iteration cobra-r algorithm solves standard constrained optimizer constrained surrogate subproblem usually latin hypercube sampling regis uses matlab’s fmincon interior-point optimizer available environment. cobra-r mostly powell’s cobyla constrained optimizer like isres implemented r-package https//cran.r-project.org/web/packages/sacobra well. details ﬁnish description main loop optimizer returns solution xn+. feasible repair algorithm described previous work tries replace feasible solution vicinity. case solution evaluated true functions compared best feasible solution found replaces better. solution added population solution allowed previous ones. idea avoid frequent passed user external parameter vector iteration cobra selects next element adds constraints xj|| constraints. measures distance proposed inﬁll solution previous inﬁll points. distance requirement cycle clever idea since small elements lead exploitation search space larger elements lead exploration. last element reached selection starts ﬁrst element again. size single components arbitrarily chosen. cobra aims ﬁnding feasible solutions extensive search surrogate functions. however models probably exact especially initial phase search factor used handle wrong predictions constraint surrogates. starting diameter search space point said holds. tighten constraints adding factor adapted search. \u0001-adaptation done counting feasible infeasible inﬁll points cinf last iterations. number counters reaches threshold feasible infeasible solutions tinf respectively divide double decreased solutions allowed move closer real constraint boundaries since last inﬁll points feasible. otherwise feasible inﬁll point found tinf iterations increased order keep points away real constraint boundary. although cobra cobra-r sharing many common principles several diﬀerences lead diﬀerent results identical problems. main diﬀerences cobra cobra-r listed follows cobra cobra-r achieve good results g-problems mopta studies regis previous work shown. however necessary papers carefully adjust parameters algorithm problem phase uses objective function rewards constraint fulﬁllment. implemented cobrar well found unnecessary problems. paper cobra-r always skips phase directly proceeds phase even feasible solution found initialization phase. sometimes even modify problem applying plog-transform objective function linear transformations constraints rescaling input space. real black-box optimization adjustments would probably require knowledge problem several executions optimization code otherwise. main contribution current paper present sacobra enhanced cobra algorithm needs manual adjustment problem hand. instead sacobra extracts execution information speciﬁc problem takes internally appropriate measures adjust parameters transform functions. present fig. ﬂowchart sacobra elements compared cobra-r highlighted gray boxes. complete sacobra algorithm presented detail algorithm describe following elements order appearance ization phase. helps better exploration search space dimensions treated same. importantly avoids numerical instabilities caused high values shown sec. adrc done initialization phase. experimental analysis showed large values harmful problems steep objective function large move input space yields large change output space. spoil model sense similar sec. lead consequence large approximation errors. therefore developed automatic adjustment selects appropriate according information extracted initialization phase. function analyzeplogeffect algorithm selects ’small’ estimated objective normally cobra starts optimization current best point. optimization starts random point search space certain constant probability rate feasibile individuals population drops replace larger probability especially beneﬁcial search gets stuck local optima gets stuck region feasible point found. analysis sec. shown ﬁtness function steep slopes poses problem approximation. problems modeling plog instead transforming result back plog− boosts optimization performance signiﬁcantly. hand tests shown plog-transform harmful problems. therefore careful decision whether plog made. idea online adjustment algorithm following given population build rbfs plog take point xnew added calculate ratio approximation errors xnew every iteration collect ratios many papers optimization strength optimization technique measured comparing ﬁnal solution achieved diﬀerent algorithms approach provides information quality results neglects speed convergence important measure expensive optimization problems. comparing convergence curve time also common benchmarking approaches although convergence curve provides good information speed convergence ﬁnal quality optimization result used compare performance several algorithms problem. often interesting compare overall capability technique solving group problems. data performance proﬁles developed mor´e wild good approach analyze performance optimization algorithm whole test suite used frequently optimization literature problems solvers number iterations solver requires solve problem problem said solved feasible objective value found larger best objective determined solver experiments below. smaller values desirable performance ratio rps. using best solver solve problem solver cannot solve problem performance ratio inﬁnity. performance changing equality constraints inequality constraints range ﬁtness values ratio largest smallest constraint range number linear inequalities number nonlinear inequalities number nonlinear equalities number constraints active optimum. prefer data proﬁles performance proﬁles performance factor intuitive meaning data proﬁles allow problem dimension budget function evaluations value interpreted fraction problems solver solve within budget evaluate sacobra using well-studied test suite g-problems described diversity g-problem characteristics makes challenging benchmark optimization techniques. table show features problems. features measured monte carlo sampling points search space g-problem. tions. corresponds month computation time high-performance computer real automotive problem since real problem requires time-consuming crash-test simulations. cobra-r optimization framework allows user choose several initialization approaches latin hypercube sampling biased optimized initialization always possible algorithms possible feasible starting point provided. cobra initialization always done randomly means latin hypercube sampling functions without feasible starting point. case mopta feasible point known. optimized initialization approach initial optimization started feasible point hooke jeeves pattern search algorithm initial provides ninit points vicinity feasible point. serves initial design mopta. table shows parameter settings used cobra-r sacobra. g-problems optimized exactly initial parameter settings. contrast that cobra results regis previous work obtained manually activating plog g-problems manually adjusting constraint factors parameters. figures show sacobra convergence plots g-problems. clearly visible problems except solved majority runs deﬁne solved target error comparison true optimum. cases worst error meet target cases does. cases indicated squares clear improvement regis’ cobra results main result shown fig.. shows data proﬁles diﬀerent sacobra variants comparison data proﬁle cobra-r. cobra-r ﬁxed parameter set. note passing ﬁxed parameter settings cobra-r tested perhaps better runs inevitably worse runs similar slightly worse data proﬁle cobra-r would emerge. sacobra increases signiﬁcantly success rate g-problem benchmark suite. comparing convergence curves g-functions realized applying logarithmic transform strictly harmful three g-functions signiﬁcantly beneﬁcial problems negligible eﬀect problems. therefore careful selection done. although demonstrated sec. steep functions better modeled logarithmic transformation trivial deﬁne correct threshold classify steep functions. also direct relation steepness function eﬀect logarithmic transformation optimization. deﬁned sec. algorithm function analyzeplogeffect measure called order quantify online whether models without plog transformation better worse. test experiments whether q-value good job. fig. shows qvalue g-problems. g-problems ranked horizontal axis according impact logarithmic transformation ﬁtness function optimization outcome. means applying plog-transformation worst eﬀect modeling ﬁtness best eﬀect measure impact optimization following g-problem perform runs plog inactive plog active. calculate median ﬁnal optimization error cases take ratio figure wilcoxon rank test paired sided signiﬁcance level shown p-value hypothesis speciﬁc g-problem full sacobra method ﬁnal iteration better another solver signiﬁcant improvements marked cells dark blue color. optimization methods sacobra\\rescale sacobra\\rs sacobra\\adrc sacobra\\aff sacobra\\acf cobra cobra note usually available normal optimization mode. close zero close much larger eﬀect plog optimization performance harmful neutral beneﬁcial striking feature fig. q-ranks similar r-ranks. means beneﬁcial harmful eﬀect plog strongly correlated approximation error. experiments shown problems optimization performance weakly inﬂuenced logarithmic transformation ﬁtness function. therefore step function adjustfitnessfunction algorithm threshold work. choose threshold largest margin g-problems plog beneﬁcial according table problems largest ﬁtness function range thus strengthening hypothesis sec. functions plog-transform used good rbf-models. g-problems plog harmful looking notable diﬀerence namely switch order seen imperfection measure although rank weaker worst-case behavior runs never produce feasible solution plog active. analytical form objective function problems three functions quadratic type mixed quadratic terms. functions ﬁtted perfectly polynomial tail sacobra plog inactive. plog become nonlinear complicated approximation radial basis functions needed. results larger approximation error. table shows comparison diﬀerent state-of-the-art optimizers g-problem suite. isres best optimizers terms solution quality require highest number function evaluations well. sacobra g-problems solution quality slightly worse. time sacobra requires small fraction function evaluations roughly compared isres compared table diﬀerent optimizers median best feasible results average number function evaluations. results independent runs diﬀerent random number seeds. numbers boldface results column sacobra cobyla calculation results column cobra isres taken papers cited. cases reported solution better true optimum possibly slight infeasibility. explicitly stated case isres equality constraint transformed approximate cobra comes close sacobra terms eﬃciency noted present results g-problems furthermore many g-problems manual transformations original ﬁtness function constraint functions done prior optimization. sacobra starts without transformations proposes instead self-adjusting mechanisms suitable transformations cobyla often produces slightly infeasible solutions numbers brackets. infeasible runs occur median taken remaining feasible runs principle optimistic favor cobyla. fig. shows comparison sacobra cobra-r well-known constraint optimization solvers available namely cobyla. right plot fig. shows achieves good results many function evaluations accordance table left plot fig. shows really competitive tight bounds budget set. r-package deoptimr available https//cran.r-project.org/web/packages/deoptimr r-package nloptr available https//cran.r-project.org/web/packages/nloptr fig. shows good results sacobra high-dimensional mopta problem well. problem said solved data proﬁle fig. away best value obtained runs algorithms. table shows results iterations regis’ recent trust-region based approach algorithms. improve already good mean best feasible results obtained cobra-r resp. sacobra algorithm capable self-adjusting parameters wide-ranging problems constraint optimization. analyzed diﬀerent elements sacobra importance eﬃcient optimization g-problem benchmark. turned important elements rescaling automatic ﬁtness function adjustment thus steep regions problems cannot solved. attributed large approximation errors well. diagnosed quality surrogate models relationship correct choice parameter controls step size iteration. desirable choose smaller step sizes functions steep slopes. automatic adjustment step sacobra identify steep functions function evaluations decide whether large small one. g-problem suite constraint functions vary number type range. experiments showed handling constraints challenging especially constraint functions widely diﬀerent ranges. reason considered automatic adjustment approach normalize constraints using information gained constraints evaluation initial population. sacobra algorithm also beneﬁts using random start mechanism avoid getting stuck local optimum ﬁtness surrogate. surrogate models like great thing eﬃcient optimization probably solve constrained optimization problems less iterations. current border surrogate modeling highly multimodal functions. function large number local minima. functions usually large ﬁrst higher order derivatives. surrogate model interpolates isolated points function tends overshoot parts function. best knowledge highly multimodal problems cannot solved surrogate models least higher dimensions high accuracy. also true sacobra. usually model good approximation region local minima approximation rest search space. research highly multimodal function approximation required solve problem. current approach cobra handle inequality constraints. reason equality constraints work together uncertainty mechanism sec. reformulation equality constraint inequality work approach regis replaced equality operator inequality operator appropriate direction. noted however approach contradicts true black-box handling constraints although viable problems viable complicated objective functions minima sides equality constraints. forthcoming paper address problem separately. summarize discussion stating good understanding capabilities limitations surrogate models often undertaken surrogate literature aware important prerequisite eﬃcient eﬀective constrained optimization. analysis errors problems occurring initially g-problems cobra algorithm given better understanding models development enhancing elements sacobra. studying widely varying problems observed certain challenges modeling steep relatively functions rbf. result large approximation errors. sacobra tackles problem making conditional plog-transform objective function. proposed online mechanism sacobra decide automatically plog not. numerical issues train models also occur case large input space. simple solution problem rescale input space. although many optimizers recommend rescale input work shown reason behind importance evidence. therefore answer ﬁrst research question positively numerical instabilities occur modeling possible avoid proper function transformations search space adjustments. sacobra beneﬁts extension elements introduced sec. element boosts optimization performance subset problems without harming optimization process ones. result overall optimization performance whole problems improved compared cobra tested problems solved eﬃciently sacobra answer sacobra capable cope many diverse challenges constraint optimization. main contribution paper propose sacobra ﬁrst surrogate-assisted constrained optimizer solves eﬃciently g-problem benchmark requires parameter tuning manual function transformations. finally provide result sacobra requires less function evaluations solve g-problems similar accuracy state-of-theart algorithms. algorithms often need times function evaluations.", "year": 2015}