{"title": "Generating Natural Language Inference Chains", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "The ability to reason with natural language is a fundamental prerequisite for many NLP tasks such as information extraction, machine translation and question answering. To quantify this ability, systems are commonly tested whether they can recognize textual entailment, i.e., whether one sentence can be inferred from another one. However, in most NLP applications only single source sentences instead of sentence pairs are available. Hence, we propose a new task that measures how well a model can generate an entailed sentence from a source sentence. We take entailment-pairs of the Stanford Natural Language Inference corpus and train an LSTM with attention. On a manually annotated test set we found that 82% of generated sentences are correct, an improvement of 10.3% over an LSTM baseline. A qualitative analysis shows that this model is not only capable of shortening input sentences, but also inferring new statements via paraphrasing and phrase entailment. We then apply this model recursively to input-output pairs, thereby generating natural language inference chains that can be used to automatically construct an entailment graph from source sentences. Finally, by swapping source and target sentences we can also train a model that given an input sentence invents additional information to generate a new sentence.", "text": "ability reason natural language fundamental prerequisite many tasks information extraction machine translation question answering. quantify ability systems commonly tested whether recognize textual entailment i.e. whether sentence inferred another one. however applications single source sentences instead sentence pairs available. hence propose task measures well model generate entailed sentence source sentence. take entailment-pairs stanford natural language inference corpus train lstm attention. manually annotated test found generated sentences correct improvement lstm baseline. qualitative analysis shows model capable shortening input sentences also inferring statements paraphrasing phrase entailment. apply model recursively input-output pairs thereby generating natural language inference chains used automatically construct entailment graph source sentences. finally swapping source target sentences also train model given input sentence invents additional information generate sentence. improving performance wide range natural language processing tasks. recognizing textual entailment task primarily designed determine whether natural language sentences independent contradictory entailment relationship second sentence inferred ﬁrst although systems perform well could potentially used improve question answering information extraction text summarization machine translation downstream tasks sentence-pairs actually available. usually single source sentence present models need come hypotheses commonsense knowledge inferences. release large stanford natural language inference corpus allowed end-to-end diﬀerentiable neural networks outperform feature-based classiﬁers task work step investigate well recurrent neural networks produce true hypotheses given source sentence. furthermore qualitatively demonstrate training input-output pairs recursively generating entailed sentence generate natural language inference chains note every inference step interpretable mapping natural language sentence another one. create entailment generation dataset simply ﬁlter stanford natural language inference corpus sentence-pairs entailment class. results training sentence pairs development pairs test pairs. instead classiﬁcation task dataset sequence transduction task. sequence-to-sequence recurrent neural networks successfully employed many sequence transduction tasks machine translation constituency parsing sentence summarization question answering consist recurrent neural networks encoder maps input sequence words dense vector representation decoder conditioned vector representation generates output sequence. speciﬁcally long short-term memory rnns encoding decoding. furthermore experiment wordby-word attention allows decoder search encoder outputs circumvent lstm’s memory bottleneck. greedy decoding test time. success lstms attention sequence transduction tasks makes natural choice baseline entailment generation leave investigation advanced models future work. stochastic gradient descent minibatch size adam optimizer ﬁrst momentum coeﬃcient second momentum coeﬃcient word embeddings initialized pre-trained wordvec vectors out-of-vocabulary words randomly initialized sampling values uni] optimized training. furthermore clip gradients using norm stop training epochs. present results various tasks given premise generate sentence inferred premise construct inference chains recursively generating sentences given sentence create premise would entail sentence i.e. make descriptive sentence adding speciﬁc information. train lstm without attention training set. training take best model terms bleu score development calculate bleu score test set. surprise found using attention yields marginally higher bleu score suspect fact generating entailed sentences larger space valid target sequences makes bleu problematic penalizes correct solutions. hence manually annotated random test sentences decided whether generated sentence indeed inferred source sentence. found sentences generated lstm attention substantially accurate generated lstm baseline gain insights model’s capabilities turn thorough qualitative figure shows examples generated sentences development set. syntactic simpliﬁcation input sentence seems common approach. model removes certain parts premise adjectives resulting abstract sentence figure demonstrates system recognize number subjects sentence includes information generated sentence. however observe ’counting’ behavior four subjects indicating system memorized frequency patterns training set. limitations reappearing limitations proposed model related dealing words diﬀerent meaning similar wordvec embeddings well ambiguous words. instance ’bar’ figure refers pole vault place out-of-corpus examples snli corpus might reﬂect variety sentences encountered downstream tasks. figure present generated sentences randomly selected examples out-of-domain textual resources. demonstrate model generalizes well out-of-domain sentences making potentially useful component improving systems question answering information extraction sentence summarization etc. next test well model generate inference chains repeatedly passing generated output sentences inputs model. stop sentence already generated chain. figure shows works well despite model trained sentence-pairs. furthermore generating inference chains sentences development construct entailment graph. graph found sentences shared semantics eventually mapped sentence captures shared meaning. square. people celebrating. people happy. people smil ing. people smiling. people smiling. couple smiling bride groom smiles. wedding party looks swapping source target sequences training train model given sentence invents additional information generate sentence believe might prove useful increase language variety investigated ability sequence-tosequence models generate entailed sentences source sentence. trained attentive lstm entailment-pairs snli corpus. found works well generalizes beyond in-domain sentences. hence could become useful component improving performance systems. able generate natural language inference chains recursively generating sentences previously inferred ones. allowed construct entailment graph sentences snli development corpus. graph shared meaning related sentences represented ﬁrst natural language sentence connects sentences. every inference step interpretable maps natural language sentence another one. future work want integrate presented model larger architectures improve performance downstream tasks information extraction question answering. furthermore plan model data augmentation train expressive neural networks tasks little annotated data available. another interesting research direction investigate methods increasing diversity generated sentences. thank guillaume bouchard suggesting reversed generation task dirk weissenborn isabelle augenstein matko bosnjak comments drafts paper. work supported microsoft research scholarship programme allen distinguished investigator award marie curie career integration award.", "year": 2016}