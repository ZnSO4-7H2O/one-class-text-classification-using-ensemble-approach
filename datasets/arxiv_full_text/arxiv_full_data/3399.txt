{"title": "Max-Margin Deep Generative Models for (Semi-)Supervised Learning", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Deep generative models (DGMs) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability. However, it is relatively insufficient to empower the discriminative ability of DGMs on making accurate predictions. This paper presents max-margin deep generative models (mmDGMs) and a class-conditional variant (mmDCGMs), which explore the strongly discriminative principle of max-margin learning to improve the predictive performance of DGMs in both supervised and semi-supervised learning, while retaining the generative capability. In semi-supervised learning, we use the predictions of a max-margin classifier as the missing labels instead of performing full posterior inference for efficiency; we also introduce additional max-margin and label-balance regularization terms of unlabeled data for effectiveness. We develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objectives in different settings. Empirical results on various datasets demonstrate that: (1) max-margin learning can significantly improve the prediction performance of DGMs and meanwhile retain the generative ability; (2) in supervised learning, mmDGMs are competitive to the best fully discriminative networks when employing convolutional neural networks as the generative and recognition models; and (3) in semi-supervised learning, mmDCGMs can perform efficient inference and achieve state-of-the-art classification results on several benchmarks.", "text": "abstract—deep generative models effective learning multilayered representations complex data performing inference input data exploring generative ability. however relatively insufﬁcient empower discriminative ability dgms making accurate predictions. paper presents max-margin deep generative models class-conditional variant explore strongly discriminative principle max-margin learning improve predictive performance dgms supervised semi-supervised learning retaining generative capability. semi-supervised learning predictions max-margin classiﬁer missing labels instead performing full posterior inference efﬁciency; also introduce additional max-margin label-balance regularization terms unlabeled data effectiveness. develop efﬁcient doubly stochastic subgradient algorithm piecewise linear objectives different settings. empirical results various datasets demonstrate that max-margin learning signiﬁcantly improve prediction performance dgms meanwhile retain generative ability; supervised learning mmdgms competitive best fully discriminative networks employing convolutional neural networks generative recognition models; semi-supervised learning mmdcgms perform efﬁcient inference achieve state-of-the-art classiﬁcation results several benchmarks. discriminative models many examples univariate-output support vector machines multivariate-output max-margin markov networks however ever-increasing size complex data makes hard construct fully discriminative model single layer adjustable weights facts that manually constructed features well capture underlying high-order statistics; fully discriminative approach cannot reconstruct input data noise missing values present. address ﬁrst challenge previous work considered incorporating latent variables max-margin model including partially observed maximum entropy discrimination markov networks structured latent svms max-margin min-entropy models work primarily focused shallow structure latent variables. improve ﬂexibility learning svms deep latent structure presented however methods address second challenge requires generative model describe inputs. recent work learning max-margin generative models includes max-margin topic models maxmargin harmoniums nonparametric bayesian latent svms infer dimension latent features data. however methods consider shal zhang department computer science technology; tnlist lab; state laboratory intelligent technology systems; center bio-inspired computing research tsinghua university beijing china. email chongxuanligmail.com; dcszjtsinghua.edu.cn; dcszbtsinghua.edu.cn. much work done learning generative models deep structure nonlinear hidden variables including deep belief networks autoregressive models stochastic variations autoencoders generative adversarial nets models inference challenging problem motivated much recent progress stochastic variational inference algorithms however primary focus deep generative models unsupervised learning goals learning latent representations generating input samples. though latent representations used downstream classiﬁer make predictions often beneﬁcial learn joint model considers input response variables. recent work semi-supervised deep generative models proves effectiveness dgms modeling density unlabeled data beneﬁt prediction task however remains open whether discriminative max-margin learning suitable task. paper revisit max-margin principle present max-margin deep generative models learn multilayered representations good classiﬁcation input inference. mmdgms conjoin ﬂexibility dgms describing input data strong discriminative ability max-margin learning making accurate predictions. given fully labeled data formulate mmdgms solving variational inference problem regularized max-margin posterior constraints bias model learn representations good prediction. deﬁne maxmargin posterior constraints linear functional target variational distribution latent presentations. optimize joint learning problems develop doubly stochastic subgradient descent algorithm generalizes pagesos algorithm consider nontrivial latent variables. variational distribution build recognition model capture nonlinearity similar reduce dependency fully labeled data propose class-conditional variant mmdgms deal partially labeled data semisupervised learning amount unlabeled data typically much larger labeled ones. speciﬁcally mmdcgms employ deep max-margin classiﬁer infer missing labels unlabeled data class-conditional deep generative model capture joint distribution data labels latent variables. unlike mmdcgms separate pathways inferring labels latent variables completely generate images given speciﬁc class. instead inferring full posterior labels computationally expensive large datasets prediction classiﬁer point estimation label speed-up training procedure. design additional max-margin label-balance regularization terms unlabeled data enhance classiﬁer signiﬁcantly boost classiﬁcation performance. consider types networks used mmdgms mmdcgms—multiple layer perceptrons convolutional neural networks case following apply unpooling convolution rectiﬁcation sequentially form highly non-trivial deep generative network generate images latent variables learned automatically recognition model using standard cnn. present detailed network structures experiment section. empirical results widely used mnist svhn small norb datasets demonstrate that mmdgms signiﬁcantly improve prediction performance supervised learning competitive best feedforward neural networks retaining capability generating input samples completing missing values; mmdcgms achieve stateof-the-art classiﬁcation results efﬁcient inference disentangle styles classes based images semisupervised learning. achieve state-of-the-art results several benchmarks semi-supervised learning competitive prediction accuracy fully discriminative cnns supervised learning. deep generative models good discovering underlying structures input data training model parameters inference posterior distribution highly nontrivial tasks. recently signiﬁcant progress made enriching representative power variational inference markov chain monte carlo methods posterior inference variational autoencoders neural adaptive mcmc vaes build recognition model infer posterior latent variables parameters trained optimize variational bound data likelihood. neural adaptive mcmc employs similar recognition model proposal distribution importance sampling estimate gradient log-posterior hence perform approximate bayesian inference dgms. learn parameters besides commonly used estimator adopted vaes recent work proposed various objectives. example generative adversarial nets construct discriminator distinguish generated samples training data parameters trained based minimax two-player game framework. generative moment matching networks generate samples directed deep generative model trained match orders statistics training data samples model. recent work extends ideas learn conditional gmmns much broader applicability. extensive work focusing realistic image generation unsupervised setting. example draw employs recurrent neural networks generative model recognition model introduces attention mechanism generate sequences real digits step step. mem-vae leverages external memory attention mechanism encode retrieve detailed information lost recognition model enhance dgms. lap-gan proposes cascade gans generate high quality natural images laplacian pyramid framework dcgan adopts fractionally strided convolution networks generator learn spatial upsampling reﬁnes generated samples. recent advances made extending dgms deal partially observed data. example conditional vaes treat labels conditions dgms describe input data; perform posterior inference labels given unlabeled data generate speciﬁc class images. adgm introduces auxiliary latent variables dgms make variational distribution expressive well semisupervised learning. cat-gan generalizes gans categorical discriminative network objective function includes mutual information input data prediction discriminative network. proposes feature mapping virtual batch normalization techniques improve performance gans intractable except handful special cases must resort approximation methods. common assumption variational distribution parametric form optimize variational bound w.r.t variational parameters dgms another challenge arises variational bound often intractable compute analytically. address challenge early work bounds intractable parts tractable ones introducing variational parameters however technique increases bound optimized log-likelihood potentially resulting poorer estimates. much recent progress made hybrid monte carlo variational methods approximates intractable expectations gradients parameters unbiased monte carlo estimates. furthermore handle large-scale datasets stochastic optimization variational objective used suitable learning rate annealing scheme. important notice variance reduction part methods order fast stable convergence. work directed dgms focusing generative capability inferring observations ﬁlling missing values relatively insufﬁcient work done investigating predictive power except recent advances semisupervised learning. below present max-margin deep generative models explore discriminative maxmargin principle improve predictive ability latent representations retaining generative capability. max-margin deep generative models ﬁrst consider fully supervised setting training data pair input features groundtruth label without loss generality consider multiclass classiﬁcation illustrated fig. max-margin deep generative model consists components deep generative model describe input features; max-margin classiﬁer consider supervision. generative model theory adopt deﬁnes joint distribution max-margin classiﬁer instead ﬁtting input features conventional deﬁne linear classiﬁer latent representations whose learning regularized supervision signal shall see. speciﬁcally latent representation given deﬁne latent discriminant function k-dimensional vector concatenates subvectors others zero corresponding weight vector. consider case random vector following prior distribution goal infer posterior distribution typically approximated variational distribution computational tractability. notice posterior different vanilla dgm. expect supervision information bias learned representations powerful predicting labels testing. account semi-supervised learning image generation. ladder network achieves excellent classiﬁcation results semi-supervised learning employing lateral connections autoencoders reduce competition invariant feature extraction reconstruction object details. work complimentary progress sense investigate criterion dgms supervised semisupervised settings. preliminary results fully supervised mmdgms published semi-supervised extensions novel. max-margin deep generative models present max-margin deep generative models supervised learning class-conditional variants semi-supervised learning. methods present efﬁcient algorithms. basics deep generative models start general setting i.i.d. data {xn}n deep generative model assumes generated vector latent variables follows distribution. joint probability follows prior latent variables likelihood model generating observations. notation simplicity deﬁne depending structure various dgms developed deep belief networks deep sigmoid networks deep latent gaussian models deep autoregressive models paper focus directed dgms easily sampled ancestral sampler. however cases learning dgms challenging intractability posterior inference. stateof-the-art methods resort stochastic variational methods maximum likelihood estimation framework argmaxθ speciﬁcally variational distribution approximates true posterior variational upper bound sample negative log-likelihood important notice make restricting assumption variational distribution bound tight simply setting equivalent solving variational problem minθq however since true posterior convexity function easy verify hinge loss upper bound training error classiﬁer ∆ln. furthermore hinge loss convex functional variational distribution linearity expectation operator. properties render hinge loss good surrogate optimize over. previous work explored idea learn discriminative topic models restriction shallow structure hidden variables. work presents signiﬁcant extension learn deep generative models pose challenges learning inference. doubly stochastic subgradient algorithm variational formulation problem naturally suggests develop variational algorithm address intractability true posterior. present algorithm solve problem method doubly stochastic generalization pegasos algorithm classic svms fully observed input features extension dealing highly nontrivial structure latent variables. first make structured mean-ﬁeld assumption qqφ. assumption discriminant function eq∆fn] eq]e qφ)]. moreover solve optimal solution analytical form. fact calculus variations show given parts solution grange multipliers details). prior normal normal posterior eqφ]. therefore even though make parametric form assumption results show optimal posterior distribution gaussian. since expectation optimization problem prediction directly solve mean parameter instead further case verify ||λ|| kl||p) equivalent objective function terms written fig. graphical models mmdgms labels given missing. graphical models mmdcgms labels given missing. solid line dash line represent generative model recognition model respectively. line stands max-margin classiﬁer. compared mmdgms mmdcgms disentangle label information latent variables separate pathways inferring labels latent variables. note different conditional puts class labels upstream generates latent representations well input data conditioning classiﬁer downstream model sense supervision signal determined conditioning latent representations. learning problem want jointly learn parameters infer posterior distribution based equivalent variational formulation deﬁne joint learning problem solving difference feature vectors; loss function measures cost predict true label nonnegative regularization parameter balancing components. objective variational bound deﬁned kl||p) margin constraints classiﬁer ignore constraints estimates subgradients stochastic optimization methods adam update parameters outlined alg. overall algorithm doubly stochastic generalization pegasos deal highly nontrivial latent variables. remaining question deﬁne appropriate variational distribution obtain robust estimate subgradients well objective. types methods developed unsupervised dgms namely variance reduction auto-encoding variational bayes though methods used models focus approach. continuous variables certain mild conditions reparameterize variational distribution using simple variables speciﬁcally draw samples simple distribution transformation sample distribution refer readers details. experiments consider special gaussian case assume variational distribution multivariate gaussian diagonal covariance matrix whose mean variance functions input data. deﬁnes recognition model. then reparameterization trick follows ﬁrst draw standard normal variables transformation sample. simplicity assume mean variance function only. however worth emphasize although recognition model unsupervised parameters learned supervised manner subgradient depends hinge loss. details experimental settings presented sec. conditional variants semi-supervised learning collecting labeled data often costly timeconsuming semi-supervised learning important setting easy-to-get unlabeled data leveraged improve quality. present extension mmdgms semi-supervised learning scenario. unlabeled dataset {xn}nu size typically much larger goal explore intrinsic structures underlying unlabeled data help learn classiﬁer. learning objective mmdgms learning needs scan full dataset compute subgradients often expensive deal largescale datasets. effective technique stochastic subgradient descent iteration randomly draw mini-batch training data variational updates small mini-batch. formally given mini batch size unbiased estimate objective second stochasticity arises stochastic estimate per-sample variational bound subgradient whose intractability calls another monte carlo estimator. samples variational distribution explicitly conditions. then estimates per-sample variational bound per-sample hinge-loss respectively ∆fn. note unbiased estimate biased estimate nevertheless still show upper bound estimate expectation. furthermore biasedness affect estimate gradient. fact using equality ∇φqφ qφ∇φ construct unbiased monte carlo estimate estimates consider general case variational bound intractable. cases compute kl-divergence term analytically e.g. prior variational distribution gaussian. cases need estimate rest intractable part sampling often reduces variance similarly could expectation features directly hinge loss second part learning objective hinge loss labeled data. speciﬁcally though labeled data contribute training classiﬁer implicitly objective function eqn. shown adding predictive loss labeled data speed-up convergence achieve better results here adopt similar idea introducing hinge loss discriminative regularization labeled data loss third part learning objective loss unlabeled data. speciﬁcally typically much larger semi-supervised learning desirable unlabeled data regularize behaviour classiﬁer explicitly. propose max-margin loss unlabeled data follows ∆fˆyn ∆lˆyn function indicates whether equals prediction not. namely treat prediction putative label apply hinge loss function unlabeled data. function called loss shape binary classiﬁcation example intuitively hinge loss enforces predictor make prediction correctly conﬁdently large margin labeled data loss requires predictor make decision conﬁdently unlabeled data. loss originally proposed svms assumes decision boundary tends low-density areas feature space. shallow models correctness assumption heavily depends true data distribution ﬁxed unknown. however constraint much relaxed building upon latent feature space learned deep model described method. practice predictive performance mmdcgms improved substantially adding regularization shown sec. label-balance regularization last part learning objective regularization term balance possible label predictions unlabeled data. speciﬁcally practical problem semi-supervised learning imbalance predictions classiﬁer classify unlabeled points class. address problem introduce balance constraint multiclass semi-supervised learning consists parts—a data likelihood classiﬁcation loss naive approach considering unlabeled data simply ignore loss term class label missing. however ignorance leads weak coupling likelihood model classiﬁer. below present conditional variant mmdgms namely max-margin deep conditional generative models strongly couple classiﬁer data likelihood. similar mmdgms mmdcgm consists components deep max-margin classiﬁer infer labels given data class-conditional deep generative model describe joint distribution data labels latent variables. fig. compares graphical models mmdgm mmdcgm. below present learning objective mmdcgm formally consists several components. notation simplicity omit parameters following formulae confusion arises. generative loss ﬁrst part learning objective generative loss describe observed data. labeled data whose visible mmdcgm maximizes joint likelihood pair lower bounded lower-bounds adopted previous method however issue method computational inefﬁciency dealing large unlabeled data large number classes. need compute lower-bounds joint likelihood possible unlabeled data point. make computationally efﬁcient propose prediction classiﬁer point estimation approximate full posterior speed-up inference procedure denote classiﬁer restricted speciﬁc form proper distribution labels unnormalized trained max-margin principle. indeed outputs classiﬁer real values transformed linear operations denoting signed distance data hyperplanes deﬁned weights. consequently entropy term zero lower-bound turns predict using classiﬁer plug predictions unlabeled data groudtruth labeled data indicator functions label-balance regularization; take gradient respect parameters generative model recognition model classﬁer optimize ﬁnal objective approximate gradients intractable expectations using techniques described sec. update parameters. experiments present experimental results supervised semi-supervised learning settings. results several benchmark datasets demonstrate mmdgms mmdcgms highly competitive classiﬁcation retaining generative ability comparison various strong competitors. experiment settings though mmdgms mmdcgms applicable dgms deﬁne joint distribution respectively concentrate variational auto-encoder conditional experiments. consider types recognition models multiple layer perceptrons convolutional neural networks denote mmdgm mlps mmva. perform classiﬁcation using unsupervised ﬁrst learn feature representations build linear classiﬁer features using pegasos stochastic subgradient algorithm baseline denoted va+pegasos. corresponding models cnns denoted conv-mmva conv-va+pegasos respectively. denote mmdcgm cnns conv-mmcva. implement experiments based theano datasets preprocessing evaluate models widely adopted mnist svhn small norb datasets. mnist consists handwritten digits different classes training samples validating samples testing samples size svhn large dataset consisting color images size task recognize center digits natural scene images. follow work split dataset training data validating data testing data. small norb dataset consisits gray images distributed across general classes animal human airplane truck car. training testing norb contain samples different lighting conditions azimuths. down-sample images assumes distribution predictions unlabeled data groundtruth labels labeled set. however sides eqn. summations indicator functions nondifferentiable respect therefore cannot optimize based gradient methods satisfy constraint directly. here relax constraint e˜q] simplify sumf mation notation. given certain class left hand side selects unlabeled data whose predictions equal according indicator functions adds corresponding activations together. right hand side computes normalized activations indicator functions class labeled data. note smaller deﬁnitions prediction indicator function ∆lˆyn gradients relaxed version still well-deﬁned indicator functions. however assuming predictions given sides eqn. summations without indicator functions differentiable respect experiments indeed ignore dependency indicator functions approximate total gradients gradients cumulative activations. approximation work constraint eqn. sides turn scalars given gradient respect zero almost everywhere cannot used optimize parameters. fact relaxed constraint balances predictions unlabeled data according groundtruth implicitly assumption cumulative activation proportional number predictions intuitively cumulative activation selected unlabeled data certain class larger labeled data probably predictor classiﬁes unlabeled data incorrectly. consequently updated reduce activations number predictions class decrease smaller moreover hard constraints unlikely satisfy practice relax using regularization penalty common l-norm optimize overall learning objective still doubly stochastic algorithm described sec. compute unbiased subgradient estimations parameters perform updates. speciﬁcally given mini-batch data consisting labeled data fair comparison supervised learning svhn perform local contrast normalization experiment conv-mmva following distribution given gaussian. cases normalize data factor choose bernoulli distribution data. supervised learning mmdgms recognition network classiﬁer share layers computation. mean variance latent variable transformed last layer recognition model afﬁne transformation. noticed could expectation also activation layer recognition model features. theoretical difference hinge loss regularization gradient back-propagate previous layers. experiments mean nonlinearity typically much lower dimension activation last layer recognition model hence often leads worse performance. different features mmva convmmva explained below. adam optimize parameters models. although adaptive gradient-based optimization method decay global learning rate factor sufﬁcient number epochs ensure stable convergence. mmva follow settings compare generative discriminative capacity mmva. recognition generative models employ twolayer hidden units layer dimension latent variables choose default mmva. concatenate activations layers features used supervised tasks. illustrate network architecture mmva appendix model obtain competitive classiﬁcation results. generative model unconvnets symmetric structure recognition model reconstruct input images approximately. speciﬁcally top-down generative model structure bottom-up recognition model replacing max-pooling unpooling operation applies unpooling convolution rectiﬁcation order. typically convolutional layers generative model recognition model kernel size either depending data. total number parameters comparable previous work split training sets same. simplicity involve mlpconv layers contrast normalization layers recognition model exclusive model. mnist svhn default. activations last deterministic layer features. illustrate network architecture conv-mmva gaussian hidden variables bernoulli visible variables fig. semi-supervised learning mmdcgm separates classiﬁer recognition model latent variables completely allows simply combine state-of-the-art classiﬁer deep generative models together without competition. consider convolutional neural networks adopt advanced techniques including global average pooling batch normalization boost performance conv-mmcva. architecture max-margin classiﬁer refers discriminator generative model similar conv-mmva concatenates feature maps additional label maps one-hot encoding format layer similar conv-mmva depth convolutional networks according conditional optimize search grid {... ...} terms validation classiﬁcation error shallow mnist given labels. best values conv-mmcva across datasets. hyper-parameters including anneal strategy batch size chosen according validation generative loss. hyperparameters ﬁxed model times different random splits labeled unlabeled data report mean standard deviation error rates. ﬁrst present results supervised learning setting. speciﬁcally evaluate predictive generative performance mmva conv-mmva mnist svhn datasets various tasks including classiﬁcation sample generation missing data imputation. predictive performance test mmva conv-mmva mnist dataset. case three rows table compare va+pegasos va+class-condtionva mmva va+class-condtionva refers best fully supervised model model outperforms baselines signiﬁcantly. t-sne algorithm embed features learned mmva plane demonstrates stronger discriminative ability mmva cases table shows effect classiﬁcation error rate variational lower bound. typically gets lager conv-mmva learns discriminative features leads worse estimation data likelihood. however small supervision enough lead predictive features. nevertheless quite good trade-off classiﬁcation performance generative performance. setting classiﬁcation performance conv-mmva model comparable state-of-the-art fully discriminative networks comparable architectures number parameters shown last four rows table focus conv-mmva svhn datset challenging. table shows predictive performance svhn. harder problem observe larger improvement conv-mmva compared convva+pegasos suggesting dgms beneﬁt maxmargin learning image classiﬁcation. also compare conv-mmva state-of-the-art results. best knowledge competitive generative models classify digits svhn dataset full labels. generative performance investigate generative capability mmva conv-mmva generating samples. fig. fig. illustrate images randomly sampled mmva models mnist svhn respectively output expectation value pixel smooth visualization. fig. demonstrates beneﬁts jointly training dgms max-margin classiﬁers. though conv-va gives tighter lower bound data likelihood reconstructs data elaborately fails learn pattern digits complex scenario could generate meaningful images. scenario hinge loss regularization recognition model useful generating main objects classiﬁed images. fig. generation mnist. images randomly generated mmva respectively; images randomly generated conv-va conv-mmva respectively. mmdgms retain similar ability baselines generate images. fig. imputation results mmva noising conditions column shows true data; column shows perturbed data; remaining columns show imputations iterations. missing data imputation classiﬁcation test mmva conv-mmva task missing data imputation. mnist consider types missing values rand-drop pixel missing randomly pre-ﬁxed probability; rect rectangle located center image missing. given perturbed images uniformly initialize missing values iteratively following steps using recognition model sample hidden variables; predicting missing values generate images; using reﬁned images input next round. svhn procedure mnist initialize missing values guassian intuitively generative models cnns could powerful learning patterns high-level structures generative models mlps learn reconstruct pixels detail. conforms results shown table conv-va conv-mmva outperform mmva missing rectangle mmva outperform conv-va conv-mmva random missing values. compared baselines mmdgms also make accurate completion large patches missing. models infer missing values iterations. considering types missing values mmva could infer unknown values reﬁne images several iterations even large ratio missing pixels. visualization results mnist svhn presented appendix present classiﬁcation results missing values mnist table makes prediction incomplete data directly. conv-va conv-mmva infer missing data iterations ﬁrst make prediction reﬁned data. scenario conv-mmva outperforms conv-va demonstrates advantages mmdgms strong discriminative generative capabilities. predictive performance compare conv-mmcva large body previous methods mnist dataset different settings table method competitive stateof-the-art results given labels. number labels increases max-margin principle signiﬁcantly boosts performance conv-mmcva relative models including ladder network indeed given labels conv-mmcva beats existing methods setting also comparable best supervised results dgms. supervised learning results convmmcva conﬁrm leveraging max-margin principle dgms achieve discriminative ability fig. effect size labeled data mnist. labeled data smaller size subset larger size curve reduce variance. generally error rates decrease number labels increase peaks caused poor quality added labeled data. nevertheless labels sufﬁcient achieve error rate comparable supervised learning results dgms. state-of-the-art cnns comparable architectures. analyze effect number labels fig. convmmcva four curves share settings different random seeds split data initialize networks. table shows classiﬁcation results challenging svhn norb datasets. following previous methods labels datasets. methods outperform previous state-of-the-art substantially. ensemble--gans refers ensemble improved-gans -layer classiﬁers employ single model shallower -layer classiﬁer. note easy improve model using advanced networks e.g. resnet without competition separated architectures. paper focus comparable architectures fairness. analyze effect regularization terms investigate possible reasons outstanding performance. omit loss regularization convmmcva suffers overﬁtting achieves error rates mnist dataset given labels. underlying reason approximate full posterior inference greedy point estimation. prediction classiﬁer wrong generative model tends interpret unlabeled data incorrect label instead enforcing classiﬁer true label previous conditional however loss provides fig. class-conditional generation mnist svhn datasets. present labeled training data sorted class mnist svhn datasets respectively. show samples corresponding datasets shares class column shares latent variables conditions points view. nevertheless conv-mmcva still separate physical semantics general categories beyond digits. best knowledge competitive generative models generate norb data class-conditionally given partially labeled data. conclusions paper propose max-margin deep generative models class-conditional variants conjoin predictive power max-margin principle generative ability deep generative models. develop doubly stochastic subgradient algorithm learn parameters jointly consider types recognition models mlps cnns respectively. evaluate mmdgms mmdcgms supervised learning semi-supervised learning settings respectively. given partially labeled data approximate full posterior labels delta distribution efﬁciency propose additional max-margin label balance losses unlabeled data effectiveness. present extensive results demonstrate methods signiﬁcantly improve prediction performance deep generative models retaining strong generative ability generating input samples well completing missing values. fact employing cnns mmdgms mmdcgms achieve error rates several datasets including mnist svhn norb competitive best fully discriminative networks supervised learning improve previous state-of-the-art semi-supervised results signiﬁcantly. acknowledgments work supported national basic research program china national china youth top-notch talent support program tsinghua tnlist data initiative. effective classiﬁer achieve sufﬁciently good classiﬁcation result ﬁne-tuned according generative loss. fact trained optimize maxmargin losses labeled unlabeled data classiﬁer without error rates mnist given labels. results demonstrate effectiveness proposed max-margin loss unlabeled data. also reduce error rate setting using label-balance regularization. besides excellent performance conv-mmcva provides potential apply class-conditional dgms large scale datasets many categories efﬁcient inference. demonstrate conv-mmcva ability disentangle classes styles given small amount labels mnist svhn norb datasets shown fig. fig. images generated conditioning label style vector mnist svhn datasets conv-mmcva able generate highquality images capture intensities scales colors images. note previous generation svhn semi-supervised learning setting either unconditioned based preprocessed data samples little blurry norb dataset contains elaborate images toys different lighting bastien lamblin pascanu bergstra goodfellow bergeron bouchard warde-farley bengio. theano features speed improvements. deep learning unsupervised feature learning workshop advances neural information processing systems workshop bengio alain vincent. generalized denoising auto-encoders generative models. advances neural information processing systems bornschein bengio. reweighted wakeinternational conference learning representasleep. tions chen xing. large-margin predictive latent subspace learning multi-view data analysis. ieee trans. pami cortes vapnik. support-vector networks. denton chintala arthur fergus. deep generative image models using laplacian pyramid adversarial networks. advances neural information processing systems grosse ranganath convolutional deep belief networks scalable unsupervised learning hierarchical representations. international conference machine learning chen yan. network network. international conference learning representations little rubin. statistical analysis missing data. journal machine learning research radford metz chintala. unsupervised representation learning deep convolutional geninternational conference erative adversarial networks. learning representations sermanet chintala lecun. convolutional neural networks applied house numbers digit classiﬁcation. international conference pattern recognition springenberg. unsupervised semi-supervised learning categorical generative adversarial networks. international conference learning representations tang. deep learning using linear support vector machines. challenges representation learning workshop international conference machine learning taskar guestrin koller. max-margin advances neural information tsochantaridis hofmann joachims altun. support vector machine learning interdepeninternational dent structured output spaces. conference machine learning vincent larochelle lajoie bengio manzagol. stacked denoising autoencoders learning useful representations deep network local journal machine learning redenoising criterion. search chongxuan received institute interdiscriplinary information sciences tsinghua university china. currently working toward degree department computer science technology tsinghua university. research interests primarily statistical machine learning especially deep generative models various learning tasks including unsupervised supervised reinforcement learning. received degrees department computer science technology tsinghua university china currently associate professor. project scientist postdoctoral fellow machine learning department carnegie mellon university. research interests primarily developing statistical machine learning methods understand scientiﬁc engineering data arising various ﬁelds. member ieee. zhang graduated department automatic control tsinghua university beijing china currently professor department computer science technology tsinghua university fellow chinese academy sciences beijing china. main interests artiﬁcial intelligence pattern recognition neural networks intelligent control. published papers four monographs ﬁelds. journal latex class files vol. august appendix network architecture mmva illustrate network structure mmva gaussian hidden variables bernoulli visible variables fig. appendix manifold visualization t-sne embedding results features learned mmva plane shown fig. fig. respectively using data points randomly sampled mnist dataset. compared va’s embedding mmva separates images different categories better especially confusable digits digit results show mmva beneﬁts max-margin principle learns discriminative representations digits appendix visualization imputation results imputation results conv-va conv-mmva mnist svhn shown fig. fig. respectively. mnist conv-mmva makes fewer mistakes reﬁnes images better accords results reported main text. svhn cases conv-mmva could complete images missing values much harder dataset. remaining cases conv-mmva fails potentially changeful digit patterns less color contrast compared handwriting digits dataset. nevertheless conv-mmva achieves comparable results conv-va inferring missing data.", "year": 2016}