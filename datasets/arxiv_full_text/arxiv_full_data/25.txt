{"title": "The loss surface of deep and wide neural networks", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "stat.ML"], "abstract": "While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.", "text": "theoretical side several interesting class networks show train globally optimal efﬁciently. however turns approaches either practical require e.g. knowledge data generating measure modify neural network structure objective class networks simpler analyze deep linear networks shown every local minimum global minimum highly non-trivial result optimization problem non-convex deep linear networks interesting practice efﬁciently learns linear function. order characterize loss surface general networks interesting approach taken randomizing nonlinear part feedforward network relu activation function making additional simplifying assumptions relate certain spin glass model analyze. model objective local minima close global optimum number local minima decreases quickly distance global optimum. interesting result based number unrealistic assumptions recently shown assumptions dropped basically recovers result linear case model still unrealistic. paper analyze case overspeciﬁed neural networks network larger required achieve minimum training error. overspeciﬁcation recently analyzed conditions possible generate initialization principle possible reach global optimum descent methods. however deal optimization problem behind deep neural networks highly non-convex frequently observed practice training deep networks seems possible without getting stuck suboptimal points. argued case local minima close being globally optimal. show true fact almost local minima globally optimal fully connected network squared loss analytic activation function given number hidden units layer network larger number training points network structure layer pyramidal. application deep learning recent years lead dramatic boost performance many areas computer vision speech recognition natural language processing. despite huge empirical success theoretical understanding deep learning still limited. paper address non-convex optimization problem training feedforward neural network. problem turns difﬁcult exponentially many distinct local minima shown training network single neuron variety activation functions turns np-hard practice local search techniques like stochastic gradient descent variants used training deep neural networks. surprisingly observed training stateof-the-art feedforward neural networks sparse connectivity like convolutional neural networks fully connected ones rameters network. paper denotes integers integers activation function assumed least continuously differentiable paper assume functions applied componentwise. mappings input space feature space layer deﬁned assumed continuously differentiable loss function prototype loss consider paper squared loss standard loss functions neural network literature. assume throughout paper minimum attained. idea backpropagation core theoretical analysis. lemma shows well-known relations feed-forward neural networks used throughout paper. derivative loss w.r.t. value unit layer evaluated single training sample denoted ∂gkj arrange vectors training samples single matrix deﬁned hidden layer networks make strong assumptions data linear independence cluster structure. paper overspeciﬁcation means exists wide layer number hidden units larger number training points. case show large class local minima globally optimal. fact argue almost every critical point globally optimal. results generalize previous work analyzed similar setting hidden layer networks networks arbitrary depth. moreover extends results shown certain deep feedforward neural networks almost local minima globally optimal whenever training data linearly independent. clear assumption number hidden units quite strong several recent neural network structures contain quite wide hidden layer relative number training points e.g. training samples network hidden layer hidden units million training samples layer hidden units. refer examples number hidden units layer order number training samples. conjecture kind wide networks still holds almost local minima globally optimal. reason expect linear separability training data wide layer. provide supporting evidence conjecture showing basically every critical point training data linearly separable wide layer globally optimal. moreover want emphasize results hold neural networks used practice. simplifying assumptions previous work. mainly concerned multi-class problems results also apply multivariate regression problems. number training samples denote input resp. output matrix training data input dimension number classes. consider fully-connected feedforward networks layers indexed correspond input layer hidden layer output layer. network structure determined weight matrices rd×n rnk−×nk .×rnl−×m; number hidden units layer bias vectors denote space possible panote lemma apply non-differentiable activation functions like relu function σrelu max{ however known approximate activation function arbitrarily well smooth function e.g. satisﬁes limα→∞ σrelu ﬁrst discuss prior work present main result together extensive discussion. improved readability postpone proof main result next section contains several intermediate results independent interest. work seen generalization work shown one-hidden layer network every local minimum global minimum work considered also multi-layer networks. convenience reader ﬁrst restate theorem using previously introduced notation. critical points continuously differentiable function points gradient vanishes note necessary condition local minimum. result already general multi-layer networks condition implies main caveat. already noted quite hard understand practical meaning requires prior knowledge every critical point. note almost impossible depends weights network. particular case training samples linearly independent i.e. rank) condition holds automatically. case discussed following theorem consider general class loss activation functions. function real analytic corresponding multivariate taylor series converges open subset results section proven following assumptions loss/activation function training data. conditions always necessary prove intermediate results presented below decided provide proof strong assumptions better readability. instance results also hold strictly monotonically decreasing activation functions. note conditions restrictive many standard activation functions satisfy them. +e−t moreover proof note well known real-analytic exponential function analytic values composition real-analytic function realanalytic real-analytic. similarly since theorem deﬁned assumptions hold. training samples linearly independent rank) every critical point weight matrices full column rank rank global minimum. proof proof based induction. critical point holds thus assumption data matrix full column rank implies using induction assume lemma assumption implies argument assumption holds global minimum thus individual entry must represent global minimum combined implies critical point must global minimum prove section similar guarantee main theorem implicitly transporting condition higher layer. similar guarantee proven single hidden layer network whereas consider general multi-layer networks. main ingredient proof main result observation following lemma. theorem implies weight matrices potential saddle points suboptimal local minima need rank particular layer. note however rank weight matrices measure zero. moment cannot prove suboptimal rank local minima cannot exist. however seems implausible suboptimal rank local minima exist every neighborhood points contains full rank matrices increase expressiveness network. thus possible degree freedom reduce loss contradicts deﬁnition local minimum. thus conjecture local minima indeed globally optimal. ﬁrst condition lemma seen generalization requirement linearly independent training inputs theorem condition linear independence feature vectors hidden layer. lemma suggests want make statements global optimality critical points sufﬁcient know critical points fulﬁll conditions. third condition trivially satisﬁed critical point requirement full column rank weight matrices similar theorem however ﬁrst fulﬁlled since rank) dependent weights also architecture. main difﬁculty proof following main theorem prove ﬁrst condition holds rather simple requirement subset critical points. deﬁnition twice-continuously differentiable function deﬁned open domain hessian w.r.t. subset variables denoted r|s|×|s|. write rn×n denote full hessian matrix. note non-degenerate critical point might nondegenerate subset variables vice versa non-degenerate subset variables necessarily imply non-degeneracy whole set. instance first note full column rank condition l=k+ theorem implicitly requires means network needs pyramidal structure layer interesting note modern neural network architectures pyramidal structure layer typically ﬁrst hidden layer thus restrictive requirement. indeed even argue theorem gives implicit justiﬁcation hints fact networks easy train layer sufﬁciently wide. note theorem require fully nondegenerate critical points non-degeneracy needed subset variables includes layer consequence theorem directly stronger result non-degenerate local minima. proof hessian non-degenerate local minimum positive deﬁnite every principal submatrix positive deﬁnite matrix positive deﬁnite particular subset variables l=k+. application theorem yields result. discuss implications results. first note theorem slightly weaker theorem requires also non-degeneracy variables including layer moreover similar theorem exclude possibility suboptimal local minima rank layers above layer hand makes also strong statements. fact even degenerate saddle points/local maxima excluded long non-degenerate respect subset parameters upper layers include layer rank condition holds. thus given weight matrices upper layers full column rank much room left degenerate saddle points/local maxima. moreover one-hidden-layer network every non-degenerate critical point respect output layer parameters global minimum full rank condition active one-hidden layer networks. concerning non-degeneracy condition main theorem might likely encounter degenerate points smooth function. answered application sard’s/morse theorem note theorem would still hold would draw uniformly random thus almost every linear perturbation function lead fact critical points non-degenerate. thus result indicates exact degenerate points might rare. note however practice hessian critical points close singular might affect training neural networks negatively argued theorem main theorem exclude possibility suboptimal degenerate local minima suboptimal local minima rank. however conjecture second case cannot happen every neighborhood local minima contains full rank matrices increase expressiveness network additional ﬂexibility used reduce loss contradicts deﬁnition local minimum. mentioned introduction condition looks ﬁrst sight strong. however mentioned introduction practice often networks used hidden layer rather wide order condition theorem sufﬁcient necessary expect continuity reasons loss surface networks condition approximately true still rather well behaved sense still local minima indeed globally optimal suboptimal ones away globally optimal ones. better readability ﬁrst prove main theorem special case whole upper layers i.e. show extend proof general case proof strategy follows. ﬁrst show output layer real analytic functions network parameters. prove exists parameters rank) using properties real analytic functions conclude parameters rank) measure zero. non-degeneracy condition apply implicit-function theorem conclude even rank) true critical point still neighborhood exists point conditions lemma true loss minimal. continuity implies loss must also minimal critical point. proof linear function real analytic real analytic functions closed addition multiplication composition e.g. prop. prop. assume activation function real analytic output functions neural network real analytic functions parameters compositions real analytic functions. concept real analytic functions important proofs functions never constant parameter space positive measure unless constant everywhere. captured following lemma. next lemma show exist network parameters rank) holds note possible fact uses non-linear activation functions. deep linear networks possible achieve maximum rank layers sufﬁciently wide. this considers fk−wk linear network rank min{rank rank} since addition rank-one term increase rank matrix one. using induction gets rank rank every would imply result. note assumption hyperplane measure zero thus condition fails corresponds union hyperplanes measure zero. thus always exists vector condition satisﬁed thus exists rows distinct. assume distinct rows argument need construct given distinct rows show construct full rank. since sufﬁcient make ﬁrst columns together all-ones vector become linearly indeparticular pendent. matrices containing outputs ﬁrst hidden units last hidden units layer respectively. rnk−×nk ×nk− every deﬁnition holds mentioned above need show exists rank) follow immediately rank) pick rnk− satisfying potentially reordering w.l.o.g. discussion vector always exists since complementary condeterminant polynomial entries thus continuous exists s.t. every holds rank) rank]) moreover since chosen ﬁrst columns always choose weights ﬁrst hidden units layer rank) case activation function fulﬁlls ρeρt consider directly determinant matrix particular pick consider family matrices deﬁned note all-ones vector zero. determinant polynomial entries matrix thus analytic function entries composition analytic functions analytic conclude determinant analytic function network parameters ﬁrst layers. lemma exists least network parameters ﬁrst layers determinant functions identically zero thus lemma network parameters determinant zero measure zero. submatrices need rank order rank) follows network parameters rank) measure zero. goes zero every permutation goes inﬁnity. since whole summation goes zero determinant would non-zero desired. that ﬁrst note permutation least component case thus sufﬁciently large holds thus shown |eπj always upper-bounded exponential function resp. afﬁne function resp. constant observations imply exist positive constants holds every construction done particular strictly monotonically decreasing sequence limr→∞ corollary previous argument choose point full rank moreover limr→∞ follows limr→∞ continuous function holds limr→∞ limr→∞ thus limr→∞ continuous function holds difference hold ﬁxed. contained arguments thus involved perturbation analysis. full rank property weight matrices layers preserved needed obtain global minimum. corresponding vectors critical point gradient mapping w.r.t. parameters upper layers since gradient vanishes critical point holds jacobian w.r.t. principal submatrix hessian w.r.t. rt×t. critv ical point assumed non-degenerate respect nonv holds singular. moreover continuously differentiable since assumption therefore satisfy conditions implicit function theorem thus exists open ball continuously differentiable function assumption rank weight matrices upper layers full column rank. note weight matrix part l=k+ full column rank. particular this combined continuity implies potentially smaller holds classiﬁcation tasks loss function encourages higher values true class lower values wrong classes. example loss function satisﬁes assumption given seen sufﬁcient condition leads rather simple structure critical points sense local minima full rank layers hessian non-degenerate subset upper layers includes layer automatically globally optimal. suggests suboptimal locally optimal points either completely absent relatively rare. motivated networks certain wide layer used practice shows condition completely unrealistic. hand want discuss section could potentially relaxed. following result provide intuition case strong main result makes statements large class critical points. main idea condition data linearly separable layer modern neural networks expressive enough represent function interesting discussion this expect layer training data becomes linearly separable. prove critical point learned network outputs layer linearly separable global minimum training error. section slightly different loss function previous section. reason standard least squares loss necessarily small data linearly separable. denote classes. consider objective function penalize deviation label encoding true class resp. wrong classes. assume minimum attained note bounded zero non-negative loss functions. results section made following assumptions activation loss function. back main proof idea prove given critical point. assume sake contradiction diag) σ′)). since diagonal matrix contains positive entries diagonal change sign pattern thus holds with every note second statement theorem considered special case ﬁrst statement. case training inputs linearly separable second statement theorem recovers similar result onehidden layer networks. even though assumptions theorem theorem different terms class activation loss functions results related. fact well known vectors linearly independent linearly separable e.g. thus theorem seen direct generalization theorem caveat also main difference theorem theorem makes statements critical points problem become separable layer whereas condition theorem however still think result practical relevance expect sufﬁciently large network stochastic gradient descent lead network structure data becomes separable particular layer. happens associated critical points globally optimal. interesting question research show directly architecture condition network outputs become linearly separable layer local minimum thus every local minimum global minimum. results show loss surface becomes wellbehaved wide layer network. implicitly wide layer often present convolutional neural networks used computer vision. thus interesting future research question result generalized neural networks sparse connectivity. think results presented paper signiﬁcant addition recent understanding deep learning works efﬁciently. particular since paper directly working neural networks used practice without modiﬁcations simpliﬁcations.", "year": 2017}