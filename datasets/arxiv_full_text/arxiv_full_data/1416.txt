{"title": "Matching neural paths: transfer from recognition to correspondence  search", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Many machine learning tasks require finding per-part correspondences between objects. In this work we focus on low-level correspondences - a highly ambiguous matching problem. We propose to use a hierarchical semantic representation of the objects, coming from a convolutional neural network, to solve this ambiguity. Training it for low-level correspondence prediction directly might not be an option in some domains where the ground-truth correspondences are hard to obtain. We show how transfer from recognition can be used to avoid such training. Our idea is to mark parts as \"matching\" if their features are close to each other at all the levels of convolutional feature hierarchy (neural paths). Although the overall number of such paths is exponential in the number of layers, we propose a polynomial algorithm for aggregating all of them in a single backward pass. The empirical validation is done on the task of stereo correspondence and demonstrates that we achieve competitive results among the methods which do not use labeled target domain data.", "text": "many machine learning tasks require ﬁnding per-part correspondences objects. work focus low-level correspondences highly ambiguous matching problem. propose hierarchical semantic representation objects coming convolutional neural network solve ambiguity. training low-level correspondence prediction directly might option domains ground-truth correspondences hard obtain. show transfer recognition used avoid training. idea mark parts matching features close levels convolutional feature hierarchy although overall number paths exponential number layers propose polynomial algorithm aggregating single backward pass. empirical validation done task stereo correspondence demonstrates achieve competitive results among methods labeled target domain data. finding per-part correspondences objects long-standing problem machine learning. level correspondences established pixels images millisecond timestamps sound signals. typically highly ambiguous match level pixel timestamp contain enough information discriminative many false positives follow. hierarchical semantic representation could help solve ambiguity could choose low-level match also matches higher levels. example contains wheel contains bolt. want check bolt matches bolt another view check wheel match well. possible hierarchical semantic representation could computed convolutional neural network. features network composed hierarchical manner lower-level features used compute higher-level features applying convolutions max-poolings non-linear activation functions them. nevertheless training convolutional neural network correspondence prediction directly might option domains ground-truth correspondences hard expensive obtain. raises question scalability approaches motivates search methods require training correspondence data. address training data problem could transfer knowledge source domain labels present target domain labels labeled data present. common form transfer classiﬁcation tasks. promise two-fold. first classiﬁcation labels easiest obtain natural task humans. allows create huge recognition datasets like imagenet second features mid-levels shown transfer well variety tasks although huge progress transfer classiﬁcation detection segmentation semantic reasoning tasks like single-image depth prediction transfer correspondence search limited propose general solution unsupervised transfer recognition correspondence search lowest level approach match paths activations coming convolutional neural network applied objects matched. precisely establish matching lowest level require features match different levels convolutional feature hierarchy. different-level features form paths. path would consist neural activations reachable lowest-level feature highest-level feature network topology since every lowest-level feature belongs many paths voting based them. although overall number paths exponential number layers thus infeasible compute naively prove voting possible polynomial time single backward pass network. algorithm based dynamic programming similar backward pass gradient computation neural network. empirical validation done task stereo correspondence datasets kitti kitti quantitatively show method competitive among methods require labeled target domain data. also qualitatively show even dramatic changes low-level structure handled reasonably method robustness recognition hierarchy apply different style transfers corresponding images kitti still successfully correspondences. method generally applicable cases input data multi-dimensional grid topology layout. assume input objects b-dimensional grids convolutional neural networks grids. per-layer activations networks contained -dimensional grids rb+. input data activations indexed -dimensional vector column index index etc. channel index search correspondences grids thus goal estimate shifts elements grid. choice shift task-dependent. example sound shifts considered. images could shifts shifts work dealing convolutional neural network architectures consisting convolutions max-poolings non-linear activation functions omit softmax transfer). assume every convolutional layer followed non-linear activation function throughout paper specify functions explicitly. computational graph architectures directed acyclic graph a|a| nodes corresponding neuron activations e|e| arcs corresponding computational dependencies represented tuple input arcs allowed previous layer next one. notation node layer position origins arcs entering layer position reference object; out) endpoints arcs exiting layer position reference object. {maxpool conv} mathematic operator corresponds forward computation layer correspondence path matching consider objects reference searched want correspondences. applying them graphs activations. goal establish correspondences input-data layers every cell reference object certain shift searched object want estimate comes cornerstone idea method establish matching shift pair parallel paths originating nodes ending last layers match. pair paths must spatial shift respect layers subsampling feature channels respect other. take subsampling account per-layer functions denotes position path intersects layer reference activation graph. paths illustrated fig. logic simple matching siamese path means recognition hierarchy detects features different perception levels shifts respect currently estimated position allows conﬁdent prediction match. fact siamese path matched established computing matching function matching function individual neurons logical-and-like operator. discussed later. since want estimate shift node consider possible shifts vote them. denote siamese paths starting ending last layer every shift introduce log-likelihood event correct shift i.e. matches collect evidence possible paths matching functions individual paths leading sum-like operator discussed later. distribution used either obtain solution maxd∈d post-process distribution kind spatial smoothing optimization take best-cost solution. obvious obstacle using distribution observation minimal number activation channels layers network number layers number paths considered computation single originating node least exponential number layers. linear-time backward algorithm theorem pair operators left-distributive i.e. compute proof since distributivity dynamic programming approach similar developed gradient backpropagation. first introduce subsampling functions introduced then introduce auxiliary variables layer deﬁnition except fact paths considered them start later layer detailed procedure listed algorithm notation subsampled shifts result applying function every element initial shifts choice neuron matching function operators max-pooling layers computational graph truncated active connection moreover max-pooling create additional features passes/subsamples existing ones. thus make sense take account pre-activations layers activations reasons neighborhood max-pooling covering node indicator function paper product another possible choice would product theoretically combinations satisfy conditions theorem nevertheless found sum/product combination working better others. could explained fact would taken huge paths robust practice. validate approach ﬁeld computer vision method requires convolutional neural network trained large recognition dataset. vision correspondence tasks table summary convolutional neural network vgg-. show part layer higher activations layer type stands convolution stride followed relu non-linear activation function max-pooling stride input convolution padded same boundary rule. chose stereo matching validate method. task input data dimensionality shift represented horizontal shifts always convert images grayscale running cnns following observation color help. pre-trained recognition chose vgg- network network summarized table refer layer indexes table. important mention used whole range layers experiments. particular usually started layer ﬁnished layer such still necessary consider multi-channel input. extend algorithm case create virtual input layer virtual per-pixel arcs real input channels. starting later layer empirical observation improves results method advantage ﬁnishing earlier layer discovered researchers well thus abbreviate methods ours starting layer last layer. stereo matching chose largest available datasets kitti kitti image pairs datasets rectiﬁed correspondences searched row. training pair ground-truth shift measured densely per-pixel. ground truth obtained projecting point cloud lidar reference image. quality measure percentage errt pixels whose predicted shift error bigger threshold pixels. considered range thresholds main benchmark measure err. measure computed pixels visible images stereo pair. comparison baselines used setup proposed seminal work introduced deep learning stereo matching currently stays best methods kitti datasets. extensive study representative comparison learning-based non-learning-based methods setup open-source code setup. whole pipeline works follows. first obtain scores algorithm shifts dmax normalize scores per-pixel dividing maximal score thus turning range suitable running post-processing code finally post-processing code exactly parameters original method measure quality validation images. kinds baselines evaluation coming simpler versions deep feature transfer similar consider paths. ﬁrst group baselines following absolute differences census transform cens normalized cross-correlation ncc. also included learning-based methods acrt completeness although training data learn features method not. second group baselines stack activation volumes given layer range up-sample layer volumes reduced resolution. compute normalized cross-correlation stacked features. baselines denoted corr table table shows percentages erroneous pixels errt thresholds kitti validation method denoted ours. rightcolumns acrt correspond learning-based methods give completeness methods including ours learning. starting layer last layer. note correlate features applying relu following last layer. thus input relu inside layers. methods including ours undergo post-processing pipeline. pipeline consists semi-global matching left-right consistency check sub-pixel enhancement ﬁtting quadratic curve median bilateral ﬁltering. refer reader full description. ﬁrst group baselines tuned take results paper tune post-processing hyper-parameters second group baselines obtain best results. dataset consists training image pairs test image pairs. reﬂective surfaces like windshields excluded ground truth. results table show method ours performs better compared baselines. time performance lower learning-based methods main promise method scalability test task huge effort invested collecting training data important tasks without extensive datasets. goal section understand important deep hierarchy features versus layers. compared following setups ours uses second layer ours uses range layer layer central considers full range layers central arcs convolutions taken account backward pass ours full method. result table shows proﬁtable full hierarchy terms depth coverage receptive ﬁeld. stereo dataset consists training image pairs test image pairs. main difference kitti images colored reﬂective surfaces present evaluation. similar conclusions kitti drawn experimental results method provides reasonable transfer inferior learning-based methods table show depth results fig. table table shows percentages erroneous pixels errt thresholds kitti validation method denoted ours. rightcolumns acrt correspond learning-based methods give completeness methods including ours learning. goal experiment show robustness recognition hierarchy transfer correspondence search something advocated introduction advantage approach. apply style transfer method implemented prisma app. different style transfers left right images. different pixel level higher level descriptions images remain allows successfully method. qualitative results show robustness path-based method fig. work presented method transfer recognition correspondence search lowest level. that re-use activation paths deep convolutional neural networks propose efﬁcient polynomial algorithm aggregate exponential number paths. empirical results stereo matching task show method competitive among methods labeled data target domain. would interesting apply technique sound become possible high-quality deep convolutional model becomes accessible public would like thank dmitry laptev alina kuznetsova andrea cohen comments manuscript. also thank valery vishnevskiy running code cluster down. work partially funded swiss project efﬁcient object-centric detection. david eigen fergus. predicting depth surface normals semantic labels common multi-scale convolutional architecture. proceedings ieee international conference computer vision pages heiko hirschmuller. accurate efﬁcient stereo processing semi-global matching mutual information. computer vision pattern recognition cvpr ieee computer society conference volume pages ieee seungryong dongbo bumsub sangryul jeon stephen kwanghoon sohn. fcss fully convolutional self-similarity dense semantic correspondence. arxiv preprint arxiv. jonathan long evan shelhamer trevor darrell. fully convolutional networks semantic segmentation. proceedings ieee conference computer vision pattern recognition pages joseph redmon santosh divvala ross girshick farhadi. look once uniﬁed real-time object detection. proceedings ieee conference computer vision pattern recognition pages shaoqing kaiming ross girshick jian sun. faster r-cnn towards real-time object detection region proposal networks. advances neural information processing systems pages russakovsky deng krause satheesh huang karpathy khosla pierre sermanet david eigen xiang zhang michaël mathieu fergus yann lecun. overfeat integrated recognition localization detection using convolutional networks. arxiv preprint arxiv. aäron oord sander dieleman heiga karen simonyan oriol vinyals alex graves kalchbrenner andrew senior koray kavukcuoglu. wavenet generative model audio. corr abs/.", "year": 2017}