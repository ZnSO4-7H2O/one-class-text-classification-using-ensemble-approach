{"title": "Managing sparsity, time, and quality of inference in topic models", "tag": ["stat.ML", "cs.AI", "cs.CV", "stat.ME"], "abstract": "Inference is an integral part of probabilistic topic models, but is often non-trivial to derive an efficient algorithm for a specific model. It is even much more challenging when we want to find a fast inference algorithm which always yields sparse latent representations of documents. In this article, we introduce a simple framework for inference in probabilistic topic models, denoted by FW. This framework is general and flexible enough to be easily adapted to mixture models. It has a linear convergence rate, offers an easy way to incorporate prior knowledge, and provides us an easy way to directly trade off sparsity against quality and time. We demonstrate the goodness and flexibility of FW over existing inference methods by a number of tasks. Finally, we show how inference in topic models with nonconjugate priors can be done efficiently.", "text": "abstract inference integral part probabilistic topic models often non-trivial derive eﬃcient algorithm speciﬁc model. even much challenging want fast inference algorithm always yields sparse latent representations documents. article introduce simple framework inference probabilistic topic models denoted framework general ﬂexible enough easily adapted mixture models. linear convergence rate oﬀers easy incorporate prior knowledge provides easy directly trade sparsity quality time. demonstrate goodness ﬂexibility existing inference methods number tasks. finally show inference topic models nonconjugate priors done eﬃciently. interested important problems developing probabilistic topic models sparsity time. sparsity problem infer sparse latent representations documents second problem asks eﬃcient inference algorithm topic model. problems attracting signiﬁcant interest recent years signiﬁcant impacts non-trivial nature. proposed folding-in variational bayesian collapsed variational bayesian collapsed gibbs sampling sampling-based methods guaranteed converge underlying distributions slow rate. much faster often performs best. although inference methods signiﬁcant developments topic models remain common limitations studied theory practice. first theoretical upper bound convergence rate approximation quality inference. second inferred latent representations documents extremely dense requires huge memory storage. previous researches attacked sparsity problem categorized main directions. ﬁrst direction probabilistic probability distributions stochastic processes employed control sparsity. direction non-probabilistic regularization techniques employed induce sparsity although approaches gained important successes suﬀer severe drawbacks. indeed probabilistic approach often requires extension core topic models complex thus complicating learning inference. meanwhile non-probabilistic often changes objective functions inference non-smooth complicates inference requires auxiliary parameters associated regularization terms. parameters necessarily require model selection acceptable setting given dataset sometimes expensive. furthermore common limitation approaches sparsity level latent representations priori unpredictable cannot directly controlled. inherently tension sparsity time previous inference approaches. approaches focusing speeding inference often ignore sparsity problem. main reason zero contribution topic document implicitly prohibited models dirichlet distributions logistic function employed model latent representations documents. meanwhile approaches dealing sparsity problem often require time-consuming inference e.g. williamson larsson ugander note many practical applications e.g. information retrieval computer vision attempts initiated speed inference time attack sparsity problem gibbs sampling sparsity methods latent representations documents lies suﬃcient statistics gibbs samples. main limitations methods cannot directly control sparsity level suﬃcient statistics theory goodness inference convergence rate. further inference methods general ﬂexible enough easily extended models nonconjugate models. model xing exception inference potentially fast. nonetheless inference method cannot applied probabilistic topic models since unnormalization latent representations required. first resolve problems uniﬁed way. particularly introduce simple framework inference topic models called general ﬂexible enough easily employed mixture models. framework enjoys following theoretical properties inference converges linear rate optimal solutions; prior knowledge easily incorporated inference; sparsity level latent representations directly controlled; easy trade sparsity quality time. would like remark last properties unspeciﬁed existing inference methods. second contribution theoretical proof existence fast inference algorithms linear convergence rate many models plsa mf-ctm interestingly best knowledge ﬁrst proof tractability inference nonconjugate models e.g. mf-ctm tr-mmlda work inference nonconjugate models believed intractable organization discussing notations deﬁnitions section introduce framework inference section also discuss inference equivalent inference. further brieﬂy discuss applied plsa lda. proof tractability inference nonconjugate models presented subsection section describes experiments practical behaviors framework. regularization techniques provide impose sparsity latent representations adding regularization term objective function plays role regularization inducing sparsity. increasing parameter associated regularization term result sparser solutions. however always provably true. further cannot priori decide desired number non-zero components solution. hence regularization techniques provide indirect control sparsity. holds existing probabilistic inference approaches. topic model often assumes given corpus composed topics document mixture topics. example models include plsa many variants. models document another latent representation. applications necessary infer topic contributes speciﬁc emission term document. nevertheless unnecessary many applications. therefore take problem account leave open future work. given document would like desired topic proportion latent representation depends heavily objective inference. popular objective likelihood many situations objective diﬀer likelihood solely. example supervised dimension reduction representations discriminative representation document remain discriminative characteristics class document belongs. serve various objectives inference propose novel framework denoted presented algorithm loosely speaking inference given document ﬁrst chooses appropriate objective function continuously diﬀerentiable concave unit simplex uses sparse approximation algorithm frank-wolfe input document topics output latent representation step select appropriate objective function continuously diﬀerentiable concave step maximize frankwolfe algorithm. algorithm topic proportion algorithm presents details frank-wolfe algorithm inference ei’s denote standard unit vectors algorithm follows greedy approach proven converge linear rate optimal solutions. moreover iteration algorithm ﬁnds provably good approximate solution lying face simplex provides explicit bound dimensionality face approximate solution lies. iterations convex combination vertices implies approximate solution inference problem sparse provably good; would like remark framework general ﬂexible. readily modiﬁed various ways. example replace second step using approximation algorithms sequential greedy approximation forward basis selection addition ﬁrst step oﬀers ﬂexibility customize objectives inference. function latent representation principle turn bears resemblance regularization techniques widely used sparse learning. fact principle implicitly employed existing inference methods folding-in shown later. discuss details applications principle plsa models next subsections. following states properties framework inference corollary theorem corollary consider topic model topics document continuously diﬀerentiable concave simplex deﬁned theorem inference converges optimal solution linear rate. addition iterations inference error topic proportion non-zero components. note convergence rate inference framework linear i.e. possible speed convergence rate sub-linear frank-wolfe algorithm replaced forward basis selection appropriate. nonetheless extensions left open future research. computational complexity inference framework exactly frank-wolfe algorithm. heavily depends complicated next would like discuss popular inference problems inference explicit prior topic proportions; inference topic proportions endowed prior distribution. note inference plsa inference whereas inference show framework naturally applicable inference. besides suitable choice objective function implies inference framework fact inference. lemma consider topic model topics topic proportions assumed samples prior distribution. assume prior distribution belongs exponential family parameterized proof inference maximize posterior probability given document bayes’ rule says hence maxθ∈∆ maxθ∈∆ maxθ∈∆ +log maxθ∈∆ ignoring constants rewriting discussed adapted inﬂuential topic models plsa lemma provides connection inference concave optimization. consequence inference plsa reformulated easy optimization problem seamlessly resolved combining corollary obtain following. corollary consider plsa topics document exists algorithm inference converges optimal solution linear rate allows eﬃciently sparse topic proportion guaranteed bound inference error. diﬀerentiable concave simplex hence frank-wolfe algorithm exploited inference. handily inference plsa modifying objective function form inference plsa studied shashanka larsson ugander methods result concave-convex objective functions thus guaranteed bound convergence. originally come dirichlet prior topic proportions. interpret regularization term induces sparse solutions however regularization always result concave objective function hence causes inference np-hard furthermore regularization requires topics non-zero contributions speciﬁc document since function requires well-deﬁned. hence cannot infer latent representations sparse common sense. sparse latent representations modiﬁcations necessary. readily apply framework objective likelihood function. employments framework yield inference suggested theorem cases amounts endowing priors dirichlet topic proportions. priors typical example logistic normal distributions model correlations topics noted various researchers non-conjugacy priors causes signiﬁcant diﬃculties deriving good inference/learning algorithms. consequence existing inference methods often slow guarantee neither convergence rate inference quality. contrary show inference many nonconjugate models done eﬃciently. substantiate claim study correlated topic models blei laﬀerty relations hidden topics document assume transformation used recover topic proportion exists algorithm inference converges optimal solution linear rate. negative semideﬁnite. combining positive deﬁniteness conclude negative deﬁnite feasible solution implies concave function interior consequence concave maximization problem simplex. even though concave maximization problem objective function speciﬁed boundary hence algorithm cannot directly applied. fortunately algorithms jaggi work well interior linear rate convergence. theorem basically says inference fact tractable done fast contrary existing belief topic modeling literature. moreover inference quality guaranteed good. believe results derived many models salomatin putthividhy virtanen worthwhile noting optimal solutions inference problem longer sparse would optimal contains zero component. insists using normal distribution full form model correlations slight modiﬁcations suﬃcient inference efﬁciently. indeed using similar arguments proof above show objective function inference concave convex region relations hidden topics document assume transformation used recover topic proportion exists algorithm inference µk∀k} converges optimal solution linear rate. slight modiﬁcations. indeed replace initial step frank-wolfe algorithm setting certain point interior believe slight modiﬁcation change signiﬁcantly convergence rate original algorithm. remark topic proportions inferred eﬃciently easily design learning algorithm ctm. forget latent variable inference document e-step. mstep maximizes likelihood training data w.r.t. model parameters. idea investigated resulting topic model many attractive properties dealing large data. believe following learning approach easily learn large scale hence enable large-scale analyses correlations latent topics. section explore well framework works compared existing inference methods. ﬁrst investigate fundamental characteristics framework including sparsity inferred topic proportions inference time inference quality. addition theoretical analysis demonstration made library practice easy researchers/users incorporate framework customized models writing objective functions. help substantially reduce complication time researchers designing topic models. library general enough applicable inference literatures topic modeling. ﬂexibility framework evidenced speciﬁc applications. ﬁrst successfully develop fully sparse topic models simpliﬁed variant plsa lda. fstm demonstrated work well various attractive properties dealing large data. second application employ design eﬀective methods supervised dimension reduction analyses previous section shown inference framework fast provably good provided suitable choice objective function. section demonstrate empirically even modest choice likelihood framework infers comparably well. three inference methods taken comparison folding-in variational bayesian denoted objective function likelihood function. five corpora used investigation statistics shown table corpus ﬁrst trained model training part. inference test criteria convergence. inference time ﬁrst measure comparison inference time. figure depicts results inference corpora. observe folding-in slowest. much quickly folding-in. iteration foldingtook computations much less however often reached convergence much less steps folding-in. overall quickly. compared folding-in framework inference signiﬁcantly faster. often reached convergence tens iterations. note complexity framework heavily depends complicated objective case objective likelihood needs computations evaluated. realize inference time quickly scaled number topics increases folding-in increased much faster. suggests framework substantially scalable folding-in document sparsity next consider sparse inferred topic proportions are. sparsity given document fraction nonzero elements inferred latent representation. averaged test depicted second figure note inference framework always found sparse topic proportions. sparsity level increases model topics. surprisingly inference folding-in sometimes achieves sparse topic proportions. possible reason folding-in inherit sparsity original data since inference folding-in simply addition multiplication sparse data. nevertheless always folding-in achieve sparse solutions without principled mechanism. unsurprisingly sparse latent representations documents. bag-of-words representation; often slowest. futhermore methods achieve comparable quality long suitable parameter settings chosen hence selected representative. experiments. last figure shows goodness diﬀerent inference methods terms perplexity loosely speaking perplexity inverse geometric mean probabilities words appearing testing documents calculated explain phenomenon thorough investigations necessary. observed cases learned small parameters dirichlet priors. remember inference np-hard np-hardness prevent variational method quickly inferring good solutions. main reason inferior performance note inference inference whose objective diﬀerent likelihood data. perplexity mainly relates likelihood. therefore asynchronous objective functions inference another reason inferior performance terms perplexity. separability documents topical space topic models often expected provide soft clustering documents space topics i.e. clustering documents topical clusters. hence would like well inference methods cluster testing documents. good method cluster documents topics separately. words topical space documents separately clustered. this inferred latent representations documents visualize ﬁrst dimensions. figure shows distribution documents topical space. observe documents projected spread around axes separated clearly clusters. similar phenomenon observed fig. separability documents space topics inferred diﬀerent methods folding-in provide separate clusters documents. meanwhile always separates documents explicitly clusters associated latent topics. folding-in. meanwhile projected document focused topics documents separated clusters explicitly. observed inference framework often places high probability topic small probabilities topics zero others. topical space documents explicitly clustered. result inference framework provides better clustering documents topical space. facing large-scale settings including large corpora extremely high dimensionality large number topics fast algorithms compact storage demands highly desired. hence principled trade quality time storage requirement sometimes necessary. fortunately frank-wolfe algorithm fulﬁll desires topic modeling also literatures. indeed provably fast provides simple decide sparsity level solutions limiting number iterations. investigated quick reaches convergence practice. experiments done enron learned topics. results shown figure realize reached convergence quickly. found cases iterations average quality almost stable. note dimension inference problem much larger sparsity level solutions stable almost iterations. phenomenon observed corpora. facts suggest converge quickly practice despite loose bound theorem property attractive practical applications. fig. illustration trading sparsity time quality. able reach convergence quickly. iterations average quality terms perplexity almost stable even though number topics much larger theoretical empirical analyses framework shown work signiﬁcantly fast always infer sparse solutions. second show inference topic models nonconjugate priors done eﬃciently contrary previous belief inference nonconjugate models intractable.", "year": 2012}