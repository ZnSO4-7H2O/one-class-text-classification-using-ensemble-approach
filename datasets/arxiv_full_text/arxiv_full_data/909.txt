{"title": "Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for training Boltzmann Machines. Similar in spirit to the Hessian-Free method of Martens [8], our algorithm belongs to the family of truncated Newton methods and exploits an efficient matrix-vector product to avoid explicitely storing the natural gradient metric $L$. This metric is shown to be the expected second derivative of the log-partition function (under the model distribution), or equivalently, the variance of the vector of partial derivatives of the energy function. We evaluate our method on the task of joint-training a 3-layer Deep Boltzmann Machine and show that MFNG does indeed have faster per-epoch convergence compared to Stochastic Maximum Likelihood with centering, though wall-clock performance is currently not competitive.", "text": "paper introduces metric-free natural gradient algorithm training boltzmann machines. similar spirit hessian-free method martens algorithm belongs family truncated newton methods exploits efﬁcient matrix-vector product avoid explicitly storing natural gradient metric metric shown expected second derivative log-partition function equivalently covariance vector partial derivatives energy function. evaluate method task joint-training -layer deep boltzmann machine show mfng indeed faster per-epoch convergence compared stochastic maximum likelihood centering though wall-clock performance currently competitive. boltzmann machines become popular method deep learning performing feature extraction probability modeling. emergence models practical learning algorithms stems development efﬁcient training algorithms estimate negative log-likelihood gradient either contrastive stochastic approximations. however success models part limited restricted boltzmann machine whose architecture allows efﬁcient exact inference. unfortunately comes cost model’s representational capacity limited single layer latent variables. deep boltzmann machine addresses deﬁning joint energy function multiple disjoint layers latent variables interactions within layer prohibited. affords model rich inference scheme incorporating top-down feedback also makes training much difﬁcult requiring recently initial greedy layer-wise pretraining scheme. since montavon muller shown difﬁculty stems ill-conditioning hessian matrix addressed simple reparameterization energy function trick called centering barrier joint-training overcoming challenging optimization problem apparent second-order gradient methods might prove effective simple stochastic gradient methods. prove especially important consider models increasingly complex posteriors higher-order interactions latent variables. explore natural gradient seems ideally suited stochastic nature boltzmann machines. paper structured follows. section provides detailed derivation natural gradient including speciﬁc form bms. equations previously appeared derivation aims accessible attempts derive natural gradient basic principles minimizing references information geometry. section represents true contribution paper practical natural gradient algorithm exploits persistent markov chains stochastic maximum likelihood hessian-free like algorithm method named metric-free natural gradient avoids explicitly storing natural gradient metric uses linear solver perform required matrix-vector product l−eq preliminary experimental results dbms presented section discussion appearing section motivation derivation main insight behind natural gradient space probability distributions {pθ; forms riemannian manifold. learning typically proceeds iteratively adapting parameters empirical distribution thus traces path along manifold. immediate consequence following direction steepest descent original euclidean parameter space correspond direction steepest descent along needs account metric describing local geometry manifold given fisher information matrix shown equation metric typically derived information geometry derivation accessible machine learning audience obtained follows. natural gradient aims search direction minimizes given objective function kullback–leibler divergence pθ+∆θ) remains constant throughout optimization. constraint ensures make constant progress regardless curvature manifold enforces invariance parameterization model. natural gradient maximum likelihood thus formalized order derive useful parameter update rule consider divergence assumption also assume discrete bounded domain deﬁne probability mass function taking taylor series expansion pθ+∆θ around denoting column vector partial derivatives i-th entry hessian matrix replacing objective function equation ﬁrst-order taylor expansion rewriting constraint lagrangian arrive following formulation loss function natural gradient seeks minimize. form reminiscent newton direction natural gradient multiplies estimated gradient inverse expected hessian equivalently fisher information matrix equivalence expressions shown trivially details appearing appendix. stress expectations computed respect model distribution thus computing metric involve empirical distribution way. boltzmann machines thus equal uncentered covariance maximum likelihood gradients. following pursue derivation form given equation derivation. boltzmann machines deﬁne joint distribution vector binary random varik bkxk weight matrix rn×n bias vector energy probability related partition function deﬁned boltzmann distribution discussion. computing taylor expansion divergence equation glossed important detail. namely handle latent variables topic ﬁrst discussed could easily derived natural gradient considering pθ+∆θ) const. alternatively since distinction visible hidden units entirely artiﬁcial simply wish consider distribution obtained analytically integrating maximal number random variables. would entail marginalizing even layers strategy employed great success context work however consider metric obtained considering divergence full joint distributions pθ+∆θ. metric-free natural gradient implementation compute natural gradient ﬁrst replacing expectations equation ﬁnite sample approximation. efﬁciently reusing model samples generated persistent markov chains sml. given size matrix estimated however expect method require larger number chains typically used. rest method similar hessian-free algorithm martens exploit efﬁcient matrix-vector implementation combined linear-solver conjugate gradient minres solve system additionally replace expectation rhs. previous equation average computed mini-batch training examples typically done stochastic learning setting. boltzmann machines matrix-vector product computed straightforward manner without recourse pearlmutter’s r-operator starting sampling approximation equation simply push product inside expectation follows generate positive phase samples initializing markov chains state generate negative phase samples initial zero vector. computely function performs equation without instantiating cgsolve) ﬁrst computing matrix-vector product easily avoid computing full matrix indeed result operation vector length leftmultiplied matrix dimension yielding matrix-vector product single iteration mfng presented algorithm full open-source implementation also available online.. performed proof-of-concept experiment determine whether metric-free natural gradient algorithm suitable joint-training complex boltzmann machines. compared method stochastic maximum likelihood diagonal approximation mfng -layer deep boltzmann machine trained mnist algorithms conjunction centering strategy montavon muller proved crucial successfully joint-train layers chose small -layer units ﬁrst second third layers respectively comparable hyper-parameters varied follows. inference iterations either meanﬁeld implemented gibbs sampling. learning rate kept ﬁxed training chosen minres damping coefﬁcient used ﬁxed tolerance finally tested algorithms minibatch sizes either elements finally since comparing optimization algorithms hyper-parameters chosen based training likelihood experiments used minres linear solver speed ability return pseudo-inverses faced ill-conditioning. https//github.com/gdesjardins/mfng centering coefﬁcients initialized otherwise held ﬁxed training. expect larger minibatch sizes preferable however simulating number markov chains figure estimated model likelihood function epochs cpu-time mfng diagonal approximation sml. methods conjunction centering trick centering coefﬁcients held ﬁxed training. grid search yielded following hyper-parameters batch size mfng/sgd; steps mean-ﬁeld sampling-based inference mfng/sgd learning rate figure shows likelihood estimated annealed importance sampling function number epochs metric mfng achieves fastest convergence obtaining training/test likelihood −./−. nats epochs. comparison mfng-diag obtains −./−. nats −./−. nats epochs. picture changes however plotting likelihood function cpu-time shown figure given wall-time mfng mfng-diag able perform upwards epochs resulting impressive likelihood score note results obtained binary-version mnist order compare results therefore directly comparable binarizes dataset sampling figure shows breakdown algorithm runtime various components algorithm. statistics collected early stages training generally representative bigger picture. linear solver clearly dominates runtime interesting observations make. small models batch sizes greater single evaluation appears order magnitude gradient evaluation. cases cost smaller sampling represents non-negligible part total computational budget. suggests mfng could become especially attractive models expensive sample from. overall however restricting number cg/minres iterations appears computational performance achieved increasing damping factor affects convergence terms likelihood left future work. wall-clock performance mfng currently competitive believe still many avenues explore improve computational efﬁciency. firstly performed almost optimization various minres hyper-parameters. particular algorithm convergence ﬁxed tolerance typically resulted relatively iterations level precision might required zero-weight base-rate model whose biases maximum likelihood; interpolating distributions target distribution; ﬁnally analytical integration odd-layers. figure breakdown algorithm runtime vary batch size model size runtime additive order given labels dotted lines denote intermediate steps continuous lines denote full steps. data collected nvidia card. algorithm). additionally could worth exploiting strategy linear solver initialized solution found previous iteration. prove much efﬁcient current approach initializing solver zero vector. pre-conditioning also well-known method accelerating convergence speed linear solvers implementation used simple diagonal regularization jacobi preconditioner could implemented easily however computing diagonal ﬁrst-pass. finally single experiment offers little evidence support either conclusion well possible mfng simply computationally efﬁcient dbms compared centering. case would worth applying method either models known ill-conditioning factored order boltzmann machines models distributions exhibiting complex posterior distributions. scenarios wish maximize positive phase statistics performing larger jumps parameter space. remains seen would interact burn-in period persistent chains directly tied magnitude references amari differential geometrical methods statistics. lecture notes statistics amari natural gradient works efﬁciently learning. neural computation amari kurata nagaoka martens deep learning hessian-free optimization. pages montavon muller k.-r. deep boltzmann machines centering trick. montavon k.-r. muller editors neural networks tricks trade volume lecture notes computer science pages", "year": 2013}