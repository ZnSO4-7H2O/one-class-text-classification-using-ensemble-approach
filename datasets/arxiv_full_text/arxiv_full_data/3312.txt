{"title": "Structured Transforms for Small-Footprint Deep Learning", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "We consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices. We propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank. Our structured transforms admit fast function and gradient evaluation, and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured. Experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training, and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques. In keyword spotting applications in mobile speech recognition, our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models, while providing more than 3.5-fold compression.", "text": "consider task building compact deep learning pipelines suitable deployment storage power constrained mobile devices. propose uniﬁed framework learn broad family structured parameter matrices characterized notion displacement rank. structured transforms admit fast function gradient evaluation span rich range parameter sharing conﬁgurations whose statistical modeling capacity explicitly tuned along continuum structured unstructured. experimental results show transforms signiﬁcantly accelerate inference forward/backward passes training offer superior accuracy-compactness-speed tradeoffs comparison number existing techniques. keyword spotting applications mobile speech recognition methods much effective standard linear low-rank bottleneck layers nearly retain performance state models providing .-fold compression. non-linear vector-valued transforms form elementwise nonlinearity input vector matrix parameters building blocks complex deep learning pipelines non-parametric function estimators arising randomized kernel methods large general dense matrix cost storing parameters computing matrix-vector products time make prohibitive deploy models lightweight mobile devices wearables battery life precious storage limited. particularly relevant always-on mobile applications continuously looking speciﬁc keywords spoken user processing live video stream onboard mobile robot. settings models need hosted specialized low-power digital signal processing components even resource constrained device cpu. parsimonious structure typically imposed parameter matrices low-rankness rank matrix product representation form columns. clearly representation reduces storage requirements parameters accelerates matrix-vector multiplication time another popular structure sparsity typically imposed optimization zero-inducing regularizers. techniques include freezing random matrix motivated approximations kernel functions storing ﬁxed-precision formats using speciﬁc parameter sharing mechanisms training smaller models outputs larger models structured matrices matrix described much fewer parameters referred structured matrix. typically structure reduce memory requirements also dramatically accelerate inference training fast matrix-vector products gradient computations. classes structured matrices arising pervasively many contexts different types parameter sharing toeplitz matrices constant values along diagonals. property holds anti-diagonals resulting class matrices called hankel matrices. toeplitz hankel matrices intimately related one-dimensional discrete convolutions arise naturally time series analysis dynamical systems. vandermonde matrix determined taking elementwise powers second column. important special case complex matrix associated discrete fourier transform vandermonde structure primitive root unity. similarly entries cauchy matrices completely deﬁned length vectors. vandermonde cauchy matrices arise naturally polynomial rational interpolation problems. superfast numerical linear algebra structure matrices exploited faster linear algebraic operations matrix-vector multiplication inversion factorization. particular matrix-vector product computed time toeplitz hankel matrices time vandermonde cauchy matrices. displacement operators ﬁrst glance matrices appear different kinds parameter sharing consequently different algorithms support fast linear algebra. turns however structured matrix class described above associated speciﬁc displacement operator rm×n rm×n transforms matrix class matrix low-rank i.e. rank) min. displacement rank approach traced back seminal paper greatly uniﬁes algorithm design complexity analysis structured matrices generalizations structured matrices consider deriving matrix taking arbitrary linear combinations products structured matrices inverses e.g. αtt− toeplitz matrix. parameter sharing structure derived matrix means apparent anymore. turns associated displacement operator remarkably continues expose underlying parsimony structure i.e. derived matrices still mapped relatively low-rank matrices displacement rank approach allows fast linear algebra algorithms seamlessly extended broader classes matrices. displacement rank parameter controls degree structure generalized matrices. technical preview contributions outline propose building deep learning pipelines parameter matrices belong class generalized structured matrices characterized displacement rank. section attempt give self-contained overview displacement rank approach drawing results relevant literature structured matrix computations completeness). section show proposed structured transforms deep learning admit fast matrix multiplication gradient computations rich statistical modeling capacity explicitly controlled displacement rank hyperparameter covering along continuum entire spectrum conﬁgurations highly structured unstructured matrices. focus paper toeplitz-related transforms proposal extends structured matrix generalizations. section study inference training-time acceleration structured transforms function displacement rank dimensionality. approach compares highly favorably numerous techniques learning size-constrained models several benchmark datasets. finally demonstrate approach mobile speech recognition applications able match performance much bigger state models fraction parameters. notation denote canonical basis elements denote identity zero matrices respectively. anti-identity reﬂection matrix whose action vector reverse entries. dimension obvious drop subscript; rectangular matrices specify dimensions explicitly e.g. zero-valued row-vector ones column vector length denotes hadamard product vectors complex vector denote vector complex conjugate entries. discrete fourier transform matrix denoted also denote denote ω−x. vector diag denotes diagonal matrix given diagii begin providing brisk background displacement rank approach. unless otherwise speciﬁed notational convenience henceforth assume squared transforms i.e. discuss rectangular transforms later. proofs various assertions found selfcontained supplementary material sylvester displacement operator denoted rn×n rn×n deﬁned carefully choosing instantiate sylvester stein displacement operators desirable properties. particular several important classes displacement operators and/or chosen f-unit-circulant matrix deﬁned follows. deﬁnition real-valued scalar f-circulant matrix denoted deﬁned follows f-unit-circulant matrix associated basic downward shift-and-scale transformation i.e. matrix-vector product shifts elements column vector downwards scales brings last element resulting several basic algebraic properties crucial results stated section figure lists rank sylvester displacement operator applied matrices belonging various structured matrix classes operator matrices eqn. chosen diagonal and/or f-unit-circulant. seen despite difference structures classes characterized displacement rank. figure shows low-rank transformation happens case toeplitz matrix embedded toeplitz matrix copies toeplitz matrix shown black boxes. shift scale action aligns sub-matrices. taking difference sylvester displacement operator nulliﬁes aligned submatrix leaving rank matrix non-zero elements along ﬁrst last column. note negative sign introduced term prevents complete zeroing value hence critical invertibility displacement action. class structured matrices listed figure naturally generalized allowing rank displacement operator higher. speciﬁcally given displacement operator displacement rank parameter consider class matrices satisﬁes rank) clearly then rank matrices refer rank) displacement rank low-rank factors rn×r associated low-displacement generators. operators listed table broader classes structured matrices correspondingly called toeplitz-like vandermonde-like cauchy-like. fast numerical linear algebra algorithms extend matrices order express structured matrices low-displacement rank directly function lowdisplacement generators need invert obtain learnable parameterization. stein type displacement operator following elegant result known theorem krylov decomposition). matrix rn×r operator matrices satisfy scalars expressed henceforth focus paper toeplitz-like matrices displacement operator interest sylvester type ∇zz−. order apply theorem switch sylvester stein operators setting satisfy conditions theorem resulting expressions involve krylov matrices generated f-unit-circulant matrices called f-circulant matrices literature. deﬁnition given vector f-circulant matrix deﬁned follows finally obtain explicit parameterization toeplitz-like matrices turns involve taking sums products circulant skew-circulant matrices. theorem rn×r written motivated theorem propose learning parameter matrices form eqn. optimizing displacement factors first properties displacement operators follows class matrices rich statistical modeling perspective. theorem matrices written learn parameter matrix structured eqn. displacement rank equal also search convolutional transforms. sense structured transforms higher displacement rank generalize convolutional layers. displacement rank provides knob modeling capacity displacement matrices highly structured compact high displacement matrices start contain increasingly unstructured dense matrices. next show associated structured transforms form admit fast evaluation gradient computations respect first recall following wellknown result concerning diagonalization f-circulant matrices. theorem result implies special cases corresponding circulant skew-circulant matrices respectively matrix-vector multiplication computed time fast fourier transform particular single matrix-vector product circulant skew-circulant matrices computational cost ffts. therefore matrices form eqn. comprising products circulant skew-circulant matrices naively computing matrix-vector product batch input vectors would take ffts. however cost signiﬁcantly lowered ffts making following observation diag here parameters computed shared across multiple input vectors minibatch input computed shared across eqn. ﬁnal inverse also shared. thus following result immediate. theorem given matrix matrix-matrix product zz−) computed cost ffts using following show structured transforms embedded deep learning pipeline gradient computation also accelerated. first note jacobian structure f-circulant matrices following pleasing form. proposition jacobian respect parameters batches forward backward inputs backpropagation. naively computed ffts. however before sharing forward backward inputs parameters lowered ffts. give matricized implementation. proposition matrices whose columns forward backward inputs respectively minibatch size backpropagation. gradient respect computed cost ffts follows rectangular transforms variants theorems exist rectangular transforms alternatively subsample outputs square transforms cost output vectors extra computations assuming multiple stack square transforms. acceleration structured transforms figure analyze speedup obtained practice using circulant toeplitz-like matrices relative dense unstructured matrix function displacement rank dimension three scenarios considered inference speed test instance training speed implicitly dictated forward passes minibatch gradient computations minibatch. factors differences cache optimization simd vectorization multithreading level- blas level- blas implementations inﬂuence speedup observed practice. speedup gains start show dimensions small circulant matrices. gains become dramatic acceleration order times several thousand dimensions even higher displacement rank toeplitz-like transforms. effectiveness learning compact neural networks next compare proposed structured transforms several existing techniques learning compact feedforward neural networks. exactly replicate experimental setting recent paper hashednets uses several image classiﬁcation datasets ﬁrst prepared mnist original -class mnist digit classiﬁcation dataset training examples test examples. bg-img-rot refers challenging version mnist digits randomly rotated placed random black white background. rect convex -class binary image datasets task distinguish tall wide rectangles whether pixels form convex region respectively. datasets input images size several existing techniques benchmarked compressing reference single hidden layer model hidden nodes. random edge removal fraction weights randomly frozen zero-valued. low-rank decomposition neural network hidden layer size reduced satisfy parameter budget. dark knowledge small neural network trained respect original hashednets approach uses low-cost hash function randomly group connection hashednets dark knowledge trains hashednet respect original consider learning models comparable size weights hidden layer structured toeplitz-like matrix. also compare fastfood approach weight matrix product diagonal parameter matrices ﬁxed permutation walsh-hadamard matrices also admitting multiplication gradient computation time. circulant neural network approach proposed special case framework results table show toeplitz-like structured transforms outperform competing approaches datasets sometimes signiﬁcant margin similar drastically lesser number parameters. also noted random weight tying hashednets reduces number parameters lack structure resulting weight matrix cannot exploited fft-like multiplication time. note passing hashednets weight matrices whose entries assume distinct values mailman algorithm used faster matrix-vector multiplication complexity still much slower matrix-vector multiplication time toeplitz-like matrices. also note distillation ideas complementary approach improve results. mobile speech recognition demonstrate techniques developed paper speech recognition application meant mobile deployment. speciﬁcally consider keyword spotting task deep neural network trained detect speciﬁc phrase google data used experiments consists utterances selected phrases larger utterances serve negative training examples. utterances randomly split training development evaluation sets ratio created noisy evaluation artiﬁcially adding babble-type cafeteria noise play-music clean data set. refer noisy data cafe. refer reader details datasets. consider task shrinking large model task whose architecture follows input layer consists dimensional log-mel ﬁlterbanks stacked temporal context produce input whose dimensions time frequency respectively. input convolutional layer ﬁlter size frequency stride ﬁlters. output convolutional layer size output layer fully connected layer followed softmax layer predicting classes constituting phrase playmusic. full training contains million samples. asynchronous distributed stochastic gradient descent parameter server framework worker nodes optimizing various models. global learning rate structured transform layers layer-speciﬁc learning rate decayed exponential factor figure play-music detection performance end-to-end keyword spotting performance terms false reject rate false alarm rate classiﬁcation accuracy function training time. displacement rank parenthesis toeplitz-like models. results different models reported figure including state keyword spotting model developed operating point false alarm hour following observations made parameters displacement rank= toeplitz-like structured transform outperforms standard low-rank bottleneck model rank= containing times parameters; also lowers false reject rates circulant fastfood transforms displacement rank false reject rate comparison times larger rank= standard low-rank bottleneck model. best toeplitz-like model comes within performance -times larger fully-connected times larger reference models. terms classiﬁcation accuracy function training time figure shows models come within accuracy fully-connected reference models easily provide much better accuracy-time tradeoffs comparison standard low-rank bottleneck models circulant fastfood baselines. conclusions similar noise conditions introduced shown effectiveness notions parsimony rooted theory structured matrices. proposal extended various structured matrix classes including block multi-level toeplitz-like matrices related multidimensional convolution hope ideas might lead generalizations convolutional neural networks. acknowledgements thank krzysztof choromanski carolina parada rohit prabhavalkar rajat monga baris sumengen kilian weinberger wenlin chen contributions work.", "year": 2015}