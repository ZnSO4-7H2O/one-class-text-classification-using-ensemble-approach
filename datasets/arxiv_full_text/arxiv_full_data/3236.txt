{"title": "A recursive divide-and-conquer approach for sparse principal component  analysis", "tag": ["cs.CV", "cs.LG", "stat.ML", "62H25, 68T10", "I.5.0; I.5.1"], "abstract": "In this paper, a new method is proposed for sparse PCA based on the recursive divide-and-conquer methodology. The main idea is to separate the original sparse PCA problem into a series of much simpler sub-problems, each having a closed-form solution. By recursively solving these sub-problems in an analytical way, an efficient algorithm is constructed to solve the sparse PCA problem. The algorithm only involves simple computations and is thus easy to implement. The proposed method can also be very easily extended to other sparse PCA problems with certain constraints, such as the nonnegative sparse PCA problem. Furthermore, we have shown that the proposed algorithm converges to a stationary point of the problem, and its computational complexity is approximately linear in both data size and dimensionality. The effectiveness of the proposed method is substantiated by extensive experiments implemented on a series of synthetic and real data in both reconstruction-error-minimization and data-variance-maximization viewpoints.", "text": "paper method proposed sparse based recursive divide-and-conquer methodology. main idea separate original sparse problem series much simpler sub-problems closed-form solution. recursively solving sub-problems analytical eﬃcient algorithm constructed solve sparse problem. algorithm involves simple computations thus easy implement. proposed method also easily extended sparse problems certain constraints nonnegative sparse problem. furthermore shown proposed algorithm converges stationary point problem computational complexity approximately linear data size dimensionality. eﬀectiveness proposed method substantiated extensive experiments implemented series synthetic real data reconstruction-error-minimization data-variance-maximization viewpoints. principal component analysis classical popular tools data analysis dimensionality reduction wide range successful applications throughout science engineering seeking so-called principal components along data variance maximally preserved always capture intrinsic latent structure underlying data. information greatly facilitates many data processing tasks feature extraction pattern recognition. despite many advantages conventional suﬀers fact component generally linear combination data variables weights linear combination also called loadings typically non-zeros. many applications however original variables meaningful physical interpretations. biology example variable gene expression data corresponds certain gene. cases derived loadings always expected sparse facilitate interpretability. moreover certain applications ﬁnancial asset trading sparsity loadings especially expected since fewer nonzero loadings imply fewer transaction costs. accordingly sparse attracted much attention recent decade variety methods topic developed ﬁrst attempt topic make certain post-processing transformation e.g. rotation jolliﬀe simple thresholding cadima jolliﬀe loadings obtained conventional enforce sparsity. jolliﬀe uddin advanced scotlass algorithm simultaneously calculating sparse model additional l-norm penalty loading vectors better results achieved spca algorithm developed based iterative elastic regression d’aspremont proposed method called dspca ﬁnding sparse solving sequence semideﬁnite programming relaxations sparse shen huang developed series methods called spca-rsvd computing sparse low-rank matrix factorization multiple sparsity-including penalties journ´ee designed four algorithms denoted gpowerl gpowerl gpowerlm gpowerlm respectively sparse formulating issue non-concave maximization problems ll-norm sparsity-inducing penalties extracting single unit sparse sequentially block units ones simultaneously based probabilistic generative model methods also attained e.g. empca method derived sigg buhmann sparse and/or nonnegative sparse sriperumbudur provided iterative algorithm called dcpca iteration consists solving quadratic programming problem recently zhang developed augmented lagrangian method sparse solving class non-smooth constrained optimization problems additionally d’aspremont derived pathspca algorithm computes full solutions target numbers nonzero coeﬃcients mainly methodologies utilized current research sparse problem. ﬁrst greedy approach including dspca spca-rsvd empca pathspca etc. methods mainly focus solving one-sparse-pc model sparse sequentially calculated deﬂated data matrix data covariance methodology ﬁrst several sparse underlying data generally properly extracted computation sparse tends incrementally invalidated cumulation computational error. second block approach. typical methods include scotlass gpowerlm gpowerlm alspca etc. methods calculate multiple sparse utilizing certain block optimization techniques. block approach sparse expected eﬃcient greedy simultaneously attain multiple generally diﬃcult elaborately rectify individual sparse based speciﬁc requirements practice paper methodology called recursive divide-and-conquer employed solving sparse problem. main idea decompose original large complex problem sparse series small simple sub-problems recursively solve them. sub-problems closed-form solution makes method simple easy implement. hand compared greedy approach method expected integratively achieve collection appropriate sparse problem iteratively rectifying sparse recursive way. group sparse attained proposed method proved stationary solution original sparse problem. hand compared block approach method easily handle constraints superimposed individual sparse certain sparsity and/or nonnegative constraints. besides computational complexity proposed method approximately linear data size dimensionality makes well-suited handle large-scale problems sparse pca. follows main idea implementation details proposed method ﬁrst introduced section convergence computational complexity also analyzed section. eﬀectiveness proposed method comprehensively substantiated based series empirical studies section paper concluded summary outlook future research. throughout paper denote matrices vectors scalars upper-case bold-faced letters lower-case bold-faced letters lower-case letters respectively. denotes trace matrix rd×r denotes array loading vectors. second formulated seeking r-dimensional linear subspace projected data original ones close possible reconstruction-errorminimization viewpoint corresponds following model norm respectively. note involved penalty models tends enforce sparsity output pcs. methods constructed include scotlass dspca dcpca alspca etc. related include spca spca-rsvd gpower etc. paper construct method reconstruction-error-minimization model experiments verify proposed method also performs well based datavariance-maximization criterion. noted orthogonality constraints loadings imposed simultaneously enforcing sparsity orthogonality generally diﬃcult task. like existing sparse methods enforce orthogonal models. brieﬂy discuss specify stopping criterion algorithm. objective function sparse model monotonically decreasing iterative process algorithm since step step iterations makes exact optimization column vector others ﬁxed. thus terminate iterations algorithm updating rate smaller preset threshold maximum number iterations reached. brieﬂy analyze computational complexity proposed redac algorithm. evident computational complexity algorithm essentially determined iterations step step i.e. calculation closed-form solutions respectively. compute simple operations involved computation needs cost. compute sorting elements d-dimensional vector required total computational cost around applying well-known heap sorting algorithm whole process algorithm thus requires around computational cost iteration. computational complexity proposed algorithm approximately linear size dimensionality input data. section evaluate convergence proposed algorithm. convergence algorithm actually implied monotonic decrease cost function iterations algorithm. speciﬁc iteration algorithm step step optimize column vector others ﬁxed respectively. since objective function evidently lower bounded algorithm guaranteed convergent. proposed redac algorithm viewed block coordinate descent method solving alteratively optimizing respectively. following theorem implies algorithm converge stationary point problem. theorem assumption function deﬁned regular holds condition open gateaux-diﬀerentiable based theorems also easily unique minimum others ﬁxed. theorem naturally followed theorem another advantage proposed redac methodology easily extended sparse applications certain constraints needed output sparse pcs. following section give extensions methodology nonnegative sparse problem. nonnegative sparse problem diﬀers conventional sparse nonnegativity constraint imposed output sparse pcs. nonnegativity property problem especially important applications microeconomics environmental science biology etc. corresponding optimization model written follows utilizing similar recursive divide-and-conquer strategy problem separated series small minimization problems respect column vector respectively follows virtue closed-form solution given theorem construct redac algorithm solving nonnegative sparse model since algorithm diﬀers algorithm step list step algorithm evaluate performance proposed redac algorithm sparse problem conduct experiments series synthetic real data sets. experiments implemented matlab platform athlon dual windows experiments method utilized initialization. proposed algorithm implemented experiments mostly similar performance. thus list better throughout. synthetic data sets ﬁrst utilized evaluate performance proposed algorithm recovering ground-truth sparse principal components underlying data. illustrate advantage sparse conventional sparse extraction. data become frequently utilized benchmark data testing eﬀectiveness sparse methods. data generated following ﬁrst three hidden factors created generate sets data contains data generated aforementioned apply algorithm extract ﬁrst sparse pcs. results show algorithm perform well experiments. speciﬁc proposed redac algorithm faithfully delivers ground-truth sparse experiments. eﬀectiveness proposed algorithm thus easily substantiated series benchmark data. rest generated applying gram-schmidt orthonormalization randomly valued -dimensional vectors. easy data generated distribution ﬁrst sparse vectors utilized sample sizes respectively. experiment ﬁrst calculated sparse method |ˆvt |ˆvt satisﬁed method considered success. proposed redac method together conventional current sparse methods including spca dspca pathspca spca-rsvdl spca-rsvdl spca-rsvdscad empca gpowerl gpowerl gpowerlm gpowerlm alspca implemented success times four series experiments recorded summarized respectively. results listed table advantage proposed redac algorithm easily observed table speciﬁc method always attains highest second highest success times compared utilized methods four series experiments. considering alspca method comparable method experiments utilizes strict constraints orthogonality output redac method utilize prior ground-truth information data capability proposed method sparse calculation prominently veriﬁed. section evaluate performance proposed redac method real data sets including pitprops colon data. quantitative criteria employed performance assessment. designed viewpoints reconstruction-error-minimization data-variance-maximization respectively corresponding original formulations sparse problem. loading matrix input data reconstructed xv−vt aforementioned. thus variance ˆx). percentage reconstructed data computed reconstructed data original calculated pitprops data consisting observations measured variables ﬁrst introduced jeﬀers show diﬃculty interpreting pcs. data commonly utilized examples sparse evaluation thus also employed testify eﬀectiveness proposed redac method. comparison methods include spca dspca pathspca spca-rsvdl spca-rsvdl spcarsvdscad empca gpowerl gpowerl gpowerlm gpowerlm alspca utilized method sparse extracted pitprops data diﬀerent cardinality settings ----- ----- ----- respectively. experiment values deﬁned above calculated results summarized table figure shows curves attained diﬀerent sparse methods experiments illumination. noted gpowerlm gpowerlm alspca methods employ block methodology introduced introduction paper calculate sparse cannot sequentially derive diﬀerent numbers sparse preset cardinality settings. thus results methods reported table calculated total sparse cardinalities respectively included figure seen table cardinality settings ﬁrst proposed redac method always achieves lowest highest values among competing methods. means redac method advantageous reconstruction-error-minimization data-variance-maximization viewpoints. furthermore figure easy superiority redac method. speciﬁc diﬀerent number extracted sparse components proposed redac method always smallest values largest values compared utilized sparse methods experiments. substantiates eﬀectiveness proposed redac method reconstruction-error-minimization data-variance-maximization views. colon data consists tissue samples gene expression proﬁles genes extracted micro-array data. typical data high-dimension low-sample-size property figure tendency curves respect number extracted sparse attained diﬀerent sparse methods pitprops data. three cardinality settings extracted sparse utilized including ----- ----- -----. always employed sparse methods extracting interpretable information high-dimensional genes. thus adopt data evaluation. speciﬁc sparse nonzero loadings calculated diﬀerent sparse methods including spca pathspca spca-rsvdl spca-rsvdl spca-rsvdscad empca gpowerl gpowerl gpowerlm gpowerlm alspca respectively. performance compared table figure terms respectively. noted dspca method also tried cannot terminated reasonable time experiment thus omit result table. besides carefully tuned parameters gpower methods sparse total cardinality around similar total nonzero elements number utilized sparse methods cannot sparse loading sequences cardinality expected. results thus demonstrated figure table easy proposed redac method achieves lowest highest values compared employed sparse methods. figure demonstrates number extracted sparse increases advantage redac method tends dominant methods respect criteria. substantiates eﬀectiveness proposed method implies potential usefulness applications various interpretable components. testify performance proposed redac method nonnegative sparse extraction. comparison existing methods nonnegative sparse nspca nonnegative empca also employed. table performance comparison success times attained nspca n-empca redac synthetic experiments diﬀerent sample sizes. best results highlighted bold. rest generated applying gram-schmidt orthonormalization randomly valued -dimensional vectors. corresponding eigenvalues preset respectively. four series experiments designed data sets generated sample sizes respectively. experiment ﬁrst calculated conventional nspca n-empca redac methods respectively. success times calculated similar introduced section utilized method series experiments recorded listed table table seen redac method achieves highest success rates experiments. advantage proposed redac method nonnegative sparse calculation compared utilized methods thus veriﬁed experiments. colon data utilized nonnegative sparse calculation. nspca n-empca methods adopted competing methods. since nspca method cannot directly pre-specify cardinalities extracted sparse thus ﬁrst apply nspca colon dinalities nonnegative sparse attained method preset n-empca redac methods fair comparison. sparse computed three methods performance compared table figure terms respectively. expected evident proposed redac method dominates viewpoints. table observe method achieves lowest highest extracted nonnegative sparse utilized methods. furthermore figure shows method advantageous compared methods preset number extracted sparse advantage tends signiﬁcant sparse calculated. eﬀectiveness proposed method nonnegative sparse calculation thus veriﬁed. conventional nspca n-empca methods applied problem performance compared application. employed data cbcl face dataset downloaded http//cbcl.mit.edu/software-datasets/facedata.html. data consists aligned face images non-face images resodepicted figure nonnegative sparse obtained redac method clearly exhibit interpretable features underlying faces compared utilized methods e.g. ﬁrst calculated method clearly demonstrate eyebrows eyes cheeks mouth chin faces respectively. advantage proposed method veriﬁed quantitatively smallest largest values among employed methods experiment shown table eﬀectiveness redac method thus substantiated. classiﬁcation data follows. first randomly choose face images non-face images cbcl face dataset take training data rest images testing data. extract utilizing nspca n-empca redac methods training respectively. projecting training data onto corresponding obtained four methods respectively ﬁtting linear logistic regression model dimension-reduced data classiﬁer testing. classiﬁcation accuracy classiﬁer obtained testing data computed results reported table table classiﬁcation accuracy attained directly ﬁtting model original training data testing original testing data also listed easy comparison. table clear proposed redac method attains best performance among implemented methods accurately recognizing face images non-face images testing data. implies potential usefulness proposed method real applications. paper proposed novel recursive divide-and-conquer method sparse problem. main methodology proposed method decompose original large sparse problem series small sub-problems. proved decomposed sub-problems closed-form global solution thus easily solved. recursively solving small sub-problems original sparse problem always eﬀectively resolved. also shown method converges stationary point problem easily extended sparse problems certain constraints nonnegative sparse problem. extensive experimental results validated method outperforms current sparse methods reconstruction-error-minimization data-variance-maximization viewpoints. many interesting investigations still worthy explored. example reformat square l-norm error sparse model l-norm robustness model always improved heavy noise outlier cases model correspondingly diﬃcult solve. adopting similar redac methodology however problem decomposed series much simpler sub-problems expected much easily solved original model. besides although proved convergence redac method know result global optimum problem. stochastic global optimization techniques simulated annealing evolution computation methods combined proposed method improve performance. also real applications proposed method current research. sharp rattray dense message passing sparse principal component analysis proceedings international conference artiﬁcial intelligence statistics archambeau bach sparse probabilistic projections koller schuurmans bengio bottou advances neural information processing systems press cambridge moghaddam weiss avidan spectral bounds sparse exact greedy algorithms weiss sch¨olkopf platt advances neural information processing systems press cambridge witten tibshirani hastie penalized matrix decomposition applications sparse principal components canonical correlation analysis biostatistics zhang ghaoui large-scale sparse principal component analysis application text data shawe-taylor zemel bartlett pereira weinberger advances neural information processing systems press cambridge cichocki zdunek phan amari nonnegative matrix tensor factorizations applications exploratory multi-way data analysis blind source separation wiley alon barkai notterman gish ybarra mack levine broad patterns gene expression revealed clustering analysis tumor normal colon tissues probed oligonucleotide arrays cell biology problem parallel k-dimensional subspace ﬁrst largest absolute value located. also constraint easy deduce optimal solution optimization problem elements nonnegative evidently satisﬁed. otherwise element denoted i-th element negative corresponding element nonzero pick nonnegative element denoted", "year": 2012}