{"title": "A Bayesian Perspective on Generalization and Stochastic Gradient Descent", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We consider two questions at the heart of machine learning; how can we predict if a minimum will generalize to the test set, and why does stochastic gradient descent find minima that generalize well? Our work responds to Zhang et al. (2016), who showed deep neural networks can easily memorize randomly labeled training data, despite generalizing well on real labels of the same inputs. We show that the same phenomenon occurs in small linear models. These observations are explained by the Bayesian evidence, which penalizes sharp minima but is invariant to model parameterization. We also demonstrate that, when one holds the learning rate fixed, there is an optimum batch size which maximizes the test set accuracy. We propose that the noise introduced by small mini-batches drives the parameters towards minima whose evidence is large. Interpreting stochastic gradient descent as a stochastic differential equation, we identify the \"noise scale\" $g = \\epsilon (\\frac{N}{B} - 1) \\approx \\epsilon N/B$, where $\\epsilon$ is the learning rate, $N$ the training set size and $B$ the batch size. Consequently the optimum batch size is proportional to both the learning rate and the size of the training set, $B_{opt} \\propto \\epsilon N$. We verify these predictions empirically.", "text": "consider questions heart machine learning; predict minimum generalize test stochastic gradient descent minima generalize well? work responds zhang showed deep neural networks easily memorize randomly labeled training data despite generalizing well real labels inputs. show phenomenon occurs small linear models. observations explained bayesian evidence penalizes sharp minima invariant model parameterization. also demonstrate that holds learning rate ﬁxed optimum batch size maximizes test accuracy. propose noise introduced small mini-batches drives parameters towards minima whose evidence large. interpreting stochastic gradient descent stochastic differential equation identify noise scale trained deep convolutional networks imagenet cifar achieving excellent accuracy training test sets. took input images randomized labels found networks unable generalize test still memorized training labels. claimed results contradict learning theory although claim disputed nonetheless results question; models assign arbitrary labels training work well practice? meanwhile keskar observed hold learning rate ﬁxed increase batch size test accuracy usually falls. striking result shows improving estimate full-batch gradient harm performance. goyal observed linear scaling rule batch size learning rate deep resnet hoffer proposed square root rule theoretical grounds. many authors suggested broad minima whose curvature small generalize better sharp minima whose curvature large indeed dziugaite argued results zhang understood using nonvacuous pac-bayes generalization bounds penalize sharp minima keskar showed stochastic gradient descent ﬁnds wider minima batch size reduced. however dinh challenged interpretation arguing curvature minimum arbitrarily increased changing model parameterization. work show results zhang unique deep learning; observe phenomenon small over-parameterized linear model. demonstrate phenomenon straightforwardly understood evaluating bayesian evidence favor model penalizes sharp minima invariant model parameterization. integrates stochastic differential equation whose noise scale \u0001n/b learning rate training size batch size. noise drives away sharp minima therefore optimal batch size maximizes test accuracy. optimal batch size proportional learning rate training size. describe bayesian model comparison section section replicate observations zhang linear model show explained bayesian evidence. section show optimum batch size maximizes test accuracy section derive scaling rules optimum batch size learning rate training size momentum coefﬁcient. throughout work generalization refers test accuracy small large batch training accuracy training test sets. bayesian model comparison ﬁrst applied neural networks mackay provide brief tutorial here since theory central remainder paper. simplicity ﬁrst consider classiﬁcation model single parameter training inputs training labels infer posterior probability distribution parameter applying bayes theorem denotes regularized cross entropy cost function regularization coefﬁcient. value minimizes cost function lies maximum posterior. predict unknown label input compute integral however integrals dominated region near since smooth usually approximate minimized wish compare different models select best one. probability ratio second factor right prior ratio describes model plausible. avoid unnecessary subjectivity usually meanwhile ﬁrst factor right evidence ratio controls much training data changes prior beliefs. germain showed maximizing evidence minimizes pac-bayes generalization bound. compute evaluate normalizing constant equation notice evidence computed integrating parameters; consequently invariant model parameterization. since integral dominated region near minimum estimate evidence taylor expanding within laplace approximation evidence controlled value cost function minimum logarithm ratio curvature minimum compared regularization constant. thus considered models single parameter; realistic models many parameters |∇∇c|ω determinant hessian denotes number model parameters contribution often called occam factor enforces occam’s razor; models describe data equally well simpler model usually better minima curvature simple parameters ﬁnetuned data. intuitively occam factor describes fraction prior parameter space consistent data. since fraction always less propose approximate equation away local minima performing summation eigenvalues evidence reframed language information theory whereby occam’s factor penalizes amount information model must learn parameters accurately model training data work compare evidence null model assumes labels entirely random assigning equal probability class. unusual model parameters evidence controlled likelihood alone denotes number model classes number training labels. thus evidence ratio evidence ratio favor null model. clearly assign conﬁdence predictions model evidence supports intuition broad minima generalize better sharp minima unlike curvature depend model parameterization. dinh showed increase hessian eigenvalues rescaling parameters must simultaneously rescale regularization coefﬁcients otherwise model changes. since occam’s factor arises ratio effects cancel out. difﬁcult evaluate evidence deep networks cannot compute hessian millions parameters. additionally neural networks exhibit many equivalent minima since permute hidden units without changing model. compute evidence must carefully account degeneracy. argue issues major limitation since intuition build studying evidence simple cases sufﬁcient explain results zhang keskar zhang showed deep neural networks generalize well training inputs informative labels model drastically overﬁt input images labels randomized; perfectly memorizing training set. demonstrate observations unique deep networks let’s consider simpler model; logistic regression. form small balanced training comprising images mnist half true label half true label test also balanced comprising mnist images zeros mnist images ones. tasks. ﬁrst task labels training test sets randomized. second task labels informative matching true mnist labels. since images contain pixels model weights bias. show accuracy model predictions training test sets ﬁgure trained informative labels model generalizes well test long weakly regularized. however model also perfectly memorizes random labels replicating observations zhang deep networks. signiﬁcant improvement model performance figure prediction accuracy mean training margin function regularization coefﬁcient logistic regression trained random informative labels inputs. weakly regularized model generalizes well informative labels memorizes random labels. observed regularization coefﬁcient increases. completeness also evaluate mean margin training examples decision boundary. random informative labels margin drops signiﬁcantly reduce regularization coefﬁcient. weakly regularized mean margin roughly larger informative labels random labels. consider ﬁgure plot mean cross-entropy model predictions evaluated training test sets well bayesian evidence ratio deﬁned previous section. looking ﬁrst random label experiment ﬁgure cross-entropy training vanishes model weakly regularized cross-entropy test explodes. model make random predictions extremely conﬁdent predictions. regularization coefﬁcient increased test cross-entropy falls settling crossentropy assigning equal probability classes. consider bayesian evidence evaluate training set. evidence ratio large positive model weakly regularized indicating model exponentially less plausible assigning equal probabilities class. regularization parameter increased evidence ratio falls always positive indicating model never expected generalize well. consider ﬁgure again training cross-entropy falls zero model weakly regularized test cross-entropy high. even though model makes accurate predictions predictions overconﬁdent. regularization coefﬁcient increases test cross-entropy falls indicating model successfully generalizing test set. consider bayesian evidence. evidence ratio large positive model weakly regularized regularization coefﬁcient increases evidence ratio drops zero indicating model exponentially plausible assigning equal probabilities class. increase regularization evidence ratio rises zero test cross-entropy rises test cross-entropy bayesian evidence strongly correlated minima regularization strength. bayesian model comparison explained results logistic regression. meanwhile krueger showed largest hessian eigenvalue also increased training random labels deep networks implying evidence falling. conclude bayesian model comparison quantitatively consistent results zhang linear models compute evidence qualitatively consistent results deep networks cannot. dziugaite recently demonstrated results zhang also understood minimising pac-bayes generalization bound penalizes sharp minima. showed generalization strongly correlated bayesian evidence weighted combination depth minimum breadth consequently bayesians often isotropic gaussian noise gradient appendix show drives parameters towards broad minima whose evidence large. noise introduced small batch training isotropic covariance matrix function parameter values empirically keskar found similar effects driving away sharp minima. paper therefore proposes bayesian principles also account generalization whereby test accuracy often falls batch size increased since gradient drives towards deep minima noise drives towards broad minima expect test performance show peak optimal batch size balances competing contributions evidence. unable observe generalization linear models instead consider shallow neural network hidden units relu hidden activations trained mnist without regularization. momentum parameter unless otherwise stated constant learning rate depend batch size decay training. furthermore train images selected random mnist training set. enables compare small batch full batch training. emphasize trying achieve optimal performance study simple model shows generalization small large batch training. ﬁgure exhibit evolution test accuracy test cross-entropy training. small batches composed images randomly sampled training set. looking ﬁrst ﬁgure small batch training takes longer converge thousand gradient updates clear generalization model accuracy emerges small large training batches. consider ﬁgure test cross-entropy small batch training lower figure test accuracy function batch size range learning rates performance peak shifts right increase overall performance falls best observed batch size proportional learning rate across orders magnitude. training; cross-entropy small large training batches increasing indicative over-ﬁtting. models exhibit minimum test cross-entropy although different numbers gradient updates. intriguingly show appendix generalization small large batch training shrinks signiﬁcantly introduce regularization. focus test accuracy ﬁgure exhibit training curves range batch sizes model cannot train batch size ﬁgure plot mean test accuracy training steps. clear peak emerges indicating indeed optimum batch size maximizes test accuracy consistent bayesian intuition. results keskar focused decay test accuracy optimum batch size. showed test accuracy peaks optimal batch size holds hyper-parameters constant. argued peak arises tradeoff depth breadth bayesian evidence. however batch size controls tradeoff underlying scale random ﬂuctuations dynamics. identify noise scale derive three scaling rules predict optimal batch size depends learning rate training size momentum coefﬁcient. gradient update figure test accuracy function batch size range training sizes. reduce noise average curve experiments. performance peak shift right increase size training set. unsurprisingly overall model performance also improves. best observed batch size proportional size training estimated gradient evaluated mini-batch. expected gradient single δij. matrix describing gradient covariances function current parameter values. adopt central limit theorem model gradient error gaussian random noise easy show typically continue interpret equation discrete update stochastic differential equation continuous variable represents noise constant controls scale random ﬂuctuations dynamics. relate differential ηdt. finally measure equate variance gradient update variance equation \u0001n/b. noise scale falls batch rearranging noise scale although treatment holds near local minima covariances stationary. analysis holds throughout training necessary since keskar found beneﬁcial inﬂuence noise pronounced start training. vary learning rate training size keep noise scale ﬁxed implies bopt ﬁgure plot test accuracy function batch size training steps range learning rates. exactly predicted peak moves right increases. additionally peak test accuracy achieved given learning rate begin fall indicating signiﬁcant discretization error integrating stochastic differential equation point. point discretization error begins dominate peak test accuracy falls rapidly. ﬁgure plot best observed batch size function learning rate observing clear linear trend bopt error bars indicate distance best observed batch size next batch size sampled experiments. figure test accuracy function batch size range momentum coefﬁcients. expected peak moves right momentum coefﬁcient increases. best observed batch size range momentum coefﬁcients. green curve exhibits scaling rule. scaling rule allows increase learning rate loss test accuracy increase computational cost simply simultaneously increasing batch size. exploit increased parallelism across multiple gpus reducing model training times similar scaling rule independently proposed jastrzebski chaudhari soatto although neither work identiﬁes existence optimal noise scale. number authors proposed adjusting batch size adaptively training balles proposed linearly coupling learning rate batch size within framework. smith show empirically decaying learning rate training increasing batch size training equivalent. ﬁgure exhibit test accuracy function batch size range training sizes steps again peak shifts right training size rises although generalization becomes less pronounced training size increases. ﬁgure plot best observed batch size function training size; observing another linear trend bopt scaling rule could applied production models progressively growing batch size training data collected. expect production datasets grow considerably time consequently large batch training likely become increasingly common. finally appendix extend analysis momentum identifying noise scale denotes momentum coefﬁcient. notice reduces noise scale conventional obtain additional scaling rule bopt scaling rule predicts optimal batch size increase momentum coefﬁcient increased. ﬁgure plot test performance function batch size gradient updates range momentum coefﬁcients. ﬁgure plot best observed batch size function momentum coefﬁcient results scaling rule above; obtaining remarkably good agreement. propose simple heuristic tuning batch size learning rate momentum coefﬁcient appendix like deep neural networks linear models generalize well informative labels memorize random labels inputs. observations explained bayesian evidence composed cost function occam factor. occam factor penalizes sharp minima invariant changes model parameterization. mini-batch noise drives away sharp minima therefore optimum batch size maximizes test accuracy. interpreting discretization stochastic differential equation predict optimum batch size scale linearly learning rate training size bopt derive additional scaling rule bopt optimal batch size momentum coefﬁcient. verify scaling rules empirically discuss implications. thank pieter-jan kindermans prajit ramachandran jascha sohl-dickstein shlens kevin murphy samy bengio yasaman bahri saeed saremi helpful comments manuscript. gintare karolina dziugaite daniel roy. computing nonvacuous generalization bounds deep neural networks many parameters training data. arxiv preprint arxiv. priya goyal piotr doll´ar ross girshick pieter noordhuis lukasz wesolowski aapo kyrola andrew tulloch yangqing kaiming accurate large minibatch training imagenet hour. arxiv preprint arxiv. geoffrey hinton drew camp. keeping neural networks simple minimizing description length weights. proceedings sixth annual conference computational learning theory nitish shirish keskar dheevatsa mudigere jorge nocedal mikhail smelyanskiy ping peter tang. large-batch training deep learning generalization sharp minima. arxiv preprint arxiv. david krueger nicolas ballas stanislaw jastrzebski devansh arpit maxinder kanwal tegan maharaj emmanuel bengio asja fischer aaron courville. deep nets don’t learn memorization. iclr workshop regularized summed cost function shown section main text. well known sample posterior simulating overdamped langevin equation described stochastic differential equation continuous variable describes gaussian noise mean variance iδ). matrix denotes identity temperature. notice langevin equation extremely similar stochastic differential equation discussed section main text. indeed gradient covariances stationary proportional identity would integrate overdamped langevin equation temperature proportional noise scale probability sampling particular parameter vector langevin equation e−c/t obtain posterior samples order draw posterior samples practice repeatedly integrate langevin equation ﬁnite step denotes gaussian random variable mean variance \u0001i/n introduces isotropic noise gradient update described section main text. note that since denotes summed cost function chose scale step size training size also matches treatment section main text. larger step size greater discretization error sufﬁciently small iterate equation sufﬁciently many times obtain valid samples posterior. since probability sampling given parameter vector proportional posterior probability sampling parameter vector belonging given local minimum proportional integral posterior bowl attraction surrounds minimum. discussed evidence dominated contributions integral near local minima. convex model minimum; allows accurately estimate model evidence. meanwhile non-convex models many minima instead deﬁne evidence local evidences favor minimum since combined bowls attraction minima perfectly tile entire parameter space equations equivalent. meanwhile equating equations that performs bayesian posterior sampling probability sampling parameter vector local minimum proportional evidence favor minimum. demonstrates bayesian posterior samples biased favor local minima whose evidence large explains single posterior sample often achieves lower test error cost function minimum figure mean test accuracy mean test cross-entropy regularized model training. full batch training takes longer converge achieves similar performance long times. noise inherent small batch training causes performance ﬂuctuate. training. excluding regularization parameter experiments identical ﬁgure surprise regularized full batch training took longer converge small batch training. another surprise regularization signiﬁcantly reduced size generalization gap. large batch regularized training achieves slightly lower test accuracy unregularized small batch training also achieves lower test cross-entropy. test cross-entropy regularized models degrade many gradient updates removing need early stopping. section main text approximated difference full batch gradient mini-batch gradient estimate gaussian random variable. enabled derive scaling rules veriﬁed empirically. motivated assumption reference central limit theorem states gradient error tend towards gaussian noise long distribution gradients individual training examples heavy tails. practice neither inﬁnite gradient distribution heavy tailed especially gradients sparse. nonetheless central limit theorem tends surprisingly robust practice consequently widely used. beyond scope work perform thorough study gradient noise distribution deep networks. however brief proof principle present distribution gradient immediately random initialization ﬁgure shallow neural network discussed sections main text. ﬁgure present distribution individual training examples gradient single matrix element softmax output layer chosen randomly. distribution double peaked clearly gaussian. however ﬁgure plot distribution gradient matrix element averaged randomly sampled mini-batches images single peak emerges distribution still slightly skewed clearly already approaching gaussian limit. conclude gaussian approximation likely reasonable commonly used mini-batch sizes. damping coefﬁcient describes gaussian noise whose statistics before coefﬁcient describes scale random ﬂuctuations dynamics describes gradient covariances parameters. include factor noise variance satisfy ﬂuctuation-dissipation theorem states vary damping coefﬁcient without changing probability sampling particular conﬁguration parameters limit proportionally increase noise variance. relate langevin equation usual momentum equations ﬁrst re-express coupled ﬁrst order differential equations denotes error gradient update. discussed main text approximate error gaussian noise statistics equations match equations step size momentum parameter finally equate noise setting solve noise scale obtain observed main text wish keep scale random ﬂuctuations constant scale batch size also predict additional scaling relation batch size momentum parameter note also interpret effective learning rate. propose simple heuristic tuning batch size learning rate momentum parameter; order maximize test accuracy batch size note worthwhile expects retrain model many times. learning rate momentum coefﬁcient experiments range batch sizes logarithmic scale identify optimal batch size maximizes validation accuracy. training stable reduce learning rate repeat. repeatedly increase batch size factor scaling learning rate validation accuracy starts fall. repeatedly increase batch size factor scaling momentum coefﬁcient either validation accuracy falls batch size reaches limits hardware. believe simple procedure increase test accuracy reduce cost tuning hyperparameters signiﬁcantly reduce ﬁnal number gradient updates required train model.", "year": 2017}