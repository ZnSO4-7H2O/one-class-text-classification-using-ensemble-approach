{"title": "Optimal Binary Autoencoding with Pairwise Correlations", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We formulate learning of a binary autoencoder as a biconvex optimization problem which learns from the pairwise correlations between encoded and decoded bits. Among all possible algorithms that use this information, ours finds the autoencoder that reconstructs its inputs with worst-case optimal loss. The optimal decoder is a single layer of artificial neurons, emerging entirely from the minimax loss minimization, and with weights learned by convex optimization. All this is reflected in competitive experimental results, demonstrating that binary autoencoding can be done efficiently by conveying information in pairwise correlations in an optimal fashion.", "text": "formulate learning binary autoencoder biconvex optimization problem learns pairwise correlations encoded decoded bits. among possible algorithms information ﬁnds autoencoder reconstructs inputs worst-case optimal loss. optimal decoder single layer artiﬁcial neurons emerging entirely minimax loss minimization weights learned convex optimization. reﬂected competitive experimental results demonstrating binary autoencoding done efﬁciently conveying information pairwise correlations optimal fashion. encoding dimension algorithm encodes data example h-dimensional representation decodes back reconstructed example using small amount additional memory evaluated quality reconstruction cross-entropy loss commonly used compare vectors. good autoencoder learns compress data bits reconstruct loss. loss squared reconstruction error goal compress data often accomplished principal component analysis projects input data eigenvectors covariance matrix baldi hornik eigenvectors constitute real values additional memory needed decode compressed data back reconstructions linear combinations eigenvectors. crucially total additional memory depend amount data making applicable data abundant. paper considers similar problem except using bit-vector data cross-entropy reconstruction loss. since compressing samples i.i.d. -bit data h-bit encodings natural approach remember pairwise statistics average correlations pairs bits encoding decoding constituting much additional memory eigenvectors used pca. decoder uses along h-bit encoded data produce -bit reconstructions. decoding function identical form used standard binary autoencoder hidden layer cross-entropy reconstruction loss. speciﬁcally decoding output logistic sigmoid artiﬁcial neuron encoded bits learned weights form emerges uniquely optimal decoding function assumed part explicit model. both situation still admits natural efﬁcient optimization algorithm loss alternately minimized held ﬁxed. algorithm practical learning incrementally minibatches data stochastic optimization setting. encodings allowed randomized represented values instead values e.g. data also allowed randomized loses hardly generality reasons discussed later write columns representing data. rows written also consider correlation encoding decoded data i.e. whose rows columns respectively write indexing clear context. alluded earlier loss incurred example cross-entropy example reconstruction expectation randomness deﬁning deﬁnitions autoencoding problem address precisely stated tasks encoding decoding. share side information goal perform steps achieve best possible guarantee reconstruction loss assumptions. written zero-sum game autoencoding algorithm seeking minimize loss adversary playing encodings reconstructions autoencoding algorithm’s best strategy parts. first optimal decoding encodings given section then resulting optimal reconstruction function outline best encoding procedure i.e. ﬁnds lead best reconstruction section combining ideas yields autoencoding algorithm section implementation interpretation speciﬁed. discussion related work section followed extensions section experiments section precisely classical autoencoder learned weight vector reconstruction expressed logistic function wv-weighted combination encoded bits logistic artiﬁcial neuron weights weight vectors learned convex optimization despite nonconvexity transfer functions. solve minimax problem optimal reconstructions played minimizing player written ˜x∗. theorem deﬁne bitwise slack function convex w.r.t. minimizing weights tells optimization problem ﬁnding minimax optimal reconstructions extremely convenient several respects. learning problem decomposes bits optimizing bitwise decoding reducing solving weight vector slack function. given weights optimal reconstruction example speciﬁed layer logistic sigmoid artiﬁcial neurons encoded bits hereafter write matrix decoding weights rows {wv}v optimal decoding weights matrix rows computed optimal decoding function previous section given switch perspectives encoder seeks compress input data encoded representations seek ensure lowest worst-case reconstruction loss decoding; recall encoder given therefore terms observe convenient deﬁne bitwise using thm. substituting feature distortion respect example encoding discussion best given decoding written solves minimization observe encoding function enc; efﬁciently computed desired precision since feature distortion convex lipschitz error reached linear-time ﬁrst-order optimization iterations. note encodings need bits e.g. unconstrained instead; proof thm. assumes structure them. ultimate goal minimize worst-case reconstruction loss. seen convex encoding decoding parameters ﬁxed minimizing respect other. suggests learning algorithm alternately performs steps ﬁnding encodings minimize ﬁxed ﬁnding decoding parameters given algorithm derivation encoding decoding functions involves model assumptions using minimax structure pairwise statistics algorithm allowed remember. nevertheless coders still learned implemented efﬁciently. decoding convex optimization dimensions done parallel relatively easy solve parameter regime primary interest data abundant similarly encoding also convex optimization problem dimensions. data examples instead sampled minibatches size encoded parallel minibatch sampled start epoch number examples essentially limited number compressed representations memory. paper stated results transductive setting data given together priori assumptions whatsoever made interdependences features. however pc-ae operates much efﬁciently might suggest. crucially encoding decoding tasks depend average function solved stochastic optimization methods ﬁrst-order gradient information like variants stochastic gradient descent remarkable minimax optimal encoding decoding efﬁciently learned methods scale computationally note result steps involves outputs coupled together complex ways. efﬁcient implementation ﬁrst-order methods turns manipulate intermediate gradient-related quantities facile interpretations. details appendix noted previously objective function optimization biconvex. means broad conditions alternating minimization algorithm specify instance alternating convex search shown literature converge broad conditions guaranteed converge global optimum iteration monotonically decrease objective function. light introductory discussion properties rate convergence would interesting compare stochastic optimization algorithms converge efﬁciently broad conditions shamir basic game used assumed perfect knowledge pairwise correlations leading equality constraints makes sense pc-ae encoding phase epoch gives exact decoding phase. however stochastic settings denoising autoencoders necessary relax constraint. relaxed corresponding weights convex optimization used regularization leads provably better generalization often practical e.g. encourage sparsity. pc-ae experiments paper. first foremost posit explicit decision rule avoid optimizing highly non-convex decision surface traversed traditional autoencoding algorithms learn backpropagation. decoding function given encodings single layer artiﬁcial neurons minimax structure problem minimizing worst-case loss. differs reasoning typically used neural work loss negative log-likelihood joint probability assumed follow form speciﬁed logistic artiﬁcial neurons weights. instead interpret loss usual direct predicted probability data given visible bits avoid assumptions decision rule even dependence score). justiﬁcation artiﬁcial neurons minimax optimal decision rules given information pairwise correlations distinctive contributions note assumptions whatsoever form encoding decoding except memory used decoding. restriction necessary rule autoencoder memorizing data typically expressed positing model class compositions artiﬁcial neuron layers. instead impose axiomiatically limiting amount information transmitted scale restrict information used. confers clear theoretical advantage allowing attain strongest robust loss guarantee among possible autoencoders correlations importantly practice avoiding explicit model class means optimize typically non-convex model long central issue backpropagation-based learning methods prior work related spirit attempted avoid convex relaxations including multi-layer optimization various structural assumptions zhang number hidden units varied algorithm bach approach also isolates beneﬁt higher dealing overﬁtting pairwise correlations measured progressively accurately increases. respect follow line research using pairwise correlations model arbitary higher-order structure among visible units rooted early work boltzmann machines smolensky rumelhart mcclelland freund haussler recently theoretical algorithms developed perspective learning correlations units network various assumptions activation function architecture weights deep shallow networks janzamin ensemble aggregation techniques study problems anticipated spirit prior work well discussed length bengio context distributed representations. established single layer logistic artiﬁcial neurons optimal decoder given indirect information data pairwise correlations. claim autoencoders need single-layer architecture worst case. sec. establishes best representations solution convex optimization artiﬁcial neurons involved computing data. unlike decoding function optimal encoding function cannot written explicitly terms artiﬁcial neurons incomparable existing architectures. also encodings optimal given pairwise correlations; training algorithms like backpropagation indirectly communicate knowledge input data derivative composition certainly learn ﬁnal decoding layers outperform ours experiments. framework explore using pairwise correlations hidden visible bits inform learning constraining adversary resulting lagrange parameter weight constraint. weights constitute parameters optimal decoding layer describing fully connected architecture. select correlations used would constrain adversary minimax problem sec. weights would introduced them giving rise sparser architectures. central choices store pairwise correlations minimize worst-case reconstruction loss play similar regularizing role explicit model assumptions autoencoding methods achieve better performance data choices conservative e.g. making distributional assumptions data. perspective architectures layers particularly highly successful ones like convolutional recurrent residual ladder networks rasmus lend autoencoding algorithm power allowing measure nuanced correlations using parameters decreases worst-case loss. applying approach would interesting future work. extending paper’s convenient minimax characterization deep representations empirical success interesting open problem. prior work stacking autoencoders/rbms learning algorithm pc-ae suggest could train deep network alternating forward backward passes. using paper’s ideas forward pass would learn weights layer given previous layer’s activations minimizing slack function backward pass learning activations layer given weights activations next layer convex optimization passes consist successive convex optimizations dictated approach quite distinct backpropagation though loosely resemble wake-sleep algorithm particularly recently autoencoders interest largely many applications beyond compression especially generative uses. directly relevant involve repurposing denoising autoencoders sec. moment matching among hidden visible units generative adversarial network ideas makhzani latter particularly since techniques paper applied binary classiﬁcation outside paper’s scope suggest future extensions approach. make sense another reconstruction loss cross-entropy instance expected hamming distance turns minimax manipulations work broad conditions nearly loss additively decomposes bits cross-entropy does. cases required partial losses monotonically decreasing increasing respectively need even convex. monotonicity natural condition loss measures discrepancy true label holds losses common use. changing partial losses changes structure minimax solution respects altering form transfer function decoding neurons univariate potential well optimized learn decoding weights. otherwise problem remains convex algorithm identical. formal statements general results appendix framework easily applied learn denoising autoencoder uses noise-corrupted data training uncorrupted data evaluation. perspective corresponds leaving learning unchanged using corrupted data learning minimization problem encodings must changed account bias introduced noise algorithm plays given noisy data minimize loss easiest zero-mean noise algorithms completely unchanged change adding noise. another common scenario illustrating technique mask fraction input bits uniformly random masking noise changes pairwise correlation amount optimand must therefore modiﬁed subtracting factor. estimated given even noisy data estimate w.h.p. extrapolating correlation bits left corresponding values section compare approach empirically standard autoencoders hidden layer trained backpropagation. goal simply verify distinct approach competitive reconstruction performance cross-entropy loss. datasets ﬁrst normalized binarized sampling pixel stochastically proportion intensity following prior work choosing binary real-valued encodings pc-ae requires line code project encodings convex optimization updates compute enc. adagrad convex minimizations algorithms; observed performance sensitive choice optimization method explained approach’s convexity. compare basic single-layer trained adam method default parameters kingma models like variational autoencoders shown optimize reconstruction loss comparably general autoencoding architectures hidden units algorithms binary unconstrained real-valued encodings; respective uses logistic relu transfer functions encoding neurons. results table reconstruction performance pc-ae indicates encode information well using pairwise correlations. loss become extremely raised giving capacity encode information. performance marginally better binary hidden units unconstrained ones accordance spirit derivations. also learning decoding layer sec. encoded representation motivated fact establishes decoding method worst-case optimal given results signiﬁcantly worse alone datasets reﬂects ae’s backprop training propagating information data beyond pairwise correlations non-convex function compositions however cost difﬁcult optimize. representations learned function pc-ae quite different capture much pairwise correlation information used decoding layer worst-case optimal fashion. attempt visually depict differences representations fig. figure random test images omniglot. middle bottom rows reconstructions pc-ae binary hidden units. difference quality particularly noticeable columns. discussed sec. claim method always achieve best empirical reconstruction loss even among single-layer autoencoders. would like make encoding function quicker compute well. believe paper’s results especially high illustrate potential using pairwise correlations autoencoding approach learning encode alternating convex minimization extremely strong worst-case robustness guarantees. figure three rows reconstructions random test images mnist fig. pc-ae achieves loss here fourth ﬁfth rows visualizations hidden units pc-ae respectively. possible visualize pc-ae encoding units image maximally activates them commonly done form function depends lacks explicit encoding weights. hidden unit depicted visible decoding encoded representation \"on\" bits \"off.\" grateful jack berkowitz sanjoy dasgupta yoav freund helpful discussions; daniel akshay krishnamurthy instructive examples; gary cottrell enjoyable chats. sanjeev arora aditya bhaskara rong tengyu provable bounds learning deep representations. proceedings international conference machine learning peter bartlett. sample complexity pattern classiﬁcation neural networks size weights important size network. ieee transactions information theory yoshua bengio nicolas roux pascal vincent olivier delalleau patrice marcotte. convex neural networks. advances neural information processing systems yoshua bengio aaron courville pierre vincent. representation learning review perspectives. pattern analysis machine intelligence ieee transactions yoshua bengio guillaume alain pascal vincent. generalized denoising auto-encoders generative models. advances neural information processing systems yuri burda roger grosse ruslan salakhutdinov. importance weighted autoencoders. international conference learning representations arxiv preprint arxiv.. yann dauphin razvan pascanu caglar gulcehre kyunghyun surya ganguli yoshua bengio. identifying attacking saddle point problem high-dimensional non-convex optimization. advances neural information processing systems yoav freund david haussler. unsupervised learning distributions binary vectors using layer networks. advances neural information processing systems jochen gorski frank pfeuffer kathrin klamroth. biconvex sets optimization biconvex functions survey extensions. mathematical methods operations research majid janzamin hanie sedghi anima anandkumar. beating perils non-convexity guaranteed training neural networks using tensor methods. arxiv preprint arxiv. antti rasmus mathias berglund mikko honkala harri valpola tapani raiko. semi-supervised learning ladder networks. advances neural information processing systems david rumelhart james mcclelland. parallel distributed processing explorations microstructure cognition. vol. foundations. computational models cognition perception cambridge press smolensky. information processing dynamical systems foundations harmony theory. parallel distributed processing explorations microstructure cognition vol. press pascal vincent hugo larochelle yoshua bengio pierre-antoine manzagol. extracting composing robust features denoising autoencoders. proceedings international conference machine learning pascal vincent hugo larochelle isabelle lajoie yoshua bengio pierre-antoine manzagol. stacked denoising autoencoders learning useful representations deep network local denoising criterion. journal machine learning research addition mnist used preprocessed version omniglot dataset burda split caltech- silhouettes dataset small notmnist dataset. notmnist comes without predeﬁned split displayed results -fold cross-validation. non-binarized versions datasets resulted nearly identical pc-ae performance would expected derivation using expected pairwise correlations. evaluate types autoencoders regularize otherwise trained direct reconstruction loss minimization. also shown performance standard convolutional autoencoder coder) somewhat better standard autoencoder still outperformed pc-ae datasets. deeper architecture could quite possibly achieve superior performance greater number channels information propagated makes fair comparison fully-connected approach difﬁcult. consider extension pc-ae approach architectures fascinating future work. bound worst-case loss invariably quite tight shown fig. similar results found datasets. consistent conclusions nature pc-ae representations conveying almost exactly information available pairwise correlations. figure actual reconstruction loss real data slack function value adagrad optimization learn using optimal monotonicity expected since convex optimization. objective function value theoretically upper-bounds actual loss practically tracks nearly perfectly. visualization mnist fig. showing even hidden units enough information pairwise correlations pc-ae learn sensible embedding. also include pictures autoencoders’ reconstructions visualizations hidden units fig. optimization proceeds gradient descent using step size found using line search. note since objective function convex optimum leads optimal residuals column null space maps residual vectors encoded space. conclude although compression perfect general) column orthogonal decoding weights equilibrium towards convex minimization problem guaranteed stably converge. second term understood hallucinated\" pairwise correlations bits encoded examples bits decodings current weights therefore interpreted hallucinated correlations written residual correlations since slack function convex optimum leads hallucinated correlations limit reached optimization algorithm many iterations. paper represent bit-vector data randomized randomizing data relaxes constraints adversary game play; worst working upper bound worst-case loss instead exact minimax loss itself erring conservative side. brieﬂy justify bound essentially tight also empirically paper’s experiments. formulation section information data pairwise correlations encoding units. data abundant w.h.p. correlations close expected values data’s internal randomization representing continuous values w.h.p. results therefore solutions effectively allowing adversary play bit’s conditional probability ﬁring rather binary realization probability. allows apply minimax theory duality considerably simplify problem convex optimization would otherwise nonconvex computationally hard fact using information data expected pairwise correlations makes possible. uses minimax theorem applied linear programming objective function linear note weights introduced merely lagrange parameters pairwise correlation constraints model assumptions. strategy solves inner maximization simply match signs precisely analogous argument holds putting cases together shown form summand also shown dependence minimizer outer minimization completes proof. proof. proof adapts proof theorem following result regularization balsubramani freund straightforward way; describe here. break constraint one-sided constraints i.e. suppose subtracting affect value always decreases therefore always decreases objective function. therefore w.l.o.g. assume deﬁning term replaced proceeding proof theorem gives result. using recent techniques balsubramani freund section extend theorem larger class reconstruction losses binary autoencoding cross-entropy loss special case. redeﬁning partial losses functions satisfying following monotonicity conditions. assumption interval decreasing increasing twice differentiable. assumption natural includes many non-convex losses detailed discussion much applies bitwise here). additive decomposability bits assumptions make reconstruction loss ˜x). latter decomposability assumption often natural loss log-likelihood tantamount conditional independence visible bits given hidden ones. given reconstruction loss deﬁne increasing function exists increasing inverse using broaden deﬁnition potential function proof nearly identical main theorem balsubramani freund proof essentially recapitulated additive decomposability loss algebraic manipulations identical proof theorem general deﬁnitions rewrite full here. notable special case interest hamming loss reconstructions allowed randomized binary values. case sigmoid used decoding neuron clipped linearity output reconstructions could restricted pairwise correlations i.e. option impose restrictions instead existing constraints leaving unrestricted. however spirit paper means indirectly conveying information decoder decoded. another option restrict possible useful propagating correlation information layers deeper architectures learning minimax solution conveniently clean structure pc-ae derivation. similar vein could restrict encoding phase using changed phase better conform true data tactic ﬁxes optimization spirit paper’s approach. also performed signiﬁcantly worse experiments.", "year": 2016}