{"title": "Causal Learning and Explanation of Deep Neural Networks via Autoencoded  Activations", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Deep neural networks are complex and opaque. As they enter application in a variety of important and safety critical domains, users seek methods to explain their output predictions. We develop an approach to explaining deep neural networks by constructing causal models on salient concepts contained in a CNN. We develop methods to extract salient concepts throughout a target network by using autoencoders trained to extract human-understandable representations of network activations. We then build a bayesian causal model using these extracted concepts as variables in order to explain image classification. Finally, we use this causal model to identify and visualize features with significant causal influence on final classification.", "text": "deep neural networks complex opaque. enter application variety important safety critical domains users seek methods explain output predictions. develop approach explaining deep neural networks constructing causal models salient concepts contained cnn. develop methods extract salient concepts throughout target network using autoencoders trained extract humanunderstandable representations network activations. build bayesian causal model using extracted concepts variables order explain image classiﬁcation. finally causal model identify visualize features signiﬁcant causal inﬂuence ﬁnal classiﬁcation. introduction artiﬁcial intelligence -enabled systems playing increasingly prominent role society signiﬁcantly disrupting commercial government organizations operate. particular rate advances machine learning using deep neural networks staggering enabling ai-enabled systems master complex tasks including autonomous driving even predicting census data incredible advances however come signiﬁcant cost dnns complex opaque derive power millions parameters must trained large data sets. thus understanding explaining came particular conclusion difﬁcult task. ai-enabled systems become prevalent lives lack explainability intepretability comes serious societal consequences. example used predict recidivism criminal justice system interventions child welfare system cancer radiology images lack explainability applications life death consequences. dnns inherent explainability achieve predictive power dnns using explainable methods. particular signiﬁcant amount progress explaining convolutional neural networks image domain likely ease visualizing explanations. example gradcam popular methods generating saliency maps indicate relevance pixels image output cnn. methods vastly improved explainability landscape lack critical element needed true understanding humans limited causal interpretations. causality means explanation deep roots within community causal explanation dnn’s operation provides end–consumer output understanding changed input results impactful change output. highly sensitive domains like credit scoring money laundering causal explanation critical system implementers must justify operation models government regulators. major challenge however causal explanation must formulated terms concepts variables understandable human; otherwise explanation obfuscated original model. hypothesis explore paper posits human–understandable causal model operation allows arbitrary causal interventions queries effective often necessary tool explainability interpretability. arbitrary interventions allows user understand chain causal effects input low–level features domain high–level human–understandable concepts outputs. critically model constructed accurately could support introspective queries supported methods counterfactual queries. example might able what probability would turned right pedestrian present input image? capability powerful tool debugging understanding bias ensuring safe operation systems. paper explore hypothesis demonstrate causal approach explanation possible yields valuable information various classiﬁcation systems. part approach extract low–dimensional vance propagation relies conservation principle redistribute prediction network backwards relevance score computed element input space shown produce interpretable heatmaps explain individual classiﬁcations method reported aims relate human understandable concepts network outputs employing deconvolution maskingbased technique score strength distributed representations input concepts across late stage feature maps. methods directly consider network rather locally approximate blackbox models simpler explainable ones shown generate results inspire trust users causal modeling causality long history numerous efforts focused building realistic accurate causal models world instead statistical models resulted plethora causal formalisms semantics various ﬁelds however work frame semantics causality terms interventional effects pearl’s do-calculus directed graphical model deﬁning causal diagram variables deﬁne joint probability distribution semantics causality important explaining dnns because essence explanations must causal models. seeks explanation network’s decisions equivalently asking what changes made input output change stay same?. formulation causation explanation well supported literature consider causal model deﬁnes joint distribution outputs inputs intermediate variables importantly notion interventions provides clear mathematically sound mechanism user understand produces different output values. another explanation observed output formulated intervention. example recognized pedestrian image head implies thinks pedestrian head image. detected head intervene input remove head image probability detecting pedestrian would changed. concepts dnns generate human–understandable vocabulary variables. perform interventional experiments learn graphical causal model relates dnn’s inputs concepts concepts dnn’s outputs. finally demonstrate explanatory power model identifying concepts several networks highest expected causal effect outputs. contributions summarized follows remainder paper organized follows. section discuss related explainability work. section formulate notions causality dnns. discuss extraction human–understandable concepts dnns section several experiments present examples results section ﬁnally conclude section related work many recent works employ saliency visual explanations. several previous works visualized predictions emphasizing pixels inﬂuential values early work domain input image maximally activated neuron interest found gradient ascent image domain; work extended obtain class-speciﬁc saliency maps. beyond manipulating image space monitoring impact output works considered analysis learned features network order glean understanding network functions. seminal work multi-layer deconvolutional network used project features activations back input pixel space. gradient information ﬂowing penultimate convolutional layer used gradcam identify discriminating patterns input images. rather manipulating input images activations within network methods explored generate images produce desired response network; constructed result help explain input maximal desired response network. techniques developed invert network constructing images activate network identically maximally fascinating images constructed maximally activate deep convolutional ﬁlters network illustrate ellaborate features generated larger deep architectures. existing explanation methods dnns particular image classiﬁcation tasks lack ability provide concrete causal interpretation user. example gradient– based methods explanation layerwise relevance propagation grad–cam attempt explain output activation class terms input activations realized speciﬁc input methods certainly useful don’t provide causal intervention semantics sufﬁcient robust explanation. discontinuous saturated gradients indicate causality restricted domain function deﬁned network approximated linear function. adversarial instances generated using gradient descent provide indication local behavior functions deﬁned trained dnns semantic relevance suggests addition interventions deﬁned gradient based methods restricted small domain also uninterpretable considering semantically dubious aspect dnns. methods like avoid practical issues gradient based methods redistributing activation levels relevant pixels provide explicit causal intervention semantics desired effective explanation. given want causal model reﬂects intervention semantics question arises represented joint distribution full access internals already causal representation terms structure weights. given deﬁne neurons network learn joint distribution experimentally intervening network representations user could counterfactual questions network i.e. input output internal neuron network. method technical level correct serves poorly model explanation. lack human–level concepts underlie arbitrary neuron network saying neuron caused network detect pedestrian technically correct satisfy needs eventual human ingest explanations. addition language interventions human would understand network represented individual neurons. example user cannot inquire causal impact head towards detection pedestrian method intervention available neuron granularity. result posit causal model must constructed conceptual granularity meaningful humans. propose causal model dnns represented joint distribution concepts process deriving described function speciﬁc transforms representation neurons activations concept want joint distribution inputs outputs neuron–level concept– level causal models. furthermore also want causal dependencies hold respect input interventions. semantics open range simple groups neuron high–level human concepts like arms legs heads. subjectivity concepts quite powerful presents method explain operation without compromising true causal semantics network provides ability allow users propose human– understandable interventions queries. computing causal effects given causal model deﬁned number interesting queries might better understand explain operation dnn. work propose measure call expected causal effect. first deﬁne causal effect intervention given evidence evidence descendant deﬁnition similar traditional measures causal effect except difference comparing effect intervention intervention all. given effect deﬁne expected causal effect note compute expectation evidence want consider effects outcomes possible given evidence. example observe output true causal effect variable false output always zero using formulation simple effective measure quantify impact various inputs concepts outputs. concept extraction order constuct causal model would ﬁrst like create interpretable concepts satisfy causal semantics. construct concepts satisfy semantics would consider network activations. goal would learn causal model relating activations. concepts chooses however need restricted represented explicitly network activations properly satisfy semantics. simple example could consider inserting linear dense layers deep neural network weight matrices inverses other. activations multiplication could take form linear combination prior activations ﬁnal network output would unaffected. speciﬁc representation instance features given activation values necessarily special relevance. instead choose concept representation transformation activations that’s maximally interpretable. addition satisfying causal intervention criteria described section interpretable concepts satisfy additional criteria concepts low-dimensional minimize amount investigation human would need employ. concepts interpretable case images would like activations restricted contiguous areas containing consistent interpretable visual features. concepts contain relevent information needed achieving target network’s task create auxilliary neural network model constructs concept representations satisfying properties training specially designed loss function. speciﬁcally form model would autoencoder whereby speciﬁc compression interpretability losses could applied retention information required classiﬁcation ensured application reconstruction losses. approach recently employed construct interpretable representations learned features used classiﬁcation approach differs main ways. first rather training autoencoder match output classiﬁcation based linear function coded features employ different reconstruction losses autoencoder. second train multiple autoencoders throughout deep neural network construct interpretable representations activations throughout network. elaborate loss function employ shallow reconstruction loss applied simply norm difference input output activations autoencoder. also employ deep reconstruction loss ensures reconstructed activations result classiﬁcation output passed rest network. loss takes form kl-divergence between output probability distributions original network copy network autoencoder inserted given activation. target activations coding function decoding function function describing application rest network layers following autoencoded activation layer deep reconstruction loss enforced much strongly large loss weighting hyperparameter λdeep λshallow. allows decoder reconstruct slightly altered network activations ensuring important downstream network activations unaffected. added ﬂexibility ﬁtting deep loss instead shallow loss enables autoencoder learn interpretable representations relaxing requirement activations decoded precisely. additionally apply interpretability loss serves mathematically quantify properties would associate interpretable concept images much like prior work particular employ sparsity loss cross-entropy loss total-varation loss encourage spatially smooth independent coded concept features. total autoencoder loss then weighting hyperparameters chosen inspecting results tuning factors output seemed reasonable took iterations manual reﬁnement dataset. trained autoencoders manner activations multiple layers throughout network. deep reconstruction loss particular beneﬁt shallower layers might expect able linear classiﬁer based simple edge detectors. training process proceeds ﬁrst training autoencoder shallowest desired layer inserting trained autoencoder network training next deepest autoencoder iterating autoencoders trained. experiments train autoencoders spaced evenly throughconvolutional layers target network. autoencoder consists convolutional layers coding decoding networks. figure depiction architecture used training. figures depictions resulting coded activations sample input image instances. trained autoencoders plausible approach intervening network. intervene directly network activations probe function would difﬁcult maintain complex correlational statistics various components layer activations. violating statistics could result activation values would impossible recreate input sample resulting misleading output. autoencoder hand expect correlations features network activations would captured encoding network. figure sample display coded features instance trained inria pedestrian dataset. left corner original input image. corresponds extracted coded feature images different autoencoder column corresponds different extracted feature image autoencoder. intervening resulting code ensure decoded activations retain statistics well greatly reduce size possible interventions might consider. experiments since autoencode convolutional layers fully encode vector valued distribution restrict interventions zeroing individual concept feature images coded activations. experiments trained autoencoders throughout network concepts intervene known causal structure representing relationship concepts. given this construct causal model describing relationships concepts output prediction known methods order causal model construct large synthetic dataset containing training input images values concept variables. also randomly intervene coded images autoencoders zeroing entire feature image. causal effect downstream layers captured pooled coded values downstream variables. intervene coded feature image independently randomly probability record resulting values. identify active coded feature images simple variance threshold many always zero result sparsifying loss terms. coded feature images ﬁnally mean-pooled binned ﬁnite bins. meanpool concept images make construction bayes nets tractable future work intend improve approach. found bins sufﬁcient maximize probability data model according techniques. feature values treated variables figure graphical depiction learned causal bayes applied inria pedestrian dataset. crossed boxes serve indicate nodes given level edges incident nodes subsequent level. figure resulting sample feature image displaying head identiﬁcation birds shallowest autoencoder displayed alongside nearest neighbors feature subset augmented dataset. query image leftmost rest ﬁgures. causal bayes layer autoencoded variables dependent variables previous layer. shallowest autoencoded layer treated causally dependent class label well labels associated data instance. then built large synthetic dataset capturing interventions made causal effect interventions construct bayes described cpds node dataset. figure graphical depiction sample learned causal model. resulting model perform query individual input instances ranks variables network according maximum causal effect output classiﬁcation single example types causal queries perform constructed model. experiments performed network architectures datasets applied birds applied inria pedestrian dataset small layer conv refer also applied inria dataset. figure feature image autoencoder trained applied inria dataset depicting identiﬁcation feet. concept image average causal impact ﬁnal classiﬁcation indicating visibility feet major impact classiﬁcation. variable level feat level feat level feat level feat level feat level feat level feat level feat level feat level feat level feat level feat level feat level feat figure resulting expected causal effect query across entire dataset applied inria ’level’ denotes autoencoder ’feat’ indicates coded feature image channel refers figure feature image autoencoder trained applied inria dataset depicting identiﬁcation person’s outline. feature large expected causal effect output input instance concept image individual causal effect largest causal effect concept images instance. pected causal effect displaying images along corresponding coded feature image. additionally visualize nearest neighbors dataset according distance concept feature images i.e. speciﬁed concept feature image input instances helps user better interpret feature image. figures instances nearest neighbor visualization. ﬁnal goal enable user interrogate input image instance interest automatically identifying concepts network highly relevant classiﬁcation visualize context instances contain concept similar manner. figure list expected causal effect dataset trained inria pedestrian dataset. addition depicted average causal effect table query causal model individual classiﬁcation instances. identify feature images maximum causal effect instance question analyze represent nearest neighbor queries. could highly useful tool instance debugging misclassiﬁcations dnn. example figure depict concept feature image largest causal effect instance level feat individual causal effect intend develop interactive tool enable queries type enable explanation instances interest. omit additional speciﬁc results lack space though intend include future release work. summarize describe approach explaining predictions deep neural networks using causal semantics relate output prediction network concepts represented within. series autoencoders loss functions encouraging interpretable properties construct concepts representing information content activations throughout target network. autoencoders trained novel deep loss allows increased ﬂexibility representation. pool features intervene autoencoded network construct variables build causal bayesian network causal relationship deﬁned network structure. ﬁnally network identify features signiﬁcant causal relevance individual classiﬁcations visualized described approach. early investigation ideas domain. number interesting possible directions future work. clear area potential improvement sophisticated methods construct variable observations bayes net. future intend explore construction variational autoencoders image structure encoded away allow compression irrelevant image structure. would greatly increase size bayes nets suggests prudent consider structure learning reducing size bayes skeleton. additionally we’d like consider causal relationship rich input labels network concept features ultimate classiﬁcation. could enable direct identiﬁcation parts network identify relevant input concepts components contribute ultimate classiﬁcation well direct identiﬁcation confounding concepts could result incorrect classiﬁcation finally interested extending approaches non-image domains. acknowledgments material based upon work supported united states force contract fa--c- darpa program. distribution approved public release. distribution unlimited. references sebastian bach alexander binder gr´egoire montavon frederick klauschen klaus-robert m¨uller wojciech samek. pixel-wise explanations non-linear classiﬁer decisions layer-wise relevance propagation. plos alexander binder sebastian bach gregoire montavon klaus-robert m¨uller wojciech samek. layer-wise relevance propagation deep neural network architectures. information science applications pages springer stephanie cuccaro-alamin regan foust rhema vaithianathan emily putnamhornstein. risk assessment decision making child protective services predictive risk modeling context. children youth services review andre esteva brett kuprel roberto novoa justin susan swetter helen blau sebastian thrun. dermatologist-level classiﬁcation skin cancer deep neural networks. nature aravindh mahendran andrea vedaldi. understanding deep image representations inverting them. proceedings ieee conference computer vision pattern recognition pages marco tulio ribeiro sameer singh carlos guestrin. trust you? explaining predictions classiﬁer. proceedings sigkdd international conference knowledge discovery data mining pages ramprasaath selvaraju abhishek ramakrishna vedantam michael cogswell devi parikh dhruv batra. grad-cam that? visual explanations deep networks gradientarxiv preprint arxiv. based localization. david silver huang christopher maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot sander dieleman dominik grewe ilya sutskever timothy lillicrap madeleine leach koray kavukcuoglu thore graepel demis hassabis. mastering game deep neural networks tree search. nature karen simonyan andrea vedaldi andrew zisserman. deep inside convolutional networks visualising image classiﬁcation models saliency maps. arxiv preprint arxiv. ning kamruzzaman sarker derek doran pascal hitzler michael raymer. relating input concepts convolutional neural network decisions. arxiv preprint arxiv.", "year": 2018}