{"title": "Class-Splitting Generative Adversarial Networks", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Generative Adversarial Networks (GANs) produce systematically better quality samples when class label information is provided., i.e. in the conditional GAN setup. This is still observed for the recently proposed Wasserstein GAN formulation which stabilized adversarial training and allows considering high capacity network architectures such as ResNet. In this work we show how to boost conditional GAN by augmenting available class labels. The new classes come from clustering in the representation space learned by the same GAN model. The proposed strategy is also feasible when no class information is available, i.e. in the unsupervised setup. Our generated samples reach state-of-the-art Inception scores for CIFAR-10 and STL-10 datasets in both supervised and unsupervised setup.", "text": "figure samples generated splitting method supervised training cifar- dataset. line samples original classes. side samples corresponding clusters generated class. resnet-b architecture generative adversarial networks produce systematically better quality samples class label conditional setup. still observed recently proposed wasserstein formulation stabilized adversarial training allows considering high capacity network architectures resnet. work show boost conditional augmenting available class labels. classes come clustering representation space learned model. proposed strategy also feasible class information available i.e. unsupervised setup. generated samples reach state-of-the-art inception scores cifar- stl- datasets supervised unsupervised setup. irruption generative adversarial nets produced great leap image data generation. samples generated simply applying neural network transform input random vector sampled uniform distribution. need markov chains unrolled approximate inference networks either training generation samples based generative models take long reach impressive image quality least speciﬁc datasets. however current models cannot produce convincing samples trained datasets images high variability even relatively resolution images. hand observed sample quality improves class information taken account conditional setup ﬁndings suggest hard learn multimodal distribution smooth transform uniform distribution providing categorical class information generator alleviates problem. inspired observations. first mentioned above conditioning generation categorical class labels high level abstraction improves image quality. second early observed adversarial network pair learns useful hierarchy representations unsupervised setup. propose exploit representation space learned model order generate class labels high level abstraction. done applying simple clustering method representation space. conditioning generation class labels model able generate better samples. done either prior class information available not. neural network single scalar output sigmoid activation. output interpreted model probability input real image dataset distribution fake image sampled generator discriminator trained using standard binary classiﬁcation formulation minimizing binary cross-entropy fake real distributions. hand generator simultaneously trained mislead discriminator. accomplished practice updating parameters minimising loss fake samples tagged ‘true’ label main issue original formulation instability training process made hard improve architectures scale bigger images. deep convolutional architecture proposed generator discriminator presents degree stability adversarial training. work ﬁrst producing convincing image samples datasets variability relatively resolution however standard formulation fails generate globally consistent samples trained datasets high variability like imagenet. generated samples adding structure latent space auxiliary classiﬁer. approach requires dataset include class labels i.e. work supervised setting. generator receives noise vector also selected label generated sample function both. furthermore discriminator addition usual objective task correctly classifying real generated samples generator optimized deceive discriminator also generate fake samples minimize auxiliary classiﬁer error i.e. produce well class-deﬁned samples. order address problem instability training arjovsky series works proposed reformulation function optimized. argue original loss function presents discontinuities vanishing gradients respect generator parameters. instead proposed distance distributions known earth-mover distance wasserstein- captures cost transporting mass order importance arjovsky’s contribution lies gain robustness adversarial training process reduction frequency mode collapse phenomenon. furthermore proposed loss function correlates well observed sample quality opposed original loss gives little information training progress. improved version wgan recently proposed gulrajani found weight clipping cause convergence problems settings propose enforce lipschitz constraint critic penalizing gradient’s norm. improved wgan formulation exhibits high robustness changing model architecture. authors tested diﬀerent network designs typically break standard training show stable wgan training cases. furthermore wgan formulation helps achieving better quality samples. quantitative results reported authors terms inception score cifar- dataset recurrent benchmark image generation literature. unsupervised setting reach state-of-the-art supervised case reach second place behind sgan advantageous features made knowledge– wgan current standard formulation adversarial training. main idea generate artiﬁcial classes based representation learned last hidden layer critic enough training iterations. done applying k-means class representation space. divide clusters class samples certain threshold. that training resumed replacing labels ones entire dataset. conditional wgan-gp implementation class labels injected batch normalization layer generative network setting speciﬁc gain bias parameters class. follow strategy proposal class splitting pair child class γchild ather bchild ather initialization classes created. formulation implies child classes start father class params eventually become diﬀerent. demonstrate eﬀectiveness proposal conducted several experiments cifar- dataset containing images corresponding diﬀerent classes unlabeled stl- containing larger diverse images based model improved wgan algorithm proposed gulrajani cases training sample images current model select best based inception score. finally sample another best model order calculate reported score following cifar- unsupervised test performed starting examples considered single class dividing clusters every approximately epochs. done gulrajani’s resnet architecture without changes modiﬁed version doubling number maps convolutional layer. supervised test also conducted architectures starting original classes cifar dividing approximately training epochs. comparison also trained resnet-b architecalso comparison samples obtained wgan-gp supervised model obtained proposed method shown. figure real samples cifar- corresponding clusters found. treat stl- downsample dimension resulting images. tested algorithm resnet-a architecture minimum changes necessary model generate images. table shows resulting inception score. figures show generated images clusters found method. several things observed results presented previous section. first regarding obtained clusterization real samples visually rules deﬁne vast majority samples least several clusters. example supervised case left side fourth cats looking forward left side eighth horse side views. compare cats horses several positions corresponding clusters right side. unsupervised case tendency generate clusters cars planes ships general regarding generated samples supervised case class splits allows model generate better samples. uniform clusters horse side views cats looking forward whole original class. compare example fourth eighth figure rows figure corresponding model trained wgan-gp. note samples diﬀer much shown even classes clustering step seem found obvious separation rule cars better sample quality observed original wgan-gp. unsupervised case cifar- although inception score similar obtained state-of-the-art samples generated seem higher quality. nevertheless reach quality generated images supervised setting. always advisable start division clusters predeﬁned classes information available. case stl- noticeable diﬀerence inception score. reason stl- much diverse dataset division large number classes beneﬁcial. noted case state-of-the-art much actual dataset score case cifar-. success splitting method suggests reinjecting high level information critic generative model improves sampling quality. breaks strictly adversarial training allows degree information sharing networks. believe simple strategy could inspire better adversarial training formulation small amount high level information directly ﬂows critic’s last layers generator input. future direction research improve current splitting version oriented understand given model architecture dataset determines optimal number clusters also clusterization could enhanced adversarial training addition extra loss term like also currently working generalization splitting ideas following paths first making high level information critic’s representation ﬂows continually generator avoiding special purpose steps predeﬁned times training process. second avoiding hard threshold clustering step replacing sort continuous representation capturing amount information.", "year": 2017}