{"title": "Fast Learning of Clusters and Topics via Sparse Posteriors", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Mixture models and topic models generate each observation from a single cluster, but standard variational posteriors for each observation assign positive probability to all possible clusters. This requires dense storage and runtime costs that scale with the total number of clusters, even though typically only a few clusters have significant posterior mass for any data point. We propose a constrained family of sparse variational distributions that allow at most $L$ non-zero entries, where the tunable threshold $L$ trades off speed for accuracy. Previous sparse approximations have used hard assignments ($L=1$), but we find that moderate values of $L>1$ provide superior performance. Our approach easily integrates with stochastic or incremental optimization algorithms to scale to millions of examples. Experiments training mixture models of image patches and topic models for news articles show that our approach produces better-quality models in far less time than baseline methods.", "text": "mixture models topic models generate observation single cluster standard variational posteriors observation assign positive probability possible clusters. requires dense storage runtime costs scale total number clusters even though typically clusters signiﬁcant posterior mass data point. propose constrained family sparse variational distributions allow non-zero entries tunable threshold trades speed accuracy. previous sparse approximations used hard assignments moderate values provide superior performance. approach easily integrates stochastic incremental optimization algorithms scale millions examples. experiments training mixture models image patches topic models news articles show approach produces better-quality models less time baseline methods. mixture models topic models fundamental bayesian unsupervised learning. models clusters topics useful exploring input dataset. mixture models assume input data fully exchangeable topic models extend mixtures handle datasets organized groups observations documents images. mixture topic models kinds latent variables. global parameters deﬁne cluster including frequency statistics associated data. local discrete assignments determine cluster explains speciﬁc data observation. global local variables bayesian analysts wish estimate posterior distribution. models full posterior inference markov chain monte carlo averages sampled cluster assignments producing asymptotically exact estimates great computational cost. optimization algorithms like expectation maximization variational bayes provide faster deterministic estimates cluster assignment probabilities. however observation methods give positive probability every cluster requiring dense storage limiting scalability. paper develops posterior approximations local assignment variables allow optimization-based inference scale hundreds thousands clusters. show adding additional sparsity constraint standard variational optimization objective local cluster assignments leads gains processing speed. unlike approaches restricted hard winner-take-all assignments approach offers tunable parameter determines many clusters non-zero mass posterior observation. approach variational algorithm regardless whether global parameters inferred point estimates given full approximate posteriors. furthermore approach integrates existing frameworks large-scale data analysis easy parallelize. open source python code exploits efﬁcient implementation selection algorithms scalability. fig. impact sparsity-level speed accuracy estimating zero-mean gaussian mixture model pixel patches natural images. comparison algorithm substep costs complete passes entire million patch dataset sec. clusters. speed comparison l−sparse approach computing responsibilities given ﬁxed weights patches. speed comparison l−sparse approach computing per-cluster statistics sk}k deﬁned patches. cumulative density function variational distance dense l−sparse responsibilities across patches using pretrained mixture model published online zoran weiss given observed data vectors mixture model assumes observation belongs clusters. hidden variable denote speciﬁc cluster assigned mixture model sets global parameters cluster frequencies {πk}k dirk scalar probability observing data cluster catk. generate observation according likelihood exponential family density sufﬁcient statistics natural parameter normalization function ensures integrates one. density conjugate parameter conjugacy convenient necessary require expectation evaluated closed-form. mean-ﬁeld variational inference seeks factorized posterior posterior factor free parameters optimized minimize divergence simpliﬁed approximate density true intractable posterior. separate factors local global parameters specially chosen forms focus free parameter deﬁnes local assignment posterior vector nonnegative sums one. interpret value ˆrnk posterior probability assigning observation cluster sometimes called cluster responsibility observation goal variational inference optimal free parameters speciﬁc objective function using full approximate posteriors global parameters yields evidence lower-bound objective function equivalent minimizing divergence point estimation global parameters instead yields maximum-likelihood objective closed-form expressions objectives appendix given objective function optimization typically proceeds coordinate ascent call update data-speciﬁc responsiblities local step alternated global update local step computes responsibility vector observation maximizes given ﬁxed global parameters. either approximate posterior treatment global parameters objective optimal update maximizes following objective function alg. updates responsibilities observation given weights deﬁned left denserespfromweights standard solution optimization problem requires evaluations function summations divisions. right proposed method toplrespfromweights optimizes objective subject additional constraint clusters non-zero posterior probability. first introspective selection algorithm ﬁnds indices largest weights. given these optimum evaluations function summations divisions. interpret posterior weight cluster observation larger values imply cluster likely assigned observation learning expectations deﬁning replaced point estimates. optimal solution simple exponentiate weight normalize resulting vector. function denserespfromweights alg. details required steps. runtime cost dominated required evaluations function. given ﬁxed assignments global step computes optimal values global free parameters whether point estimation approximate posterior inference update requires ﬁnitedimensional sufﬁcient statistics rather values. cluster must compute expected count assigned observations expected data statistic vector contribution variational objective algorithm scales better large numbers clusters much runtime cost standard variational inference algorithms comes representing dense vector. although total clusters observation entries appreciable mass vast majority close zero. thus constrain objective allow non-zero entries function toplrespfromweights alg. solves constrained optimization problem. first identify indices values weight vector descending order. denote topranked cluster indices distinct value given active clusters simply exponentiate normalize indices. represent solution l-sparse vector real values ˆrnl integer indices inl. solutions unique posterior weights contain duplicate values. handle ties arbitrarily since swapping duplicate indices leaves objective unchanged. active means equal vector objective function value increases alternative runtime. selection algorithms designed values descending order within array size methods divide-and-conquer strategies recursively partition input array blocks values pivot below. musser introduced selection procedure uses introspection smartly choose pivot values thus guarantee worst-case runtime. procedure implemented within standard library nth_element selecttopl practice. function operates in-place provided array rearranging values ﬁrst entries bigger remainder. importantly internal sorting within either partition. example code found appendix choosing sparsity-level naturally trades execution speed training accuracy. recover original dense responsibilities assigns point exactly cluster k-means. focus modest values fig. shows large values toplrespfromweights faster denserespfromweights dense method’s required exponentiations dominates introspective selection procedure. l−sparse responsibilities computing statistics scales linearly rather gain useful applying gaussian mixture models unknown covariances image patches fig. shows cost patch requires expensive -dimensional data statistic xnxt summary step virtually disappears rather savings makes overall algorithm twice fast remaining bottleneck dense calculation weights might sped likelihoods using fast data structures ﬁnding nearest-neighbors. fig. shows captures nearly identical responsibility values indicating modest values bring speed gains without noticeable sacriﬁce model quality. hard assignments. widespread practice used decades consider hard assignments observation assigned single cluster instead dense vector responsibilities. equivalent setting l−sparse formulation. k-means algorithm nonparametric extension dp-means justify sparsity small-variance asymptotics. so-called hard viterbi training maximization-expectation algorithms hard assignments. however expect coarse many applications moderate values like offer better approximations shown fig. sparse prominent early method exploit sparsity responsibilities sparse algorithm proposed neal hinton sparse maintains dense vector observation edits subset vector local step. edited subset consist largest entries entries threshold. inactive entries frozen current non-zero values newly edited entries normalized length-k vector preserves sum-to-one constraint. sparse effective small datasets thousand examples found applications medical imaging however l−sparse approach three primary advantages relative sparse l−sparse method requires less per-observation memory responsibilities. sparse must store ﬂoating-point values represent responsibility vector need store l−sparse method easily scales minibatch-based training algorithms sec. sparse em’s required storage prohibitive. approach safely discard responsiblity vectors required sufﬁcient statistics computed. sparse must explicitly store responsibilities every observation dataset cost future sparse updates desired. prohibits scaling millions examples processing small minibatches unless minibatch full responsibility array written disk needed. proved sec. top-l selection optimal compute l−sparse responsibilities monotonically improve training objective function. neal hinton suggest selection method heuristic without justiﬁcation. expectation truncation. undertook research unaware related method lücke eggert called expectation truncation constrains approximate posterior probabilities discrete multivariate binary variables l−sparse. lücke eggert considered non-negative matrix factorization sparse coding problems. later extensions applied core algorithm mixture-like sprite models cleaning images text documents spike-and-slab sparse coding work ﬁrst apply l−sparse ideas mixture models topic models. original expectation truncation algorithm expects user-deﬁned selection function identify entries non-zero responsibility speciﬁc observation. practice selection functions suggest chosen heuristically upper bound original authors freely admit selection functions optimal monotonically improve objective function contrast proved sec. top-l selection optimally improve objective function. advantage work previous expectation truncation efforts thorough experiments exploring different values impact training speed predictive power. comparisons range possible values real datasets lacking lücke eggert papers. empirical insight modest values like frequently better especially topic models. stochastic variational inference introduced hoffman scales standard coordinate ascent large datasets processing subsets data time. proposed sparse local step easily svi. iteration performs following steps sample batch full dataset uniformly random; observation batch local step update responsibilities given ﬁxed global parameters; update global parameters stepping current values direction natural gradient rescaled batch objective procedure guaranteed reach local optima step size gradient update decays appropriately increases incremental algorithms inspired incremental hughes sudderth introduced memoized variational inference data divided ﬁxed batches iterations begin. iteration completes four steps select single batch visit; observation batch compute optimal local responsibilities given ﬁxed global parameters summarize sufﬁcient statistics batch incrementally update whole-dataset statistics given statistics batch compute optimal global parameters given whole-dataset statistics. incremental update step requires caching summary statistics batch. algorithm per-iteration runtime stochastic inference guarantees monotonic increase objective local step closed-form solution like mixture model. ﬁrst pass entire dataset equivalent streaming variational bayes evaluate dense l−sparse mixture models natural images inspired zoran weiss train model image patches taken overlapping regular grids stride pixels. observation vector preprocessed remove mean. apply mixture model zero-mean full-covariance gaussian likelihood function concentration evaluate track log-likelihood score ˆσk). here heldout observations point estimates computed trained global parameters using standard formulas. function probability density function multivariate normal. fig. compares l−sparse implementations million patches images. algorithms process minibatches patches. sparse methods consistently reach good predictive scores times faster dense runs finally modestly sparse runs often reach higher values heldout likelihood hard runs especially plots fig. analysis million pixel image patches using zero-mean gaussian mixture model trained l−sparse stochastic memoized variational algorithms. train total images processed images time. panel shows heldout likelihood score time training runs various sparsity levels ﬁxed number clusters training time plotted log-scale. fig. impact sparsity-level speed accuracy training topic models. comparison runtime costs complete passes wikipedia documents topics. l−sparse methods show breakdown algorithm substeps. text indicates number documents processed second. timings l−sparse local step restart proposals nytimes articles using iterations document. timings l-sparse summary step nytimes articles. cumulative density function variational distance dense l−sparse document-topic distributions across nytimes documents. deﬁne empirical topic distribution document normalizing count vector develop sparse local step topic models. topic models hierarchical mixtures applied discrete data documents document consist observed word tokens ﬁxed vocabulary word types though could easily build topic model observations type document contains observed word tokens {xdn}nd token identiﬁes type n-th word. latent dirichlet allocation topic model generates document’s observations document-speciﬁc frequencies topic dirv mixture model common topics {φ}k probability type topic document-speciﬁc frequencies drawn symmetric dirichlet dirk scalar. assignments drawn catk observed words drawn catv goal posterior inference estimate common topics well frequencies assignments document. standard mean-ﬁeld approximate posterior factorization standard optimization objective complete expressions appendix optimize objective coordinate ascent alternating local global steps. focus local step requires updating assignment factor frequencies factor document next derive interative update algorithm estimating assignment factor frequencies factor document alg. lists conventional algorithm sparse version. incorporate l-sparse constraint obtain sparse rather dense responsibilties. procedure toplrespfromweights alg. still provides optimal solution. iterative joint update dense case. following standard practice dense assignments block-coordinate ascent algorithm iteratively updates using closed-form steps above. initialize update cycle recommend setting initial weights document-topic frequencies uniform wdnk lets topic-word likelihoods drive initial assignments. alternate updates either maximum number iterations reached maximum change document-topic counts falls threshold appendix provides detailed algorithm. fig. compares runtime cost local summary global steps topic model showing local iterations dominate overall cost. iterative joint update sparsity. l-sparse constraint responsibilities leads fast local step algorithm topic models. procedure primary advantages dense baseline. first toplrespfromweights update per-token responsibilities ˆrdn resulting faster updates. second assume topic’s mass decays near zero never rise again. assumption every iteration identify active topics document topics weight large enough chosen token. thus throughout local iterations consider active topics reducing steps cost cost discarding topics within document mass becomes small justiﬁed previous empirical observations digamma problem described mimno topics negligible mass expected prior weight becomes vanishingly small. example gets smaller increases. practice ﬁrst iterations active stabilizes token’s topics rarely change relative responsibilities continue improve. regime reduce runtime cost avoiding selection altogether instead reweighting token’s current topics. perform selection ﬁrst iterations every iterations yields large speedups without loss quality. fig. compares runtime sparse local step across values sparsity-level comparable implementation standard dense algorithm. fig. shows l−sparse local step least times faster larger values lead even larger gains. fig. shows sparsity improves speed summary step though step less costly local step topic models. finally fig. shows modest sparsity yields document-topic distributions close found dense local step much coarser. restart proposals. scalable applications assume cannot afford store document-speciﬁc information iterations. thus time visit document must infer scratch. joint update non-convex thus recommended cold-start initialization guaranteed monotonically improve across repeat visits document. however even could store document counts across iterations warm-starting often gets stuck poor local optima instead combine cold-starting restart proposals. hughes introduced restarts post-processing step single document local iterations results solutions better objective function scores. given ﬁxed point restart proposal constructs candidate forcing responsibility mass active topic zero running iterations forward. accept proposal improves objective proposals escape local optima ﬁnding nearby solutions favor prior’s bias toward sparse document-topic probabilities. frequently accepted practice always include sparse dense local steps. topic pseudocounts single alg. algorithms computing per-unique-token responsibilities {ˆrdu}ud document given ﬁxed topics. left standard dense algorithm step scales linearly number total topics regardless many topics used document. right l−sparse algorithm forcing observation topics tracking active topics document leads update steps scale linearly number active topics |ad| much less total number topics related work. mcmc methods specialized topic models text data exploit sparsity huge speed gains. sparselda clever decomposition gibbs conditional distribution make per-token assignment step cost less aliaslda lightlda improve amortized methods still limited hard assignments applicable discrete data. contrast approach allows expressive intermediate sparsity apply broader family mixtures topic models real-valued data. recently several efforts used mcmc samplers approximate local step within larger variational algorithm estimate approximate posterior averaging many samples sample hard assignment. number ﬁnite samples needs chosen balance accuracy speed. contrast sparsity-level provides intuitive control approximation accuracy optimizes exactly expectation. compare l−sparse implementations external baselines sparselda fast implementation standard gibbs sampling svigibbs stochastic variational method uses gibbs sampling approximate local gradients. algorithms java code mallet also compare public implementation lightlda external methods default initialization sample diverse documents using methods explore several values sparsity-level lightlda sparselda tunable sparsity parameters. svigibbs allows specifying number samples used approximate consider always discarding half samples burn-in. methods document-topic smoothing topic-word smoothing stochastic learning rate iteration grid search best heldout score validation data considering delay decay fig. compares methods datasets nips articles wikipedia articles million york times articles. curve represents best many random initializations. following wang evaluate heldout likelihoods document completion task. given test document divide estimate document-topic words random type pieces |ˆπd ˆφ). supplement details. probabilities evaluate estimate computing fall progressively worse local optima failure mode occur topic models re-estimate scratch time visit document. baselines converge slowly. throughout fig. runs sparselda svigibbs reaches competitive predictions allowed time limit svigibbs beneﬁts using instead samples nytimes. samples improve performance further. expected lightlda higher throughput l−sparse methods small datasets eventually makes slightly better predictions however across values l−sparse methods reach competitive values faster especially large nytimes dataset. large lightlda never catches allotted time. note lightlda’s speed comes metropolis-hastings proposal highly specialized topic models discrete data methods broadly applicable cluster-based models non-multinomial likelihoods. introduced simple sparsity constraint approximate posteriors enjoys faster training times equal better heldout predictions intuitive interpretation. algorithms dropped-in full-posterior variational clustering objective easy parallelize across minibatches. unlike previous efforts encouraging sparsity sparse expectation truncation procedures easily scale millions examples without prohibitive storage costs present proof chosen top−l selection procedure optimal done rigorous experiments demonstrating often modest values much better released python code fast subroutines encourage reuse practioners. anticipate research adapting sparsity sequential models like hmms structured variational approximations bayesian nonparametric models adaptive truncations fast methods like kd-trees computing cluster weights fig. analysis nips articles wikipedia articles million york times articles batches times batches otherwise. panel shows single value heldout likelihood changes time sparse dense versions algorithms external baselines. training time plotted log-scale.", "year": 2016}