{"title": "Evolving Ensemble Fuzzy Classifier", "tag": ["cs.LG", "cs.AI"], "abstract": "The concept of ensemble learning offers a promising avenue in learning from data streams under complex environments because it addresses the bias and variance dilemma better than its single model counterpart and features a reconfigurable structure, which is well suited to the given context. While various extensions of ensemble learning for mining non-stationary data streams can be found in the literature, most of them are crafted under a static base classifier and revisits preceding samples in the sliding window for a retraining step. This feature causes computationally prohibitive complexity and is not flexible enough to cope with rapidly changing environments. Their complexities are often demanding because it involves a large collection of offline classifiers due to the absence of structural complexities reduction mechanisms and lack of an online feature selection mechanism. A novel evolving ensemble classifier, namely Parsimonious Ensemble pENsemble, is proposed in this paper. pENsemble differs from existing architectures in the fact that it is built upon an evolving classifier from data streams, termed Parsimonious Classifier pClass. pENsemble is equipped by an ensemble pruning mechanism, which estimates a localized generalization error of a base classifier. A dynamic online feature selection scenario is integrated into the pENsemble. This method allows for dynamic selection and deselection of input features on the fly. pENsemble adopts a dynamic ensemble structure to output a final classification decision where it features a novel drift detection scenario to grow the ensemble structure. The efficacy of the pENsemble has been numerically demonstrated through rigorous numerical studies with dynamic and evolving data streams where it delivers the most encouraging performance in attaining a tradeoff between accuracy and complexity.", "text": "abstract— concept ensemble learning offers promising avenue data streams complex environments addresses bias variance dilemma better features reconfigurable structure well-suited given context. various extensions ensemble learning mining nonstationary data streams found literature crafted static base-classifier revisits preceding samples sliding window retraining step. feature causes computationally prohibitive complexity flexible enough cope rapidly changing environments. complexities often demanding involves large collection offline classifiers absence structural complexities reduction mechanisms lack online feature selection mechanism. novel evolving ensemble classifier namely parsimonious ensemble proposed paper. pensemble differs existing architectures fact built upon evolving classifier data streams termed parsimonious classifier pensemble equipped ensemble pruning mechanism estimates localized generalization error base-classifier. dynamic online feature selection scenario integrated pensemble. method allows dynamic selection deselection input features fly. pensemble adopts dynamic ensemble structure output final classification decision features novel drift detection scenario grow ensemble’s structure. efficacy pensemble numerically demonstrated rigorous numerical studies dynamic evolving data streams delivers encouraging performance attaining tradeoff accuracy complexity. data-intensive data collected continuously fast rate dynamic evolving environments opens research direction process data streams efficiently unlike classical paradigm machine learning dataset utilised construct hypothesis executed multiple passes data streams requires strictly online learning framework memory requirement even possible memory one-pass learning mode. another challenging trait data streams lies non-stationary characteristics data follow static predictable distributions contains variety concept drifts facts make retraining phase incorporating sample dataset impossible performed leads socalled catastrophic forgetting previously valid knowledge scalable dealing massive data streams. evolving intelligent system provides unique solution data stream mining strictly one-pass learning procedure involved delivered great success cope time-critical applications data streams generated fast sampling rate furthermore adopts open structure components automatically generated pruned merged recalled well-suited given problem. trait reflects true data distributions tracks changing data distributions transformed active research area computational intelligence research evidenced number published works area nonetheless typically built upon single classifier architecture often produce adequate accuracy complex problems fact classical batch learning perspective well-known ensemble classifiers outperform single base classifiers case high noise levels number available training samples better resolve bias-variance dilemma proper subspace data exploration using weak classifiers works synergy ensemble structure found literature utilise static ensemble architecture predetermined advance. although diversity base classifiers guaranteed varying user-defined parameters applying different data partitions base classifiers issue concept drifts remains open challenge fixed structure. ensemble learning concept uses combination individual base classifiers modularity principle enables dynamic evolution ensemble structure ensemble learning lies diversity base classifiers makes robust various forms uncertainty data streams nonetheless must bear mind diversity ensemble classifier might counterproductive realm data streams opens door outdated base-classifiers ensemble structure. adaptability ensemble classifier plays vital role success ensemble learning formulates mechanisms ensemble classifier adapts changing data distributions presented ensemble classifier also distinguished groups active passive approach passive approach relies continuous updates components assumes concept drifts occur ongoing fashion; active approach equipped dedicated drift detection mechanism restructured parameters fine-tuned drift captured practise drift detection mechanism plays role alert operators possible changing system behaviours identify whether change causes catastrophic effect operation’s cycle vital process’s safety. best knowledge local concept drift curse dimensionality structural complexity three open issues current literatures. case local concept drift changes ensue whole feature space rather local regions different rates severities remains open question existing ensemble classifiers mostly constructed using batch classifier accumulate already seen samples sliding window retraining steps considers global change data distribution. although ensemble algorithms like dela excluded local concept drift bottleneck three levels adaptivity namely structural adaptivity combination adaptivity model adaptivity suffers absence dedicated drift detection method furthermore structural complexities existing ensemble classifiers considerable usually involve large number base classifiers assure acceptable accuracy. suffer absence structural complexity reduction mechanism alleviates complexities ensemble classifiers existing ensemble classifiers also assume input features pre-selected pre-processing steps. issue hinders viability time-critical applications data streams generated continuously fast sampling rate makes iterative pre-processing step impractical. furthermore pre-recorded data often irrelevant later stage rapidly changing environments. novel ensemble learning algorithm namely parsimonious ensemble proposed paper. pensemble features open structure local expert created pruned dynamically strictly one-pass learning mode. constructed recently published evolving classifier namely parsimonious classifier evolving classifier strengthens adaptive nature evolving ensemble handles local concept drift better classical batch classifier dynamic online paradigm. features open structure paradigm self-evolving track variations local data space. pensemble works fully single-pass learning mode well-suited online life-long learning scenario. pensemble also equipped dynamic feature selection scenario address high input dimensionality best knowledge absent majority existing ensemble classifiers. final class prediction pensemble inferred dynamic ensemble paradigm dynamically grow shrink adjust weights local experts data streams. dynamic ensemble concept inspired evolving trait different criteria applied perform structural learning scenarios pensemble. pensemble puts forward three learning components follows online drift detection scenario pensemble adopts dynamic ensemble structure local expert added concept change presents data streams procedure governed non-parametric drift detection method derived concept hoeffding’s bounds method monitors performance metric sends warning signal significant variation identified. method threshold-free relies probability inequalities assumption independent univariate bounded random variables theoretically proven. learning feature lowers ensemble complexity ensemble size expands demands independent number data streams. ensemble pruning scenario pensemble presents ensemble pruning scenario crafted notion localized generalization error method estimates generalization performance local expert determines local experts pruned technique analyses upper bound error local expert within neighbourhood reflects generalization power local expert. notion proposed radial basis function neural network adapted working principle pensemble applying generalized neuro fuzzy local expert non-axis parallel gaussian rule rotating direction. online feature selection scenario pensemble capable performing online feature selection scenario using socalled generalized online feature selection method extension method advantage gofs counterparts lies capability selection deselection input attributes assigning crisp values allows flexibility feature selection process avoids discontinuity bottleneck input variable recovered future needed another salient feature gofs concept seen aptitude handling partial input information relieves computational storage burdens learning process necessarily start full-scale input variables. paper conveys four major contributions follows novel ensemble learning algorithm inspired seminal work namely proposed. modifies introduction drift detection scenario ensemble pruning scenario online feature selection evolving local expert; pensemble puts forward perspective fully evolving ensemble learning concept evolving ensemble level base-classifier level; three novel learning modules namely drift detection method ensemble pruning scenario online feature selection proposed; efficacy pensemble numerically validated using numerous real-world synthetic data streams. compared state-of-the classifiers showing pensemble outperformed counterparts terms accuracy complexity. paper structured follows section outlines literature survey current ensemble learning algorithms evolving learning algorithms section discusses architecture learning policy pensemble section elaborates working principles base classifier pclass section describes numerical studies comparisons prominent algorithms concluding remarks drawn last section. research area started algorithmic development number works. evolving rule-based model exemplifies concept using incremental unsupervised learning denfis another early example combines working principle fuzzy system evolving clustering method angelov filev proposed so-called benefits data potential theory forming evolving version mountain clustering. work modified classification problem formed first evolving classifier termed eclass. term however formalised clarification since term evolving sometime confused concept evolutionary computation. motivated significant progress real-time data collection capture notion gained popularity community shown effective addressing lifelong learning situation non-stationary environments. several extensions variations forward literature evolving version vector quantization designed algorithmic backbone flexfis later extended robust version including rule merging generalized rules incremental feature weighting mechanism generalized fuzzy rule forward generates non-axis parallel ellipsoidal cluster happens better coverage flexibility conventional fuzzy rules pratama developed theory rule evolving ensemble proposed makes base-classifier realised different configurations ensemble classifier. work extended estacking forward using concept stacking ensemble. parallel implementation tedaclass proposed work classified ensemble strict sense data distributed number computing nodes. ensemble deep learning classifiers designed handwriting recognition adopted concept data parallerization all-pair classifier also grouped ensemble approach. solely concentrated class decomposition approach multi-class problems order reduce class imbalance. notwithstanding well-established literature still deserves in-depth investigation least three reasons vast majority constructed single model framework diversity. ensemble learning concept well-known powerful generalization power address bias-and-variance better produces model high diversity covering rich data region; evolving base classifier ensemble structure initiated relies static ensemble structure predetermined training process; existing eiss categorized passive approach handling concept drift changing data distributions overcome continuously adapting classifier. lacks capability signal presence concept drift identify type drift. trait plays vital role practice provides feedback operator whether drift alarming not. pensemble developed generalized working framework working principle displayed algorithm pensemble stores collection local experts automatically generated drift detected pruned longer relevant capture current data trends evolving algorithm namely pclass deployed base learner implements open structure paradigm created mimo architecture rule possesses multiple consequents representing class final output inferred generating maximum output. reason behind choice mimo architecture aptitude handling class overlapping class looked unique rule consequent. local expert assigned voting weight necessary keep ensemble structure relevant up-to-date context compromises diversity ensemble. correct shortcoming weight local expert augmented makes correct prediction maintain ensemble’s diversity open possibility local expert pick mechanism plays vital role dealing cyclic drift. addition pensemble equipped another rule pruning pensemble starts learning scenario scratch base classifier all. first base classifier initialized using first data chunk. ensemble structure grows automatically changing data distributions seen. performance individual local experts assessed misclassification made using local expert whereas reward granted increasing voting weight correct prediction returned. carrying procedure online concept drift detection method performed. drift detection strategy relies concept hoeffding’s bounds determine drift’s level statistical process control approach integrated monitor dynamic data streams classifies system behaviours three stages namely normal warning drift. base classifier created using data streams drift level reached. weight learner initialized final output ensemble classifier inferred class highest accumulated weight. output local learner weighted corresponding weight. outputs combined arrive weighted class. weight base classifiers normalized assure partition unity normalization step aims avoid classifier outweigh classifiers. note pensemble still aligns one-pass learning concept learns data-chunk single scan without revisiting previous data chunks without iterative learning data chunk. algorithm parsimonious ensemble calculate localized generalization error estimate generalization power local expert. local expert poor generalization capability removed section discard i-th local expert undertakes drift detection method determine suitable learning actions whether classifier introduced learning process committed update winning classifier learning process carried section learning algorithm parsimonious ensemble section focusses learning procedure pensemble encompasses drift detection strategy ensemble pruning strategy online feature selection strategy. drift detection method drift detection scenario vital pensemble controls ensemble complexity. allows ensemble structure expand size uncharted training region comes picture online non-parametric drift detection method integrated using hoeffding’s inequalities determine acceptable level concept changes data streams method capable capturing significant distributional changes data streams one-pass mode confirmed solid theoretical guarantees rely assumption probability density function rather performance metrics regarded independent bounded random variables. worth mentioning drift handling strategy specifically detect exact time period drift presents since derived forgetting concept categorized passive approach. drift detection scenario starts monitoring statistics data streams defines three conditions stable seems change warning possible concept drift appear drift drift clearly identified. underlying task drift detection method pinpoint drift occurs data streams also track transition stable condition drift condition drift ascertained severe enough occurs period time. wide range performance metrics used assess existence drift data streams. referring original work performance metrics namely moving average weighted moving average forward. since moving average sensitive weighted version concept change thus suitable detecting abrupt drifts used current data chunk. transition period warning drift required bear whether change really occurs caused noise outliers. buffer deployed accumulate data transition period prevent mixed-up concept classifier. first start finding point current chunk indicates point population mean increases. point switching point note calculated recursively ease. approach similar idea statistical process control except basis normality relaxed here. moreover standard deviation confidence interval replaced significance level corresponds observed algorithm learning scenario carried warning stage. mechanism chosen since warning phase constitutes transition period presence concept drift still calls investigation. stable phase implies concept remains induce introduction classifier. however calls winning classifier updated using current data chunk assure generalization’s capability ensemble classifier reduces risk overfitting feeding observations base-classifiers. winning classifier selected simply inspecting predictive error mean square error used pensemble. ensemble pruning strategy based local generalization error success ensemble classifier highly determined generalization potential base classifiers. although well-known collection weak classifiers often promotes better performance strong classifiers case realm data streams. diversity comes cost complexity predictive performance data stream inherent non-stationary contexts. base classifier generalization potential expected play little lifespan even jeopardize final predictions therefore pruning base classifier reduces ensemble complexity approach inspired localized generalization error method quantifies generalization capability classifier within predefined local region technique meant approximate upper bound mean square error unseen samples lying region. predetermined region plausible approach study model’s generalization since training samples occupy dense local region inter-related drawn unknown distribution. finding upper bound generalization error hidden context entire input space extremely difficult safely ignore irrelevant concept sitting away training samples. squared output perturbations already seen samples unknown samples local region. analyses sensitive classifier’s output variation input data. expression stochastic sensitivity measure perturbations plausible considering strictly single-pass working principle pensemble without prior knowledge. albeit input perturbations relaxed provided variance input dropped localized generalization error formula three components training error stochastic sensitivity measure constants. high training error pinpoints under-training case results poor generalization unseen samples. stochastic sensitivity measure illustrates sensitivity classifiers output’s change outputs varying dramatically input variation characterize high stochastic sensitivity. good generalization attained minimizing terms forming sound tradeoff two. words ensemble pruning scenario aims discover classifiers large generalization although formula aims analyse upper bound targets regression cases direct regression class indices cases results poor performance strategy still applicable classification problems. relationship localized generalization error misclassification rate studied error distribution known percentage unseen samples framework gofs method feature selection approach extendible partial input information condition subset input attributes obtained training process. gofs performs crisp feature selection input features assigned crisp weights allows dynamic activation deactivation input attributes training process. system corresponds intercept linear function excluded summation output weights. realm fuzzy system rule consequent depicts local tendency rule substitute gradient information sensitivity analysis input variables since gradient information changes point case nonlinear function. concept confirmed fact base classifier pensemble employs local learning scheme rule consequent represents specific region approximation curve. data standardization required different input ranges mislead contribution input feature. guarantee transparency feature contribution normalization done expression adopts -sigma rule principle aims track downtrend model’s generalization. assuming localized generalization error follows gaussian distribution values occupy three sigma range incurs confidence level. case beyond range three sigma said anomalies. although concept localized generalization error exploited various problems efficacy data stream analytics best knowledge unexplored. online feature selection strategy high input dimension commonly found various real-world data stream cases undermines learning capability online real-time scenario imposes considerable complexity transparency fuzzy rule also affected rule consists many atomic clauses. notwithstanding online feature selection strategy drawn considerable research interest date focus single classifier only. online feature selection technique ensemble learner proposed paper constructed number base classifiers base classifiers ensemble. addition sparsity property norm examined understand whether value input features accumulated ball. referring theory input pruning process takes place given misclassification occurs. input pruning scenario executed global prediction ensemble network match true class label predicted class label. approach plausible feature selection scenario aims take corrective actions getting influence poor features. feature selection necessary correct prediction returned save computational cost. rule consequent first adjusted using gradient descent approach projected ball assure bounded norm. detailed procedure gofs method shown algorithm algorithm gofs procedure full input attributes elicited assumption fuzzy rules structured first order fuzzy neural network pclass framework. words fuzzy rules base classifiers combined treated unified local expert. scenario made possible local property pensemble fuzzy rule functions loosely coupled sub-model. stochastic gradient descent approach applied algorithm rather fwgrls method covariance matrix allocated assigned local model thereby greatly simplifying overall optimization process. worth noting feature selection process done centralistic manner fuzzy rules base classifiers together. hence output covariance matrix local level cannot used represents different optimization objectives. convergence gofs method proven upper bound obtained. gofs method allows different subsets input variables selected every training observation. since partial input information situation entails minor variation full counterpart explained here. complexity analysis section aims analyse computational burden pensemble presents generalized version dwm. pensemble utilizes drift detection method imposes computational complexity drift detection method denotes ensemble pruning short input pruning. data chunk size data samples chunk learned single scan revisited again. note term aforementioned notation influenced computational complexity pclass base classifier. pclass fully evolving algorithm working single-pass learning mode. computational complexity pclass derived section briefly outlines algorithmic procedure pclass serves local expert pensemble. includes network structure pclass rule growing strategy rule pruning recall strategy parameter learning strategy. since pensemble deploys online feature selection scenario level input weighting mechanism pclass switched sleep mode. network structure pclass pclass class neural-fuzzy systems generating generalized first-order fuzzy rule. utilises multivariate gaussian function evolving non-axisparallel ellipsoidal cluster rule premise exploiting first order polynomial rule consequent. multivariate gaussian function offers appealing input space partition notably data distributed underlying axes ellipsoidal cluster rotates direction trait capable lowering fuzzy rule demand retains inter-relations among input variables although rule premise less transparent conventional fuzzy rule pclass fitted transformation strategy allows extraction classical rule. rule growing strategy rule growing process pclass orchestrated three rule growing modules determines novelty data point whether deserves prototype rule. first rule growing strategy namely datum significance method estimates statistical contribution data sample indicates possible contribution whole course training process. derived assumption uniform distribution statistical contribution expressed zone influence ellipsoidal cluster. ignores summarization power rule consider strategic current position rule feature space hinders capability capture concept drift distance information provided enumerating importance fuzzy rules. second rule growing strategy namely data quality method forward. concept follows concept recursive density estimation density local region computed recursively. concept concludes rule addition necessary either data point represents relevant concept highest density data point beyond coverage existing rules method differs method facets involves weighting strategy reducing influence outliers causes drop density next samples; uses inverse multi-quadratic function lieu cauchy function; tailored multivariate gaussian function. oversized rule prone cluster delamination problem pinpoints situation distinct data clouds contained cluster. situation undermines generalization specificity cluster decreases significantly. third rule growing strategy aims overcome issue borrowing concept gart+ monitors coverage span winning rule obtained bayesian concept rule maximum posterior probability. limits growth winning rule rule introduced size winning rule exceeds prespecified level rule pruning recall strategy pclass equipped rule pruning strategies namely extended rule significance method potential+ method. method shares principle method estimates statistical discover inconsequential rules play little role final output lifespan. combines significance rule premise rule consequence quantify rule contribution. significance rule premise derived approximation accumulated contribution multivariate gaussian function lifespan without revisiting preceding samples. obtained uniform distribution assumption assumption results zone influence fuzzy rules indicator rule premise significance. contribution rule consequent measured weighted output weight vector since small rule weight normally generate negligible outputs. method monitors evolution rule respect current data trend vital non-stationary environments. aims find obsolete rules longer relevant delineate recent concept drift. scenario realised extending concept data potential rule pruning scenario. concept data potential performs recursive density estimation fuzzy regions pinpoints relevance fuzzy rules since fuzzy rules supported current data distribution expected return density. method however differs data potential concept kernel function using inverse multiquadratic function instead cauchy function. method also functions rule recall scenario capable handling recurring drift. recurring drift refers situation previous data distribution reappears future. trigger previously pruned rules portraying concept valid again. adding completely rule address cyclic drift coincide flexible nature human recall previous knowledge ease. furthermore adding rule risks catastrophic forgetting previously valid knowledge ignores learning history. previously pruned rules reactivated future provided relevance indicated method beats existing rules newly observed data point. worth noting previously pruned rules discounted training scenarios except update densities. paradigm ensures rule pruning scenario still relieves computational burden. parameter learning strategy data streams incur sufficient novelty prototype rule data streams useful refine influence zone existing rule base situation addressed fine-tuning rule premise winning rule. adaptation scenario derived sequential version maximum likelihood adapted multivariate gaussian function. furthermore pclass utilises direct update scheme inverse covariance matrix according formulas derived shelves reinversion covariance matrix. winning rule determined using bayesian concept rule maximum posterior probability selected winning rule. winning rule selection preferred compatibility measure since takes account rule’s population. rule consequent adjusted using fuzzily weighted generalized recursive least square method. fwgrls derivation fwrls method originally proposed angelov borrows concept weight decay function grls method fwgrls method works local learning scenario well-suited since offers decoupled adaptation scheme adaptation local region incurs cross correlation since local sub-model features unique output covariance matrix learning particular sub-model effect stability convergence rules. salient feature fwgrls method compared fwrls method lies generalized weight decay term cost function aims alleviate overfitting situation. weight decay term also supports compactness parsimony rule base forces rule consequent inconsequential rule small range. therefore inconsequential rules located method easily. quadratic weight decay term incorporated since capable reducing weight vector proportionally current values elaborate numerical validations pensemble using real-world data streams comparisons prominent classifiers. furthermore sensitivity predefined parameters analysed confirm user-friendly characteristic pensemble. simulations undertaken intel benchmarked prominent classifiers data streams falling three categories evolving classifiers metacognitive classifiers dynamic ensemble classifiers. underlying feature consolidated algorithms elaborated follows learn++nse seen pioneer works dynamic ensemble classifier non-stationary environments presents extension learn++ tackle concept drifts data streams. adaboost-like algorithm consists weak learners adopts concept sample weighting. underlying contribution observed dynamically weighted majority voting reflects dynamic contexts data streams. learn++cde generalized version learn++nse integrating specific mechanism handle class imbalanced problem data streams combines learn++nse well-established smoote using concept undersampling oversampling approaches imbalanced data streams. also proposes concepts subensemble class independent error weighting penalty constraint. learn++nse learn++cde built upon cart base classifier. pclass class evolving classifier putting forward open principle paradigm online learning capability pclass structured five-layered neural network working tandem actualising generalized fuzzy inference system. addition flexible network structure pclass equipped online feature weighting strategy. summed section paper. comparison necessary illustrate proposed ensemble learning scheme better single classifier version. etclass another case evolving classifiers unifying dynamic network structure online learning capability differs pclass since incorporates interval type- fuzzy working principle. features fast type-reduction method scalable online data stream processing. mcfis characterises so-called metacognitive learning machine assumed extension evolving classifiers metacognitive classifier shares principles evolving classifier except additional learning modules namely what-to-learn when-to-learn. mcfis metacognitive learning concept implemented roof neural-fuzzy system applies mimo architecture infer class label. consolidated algorithms numerically validated using real-world synthetic data streams featuring highly dynamic characteristics. popular problems characterizing abrupt gradual drifts namely sinh line dplane explored investigate performance consolidated algorithms unique property problems seen three versions correspond duration rate concept change. third version presenting complex variant used drift lasts longest duration. problems equipped stream generator offering concrete data stream environments. addition semi-artificial data streams namely iris also parts database incorporated. problems modified original version incorporating concept drift. problem introduced used bear efficacy benchmarked algorithms. moreover extension problem contributed ditzler polikar forward instead original version since offers class imbalance property cyclical drift often occurs real-world data streams. another popular problem data stream mining area namely gaussian problem exploited problem examine consolidated algorithms class contains gradual independent drift controlled mean variance parametric equations. hyperplane problem exploited inspect learning performance consolidated algorithms. problem well-known benchmark problem massive online analysis characterises gradual concept drift data initially drawn distribution slowly shifts another distribution probabilistic fashion. artificial semi-artificial data streams electricity pricing weather problems included experiments. problems widely used field data stream electricity pricing problem affected dynamic external attributes weather condition well-known recurring drift seasonal changes. characteristics data streams along detailed experimental procedures encapsulated table consolidated algorithms assessed evaluation criteria namely classification rate fuzzy rule input attribute network parameters execution time ensemble size. classification rate refers accuracy testing samples defined rate correctly classified testing samples fuzzy rule pensemble inspected total number fuzzy rules across local experts. input attributes pensemble sampled dynamically every training instance assigning crisp weights desired number input attributes predetermined process runs whereas input attributes algorithms conversely happens fixed. network parameters enumerated total number network parameters across local experts determined type network architecture. structural complexities base classifiers discussed recounted here. execution time obtained running time accomplish training process ensemble size measured number base classifiers deployed training process. numerical results consolidated algorithms tabulated table averaged number time stamps. table pensemble outperforms counterparts viewpoint accuracy produces highest accuracy study cases. depicted pensemble delivers almost improvement classification rate compared single model version pclass. pensemble’s accuracy inferior learn++nse learn++cde sinh gaussian iris+ data streams. nonetheless learn++nse learn++cde possess intractable structural complexities since ensemble size grows exponentially number data streams might wise option real-world data stream environments total number computing platform. pensemble overcomes learn++nse learn++cde context ensemble size study cases. worth noting pensemble makes drift detection method controlling growth ensemble structures. drift detection method brings step forward learn++.nse learn++.cde since data stream necessarily trigger introduction local expert local expert added conflict attributed concept change severe enough beyond scope existing local experts. scenario leads resilient approach deal plasticity-stability dilemma static ensemble greedy ensemble data streams unpredictable possibly infinite. realm fuzzy rule network parameters pensemble generates comparable level complexities even compared nonensemble classifiers. facts acceptable since pensemble features rule pruning scenarios analysing relevance base classifiers also approximation generalization performance base classifiers. moreover scenario contributes dynamic online substantially parameters without compromising predictive accuracy. compact parsimonious structures pensemble expedite execution times happened comparable single model counterparts even faster study cases. note claim execution time made consolidated algorithms executed classification rate fuzzy rule input attribute network parameters execution time ensemble size classification rate fuzzy rule input attribute network parameters execution time ensemble size classification rate fuzzy rule input attribute network parameters execution time ensemble size classification rate fuzzy rule input attribute network parameters execution time ensemble size classification rate fuzzy rule input attribute network parameters execution time ensemble size classification rate fuzzy rule input attribute network parameters execution time ensemble size classification rate fuzzy rule input attribute network parameters execution time ensemble size classification rate fuzzy rule input attribute network parameters execution time ensemble size classification rate fuzzy rule input attribute network parameters execution time ensemble size classification rate fuzzy rule input attribute network parameters execution time ensemble size classification rate fuzzy rule input attribute network parameters execution time ensemble size classification rate fuzzy rule input attribute network parameters execution time ensemble size classification rate fuzzy rule input attribute network parameters execution time ensemble size classification rate fuzzy rule input attribute network parameters execution time ensemble size classification rate fuzzy rule input attribute network parameters execution time ensemble size pensemble tested real-world problem namely prediction coronary heart disease study done using real-world dataset derived nested-case-control experiment within singaporean chinese health study cohort participants. subjects experiment donated blood never suffered stroke verified self-reported diagnosis data hospital discharge database .the goal study case identify disease outcome participants whether occurred december myocardial infarction coronary heart disease death grouped cases whereas others classified control. exclusion performed based several statistical criteria consequently shrunk scope study patients only. predictive analytics supported input attributes time baseline event baseline cholesterol level baseline cholesterol level baseline systolic blood pressure reading baseline whether subject anti-hypertensive medication baseline whether subject smoked baseline levels haemoglobin protein baseline body mass index baseline whether subject diabetes baseline sampling weight. prediction carried gender group samples male patients indicate cases female patients represent cases simulation followed -fold cross validation avoid data order dependency problem compared algorithms. numerical results reported table referring table pensemble produced competitive accuracy much lower parameter burden number input attributes pclass etclass. note online feature weighting mechanism pclass alleviate number input attributes. structural complexities directly affected running time pensemble occurred fastest male female participants datasets. comparison learn++nse learn++cds algorithm outperformed algorithms facets. prognostic health tool state management case namely automatic identification metal turning operation experiment took place variable speed centered lathe type lang swing work-piece materials namely alloy steel using coated cemented dynamometer installed record vibration signal cutting force signal three dimensional cutting axes machining process frequency collecting data samples channel measured variables came signal conditioning unit attached peripheral signal conditioning instruments main server. different machining parameters terms cutting speed feed-rate depth applied experiment simulate non-stationary machining environments. vibration well cutting force signals captured online generated data streams. true class label assigned using visual inspection flank nose wears cut. cuts lasted around seconds beginning machining process increased seconds complete stabilization cutting process. measurement flank nose wears manual inspection compared predefined thresholds categorize every observation classes follows input features extracted vibration force signals. encompass static dynamic forces acceleration three-dimensional axes feed rate cutting speed depth target variable consists four classes nominally sharp high flank wear high flank nose wear high flank wear chipped/fractured nose. experiment comprises parts first part benefited full input attributes reduced dimension injected second part. cutting speed depth deemed play little influence predictive quality aside. diagnosis process undertaken time stamps where time stamp samples utilized build hypothesis remaining samples testing samples. pensemble benchmarked algorithms previous sections evaluated criteria. consolidated numerical results reported table efficacy pensemble counterparts evident table pensemble best-performing algorithm almost evaluation criteria. results better expected since pensemble outperformed single-classifier algorithm realm fuzzy rule network parameters execution time. pensemble slightly worse etclass context network parameters recall attained better classification rate etclass. sensitivity analysis predefined parameters section concerns sensitivity analysis predefined parameters pensemble. goal study effect user-defined parameters learning performance. pensemble involves four user-defined parameters namely respectively fixed variations parameters committed delve influence overall performance. note parameters default values varying parameter. following values selected investigate sensitivity problem illustrating predictive analytics coronary heart diseases female patients schs cohort. experimental procedure remained section v.b. numerical results summarized table claim confirmed table predefined parameters except case-insensitive. shown table different values predefined parameters made little performance difference pensemble. decreasing factor impact learning performance lead substantial performance deterioration. decreasing factor affects compactness parsimony ensemble structure. higher value precludes ensemble pruning process discarding inactive classifier weight penalty imposed misclassification committed. assumption substantiated table ensemble complexity rises assigning decreasing factor based facts pensemble user-friendly simply apply userdefined parameters recommended paper. paper presents novel evolving ensemble classifier termed parsimonious ensemble pensemble feature unique characteristics evolving classifier namely pclass utilised local expert. flexible working principle pclass helps pensemble handle local drift data streams effectively features open structure fully online working principle. pensemble constitutes fully evolving ensemble classifier structure automatically generated self-expands concept drift detected. pensemble offers parsimonious working principle resulted pruning activities inactive classifiers. equipped ensemble pruning strategies assess relevance generalization power local expert. online feature selection strategy incorporated pensemble. mechanism actively selects subset input attributes differs common practise literature allows arrive different subsets input attributes every training observation. efficacy pensemble numerically validated realworld synthetic data streams. compared well-known algorithms algorithm delivers highest accuracy study cases. also found pensemble generated comparable complexities single classifier variants less complexities ensemble classifier variants. future work directed toward investigation granular computing data stream analytics address high-level data abstraction. first author thank prof. plamen angelov thorough discussion history eis. first author also thank agus salim prof. eric dimla sharing datasets. third author acknowledges support austrian comet-k programme linz center mechatronics funded austrian federal government federal state upper austria. publication reflects authors' views.", "year": 2017}