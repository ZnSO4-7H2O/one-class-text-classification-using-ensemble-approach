{"title": "Binarized Neural Networks on the ImageNet Classification Task", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "We trained Binarized Neural Networks (BNNs) on the high resolution ImageNet ILSVRC-2102 dataset classification task and achieved a good performance. With a moderate size network of 13 layers, we obtained top-5 classification accuracy rate of 84.1 % on validation set through network distillation, much better than previous published results of 73.2% on XNOR network and 69.1% on binarized GoogleNET. We expect networks of better performance can be obtained by following our current strategies. We provide a detailed discussion and preliminary analysis on strategies used in the network training.", "text": "trained binarized neural networks high resolution imagenet ilsvrc- dataset classiﬁcation task achieved good performance. moderate size network layers obtained top- classiﬁcation accuracy rate percent validation network distillation much better previous published results xnor network binarized googlenet. expect networks better performance obtained following current strategies. provide detailed discussion preliminary analysis strategies used network training. recent wave deep learning research brought deep neural network frontier applications. current implementations deep neural networks wide variety ﬁelds heavily rely high computing power energy hungry hardware gpus limits implementations neural networks embedded environment mobile phones wearable hardware. much effort reduce computing cost deep neural networks example recently trying reduce synapse weights and/or intermediate signal representation binary form. shown binariﬁcation neural networks dramatically reduce computing cost well amount memory needed weight intermediate results storage. approaches especially useful memory constrained environment fpga implementations. work designed network architecture based binarized neural networks neural network structure originally proposed trained networks ilsvrc image classiﬁcation task analyzed behavior weights adaptation training observation guide setting proper learning rates used network training. leads signiﬁcant gain rate convergence. order increase recognition accuracy maintains high efﬁciency special neural network architecture proposed here. network usual channels regular networks early layers normal layers used rest layers. obtained percent top- accuracy network. improvement network performance introduced network distillation moderate size network percent top- accuracy single crop validation ilsvrc classiﬁcation task achieved much better previous figure comparison network convergence speed different learning rates. distribution synapse states epoch training. synapses layer bottom synapses layer lower learing rate signiﬁcantly accerelated convergence speed. networks silmar alexnet used batch size original layer designed cifar- dataset layers network binary. natural images usually ﬁrst layer input input channels thus similar also regular weight convolutional layer ﬁrst layer network. typical layer training stage real-valued synaptic weights used training. extra states removed stage computing layer output. believed real-valued weights needed smooth noise learning ﬁeld computational neuroscience called binary synapses internal/hidden states. show indeed case. high learning rate used training epoch training observed weight values stay around weight clipping. condition weights quickly jumped edge values little accumulation process equivalently less hidden synapse states available training process. correspondingly rather slow accuracy rate climbing process observed lower learning rate observed weight distribution uniform concentrated around indicating weights taking steps travel resemble behavior synapse tags hidden states correspondingly much faster accuracy climbing observed training process following aforementioned strategies managed train bvlc-alexnet top- accuracy rate slightly better shown hubara pretraining network soft tanh activation gave improvement reach accuracy rate. also trained layer network obtained top- accuracy rate network used regular weights last fully connected layer dropout ratio used dropout layer. notice wider usual layers used ﬁrst second layers figure training high performance networks bvlc-alexnet network trained regular targets. layer network trained regular targets achieved top- accuracy. layer network trained soft targets. fine tuning layer network combined soft regular target achieved top- accuracy designed avoid information bottleneck associated binarized networks. feature wider layers used early layers rest part network stay normal width. feature crucial maintains computing cost brought bnns. improvement appears hindered network over-ﬁtting. adding layers network might improve network performance however strategy computing budget. since maintaining high efﬁciency crucial real world implementations adopted network distillation used pre-trained layer residual generate soft training targets. agreement found purely training soft targets improve network performance. combined soft regular targets training signiﬁcantly improved network performance network pre-trained soft targets. network structure used part table except dropout used ﬁxed scaling layer softmax. limited computing resource available test bigger networks better performance. results show networks trained ilsvrc data achieve good performance proper network architectures training strategies. moment best network trained layers network percent top- accuracy rate single crop test validation set. expect improvement obtained through testing multiple scales/crops average distill-training networks better accuracy tuning hyper-parameters beyond handful distillation networks trained limited computing resources. cleaning code release soon. matthieu courbariaux yoshua bengio jean-pierre david. binaryconnect training deep neural networks binary weights propagations. advances neural information processing systems matthieu courbariaux itay hubara daniel soudry el-yanivand yoshua bengio. binarynet training deep neural networks weights activations constrained or-. arxiv preprint arxiv. itay hubara matthieu courbariaux daniel soudry el-yaniv yoshua bengio. quantized neural networks training neural networks precision weights activations. arxiv preprint arxiv. alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems", "year": 2016}