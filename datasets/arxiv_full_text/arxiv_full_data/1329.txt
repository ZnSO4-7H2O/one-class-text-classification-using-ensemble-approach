{"title": "Generative Modeling of Convolutional Neural Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "The convolutional neural networks (CNNs) have proven to be a powerful tool for discriminative learning. Recently researchers have also started to show interest in the generative aspects of CNNs in order to gain a deeper understanding of what they have learned and how to further improve them. This paper investigates generative modeling of CNNs. The main contributions include: (1) We construct a generative model for the CNN in the form of exponential tilting of a reference distribution. (2) We propose a generative gradient for pre-training CNNs by a non-parametric importance sampling scheme, which is fundamentally different from the commonly used discriminative gradient, and yet has the same computational architecture and cost as the latter. (3) We propose a generative visualization method for the CNNs by sampling from an explicit parametric image distribution. The proposed visualization method can directly draw synthetic samples for any given node in a trained CNN by the Hamiltonian Monte Carlo (HMC) algorithm, without resorting to any extra hold-out images. Experiments on the challenging ImageNet benchmark show that the proposed generative gradient pre-training consistently helps improve the performances of CNNs, and the proposed generative visualization method generates meaningful and varied samples of synthetic images from a large-scale deep CNN.", "text": "paper investigates generative modeling convolutional neural networks main contributions include construct generative model cnns form exponential tilting reference distribution. propose generative gradient pre-training cnns non-parametric importance sampling scheme fundamentally different commonly used discriminative gradient computational architecture cost latter. propose generative visualization method cnns sampling explicit parametric image distribution. proposed visualization method directly draw synthetic samples given node trained hamiltonian monte carlo algorithm without resorting extra hold-out images. experiments imagenet benchmark show proposed generative gradient pre-training helps improve performances cnns proposed generative visualization method generates meaningful varied samples synthetic images large deep cnn. recent years witnessed triumphant return feedforward neural networks especially convolutional neural networks despite successes discriminative learning cnns generative aspect cnns thoroughly investigated. useful following reasons generative pre-training potential lead network better local optimum; samples drawn generative model reveal knowledge learned cnn. although many generative models learning algorithms proposed applied learning large deep cnns. paper study generative modeling cnns. start deﬁning probability distributions images given underlying object categories class labels ﬁnal logistic regression layer serves corresponding conditional distribution class labels given images. distributions form exponential tilting reference distribution i.e. exponential family models energy-based models relative reference distribution. generative model proceed study along related themes differ handle reference distribution null model. ﬁrst theme propose non-parametric generative gradient pre-training learned stochastic gradient algorithm seeks minimize log-likelihood generative model. gradient loglikelihood approximated importance sampling method keeps reweighing images sampled non-parametric implicit reference distribution distribution training images. generative gradient fundamentally different commonly used discriminative gradient batch training shares computational architecture well computational cost discriminative gradient. generative learning scheme used pre-training stage followed usual discriminative training. generative log-likelihood provides stronger driving force discriminative criteria stochastic gradient requiring learned parameters explain images instead labels. experiments mnist imagenet classiﬁcation benchmarks show generative pre-training scheme helps improve performance cnns. second theme study generative modeling assume explicit parametric form reference distribution gaussian white noise model draw synthetic images resulting probability distributions images. sampling accomplished hamiltonian monte carlo algorithm iterates bottomconvolution step top-down deconvolution step. proposed visualization method directly draw samples synthetic images given node trained without resorting extra hold-out images. experiments show meaningful varied synthetic images generated nodes large deep discriminatively trained imagenet. generative model study energy-based model. models include ﬁeld experts product experts boltzmann machines model based neural networks etc. however generative models learning algorithms applied learning large deep cnns. relationship generative models discriminative approaches extensively studied usefulness generative pre-training deep learning studied erhan etc. however issue thoroughly investigated cnns. visualization work related erhan girshick zeiler fergus long girshick long highscoring image patches directly presented. zeiler fergus top-down deconvolution process employed understand contents emphasized high-scoring input image patches. erhan simonyan images synthesized maximizing response given node network. work generative model formally deﬁned. sample well-deﬁned probability distribution algorithm generating meaningful varying synthetic images without resorting large collection holdimages distribution form exponential tilting reference distribution considered energy-based model exponential family model. model reference distribution unique. change change log/q] correspond different parametrization ﬂexible enough. want choose either reasonably close non-parametric generative gradient method resulting based easy sample generative visualization method. form multi-class logistic regression treated intercept parameter estimated directly model trained discriminatively. thus notational simplicity shall assume intercept term already absorbed rest paper. note unique change common categories still non-uniqueness corresponds non-uniqueness mentioned above. given labeled data equations suggest different methods estimate prior probability estimated class frequency category maximize discriminative model popular choice multi-layer perceptron connection weights top-layer multi-class logistic regression. choice adopt throughout paper. approximated importance sampling. speciﬁcally {˜xj}m samples instance distribution images categories. attempt model parametrically instead treat implicit non-parametric distribution. importance sampling discriminative gradient generative gradient differ subtly fundamentally calculating e/∂w] whose difference observed ∂fyi provides driving force updating discriminative gradient expectation respect posterior distribution class label image ﬁxed whereas generative gradient expectation respect distribution images class label ﬁxed. general easier adjust parameters predict class labels reproduce features images. expected generative gradient provides stronger driving force updating non-parametric generative gradient especially useful beginning stage training called pre-training small current category separated overall distribution stage importance weights skewed effective sample size importance sampling large. updating according generative gradient provide useful pre-training potential lead toward good local optimum. importance weights start become skewed effective sample size starts dwindle indicates categories start separate well other switch discriminative training separate categories. ﬁrst glance generative gradient appears computationally expensive need sample fact collection images categories batch samples approximation batch training mode. speciﬁcally moreover computation generative gradient induced share back propagation architecture discriminative gradient. speciﬁcally calculation generative gradient decoupled calculation generative loss layer calculation lower layers. speciﬁc replacing {˜xj}m rewrite following form pyi/∂fy called generative loss layer treated variable chain rule) calculation ∂fy/∂w exactly discriminative gradient. decoupling brings simplicity programming. notation pyi/∂fy generative layer mainly make conformal chain rule calculation. according pyi/∂fy deﬁned recently researchers interested understanding machine learns. suppose care node layer consider generating samples already learned discriminative training purpose need assume parametric reference distribution gaussian white noise distribution. discriminatively learning sample corresponding hamiltonian monte carlo speciﬁcally category write exp) −fy+ |x|/ physics context position vector potential energy function. implement hamiltonian dynamics need introduce auxiliary momentum vector corresponding kinetic energy function |φ|/m denotes mass. thus ﬁctitious physical system described canonical coordinates deﬁned total energy iteration draws random sample marginal gaussian distribution evolve according hamiltonian dynamics conserves total energy. step leapfrog algorithm computation derivative potential energy function ∂u/∂x includes calculating ∂fy/∂x. computation ∂fy/∂x involves bottom-up convolution max-pooling followed top-down deconvolution arg-max un-pooling. max-pooling arg-max un-pooling applied current synthesized image top-down derivative computation derived different zeiler fergus visualization sequence category shown fig. generative pre-training experiments three different training approaches studied discriminative gradient generative gradient iii) generative gradient pre-training discriminative gradient reﬁning build algorithms code caffe experiment settings identical experiments performed commonly used image classiﬁcation benchmarks mnist handwritten digit recognition imagenet ilsvrc- natural image classiﬁcation. mnist handwritten digit recognition. ﬁrst study generative pre-training mnist dataset. lenet network utilized default mnist caffe. although higher accuracy achieved utilizing deeper networks random image distortion stick baseline network fair comparison experimental efﬁciency. network training testing performed train test sets respectively. three training approaches stochastic gradient descent performed training batch size base learning rate weight decay term momentum term epoch number gg+dg pre-training stage stops epochs discriminative gradient tuning stage starts base learning rate experimental results presented table error rate lenet trained discriminative gradient trained generative gradient error rate reduces generative gradient pre-training discriminative gradient reﬁning applied error rate reduces lower discriminative gradient. imagenet ilsvrc- natural image classiﬁcation. experiments imagenet ilsvrc networks utilized namely alexnet zeilerfergusnet network training testing performed train sets respectively. training single network trained stochastic gradient descent batch size base learning rate weight decay term momentum term epoch number gg+dg pre-training stage stops epochs discriminative gradient tuning stage starts base learning rate testing top- classiﬁcation error rates reported classifying center four corner crops input images. shown table error rates discriminative gradient training applied alexnet zeilerfergusnet respectively error rates generative gradient respectively. generative gradient pre-training followed discriminative gradient reﬁning achieves error rates respectively lower discriminative gradient. experiment results mnist imagenet ilsvrc- show generative gradient pretraining followed discriminative gradient reﬁning improves classiﬁcation accuracies varying networks. beginning stage training updating network parameters according generative gradient provides useful pre-training leads network parameters toward good local optimum. computational cost generative gradient discriminative gradient. computational cost generative loss layer ignorable network compared computation convolutional layers fully-connected layers. total epoch numbers gg+dg generative visualization experiments visualize nodes lenet network alexnet network trained discriminative gradient mnist imagenet ilsvrc- respectively. algorithm visualize networks trained generative gradient well. ﬁrst visualize nodes ﬁnal fully-connected layer lenet. experiments delete drop-out layer avoid unnecessary noise visualization. beginning visualization initialized gaussian distribution standard deviation iteration number leapfrog step size leapfrog step number standard deviation reference distribution particle mass respectively. visualization results shown fig. visualize nodes alexnet much larger network compared lenet. nodes intermediate convolutional layers ﬁnal fully-connected layer visualized. visualize intermediate layers instance layer conv ﬁlters layers conv removed generative visualization layer. size synthesized images designed dimension response conv visualize ﬁlter assigning label leapfrog step size leapfrog step number standard deviation reference distribution particle mass respectively. iteration numbers nodes intermediate convolutional ﬁnal fully-connected layer respectively. synthesized images ﬁnal layer initialized zero image. samples intermediate convolutional layers ﬁnal fully-connected layer alexnet shown fig. respectively. algorithm produces meaningful varied samples reveals learned nodes different layers network. note samples generated trained model directly without using large hold-out collection images girshick zeiler fergus long computational cost varies nodes different layers within different networks. desktop titian takes minute draw sample nodes ﬁnal fully-connected layer lenet. alexnet nodes ﬁrst convolutional layer ﬁnal fully-connected layer takes minute minute draw sample respectively. code downloaded http//www.stat.ucla.edu/˜yang.lu/ project/generativecnn/main.html given recent successes cnns worthwhile explore generative aspects. work show simple generative model constructed based cnn. generative model helps pre-train cnn. also helps visualize knowledge learned cnn. proposed visualizing scheme sample generative model turned parametric generative learning algorithm generative gradient approximated samples generated current model. references deng dong socher richard li-jia fei-fei imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference ieee erhan dumitru bengio yoshua courville aaron manzagol pierre-antoine vincent pascal bengio samy. unsupervised pre-training help deep learning? journal machine learning research girshick ross donahue jeff darrell trevor malik jitendra. rich feature hierarchies accurate object detection semantic segmentation. computer vision pattern recognition ieee conference ieee hinton geoffrey osindero simon welling yee-whye. unsupervised discovery nonlinear structure using contrastive backpropagation. cognitive science yangqing shelhamer evan donahue jeff karayev sergey long jonathan girshick ross guadarrama sergio darrell trevor. caffe convolutional architecture fast feature embedding. arxiv preprint arxiv. krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems quoc monga rajat devin matthieu chen corrado greg dean jeff andrew building high-level features using large scale unsupervised learning. international conference machine learning lecun yann boser bernhard denker john henderson donnie howard richard hubbard wayne jackel lawrence backpropagation applied handwritten code recognition. neural computation rifai salah vincent pascal muller xavier glorot xavier bengio yoshua. contractive autoencoders explicit invariance feature extraction. proceedings international conference machine learning simonyan karen vedaldi andrea zisserman andrew. deep inside convolutional networks visualising image classiﬁcation models saliency maps. workshop international conference learning representations category ﬁxed whereas generative gradient want assign high score well observations belong assign scores observations belong constraint regardless discriminative gradient want work together different assigns high score apparently discriminative constraint weaker involves generative constraint stronger involves single generative learning well behaved continue reﬁne satisfy discriminative constraint.", "year": 2014}