{"title": "Driven to Distraction: Self-Supervised Distractor Learning for Robust  Monocular Visual Odometry in Urban Environments", "tag": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "abstract": "We present a self-supervised approach to ignoring \"distractors\" in camera images for the purposes of robustly estimating vehicle motion in cluttered urban environments. We leverage offline multi-session mapping approaches to automatically generate a per-pixel ephemerality mask and depth map for each input image, which we use to train a deep convolutional network. At run-time we use the predicted ephemerality and depth as an input to a monocular visual odometry (VO) pipeline, using either sparse features or dense photometric matching. Our approach yields metric-scale VO using only a single camera and can recover the correct egomotion even when 90% of the image is obscured by dynamic, independently moving objects. We evaluate our robust VO methods on more than 400km of driving from the Oxford RobotCar Dataset and demonstrate reduced odometry drift and significantly improved egomotion estimation in the presence of large moving vehicles in urban traffic.", "text": "fig. robust motion estimation urban environments using single camera learned ephemerality mask. making left turn onto main road large passes front vehicle obscuring view scene learned ephemerality mask correctly identiﬁes unreliable region image purposes motion estimation traditional visual odometry approaches incorrectly estimate strong translational motion right dominant motion whereas approach correctly recovers vehicle egomotion rejection scheme. leveraging depth ephemerality outputs network produce robust metricscale using single camera mounted vehicle. approach leads signiﬁcantly reliable motion estimation evaluated hundreds kilometres driving complex urban environments presence heavy trafﬁc challenging conditions. estimating ephemerality mask closely related background subtraction approaches build statistics background appearance based training data static camera identify discrepancies live images. methods typically used surveillance applications abstract— present self-supervised approach ignoring distractors camera images purposes robustly estimating vehicle motion cluttered urban environments. leverage ofﬂine multi-session mapping approaches automatically generate per-pixel ephemerality mask depth input image train deep convolutional network. run-time predicted ephemerality depth input monocular visual odometry pipeline using either sparse features dense photometric matching. approach yields metric-scale using single camera recover correct egomotion even image obscured dynamic independently moving objects. evaluate robust methods driving oxford robotcar dataset demonstrate reduced odometry drift signiﬁcantly improved egomotion estimation presence large moving vehicles urban trafﬁc. autonomous vehicle operation crowded urban environments presents number challenges system based visual navigation motion estimation. urban trafﬁc image obscured large moving object standard outlier rejection schemes ransac produce incorrect motion estimates large consensus features tracked moving object. robust distractionfree visual navigation deeper understanding image regions static ephemeral order better decide features motion estimation. paper leverage large-scale ofﬂine mapping deep learning approaches produce per-pixel ephemerality mask run-time without requiring semantic classiﬁcation manual labelling illustrated fig. project video. ephemerality mask predicts stable image regions likely useful motion estimation contrast dynamic ephemeral objects contrast semantic segmentation approaches explicitly label objects belonging a-priori chosen classes hence require manually annotated training data approach trained using repeated traversals route lidar-equipped survey vehicle producing per-pixel depth ephemerality labels deep convolutional network fully selfsupervised process. conversely signiﬁcant body work detection tracking moving objects applied robust dynamic environments scale references monocular slam however approaches require large quantities manually-labelled training data moving objects chosen object classes must cover possibly-moving objects avoid false negatives. recent slam approaches integrated per-pixel semantic segmentation layers improve reconstruction quality rely laboriously manually-annotated training data chosen classes encompass object categories. unsupervised approaches recently introduced estimate depth egomotion reconstruction methods attractive large-scale require video footage monocular stereo camera without ground-truth motion estimates semantic labels. particular introduces explainability mask highlights image regions disagree dominant motion estimate. however explainability mask differs ephemerality mask recognises non-dominant moving objects hence still produce incorrect motion estimates signiﬁcantly occluded large independently moving object. approach inspired distraction-suppression methods presented methods prior estimate mask quantiﬁes reliability motion estimation integrated pipeline. signiﬁcantly extend prior approach multi-session mapping quantify ephemerality using structural entropy metric result automatically generate training data deep convolutional network. result approach rely live localisation prior live dense depth estimation stereo hence operate wider range locations reduced sensor suite. section outline approach automatically building ephemerality masks leveraging ofﬂine mapping pipeline. note lidar stereo camera sensors required survey vehicle collect training data; run-time monocular camera required. method takes following steps prior mapping using survey vehicle equipped stereo camera lidar scanner perform multiple traversals target environment. analysing structural consistency across multiple mapping sessions entropy-based approach determine constitutes static structure scene. ephemerality labelling project prior static structure every stereo camera image collected survey compare structure computed dense stereo approach presence fig. multi-session mapping pointcloud entropy computation. traversal compute global pose vehicle timestamp project points global frame analyse neighbourhood point neighbourhoods points well distributed traversals scene likely static points mostly derived traversal structure ephemeral. quantify static scenes using entropy metric applied neighbourhood trafﬁc dynamic objects differ considerably; compute ephemerality weighted disparity normal difference prior true structure. network training train deep convolutional network predict resulting pixel-wise depth ephemerality mask using input monocular images. run-time produce live depth ephemerality masks even locations traversed survey vehicle. given survey vehicle equipped camera lidar illustrated fig. performed number traverses environment recover global camera pose time relative world frame largescale ofﬂine process using stereo mapping navigation approach compute position world frame using camera lidar point pose lidar-camera calibration follows given pointcloud points collected traversals wish compute local entropy region pointcloud quantify reliable region across traversal. deﬁne neighbourhood function point belongs neighbourhood satisﬁes following condition fig. prior mapping determine static scene structure. alignment multiple traversals route yield large number points present single traversals e.g. trafﬁc parked vehicles shown white. points corrupt synthetic depth entropy-based approach removes points observed traversals retains structure remained static duration data collection resulting high-quality synthetic depth maps neighbourhood entropy exceeds minimum threshold points estimated ephemeral removed static prior. pointcloud construction neighbourhood entropy ephemeral point removal process illustrated fig. given prior static pointcloud globally aligned camera poses produce synthetic depth survey image illustrated fig. handle visibility constraints make hidden point removal approach every pixel valid prior point projects compute expected disparity using local structure pointcloud. presence dynamic objects scene observed camera differ expected prior map. ofﬂine dense stereo reconstruction approach compute true disparity normal pixel survey image illustrated fig. deﬁne ephemerality mask weighted difference expected static true disparity normals follows adopt convolutional encoder-multi-decoder network architecture predict disparity ephemerality masks single image illustrated fig. adding additional decoder architecture train disparity output stereo photometric loss proposed optionally semi-supervised using prior lidar disparity ensure metric-scaled outputs. ephemerality output loss pixel valid ephemerality label. balance losses using multi-task learning approach continuously updates inter-task weighting training. trained model scratch epochs batch size using adam optimiser used initial learning rate kept constant ﬁrst epochs halving every epochs end. leverage live depth ephemerality mask produced network produce reliable visual odometry estimates accurate metric scale. present robust approaches sparse feature-based approach dense photometric approach. integrates ephemerality mask order estimate egomotion using static parts scene uses learned depth estimate relative motion correct scale. improves upon traditional monocular systems cannot recover absolute scale odometry approaches optimised realtime performance vehicle platform. fig. ephemerality labelling process. input images compute true disparity normals using ofﬂine dense stereo approach. project prior pointcloud image form prior disparity disparity normal error terms combined form ephemerality mask sparse monocular approach derived wellknown stereo approaches sets features detected matched across successive frames build relative pose estimate. feature parameterised follows warping function projects matched feature current image according relative pose camera intrinsics. extracted features typically small subset total number pixels image. step function used disable residual according predicted ephemerality follows maximum ephemerality threshold valid feature typically practice detect sparse features using fast corners match using brief descriptors real-time operation. dense monocular approach adopt method combine learned depth maps photometric relative pose estimation rather subset pixels pixels within reference keyframe image warped current image relative pose recovered minimising photometric error follows fig. network architecture ephemerality disparity learning. width block indicates spatial dimensions feature vary factor blocks. number output channels ﬁlter dimensions also detailed block. input data ephemerality-aware visual odometry. given fig. input image network predicts dense depth ephemerality mask. sparse approaches ephemerality mask used select features used optimisation dense approaches photometric error term weighted directly ephemerality mask image function returns pixel intensity ephemerality mask used directly weight photometric residual; thresholding required. fig. illustrates predicted depth selected sparse features weighted dense intensity values used typical urban scene. benchmarked approach using hundreds kilometres data collected autonomous vehicle platform complex urban environment. goal quantify performance ephemerality-aware visual odometry approach presence large dynamic objects trafﬁc. train approach using eight traversals oxford robotcar dataset total approximately driving. robotcar vehicle equipped bumblebee stereo camera lms- pushbroom lidar scanner. training downsample input images pixels subsample image every metre use; total images used training. run-time produce ephemerality masks depth maps using single gpu. evaluate approach oxford traversals total evaluation datasets contain multiple detours alternate routes ensuring method tested locations present training datasets. quantify performance ephemeralityaware compute translational rotational drift rates using approach proposed kitti odometry benchmark speciﬁcally compute average end-pointerror subsequences length metres compared system installed vehicle. addition compare instantaneous translational velocities method reported system manually selected locations include distractors evaluate velocity estimation errors comparison average locations. allows focus dynamic scenes independently moving objects produce erroneous velocity estimates baseline methods. end-point-error evaluation methods presented table cases addition ephemerality mask reduced average translational rotational drift full evaluation datasets. note metric scale translational drift derived depth produced network hence systems report translation units metres overall error rates using monocular camera. sparse velocity error evaluation methods evaluation presented table across ephemerality-aware odometry approaches datasets produce lower average velocity errors. however locations distractors ephemerality-aware approaches produce signiﬁcantly accurate velocity estimates baseline approaches. particular robust sparse approach almost unaffected distractors whereas baseline method reports errors times greater. dense approach generally produces poorer translational velocity estimates sparse approach corresponds higher rates reported previous section. fig. presents distribution velocity errors approaches presence distractors. paper introduced concept ephemerality mask estimates likelihood pixel input image corresponds either reliable static structure dynamic objects environment learned using automatic self-supervised approach. crucially require manual labelling choice semantic classes order train approach run-time require single monocular camera produce reliable ephemerality-aware visual odometry metric scale. hundreds kilometres approach produces improved odometry resulting lower drift rates signiﬁcantly robust velocity estimates presence large dynamic objects urban scenes. restricted improving motion estimation number avenues explore future work. fig. illustrates foreground/background segmentation performed using ephemerality mask; currently background guide motion estimation detection classiﬁcation approach could guided foreground mask efﬁciently track dynamic objects scene. plan integrate approaches paper improved localisation motion estimation obstacle avoidance scene understanding fully autonomous vehicles operating complex urban environments. fig. ephemerality masks produced challenging urban environments. masks reliably highlight diverse range dynamic objects highly varied distances orientations. even buses trucks almost entirely obscure camera image successfully masked despite lack scene context. robust approaches make ephemerality mask provide correct motion estimates even static scene occluded independently moving object. song chandraker robust scale estimation real-time monocular autonomous driving proceedings ieee conference computer vision pattern recognition civera g´alvez-l´opez riazuelo tard´os montiel towards semantic slam using monocular camera intelligent robots systems ieee/rsj international conference mcmanus churchill napier davis newman distraction suppression vision-based pose estimation city scales robotics automation ieee international conference linegar churchill newman made measure bespoke landmarks -hour all-weather localisation camera ieee international conference robotics automation ieee katz basri direct visibility point sets transactions graphics vol. hirschmuller accurate efﬁcient stereo processing semiglobal matching mutual information computer vision pattern recognition cvpr ieee computer society conference vol. fig. velocity estimation errors presence distractors. sparse ephemerality-aware approach signiﬁcantly outperforms baseline approach producing fewer outliers m/s. dense ephemerality-aware approach perform well still outperforms baseline. vertical axis scaled highlight outliers. fig. ephemerality masks widely applicable autonomous vehicles. scene ephemerality mask used inform localisation static scene whilst guiding object detection ephemeral elements felzenszwalb girshick mcallester ramanan object detection discriminatively trained part-based models ieee transactions pattern analysis machine intelligence vol.", "year": 2017}