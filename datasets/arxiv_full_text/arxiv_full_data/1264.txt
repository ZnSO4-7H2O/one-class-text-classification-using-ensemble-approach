{"title": "\"Zero-Shot\" Super-Resolution using Deep Internal Learning", "tag": ["cs.CV", "cs.LG", "cs.NE", "eess.IV"], "abstract": "Deep Learning has led to a dramatic leap in Super-Resolution (SR) performance in the past few years. However, being supervised, these SR methods are restricted to specific training data, where the acquisition of the low-resolution (LR) images from their high-resolution (HR) counterparts is predetermined (e.g., bicubic downscaling), without any distracting artifacts (e.g., sensor noise, image compression, non-ideal PSF, etc). Real LR images, however, rarely obey these restrictions, resulting in poor SR results by SotA (State of the Art) methods. In this paper we introduce \"Zero-Shot\" SR, which exploits the power of Deep Learning, but does not rely on prior training. We exploit the internal recurrence of information inside a single image, and train a small image-specific CNN at test time, on examples extracted solely from the input image itself. As such, it can adapt itself to different settings per image. This allows to perform SR of real old photos, noisy images, biological data, and other images where the acquisition process is unknown or non-ideal. On such images, our method outperforms SotA CNN-based SR methods, as well as previous unsupervised SR methods. To the best of our knowledge, this is the first unsupervised CNN-based SR method.", "text": "deep learning dramatic leap superresolution performance past years. however supervised methods restricted speciﬁc training data acquisition lowresolution images high-resolution counterparts predetermined without distracting artifacts real images however rarely obey restrictions resulting poor results sota methods. paper introduce zero-shot exploits power deep learning rely prior training. exploit internal recurrence information inside single image train small image-speciﬁc test time examples extracted solely input image itself. such adapt different settings image. allows perform real photos noisy images biological data images acquisition process unknown non-ideal. images method outperforms sota cnn-based methods well previous unsupervised methods. best knowledge ﬁrst unsupervised cnn-based method. super-resolution single image recently received huge boost performance using deep-learning based methods recent sota method exceeds previous non-deep methods unsupervised huge margin boost performance obtained deep well engineered cnns trained exhaustively external databases lengthy periods time however externally supervised methods perform extremely well data satisfying conditions trained performance deteriorates signiﬁcantly conditions satisﬁed. example cnns typically trained highquality natural images low-resolution images generated speciﬁc predeﬁned downscaling kernel without distracting artifacts predeﬁned scaling-factor fig. shows happens conditions satisﬁed e.g. image generated nonideal downscaling kernel contains aliasing effects simply contains sensor noise compression artifacts. fig. shows contrived cases rather occur often dealing real images images downloaded internet images taken iphone historic images etc. ‘non-ideal’ cases sota methods often produce poor results. paper introduce zero-shot exploits power deep learning without relying prior image examples prior training. exploit internal recurrence information within single image train small image-speciﬁc test time examples extracted solely input image such adapted different settings image. allows perform real images acquisition process unknown non-ideal ‘non-ideal’ images method outperforms externallytrained sota methods large margin. figure real images real-world images rarely obey ‘ideal conditions’ assumed supervised methods. example historic photos images taken smartphones random images internet etc. since zssr trains test time examples extracted test image better performing ‘in-the-wild’ full sized images found project website. figure ‘non-ideal’ images controlled experiment. image generated aliasing image generated non-ideal downscaling kernel. unknown image-speciﬁc kernel estimated directly test image using image-speciﬁc downscaling kernel full sized images found project website. quantitative evaluation hundreds ‘non-ideal’ images found sec. shown strong property natural imformed basis many unsuages pervised image enhancement methods including unsupervised blind-sr blind-deblurring blinddehazing more. unsupervised methods exploit image-speciﬁc information typically rely simple eucledian similarity small image patches predeﬁned size using knearest-neighbours search. such generalize well patches exist image implicitly learned similarity measures adapt non-uniform sizes repeating structures inside image. image-speciﬁc leverages power cross-scale internal recurrence image-speciﬁc information without restricted above-mentioned limitations patch-based methods. train infer complex image-speciﬁc hr-lr relations image downscaled versions apply learned relations input image produce output. outperforms unsupervised patchbased large margin. since visual entropy inside single image much smaller general external collection images small simple sufﬁces image-speciﬁc task. hence even though network trained test time train+test runtime comparable test runtime sota supervised cnns. interestingly image-speciﬁc produces impressive results ‘ideal’ benchmark datasets used sota supervised methods surpasses sota supervised large margin ‘non-ideal’ images. provide visual empirical evidence statements. term zero-shot used here borrowed domains recognition/classiﬁcation. note however unlike approaches zero-shot learning one-shot learning approach require side information/attributes additional images. single test image hand kind nothing else. nevertheless additional information available provided image-speciﬁc make good test time improve results. contributions therefore several-fold best knowledge ﬁrst unsupervised cnn-based method. handle non-ideal imaging conditions wide variety images data types require pretraining modest amounts computational resources. applied size theoretically also aspect-ratio. adapted known well unknown imaging conditions provides sota results images taken ‘nonideal’ conditions competitive results ‘ideal’ conditions sota supervised methods trained fundamental approach fact natural images strong internal data repetition. example small image patches shown repeat many times inside single image within scale well across different image scales. observation empirically veriﬁed using hundreds natural images shown true almost small patch almost natural image. fig. shows example simple single-image based internal patch recurrence note able recover tiny handrails tiny balconies since evidence existence found elsewhere inside image larger balconies. fact evidence existence tiny handrails exists internally inside image different location different scale. cannot found external database examples matter large dataset seen sota methods fail recover imagespeciﬁc information relying externally trained images. strong internal predictive-power exempliﬁed using ‘fractal-like’ image internal predictivepower analyzed shown strong almost natural image fact empirically shown internal entropy patches inside single image much smaller external entropy patches general collection natural images. gave rise observation internal image statistics often provides stronger predictive-power external statistics obtained general image collection. preference further shown particularly strong growing uncertainty image degradations details). figure internal predictive power image-speciﬁc information. simple unsupervised internal-sr able reconstruct tiny handrail tiny balconies whereas externally-trained sota methods fail evidence existence tiny handrails exists internally inside image different location scale evidence found external database images image-speciﬁc combines predictive power entropy internal image-speciﬁc information generalization capabilities deep-learning. given test image external examples available train construct image-speciﬁc tailored solve task speciﬁc image. train examples extracted test image itself. examples obtained downscaling image generate lower-resolution version itself relatively light train reconstruct test image lower-resolution version apply resulting trained test image using input network order construct desired output note trained fully convolutional hence applied images different sizes. since training consists instance employ data augmentation extract lr-hr example-pairs train augmentation done downscaling test image many smaller versions play role supervision called fathers. fathers downscaled desired scale-factor obtain sons form input training instances. resulting training consists many image-speciﬁc lr-hr example pairs. network stochastically train pairs. figure image-speciﬁc zero-shot externally-supervised cnns pre-trained large external databases images. resulting deep network applied test image proposed method small image-speciﬁc trained examples extracted internally test image itself. learns recover test image coarser resolutions. resulting self-supervised network applied image produce output. sake robustness well allow large scale factors even small images performed gradually algorithm applied several intermediate scale-factors intemediate scale generated image downscaled/rotated versions gradually growing training-set fathers. downscale next gradual scale factor generate lrhr training example pairs. repeated reaching full desired resolution increase architecture optimization supervised cnns train large diverse external collection lr-hr image examples must capture learned weights large diversity possible lr-hr relations. such networks tend extremely deep complex. contrast diversity lr-hr relations within single image signiﬁcantly smaller hence encoded much smaller simpler image-speciﬁc network. simple fully convolutional network hidden layers channels. relu activations layer. network input interpolated output size. done previous cnn-based methods learn residual interpolated parent. loss adam optimizer start learning rate periodically take linear reconstruction error standard deviation greater factor slope linear divide learning rate stop learning rate accelerate training stage make runtime independent size test image iteration take random crop ﬁxed size randomly-selected father-son example pair. crop typically probability sampling lr-hr example pair training iteration non-uniform proportional size hrfather. closer size-ratio higher probability sampled. reﬂects higher reliability non-synthesized examples synthesize ones. lastly method similar geometric self-ensemble proposed take median outputs rather mean. combine back-projection technique output images undergoes several iterations back-projection ﬁnally median image corrected back-projection well. scale-factor runtime independent image size relative scale-factor nevertheless better results obtained using gradual increase resolution. example gradual increase using intermediate scale-factors typically improves psnr ∼.db increases runtime ∼min image. therefore tradeoff runtime output quality user choose. results reported paper produced using intermediate scale-factors. comparison test-time leading edsr+ image. however edsr’s time grows quadratically image size reaches image. beyond size network faster edsr+. acquisition parameters images ones ﬁxed images current supervised methods achieve incredible performance practice however acquisition process tends change image image since cameras/sensors differ well individual imaging conditions results different downscaling kernels different noise characteristics various compression artifacts etc. could practically train possible image acquisition conﬁgurations/settings. moreover single supervised unlikely perform well possible types degradations/settings. obtain good performance would need many different specialized networks trained different types degradations/settings. advantage image-speciﬁc network comes network adapted speciﬁc degradations/settings test image hand test time. network receive user test time following parameters desired downscaling kernel desired scale-factor desired number gradual scale increases whether enforce backprojection image whether ‘noise’ sons lr-hr example pair extracted test image found adding small amount gaussian noise improves performance wide variety degradations attribute phenomenon fact image-speciﬁc information tends repeat across scales whereas noise artifacts adding synthetic noise sons teaches network ignore uncorrelated crossscale information learning increase resolution correlated information indeed experiments show low-quality images wide variety degradation types image-speciﬁc obtains signiﬁcantly better results sota edsr+ similarly case non-ideal downscaling kernels image-speciﬁc obtains signiﬁcant improvement sota downscaling kernel known provided network. downscaling kernel unknown rough estimate kernel computed directly test image rough kernel estimations sufﬁce obtain improvement edsr+ nonideal kernels note providing estimated downscaling kernel externally-supervised sota methods test time would use. would need exhaustively re-train network collection lr-hr pairs generated speciﬁc downscaling kernel. method primarily aimed real images obtained realistic acquisition setting. usually ground truth hence evaluated visually order quantitatively evaluate zssr’s performance several controlled experiments variety settings. interestingly zssr produces competitive results ‘ideal’ benchmark datasets sota supervised methods train specialize however ‘non-ideal’ datasets zssr surpasses sota large margin. reported numerical results produced using evaluation script ‘ideal’ case figure images strong internal repetitive structures zssr tends surpass vdsr sometimes also edsr+ even though image generated using ‘ideal’ supervised setting mand table shows image-speciﬁc zssr achieves competitive results externally-supervised methods exhaustively trained conditions. fact zssr signiﬁcantly better older srcnn cases achieves comparable better results vdsr within unsupervised-sr regime zssr outperforms leading method selfexsr large margin. moreover images strong internal repetitive structures zssr tends surpass vdsr sometimes also edsr+ even though images generated using ‘ideal’ supervised setting. example shown fig. although image typical natural image analysis shows preference internal learning exhibited fig. exists ‘fractal-like’ images also found general natural images. several examples shown fig. seen pixels image beneﬁt exploiting internally learned data recurrence deeply learned external information whereas pixels beneﬁt externally learned data expected internal approach mostly advantageous image area high recurrence information especially areas patterns extremely small like small windows building. tiny patters larger examples elsewhere inside image indicates potential improvement combining power internallearning external-learning single computational framework. remains part future work. real images tend ideally generated. experimented non-ideal cases result either non-ideal downscaling kernels low-quality images non-ideal cases image-speciﬁc zssr provides signiﬁcantly better results sota methods quantities experiments described next. fig. shows visual results. additional visual results full images found project website. table presence unknown image degradation. image dataset randomly degraded using types degradations gaussian noise speckle noise jpeg compression. applied images without knowing type degradation. zssr shows robustness unknown degradations whereas sota methods not. fact conditions bicubic interpolation outperforms current sota methods. created dataset downscaling images using random gaussian kernels. image covariance matrix downscaling kernel chosen random angle random lengths axis hr-lr downscaling factor. thus image subsampled different random kernel. table compares performance leading externally-supervised methods also compared performance unsupervised blind-sr method considered cases applying zssr realistic scenario unknown downscaling kernel. mode used evaluate kernel directly test image zssr. unknown kernel estimated seeking nonparametric downscaling kernel maximizes similarity patches across scales test image. applied zssr true downscaling kernel used create image. scenario potentially useful images obtained sensors known specs. note none externally-supervised methods able beneﬁt knowing blur kernel test image since trained optimized exhaustively speciﬁc kernel. table shows zssr outperforms sota methods large margin unknown kernels provided true kernels. visually images figure internal external preference. green pixels favor internal-sr pixels favour external-sr notice edges prefer external-sr whereas unique image structures prefer internal-sr. tiny patters larger examples themselves elsewhere inside image generated sota methods blurry interestingly unsupervised blind-sr method deep learning also outperforms sota methods. supports analysis observations accurate donwscaling model important sophisticated image priors using wrong donwscaling kernel leads oversmoothed results. poor-quality images experiment tested images different types quality degradation. test robustness zssr coping unknown damage chose image random type degradation degradations gaussian noise speckle noise jpeg compression table shows zssr robust unknown degradation types typically damage supervised methods point bicubic interpolation outperforms current sota methods martin fowlkes malik. database human segmented natural images application evaluating segmentation algorithms measuring ecological statistics. proc. int’l conf. computer vision volume pages july timofte agustsson m.-h. gool yang zhang ntire challenge single image superieee conference resolution methods results. computer vision pattern recognition workshops july introduce concept zero-shot exploits power deep learning without relying external examples prior training. obtained small image-speciﬁc trained test time internal examples extracted solely test image. yields real-world images whose acquisition process non-ideal unknown changes image image real-world ‘nonideal’ settings method substantially outperforms sota methods qualitatively quantitatively. best knowledge ﬁrst unsupervised cnn-based method.", "year": 2017}