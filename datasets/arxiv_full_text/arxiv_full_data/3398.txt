{"title": "Deep Variational Inference Without Pixel-Wise Reconstruction", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Variational autoencoders (VAEs), that are built upon deep neural networks have emerged as popular generative models in computer vision. Most of the work towards improving variational autoencoders has focused mainly on making the approximations to the posterior flexible and accurate, leading to tremendous progress. However, there have been limited efforts to replace pixel-wise reconstruction, which have known shortcomings. In this work, we use real-valued non-volume preserving transformations (real NVP) to exactly compute the conditional likelihood of the data given the latent distribution. We show that a simple VAE with this form of reconstruction is competitive with complicated VAE structures, on image modeling tasks. As part of our model, we develop powerful conditional coupling layers that enable real NVP to learn with fewer intermediate layers.", "text": "variational autoencoders built upon deep neural networks emerged popular generative models computer vision. work towards improving variational autoencoders focused mainly making approximations posterior ﬂexible accurate leading tremendous progress. however limited efforts replace pixel-wise reconstruction known shortcomings. work real-valued non-volume preserving transformations exactly compute conditional likelihood data given latent distribution. show simple form reconstruction competitive complicated structures image modeling tasks. part model develop powerful conditional coupling layers enable real learn fewer intermediate layers. recent years variational autoencoders become extremely popular many machine learning problems. used variety applications image modeling interpretable representation learning conditional image generation structure learning images vaes provide mathematically sound framework unsupervised learning optimizing variational lower bound data likelihood. lower bound involves terms divergence approximate posterior ﬁxed prior conditional likelihood data given latent distribution much work improving vaes focused modifying approximate posterior better expressivity approximations hand little work done improve upon form reconstruction. models assume standard normal distribution pixels reconstructed image space leads mean-squared reconstruction cost. previously shown cause blurriness reconstructed images. previous work attempted circumvent problem augmenting model generative adversarial networks however models allow compute conditional likelihood term exactly limits ability objectively compare models. models used alternatives like discrete softmax distribution discretized logistic distribution pixels well studied own. real-valued non-volume preserving transformations offer exact likelihood computation non-linear invertible transformations whose jacobian determinants easy compute. model also provides exact inversion latent space data space enabling efﬁcient sampling available exact likelihood methods pixel recurrent neural networks pixel convolutional neural networks real transformations exactly compute conditional likelihood term thus alleviate problem mean-squared reconstruction. show using modiﬁcation compete complicated models convolutional draw well real summary contributions follows propose conditional coupling layer make conditioning latent distribution stronger adding multiplicative interactions enable expressivity model fewer layers. here arbitrary functions realized using neural networks. transform divides input vector parts ﬁrst part kept unchanged second part transformed using function unchanged part. shows compute determinant jacobian taken series coupling layers multplying determinants jacobians individual coupling layers. enables compose using arbitrary number coupling layers. mentioned properties coupling layer transform compute exact likelihood data. thus network optimized using maximum likelihood framework. variational autoencoders differ regular autoencoders stochastic layers latent variables. latent variables form approximate posterior forced close chosen prior standard normal distrbution. achieved minimizing divergence approximate posterior prior terms variational lower bound log-likelihood data here approximate posterior modeled encoder conditional likelihood modeled decoder ﬁxed prior distribution. unsupervised learning setup maximizes variational lower bound surrogate loglikelihood. expectation term estimated using monte carlo sampling batch eq)] log) training example batch. work improve upon technique calculate instead assuming reconstructed image space follows standard normal distribution assume intermediate layer follows parametrized normal distribution. provide details next section. real exact likelihood model transforms data prior probability distribution. data space transformed space function change variable formula transformation given following equation here point data space. equation likelihood data estimated compute terms right. likelihood space computed analytically assume prior standard normal distribution space. compute second term need able calculate determinant jacobian transformation enabled coupling layer transform. also invertible easily latent space data space figure block diagram vapnev color coded indicate independent components. dotted line suggests latent distribution used conditioning coupling layer transforms. ﬁgure illustrative relative network sizes. regular reconstructed image space asinstead sumed follow normal distribution. assume intermediate space follow normal distribution. order calculate transform data space intermediate space depends latent space change variable formula transformation dimensionality space. determinant jacobian taken series coupling layers seen previous section. thus using formulation real completely avoid pixel-wise computation still exactly calculate conditional likelihood. noted formulation holds even depend seen figure vapnev models prior space using decoder output. conditional coupling layer order make conditional latent distribution propose conditional coupling layer. layer satisﬁes following conditions necessary useful scenario determinant jacobian easy compute invertible given ﬁrst condition ensures efﬁcient computation cost model whereas since outputs convolutional networks tensors rhxw multipliers bias trainable parameters. elementwise multiplication multipliers uses broadcasting. conditional coupling layer allows shortcut connections latent distribution allows stronger conditioning faster training. include encoder well respective projections mean variance approximate posterior. sampled approximate posterior using reparametrization trick include decoder well respective projections mean variance latent space. network consisting conditional coupling layers. using divergence term computed since assume used calculate log|z)) adjustment provided gives log). face problem divergence term quickly goes zero leading undesirable local optima. annealing procedure suggested alleviate issue. optimize model using adam optimizer default hyperparameters. here input layer output layer d-dimensional. arbitrary functions dependent regular coupling layers transformation gives diagonal jacobian whose determinant easy compute. inverse also computed using similar equations provided known value graphical representation conditional coupling layer shown figure discuss exact form also used transformation equations apparent project rd−d. achieve this project rd−d using functions respectively operate elementwise fashion computed function values. note take residual network maintains dimensionality since generally dimensional vector take deconvolution network. inspired multiplicative interactions increase expressivity function. summarize using following equation test model task generative image modeling. model compared using natural images well ﬁxed domain images first specify common details used experiments. discrete image data ﬁrst corrupted uniform noise make continuous scaled since real gives transformation model density takes here done avoid numerical errors within log. experiments take compute actual variational lower bound account transformation. correction components also horizontal ﬂips dataset images data augmentation. encoder taken -layer convolutional neural network. every alternate layer doubles number ﬁlters halves spatial resolution directions. start ﬁlters ﬁrst layer. analogous encoder decoder -layer deconvolution network doubles spatial resolution halves number ﬁlters every alternate layer. ﬁrst layer deconvolution network starts dimensions output last encoder layer. mean variance latent space computed using separate fully-connected linear projections encoder output; dimensionality latent space taken case latent space mean variance separate convolutional linear projections. conditional coupling layer transform checkerboard masking channel-wise masking squeeze operation mentioned compute take network residual blocks small deconvolution network. deconvolution network starts tensor doubles spatial resolution layer. take number ﬁlters residual blocks coupling layer. conﬁguration used multi-scale architecture mentioned scales. scale conditional coupling layers checkerboard masking channel-wise masking. start ﬁlters residual blocks ﬁrst scale double next scale. architecture datasets. cifar- table vapnev competitive convolutional draw complicated structure multiple stochastic layers recurrent feedback. establishes replacing pixel-wise reconstruction exact likelihood methods like real beneﬁcial performance vaes. model also competitive real uses much bigger architecture shows power conditional coupling layer transform able effectively utilize semantic representation learned latent distribution. seen reconstructions figure vapnev learns model high level semantics latent distribution background color pose location object. samples also show model able learn better global structure. celeba figure left panel shows original images cifar- dataset. middle panel shows reconstructions images left panel. right panel contains images sampled randomly model. figure left panel shows original images celeba dataset. middle panel shows reconstructions images left panel. right panel contains images sampled randomly model. tecture suggests improved using better global representations learned vae. looking reconstructions figure model learns high level semantic features hair color face pose expressions. paper suggest replace pixel-wise reconstruction maximum likelihood based alternative. show greatly beneﬁts formulation simple augmented transformations able compete complicated models multiple stochastic layers recurrent connections. develop powerful conditional coupling layer transforms enable model learn smaller architectures. vapnev provides advantages provides replace pixelwise reconstruction known shortcomings gives generative model trained sampled efﬁciently latent variable model used downstream supervised semi-supervised learning. work extended several ways. using deeper architectures combining expressive posterior computations like inverse autoregressive possible compete even beat state-of-theart models. technique used improve models tasks semi-supervised learning conditional density modeling. conditional coupling layer used constructing conditional real models.", "year": 2016}