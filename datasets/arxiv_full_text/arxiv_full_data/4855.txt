{"title": "Online Learning of Event Definitions", "tag": ["cs.LG", "cs.AI"], "abstract": "Systems for symbolic event recognition infer occurrences of events in time using a set of event definitions in the form of first-order rules. The Event Calculus is a temporal logic that has been used as a basis in event recognition applications, providing among others, direct connections to machine learning, via Inductive Logic Programming (ILP). We present an ILP system for online learning of Event Calculus theories. To allow for a single-pass learning strategy, we use the Hoeffding bound for evaluating clauses on a subset of the input stream. We employ a decoupling scheme of the Event Calculus axioms during the learning process, that allows to learn each clause in isolation. Moreover, we use abductive-inductive logic programming techniques to handle unobserved target predicates. We evaluate our approach on an activity recognition application and compare it to a number of batch learning techniques. We obtain results of comparable predicative accuracy with significant speed-ups in training time. We also outperform hand-crafted rules and match the performance of a sound incremental learner that can only operate on noise-free datasets. This paper is under consideration for acceptance in TPLP.", "text": "systems symbolic event recognition infer occurrences events time using event deﬁnitions form ﬁrst-order rules. event calculus temporal logic used basis event recognition applications providing among others direct connections machine learning inductive logic programming present system online learning event calculus theories. allow single-pass learning strategy hoeffding bound evaluating clauses subset input stream. employ decoupling scheme event calculus axioms learning process allows learn clause isolation. moreover abductive-inductive logic programming techniques handle unobserved target predicates. evaluate approach activity recognition application compare number batch learning techniques. obtain results comparable predicative accuracy signiﬁcant speed-ups training time. also outperform hand-crafted rules match performance sound incremental learner operate noise-free datasets. paper consideration acceptance tplp. event recognition systems process sequences simple events sensor data recognize complex events interest i.e. events satisfy pattern. logic-based event recognition typically uses knowledge base ﬁrst-order rules represent complex event patterns reasoning engine detect patterns incoming data. dialects event calculus used language specifying deﬁnitions complex events advantage approach offers direct connections machine learning inductive logic programming alleviating task manual authoring event deﬁnitions. event recognition applications deal noisy data streams. methods extract insights streams need operate within tight memory time constraints building decision model single pass training data framework under-explored data typically place learning begins. alternatively systems capable theory revision still systems need multiple scans data optimize theories. tool allows build decision models using small subset data relating size subset user-deﬁned conﬁdence level error margin making optimal decision oled learns clause top-down fashion gradually adding literals body. instead evaluating candidate specialization entire input accumulates training data stream hoeffding bound allows select best specialization. instances used make decision stored reprocessed discarded soon oled extracts necessary statistics clause evaluation. learning problem address work target clauses unrelated depend axioms making difﬁcult common practices learn clauses isolation. handle issue decoupling scheme axioms learning thereby allowing assess quality clause separately using scoring function. additionally learning programs involves non-observational predicate learning setting instances target predicates directly observable data. handle non-opl abduction framework used reasoning incomplete information. evaluate approach activity recognition application compare number batch learning techniques. obtain results comparable predicative accuracy signiﬁcant speed-ups training time. also outperform hand-crafted rules match performance sound incremental learner operate noise-free datasets. rest paper structured follows section discuss related work section present necessary background hoeffding bound. section present oled section show results empirical analysis. finally section discuss directions future research conclude. hoeffding bound used propositional machine learning tasks data streams learning decision trees decision rules clustering however usage learning relational models limited. reason requires independence observations cannot always ensured relational domains dependencies data approach uses hoeffding bound relational learning htilde extension tilde system learning ﬁrst-order decision trees decision trees internal node consists conjunction literals leaf propositional predicate representing class. tilde constructs trees testing conjunctions literals node using reﬁnement operator generate conjunctions information gain guiding heuristic. htilde extends tilde using hoeffding bound perform internal tests subset training data. ensure independence observations htilde learns interpretations setting used also oled training instance assumed disconnected part dataset. like tilde htilde learns clauses propositional predicate head however head complex event deﬁnition typically ﬁrst-order predicate containing variables appear body clause express relations entities. therefore htilde general enough problem address work. additionally learning programs challenging task learners cannot fully undertake mainly non-monotonicity negation failure uses. xhail tal/aspal/raspal systems handle task combining non-monotonic semantics abductive logic programming. approaches ensure soundness outcome presence requires learning whole theories jointly optimizing clauses. implies intractable search space even relatively small amounts data. result aforementioned approaches scale event recognition applications temporal data streams. contrast oled learns clauses separately using fragments data online setting trading soundness efﬁciency. iled recently proposed scalable extension xhail system able learn theories. incremental learner revises past hypotheses observations full-memory system meaning revisions account growing historical memory accumulated data. using compressive memory structure encode positive examples clause entails historical memory iled requires pass past data revise hypothesis. difference oled latter learns online fashion thus re-process past examples. also iled designed learn sound theories assumption scalable strategy training data noisefree. incremental systems inthelex forte cannot applied task address work since cannot handle negation non-observable target predicates assume logic programming setting predicates terms atoms literals clauses programs deﬁned denotes naf. following prolog’s convention predicates ground terms logical formulae start lower case letter variable terms start capital letter. event calculus temporal logic reasoning events effects. ontology comprises time points represented integers; ﬂuents i.e. properties certain values time; events i.e. occurrences time affect ﬂuents alter value. axioms incorporate common sense inertia according ﬂuents persist time unless affected event. simpliﬁed version shown sufﬁce event recognition basic predicates domain-independent axioms presented table axiom table example data activity recognition. instance time point person walking coordinates direction annotation time point states persons moving together contrast annotation time point example domain-speciﬁc axioms ﬁrst clause dictates moving persons initiated time walking time euclidean distance less difference direction less second clause dictates moving terminated time standing still time euclidean distance greater deﬁnitions initiatedat/ terminatedat/ predicates provided domain-speciﬁc axioms. illustrate learning approach task activity recognition deﬁned caviar project. caviar dataset consists videos public space actors perform activities. videos manually annotated caviar team provide ground truth types activity. ﬁrst type corresponding simple events consists knowledge person’s activities certain video frame/time point second type corresponding complex events consists activities involve person instance people moving together meeting other ﬁghting recognize complex events means combinations simple events additional domain knowledge person’s position direction. table presents example caviar data consisting narrative simple events terms happensat/ expressing short-term activities people context properties terms holdsat/ denoting coordinates direction people. table also shows annotation complex events time-point narrative. annotation complex events obtained closed world assumption example domainspeciﬁc axioms presented table goal learn domain-speciﬁc axioms specifying complex events. inductive logic programming provides techniques learning logical theories examples. learning interpretations setting work training example interpretation i.e. true ground atoms table given training interpretations background theory case consists domain-independent axioms goal theory allow online learning setting hoeffding bound statistical tool used probabilistic estimator generalization error model given empirical error given random variable range observed mean values independent observations hoeffding bound states that probability true mean variable lies interval words true average approximated observed probability given error margin decreases number observations learners typically employ separate-and-conquer strategy clauses cover subsets examples constructed recursively examples covered. clause constructed top-down fashion starting overly general clause gradually specializing adding literals body. process guided heuristic function assesses quality specialization entire training set. step literal optimal g-score selected process continues certain criteria met. adapt strategy online setting hoeffding bound evaluate candidate specializations subset training interpretations instead evaluating entire input. argument adapted clause clause evaluation function range evaluation function work discussed shortly. assume also training instances specialization highest observed mean g-score second best i.e. hoeffding bound true mean scores’ difference holds probability hence implying indeed best specialization select point probability order decide specialization select thus sufﬁces accumulate observations input stream since decreases number observations given desired number observations needed reach decision traded tolerable generalization error selecting optimal specialization certain choice point. observations need stored reprocessed. process observation extract necessary statistics computation g-score candidate specialization. gives rise single-pass clause construction strategy. interpretation independent form others guarantees independence observations necessary using hoeffding bound. setting interpretation consists ground atoms known true consecutive time points table dialect initiation/termination complex events depends simple events contextual information previous time-point therefore interpretation independent training instance. relax requirement hypothesis covers every training interpretation account noise thus seek theory good training data. deﬁne true positive false positive false negative atoms follows deﬁnition consist domain-independent axioms clause interpretation. denote narrative annotation narrative annotation part respectively answer narrative∪ given annotation denote atom that seek theory maximizes atoms minimizing atoms collectively clauses. maintain count clause atom. initiatedat clause count increased time correctly initiates complex event terminatedat clause count increased time correctly allows complex event persist terminating count increased incorrectly terminates complex event. learning structure horn logic theory augmented clauses increase total count existing clauses specialized decrease count. strategy directly applicable problem hand. learning programs addition clauses necessary eliminate clause specialization necessary increase explain below. given theory interpretation assume cover following holds terminatedat clause ﬁres failing terminate complex event corresponds should erroneously persists inertia. generating terminatedat clause eliminates initiatedat terminatedat clauses affect total count theory therefore counts clause taken account evaluation types clauses. additionally specializing existing clauses improves quality eliminating initiatedat case above) favor terminatedat case therefore also taken account evaluating initiatedat clauses. hand total count theory affected existing terminatedat clauses instead requires clauses generated respectively). therefore irrelevant evaluation existing terminatedat initiatedat clauses respectively. combining observations derive scoring function deﬁnition uses precision recall initiatedat terminatedat clauses respectively. section discuss main functionality oled presented algorithm detail. learning begins empty hypothesis arrival interpretations oled either expands generating clause tries expand existing clause. clauses quality pruned evaluated sufﬁcient number examples. incoming interpretation processed once extract necessary statistics clause evaluation form counts subsequently discarded. distinguish different cases presented section initiation termination clauses learnt separately parallel processes linit lterm respectively input stream forwarded processes simultaneously. thanks decoupling either process fails account training interpretation able infer causes failure terms atoms. particular linit detects fp/fn-failures based cases respectively lterm detects fp/fn-failures based cases according cause failure process dispatches control either theory expansion clause expansion subroutines. choice among actions made boolean function expandtheory line algorithm action algorithm onlinelearning input stream training interpretations; background knowledge; clause evaluation function; conﬁdence hoeffding test; specialization depth; smin clause g-score quality threshold. initially processes linit lterm start empty hypotheses hinit hterm. assume annotation incoming interpretations dictates moving complex event holds time hold time since clause hinit exists initiate moving time linit detects moving instance time proceeds theory expansion generating initiation clause moving. lterm concerned initiation conditions take actions case. then interpretation arrives annotation dictates moving holds time hold time case since clause exists hterm terminate moving time lterm detect instance time proceed theory expansion generating termination condition moving. time assume initiation clause hinit over-general erroneously re-initiates moving time generating instance linit process time response that linit proceed clause expansion penalizing over-general initiation clause increasing count thus contributing towards potential replacement specializations. theory expansion. theory expansion process handled startnewclause function algorithm clause generated data-driven fashion constructing bottom clause training interpretation theory expansion consists addition clause head theory point gradually specialized addition literals body. denote bottom clause associated clause typical setting bottom clause constructed selecting target predicate instance seed placing head newly generated clause empty body. atoms follow deductively background knowledge placed body constants replaced variables appropriate indicated particular language bias typically mode declarations clause good data reﬁnement operator used generate candidate clauses θ-subsume aforementioned approach cannot applied directly problem address here falls non-observational predicate learning class problems non-opl instances target predicates normally used seeds construction directly observable training data. case target predicates initiatedat/ terminatedat/ annotation training interpretations consists complex event instances terms holdsat/ predicate workaround abduction obtain missing target predicate instances construct bottom clauses them. approach followed xhail system also adopt here. like xhail oled also uses mode declarations specifying language bias. oled output hypothesis constructed time learning process. allow warm-up period form minimum number training instances nmin clause must evaluated included output hypothesis. clause expansion. hoeffding bound select among competing specializations clause specializations generated adding literals body input parameter specialization depth serves upper bound number literals added time. denote specializations clause formally {head body∧ body e.g. consists one-step specializations consists plus two-step specializations clause expanded i.e. replaced best-scoring specialization sufﬁcient number interpretations seen observed difference mean g-scores best second best specializations. ensure clause replaced specialization lower quality also considered potential candidate along specializations ensures expanding clause best-scoring specialization better probability expanding all. online learners typically subject order effects i.e. sensitive order examples presented. using hoeffding bound allows oled mitigate effects since clause expansion postponed sufﬁcient evidence quality candidate specializations provided data. tie-breaking. scores specializations similar large number training instances required decide them. could wasteful since specializations chosen. cases break ties follows instead waiting required hoeffding bound-based heuristic expand best-scoring specialization tie-breaking threshold follow adaptive tie-breaking threshold mean value observed training process case best-scoring specialization follow conservative approach expand clause pruning. oled supports removal clauses whose score smaller quality threshold smin. decide clause removed also hoeffding bound. smin−g probability true mean g-score lower quality threshold smin therefore removed. evaluate oled’s performance caviar benchmark dataset activity recognition. caviar contains total training interpretations mean size atoms each. size search space determined size bottom clauses experiments consisted average literals each. experiments conducted linux machine .ghz processor ram. code data available online. purpose experiment assess whether oled able efﬁciently learn theories comparable quality hand-crafted rules state-of-the-art batch learning approaches. compare oled following eccrisp hand-crafted clauses caviar dataset described ecmm probabilistic version eccrisp weights learnt max-margin weight learning method markov logic networks xhail hybrid abductiveinductive learner capable learning programs ecmm selected shown achieve good results caviar xhail selected systems able learn theories oled xhail implemented using clingo answer solver core reasoning component ecmm approach used experiment implemented lomrf framework mlns. evaluate ecmm used fragment caviar dataset also experiment. target complex events dataset related persons meeting moving together training data consists parts caviar involve complex events. fragment dataset contains total training interpretations. interpretations moving occurs meeting occurs. oled’s results achieved using signiﬁcance clause pruning threshold smin meeting moving specialization depth parameter meeting moving. results reported parameter conﬁguration best among several parameter settings tried smin training time oled maximum training time parallel processes linit lterm. results obtained using -fold cross validation presented table form precision recall f-score. statistics micro-averaged instances recognized complex events fold -fold cross validation process. table also presents average training time fold approaches except eccrisp average theory sizes oled xhail well ﬁxed theory size eccrisp ecmm. ecmm achieves best f-score complex events followed closely xhail. oled achieves comparable predictive accuracy outscores handcrafted rules. moreover oled achieves speed-ups several orders magnitude compared ecmm xhail single-pass strategy. superior accuracy ecmm xhail batch learners optimizing respective outcomes entire training set. also explains increased training times both. regarding theory size xhail learns signiﬁcantly compressed hypotheses oled. reason xhail learns whole theories oled learns clause separately gain efﬁciency. also present experimental results running oled entire caviar dataset. target complex events meeting moving previously. number positive interpretations complex events also before since data fragment used previous experiment contains parts caviar complex events occur. contrast number negative training instances much larger experiment. imental setting follows used -fold cross validation fragment used previous experiment fold training test sets augmented number negative training sequences. particular fold negative training sequences remaining part caviar added training fold remaining added test set. parameter conﬁguration oled previous experiment exception specialization depth meeting limited size training sets experiment section prevented oled sufﬁciently expanding clauses resulting over-general theories. setting thus trying -step specializations well made possible obtain results reported table contrast necessary experiment signiﬁcantly larger training size oled able good clauses trying -step specializations only. table shows results. approaches’ performance decreased compared previous experiment increased number false positives caused large number additional negative instances. oled still outscores hand-crafted knowledge base. compared oled iled incremental learner able learn theories recall iled cannot learn noisy data therefore cannot used caviar exhibits various types noise details. order compare systems thus generated noise-free version caviar artiﬁcial annotation moving meeting complex events. produce annotation used hand-crafted knowledge base eccrisp inference caviar narrative. used -fold cross validation assess performance compared systems. fold training consisted positive negative interpretations complex event. oled’s parameter setting reported section results presented table predictive accuracy systems comparable iled’s slightly better. expected since iled re-scans historical memory past data revise theories. training times also comparable oled’s slightly higher compared iled’s. iled able avoid certain computations inferring redundant based assumption data noise-free. regarding theory size oled learns signiﬁcantly shorter hypotheses iled. oled prunes number learnt clauses effort avoid ﬁtting potential noise data also follows conservative clause expansion strategy. contrast iled tries account every positive example since designed learning sound hypotheses. experiment assess oled’s scalability. learning entire caviar dataset average processing time training interpretation milliseconds frame rate caviar i.e. rate video frames containing data arrive stress-test evaluated oled’s performance demanding learning tasks. generated different datasets consisted number copies caviar. datasets differ original constants referring tracked entities simple complex events. generated datasets consisting copies contained different entities respectively. like previous experiments interpretation includes narrative annotation atoms time points. experiment however number atoms interpretation grows proportionally number copies dataset. performed learning oled original enlarged datasets measured average processing time training interpretation. figure presents results. instance interpretations copies caviar handled approximately standard desktop computer. growth average processing time increased number annotation atoms datasets well additional domain constants result exponential increase size ground program produced clause evaluation process oled’s performance improved optimizations taking advantage domain knowledge relational dependencies data. instance caviar complex events involve different entities therefore learning split across different processing cores learn independent parts data. optimizations part current work. presented oled system online learning complex event deﬁnitions event calculus. oled any-time system learns single-pass stream using hoeffding bound evaluate candidate clauses subset input. results empirical evaluation indicate oled achieves speed-ups several orders magnitude compared batch learners comparable predictive accuracy. also outscores hand-crafted rules matches performance sound incremental learner operate noise-free datasets. intend improve oled several aspects including scalability development adaptive techniques automated conﬁguration parameters. also plan experiment dialects allow long-term temporal relations entities combine oled weight learning techniques towards online statistical relational learning.", "year": 2016}