{"title": "Attention-based Graph Neural Network for Semi-supervised Learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches. These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer. Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models. This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small. This in turn allows a room for designing more innovative propagation layers. Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph. The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions. In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods. By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.", "text": "recently popularized graph neural networks achieve state-of-the-art accuracy number standard benchmark datasets graph-based semi-supervised learning improving signiﬁcantly existing approaches. architectures alternate propagation layer aggregates hidden states local neighborhood fully-connected layer. perhaps surprisingly show linear model removes intermediate fullyconnected layers still able achieve performance comparable state-of-the-art models. signiﬁcantly reduces number parameters critical semi-supervised learning number labeled examples small. turn allows room designing innovative propagation layers. based insight propose novel graph neural network removes intermediate fully-connected layers replaces propagation layers attention mechanisms respect structure graph. attention mechanism allows learn dynamic adaptive local summary neighborhood achieve accurate predictions. number experiments benchmark citation networks datasets demonstrate approach outperforms competing methods. examining attention weights among neighbors show model provides interesting insights neighbors inﬂuence other. major bottlenecks applying machine learning practice collecting sizable reliable labeled data essential accurate predictions. overcome problem limited labeled data semi-supervised learning using additional unlabeled data might freely available. paper interested scenario additional unlabeled data available form graph. graph provides underlying pairwise relations among data points labeled unlabeled. particular interest applications presence absence edge data points determined nature instance result human activities natural relations. concrete example consider citation network. node graph published research paper associated bag-of-words feature vector. edge indicates citation link. presence edge indicates authors paper consciously determined refer paper hence captures underlying relation might inferred bag-of-words feature vectors alone. external graph data available several applications interest classifying users connected social network items customers connected purchase history users movies connected viewing history entities knowledge graph connected relationships. paper interested setting graph explicitly given represents additional information present feature vectors. goal graph-based semi-supervised learning problems classify nodes graph using small subset labeled nodes node features. long line literature topic since seeks graph cuts preserve known labels uses graph laplacian regularize nearby nodes similar labels. however recently demonstrated existing approaches signiﬁcantly improved upon ∗department electrical computer engineering university illinois urbana-champaign thekumpillinois.edu. work done †google inc. kirkland chongwgoogle.com ‡department industrial enterprise systems engineering university illinois urbana-champaign swohillinois.edu §google inc. sunnyvale lijialics.stanford.edu inspired success seek understand reason behind power graph neural networks guide design novel architecture semi-supervised learning graphs. ﬁrst found linear classiﬁer multinomial logistic regression achieves accuracy comparable best known graph neural network. linear classiﬁer removes intermediate non-linear activation layers keeps linear propagation function neighbors graph neural networks. suggests importance aggregation information form neighbors graph. motivates design aggregating neighborhood information attention mechanism since intuitively neighbors might equally important. proposed attention-based graph neural network captures intuition greatly reduces model complexity single scalar parameter intermediate layer; discovers dynamically adaptively nodes relevant target node classiﬁcation; improves upon state-of-the-art methods terms accuracy standard benchmark datasets. further learned attention strengths provide form interpretability. provide insights particular prediction made target node neighbors relevant making decision. related work given graph nodes edges denote feature vector node denote true label. denote labels revealed subset denote features labeled unlabeled. traditionally semi-supervised learning using labeled un-labled data solved using different approaches graph laplacian based algorithms solving locally consistent solutions expectation maximization based algorithms true-labels unlabeled data points considered latent variables generative model. graph laplacian regularization. based assumption nearby nodes graph likely labels graph information used explicit regularization standard supervised loss loss functions graph-based called graph laplacian regularization. earlier approaches non-parametric searches considering look-up table. popular label propagation forces estimated labels agree labeled instances uses weighted graph laplacian. innovative formulation admits closed form solution makes practically attractive computationally cost. manireg replaces supervised loss support vector machine. generalizes allowing general local updates. thorough survey using non-neural network methods semi-supervised learning found recent approaches parametric using deep neural networks. semiemb ﬁrst deep neural network model minimize loss. planetoid signiﬁcantly improves upon existing graph regularization approaches replacing regularization another loss based skip-grams slightly different context buchnik cohen show accuracy approaches improved bootstrapping models sequentially. unsupervised node embedding semi-supervised learning. several approaches proposed embed nodes latent euclidean space using connectivity graph embedding learned standard supervised learning applied embedded features train model. inspired success wordvec several approaches deﬁne skip-grams graphs neighborhood node graph tries maximize posterior probability observing skip-grams. deepwalk nodevec random walks skip-grams line uses local proximities lasagne uses personalized pagerank random walk. graphgauss represents node gaussian distribution minimizes divergence connected pairs. yang provide post-processing scheme takes node embedding attempts improve taking weighted given embeddings personalized pagerank weights. strength approaches universality node embedding depend particular task hand however node features training happens embedding cannot meet performance state-of-the-art approaches graph neural network graph neural networks extensions neural networks structured data encoded graph. originally introduced extensions recurrent neural networks gnns apply recurrent layers node additional local averaging layer however weights shared across nodes gnns also interpreted extensions convolutional neural networks grid general graphs. typically message aggregation step followed neural network architecture iteratively applied. model parameters trained supervised examples labels. give typical example section several diverse variations proposed gnns successfully applied diverse applications molecular activation prediction community detection matrix completion combinatorial optimization detecting similar binary codes particular benchmark datasets consider paper kipf welling proposed simple powerful architecture called graph convolutional network achieves state-of-the-art accuracy. following section show performance linear classiﬁer; insight introduce novel graph neural networks compare favourably state-of-the-art approaches benchmark datasets. section propose novel graph neural network model call attention-based graph neural network compare performance state-of-the-art models benchmark citation networks section seek model rn×dy predicts node classes. estimated probability label node given features graph data features rn×dx features node }n×n adjacency matrix forward pass typical alternates propagation layer single layer perceptron. layer index. rn×dh denote current states i-th dimensional hidden state node propagation layer respect propagation matrix rn×n deﬁned example natural random walk gives neighborhood node denoted diag. simple local averaging common consensus random walk based approaches. typical propagation layer respects adjacency pattern performing variation local averaging. gnns encode graph structure model propagation layer also interpreted performing graph convolution operation discussed next single layer perceptron applied node separately weights shared across nodes rdht+×dht weight matrix entry-wise activation function. weight sharing reduces signiﬁcantly number parameters trained encodes invariance property graph data i.e. nodes apart similar neighboring features structures classiﬁed similarly. several extensions model discussed previous section standard graph neural network proved powerful several problems graphs e.g. graph convolutional network kipf welling introduced simple powerful architecture achieved state-of-the-art performance benchmark citation networks special case stacks layers speciﬁc propagation perceptron applied row-wise. hence output predicted likelihoods dimensional probability simplex. weights trained minimize crossentropy loss labeled examples choice ˜d−/ ˜d−/ gcn. weights dimensions trained cross entropy loss propagation layers simply take local average features weighted degrees output layer simple linear classiﬁer applied. allows separate gain linear propagation layer non-linear perceptron layer. comparing differences performances table show that perhaps surprisingly achieves accuracy comparable best sometimes better. suggests that citation networks strength general architectures propagation layer perceptron layer. hand propagation layers critical achieving desired performance suggested table signiﬁcant gaps accuracy approaches using graph i.e. t-svm also graph differently label propagation planetoid. based observation propose replacing propagation layer attention mechanism test benchmark datasets. need novel dynamic adaptive propagation layers capable capturing relevance different edges leads complex graph neural networks parameters. however training complex models challenging semi-supervised setting typical number samples class small; standard benchmark dataset. evidenced table complex graph neural network models verma boyer verbeek monti improve upon simple gcn. hand experiments suggests remove perceptron layers focus improving propagation layers. introduce novel attention-based graph neural network agnn simple; single scalar parameter intermediate layer. agnn captures relevance; proposed attention mechanism neighbors learns neighbors relevant weighs contributions accordingly. builds long line successes attention mechanisms summarizing long sentences large images capturing word part-of-image relevant particularly attention formulation similar used parameter found important successfully training model number labels small semi-supervised learning setting. y/xy norm integer number propagation layers. note propagation dynamic; propagation changes layers differing also hidden states. also adaptive; learns weight relevant neighbors higher. self-loop propagation ensure features hidden states node lost propagation process. output layer weight rdh×dy weights trained cross entropy loss ease notations assumed input feature vectors ﬁrst last layers augmented scalar constant standard bias term included parameters captures relevant measured cosine angle corresponding hidden states. show interpret attentions section show attention selects neighbors class relevant. standard benchmark datasets citation networks show section architecture achieves best performance table note independently work attention sets proposed neighborhood attention different application. main difference agnn respect work fact agnn attention computed neighborhood node graph whereas work attention entities used construct soft neighborhood. standard benchmark datasets three citation networks test proposed agnn model semi-supervised learning tasks. test ﬁxed split labeled/validation/test sets compare baseline methods table also test random splits sizes table random splits larger number labeled nodes table benchmark datasets. citation network dataset consists documents nodes citation links directed edges. node human annotated topic ﬁnite classes feature vector. consider three datasets. citeseer cora datasets feature vector binary entries indicating presence/absence corresponding word dictionary. pubmed dataset feature vector real-values entries indicating term frequencyinverse document frequency corresponding word dictionary. although networks directed undirected versions graphs experiments common baseline approaches. experimental setup. accuracy baseline methods taken existing literature. baseline result reported existing literature intentionally left cells empty table fairness opposed running experiments untuned hyperparameters. train test models propose comparisons proposed agnn model. validation labels training optimizing hyper-parameters like dropout rate learning rate l-regularization factor. agnn ﬁxed number units hidden layers propagation layers citeseer pubmed propagation layers cora deﬁned propagation layers deﬁned row-normalize input feature vectors standard literature. tables show average accuracy standard error training instances random weight initializations. implement model tensorflow computational complexity evaluating agnn odh|e| dxdhn). detailed desription experiments provided appendix quantitative results fixed data splits. ﬁrst experiment ﬁxed data splits standard benchmark data splits literature. experiments ﬁxed split labeled nodes class nodes validation nodes test rest nodes unlabeled data. perhaps surprisingly linear classiﬁer proposed achieves performance comparable exceeding state-of-the-art performance gcn. leads novel attention-based model agnn deﬁned achieves best accuracy datasets larger standard error. classiﬁcation accuracy baseline methods collected method singlelayer perceptron multilayer perceptron t-svm deepwalk nodevec manireg semiemb dcnn planetoid monet graph-cnn dynamicfilter bootstrap agnn semi-supervised learning graphs critical utilize structure graph node features. methods using given data achieve performance state-of-the-art. supervised methods–single multi-layer perceptrons–only labeled examples semi-supervised methods e.g. t-svm labeled unlabeled examples skip-gram based approaches deepwalk ignores node features labels graph breakthrough result planetoid yang cohen salakhutdinov signiﬁcantly improved upon existing skip-gram based method deepwalk nodevec laplacian regularized methods manireg semiemb. kipf welling ﬁrst apply graph neural network citation datasets achieved state-of-the-art performance gcn. variations graph neural networks immediately followed achieving comparable performance monet graph-cnn dynamicfilter. bootstrap uses laplacian regularized approach sub-routine bootstrapping feed high-margin predictions seeds. random splits. next following setting buchnik cohen experiments keeping size labeled validation test sets table selecting nodes uniformly random. this along fact different topics different number nodes means labels might spread evenly across topics. randomly drawn dataset splits average accuracy shown table standard error. force equal number labeled data class observe performance degrades methods compared table except deepwalk. agnn achieves best performance consistently. here note kipf welling similar different experiment using random labeled nodes evenly spread across topics topic exactly labeled examples. difference sampling might affect accuracy report results table. larger training set. following setting experiments larger number labeled data cora dataset. perform k-fold cross validation experiments uniformly randomly dividing nodes equal sized partitions performing runs training masking labels partitions followed validation masked nodes. finally average validation accuracy across runs reported. trials experiment reports mean standard error average k-fold validation accuracy. compared table performance increases size training agnn consistently outperforms current state-of-the-art architecture experiment. performance versus number propagation layers. section provide experimental results justifying choice number propagation layers dataset. data split hyper-parameters ﬁxed data splits setting section similar settings ﬁrst propagation layer cora dataset non-trainable tables gives average testing accuracy respectively various choices number propagation layers. note different datasets require different number propagation layers best performance. interpretation capability learned represent attention node node provide insights relevant node classifying node figure provide statistics attention adjacent pairs nodes cora citeseer datasets. refer figure similar statistics pubmed. figure show average attention node topic node topic call relevance deﬁned degree node edge augmented self-loops include attentions learned. using attention typical propagation uniform case normalized attention zero. measuring edge variation attention uniform multiplicative error normalized believe right normalization attention measure relative strength others neighborhood absolute additive differences. measuring multiplicative variation attention averaged ordered pairs classes. figure shows relevance score citeseer cora datasets. datasets diagonal entries dominant indicating attention learning weight class. higher value relevance indicates that average node topic pays attention neighbor topic neighbors topics. citeseer dataset showing average attention ﬁrst propagation layer illustration. off-diagonals inﬂuential relations hci→agents agents→ml agents→hci ml→agents least inﬂuential relations ai→ir db→ml. note papers computer science late early cora dataset showing relevance score second propagation layer illustration. off-diagonals inﬂuential relations cb→pm pm→cb rule→pm pm→rule least inﬂuential relations ga→pm pm→rl. dataset papers computer science note relations estimated solely based available datasets period time might accurately reﬂect relations entire academic ﬁelds. also consider relations static property analysis. larger corpus longer period time possible learn inﬂuence conditioned period visualize relations change. figure relevance score neighbor node column-class center node row-class. example average normalized attention agents largest off-diagonal entry citeseer. average attention probabilistic methods case based largest off-diagonal entry cora. respect relevance scores report fraction edges connecting nodes class. table shows result benchmark datasets relevance scores calculated using last propagation layer. suggests architecture learns higher attention nodes class. figure show three selected target nodes test mistaken correctly classiﬁed agnn. denote target node node thick outline show strength attention node -hop neighborhood target node size corresponding node. colors represent hidden true classes none nodes ﬁgure training hence none colors revealed. still observe agnn managed attention nodes classes allowing trained model correct labels. neighborhood attention. finally analyze nodes test sets mistaken correctly classiﬁed agnn show attention mechanism weighted contribution local neighborhood show three illustrative examples figure examples local attention network provided appendix show entire -hop neighborhood target node test ﬁxed data splits citeseer cora pubmed. colors denote true classes nodes target’s neighborhood unknown models training time. radius node proportional attention target node aggregated layers i.e. citeseer. size target node reﬂects self-attention deﬁned similar way. ﬁrst example left node pubmed. agnn correctly classiﬁes target node light blue whereas mistakes yellow possibly connected yellow nodes. attention mechanism learned weight light blue -hop neighbor equally heavy weights path light blue neighbors immediately connected target node. second example middle node pubmed. agnn correctly classiﬁes yellow whereas mistakes possibly neighbors. attention mechanism learned weight yellow neighbor weighted yellow neighbor even itself. last example right node citeseer. agnn correctly classiﬁes light blue whereas mistakes white. special example nodes completely isolated. static non-adaptive propagation ends giving prediction isolated pairs. pair different true classes always fails least however agnn ﬂexible adapting graph topology puts weight target node itself correctly classifying both. paper present attention-based graph neural network model semi-supervised classiﬁcation graph. demonstrate method consistently outperforms competing methods standard benchmark citation network datasets. also show learned attention also provides interesting insights neighbors inﬂuence other. training tried complex attention models. however increased model complexity training stable give higher accuracy. believe semi-supervised setting limited number labeled examples reducing model complexity important. note able train deeper models compared shallower model part fact remove non-linear layers reduce model complexity signiﬁcantly. comparison deeper models known unstable give performance shallower gcns would like thank wang mason helping experiments efﬁciently google servers. would also like thank fei-fei xinlei chen achal dave insightful discussions.", "year": 2018}