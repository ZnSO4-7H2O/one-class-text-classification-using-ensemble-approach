{"title": "Speaking the Same Language: Matching Machine to Human Captions by  Adversarial Training", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "While strong progress has been made in image captioning over the last years, machine and human captions are still quite distinct. A closer look reveals that this is due to the deficiencies in the generated word distribution, vocabulary size, and strong bias in the generators towards frequent captions. Furthermore, humans -- rightfully so -- generate multiple, diverse captions, due to the inherent ambiguity in the captioning task which is not considered in today's systems.  To address these challenges, we change the training objective of the caption generator from reproducing groundtruth captions to generating a set of captions that is indistinguishable from human generated captions. Instead of handcrafting such a learning target, we employ adversarial training in combination with an approximate Gumbel sampler to implicitly match the generated distribution to the human one. While our method achieves comparable performance to the state-of-the-art in terms of the correctness of the captions, we generate a set of diverse captions, that are significantly less biased and match the word statistics better in several aspects.", "text": "strong progress made image captioning recently machine human captions still quite distinct. primarily deﬁciencies generated word distribution vocabulary size strong bias generators towards frequent captions. furthermore humans rightfully generate multiple diverse captions inherent ambiguity captioning task explicitly considered today’s systems. address challenges change training objective caption generator reproducing groundtruth captions generating captions indistinguishable human written captions. instead handcrafting learning target employ adversarial training combination approximate gumbel sampler implicitly match generated distribution human one. method achieves comparable performance state-of-the-art terms correctness captions generate diverse captions signiﬁcantly less biased better match global uni- bitri-gram distributions human captions. image captioning systems variety applications ranging media retrieval tagging assistance visually impaired. particular models combine state-of-the-art image representations based deep convolutional networks deep recurrent language models ever increasing performance evaluation metrics cider meteor seen e.g. coco image caption challenge leaderboard figure four images test related skiing shown captions adversarial model baseline. baseline model describes four images generic caption whereas model produces diverse image speciﬁc captions. analyze paper likely artifacts deﬁciencies statistics generated captions apparent observing multiple samples. specifically observe state-of-the-art systems frequently reveal themselves generating different word distribution using smaller vocabulary. scrutiny reveals generalization training still challenging generation biased frequent fragments captions. also today’s systems evaluated produce single caption. multiple potentially distinct captions typically correct single image property reﬂected human ground-truth. diversity equally reproduced state-of-the-art caption generators image description. early captioning models rely ﬁrst recognizing visual elements objects attributes activities generating sentence using language models template model n-gram model statistical machine translation advances deep learning end-to-end trainable models combine deep convolutional networks extract visual features recurrent networks generate sentences though modern description models capable producing coherent sentences accurately describe image tend produce generic sentences replicated train furthermore image correspond many valid descriptions. however test time sentences generated methods beam search generally similar. focus increasing sentence diversity integrating diversity promoting heuristic beam search. attempts increase diversity caption generation training ensemble caption generators specializing different portions training set. contrast focus improving diversity generated captions using single model. method achieves learning corresponding model using different training loss opposed training completed. note generating diverse sentences also challenge visual question generation concurrent work language-only dialogue generation studied linguistic community e.g. training recurrent description models common method predict word conditioned image previous ground truth words. test time word predicted conditioned image previously predicted words. consequently test time predicted words conditioned words incorrectly predicted model. training ground truth words model suffers exposure bias caneffectively learn recover predicts incorrect word training. avoid this proposes scheduled sampling training scheme begins training ground truth words slowly conditions generated words words previously produced model. however shows scheduled sampling algorithm inconsistent optimal solution objective converge true data distribution. taking different direction proposes address exposure bias gradually mixing sequence level loss using reinforce rule standard maximum likelihood training. several works followed using reinforcement learning based approaches directly optimize evaluation metrics like bleu meteor cider however optimizing evaluation metrics directly address diversity figure examples comparing multiple captions generated adversarial model baseline. bi-grams top- frequent bi-grams training marked captions replicas training marked test. also embrace ambiguity task extend investigation predicting sets captions single image evaluating quality particularly terms diversity generated set. contrast popular approaches image captioning trained objective reproduce captions provided ground-truth. instead relying handcrafting loss-functions achieve goal propose adversarial training mechanism image captioning. build generative adversarial networks successfully used generate mainly continuous data distributions images although exceptions exist contrast images captions discrete poses challenge trying backpropagate generation step. overcome obstacle gumbel sampler allows end-to-end training. address problem caption generation images discuss metrics measure caption diversity compare human ground-truth. contribute novel solution problem using adversarial formulation. evaluation model shows accuracy generated captions state-of-the-art greatly increase diversity caption sets better match ground-truth statistics several measures. qualitatively model produces diverse captions across images containing similar content sampling multiple captions image generated captions. since current evaluation metrics n-gram matching score captions captions using frequent n-grams likely achieve better scores ones using rarer diverse n-grams. work formulate caption generator generative adversarial network. design discriminator explicitly encourages generated captions diverse indistinguishable human captions. generator trained adversarial loss discriminator. consequently model generates captions better reﬂect humans describe images maintaining similar correctness determined human evaluation. generative adversarial networks. generative adversarial networks framework learns generative models without explicitly deﬁning loss target distribution. instead gans learn generator using loss discriminator tries differentiate real generated samples generated samples come generator. training generate real images gans shown encouraging results works target distribution continuous. contrast target sequence words discrete. applying gans discrete sequences challenging unclear best back-propagate loss sampling mechanism. works looked generating discrete distributions using gans. generate semantic image segmentation discrete semantic labels pixel. uses reinforce trick train unconditional text generator using framework diversity generated text considered. similar work concurrent works gans dialogue generation image caption generation rely reinforcement rule handle backpropagation discrete samples gumbel softmax section discussion. aims generate diverse dialogue multiple sentences produce diverse sentences single image. additionally uses adversarial maximum likelihood loss step generator training. however train generator adversarial loss pre-training. concurrent work also applies gans diversify generated image captions. apart using gumbel softmax discussed above work differs discriminator design quantitative evaluation generator diversity. image captioning task formulated follows given input image generator produces caption describing contents image. inherent ambiguity task multiple possible correct captions image also reﬂected diverse captions written human annotators however image captioning architectures ignore diversity training. standard approach model recurrent language model conditioned input image train using maximum likelihood loss considering every image–caption pair independent sample. ignores diversity human captions results models tend produce generic commonly occurring captions training show section propose address explicitly training generator produce multiple diverse captions input image using adversarial framework adversarial frameworks generative model trained pairing adversarial discriminator tries distinguish generated samples true data samples. generator trained objective fool discriminator optimal exactly matches data distribution. well-suited goal because appropriate discriminator network could coax generator capture diversity human written captions without explicitly design loss function enable adversarial training introduce second network takes input image caption classiﬁes either real fake. providing captions image input discriminator allows factor diversity caption classiﬁcation. discriminator penalize generator producing similar repeated captions thus encourage diversity generator. speciﬁcally discriminator trained classify captions drawn reference captions {r··· rk−} real classifying captions produced generator fake. generator trained using adversarial objective i.e. trained fool discriminator classify real. near state-of-the caption generator model based uses standard encoder-decoder framework stages encoder model extracts feature vectors input image decoder translates features word sequence. image features. images encoded activations pre-trained convolutional neural network captioning models also beneﬁt augmenting features explicit object detection features accordingly extract feature vector containing probability occurrence object provide input generator. language model. decoder shown figure adopted long-short term memory based language model architecture presented image captioning. consists three-layered lstm network residual connections layers. lstm another option input softmax distribution discriminator instead samples. experimented this found discriminator easily distinguishes softmax distribution produced generator sharp reference samples training fails. last option rely work continuous relaxation samples encoded onehot vectors using gumbel-softmax approximation proposed continuous relaxation combined re-parametrization sampling process allows backpropagation samples categorical distribution. main beneﬁt approach plugs model differentiable node need additional steps estimate gradients. whereas previous methods applying discrete output generators policy gradient algorithms show gumbel-softmax approximation also used successfully setting. empirical comparison approaches found gumbel-softmax approximation consists steps. first gumbel-max trick used re-parametrize sampling categorical distribution. given random variable drawn categorical distribution parametrized θ··· expressed gi’s i.i.d. random variables standard gumbel distribution. next argmax equation replaced softmax obtain continuous relaxation discrete random variable gumbelsoftmax approximation output generator sample words adversarial training. straight-through variation sample used forward path soft approximation used backward path allow backpropogation. discriminator model discriminator network takes image represented using feature captions input classiﬁes either real fake. ideally want base decision criteria describe image correctly diverse enough match diversity human captions figure caption generator model. deep visual features input lstm generate sentence. gumbel sampler used obtain soft samples softmax distribution allowing backpropagation samples. network takes features input. first object detection feature input lstm time step shares input matrix word vectors. second global image feature input lstm time-steps input matrix. lstm cell state time scalar parameter controls peakyness distribution. parameter allows control large hypothesis space generator explores adversarial training. additional uniform random noise vector input lstm adversarial training allow generator noise produce diversity. discreteness problem. produce captions generator could simply sample distribution recursively feeding back previously sampled word step sample token. generate multiple sentences sampling pick sentence highest probability done alternatively could also greedy search approaches like beam-search. however directly providing discrete samples input discriminator allow backpropagation discontinuous. alternatives overcome reinforce rule/trick using softmax distribution using gumbel-softmax approximation using policy gradient algorithms reinforce rule/trick allows estimation gradients discrete samples however learning using reinforce trick unstable high variance mechanisms make learning stable like esadversarial training generator discriminator trained alternatively steps spectively. discriminator tries classify fake. addition this found real important also train discriminator classify reference captions drawn random image fake i.e. forces discriminator learn match images captions rely diversity statistics caption set. complete loss function discriminator deﬁned training objective generator fool disp real. found criminator classifying helpful additionally feature matching loss loss trains generator match activations induced generated true data intermediate layer discriminator. case loss match expected value distance vectors dists distx real generated data. generator loss function given conduct experiments ms-coco dataset training consists images human captions each. publicly available test split images experiments. section uses validation split images. image feature extraction activations resc layer -layered resnet convolutional neural network pre-trained imagenet. input images scaled dimensions resnet feature extraction. additionally features network ablation study section following additionally extract -dimensional object detection features using faster region-based convolutional neural network trained object categories coco dataset. features input generator discriminator. object detection features input generator input used generator models reported here. enable this separate distance measuring kernels discriminator network shown figure ﬁrst kernel computes distances image sentence second kernel computes distances sentences architecture distance measuring kernels based minibatch discriminator presented however unlike compute distances captions corresponding image entire minibatch. input captions encoded ﬁxed size sentence embedding vector using lstm encoder obtain vectors image feature also embedded smaller image embedding vector distances computed dists rp×o dimensional tensor number different distance kernels use. distances obtained similar procedure above using different tensor dimensions m×n×o yield distx rp×o. distance vectors capture aspects want discriminator focus distx captures well matches image dists captures diversity distance vectors concatenated multiplied output matrix followed found necessary pre-train generator using standard maximum likelihood training. without pretraining generator gets stuck producing incoherent sentences made random word sequences. also found pre-training discriminator classifying correct imagecaption pairs random image-caption pairs helpful achieve stable training. train discriminator iterations every generator update. also periodically monitor classiﬁcation accuracy discriminator train drops prevents generator updating using discriminator. without feature matching term generator loss training found unstable needed additional maximum likelihood update stabilize also reported however feature matching loss training stable update needed. good range values gumbel temperature found beyond range training unstable within range results sensitive ﬁxed temperature setting experiments reported here. softmax scaling factor value training adversarial models reported here. sampling results also conduct experiments evaluate adversarial caption generator w.r.t. aspects human-like generated captions accurately describe contents image. using diversity statistics word usage statistics proxy measuring closely generated captions mirror distribution human reference captions show adversarial model human-like baseline. using human evaluation automatic metrics also show captions generated adversarial model performs similar baseline model terms correctness caption. diversity. analyze n-gram usage statistics compare vocabulary sizes diversity metrics presented understand measure gaps human written captions automatic methods show adversarial training helps bridge gaps. measure diversity captions corresponding single image div- ratio number unique unigrams div- ratio number unique bigrams mbleu bleu score computed caption rest. mean bleu scores mbleu score. lower values indicate diversity. correctness. generating diverse captions useful correctly describe content image. measure correctness generated captions automatic evaluation metrics meteor spice however since known automatic metrics always correlate well human judgments correctness also report results human evaluations comparing baseline model adversarial model. table presents comparison adversarial model baseline model. baseline adversarial models resnet features. beamsearch results beam size sampling results taking best samples. best caption obtained ranking captions probability assigned model. table also shows metrics recent methods image captioning literature. purpose comparison illustrate strong baseline baseline model competitive recent published work seen meteor spice metrics. comparing baseline adversarial models table adversarial model worse in-terms meteor scores overall spice metrics. look spice scores individual categories shown table adversarial models excel counting relative baseline describing size object correctly. however well known automatic metrics always correlate human judgments correctness caption. primary reason adversarial models poorly automatic metrics produce signiﬁcantly unique sentences using much larger vocabulary rarer n-grams shown section thus less likely well metrics relying n-gram matches. verify claim conduct human evaluations comparing captions baseline adversarial model. human evaluators amazon mechanical turk shown image caption models asked judge sentences better description image choices either sentences report same. results evaluation presented table adversarial baseline models perform similarly adversarial models slightly better. shows despite poor performance automatic evaluation metrics adversarial models produce captions similar even slightly better accuracy baseline model. characterize well captions produced automatic methods match statistics human written captions look n-gram usage statistics generated captions. speciﬁcally compute ratio actual count n-gram caption produced model expected n-gram count based training data. given n-gram occurred times training expect occurs |test-set|/|train-set| times test set. however actual counts vary depending different test training set. compute ratios reference captions test figure comparison n-gram count ratios generated test-set captions different models. left side shows mean n-gram count-ratios function counts training set. right side shows histogram count-ratios. well training test human captions count ratios spread around small variance. baseline model shows clear bias towards frequently occurring n-grams. consistently overuses frequent n-grams training under-uses less frequent ones trend seen three plots frequent tri-grams particularly prone overuse. also observed histogram plots count ratios baseline model poor matching statistics test set. adversarial model much better matching statistics. histogram uni-gram count ratios clearly closer test reference captions. seem signiﬁcantly overusing popular words still trend utilizing rarer words. however clearly better baseline model aspect. improvement less pronounced bitri-grams still present. another clear beneﬁt using adversarial training observed terms diversity captions produced model. diversity terms global statistics image diversity statistics much higher captions produced adversarial models compared baseline models. result presented table vocabulary size approximately doubles baseline model adversarial model using beamsearch. similar trend also seen comparing sampling variants. expected diversity achieved sampling adversarial model instead using beamsearch vocabulary size increasing adv-samp. effect increased diversity qualitative examples shown figure qualitative samples included supplementary material. construct signiﬁcantly novel sentences compared baseline model adv-bs producing novel captions time compared beambs. three per-image diversity statistics also improve adversarial models indicating produce diverse captions input image. table also shows diversity statistics reference captions test set. shows although adversarial models considerably better baseline still diversity statistics compared human written captions especially vocabulary size. finally figure plots vocabulary size function word count threshold curve adversarial model better matches human written captions compared baseline values illustrates gains vocabulary size adversarial models arise using words speciﬁc frequency instead distributed evenly across word frequencies. conducted experiments understand importance different components architecture. results presented table baseline model experiment uses features input trained using maximum likelihood loss shown ﬁrst table four models adversarial training. comparing rows table adversarial training discriminator evaluating single caption badly. diversity meteor score drop compared baseline. setting generator away producing good caption image discriminator unable penalize lack diversity generator. however comparing rows adversarial training using discriminator evaluating captions simultaneously much better terms div- vocabulary size. adding feature matching loss improves diversity also slightly improves accuracy terms meteor score. thus simultaneously evaluating arjovsky bottou. towards principled methods training generative adversarial networks. proceedings international conference learning representations bengio vinyals jaitly shazeer. scheduled sampling sequence prediction recurrent neural networks. advances neural information processing systems upgrading resnet increases meteor score greatly slightly increases vocabulary size. resnet features provide richer visual information used generator produce diverse still correct captions. also notice generator learns ignore input noise. sufﬁcient stochasticity generation process sequential sampling words thus generator doesn’t need additional noise input increase output diversity. similar observation reported conditional architectures presented adversarial caption generator model explicitly trained generate diverse captions images. achieve utilizing discriminator network designed promote diversity adversarial learning framework train generator. results show adversarial model produces captions diverse match statistics human generated captions signiﬁcantly better baseline model. adversarial model also uses larger vocabulary able produce signiﬁcantly novel captions. increased diversity achieved preserving accuracy generated captions shown human evaluation.", "year": 2017}