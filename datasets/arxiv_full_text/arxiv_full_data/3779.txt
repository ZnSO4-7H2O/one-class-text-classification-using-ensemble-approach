{"title": "Robust Classification by Pre-conditioned LASSO and Transductive  Diffusion Component Analysis", "tag": ["cs.LG", "cs.CV", "math.ST", "stat.ML", "stat.TH"], "abstract": "Modern machine learning-based recognition approaches require large-scale datasets with large number of labelled training images. However, such datasets are inherently difficult and costly to collect and annotate. Hence there is a great and growing interest in automatic dataset collection methods that can leverage the web. % which are collected % in a cheap, efficient and yet unreliable way. Collecting datasets in this way, however, requires robust and efficient ways for detecting and excluding outliers that are common and prevalent. % Outliers are thus a % prominent treat of using these dataset. So far, there have been a limited effort in machine learning community to directly detect outliers for robust classification. Inspired by the recent work on Pre-conditioned LASSO, this paper formulates the outlier detection task using Pre-conditioned LASSO and employs \\red{unsupervised} transductive diffusion component analysis to both integrate the topological structure of the data manifold, from labeled and unlabeled instances, and reduce the feature dimensionality. Synthetic experiments as well as results on two real-world classification tasks show that our framework can robustly detect the outliers and improve classification.", "text": "modern machine learning-based recognition approaches require large-scale datasets large number labelled training images. however datasets inherently difﬁcult costly collect annotate. hence great growing interest automatic dataset collection methods leverage web. collecting datasets however requires robust efﬁcient ways detecting excluding outliers common prevalent. limited effort machine learning community directly detect outliers robust classiﬁcation. inspired recent work pre-conditioned lasso paper formulates outlier detection task using pre-conditioned lasso employs unsupervised transductive diffusion component analysis integrate topological structure data manifold labeled unlabeled instances reduce feature dimensionality. synthetic experiments well results real-world classiﬁcation tasks show framework robustly detect outliers improve classiﬁcation. recent years machine learning-based approaches profoundly helped push performance computer vision algorithms. modern computer vision recognition approaches take form supervised learning rely large corpora labeled data train classiﬁcation models. large extent corpora datasets coco collected searching query terms relevant particular object label verifying consistency though crowdsource labeling however data collection methods every image needs veriﬁed labeled annotators costly difﬁcult scale particularly multi-label datasets. address difﬁculties data collection annotation number automated semiautomated ways leverage data proposed. example berg forsyth later neil chen proposed ﬁrst cluster collected images either latent dirichlet allocation exemplar-based afﬁnity propagation label clusters attempt assign query labels largest clusters directly. automated semi-automated ways dataset construction large result datasets learned models plagued outliers. outliers problematic adversely bias decision boundary classiﬁers degrade overall performance. further ideally would want eliminate human labeling together data collection process rather treating results returned search engine weakly-supervised noisy data shifting onus learning identify robust outliers. detecting outliers training datasets however extremely challenging reasons. masking swamping outliers owen masking refers phenomena arises outlying observation detected left remaining outliers cause resulting model less accurate therefore making look like removed outlier inlier. example consider line ﬁtting inlier observations outliers sides line; removing outliers actually cause more less bias resulting regression. swamping refers fact outliers make inliers look like outliers leads inlier observations removed reducing accuracy learned model; swamping becomes serious presence multiple outliers. high dimensionality feature representation lack good similarity metrics feature space. ideally inliers outliers different low-level feature distributions. however aforementioned factors extremely difﬁcult distributions feature space outlier detection. inspired theoretical analysis pre-conditioned lasso wauthier design general framework outlier detection. formulate outlier detection multi-class classiﬁcation pre-conditioned lasso problem. design unsupervised transductive diffusion component analysis feature dimension reduction meet conditions recover signed support true outliers. tdca construction also limits negative effects data bias data unlabelled data diffusion transductive graphs softmax embedding inferring comparable node features formally multi-class classiﬁcation problems assume true feature coefﬁcient vectors help infer corresponding class labels instances. particularly inferred label instance follows gaussian distribution label space ground-truth labels instance variance. simplicity labels multi-label problem encoded using real-valued numbers note effectively converts multi-class classiﬁcation problem regression problem. low-level features instances assumed form low-dimensional manifold high-dimensional feature space problem leveraging data learn recognition models minimal additional human supervision dates back work fergus bergamo torresani attempts made re-ranking images obtained google image search using visual information. fergus models re-ranking images learned either unsupervised mode returned images used relevance feedback mode user tasked annotating images. berg forsyth constructed dataset automatically several animal categories. latent dirichlet analysis used identify latent image topics corresponding images; users asked judge whether clusters relevant not. forms clustering latent models also tried e.g. plsa fergus exemplar-based afﬁnity propagation chen schrof colleagues schrof used combination textual image-based analysis arrive training data. attempts made shift onus learning algorithms proposing active learning collins incremental learning architectures implement forms iterative model image ranking reﬁnement. best knowledge none methods frenay verleysen provided theoretically sound dealing outliers suffer masking swamping problems; model attempt address explicitly particularly image classiﬁcation context. statistics economics variables also called incidental parameters ﬁrst studied neyman scott recently robust regression methods instance-wise outlier indicators studied owen witten katayama fujisawa nguyen tran showed penalized least square penalty fact equivalent huber m-estimator huber gannaz thus introduced non-convex penalty outlier detection. contrast framework inspired analysis pre-conditional lasso wauthier many practical applications lasso always sufferred high-dimension correlated features. thus introduce tdca performs vital role making outlier detection work; otherwise soft-thresholding fail reported owen discussed problem outliers detection using lasso framework crowdsourced pairwise comparison graphs. particularly work incident matrix greatly simpliﬁes problem decomposing original pairwise graph space gradient cyclic ﬂow; outliers projected cyclic ﬂow; contrast approach aims general classiﬁcation scenario learning noisy data without assuming speciﬁc graph structure. graph-based transductive learning methods attracted considerable attention recent years. beneﬁt graphs capture manifold structure data transductive setting. however potential high dimensionality node make properties graphs hard analyze. classical linear dimensionality reduction techniques principle component analysis work dimensionality reduction graphs fail encode graph’s topological structure. contrast softmax embedding around decade hinton roweis maaten hinton showed technique improvement ’traditional’ manifold dimension reduction methods widely separated data-points collapsed near neighbors low-dimensional space. besides softmax embedding tdca built diffusion maps unsupervised transductive learning capture topological structure labelled unlabelled data feature dimension reduction. note idea using diffusion maps dimension reduction used whilst tdca transductive learning designed weakly labelled tasks computing concept manifold training testing images. approach parts focus label space low-level feature space respectively. label space formulate pre-conditioned lasso outlier detection tdca utilized graph-based feature dimension reduction feature space. outliers found tdca features inliers used train classiﬁer choice. according gaussian-markov theorem best unbiased estimator linear regression model absence outliers ordinary least square estimator. assume presence outliers training outliers sparse. observations conjunction lead following problem formulation tion simplify problem; while shrink full singular value decomposition orthogonal basis column space kernel space respectively rank thus conjugate transpose simpliﬁed following pre-conditioned lasso wauthier standard cross-validation also work instance associated outlier variable makes classical leave-out cross-validation unstable. information assign values outlier variables left-out samples. conditions inspired turn problem solving problem ordering training instances speciﬁcally compute regularization path lasso changed lasso ﬁrst select variable subset accounting highest variances observations noted owen subset assigned nonzero elements thus higher likelihood outliers. thus order samples checking changed subset ordered list taken outliers problems. furthermore cross-validation respect speciﬁcally value take outliers instances nonzero coefﬁcients values leave out. cross-validation remaining training select subset achieves highest accuracy withheld data. sign consistency lasso recover signed support outlier variables task requires either increase number training instances reduce feature dimension furthermore correlated variables perennial problem lasso frequently lead systematic failures. essential reduce feature dimensions disentangle correlated feature dimensions. paper assume tdca reduction. here indicates function well suggestions page hastie this always option high number instances internet. however instances take outliers likely fraction outliers increases number responses query. discuss using tdca feature dimension reduction. another difﬁculty solving lasso columns correlated wauthier hope dimensionality reduction process help alleviate removing correlations redundancies data. ﬁnal observation high dimension data image features often lives low-dimensional manifold roweis saul leverage this want manifold-aware manifold-based dimensionality reduction technique also robust noise outliers. motivates transductive diffusion component analysis feature dimension reduction. diffusion-based methods normally used model topological structure data. however training datasets much noise outlier data points. prevent artifacts degrading results introduce transductive diffusion component analysis help unravel transductive graphs composed training testing data. transductive graph construction. suppose construct graph nodes corresponding training testing images. assuming original low-level features nodes graph similarity weight among nodes deﬁned square inner product features node free parameter computational efﬁciency k-nearest-neighbour graph instead median kl=...n fully connected graph. thus graph transition probability instances thus deﬁned restart probability balancing inﬂuence local global topological information diffusion; diffusion state node t-th step. diffusion state deﬁned node softmax embedding dimensionality reduction. noise outlier data samples cause missing spurious interactions graph thus signiﬁcant negative impact towards random walks. softmax normalization contrast reducing inﬂuence extreme values outliers data. thus employ softmax function approximate probability assigned node diffusion state node i.e. original feature node unraveled reduced vector representations model topological structures node graph. referred node features context features capture connections node nodes undirected graphs close direction capture ﬁne-grained topological structures useful classiﬁcation tasks. order ˆskl vectors {ˆso}n following optimization problem figure synthetic experiment experimental setup data illustrate shows regularization path p-lasso. notably fact outliers appear right inliers regularization path indicates ability p-lasso effectively detect outliers. accuracy outlier detection illustrated function percentage outlier respect inliers. text discussion. kl-divergence used objective function. optimization problem efﬁciently solved using l-bfgs here reduced low-level feature pre-conditioned lasso linear classiﬁer classiﬁcation. liers class. outliers uniformly sampled neighborhood around mean class. note outliers typically larger magnitude gaussian noise data visualized fig. indicates outliers blue inliers three classes. assign labels class. instances indexed follows inliers; outliers. regularization path shown fig. index instance labelled. graph shows outliers encountered ﬁrst changes along regularization path. note fact blue inliers encountered left fig. outliers encountered indicates outliers effectively identiﬁed. number instances removed exactly number outliers; compute accuracy outlier detection experiments shown fig. ability detect outliers clearly function number outliers themselves. typically much difﬁcult detect outliers fraction function inliers large. validate approach handles scenario additional experiment vary percentage outliers. speciﬁcally keep inliers vary percentage outliers number inliers. note outliers number outliers larger number inliers making outlier detection rather challenging. p-lasso framework detect corresponding outliers. accuracy outlier detection shown fig. experiments repeated times mean standard deviation bars shown. fig. shows accuracy outlier detection drop signiﬁcantly ratio outlier increases making approach applicable scenarios large number outliers. signiﬁcantly even outliers algorithm still remove outliers. validates efﬁcacy p-lasso framework. cutoff value faciliate evaluation fig. note value doesnot affect p-lasso step; real applications usually donot know number roughly estimate outlier percentage randomly sampling small portion instances manually identify outliers. apply approach automatically actor face labeling context buffy vampire slayer bauml everingham dataset. dataset consists episodes season show labels provided bauml previous work requires either scripts manual labeling training data classiﬁcation tapaswi both. contrast attempt recognize actors fully automatic setting using training images. episode buffy cast list downloadable imdb website three queries google actor name series name actor name series name character name actor download results google image search training images. main challenge using data images downloaded noisy. show framework deal challenge achieves reasonably good performance using images. features dimensions extracted using standard face pipeline bauml detect facial landmarks aligning faces describing face fisher vector faces large-margin dimensionality reduction simonyan around different actors buffey series. groundtruth provided bauml classiﬁcation accuracy results reported tab. dimensionality respectively tdca. linear used ﬁnal classiﬁer outliers removed. experiment always compare framework different baselines directly using features classiﬁcation; constructing transductive graphs labelled unlabelled data using lazy random walk zhou classiﬁcation; tdca using reduced features xi}n classiﬁcation; p-lasso using soft-thresholding features detect outliers classiﬁcation; ipod using hard-thresholding owen features detect outliers classiﬁcation results shown tab. averaged results p-lasso-tdca better alternatives. shows efﬁcacy framework. evaluate signiﬁcance component compare results tdca better bf-. however better bf-. argue differences caused outier images downloaded web. training images higher ratio outliers destroyed random walk label propagation. tdca always better since employs softmax function approximate stationary distribution graphs unravel topological structures low-dimensional representations. actually limit negative effects outliers original label propagation steps softmax normalization reduce inﬂuence extreme values make representation node consistent. classiﬁcation low-dimensional representations limited cases label propagation labels outliers faster propagated good observations. p-lasso ipod features compared alternative framework. even features p-lasso still able detect leave true positive outliers out. thus p-lasso feature also improve results raw. ipod initialized p-lasso also good detection outliers. however ipod using penalty outliers thus nonconvex. works cases except actually makes performance worse p-lasso-tdca. note results rely solely training images downloaded thus reported performances still lower reported bauml utilize script data obtain high quality training images within video. using supervision case challenging since faces queried actor name necessarily related series thus facial appearance even hairstyles different. dataset evaluate framework. dataset consists classes animals images. randomly select images class testing set. animal names words automatically download around images class. gives training images total. overfeat features sermanet used feature extractor images. since animal names common unique training images much better quality last experiment. thus experiment employed reveal insights differences ipod p-lasso p-lasso-tdca. since different classes reduce dimensions respectively tdca. case gives tdca boosts result p-lasso-tdca improves tdca performance surprisingly comparable results obtained using standard supervised learning number instances hand annotated training split dataset train model. compare ipod p-lasso p-lasso-tdca. feature dimension much larger training instances speciﬁcally compare regularization path generated p-lasso ipod p-lasso-tdca. results shown fig. following regularization path computed fig. list several outliers detected p-lasso ipod p-lasso-tdca fig. green boxes indicate successfully detected outliers; boxes indicate failures. style bobcat image makes similar tiger class; moose image animal walking highway atypical. image also atypical since black image entire training data. p-lasso ipod dataset case shown fig. p-lasso ipod conservative detect outliers ipod also better p-lasso removing false positive instance. ipod p-lasso however improve classiﬁcation accuracy paper introduced novel framework p-lasso-ipod robust classiﬁcation. inspired recent theoretical analysis pre-conditioned lasso wauthier employ p-lasso regularization path directly outlier detection label space; also design dtca manifold-based feature dimension reduction feature space. experiments validate efﬁcacy framework. hyunghoon berger bonnie peng jian. diffusion component analysis unraveling functional topology biological networks. annual international conference research computational molecular biology yanwei hospedales timothy xiang xiong jiechao gong shaogang wang yizhou yuan. robust subjective visual property prediction crowdsourced pairwise labels. ieee tpami appear.", "year": 2015}