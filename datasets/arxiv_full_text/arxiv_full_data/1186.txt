{"title": "Striving for Simplicity: The All Convolutional Net", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.", "text": "jost tobias springenberg∗ alexey dosovitskiy∗ thomas brox martin riedmiller department computer science university freiburg freiburg germany {springj dosovits brox riedmiller}cs.uni-freiburg.de modern convolutional neural networks used object recognition built using principles alternating convolution max-pooling layers followed small number fully connected layers. re-evaluate state object recognition small images convolutional networks questioning necessity different components pipeline. max-pooling simply replaced convolutional layer increased stride without loss accuracy several image recognition benchmarks. following ﬁnding building recent work ﬁnding simple network structures propose architecture consists solely convolutional layers yields competitive state performance several object recognition datasets analyze network introduce variant deconvolution approach visualizing features learned cnns applied broader range network structures existing approaches. vast majority modern convolutional neural networks used object recognition built using principles alternating convolution max-pooling layers followed small number fully connected layers krizhevsky ciresan al.). within layers piecewise-linear activation functions used. networks typically parameterized large regularized training using dropout. considerable amount research last years focused improving performance basic pipeline. among major directions identiﬁed. first plethora extensions recently proposed enhance networks follow basic scheme. among notable directions work using complex activation functions techniques improving class inference well procedures improved regularization layer-wise pre-training using label information second success cnns large scale object recognition imagenet challenge stimulated research towards experimenting different architectural choices cnns. notably entries imagenet challenge deviated standard design principles either introducing multiple convolutions pooling layers building heterogeneous modules performing convolutions pooling multiple scales layer since extensions different architectures come parameters training procedures question arises components cnns actually necessary achieving state performance current object recognition datasets. take ﬁrst step towards answering question studying simple architecture could conceive homogeneous network solely consisting convolutional layers occasional dimensionality reduction using stride surprisingly basic architecture trained using vanilla stochastic gradient descent momentum reaches state performance without need complicated activation functions response normalization max-pooling. empirically study effect transitioning standard architecture simpliﬁed performing ablation study cifar- compare model state cifar- cifar- ilsvrc- imagenet dataset. results conﬁrm effectiveness using small convolutional layers recently proposed simonyan zisserman give rise interesting questions necessity pooling cnns. since dimensionality reduction performed strided convolution rather max-pooling architecture also naturally lends studying questions invertibility neural networks ﬁrst step direction study properties network using deconvolutional approach similar zeiler fergus models experiments differ standard cnns several aspects. first interestingly replace pooling layers present practically modern cnns used object recognition standard convolutional layers stride two. understand procedure work helps recall standard formulation deﬁning convolution pooling operations cnns. denote feature produced layer cnn. described -dimensional array size width height number channels p-norm subsampling pooling size stride applied feature -dimensional array following entries function mapping positions positions respecting stride order p-norm pooling regions overlap; however current architectures typically include overlapping pooling compare pooling operation deﬁned standard deﬁnition convolutional layer applied feature given convolutional weights activation function typically rectiﬁed linear activation relu number output feature convolutional layer. formalized like becomes clear operations depend elements previous layer feature map. pooling layer seen performing feature-wise convolution activation function replaced p-norm. therefore question whether special layers need introduced network. complete answer question easy give assume general exist three possible explanations pooling help cnns p-norm makes representation invariant; spatial dimensionality reduction performed pooling makes covering larger parts input higher layers possible; feature-wise nature pooling operation could make optimization easier. assuming second part dimensionality reduction performed pooling crucial achieving good performance cnns easily pooling removed network without abandoning spatial dimensionality reduction means ﬁrst option downside signiﬁcantly reduce overlap convolutional layer preceded pooling layer. equivalent pooling operation top-left feature response considered result less accurate recognition. second option suffer problem since existing convolutional layers stay unchanged results increase overall network parameters. worth noting replacing pooling convolution adds inter-feature dependencies unless weight matrix constrained. emphasize replacement also seen learning pooling operation rather ﬁxing previously considered using different parameterizations literature evaluate options experiments ensuring fair comparison w.r.t. number network parameters. although aware existing studies containing controlled experiments replacing pooling convolution layers noted idea removing pooling entirely unprecedented first nomenclature early work cnns lecun suggests usage different operations subsampling. second albeit considering small networks experiments using convolution layers architecture similar traditional cnns already appeared work neural abstraction pyramidbehnke second difference network model consider standard cnns similar models recently used achieving state-of-the-art performance ilsvrc- competition make small convolutional layers greatly reduce number parameters network thus serve form regularization. additionally unify architecture further make fact image area covered units topmost convolutional layer covers portion image large enough recognize content fully connected layers also replaced simple -by- convolutions. leads predictions object classes different positions simply averaged whole image. scheme ﬁrst described regularizes network convolution much less parameters fully connected layer. overall architecture thus reduced consist convolutional layers rectiﬁed linear non-linearities averaging softmax layer produce predictions whole image. although order implement proper pooling sense commonly considered literature special nonlinearity needs considered. simple convolution layer rectiﬁed linear activation cannot implement p-norm computation. order quantify effect simplifying model architecture perform experiments three datasets cifar- cifar- ilsvrc- imagenet speciﬁcally cifar- perform in-depth study different models since large model dataset trained moderate computing costs hours modern gpu. test best model found cifar- cifar- without augmentations perform ﬁrst preliminary experiment ilsvrc- imagenet dataset. performed experiments using caffe framework. experiments cifar- cifar- three different base network models intended reﬂect current best practices setting cnns object recognition. architectures networks described table starting model depth number parameters network gradually increases model several things noted here. first described table base networks consider -by- convolution produce outputs compute average positions softmax produce class-probabilities performed additional experiments fully connected layers instead -by- convolutions found models consistently perform worse fully convolutional counterparts. line similar ﬁndings prior work hence report numbers avoid cluttering experiments. second observed model table variant network network architecture proposed -by- convolution performed normal convolution layer. third model replaces convolutions simple convolutions. serves purposes uniﬁes architecture consist layers operating spatial neighborhoods previous layer feature max-pooling replaced convolutional layer minimum ﬁlter size allow overlapping convolution stride also highlight model resembles deep models used simonyan zisserman years imagenet competition. table model description three networks derived base model used evaluating importance pooling case classiﬁcation cifar- cifar-. derived models base models built analogously. higher layers table base models experiment three additional variants. additional models base model described table derived models base models built analogously shown table avoid cluttering paper. general additional models base model consist model dense convolution placed max-pooling layer model convpool-cnn-c table. experiments model necessary ensure effect measure solely increasing model size going normal all-cnn counterpart. finally test whether network solely using convolutions also performs well larger scale recognition problem trained up-scaled version all-cnn-b ilsvrc part imagenet database. although expect larger network using convolutions stride ﬁrst layer would perform even better dataset training would take several weeks could thus completed time manuscript. parameters model without data augmentation model strided-cnn-a convpool-cnn-a all-cnn-a model strided-cnn-b convpool-cnn-b all-cnn-b model strided-cnn-c convpool-cnn-c all-cnn-c ﬁrst experiment compared models section cifar- dataset without using augmentations. networks trained using stochastic gradient descent ﬁxed momentum learning rate adapted using schedule multiplied ﬁxed multiplier epochs respectively. keep amount computation necessary perform comparison bearable treat changeable hyperparameter method. learning rate schedule total amount training epochs determined preliminary experiment using base model ﬁxed experiments. used trained networks total epochs. noted strategy guaranteed result best performance methods thus care must taken interpreting following results experiments. learning rate individually adapted model searching ﬁxed following report results best method. dropout used regularize networks. applied dropout input image well after pooling layer dropout probabilities dropping inputs otherwise. also experimented additional dropout however result increased accuracy additionally models regularized weight decay experiments data augmentation perform augmentations also used previous work order keep results comparable. include adding horizontally ﬂipped examples images well randomly translated versions experiments images whitened contrast normalized following goodfellow results models considered given table several trends observed table. first conﬁrming previous results literature simplest model already performs remarkably well achieving error. second simply removing max-pooling layer increasing stride previous layer results diminished performance settings. expected already drop performance dramatic might expect drastic change network architecture. third surprisingly pooling replaced additional convolution layer stride performance stabilizes even improves base model. check increase number trainable parameters compare results convpool versions respective base model. cases performance model without pooling model pooling additional convolution perform par. surprisingly suggests pooling help regularize cnns generally hurt performance strictly necessary achieve state-of-the-art results addition results conﬁrm small convolutions stacked seem enough achieve best performance. perhaps even interesting comparison simple convolutional network derived base model state cifar- shown table without data augmentation. cases simple network performs better best previously reported result. suggests order perform well current benchmarks almost need stack convolutional layers occasional stride perform subsampling. table test error cifar- cifar- all-cnn compared state literature. all-cnn version adapted base model results from number parameters given million parameters. method without data augmentation maxout network network deeply supervised all-cnn data augmentation maxout dropconnect dasnet network network deeply supervised all-cnn performed additional experiment cifar- dataset conﬁrm efﬁcacy best model found cifar-. common practice used model cifar- also kept hyperparameters ﬁxed. note necessarily give best performance. results experiment given table seen simple model using convolutions performs comparable state dataset even though methods either complicated training schemes network architectures. outperformed fractional max-pooling approach uses much larger network performing experiments became aware recent results graham report state cifar-/ data augmentation. results achieved using deep cnns convolution layers combination aggressive data augmentation images placed large pixel images hence heavily scaled rotated color augmented. thus implemented large-all-cnn convolutional version network report results additional experiment table seen large-all-cnn achieves performance comparable network max-pooling. outperformed fractional max-pooling approach performing multiple passes network. note networks vastly parameters networks previous experiments. currently re-training large-all-cnn network cifar- include results table training ﬁnished. performed additional experiments using ilvrc- subset imagenet dataset. since training state model dataset take several weeks computation modern best performance rather performed simple ’proof concept’ experiment. test architectures performing best cifar- also apply larger datasets trained upscaled version all-cnn-b network convolutional layers trained iterations batches samples each starting learning rate dividing every iterations. weight decay used layers. exact architecture used given table appendix. network achieves top- validation error ilsvrc- evaluating center patch comparable top- error reported krizhevsky less million parameters taking roughly days train titan gpu. supports intuition max-pooling necessary training large-scale convolutional networks. however thorough analysis needed precisely evaluate effect max-pooling imagenet-scale networks. complete quantitative analysis using multiple networks imagenet extremely computation-time intensive thus scope paper. order still gain insight effects getting max-pooling layers analyze representation learned convolutional network next section. order analyze network trained imagenet ﬁrst impression well model without pooling lends approximate inversion ’deconvolution’ approach. start idea using deconvolutional network visualizing parts image discriminative given unit network approach recently proposed zeiler fergus following initial attempt observing always work well without max-pooling layers propose efﬁcient visualizing concepts learned higher network layers. deconvolutional network approach visualizing concepts learned neurons higher layers summarized follows. given high-level feature ’deconvnet’ inverts data going neuron activations given layer image. typically single neuron left non-zero high level feature map. resulting reconstructed image shows part input image strongly activating neuron schematic illustration procedure shown figure order perform reconstruction max-pooling layers general invertible method zeiler fergus requires ﬁrst perform forward pass network compute ’switches’ positions maxima within pooling region. switches used ’deconvnet’ obtain discriminative reconstruction. using switches forward pass ’deconvnet’ hence conditioned image directly visualize learned features. architecture include maxpooling meaning theory ’deconvolve’ without switches i.e. conditioning input image. insight lower layers network learn. visualizations figure schematic visualizing activations high layer neurons. given input image perform forward pass layer interested zero activations except propagate back image reconstruction. different methods propagating back relu nonlinearity. formal deﬁnition different methods propagating output activation back relu unit layer note ’deconvnet’ approach guided backpropagation compute true gradient rather imputed version. higher layers network method zeiler fergus fails produce sharp recognizable image structure. agreement fact lower layers learn general features limited amount invariance allows reconstruct single pattern activates them. however higher layers learn invariant representations single image maximally activating neurons. hence reasonable reconstructions necessary condition input image. alternative visualizing part image activates given neuron simple backward pass activation single neuron forward pass network; thus computing gradient activation w.r.t. image. backward pass design partially conditioned image activation functions network maxpooling switches connections deconvolution backpropagation figure visualizations patterns learned lower layers network trained imagenet. single patch corresponds ﬁlter. interestingly gabor ﬁlters appear third layer. order obtain reconstruction conditioned input image network without pooling layers propose modiﬁcation ’deconvnet’ makes reconstructions signiﬁcantly accurate especially reconstructing higher layers network. ’deconvolution’ equivalent backward pass network except propagating nonlinearity gradient solely computed based gradient signal ignoring bottom input. case relu nonlinearity amounts setting zero certain entries based gradient. different approaches depicted figure rows propose combine methods rather masking values corresponding negative entries gradient bottom data mask values least values negative figure call method guided backpropagation adds additional guidance signal higher layers usual backpropagation. prevents backward negative gradients corresponding neurons decrease activation higher layer unit visualize. interestingly unlike ’deconvnet’ guided backpropagation works remarkably well without switches hence allows visualize intermediate layers well last layers network sense bottom-up signal form pattern bottom relu activations substitutes switches. compare guided backpropagation ’deconvnet’ approach replace stride network max-pooling training allows obtain values switches. visualize high level activations using three methods backpropagation ’deconvnet’ guided backpropagation. striking difference image quality visible feature visualizations highest layers network figures appendix. guided backpropagation works equally well without switches ’deconvnet’ approach fails completely absence switches. potential reason ’deconvnet’ underperforms experiment max-pooling ’artiﬁcially’ introduced training. control figure shows visualizations units fully connected layer network initially trained max-pooling. guided backpropagation produces cleaner visualizations ’deconvnet’ approach. modern methods training convolutional neural networks simple architectures perform well network using nothing convolutions subsampling matches even slightly outperforms state cifar- cifar-. similar architecture shows competitive results imagenet. particular opposed previous observations including explicit pooling operations network always improve performance cnns. seems especially case network large enough dataset trained learn necessary invariances convolutional layers. propose method visualizing representations learned higher layers convolutional network. simple produces sharper visualizations descriptive image regions previously known methods used even absence ’switches’ positions maxima max-pooling regions. want emphasize paper meant discourage pooling sophisticated activation functions altogether. rather understood attempt search minimum necessary ingredients recognition cnns establish strong baseline often used datasets. also want stress results models evaluated paper could potentially improved increasing overall model size thorough hyperparameter search. sense fact makes even surprising simple model outperforms many existing approaches. figure visualization patterns learned layer conv layer conv network trained imagenet. corresponds ﬁlter. visualization using guided backpropagation based image patches activating ﬁlter taken imagenet dataset. note image sizes preserved acknowledge funding starting grant videolearn work also partly supported brainlinks-braintools cluster excellence funded german research foundation ciresan meier ueli masci jonathan gambardella luca schmidhuber j¨urgen. high-performance neural networks visual object classiﬁcation. arxivcs/arxiv.. http//arxiv.org/abs/.. hinton geoffrey srivastava nitish krizhevsky alex sutskever ilya salakhutdinov ruslan improving neural networks preventing co-adaptation feature detectors. preprint arxivcs/.v. yangqing shelhamer evan donahue jeff karayev sergey long jonathan girshick ross guadarrama sergio darrell trevor. caffe convolutional architecture fast feature embedding. arxiv preprint arxiv. simonyan karen vedaldi andrea zisserman andrew. deep inside convolutional networks visualising image classiﬁcation models saliency maps. also appeared iclr workshop http//arxiv.org/abs/.. srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. journal machine learning research szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. going deeper convolutions. arxivcs/arxiv. complete model architecture large all-cnn derived spatially sparse network benjamin graham explanation) givenin table note network uses leaky relu units instead relus found speed training. seen also requires much larger input size pixel image centered result subsampling performed convolutional layers stride hence applied much slowly. also note network consists convolutions occasional subsampling spatial dimensionality reduced hence employ global average pooling network. sense architecture hence represents simple convolutional network usable task. conv. leakyrelu stride dropout conv. leakyrelu stride dropout conv. leakyrelu stride dropout conv. leakyrelu stride dropout conv. leakyrelu stride dropout conv. leakyrelu stride dropout conv. leakyrelu stride dropout conv. leakyrelu stride dropout conv. leakyrelu stride dropout conv. leakyrelu stride dropout input image conv. relu stride conv. relu stride conv. relu stride conv. relu stride conv. relu stride conv. relu stride conv. relu stride conv. relu stride conv. relu stride conv. relu stride global average pooling additional visualizations features learned last convolutional layer ’conv’ well pre-softmax layer ’global pool’ depicted figure figure respectively. allow fair comparison ’deconvnet’ guided backpropagation additionally show figure visualizations model max-pooling trained imagenet. figure visualization descriptive image regions different methods single largest activation last convolutional layer conv network trained imagenet. reconstructions different images shown. figure visualization descriptive image regions different methods single largest activation pre-softmax layer global pool network trained imagenet. figure visualization descriptive image regions different methods single largest activation last layer caffenet reference network trained imagenet. reconstructions different images shown.", "year": 2014}