{"title": "Learning using Local Membership Queries", "tag": ["cs.LG", "cs.AI"], "abstract": "We introduce a new model of membership query (MQ) learning, where the learning algorithm is restricted to query points that are \\emph{close} to random examples drawn from the underlying distribution. The learning model is intermediate between the PAC model (Valiant, 1984) and the PAC+MQ model (where the queries are allowed to be arbitrary points).  Membership query algorithms are not popular among machine learning practitioners. Apart from the obvious difficulty of adaptively querying labelers, it has also been observed that querying \\emph{unnatural} points leads to increased noise from human labelers (Lang and Baum, 1992). This motivates our study of learning algorithms that make queries that are close to examples generated from the data distribution.  We restrict our attention to functions defined on the $n$-dimensional Boolean hypercube and say that a membership query is local if its Hamming distance from some example in the (random) training data is at most $O(\\log(n))$. We show the following results in this model:  (i) The class of sparse polynomials (with coefficients in R) over $\\{0,1\\}^n$ is polynomial time learnable under a large class of \\emph{locally smooth} distributions using $O(\\log(n))$-local queries. This class also includes the class of $O(\\log(n))$-depth decision trees.  (ii) The class of polynomial-sized decision trees is polynomial time learnable under product distributions using $O(\\log(n))$-local queries.  (iii) The class of polynomial size DNF formulas is learnable under the uniform distribution using $O(\\log(n))$-local queries in time $n^{O(\\log(\\log(n)))}$.  (iv) In addition we prove a number of results relating the proposed model to the traditional PAC model and the PAC+MQ model.", "text": "introduce model membership query learning learning algorithm restricted query points close random examples drawn underlying distribution. learning model intermediate model pac+mq model membership query algorithms popular among machine learning practitioners. apart obvious diﬃculty adaptively querying labelers also observed querying unnatural points leads increased noise human labelers motivates study learning algorithms make queries close examples generated data distribution. restrict attention functions deﬁned n-dimensional boolean hypercube membership query local hamming distance example training data show following results model class sparse polynomials polynomial time learnable large class locally smooth distributions using o)-local queries. class also includes class o)-depth decision trees. valiant’s probably approximately correct model used widely study computational complexity learning. model goal design algorithms learn unknown target function concept class decision lists class linear separators). hand known rich concept classes polynomial-sized circuits pac-learnable cryptographic assumptions interesting classes eﬃcient learning algorithms cryptographic lower bounds remained elusive polynomial-size decision trees polynomial-size formulas. membership query model learning setting extension model allows learning algorithm query label point choice domain. queries called membership queries. additional power shown classes ﬁnite automata monotone formulas polynomial-size decision trees sparse polynomials learnable polynomial time. celebrated result jackson showed class formulas learnable pac+mq model uniform distribution. jackson used fourier analytic techniques prove result building upon previous work kushilevitz mansour learning decision trees using membership queries uniform distribution. model despite several interesting theoretical results membership query model received enthusiastically machine learning practitioners. course obvious diﬃculty getting labelers perform task learning algorithm executed. another probably signiﬁcant reason disparity quite often queries made algorithms labels points look like typical points sampled underlying distribution. observed lang baum experiments handwritten characters digits revealed query points generated algorithms often structure looked meaningless human eye. cause problems learning algorithm receive noisy labels query points. motivated observations propose model membership queries learning algorithm restricted query labels points look like points drawn distribution. paper focus attention case instance space boolean cube i.e. however similar models could deﬁned case subset suppose natural example i.e. received part training dataset restrict learning algorithm make queries close hamming distance. precisely membership query r-local respect point hamming distance x′|h imagine settings queries could realistic powerful. suppose want learn hypothesis predicts particular medical diagnosis using patient records. could helpful learning algorithm could generate medical record query label. however learning algorithm entirely unconstrained might come record looks gibberish doctor. hand query chosen learning algorithm obtained changing existing record locations likely doctor able make sense record. fact might powerful learning algorithm identify important features record. interesting study power local membership queries learning setting. extremes model mq-model easily observed using -local queries class parities learned polynomial time even presence random classiﬁcation noise. problem known notoriously diﬃcult learning setting time pac+mq algorithms aware algorithms learning decision trees learning formulas rely crucially using strongly non-local way. also easy show formal sense allowing learner make -local queries gives strictly power setting. fact essentially argument used show -local queries powerful r-local queries. separation results easily proved standard cryptographic assumptions presented appendix means points close cannot vastly diﬀerent frequencies. frequency point reﬂects naturalness sense assumption distribution assumption underlying local queries. note uniform distribution locally smooth interested class locally smooth distributions constant distributions changing bits change weight point polynomial factor. distributions include product distributions mean locally α-smooth. alternatively locally α-smooth distributions deﬁned class distributions logarithm density function log-lipschitz respect hamming distance. results give several learning algorithms general locally smooth distributions special case product/uniform distributions. main result general locally smooth distributions sparse polynomials eﬃciently learnable membership queries logarithmically local. important subclass sparse polynomials o-depth decision trees. subclass also give conceptually simpler analysis algorithm richer concept classes also included class sparse polynomials. includes class disjoint log-dnf expressions log-depth decision trees node monomial special case decision trees o)-term expressions. theorem class product distributions mean bounded away constant. then class polynomial-size decision trees learnable respect class distributions algorithm uses o)-local membership queries. results learning sparse polynomials locally smooth distributions rely able identify important monomials low-degree using o)local queries. identify monomials size bounded polynomial required parameters includes important monomials. crucial idea using o)-local queries identify given subset variables whether function remaining variables zero not. fact distribution locally smooth show performing regression monomials give good hypothesis. uniform distributions make fourier techniques numerous algorithms based them. natural approach problem adapt famous algorithm kushilevitz mansour learning decision trees local mqs. algorithm relies procedure isolates fourier coeﬃcients share common preﬁx computes squares. isolation coeﬃcients share preﬁx length requires k-local therefore cannot algorithm directly. instead isolate fourier coeﬃcients contain certain variables grow sets variable-by-variable long squares coeﬃcients large enough. using k-local possible grow sets size importantly preﬁxes algorithm ensures fourier coeﬃcients split disjoint collections therefore many collections relevant. case collections coeﬃcients disjoint prove algorithm take superpolynomial time rely strong concentration properties fourier transform decision trees. case formulas result feldman kalai shows learn formula given heavy logarithmic-degree fourier coeﬃcients. recover coeﬃcients algorithm decision tree case. however case delicate analysis required obtain even running time bound give theorem rely concentration bound mansour shows total weight fourier coeﬃcients degree decays exponentially grows. remark mansour also gives pac+mq algorithm learning running time aside concentration bound algorithm analysis diﬀerent known eﬃcient algorithms learning uniform distribution rely agnostic learning parities using algorithm agnostic case cannot rely concentration properties crucial analysis therefore unclear whether poly-size formulas learned eﬃciently logarithmically-local mqs. evidence hardness problem section show constant k-local queries help agnostic learning uniform distribution. point note locally α-smooth distributions constant main diﬃculty designing algorithms faster time no). designing time algorithm trivial decision trees formulas. fact even require local-membership queries this. follows observation agnostic learning o)-size parities easy time. related work models address problems arise membership queries answered humans studied before. work blum proposed noise model wherein membership queries made points lying probability region distribution unreliable. model authors design algorithms learning intersection halfspaces also learning special subclass monotone formulas. result learning sparse polynomials compared schapire sellie provided algorithm learn sparse polynomials arbitrary distributions angluin’s exact learning model. however algorithm required make membership queries local. bshouty gave algorithm learning decision trees using membership queries. cases seems unlikely algorithms modiﬁed local membership queries even class locally smooth distributions. considerable work investigating learnability beyond framework. consider results body work. many models motivated theoretical well real-world interest. hand interesting study minimum extra power needs setting make class polynomial-size decision trees formulas eﬃciently learnable. work aldous vazirani studies models learning examples generated according markov process. interesting special case models examples generated random recent developments). could simulate random walks length using local membership queries adapting learning algorithm model runs issues adapting algorithm. work kalai provided polynomial time algorithms learning decision trees formulas framework learner gets examples smoothed distribution. model inspired celebrated smoothed analysis framework spielman teng hand models proposed capture plausible settings learner indeed power pac-setting. situations arise example scientiﬁc studies learner black-box access function. recent examples line work learning using injection queries angluin learning using restriction access dvir organization section introduces notation preliminaries also formal deﬁnitions model introduce paper. section presents result learning sparse multi-linear polynomials. space limitations appendix contains results learning decision trees also implementation algorithms presence random classiﬁcation noise. section presents algorithm learning dnfs uniform distribution. section contains result showing agnostic learning setting constant local queries equivalent. appendix shows model introduce strictly powerful setting strictly weaker setting. finally section discusses directions future work. appendix apply fourier techniques. section concept class functions distribution hypothesis deﬁne errd prx∼d squared loss error measure i.e. ex∼d h)]. vector subset denotes bits corresponding variables denotes disjoint sets xsxt denote variables corresponding particular distribution subset denotes marginal distribution variables denote function then denotes thus variables values deﬁned function denote property otherwise). distribution denotes conditional distribution given r-local respect example received querying oracle returns random example labeled according think examples coming natural examples. thus learning algorithm draws natural examples makes queries class distributions pac-learnable using r-local membership queries respect distribution class exist learning algorithm every every distribution every target concept following hold locally smooth distributions since want talk locally smooth distributions both consider state properties interest general terms. distribution locally α-smooth every pair hamming distance x′|h holds squared loss polynomials ex∼d h)]. main result learnable respect class α-smooth distributions theorem class using log)-local time polyα log). output hypothesis multi-linear polynomial that probability ex∼d truncation first show low-degree polynomials approximate multi-linear polynomial arbitrary accuracy. polynomials truncations itself. denote multi-linear polynomial obtained discarding terms degree least note multi-linear t-sparse coeﬃcients magnitude thus could guarantee contains coeﬃcients i.e. non-zero coeﬃcients identiﬁed. guarantees regression step give good approximation since error hypothesis obtained regression smaller ex∼d d)]. lemma show polynomial non-zero coeﬃcient degree d−|s| probability least )d+log. lemma show non-zero coeﬃcient degree less probability .)d+log. thus never subset unless co-eﬃcient size however number hence also running time algorithm section prove concept class eﬃciently agnostically learnable uniform distribution k-local also eﬃciently agnostically learnable random examples alone. result compared feldman shown membership queries help distribution-independent agnostic learning. remark -local suﬃce learning parities size random classiﬁcation noise. time learning random examples alone agnostic learning parities reduced learning parities random classiﬁcation noise however reduction lead agnostic algorithm learning parities -local since highly-nonlocal label every example inﬂuenced labels points chosen randomly uniformly whole hypercube. reduction based embedding unknown function higher dimensional domain original points mapped points least distance apart crucial property embedding that scaling preserves correlation function embedding achieved using linear binary error-correcting code speciﬁcally classic binary code proof following theorem provided appendix particular implies highly unlikely class parities eﬃciently agnostically learnable using k-local mqs. class parities particular interest eﬃcient agnostic algorithm learning sized parities would yield eﬃcient dnf-learning algorithm. introduced local membership query model goal studying query algorithms useful practice. rise crowdsourcing tools increasingly possible human labelers variety tasks. thus membership queries beyond standard active learning paradigm could prove useful increase eﬃciency accuracy learning. order make human labelers necessary make queries make sense them. ways algorithms understood searching higher-dimensional features using queries modify examples locally. model local membership queries also natural simple theoretical model. several interesting open questions class t-leaf decision trees learned class locally smooth distributions? class formulas learnable polynomial time least uniform distribution? another interesting question whether general purpose boosting algorithm exists uses locally α-smooth distributions. looks diﬃcult since boosting algorithms decrease weights points substantially. also interesting whether agnostic learning interesting concept classes possible learning model. results show constant local queries useful agnostic learning. however o)-local queries help learning o)-sized parities agnostic setting? observe learning class o)-sized parities class decision-trees equivalent agnostic learning setting since weak strong agnostic learning equivalent even respect ﬁxed distribution agnostic learning o)-sized parities would also imply learning model local membership queries", "year": 2012}