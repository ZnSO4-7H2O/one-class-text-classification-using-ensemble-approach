{"title": "Long Short-Term Memory Over Tree Structures", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we propose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies, e.g., language or image parse structures. We leverage the models for semantic composition to understand the meaning of text, a fundamental problem in natural language understanding, and show that it outperforms a state-of-the-art recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that without considering the structures.", "text": "chain-structured long short-term memory showed effective wide range problems speech recognition machine translation. paper propose extend tree structures memory cell reﬂect history memories multiple child cells multiple descendant cells recursive process. call model s-lstm provides principled considering long-distance interaction hierarchies e.g. language image parse structures. leverage models semantic composition understand meaning text fundamental problem natural language understanding show outperforms state-of-theart recursive model replacing composition layers s-lstm memory blocks. also show utilizing given structures helpful achieving performance better withconsidering structures. recent years seen revival long short-term memory effectiveness demonstrated wide range problems speech recognition machine translation image-to-text conversion recursion fundamental process associated many problems—a recursive process hierarchical structure formed common different modalities. example semantics sentences human languages believed carried merely linear concatenainstead sentences parse structures tion words; image understanding another example beneﬁts recursive modeling structures yielded state-of-the-art performance tasks like scene segmentation paper extend lstm tree structures learn memory cells reﬂect history memories multiple child cells hence multiple descendant cells. call model s-lstm. compared previous recursive neural networks s-lstm potentials avoiding gradient vanishing hence model long-distance interaction trees. desirable characteristic many structures deep. s-lstm considered bringing merrecursive neural network recurrent neural network together. short s-lstm wires memory blocks partial-order structures instead full-order sequence chain-structured lstm. leverage s-lstm model solve semantic composition problem learns meaning piece texts—learning good representations meaning text core automatically understanding human languages. speciﬁcally experiment models stanford sentiment tree bank determine sentiment different granularities phrases tree. dataset favorable properties addition benchmark much previous work provides human annotations nodes trees enabling comprehensively explore properties s-lstm. experimentally show s-lstm outperforms stateof-the-art recursive model simply replacing original tensor-enhanced composition s-lstm memory block propose here. showed utilizing given structures helpful achieving better performance without considering structures. recursive neural networks recursion fundamental process different modalities. recent years recursive neural networks introduced demonstrated achieve state-of-the-art performances different problems semantic analysis natural language processing image segmentation networks deﬁned recursive tree structures—a tree node vector computed children. recursive fashion information leaf nodes tree internal nodes combined bottom-up manner tree. derivatives errors computed backpropagation structures addition literature also included many efforts applying feedforward-based neural network structures including amongst others. instance legrand collobert leverage neural networks greedy syntactic parsing deep recursive neural network proposed nevertheless often deep structures networks potentially subject vanishing gradient problem resulting difﬁculties leveraging long-distance dependencies structures. paper propose s-lstm model wires memory blocks recursive structures. compare model rvnn models presented directly replaced tensor-enhanced composition layer tree node s-lstm memory block. show advantages proposed model achieving signiﬁcantly better results. recurrent neural networks lstm unlike feedforward network recurrent neural network shares hidden states across time. sequential history summarized hidden vector. also suffers decaying gradient less frequently blowing-up gradient problem. lstm replaces hidden vector recurrent neural network memory blocks equipped gates; principle keep longterm memory training proper gating weights intuitive illustrations good discussions) practically showed useful achieving state range problems including speech recognition digit handwriting recognition achieve interesting results statistical machine translation music composition deep lstm network achieved state-of-the-art results timit phoneme recognition benchmark. pair lstm networks trained encode decode human language automatic machine translation particular effective challenging long sentence translation. lstm networks found useful digit writing recognition network’s capability memorizing context information long sequence. lstm networks trained effectively capture global structures temporal data. memory cells lstm able keep track temporally distant events indicate global music structures. result lstm successfully trained compose music rnns failed although promising results observed applying chain-structured lstm many interesting problems inherently associated input structures complicated sequence. example sentences human languages believed carried merely linear sequence words; instead meaning thought interweave structures. sequential application lstm capture structural information implicitly practice sometimes lacks claimed power. example even simply reversing input sequences result signiﬁcant differences modeling performances tasks machine translation speech recognition. unlike previous work propose directly wire memory blocks recursive structures. show proposed s-lstm model utilize structures achieve results better ignoring priori structures. model brief paper extend lstm structures memory cell reﬂect history memories multiple child cells hence multiple descendant cells hierarchical structure. intuitively showed figure root tree principle consider information long-distance interactions tree—in ﬁgure gray light-blue leaf. ﬁgure small circle short line arrowhead indicates pass block information respectively. note ﬁgure shows binary case real models soft version gating applied gating signal range often enforced logistic sigmoid function. learning gating signals detailed later section s-lstm provides principled considering long-distance interplays input structures. figure.an example s-lstm long-short term memory network tree structures. tree node consider information multiple descendants. information nodes white blocked. small circle short line arrowhead indicates pass block information respectively real model gating soft version gating. memory block node figure composed s-lstm memory block. present speciﬁc wiring block figure memory block contains input gate output gate. number forget gates depends structure i.e. number children node. paper assume children nodes therefore data experiments. forget gates. extension model handle children rather straightforward. shown ﬁgure hidden vectors children denoted right taken input current block. input gate consider four resources information hidden vectors children. four sources information also used form gating signals left forget gate right forget gate weights used combining speciﬁc gates denoted figure.a s-lstm memory block consisting input gate forget gates output gate. hidden vectors cell vectors left right children deployed compute denotes hadamard product shaped sign squashing function different formulas below. different process regular lstm cell considers copies gated children’s cell vectors left right forget gate respectively; bias network weight matrices; sign hadamard product i.e. element-wise product. subscripts weight matrices indicate used for. example matrix mapping hidden vector output gate. backpropagation structures training gradient objective function respect parameter calculated efﬁciently backpropagation structures major difference lstm-like backpropagation unlike regular lstm pass error needs discriminate between left right children topology children needs discriminate children. obtaining backprop formulas tedious list facilitate duplication work discuss speciﬁc objective function later experiments. memory block assume error passed derivatives output gate hidden vector left forget gate input gate computed element-wise derivative logistic function vector since computed activation abuse notation write activated vectors equations. derivative cell vector. current node left child parent equation calculate otherwise formula used derivatives gate computed derivatives weight matrices used formula calculated accordingly omitted here. checked correctness s-lstm implementation standard approximated gradient approach. objective trees objective function deﬁned structures complicated could consider output structures depending properties problem. following overall objective function used learn s-lstm paper simply minimizing overall cross-entropy errors nodes. discussed earlier recursion basic process inherent many problems. paper leverage proposed model solve semantic composition meanings pieces text fundamental problem understanding human languages. determine sentiment speciﬁcally attempt different granularities phrases tree within stanford sentiment tree bank benchmark data obtaining sentiment long piece early work often factorized problem consider smaller pieces component words phrases bag-of-words bag-ofphrases models recent work started model composition principled approach modeling formation semantics. paper proposed lstm memory blocks tree nodes—we replaced tensorenhanced composition layer tree node presented s-lstm memory block. used dataset stanford sentiment tree bank evaluate performances models. addition benchmark much previous work data provide human annotations nodes trees facilitating comprehensive exploration properties s-lstm. default setting default setting conducted experiments table shows accuracies different models test stanford sentiment tree bank. present results -category sentiment prediction sentence level phrases including roots table naive bayes support vector machine classiﬁers respectively; rvnn corresponds described earlier refer recursive neural networks rvnn avoid confusion recurrent neural networks. rntn different rvnn merging nodes obtain hidden vector parent tensor used obtain second-degree polynomial interactions. table.performances different models test stanford sentiment tree bank sentence level phrase level. shows performance statistically signiﬁcantly better corresponding models. table showed s-lstm achieved best predictive performance compared models reported s-lstm results reported obtained setting size hidden units batch size learning rate experiments tuned hyper-parameters feel ﬁner tuning discriminating classiﬁcation weights leaves nodes using different numbers hidden units memory blocks different initializations word embedding improve performances reported here. evaluate s-sltm model’s convergence behavior figure depicts converging time training. speciﬁcally show sub-ﬁgures roots phrases ﬁgures observe s-lstm converge faster rntn. instance phrase-level task s-lstm started converge minutes rntn needed minutes. s-lstm stanford sentiment tree bank contains sentences movie reviews originally discussed sentences parsed stanford parser phrases tree nodes manually annotated sentiment values. split training test data predict sentiment categories roots phrases root sentiment training development test sentences respectively. phrase sentiment task includes phrases three sets. following also classiﬁcation accuracy measure performances. mentioned before follow minimize cross-entropy error nodes roots only depending speciﬁc experiment settings. phrases error calculated regularized yseni predicted distribution target distribution. number classes categories denotes j-th element multinomial target distribution; iterates nodes model parameters regularization parameter. tuned model development data split results table show settings s-lstm outperforms rntn large margin. root labels used train models s-lstm obtains accuracy compared rntn. leaf labels also used s-lstm achieves accuracy rntn improvements statistically signiﬁcant rntn without supervising signals internal nodes composition parameters learned well potentially tensor much parameters learn. hand controlling gates s-lstm shows good ability learn trees. performance different levels trees figure depict performances models different levels nodes trees. figure x-axis corresponds different depths lengths y-axis accuracy. depth deﬁned longest distance root phrase descendant leafs. length simply number words node depth necessarily length—e.g. balanced tree leafs different depths unbalanced tree number leafs. trends ﬁgure similar. ﬁgures s-lstm performs better depths showing advantages nodes depth. deeper levels tree tend complicated syntax semantics s-lstm model complicated syntax semantics better. explicit structures structures efforts literature attempt learn distributed representation utilizing input structures available others prefer assume chain-structured recurrent neural networks actually capture structures implicitly though linear coding process. paper attempt give empirical evidences experiment setting comparing several different models. first special case s-lstm model considered sentential structures given. instead words read left right combined order. call left recursive s-lstm sreal-life settings compare s-lstm rntn experimental settings. ﬁrst setting keep training signals roots train s-lstm rntn depicted model table root lbls besides model names stands root labels; gold labels sentence level used train model. sentiment analysis circumstances phrase level annotations available nodes tree fragments interesting; e.g. fragment good movie also annotating phrases expensive. however regarded comments value sentiment tree bank. detailed annotations tree bank enable much interesting work possible e.g. study effect negation changing sentiment second setting corresponding model table slightly different keep annotation tree leafs well simulate sentiment lexicon available covers leafs out-of-vocabulary concern. using real sentiment lexicons expected performance settings here. table.performances models given sentence structures. s-lstm-lr degenerated version slstm reads input words left right s-lstm-rr reads words right left. proposed s-lstm memory cell reﬂect history memories multiple descendants gated copying memory vectors. model provides principled consider long-distance interplays structures. leveraged model learn distributed sentiment representations texts showed outperforms stateof-the-art recursive model replacing tensor-enhanced composition layers s-lstm memory blocks. showed structure information useful helping s-lstm achieve state-of-the-art performance. research community seems contain lines wisdom; attempts learn distributed representation utilizing structures available prefers believe recurrent neural networks actually capture structures implicitly linear-chain coding process. paper also attempt give empirical evidences toward answering question. least settings experiments explicit input structures helpful inferring high-level semantics.", "year": 2015}