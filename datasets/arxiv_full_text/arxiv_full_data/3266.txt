{"title": "Large-Margin Determinantal Point Processes", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Determinantal point processes (DPPs) offer a powerful approach to modeling diversity in many applications where the goal is to select a diverse subset. We study the problem of learning the parameters (the kernel matrix) of a DPP from labeled training data. We make two contributions. First, we show how to reparameterize a DPP's kernel matrix with multiple kernel functions, thus enhancing modeling flexibility. Second, we propose a novel parameter estimation technique based on the principle of large margin separation. In contrast to the state-of-the-art method of maximum likelihood estimation, our large-margin loss function explicitly models errors in selecting the target subsets, and it can be customized to trade off different types of errors (precision vs. recall). Extensive empirical studies validate our contributions, including applications on challenging document and video summarization, where flexibility in modeling the kernel matrix and balancing different errors is indispensable.", "text": "determinantal point processes oﬀer powerful approach modeling diversity many applications goal select diverse subset. study problem learning parameters labeled training data. make contributions. first show reparameterize dpp’s kernel matrix multiple kernel functions thus enhancing modeling ﬂexibility. second propose novel parameter estimation technique based principle large margin separation. contrast state-of-the-art method maximum likelihood estimation large-margin loss function explicitly models errors selecting target subsets customized trade diﬀerent types errors extensive empirical studies validate contributions including applications challenging document video summarization ﬂexibility modeling kernel matrix balancing diﬀerent errors indispensable. imagine design search engine retrieve images match user queries. response search term jaguar retrieve images animal jaguar images automobile jaguar? frequently cited example illustrates need incorporate notion diversity. many tasks want select subset items ground set. ground might contain many similar items goal discover ones rather subset diverse items ensure coverage example retrieving images jaguar achieve diversity including types images. recently determinantal point process emerged promising technique modeling diversity deﬁnes probability distribution power ground set. intuitively subsets higher diversity assigned larger probabilities thus likely selected lower diversity. since original application quantum physics found many applications modeling random trees graphs document summarization search ranking information retrieval clustering various extensions also studied including k-dpp structured markov continuous spaces probability distribution depends crucially kernel square symmetric positive semideﬁnite matrix whose elements specify similar every pair items ground are. kernel matrix often unknown needs estimated training data. challenging problem several reasons. first number parameters i.e. number elements kernel matrix quadratic number items ground set. many tasks ground large. thus impractical directly specify every element matrix suitable reparameterization matrix necessary. secondly number training samples often limited many practical applications. example task document summarization select succinct subset sentences long document. there acquiring accurate annotations human experts costly diﬃcult. thirdly many tasks need evaluate performance learned accuracy predicting whether item selected also measures like precision recall. instance failing select sentences summarizing documents might regarded catastrophic injecting sentences repetitive information summary. existing methods parameter estimation dpps inadequate address challenges. example maximum likelihood estimation typically requires large number training samples order estimate underlying model correctly. also limits number parameters estimate reliably restricting dpps whose kernels parameterized degrees freedom. also oﬀer control precision recall. propose two-pronged approach learning labeled data. first improve modeling ﬂexibility reparameterizing dpp’s kernel matrix multiple base kernels. representation could easily incorporate domain knowledge requires learning fewer parameters then optimize parameters probability correct subset larger erroneous subsets large margin. margin task-speciﬁc customized reﬂect desired performance measure—for example monitor precision recall. such approach deﬁnes objective functions closely track selection errors work well training samples. principle large margin separation widely used classiﬁcation structured prediction formulating learning large margin principle novel. empirical studies show proposed method attains superior performance challenging tasks practical interest document video summarization. rest paper organized follows. provide background section followed approach section discuss related work section report empirical studies section conclude section ﬁrst review background determinantal point process standard maximum likelihood estimation technique learning parameters data. details found excellent tutorial given ground items deﬁnes probabilistic measure power i.e. possible subsets concretely denote symmetric positive semideﬁnite matrix rn×n. probability selecting subset given denotes submatrix rows columns selected indices identity matrix proper size. deﬁne deﬁning called lensemble. equivalent deﬁning kernel matrix deﬁne marginal probability selecting random subset submatrix indexed despite exponential number summands marginalization analytically tractable computable polynomial time. modeling diversity particularly useful property ability model pairwise repulsion. consider marginal probability items simultaneously subset thus unless probability observing jointly always less observing either separately. namely subset repulsively excludes vice versa. another extreme case same; leads p{ij} namely never allow together subset. consequently subset large probability cannot many items similar words probability provides gauge diversity subset. diverse subset balances pairwise repulsions subset attains highest probability note inference computed respect l-ensemble interested mode marginal probability subset. unfortunately inference nphard various approximation algorithms investigated maximum likelihood estimation suppose given training ground annotated diverse subset discover underlying parameters note diﬀerent ground sets need overlap. thus directly specifying kernel values every pair items unlikely scalable. instead need assume either ground represented shared parameters items suppose kernel values knij computed function features characterizing items. learning objective optimize diverse subset attains highest probability. gives rise following maximum likelihood estimate approach consists components developed parallel work concert multiple kernel functions represent dpp; applying principle large margin separation optimize parameters. former reduces number parameters learn thus especially advantageous number training samples limited. latter strengthens advantage optimizing objective functions closely track subset selection errors. learning matrix instance learning kernel functions matrices positive semideﬁnite matrices interpretable kernel functions evaluated items ground set. thus goal essentially learn right kernel function measure similarity. however many applications similarity criteria selecting items. instance previous example image retrieval retrieved images need diverse also need strong relevance query term. similarly document summarization selected sentences need succinct redundant also need represent contents document referred quality factor modeling representative relevant selected items are. depends item feature vector encodes contextual information representativeness items. example document summarization possible features sentence lengths positions sentences text others. hand measures similar sentences computed diﬀerent features bag-of-words descriptors represent item’s individual characteristics. however prior work investigate whether speciﬁc deﬁnition similarity could made optimal adapted data thus limiting modeling power largely infer quality empirical studies show limitation severe especially modeling choice erroneous paper retain aspect quality modeling improve modeling similarity ways. first nonlinear kernel functions gaussian kernel determine similarity. secondly importantly combine several base kernels maximum likelihood estimation closely track discriminative errors improving likelihood ground-truth subset could also improve likelihoods competing subsets. consequentially model learned could modes diﬀerent subsets close probability values. highly confusable modes especially problematic dpp’s np-hard inference diﬀerence modes fall within approximation errors approximate inference algorithms true cannot easily extracted. multiplicative large margin constraints address deﬁciencies large-margin based approach aims maintain increase margin correct subset alternative incorrect ones. speciﬁcally formulate following large margin constraints similar intuitions explored multiway classiﬁcation structured prediction margin multiplicative instead additive design leads tractable optimization exponential number constraints explain later. design loss function natural choice loss function hamming distance counting number disagreements subsets loss function failing select right item costs adding unnecessary item. many tasks however symmetry hold. example summarizing document omitting sentence severe consequences adding sentence. greater learning biases towards higher recall select many items possible. signiﬁcantly less learning biases towards high precision avoid incorrect items much possible. empirical studies demonstrate ﬂexibility advantages real-world summarization tasks. numerical optimization overcome challenge dealing exponential number constraints reformulate tractable optimization problem. ﬁrst upper-bound hard-max operation jensen’s inequality knii i-th element diagonal marginal kernel matrix corresponding detailed derivation result supplementary material. note computed eﬃciently identity tradeoﬀ coeﬃcient tuned validation datasets. note objective function subsumes maximum likelihood estimation optimize objective function subgradient descent. details supplementary material. arises random matrix theory quantum physics machine learning researchers proposed diﬀerent variations improve modeling capacity. kulesza taskar introduced k-dpp restrict sets constant size aﬀandi proposed markov oﬀers diversity adjacent time stamps structured presented model trees graphs. inference generally np-hard gillenwater developed /-approximation algorithm practice greedy inference gives rise decent results though lacks theoretical guarantees. another popular alternative resort fast sampling algorithms spite much research activity surrounding dpps little work exploring eﬀectively learn model parameters. popular estimator. compared approach robust number training data mis-speciﬁed models oﬀers greater ﬂexibility incorporating customizable error functions. recent bayesian approach works posterior parameters contrast work develop large-margin training approach dpps directly minimize selection errors. large-margin principle widely used classiﬁcation structured prediction application original. order make tractable dpps multiplicative rather additive margin constraints. validate large-margin approach learn parameters extensive empirical studies synthetic data real-world summarization tasks documents videos. also applications beyond summarization particularly good testbed illustrate diverse subset selection compact summary ought include high quality items that taken together oﬀer good coverage source content. report results section provide extensive results supplementary material. data ground items item sample -dimensional feature vector spherical gaussian generate matrix follow model parameter vector sample spherical gaussian similarity simply compute identify diverse subset exhaustive search subsets possible given small ground set. resulting items average. noise randomly adding dropping item repeat process sampling another pair ground diverse set. times pairs holdout testing. repeat process yield training sets various sizes. evaluation metrics evaluate quality selected subset ymap ground-truth using f-score harmonic mean precision recall three quantities higher values better. learning inference compare large-margin approach using hamming loss standard method learning parameters. hyperparameters tuned cross-validation. learning apply inference testing ground sets. results parameterized things quality items similarity among them. since ground-truth parameters known conduct experiments isolate impact learning either one. fig. contrasts methods learning only assuming known groundtruths used. dpplme method signiﬁcantly outperforms dppmle. number training samples increased performance method generally improves gets close oracle’s performance true values used. figure synthetic datasets method dpplme signiﬁcantly outperforms state-of-the-art parameter estimation technique dppmle various learning settings. text details. best viewed color. fig. compares methods need learned data. apply multiple kernel parameterization technique model except zero avoid including ground-truth. parameterization overcomes problems model misspeciﬁcation fig. demonstrating eﬀectiveness approximating unknown similarities. fact learning methods match performance corresponding methods ground-truth similarity values respectively. nonetheless large-margin estimation still outperforms signiﬁcantly. summary results synthetic data encouraging. multiple kernel parameterization avoids pitfall model mis-speciﬁcation large margin estimation outperforms ability track selection errors closely. next apply task extractive multi-document summarization task input document cluster consisting several documents single topic. desired output subset sentences cluster serve summary entire cluster. naturally want sentences subset representative diverse. setup text data document understanding conference training testing sets respectively. document clusters collected short time period single topic. cluster includes news articles average sentences. four human reference summaries provided along cluster. following prior work generate oracle/ground-truth summary identifying subset original sentences best agree human reference summaries average oracle summary consists sentences. standard practice oracles training. testing algorithm output evaluated four human reference summaries separately report average accuracy widely-used evaluation package rouge scores document summaries based n-gram overlap statistics. rouge along wordnet report f-score precision recall unigram bigram matchings denoted rouge-x rougex respectively additionally limit maximum length summary allow fairest comparison existing work task features designated model quality features sentence length position original document mean cluster similarity lexrank personal pronouns. model similarity features standard normalized term frequency-inverse document frequency vectors. learning consider ways modeling similarities. ﬁrst cosine similarity feature vectors second multiple kernel based similarity bandwidths combination coeﬃcients learned data. implement method baseline also test enhanced variant method replacing cosine similarity multiple kernel based similarity results table compares several dpp-based methods well three results competition dpp-based since inference np-hard sampling technique extract diverse subset inference times report mean accuracy standard error. state-of-the-art mle-trained model achieves performance best peer results obtain noticeable improvement applying large-margin estimation applying multiple kernels model similarity obtain signiﬁcant improvements parameter estimation techniques. particular complete method dpplme+mkr attains best performance across evaluation metrics. finally demonstrate broad applicability method applying video summarization. case goal select representative diverse frames video sequence. setup dataset consists videos open video project pixels vary minutes distributed across several genres including documentary educational historical etc. provided ground truth frame summaries video labeled annotators independently. perform -fold validation report average result. apply several preprocessing steps remove frames trivially redundant visual quality. similar procedure document summarization task generate groundtruth subsets. average ground-truth frames public evaluation package vsumm evaluate system-generated summary frames compute precision recall f-score details supplementary material. features extract frame color histogram sift-based fisher vector model pairwise frame similarity features combined multiple kernel representation. model quality frame extract intra-frame inter-frame representativeness features. computed saliency maps include mean standard deviation median quantiles maps well visual similarities frame neighbors. z-score within video sequence. results table compares several methods selecting frames unsupervised clustering method vsumm dppmle multiple kernel parameterization margin-based approach. method illustrate ﬂexibility target diﬀerent operating points varying tradeoﬀ constant generalized hamming distance loss function recall higher values promote higher recall lower promote higher precision. results clearly demonstrate advantage approach particularly oﬀers ﬁner control tradeoﬀ precision recall. adjusting method performs best three metrics outperforms baselines statistically signiﬁcant margin measured standard errors. controlling tradeoﬀ quite valuable application; example high precision preferable user summarizing video captured whereas high recall preferable user summarizing video taken third party detailed analysis including exemplar video frames provided supplementary material. determinantal point process oﬀers powerful probabilistically grounded approach selecting diverse subsets. proposed novel technique learning dpps annotated data. contrast status maximum likelihood estimation method ﬂexible modeling pairwise similarity avoids pitfall model mis-speciﬁcation. empirical results demonstrate advantages synthetic datasets challenging real-world summarization applications.", "year": 2014}