{"title": "Incremental Network Quantization: Towards Lossless CNNs with  Low-Precision Weights", "tag": ["cs.CV", "cs.AI", "cs.NE"], "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization, our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. The code is available at https://github.com/Zhouaojun/Incremental-Network-Quantization.", "text": "paper presents incremental network quantization novel method targeting efﬁciently convert pre-trained full-precision convolutional neural network model low-precision version whose weights constrained either powers zero. unlike existing methods struggled noticeable accuracy loss potential resolve issue beneﬁting innovations. hand introduce three interdependent operations namely weight partition group-wise quantization re-training. wellproven measure employed divide weights layer pre-trained model disjoint groups. weights ﬁrst group responsible form low-precision base thus quantized variable-length encoding method. weights group responsible compensate accuracy loss quantization thus ones re-trained. hand three operations repeated latest re-trained group iterative manner weights converted low-precision ones acting incremental network quantization accuracy enhancement procedure. extensive experiments imagenet classiﬁcation task using alknown deep architectures including alexnet vgg- googlenet resnets well testify efﬁcacy proposed method. speciﬁcally -bit quantization models improved accuracy -bit ﬂoating-point references. taking resnet- example show quantized models -bit -bit -bit ternary weights improved similar accuracy -bit ﬂoating-point baseline. besides impressive results combination network pruning also reported. believe method sheds insights make deep cnns applicable mobile embedded devices. code available https//github.com/zhouaojun/incrementalnetwork-quantization. deep convolutional neural networks demonstrated record breaking results variety computer vision tasks image classiﬁcation face recognition semantic segmentation object detection regardless availability signiﬁcantly improved training resources abundant annotated data powerful computational platforms diverse training frameworks promising results deep cnns mainly attributed large number learnable parameters ranging tens millions even hundreds millions. recent progress shows clear evidence cnns could easily enjoy accuracy gain increased network depth width however turn lays heavy burdens memory ∗this work done aojun zhou intern intel labs china supervised anbang proposed original idea responsible correspondence. ﬁrst three authors contributed equally writing paper. computational resources. instance resnet- speciﬁc instance latest residual network architecture wining imagenet classiﬁcation challenge model size needs perform billion flops classify image crop. therefore challenging deploy deep cnns devices limited computation power budgets. substantial efforts made speed-up compression cnns training feedforward test them. among existing methods category network quantization methods attracts great attention researches developers. network quantization works compress pre-trained full-precision models directly. gong address storage problem alexnet vector quantization techniques. replacing weights three fully connected layers respective ﬂoating-point centroid values obtained clustering model compression loss top- recognition rate. hashednet uses hash function randomly pre-trained weights hash buckets weights hash bucket constrained share single ﬂoating-point value. hashednet fully connected layers several shallow models considered. better compression present deep compression method combines pruning vector quantization huffman coding reduce model storage alexnet vgg- vanhoucke -bit ﬁxed-point implementation improve computation neural networks modern intel cpus feed-forward test yielding speed-up optimized ﬂoating-point baseline. training cnns substituting -bit ﬂoating-point representation -bit ﬁxed-point representation also explored gupta seminal works attempt restrict cnns low-precision versions training phase. soudry propose expectation backpropagation estimate posterior distribution deterministic network weights. network weights constrained feed-forward test probabilistic way. binaryconnect extends idea behind binarize network weights training phase directly. versions network weights ﬂoating-point binary. ﬂoating-point version used reference weight binarization. binaryconnect achieves state-of-the-art accuracy using shallow cnns small datasets mnist cifar-. later series efforts invested train cnns low-precision weights low-precision activations even low-precision gradients including limited binarynet xnor-net ternary weight network dorefa-net quantized neural network despite tremendous advances quantization still remains open problem critical issues well resolved especially scenarios using low-precision weights quantization. ﬁrst issue non-negligible accuracy loss quantization methods issue increased number training iterations ensuring convergence. paper attempt address issues presenting novel incremental network quantization method. assumption architecture basic goal efﬁciently convert pre-trained full-precision model low-precision version whose weights constrained either powers zero. advantage kind low-precision models original ﬂoating-point multiplication operations replaced cheaper binary shift operations dedicated hardware like fpga. noticed existing network quantization methods adopt global strategy weights simultaneously converted low-precision ones considered different importance network weights leaving room retain network accuracy limited. sharp contrast existing methods makes careful handling model accuracy drop network quantization. speciﬁc incorporates three interdependent operations weight partition group-wise quantization re-training. weight partition uses pruning-inspired measure divide weights layer pre-trained full-precision model disjoint groups play complementary roles inq. weights ﬁrst group quantized either powers zero variable-length encoding method forming low-precision base original model. weights group re-trained keeping quantized weights ﬁxed compensating accuracy loss resulted quantization. furthermore three operations repeated pre-trained fullfigure overview incremental network quantization method. precision model used reference. model update three proposed operations weight partition group-wise quantization re-training final low-precision model weights constrained either powers zero. ﬁgure operation represents single operation denotes procedure repeating operation latest re-trained weight group non-zero weights quantized. method lead accuracy loss using -bit -bit even -bit approximations network quantization. better visualization -layer fully connected network illustrative example newly re-trained weights divided disjoint groups size operation except last performs quantization re-trained ﬂoating-point weights occupying model weights. main insight compact combination proposed weight partition groupwise quantization re-training operations potential lossless low-precision model full-precision reference. conduct extensive experiments imagenet large scale classiﬁcation task using almost known deep architectures validate effectiveness method. show that alexnet vgg- googlenet resnets -bit quantization achieves improved accuracy comparison respective full-precision baselines. absolute top- accuracy gain ranges absolute top- accuracy gain range property easy convergence training. general re-training less epochs could consistently generate lossless model -bit weights experiments. taking resnet- example quantized models -bit -bit -bit ternary weights also improved similar accuracy compared -bit ﬂoating-point baseline. taking alexnet example combination network pruning outperforms deep compression method signiﬁcant margins. suppose pre-trained full-precision model represented denotes weight layer denotes number learnable layers model. simplify explanation consider convolutional layers fully connected layers. models like alexnet vgg- googlenet resnets tested paper tensor convolutional layer matrix fully connected layer. simplicity dimension difference considered expression. given pre-trained full-precision model main goal convert -bit ﬂoating-point weights either powers zero without loss model accuracy. besides also attempt explore limit expected bit-width premise guaranteeing lossless network quantization. here start basic network quantization method integer numbers satisfy mathematically help bound sense non-zero elements constrained range either network weights absolute values smaller pruned away ﬁnal low-precision model. obviously problem determine expected bit-width storing indices beforehand thus hyper-parameter shall determined naturally computed available. here calculated using tricky practically effective formula element-wise operation outputs largest element input. fact equation helps match rounding power could easily implemented practical programming. obtained naturally determined instance easy adjacent elements sorted making equation numerical rounding quantum values. emphasized factor equation make sure elements correspond quantization rule deﬁned equation words factor equation highly correlates factor equation here important thing want clarify deﬁnition expected bit-width taking -bit quantization example since zero value cannot written power represent zero value remaining bits represent different values powers two. number candidate quantum values quantization method actually adopts variable-length encoding scheme. clear quantization described performed linear scale. alternative solution perform quantization scale. although also effective little difﬁcult implementation cause extra computational overhead comparison method. naturally described method quantize pre-trained full-precision model. however noticeable accuracy loss appeared experiments using small bit-width values literature many existing network quantization works hashednet vector quantization ﬁxed-point representation binaryconnect binarynet xnor-net dorefa-net similar basic network quantization method also suffer non-negligible accuracy loss deep cnns especially applied imagenet large scale classiﬁcation dataset. methods common fact adopt global strategy weights simultaneously converted low-precision ones turn causes accuracy loss. compared methods focusing pre-trained models accuracy loss becomes worse methods xnor-net dorefa-net intend train low-precision cnns scratch. figure result illustrations. first results iteration proposed three operations. left cube illustrates weight partition operation generating disjoint groups middle image illustrates quantization operation ﬁrst weight group right cube illustrates re-training operation second weight group second results iterations inq. ﬁgure accumulated portion weights quantized undergoes %→%→.%→%. handling strategy suppressing resulting quantization loss model accuracy. partially inspired latest progress network pruning methods accuracy loss removing less important network weights pre-trained neural network model could well compensated following re-training steps. therefore conjecture nature changing network weight importance critical achieve lossless network quantization. base assumption present incorporates three interdependent operations weight partition group-wise quantization re-training. weight partition divide weights layer pre-trained full-precision model disjoint groups play complementary roles inq. weights ﬁrst group responsible forming low-precision base original model thus quantized using equation weights second group adapt compensate loss model accuracy thus ones re-trained. ﬁrst quantization re-training operations ﬁnished three operations conducted second weight group iterative manner weights converted either powers zero acting incremental network quantization accuracy enhancement procedure. result accuracy loss low-precision quantization well suppressed inq. illustrative results iterative steps provided figure denotes ﬁrst weight group needs quantized denotes weight group needs re-trained. leave strategies group partition chosen experiment section. here deﬁne binary matrix help distinguish categories weights. means means network loss regularization term positive coefﬁcient constraint term indicates weight entry chosen consisting ﬁxed number values powers plus zero. direct solving optimization problem training scratch challenging since easy undergo convergence problem. performing weight partition group-wise quantization operations beforehand optimization problem deﬁned reshaped easier version. need optimize following objective function determined group-wise quantization operation binary matrix acts mask determined weight partition operation. since known optimization problem solved using popular stochastic gradient decent method. update scheme re-training positive learning rate. note binary matrix forces zero update weights quantized. weights still keep ﬂoating-point values updated akin latest pruning methods weights currently removed re-trained enhance network accuracy. whole procedure summarized algorithm would like highlight merits three aspects weight partition introduces importance-aware weight quantization. group-wise weight quantization introduces much less accuracy loss simultaneously quantizing network weights thus making retraining larger room recover model accuracy. integrating operations weight partition group-wise quantization re-training nested loop potential obtain lossless low-precision model pre-trained full-precision reference. reset base learning rate learning policy according perform layer-wise weight partition update based quantize weights equation layer-wisely calculate feed-forward loss update weights analyze performance perform extensive experiments imagenet large scale classiﬁcation task known challenging image classiﬁcation benchmark far. imagenet dataset million training images thousand validation images. image annotated object classes. apply alexnet vgg- googlenet resnet- resnet- covering almost known deep architectures. using center crops validation images report results standard measures top- error rate top- error rate. fair comparison pre-trained full-precision models except resnet- taken caffe model zoo. note release pre-trained resnet- model public publicly available re-implementation facebook. since method implemented caffe make open source tool convert pre-trained resnet- model torch caffe. setting expected bit-width ﬁrst experiments performed testify efﬁcacy different architectures. regarding weight partition several candidate strategies tried previous work efﬁcient network pruning found random partition pruning-inspired partition best choices compared others. thus paper directly compare strategies weight partition. random strategy weights layer pre-trained full-precision deep model randomly split disjoint groups. pruning-inspired strategy weights divided disjoint groups comparing absolute values layer-wise thresholds automatically determined given splitting ratio. directly pruning-inspired strategy experimental results section show why. re-training epochs pre-trained full-precision model obtain results shown table concluded -bit models generated show consistently improved top- top- recognition rates compared respective full-precision references. parameter settings described below. alexnet alexnet convolutional layers fully-connected layers. accumulated portions quantized weights iterative steps batch size weight decay momentum vgg- compared alexnet vgg- convolutional layers parameters. accumulated portions quantized weights iterative steps batch size weight decay momentum https//github.com/bvlc/caffe/wiki/model-zoo https//github.com/facebook/fb.resnet.torch/tree/master/pretrained https//github.com/zhanghang/fb-caffe-exts googlenet compared alexnet vgg- googlenet difﬁcult quantize smaller number parameters increased network width. accumulated portions quantized weights iterative steps batch size weight decay momentum resnet- different three networks resnets batch normalization layers relief vanishing gradient problem using shortcut connections. ﬁrst test -layer version exploratory purpose test -layer version later network architectures resnet resnet- similar. difference number ﬁlters every convolutional layer. accumulated portions quantized weights iterative steps batch size weight decay momentum resnet- besides signiﬁcantly increased network depth resnet- complex network architecture comparison resnet-. however regarding network architecture resnet- similar resnet- resnet-. difference number ﬁlters every convolutional layer. accumulated portions quantized weights iterative steps batch size weight decay momentum ﬁrst operation weight partition whose result directly affect following group-wise quantization re-training operations. therefore second experiments conducted analyze candidate strategies weight partition. mentioned previous section pruning-inspired strategy weight partition. unlike random strategy weights equal probability fall disjoint groups pruning-inspired strategy considers weights larger absolute values important smaller ones form low-precision base original model. resnet- test case compare performance strategies. experiments parameter settings completely described section epochs weight re-training. table summarizes results -bit quantization. seen achieves top- error rate top- error rate using random partition. comparatively pruning-inspired partition brings decrease top- top- error rates respectively. apparently pruning-inspired partition better random partition reason paper. future works weight partition based quantization error could also option worth exploring. third experiments performed explore limit expected bit-width still achieve lossless network quantization. similar second experiments also resnet- test case parameter settings batch size weight decay momentum completely same. finally lower-precision models -bit -bit even -bit ternary weights generated comparisons. expected bit-width goes down number candidate quantum values decreased signiﬁcantly thus shall increase number iterative steps accordingly enhancing accuracy ﬁnal low-precision model. speciﬁcally accumulated portions quantized weights iterative steps -bit -bit -bit ternary models respectively. required number epochs also increases expected bit-width goes down reaches training -bit ternary model. although -bit model shows slightly decreased accuracy compared -bit model accuracy still better pre-trained full-precision model. comparatively even expected bit-width goes low-precision model shows losses top- top- recognition rates respectively. -bit ternary model although incurs decrease top- error rate decrease top- error rate comparison pre-trained full-precision reference accuracy considerably better stateof-the-art results reported binary-weight network ternary weight network detailed results summarized table table literature recently proposed deep compression method reports best results network compression without loss model accuracy. therefore last experiments conducted explore potential much better deep compression. note hybrid network compression solution combining three different techniques namely network pruning vector quantization huffman coding. taking alexnet example network pruning gets compression however result mainly obtained fully connected layers. actually compression performance convolutional layers less besides network pruning realized separately performing pruning re-training iterative time-consuming. cost least several weeks compressing alexnet. solved problem dynamic network surgery method achieves speed-up training improves performance network pruning network pruning vector quantization improves compression ratio huffman coding ﬁnally boosts compression ratio fair comparison combine proposed compare resulting method detailed results summarized table combing proposed achieve much better compression results compared speciﬁcally -bit quantization achieve compression slightly larger gains top- top- recognition rates yielding .%/.% absolute improvement compression performance compared full version/fair version respectively. consistently better results also obtained -bit -bit models. besides also perform experiments alexnet compare performance vector quantization fair comparison re-training also used enhance performance vector quantization number cluster centers convolutional layers fully connect layers experiment vector quantization incurs loss model accuracy. change number cluster centers convolutional layers gets accuracy loss consistent results reported comparatively vector quantization mainly proposed table comparison combination deep compression method alexnet. conv convolutional layer fully connected layer pruning quantization huffman coding. compress parameters fully connected layers pre-trained full-precision model addresses network layers simultaneously accuracy loss -bit -bit quantization. therefore evident much better vector quantization. last least ﬁnal weights vector quantization network pruning deep compression still ﬂoating-point values ﬁnal weights form either powers zero. direct advantage original ﬂoating-point multiplication operations replaced cheaper binary shift operations dedicated hardware like fpga. paper present network quantization method address problem convert pre-trained full-precision model lossless lowprecision version whose weights constrained either powers zero. unlike existing methods usually quantize network weights simultaneously compact quantization framework. incorporates three interdependent operations weight partition groupwise quantization re-training. weight partition splits weights layer pre-trained full-precision model disjoint groups play complementary roles inq. weights ﬁrst group directly quantized variable-length encoding method forming low-precision base original model. weights group re-trained keeping quantized weights ﬁxed compensating accuracy loss network quantization. importantly operations weight partition group-wise quantization re-training repeated latest re-trained weight group iterative manner weights quantized acting incremental network quantization accuracy enhancement procedure. imagenet large scale classiﬁcation task conduct extensive experiments show quantized models -bit -bit -bit even -bit ternary weights improved least comparable accuracy full-precision baselines including alexnet vgg- googlenet resnets. future works plan extend incremental idea behind low-precision weights low-precision activations low-precision gradients also investigate computation power efﬁciency implementing low-precision models hardware platforms. matthieu courbariaux itay hubara daniel soudry el-yaniv yoshua bengio. binarized neural networks training deep neural networks weights activations constrained arxiv preprint arxiv.v itay hubara matthieu courbariaux daniel soudry el-yaniv yoshua bengio. quantized neural networks training neural networks precision weights activations. arxiv preprint arxiv.v mohammad rastegari vicente ordonez joseph redmon farhadi. xnor-net imagenet classiﬁcation using binary convolutional neural networks. arxiv preprint arxiv.v christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. cvpr shuchang zhou zekun xinyu zhou yuxin yuheng zou. dorefa-net training bitwidth convolutional neural networks bitwidth gradients. arxiv preprint arxiv.v taking -bit alexnet model example analyze distribution quantized weights. detailed statistical results summarized table convolutional layers values occupy quantized weights respectively; distributions quantized weights convolutional layers similar convolutional layer weights quantized zero convolutional layers compared convolutional layer; fully connected layer values occupy quantized weights similar results seen fully connected layer; generally distributions quantized weights convolutional layers usually scattered compared fully connected layers. partially reason much easier good compression performance fully connected layers comparison convolutional layers using methods network hashing vector quantization -bit alexnet model required bit-width layer actually recently made good progress developing lossless cnns low-precision weights low-precision activations. according results summarized table seen vgg- model -bit weights -bit activations shows improved top- top- recognition rates comparison pre-trained reference -bit ﬂoating-point weights -bit ﬂoating-point activations. best knowledge best results reported vgg- architecture far.", "year": 2017}