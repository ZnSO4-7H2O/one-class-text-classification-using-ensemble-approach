{"title": "Market-Based Reinforcement Learning in Partially Observable Worlds", "tag": ["cs.AI", "cs.LG", "cs.MA", "cs.NE", "I.2"], "abstract": "Unlike traditional reinforcement learning (RL), market-based RL is in principle applicable to worlds described by partially observable Markov Decision Processes (POMDPs), where an agent needs to learn short-term memories of relevant previous events in order to execute optimal actions. Most previous work, however, has focused on reactive settings (MDPs) instead of POMDPs. Here we reimplement a recent approach to market-based RL and for the first time evaluate it in a toy POMDP setting.", "text": "abstract. unlike traditional reinforcement learning market-based principle applicable worlds described partially observable markov decision processes agent needs learn short-term memories relevant previous events order execute optimal actions. previous work however focused reactive settings instead pomdps. reimplement recent approach market-based ﬁrst time evaluate pomdp setting. major reason importance methods learn sequential event-memorizing algorithms this sensory information usually insuﬃcient infer environment’s state complicates goal-directed behavior. instance suppose instructions station follow road traﬃc light turn left follow road next traﬃc light turn right are. suppose reach traﬃc lights. right thing need know whether ﬁrst second. requires least memory current environmental input suﬃcient. learning algorithm supposed learn algorithms store relevant bits information training examples tell whether current attempt successful typical reinforcement learning situation face major temporal credit assignment problem many past inputs relevant not? traditional work requires markovian interfaces environment current input must provide information probabilities possible inputs observed next action e.g. approaches however essentially learn react response given input cannot learn identify memorize important past events complex partially observable settings introductory example above. several recent non-standard approaches principle able deal partially observable environments learn memorize certain types relevant events none them however represents satisfactory solution general problem learning worlds described partially observable markov decision processes approaches either work restricted domains only suﬀer problems concerning state space exploration versus exploitation non-optimal learning rate limited generalization capabilities. problems overcome model computationally intractable. consider novel approach pomdprl called market-based principle suﬀer limitations traditional ﬁrst give overview approach history evaluate pomdp setting discuss potential limitations. classiﬁer systems. probably ﬁrst market-based approach embodied holland’s classiﬁer systems bucket brigade algorithm messages form bitstrings size placed global message list either environment entities called classiﬁers. classiﬁer consists condition part action part deﬁning message might send message list. parts strings serves ‘don’t care’ appears condition part. non-negative real number associated classiﬁer indicating ‘strength’. cycle messages message list compared condition parts classiﬁers system. matching classiﬁer computes ‘bid’ based strength. highest bidding classiﬁers place message message list next cycle distributed among classiﬁers active last time step triggering conditions certain messages result action within environment actions regarded ’useful’ external critic give payoﬀ increasing strengths currently active classiﬁers learning take place. central idea classiﬁers active environment gives payoﬀ important role setting stage directly rewarded classiﬁers earn credit participating ‘bucket brigade chains’. success active classiﬁer recursively depends success classiﬁers active following time ticks. bankrupt classiﬁers removed replaced freshly generated ones endowed initial amount money. psalms. holland’s approach suﬀers certain drawbacks though. instance credit conservation money generated nothing. pages devoted general approach called prototypical self-referential associating learning mechanisms competing/cooperating agents executing actions. winners receive external reward achieving goals. agents supposed learn credit assignment process purpose execute actions collectively constructing connecting modifying agents transferring credit agents. psalms ﬁrst systems enforce important constraint total credit conservation constraint enforced holland’s classiﬁer economy cause money inﬂation problems. psalms also inspired money-conserving neural bucket brigade money weight substance reinforcement learning neural hayek machines designed avoid loopholes holland’s credit assignment process allow agent proﬁt actions beneﬁcial system whole. property rights agents strictly enforced. current owner right world computes minimal price right sells highest bidder. agents create children invest part money them proﬁt children’s success. wealth agent separate quantities. makes hayek stable original bucket brigade. impressive applications hayek learned solve complex blocks world problems hayek reminiscent psalm crucial diﬀerence psalm hayek psalm strictly enforce individual property rights. instance agents steal money agents temporally contribute system’s overall progress. economic model. hayek consists collection agents rule possible action wealth numerical bid. agents required solve instance complex task. agent perform single action need cooperate. single instance computation proceeds series auctions. highest bidding agent wins owns world pays amount equal previous owner perform action world. owner world collects reward plus payment next owner. actual revenue larger agent increase wealth. birth death taxes. agents allowed create children wealth larger predetermined number parents endorse oﬀspring minimum amount money subtracted wealth return parents receive share children’s future proﬁt. children’s rules randomly created copied mutated parents probabilities respectively. table numerical values. deﬁnition wealth birth processes automatically focus rather successful agents. improve eﬃciency instance agents small amount proportional amount computation time used. remove unuseful agents earlier hayek versions required agents small ﬁxed amount tax; agents without money removed system. hayek removes inactive last instances. baum durdanovic’s parameter reward value children always amount higher highest increase level instances solved; decrease less execution copyright share mutation ratio post production system. agents hayek essentially post production system. baum durdanovic argue provides richer representation s-expressions earlier hayek versions post systems turing complete agent forms post rule form antecedent consequent state encoded string agent allowed perform matches string procedure iterated rules match computation halts. found hayek rather diﬃcult reproduce. several parameters require tuning making hayek stable still seems remain art. parameter settings hayek system summarized table detailed explanation small task sizes program tends reliable solutions larger sizes cause stability problems prevent hayek ﬁnding solution within several days. tried reimplement hayek closely possible except certain deviations justiﬁed improved performance implementation issues blockworld. baum durdanovic reported hayek agents collectively form universal solver arbitrary blockworld problems involving arbitrarily high stacks blocks. consists rules listed table below. tested hayek ﬁrst hardwiring system consisting socalled universal agents only. indeed reliably generated stacks increasing size reaching stack level within instances level within instances. figure shows typical resulting stack height=. fig. example blockworld instance stack height=. objective replicate stack color sequence represented blocks stack moving blocks stacks optimal next move would move block stack stack fig. typical blockworld solution stack height=. colon-separated ﬁeld world string represents stack blocks colors {abc}. agent’s rule matches world string; represents don’t care symbol; numbers rule encode replacements matched substrings stack actions move blocks between stacks switched creation children kept mutation turned caused hayek progress much slower pace probably children disrupt universal solver always auctions using ǫ-bid scheme. problems. unfortunately system able universal solver itself. typically reached stack size failed progress further. believe increasing task size system encountering stability problems. much remains done here. unlike traditional market-based principle applicable pomdps. previous work however usually focused reactive settings instead pomdps notable exception though). memory register hayek machine apply pomdps. memory register acts additional environmental variable manipulated agent using write actions. test problem. study hayek’s performance pomdps focus simplest possible pomdp problems. figure shows example adapted version woods standard test case non-markov search problems. agents start either positions reach food agents limited perception four immediate neighbouring positions north east south west plus value memory register. without memory markov solution problem aliased positions identical environmental input optimal agent needs execute diﬀerent actions optimal action east; optimal action west. note starting positions also aliased. however fig. woods partially observable environment; grey positions represent walls. left agent starts positions reach food aliased positions -bit memory register available writing/reading. right agent senses four neigbouring positions value memory register; here example agent might either table policies starting position auctions proceed ﬁrst line downwards. left memory going north reset aliased position reached. right memory initially memory agent ﬁrst four ﬁelds rule represent values north east south west respectively; ﬁfth ﬁeld matches value memory register. star variables represent don’t care symbols. results. hayek test problem. instances learned policy solved pomdp problem using -bit memory. early instances number agents grew ﬁnally agents remained. bids agents converged actual reward value wealth values varied considerably. sample histories solved instances starting shown table histories solved instances starting shown table results summarized follows. instance starts hayek assures memory reaches aliased position memory initially write agent sets memory memory already nothing done. agent starts right opposite occurs hayek reaches aliased position checks memory properly diﬀerent agents identical perception environment diﬀerent perception memory aliased positions east agent memory west agent memory hayek also evolves agent executing action south sequence ends disregards value memory using ‘*’-symbol don’t care symbols useful prevent unnecessary over-specialization i.e. need separate agents memory value. fact over-specialization occurred starting agent going north table policies starting position auctions proceed ﬁrst line downwards. left memory initially agent proceeds directly north right memory initially memory agent sets proceeding north. discussion. simple test problem shows hayek principle able solve pomdps. precisely using memory register hayek de-aliases aliased positions pomdp tagging situations actually occur. note solution found hayek unique matter whether chooses write left path long right path uses complement. policy seems perform well hayek converges toward local optimum sticks note found solution nearly exactly optimal; extra mmemory agent necessary left policy table hayek able generalize optimally case agent going north failing don’t care symbol memory value. started evaluate market-based pomdp settings focusing hayek machine vehicle learning memorize relevant events short-term memory. using memory register hayek able distinguish aliased positions pomdp evolve stable solution. approach promising much remains done make scalable.", "year": 2001}