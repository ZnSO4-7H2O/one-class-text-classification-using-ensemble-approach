{"title": "Max-Pooling Dropout for Regularization of Convolutional Neural Networks", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.", "text": "abstract. recently dropout seen increasing deep learning. deep convolutional neural networks dropout known work well fully-connected layers. however effect pooling layers still clear. paper demonstrates max-pooling dropout equivalent randomly picking activation based multinomial distribution training time. light insight advocate employing proposed probabilistic weighted pooling instead commonly used max-pooling model averaging test time. empirical evidence validates superiority probabilistic weighted pooling. also compare max-pooling dropout stochastic pooling introduce stochasticity based multinomial distributions pooling stage. deep convolutional neural networks recently substantially improving state computer vision. standard consists alternating convolutional pooling layers fully-connected layers top. compared regular feed-forward networks similarly-sized layers cnns much fewer connections parameters local-connectivity shared-filter architecture convolutional layers less prone over-fitting. another nice property cnns pooling operation provides form translation invariance thus benefits generalization. despite attractive qualities despite fact cnns much easier train regular deep feed-forward neural networks cnns millions billions parameters still easily overfit relatively small training data. dropout recently proposed regularizer fight over-fitting. regularization method stochastically sets zero activations hidden units training case training time. breaks co-adaptions feature detectors since dropped-out units cannot influence retained units. another interpret dropout yields efficient form model averaging number trained models exponential units models share parameters. dropout also inspired stochastic model averaging methods stochastic pooling drop-connect maxout networks neural nets effect pooling layers however well studied. paper shows using max-pooling dropout training time equivalent sampling activation based multinomial distribution distribution tunable parameter light this probabilistic weighted pooling proposed employed test time efficiently average possibly max-pooling dropout trained networks. empirical evidence confirms superiority probabilistic weighted pooling max-pooling. like fully-connected dropout number possible max-pooling dropout models also grows exponentially increase number hidden units pooling layers decreases increase pooling regionâ€™s size. stochastic pooling max-pooling dropout randomly sample activation based multinomial distributions pooling stage becomes interesting compare performance. experimental results show stochastic pooling performs max-pooling dropout different retaining probabilities max-pooling dropout typical retaining probabilities often outperforms stochastic pooling large margins. paper dropout input max-pooling layers also called max-pooling dropout brevity. similarly dropout input fully-connected layers called fully-connected dropout. dropout regularization technique recently employed deep learning. pioneering work hinton applied dropout fully connected layers. reason provided convolutional shared-filter architecture drastic reduction number parameters thus reduced possibility overfit convolutional layers. krizhevsky trained convolutional neural classify million imagenet images. primary methods used reduce over-fitting experiments. first data augmentation easiest commonly used approach reduce over-fitting image data. dropout exactly second one. also used fully-connected layers. stochastic pooling dropout-inspired regularization method. instead always capturing strongest activity within pooling region max-pooling does stochastic pooling randomly picks activations according multinomial distribution. maxout network another model inspired dropout. combining dropout maxout networks shown achieve best results five benchmark datasets. however authors train maxout networks without dropout. besides train rectified counterparts dropout directly compare maxout networks. dropout also motivated stochastic model averaging methods drop-connect adaptive dropout consider standard composed alternating convolutional pooling layers fully-connected layers top. presentation training example layer followed pooling layer forward propagation without dropout described pooling region layer activity neuron within pool denotes pooling function. pooling number units operation provides form spatial transformation invariance well reduces computational complexity upper layers. ideal pooling method expected preserve task-related information discarding irrelevant image details. popular choices averagemax-pooling. average-pooling takes activations pooling region consideration equal contributions. downplay high activations many activations averagely included. max-pooling captures strongest activation disregards units pooling region. show employing dropout max-pooling layers avoids disadvantages introducing stochasticity. denotes element wise product drawn independently bernoulli distribution. mask multiplied activations modified activations passed pooling layers. fig. presents concrete example illustrate effect dropout max-pooling layers. clearly without illustrating example showing procedure max-pooling dropout. activation pooling region respectively. without dropout strongest activation always selected output. dropout unit pooling region could possibly dropped out. example retained pooled output. dropout strongest activation pooling regions always selected pooled activation. dropout necessary strongest activation output. therefore max-pooling training time becomes stochastic procedure. forpooling mulate stochasticity suppose activations region reordered non-decreasing order i.e. dropout unit pooling region could possibly zero probability dropout probability retaining probability). result special event occurring probability units pooling region dropped pooled output becomes therefore performing max-pooling dropout-modified pooling region exactly sampling following multinomial distribution select index pooled activation simply size feature layer size pooling regions. number pooling region therefore rs/t non-overlapping pooling. pooling region provides choices indices number possibly trained models layer number possibly max-pooling dropout trained models exponential number units pooling max-pooling layers base depends size pooling regions. obviously increase size pooling regions base decreases number possibly trained models becomes smaller. note number possibly fully-connected dropout trained models also exponential number units fullyconnected layers base. using dropout fully-connected layers training whole network containing hidden units used test time outgoing weights halved compensate fact twice many active activations halved. using max-pooling dropout training might intuitively pick output strongest activation multiplied retaining probability test time scaled max-pooling generally works well practice optimal. instead propose probabilistic weighted pooling efficiently accurate approximation averaging possibly trained dropout networks. pooling scheme pooled activity linear weighted summation activations region exactly probability calculated eqn. type probabilistic weighted summation interpreted efficient form model averaging selection index corresponds different model. empirical evidence confirm probabilistic weighted pooling accurate approximation averaging possible dropout models scaled max-pooling. experiments conducted three datasets mnist cifar- cifar-. mnist consists pixel grayscale images containing digit training test examples. perform preprocessing except scaling pixel values cifar- dataset consists classes natural images examples training testing. example image taken tiny images dataset collected web. cifar- like cifar- categories. also scale cifar- cifar- subtract mean value channel computed dataset image. rectified linear function convolutional fully-connected layers softmax activation function output layer. commonly used sigmoidal tanh nonlinearities adopted gradient vanishing problem them. models trained using stochastic mini-batch gradient descent batch size momentum learning rate minimize cross entropy loss. weights layers initialized zero-mean gaussian distribution standard deviation constant neuron biases layers. architecture mnist xx-c-p-c-p-n-n represents input image size convolutional layer feature maps filters pooling layer pooling region stride convolutional layer feature maps filters pooling layer pooling region stride fully-connected layer hidden units output layer units architecture cifar- xx-cp-c-p-c-p-n-n-n. architecture cifar- cifar- except output units. scaled max-pooling using mnist. cnns trained epochs. max-pooling dropout models trained different retaining probabilities. fig. compares test performances produced different pooling methods test time. generally probabilistic weighted pooling performs better max-pooling scaled max-pooling different retaining probabilities. small max-pooling scaled max-pooling performs poorly; probabilistic weighted pooling considerably better. increase performance becomes smaller. surprising pooled outputs different pooling methods close large extreme case scaled maxpooling probabilistic weighted pooling exactly max-pooling. compares different pooling methods test time max-pooling dropout trained models cifar- cifar-. retaining probability respectively. test time max-pooling scaled max-pooling probabilfig. cifar- cifar- test errors different pooling methods test time. maxpooling dropout used train cnns different retaining probabilities training time. istic weighted pooling respectively used model averaging. fig. presents test performance pooling methods. again small retaining probability scaled max-pooling probabilistic weighted pooling perform poorly. probabilistic weighted pooling best performer different retaining probabilities. increase narrows different pooling methodsâ€™ performance gap. similar max-pooling dropout stochastic pooling also randomly picks activation according multinomial distribution training time also involves probabilistic weighting test time. concretely training time first computes probability unit within pooling region layer normalizing activations found stochastic pooling bears much resemblance max-pooling dropout involve stochasticity pooling stage. therefore interested performance differences. compare performances train models different retaining probabilities mnist cifar- cifar-. maxpooling dropout trained models probabilistic weighted pooling used test time. fig. compares test performances max-pooling dropout different retaining probabilities stochastic pooling. relation performance maxpooling dropout retaining probability u-shape. small large max-pooling dropout performs poorer stochastic pooling. max-pooling dropout typical outperforms stochastic pooling large margin. therefore although stochastic pooling hyper-parameter free saves tuning retaining probability performance often inferior max-pooling dropout. paper mainly addresses problem understanding using dropout input max-pooling layers convolutional neural nets. training time max-pooling dropout equivalent randomly picking activation according multinomial distribution number possibly trained networks exponential number input units pooling layers. test time pooling method probabilistic weighted pooling proposed model averaging. experimental evidence confirms benefits using max-pooling dropout validates superiority probabilistic weighted pooling max-pooling scaled max-pooling. considering stochastic pooling similar max-pooling dropout empirically compare show performance stochastic pooling produced maxpooling dropout different retaining probabilities.", "year": 2015}