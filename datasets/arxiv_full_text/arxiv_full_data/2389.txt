{"title": "MOB-ESP and other Improvements in Probability Estimation", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "A key prerequisite to optimal reasoning under uncertainty in intelligent systems is to start with good class probability estimates. This paper improves on the current best probability estimation trees (Bagged-PETs) and also presents a new ensemble-based algorithm (MOB-ESP). Comparisons are made using several benchmark datasets and multiple metrics. These experiments show that MOB-ESP outputs significantly more accurate class probabilities than either the baseline BPETs algorithm or the enhanced version presented here (EB-PETs). These results are based on metrics closely associated with the average accuracy of the predictions. MOB-ESP also provides much better probability rankings than B-PETs. The paper further suggests how these estimation techniques can be applied in concert with a broader category of classifiers.", "text": "prerequisite optimal reasoning uncertainty intelligent systems start good class probability estimates. paper improves current best probability estimation trees also presents ensemble-based algorithm comparisons made using several benchmark datasets multiple metrics. experiments show mob-esp outputs signiﬁcantly accurate class probabilities either baseline bpets algorithm enhanced version presented results based metrics closely associated average accuracy predictions. mob-esp also provides much better probability rankings b-pets. paper suggests estimation techniques applied concert broader category classiﬁers. standard form supervised machine learning involves inducing classiﬁer labeled training examples vector composed several predictive attributes class label predicted. goal induce classiﬁer minimizes expected error rate resulting zero-one loss predicted label true label many circumstances enough simply predicted class label would also like prediction conditional probability class henceforth written simply consider medical domain certainty diagnosis directly effects whether additional tests possibly invasive tests carried treatment protocol followed. making cost-sensitive decisions also important understand likelihood possible outcome cost beneﬁt associated acting various class predictions might signiﬁcantly different high-quality decision making requires good class probability estimates. focus paper probabilistic classiﬁcation task. speciﬁcally analyzes probability estimation techniques based ensemble decision tree classiﬁers. frequently done estimating class probabilities leaf tree based proportion training examples class reach leaf nk/n total number training examples reach leaf number examples class predictors called probability estimation trees margineantu dietterich point three problems approach. first since estimates based training data used build tree tend close extremes second additional inaccuracies result small number examples often reach leaf. third technique assigns probability entire region space deﬁned given leaf without regard true probability density function across region. generally estimates smoothed mitigate problems caused examples reaching many leaves simplest form smoothing laplace smoothing probability estimate given leaf calculated goal paper develop algorithm performs extremely well average prediction accuracy. therefore beneﬁts single trees disregarded favor potential accuracy improvements provided ensembles. additional goals include high accuracy probability rankings relatively computational costs classiﬁcation time reasonable performance small datasets. reasons b-pets rather b-lots implemented comparison purposes paper. algorithms presented enhanced version b-pets algorithm uses entirely different strategy predict conditional class probabilities work builds directly work breiman margineantu dietterich provost domingos though work many others inﬂuential. algorithms evaluated using benchmark datasets multiple evaluation metrics used extensively literature. experiments show eb-pets mob-esp signiﬁcantly outperform baseline b-pets algorithm metrics. also show mob-esp performs much better eb-pets regard average accuracy class probability predictions. section describe algorithms evaluated paper including b-pets eb-pets mob-esp. section presents experiments including subsections design datasets evaluation metrics results discussion. concluding remarks provided section baseline comparison bagged pets implemented described basic algorithm pruning collapsing disabled laplace smoothing describe equation estimate class probabilities leaves. turning pruning tree grown purity must contain least examples leaf. bagging added complete b-pets ﬁnal probability estimate class calculated taking average estimates output trees ensemble margineantu dietterich point that smoothing addresses issue small leaves bagging combined pruning addresses issue regionwide probability estimation b-pets address probabilities also calculated using ensemble forecasting techniques class probability estimate determined percentage deterministic classiﬁers ensemble vote class tk/t total number classiﬁers ensemble number classiﬁers output ˆy=k). work reasonably well technique requires underlying classiﬁers independent ratio classiﬁers classes relatively large. example suppose even likely several classes assigned zero probability. pets smoothing could used mitigate problem. bagging improves smoothed estimates pets. sense technique combines ideas pets ensemble forecasting. baggedpets estimates output classiﬁer ensemble averaged provide ﬁnal prediction bagging seen partially addressing problem region-wide probability estimates unlabeled examples unlikely classiﬁed leaf every tree will therefore generally different ﬁnal probability estimates. margineantu dietterich lessen region-wide estimation problem improve probability estimates using b-lots modiﬁcation includes lazy decision tree learning options lazy decision trees built single branch time classifying unlabeled example branch constructed based partly relatively speciﬁc therefore probability estimate speciﬁc point rather broader region includes somewithin boundaries. option trees involve multiple split decisions node ﬁnal probabilities computed averaging results paths. increases diversity ensemble improves subsequent probability estimates. pointed margineantu dietterich b-lots require computational resources classiﬁcation therefore inappropriate many situations. furthermore blots provide accurate probability estimates provide nearly good probability rankings b-pets. hypothesis including examples further mitigate problems pets described previously. first alleviate problems basing probabilities strictly examples used construct tree. approximately examples involved constructing tree second including examples increases size leaves. effectively equation increased best knowledge technique implemented prior work. various prior work shown unsmoothed estimates circumstantially achieve better accuracy. example margineantu dietterich smoothing b-lots options aspect algorithm implicitly provided smoothing. wondering whether true bagging alone version algorithm implemented without smoothing additionally hypothesized increase probability estimation points including examples diversity injected random feature selection would decrease need explicit smoothing. last algorithm enhancement incorporates random feature selection tree building process. node tree random subset attributes considered making split. speciﬁcally examine attributes according following formula sense random feature selection addressing issues options margineantu dietterich indicate advantage options splits information gain almost high best test created guaranteed decision trees bagged trees. leads diversity better probability estimates. random feature selection problems caused basing estimates training data chieﬂy estimates tend biased toward extremes attempted address problem calculating based out-of-bag examples found examples generate meaningful estimates. training data issue directly addressed mob-esp described section algorithm differs implementation provost domingos minor ways. first rather follow standard bootstrap selection procedure inbag examples drawn according training priors. normally examples used build bagged classiﬁer bootstrap replicate full training dataset; training examples drawn uniformly replacement size full training here class class examples chosen number class examples full training set. second attributes result information gain branching decision determined uniform random selection among attributes. b-pets enhanced three ways form eb-pets. enhancement described implemented separately allow individual analysis. first examples included estimation probabilities. second laplace smoothing eliminated b-pets. third random feature selection incorporated tree construction process. enhancements detailed successively following three sections. ﬁrst improvement made b-pets include examples probability estimation process. examples stochastically left bootstrap replicate generation therefore used construct tree constructing tree examples classiﬁed tree binned leaves reach. laplace smoothed probability estimates tree calculated follows hypothesis technique lessen problems described earlier. first provide additional implicit smoothing injecting randomness makeup leaves. mitigate problems caused small leaves. second diversity also address issue region-wide probability estimates since becomes even less likely regions input space covered leaves different members ensemble overlap greatly. speciﬁcally less probable unlabeled examples reach classiﬁed exact leaves across entire ensemble. several previous modiﬁcations attempt alleviate problems caused using data points build tree estimate probabilities. algorithm described here mean out-of-bag example-speciﬁc probability estimator explicitly addresses issue. using true predicted class examples estimate conditional class probabilities unlabeled examples fall region input space. constructing mob-esp three step process. first bagged tree classiﬁer random feature selection constructed described previous sections. tree’s construction in-bag examples compose leaf binned leaf. additionally examples passed tree construction examples classiﬁed leaf abilities examples different classiﬁcations. motivation hypothesis that given group examples falling leaf tree examples ensemble classiﬁcation likely contextually similar regions space hypothesis tested empirically found true benchmark datasets estimated class probabilities given ensemble classiﬁcation ˆy=j computed according true class frequencies training examples binned leaf classiﬁcation words compute class probability estimates point classiﬁcation binned training examples processed follows. filter sets include points whose classiﬁcation i.e. compute total number training examples number examples whose parallel deﬁnitions true class based training examples weighting factor increase role examples result process matrix predicted class probabilities conditioned possible classiﬁcations. estimate class probabilities unlabeled example point ﬁrst classiﬁed ensemble using majority voting scheme bagging obtain f=ˆyu. leaf involved classiﬁcation conditional class probability retrieved leaf’s probability matrix. finally ensemble probability prediction class computed averaging estimates trees second trees ensemble constructed entire training dataset classiﬁed trees using standard majority voting scheme bagging. results classiﬁcations {hxi ˆyii}n training point trees trees example. average training example example trees number trees ensemble since classiﬁcations based trees unbiased sense examples used construction trees employed classify therefore classiﬁcation performance almost indistinguishable unlabeled test examples. classiﬁcations used subsequently estimate class probabilities similarly classiﬁed test examples. ﬁnal step mob-esp construction assign probabilities leaf every tree ensemble. probabilities conditioned ensemble classiﬁcation point rationale points classiﬁed similarly similar true class prob. first mob-esp signiﬁcantly mitigates problems might caused basing probability estimates data used construct trees. result factors examples used calculate estimates used construction tree; even examples used construction estimates determined strictly training class labels also tree ensemble classiﬁcations effectively nontraining classiﬁcations. second problem small leaf sizes mitigated eb-pets factors increase including examples implicit smoothing resulting bagging random feature selection. third problem region-wide estimation alleviated bagging random feature selection conditioning estimates ensemble classiﬁcations. experiments compare algorithm using benchmark datasets indicated table hundred trials conducted dataset holding random examples test. trial ensemble consisted trees. resulted total ensembles total trees utilized algorithm. formulas throughout paper written general k-class scenario current implementation restricted -class case. multi-class datasets predictors constructed using classes indicated dataset name column. analyze results based multiple metrics variety reasons. consensus among researchers metric best; different metrics strengths weaknesses presenting multiple metrics allows others easily compare results. ﬁrst metrics closely linked average error predictions third extent fourth associated implementation explicitly accommodate unknown feature values nominal values. therefore datasets chosen minimize associated problems. dataset examples missing attribute values removed. audiology attributes bone bser removed large number missing values. nominal attribute values converted integers accordance fairly natural progression exact datasets available upon request. third metric area lift chart lift computed ﬁrst sorting predictions class descending order. proportion examples highest probability predictions lift aulc used rather area curve aulc employed widely commercial probability estimation fails distinguish between properly ranked estimates estimates properly ranked error class aulc calculated numeric integration often done increments implementation point-by-point across uniquely ranked probability estimates. typically class scenario group higher value associated identiﬁcation datasets here clear class aulck computed class aulc calculated weighting values associated prior class probabilities according training set. table shows combined effect enhancements described section relative baseline b-pets probability estimations. column after metric indicates whether enhancements significantly improved associated metric hurt metric signiﬁcant effect signiﬁcance determined t-test level conﬁdence enhancements result wins ties losses according -mse metric using datasets. average log-loss metric wins/ties/losses area lift chart gives change accuracy scores clearly combined effect enhancements positive overall comparison indicate enhancements biggest positive impact metrics. help understand effect enhancement baseline b-pets results compared results based individual enhancements. summary win/tie/loss results shown table along effect incorporating enhancements. including examples calculation probabilities seems biggest overall positive impact baseline. enhancement that itself improves metrics. additionally results largest improvement average probability estimates measured /-mse avll. omitting smoothing also appears positive effect average probability estimates obvious effect aulc ∆acc. random feature selection hand large positive effect aulc ∆acc obvious effect average prediction error. results table address interaction between enhancements. analyze issues comparison made eb-pets results results obtained removing enhancement individually. table summarizes win/tie/loss results removing enhancement full eb-pets. case higher number losses implies enhancement positive effect ﬁnal algorithm. results show substantially different effects enhancements combined taken individually. however enhancements largely result losses wins across metrics main exception single aulc metric examples enhancement. ﬁnal algorithm omission smoothing clearly positive impact average prediction error enhancement. random feature selection real impact average prediction error itself shows small positive impact combined algorithm still large positive impact aulc ∆acc. hypotheses behind algorithm enhancements seem supported. increase examples used estimating probabilities including points signiﬁcant positive effect baseline though less ﬁnal combined method. appears bagging provide enough diversity justify elimination smoothing leaf estimates. furthermore removal smoothing seems play extremely important role performance ﬁnal predictor. surprising given radical change estimates result typical laplace smoothing. however results imply smoothing technique outperform unsmoothed estimates. fact likely methods found alternatively ferri also imply pruning might beneﬁcial case since smoothing disabled. finally diversity injected random feature selection also appears contribute signiﬁcantly quality ﬁnal predictor. second experiment mob-esp compared baseline b-pets algorithm eb-pets. table summarizes win/tie/loss results comparisons table presents full detail relative eb-pets metric dataset. seen mob-esp signiﬁcantly outperforms baseline b-pets algorithm metrics datasets. also performs much better ebpets terms average prediction accuracy measured /-mse avll. eb-pets hand better aulc. again hypotheses behind mob-esp appear largely validated. basing estimates classiﬁcation technique conditioning ensemble classiﬁcation provides much larger increases average accuracy measures achieved eb-pets. however surprising eb-pets much better aulc metric topic future investigation. noted earlier classiﬁcation based trees. given experiments unlikely datasets enough trees reach peak classiﬁcation performance. also since equal number trees involved ensemble classiﬁcation unlabeled examples almost certainly differences classiﬁcation performance examples unlabeled examples. implies better results might achievable constructing trees using classify unlabeled examples. experiments equation seems probable examples better predictors class probabilities examples since used construction trees. would especially true sufﬁcient number examples. automatically choosing appropriate value accommodate hypothesis area future research. mob-esp estimation method applied concert broader category classiﬁers. prerequisite classiﬁer provide association unlabeled example subset training examples primary responsibility classiﬁcation combining bagging random feature selection shown improve ensemble tree classiﬁcation accuracy models output example-speciﬁc class probability estimates bpets hand explicitly compute class probability estimates good rankings however margineantu dietterich others demonstrated b-pets provide particularly accurate estimates. eb-pets introduced improve estimates. experiments benchmark datasets show ebpets provide much accurate average predictions according /-mse avll metrics b-pets. experiments also suggest eb-pets provide betpaper also introduced mob-esp technique calculates probability estimates based examples used construction trees. experiments show method performs signiﬁcantly better either baseline b-pets algorithm enhanced eb-pets algorithm regard estimating probabilities. experiments also suggest probability rankings mobesp better b-pets. work required determine whether mob-esp modiﬁed perform well eb-pets area lift chart metric. objective calculate accurate probability estimates results strongly suggest mob-esp analyzed relevant domain. often case medical diagnosis weather forecasting economic predictions cost-sensitive decision making general.", "year": 2012}