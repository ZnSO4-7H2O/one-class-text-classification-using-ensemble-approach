{"title": "Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "We present an approach to learning multi-sense word embeddings relying both on monolingual and bilingual information. Our model consists of an encoder, which uses monolingual and bilingual context (i.e. a parallel sentence) to choose a sense for a given word, and a decoder which predicts context words based on the chosen sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time.", "text": "present approach learning multi-sense word embeddings relying monolingual bilingual information. model consists encoder uses monolingual bilingual context choose sense given word decoder predicts context words based chosen sense. components estimated jointly. observe word representations induced bilingual data outperform monolingual counterparts across range evaluation tasks even though crosslingual information available test time. approaches learning word embeddings relying word context received much attention recent years induced representations shown capture syntactic semantic properties words. evaluated intrinsically also used concrete applications deal word sparsity improve generalization work date focused developing embedding models represent word single vector researchers attempted capture polysemy explicitly encoded properties word multiple vectors parallel work multi-sense word embeddings another line research investigated integrating multilingual data largely distinct goals mind. ﬁrst goal obtain representations several languages semantic space enables transfer model trained annotated training data language another language lacking annotation secondly information another language also leveraged yield better ﬁrstlanguage embeddings paper falls latter much less explored category. adview multilingual learning means language grounding intuitively polysemy language least partially resolved looking translation word context another language better sense assignment lead better sense-speciﬁc word embeddings. propose model uses second-language embeddings supervisory signal learning multisense representations ﬁrst language. supervision easy obtain many language pairs numerous parallel corpora exist nowadays. model seen autoencoder discrete hidden layer encoding word senses leverages bilingual data encoding part decoder predicts surrounding words relying autoencoders trained reproduce input ﬁrst mapping input hidden layer predicting approximation input relying hidden layer. case hidden layer real-valued vector categorical variable encoding sense word. discrete-state autoencoders successful several natural language processing applications including tagging word alignment semantic role induction relation discovery formally model consists components encoding part assigns sense pivot word reconstruction part recovering context words based pivot word sense. predictions probabilistic reconstruction step involves summation potential word senses. goal embedding parameters minimize error recovering context words based pivot word sense assignment. parameters encoding reconstruction jointly optimized. intuitively good sense assignment make reconstruction step easy possible. encoder uses words ﬁrst-language sentence choose sense also training time conditioning decisions words second-language sentence. hypothesize injection crosslingual information guide learning towards inducing informative sense-speciﬁc word representations. consequently using information training time would beneﬁt model even though crosslingual information available encoder test time. choose sense word context words ﬁrst language well context words second language context deﬁned also considered formulation included sense-speciﬁc bias bxis capture relative frequency latent senses seem affect performance. ﬁndings second-language signal effectively improves quality multi-sense embeddings seen variety intrinsic tasks english results superior baseline skip-gram model even though crosslingual information available test time. extrinsic pos-tagging task secondlanguage signal also offers improvements monolingually-trained multi-sense embeddings however standard skip-gram embeddings turn robust task. sigmoid non-linearity function word embedding sample negative words optimizing autoencoding objective broadly similar learning algorithm deﬁned multi-sense embedding induction previous work note though previous work considered monolingual context. minibatch training regime seek optimize objective function minibatch found optimizing objective directly often resulted inducing posterior distributions. therefore form posterior regularization encode prior expectations posteriors sharp. regularized objective minibatch deﬁned entropy function posterior distributions encoder θ)). modiﬁed objective also motivated variational approximation perspective marcheggiani titov details. varying parameter easy control amount entropy regularization. objective optimized ﬂatter posteriors infers peaky posteriors. probability mass needs concentrated single sense resulting algorithm similar hard practice found using hard-update training closely related setting best performance. obtaining word representations test time construct word representations averaging sense embeddings word weighting sense expectations multiset {xi−n xi+n} including words around pivot word window size side. experiments. crosslingual context discussed either rely word alignments entire second-language sentence context. distinguish sense-speciﬁc embeddings denoted generic sense-agnostic ones denoted ﬁrst second language respectively. number sense-speciﬁc embeddings words. denote embedding parameters. learned jointly exception pre-trained secondlanguage embeddings. hyperparameter weights contribution language. setting would drop second-language component ﬁrst language. formulation allows addition languages easily provided second-language embeddings live semantic space. index goes pivot words ﬁrst language context words predict marginalizes possible senses word practice avoid costly computation normalization factor softmax computation negative sampling instead unlike training sense prediction step crosslingual context since available evaluation tasks. work instead marginalizing unobservable crosslingual context simply ignore computation. deﬁning crosslingual signal draw heuristic inspired devlin secondlanguage context words taken multiset words around including pivot afﬁliated word afﬁliated parameter regulates context window size. choosing afﬁliated word used context choosing context entire sentence obtain index following aligns exactly second-language word large body work multilingual word representations europarl preferred source parallel data. however domain europarl rather constrained whereas would like obtain word representations general language also carry effective evaluation semantic similarity datasets domains usually broader. therefore following parallel corpora news commentary yandex-m czeng exclude legislation texts gigafren sizes corpora reported table word representations trained corpora evaluated intrinsically small sizes. table parallel corpora used paper. word sizes reported based english part corpus. language pair different english part hence varying number sentences target language. learning parameters adagrad optimizer initial learning rate minibatch size number negative samples sampling factor window size parameter embeddings dimensional initialized sampling uniform distribution between include vocabulary words occurring corpus least times. number senses word parameters interested well semantic similarity ratings obtained embedding comparisons correlate human ratings. purpose variety similarity benchmarks english report spearman correlation scores human ratings cosine ratings obtained word representations. scws benchmark probably suitable benchmarks provide ratings word pairs without context. contains human-rated word pairs agirre separate benchmark similarity relatedness benchmarks contain nouns only. mturk- mturk- include word pairs whose similarity crowdsourced amt. similarly amt-annotated dataset word pairs. verb- measure verb similarity. rare-word contains rare-word pairs. finally simlex- intended measure pure similarity opposed relatedness. benchmarks prepare word representations taking uniform average sense embeddings word. evaluation carried using tool described faruqui dyer space constraints report results averaging benchmarks include individual results online repository. supersense similarity also evaluate task measuring similarity embeddings—in case uniformly averaged case multi-sense embeddings—and matrix supersense features extracted english semcor using qvec tool choose method shown output scores correlate well extrinsic tasks e.g. text classiﬁcation sentiment analysis. believe this combination word similarity tasks previous section give reliable picture generic quality word embeddings studied work. tagging downstream evaluation task learned word representations initialize embedding layer neural network tagging model. convolutional architecture jurafsky input layer taking concatenation neighboring embeddings input three hidden layers rectiﬁed linear unit activation function softmax output layer. train epochs using sentence batch. hyperparameters examined source code. multi-sense word embeddings inferred sentential context evaluation scws dataset. standard splits wall street journal portion penn treebank training development testing. compare three embeddings models skip-gram multi-sense bilingual multi-sense using implementation them. ﬁrst seen simpler variants bimu model omit encoder entirely omit second-language part encoder train models english part parallel corpora. parameters common methods kept ﬁxed experiments. values controlling second-language signal bimu pos-tagging development results scws benchmark show consistent improvements bimu model across parallel corpora except small cz-en corpus. also measured conﬁdence intervals difference correlation coefﬁcients bimu following method described according values bimu signiﬁcantly outperforms ru-en french russian spanish corpora. next ignoring language-speciﬁc factors would expect observe trend according larger corpus higher correlation score. however ﬁnd. among largest corpora i.e. ru-en cz-en fr-en models trained ru-en perform surprisingly well practically -times larger fr-en corpus. similarly quality embeddings trained cz-en generally lower trained model achieves competitive correlation score. results similarity benchmarks qvec largely conﬁrm scws despite lack sentential context would allow weight contribution different senses accurately multi-sense models. then simply averaging bimu embeddings lead better results using embeddings? hypothesize single-sense model tends over-represent dominant sense generic one-vector-per-word representation whereas uniformly averaged embeddings yielded multisense models better encode range potential senses. similar observations made context selectional preference modeling polysemous verbs tagging relationship bimu models similar discussed above. overall however neither multi-sense models outperforms embeddings. neural network tagger able implicitly perform disambiguation single-sense embeddings similarly argued jurafsky tagging accuracies obtained cz-en fr-en similar obtained jurafsky multi-sense model accuracy competitive case although larger corpus training word representations. table results per-row best bold. trained english part parallel corpora. bimu-sg report difference bimu together difference. similarity scores averaged benchmarks described tagging report accuracy. times smaller ru-en corpus. explanation might different text composition corpora ru-en matching domain evaluation task better larger corpora. also fr-en known noisy containing webcrawled sentences parallel natural language furthermore language-dependent effects might playing role example signs czech least helpful language among studied. evidence intrinsic tasks situation tagging conﬁrm speculation. relate models previously reported scws scores literature using -dimensional models table even though train much smaller corpus previous works bimu figure effect amount data used learning scws correlation scores. effect embedding dimensionality models trained ru-en evaluated scws either full vocabulary top- words. larger sub-samples fr-en largest parallel corpus. bimu embeddings show relatively stable improvements especially embeddings. performance achieved bimu sooner using around corpus. dimensionality frequent words argued jurafsky often increasing dimensionality model sufﬁces obtain better results multi-sense model. look effect dimensionality semantic similarity simply increasing dimensionality model sufﬁcient outperform bimu models. constraining vocabulary frequent words representations obtain higher quality. models especially beneﬁt slightly increased dimensionality looking frequent words. according expectations—frequent words need representational capacity complex semantic syntactic behavior role bilingual signal degree contribution second language learning affected parameters trade-off importance ﬁrst second language sense prediction part value size window around second-language word afﬁliated pivot. fig. suggests context second language regarding role context-window size sense disambiguation literature reported smaller larger monolingual contexts useful e.g. v´eronis overview. considering narrow context second language—the afﬁliated word window around it—performs best little gain using broader window. understandable since representation participating sense selection simply average generic embeddings window means averaged representation probably becomes noisy large i.e. irrelevant words included window. however negative effect accuracy still relatively small around models using french russian second languages czech setting inﬁnite window size setting corresponding sentence-only alignment performs well also scws improving monolingual multi-sense baseline corpora figure controlling bilingual signal. effect varying parameter controlling importance second-language context effect second-language window size accuracy. reported accuracies measured pos-tagging development set. work number senses model parameter keep ﬁxed throughout empirical study. comment brieﬂy choices found good choice ru-en fr-en corpora around .-point improvement scws tagging. larger values performance tends degrade. example ru-en score scws point default setting. multi-sense models. line research dealt sense induction separate clustering problem followed embedding learning component another sense assignment embeddings trained jointly neelakantan propose extension skip-gram introducing sense-speciﬁc parameters together k-means-inspired ‘centroid’ vectors keep track contexts word senses occurred. explore model variants number senses words anthreshold value determines number senses word. results comparing variants inconclusive advantage dynamic variant virtually nonexistent. work static approach. whenever evidence less senses number available sense vectors unlikely serious issue learning would concentrate senses would preferred predictions also test time. jurafsky build upon work neelakantan principled method introducing senses using chinese restaurant processes experiments conﬁrm ﬁndings neelakantan multi-sense embeddings improve skip-gram embeddings intrinsic tasks well jurafsky multi-sense embeddings offer little beneﬁt neural network learner extrinsic tasks. discrete-autoencoding method viewed without bilingual part encoder common methods. multilingual models. research using multilingual information learning multi-sense embedding models scarce. perform sense induction step based clustering translations prior learning word embeddings. translations clustered mapped source corpus using heuristics recurrent neural network trained obtain sense-speciﬁc representations. unlike work sense induction embedding learning components entirely separated without possibility inﬂuence another. similar vein bansal bilingual corpora perform soft word clustering extending previous work monolingual case single-sense representations multilingual context studied extensively goal bringing representations semantic space. related line work concerns crosslingual setting tries leverage training data language build models typically lower-resource languages recent works kawakami dyer nalisnick ravi also interest. latter work inﬁnite skip-gram model embedding dimensionality stochastic relevant since demonstrates embeddings exploit different dimensions encode different word meanings. like kawakami dyer bilingual supervision complex lstm network trained predict word translations. although represent different word senses separately method produces representations depend context. work second-language signal introduced sense prediction component ﬂexible—it deﬁned various ways obtained sentence-only alignments special case. presented method learning multi-sense embeddings performs sense estimation context prediction jointly. monobilingual information used sense prediction training. explored model performance variety tasks showing bilingual signal improves sense predictor even though crosslingual information available test time. able obtain word representations better quality monolingually-trained multi-sense representations outperform skip-gram embeddings intrinsic tasks. analyzed model performance several conditions namely varying dimensionality vocabulary size amount data size second-language context. latter parameter bilingual information useful even using entire would like thank jiwei providing tagger implementation robert grimm diego marcheggiani anonymous reviewers useful comments. computational work carried peregrine cluster university groningen. second author supported vidi grant", "year": 2016}