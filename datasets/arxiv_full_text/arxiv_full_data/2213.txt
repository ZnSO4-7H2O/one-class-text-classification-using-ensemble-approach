{"title": "Imagination-Augmented Agents for Deep Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.", "text": "introduce imagination-augmented agents novel architecture deep reinforcement learning combining model-free model-based aspects. contrast existing model-based reinforcement learning planning methods prescribe model used arrive policy learn interpret predictions learned environment model construct implicit plans arbitrary ways using predictions additional context deep policy networks. show improved data efﬁciency performance robustness model misspeciﬁcation compared several baselines. hallmark intelligent agent ability rapidly adapt circumstances \"achieve goals wide range environments\" progress made developing capable agents numerous domains using deep neural networks conjunction model-free reinforcement learning observations directly values actions. however approach usually requires large amounts training data resulting policies readily generalize novel tasks environment lacks behavioral ﬂexibility constitutive general intelligence. model-based aims address shortcomings endowing agents model world synthesized past experience. using internal model reason future also referred imagining agent seek positive outcomes avoiding adverse consequences trial-and-error real environment including making irreversible poor decisions. even model needs learned ﬁrst enable better generalization across states remain valid across tasks environment exploit additional unsupervised learning signals thus ultimately leading greater data efﬁciency. another appeal model-based methods ability scale performance computation increasing amount internal simulation. neural basis imagination model-based reasoning decision making generated interest neuroscience cognitive level model learning mental simulation hypothesized demonstrated animal human learning successful deployment artiﬁcial model-based agents however hitherto limited settings exact transition model available domains models easy learn e.g. symbolic environments low-dimensional systems complex domains simulator available agent recent successes dominated model-free methods domains performance model-based agents employing standard planning methods usually suffers model errors resulting function approximation errors compound planning causing over-optimism poor agent performance. currently planning model-based methods robust model imperfections inevitable complex domains thereby preventing matching success model-free counterparts. seek address shortcoming proposing imagination-augmented agents approximate environment models \"learning interpret\" imperfect predictions. algorithm trained directly low-level observations little domain knowledge similarly recent model-free successes. without making assumptions structure environment model possible imperfections approach learns end-to-end extract useful knowledge gathered model simulations particular relying exclusively simulated returns. allows agent beneﬁt model-based imagination without pitfalls conventional model-based planning. demonstrate approach performs better modelfree baselines various domains including sokoban. achieves better performance less data even imperfect models signiﬁcant step towards delivering promises model-based figure architecture. notation indicates imagined quantities. imagination core predicts next time step conditioned action sampled rollout policy imagines trajectories features encoded rollout encoder. full aggregated rollout encodings input model-free path determine output policy order augment model-free agents imagination rely environment models models that given information present queried make predictions future. environment models simulate imagined trajectories interpreted neural network provided additional context policy network. general environment model recurrent architecture trained unsupervised fashion agent trajectories given past state current action environment model predicts next state number signals environment. work consider particular environment models build recent successes action-conditional next-step predictors receive input current observation current action predict next observation potentially next reward. roll environment model multiple time steps future initializing imagined trajectory present time real observation subsequently feeding simulated observations model. actions chosen rollout result rollout policy environment model together constitute imagination core module predicts next time steps imagination core used produce trajectories ˆtn. imagined trajectory sequence features current time length rollout ˆft+i output environment model despite recent progress training better environment models issue addressed learned model cannot assumed perfect; might sometimes make erroneous nonsensical predictions. therefore want rely solely predicted rewards often done classical planning. additionally trajectories contain information beyond reward sequence reasons rollout encoder processes imagined rollout whole learns interpret i.e. extracting information useful agent’s decision even ignoring necessary trajectory encoded separately rollout embedding finally aggregator converts different rollout embeddings single imagination code ﬁnal component policy module network takes information model-based predictions well output model-free path outputs imagination-augmented policy vector estimated value therefore learns combine information model-free imagination-augmented paths; note without model-based path reduce standard model-free network thus thought augmenting model-free agents providing additional information model-based planning strictly expressive power underlying model-free agent. experiments perform rollout possible action environment. ﬁrst action rollout action action subsequent actions rollouts produced shared rollout policy investigated several types rollout policies found particularly efﬁcient strategy distill imagination-augmented policy model-free policy. distillation strategy consists creating small model-free network adding total loss cross entropy auxiliary loss imagination-augmented policy computed current observation policy computed observation. imitating imagination-augmented policy internal rollouts similar trajectories agent real environment; also ensures rollout corresponds trajectories high reward. time imperfect approximation results rollout policy higher entropy potentially striking balance exploration exploitation. experiments encoder lstm convolutional encoder sequentially processes trajectory features lstm reverse order ˆft+τ ˆft+ mimic bellman type backup operations. aggregator simply concatenates summaries. model-free path chose standard network convolutional layers plus fully connected also architecture baseline agent. environment model deﬁnes distribution optimized using negative loglikelihood loss lmodel. either pretrain environment model embedding within architecture jointly train agent adding lmodel total loss auxiliary loss. practice found pre-training environment model faster runtime architecture adopted strategy. environments training data environment model generated trajectories partially trained standard model-free agent partially pre-trained agents random agents rewards domains. however means account budget required pretrain data-generating agent well generate data. experiments address concern ways explicitly accounting number steps used pretraining demonstrating pretrained model reused many tasks using ﬁxed pretrained environment model trained remaining parameters asynchronous advantage actor-critic added entropy regularizer policy encourage exploration auxiliary loss distill rollout policy explained above. distributed asynchronous training workers; used rmsprop optimizer report results initial round hyperparameter exploration learning curves averaged three agents unless noted otherwise. separate hyperparameter search carried agent architecture order ensure optimal performance. addition following baseline agents standard model-free agent. main baseline agent chose model-free standard architecture similar consisting convolutional layers followed fully connected layer. ﬁnal layer fully connected outputs policy logits value function. sokoban also tested ‘large’ standard architecture double number feature maps hidden units resulting architecture slightly larger number parameters copy-model agent. aside internal environment model architecture different standard agent. verify information contained environment model rollouts contributed increase performance implemented baseline replaced environment model ‘copy’ model simply returns input observation. lacking model agent imagination uses architecture number learnable parameters beneﬁts amount computation model effectively corresponds architecture policy logits value ﬁnal output lstm network skip connections. demonstrate performance baselines puzzle environment sokoban. address issue dealing imperfect models highlighting strengths approach planning baselines. also analyze importance various components sokoban classic planning problem agent push number boxes onto given target locations. boxes pushed many moves irreversible mistakes render puzzle unsolvable. human player thus forced plan moves ahead time. expect artiﬁcial agents similarly beneﬁt internal simulation. implementation sokoban procedurally generates level episode means agent cannot memorize speciﬁc puzzles. together planning aspect makes challenging environment model-free baseline agents solve less levels billion steps training provide videos agents playing version sokoban online underlying game logic operates grid world agents trained directly sprite graphics shown fig. aspects make speciﬁc grid world games. figure random examples procedurally generated sokoban levels. player needs push boxes onto target squares solve level avoiding irreversible mistakes. agents receive sprite graphics observations. figure shows learning curves architecture various baselines explained throughout section. first compare standard model-free agent. clearly outperforms latter reaching performance levels solved maximum baseline. baseline increased capacity reaches still signiﬁcantly similarly sokoban outperforms copy-model. figure sokoban learning curves. left training curves baselines. note additional environment observations pretrain environment model main text discussion. right training curves various values imagination depth. since using imagined rollouts helpful task investigate length individual rollouts affects performance. latter hyperparameters searched over. breakdown number unrolling/imagination steps fig. shows using longer rollouts increasing number parameters increases performance unrolling steps improves speed learning performance signiﬁcantly unrolling step outperforms test signiﬁcantly longer rollouts outperforms reaching levels solved. however general found diminishing returns using longer rollouts. noteworthy steps relatively small compared number steps taken solve level best agents need steps average. implies even short rollouts highly informative. example allow agent learn moves cannot recover rollouts length signiﬁcantly slower rest section choose rollouts length canonical architecture. terms data efﬁciency noted environment model pretrained conservatively measured total number frames needed pretraining lower thus even taking pretraining account outperforms baselines seeing frames total course data efﬁciency even better environment model reused solve multiple tasks environment strengths able handle learned thus potentially imperfect environment models. however sokoban task learned environment models actually perform quite well rolling imagined trajectories. demonstrate deal less reliable predictions another experiment used environment model shown much worse performance strong artifacts accumulating iterated rollout predictions fig. shows even clearly ﬂawed environment model performs similarly well. implies learn ignore latter parts rollout errors accumulate still initial predictions errors less severe. finally note experiments surprisingly agent poor model ended outperforming agent good model. posit random initialization though cannot exclude noisy model providing form regularization work required investigate effect. figure experiments noisy environment model. left shows example -step rollout conditioning environment observation. errors accumulate lead various artefacts including missing duplicate sprites. right comparison monte-carlo search using either accurate noisy model rollouts. learning rollout encoder enables deal imperfect model predictions. demonstrate point comparing setup without rollout encoder classic monte-carlo search algorithm tesauro galperin explicitly estimate value action rollouts rather learning arbitrary encoding rollouts select actions according values. speciﬁcally learn value function states using rollout policy sample trajectory rollout initial action compute ))t=..t comes trajectory initialized action action chosen probability proportional ))/δ) learned temperature. thought form ﬁxed summarizer model-free path simple policy head. architecture learned. rollout encoder-free agent sokoban accurate noisy environment model. chose length rollout optimal environment model seen fig. using high accuracy environment model performance encoder-free agent similar baseline standard agent. however unlike performance degrades catastrophically using poor model showcasing susceptibility model misspeciﬁcation. studied role rollout encoder. show importance various components performed additional control experiments. results plotted fig. comparison. first copy model performs worse demonstrating environment model indeed crucial. second trained environment model predicting rewards observations. also performed worse. however much longer training agents recover performance close original never case baseline agent even many steps. hence reward prediction helpful absolutely necessary task imagined observations alone informative enough obtain high performance sokoban. note contrast many classical planning model-based reinforcement learning methods often rely reward prediction. previous sections illustrated used efﬁciently solve planning problems robust face model misspeciﬁcation. here different question assume nearly perfect model compare competitive planning methods? beyond performance focus particularly efﬁciency planning i.e. number imagination steps required solve ﬁxed ratio levels. compare regular agent variant monte carlo tree search modern guided tree search algorithm mcts implementation aimed strong baseline using recent ideas include transposition tables evaluate returns leaf nodes using value network running mcts sokoban achieve high performance cost much higher number necessary environment model simulation steps mcts reaches performance levels solved using model simulation steps average solve level compared environment model calls using even simulation steps mcts performance increases further e.g. reaching steps. assume access high-accuracy environment model also push performance further performing basic monte-carlo search trained rollout policy agent play whole episodes simulation execute successful action sequence found maximum number retries; reminiscent nested rollouts ﬁxed maximum retries obtain score total average number model simulation steps needed solve level including running model outer loop much lower corresponding mcts steps. note again approach requires nearly perfect model; don’t expect search perform well approximate models. table summary imagination efﬁciency different methods. lastly probe generalization capabilities beyond handling random level layouts sokoban. agents trained levels boxes. table shows performance agent tested levels different numbers boxes standard model-free agent comparison. found generalizes well; boxes agent still able solve half levels nearly many standard agent boxes. ﬁnal experiments demonstrate single model provides general understanding dynamics governing environment used solve collection different tasks. designed simple light-weight domain called minipacman allows easily deﬁne multiple tasks environment shared state transitions enables rapid experimentation. minipacman player explores maze contains food chased ghosts. maze also contains power pills; eaten ﬁxed number steps player moves faster ghosts away eaten. dynamics common tasks. task deﬁned vector wrew associating reward following events moving eating food eating power pill eating ghost eaten ghost. consider different reward vectors inducing different tasks. empirically found reward schemes sufﬁciently different lead different high-performing policies events note environment model effectively shared across tasks marginal cost learning model nil. training testing access frame reward predictions generated model; latter computed model event predictions task reward vector wrew. such reward vector wrew interpreted ‘instruction’ task solve environment fair comparison also provide baseline agents event variable input. trained baseline agents separately task. results fig. indicate beneﬁt architecture outperforming standard agent tasks copy-model baseline task. moreover found performance baselines particularly high tasks rewards particularly sparse anticipation ghost dynamics especially important. posit agent leverage environment reward model explore environment much effectively. figure minipacman environment. left frames minipacman game. frames images. player green dangerous ghosts food dark blue empty corridors black power pills cyan. eating power pill player weak ghosts right performance million environment steps different agents tasks. note clearly outperforms agents tasks sparse rewards. recent work focused applying deep learning model-based common approach learn neural model environment including observations classical planning algorithms trajectory optimization studies however address possible mismatch learned model true environment. model imperfection attracted particular attention robotics transferring policies simulation real environments there environment model given learned used pretraining planning test time. also learn extract information trajectories context imitation learning. bansal take bayesian approach model imperfection selecting environment models basis actual control performance. problem making imperfect models also approached simpliﬁed environment talvitie using techniques similar scheduled sampling however techniques break stochastic environments; mostly address compounding error issue address fundamental model imperfections. principled deal imperfect models capture model uncertainty e.g. using gaussian process models environment deisenroth rasmussen disadvantage method high computational cost; also assumes model uncertainty well calibrated lacks mechanism learn compensate possible miscalibration uncertainty. cutler consider hierarchy models increasing ﬁdelity. recent multi-task extension study help mitigate impact model misspeciﬁcation suffers high computational burden large domains marco number approaches models create additional synthetic training data starting dyna recent work e.g. venkatraman models increase data efﬁciency used agent test time. tamar silver present neural networks whose architectures mimic classical iterative planning algorithms trained reinforcement learning predict user-deﬁned high-level features; these explicit environment model. case explicit environment models trained predict low-level observations allows exploit additional unsupervised learning signals training. procedure expected beneﬁcial environments sparse rewards unsupervised modelling losses complement return maximization learning target recently explored jaderberg mirowski internal models also used improve credit assignment problem reinforcement learning henaff learn models discrete actions environments exploit effective differentiability model respect actions applying continuous control planning algorithms derive plan; schmidhuber uses environment model turn environment cost minimization network activity minimization. kansky learn symbolic networks models environment planning given relevant abstractions hand-crafted vision system. close work study hamrick present neural architecture queries learned expert models focus meta-control continuous contextual bandit problems. pascanu extend work focusing explicit planning sequential environments learn construct plan iteratively. general idea learning leverage internal model arbitrary ways also discussed schmidhuber presented approach combining model-free model-based ideas implement imagination-augmented learning interpret environment models augment model-free decisions. outperforms model-free baselines minipacman challenging combinatorial domain sokoban. demonstrated that unlike classical model-based planning methods able successfully imperfect models hence signiﬁcantly broadening applicability model-based concepts ideas. model-based methods trade-off environment interactions computation pondering acting. essential irreversible domains actions catastrophic outcomes sokoban. experiments always less order magnitude slower interaction model-free baselines. amount computation varied therefore expect greatly beneﬁt advances dynamic compute resource allocation another avenue future research abstract environment models learning predictive models \"right\" level complexity evaluated efﬁciently test time help scale richer domains. remarkably sokoban compare favourably strong planning baseline perfect environment model comparable performance require fewer function calls model mcts model rollouts guided towards relevant parts state space learned rollout policy. points potential improvement training rollout policies \"learn query\" imperfect models task-relevant way. thank victor valdes designing implementing sokoban environment joseph modayil reviewing early version paper eslami hado hasselt neil rabinowitz schaul yori zwols various help feedback. volodymyr mnih koray kavukcuoglu david silver alex graves ioannis antonoglou daan wierstra martin riedmiller. playing atari deep reinforcement learning. arxiv preprint arxiv. volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. international conference machine learning pages john schulman sergey levine pieter abbeel michael jordan philipp moritz. trust region policy optimization. proceedings international conference machine learning pages daniel schacter donna rose addis demis hassabis victoria martin nathan spreng karl szpunar. future memory remembering imagining brain. neuron demis hassabis dharshan kumaran seralynne vann eleanor maguire. patients hippocampal amnesia cannot imagine experiences. proceedings national academy sciences david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature marc deisenroth carl rasmussen. pilco model-based data-efﬁcient approach policy search. proceedings international conference machine learning pages junhyuk xiaoxiao honglak richard lewis satinder singh. action-conditional video prediction using deep networks atari games. advances neural information processing systems pages felix leibfried nate kushman katja hofmann. deep learning approach joint video frame reward prediction atari games. corr abs/. http//arxiv.org/abs/. benjamin childs james brodeur levente kocsis. transpositions move groups monte carlo tree search. computational intelligence games cig’. ieee symposium pages ieee manuel watter jost springenberg joschka boedecker martin riedmiller. embed control locally linear latent dynamics model control images. advances neural information processing systems pages eric tzeng coline devin judy hoffman chelsea finn xingchao peng sergey levine kate saenko trevor darrell. towards adapting deep visuomotor representations simulated real environments. arxiv preprint arxiv. paul christiano zain shah igor mordatch jonas schneider trevor blackwell joshua tobin pieter abbeel wojciech zaremba. transfer simulation real world learning deep inverse dynamics model. arxiv preprint arxiv. samy bengio oriol vinyals navdeep jaitly noam shazeer. scheduled sampling sequence prediction recurrent neural networks. advances neural information processing systems pages alonso marco felix berkenkamp philipp hennig angela schoellig andreas krause stefan schaal sebastian trimpe. virtual real trading simulations physical experiments reinforcement learning bayesian optimization. arxiv preprint arxiv. richard sutton. integrated architectures learning planning reacting based approximating dynamic programming. proceedings seventh international conference machine learning pages arun venkatraman roberto capobianco lerrel pinto martial hebert daniele nardi andrew bagnell. improved learning dynamics models control. international symposium experimental robotics pages springer david silver hado hasselt matteo hessel schaul arthur guez harley gabriel dulacarnold david reichert neil rabinowitz andre barreto predictron end-to-end learning planning. arxiv preprint arxiv. jaderberg volodymyr mnih wojciech marian czarnecki schaul joel leibo david silver koray kavukcuoglu. reinforcement learning unsupervised auxiliary tasks. arxiv preprint arxiv. piotr mirowski razvan pascanu fabio viola hubert soyer andy ballard andrea banino misha denil ross goroshin laurent sifre koray kavukcuoglu learning navigate complex environments. arxiv preprint arxiv. jürgen schmidhuber. on-line algorithm dynamic reinforcement learning planning reactive environments. neural networks ijcnn international joint conference pages ieee kansky silver david mély mohamed eldawy miguel lázaro-gredilla xinghua nimrod dorfman szymon sidor scott phoenix dileep george. schema networks zero-shot transfer generative causal model intuitive physics. accepted international conference machine learning jessica hamrick andy ballard razvan pascanu oriol vinyals nicolas heess peter battaglia. metacontrol adaptive imagination-based optimization. proceedings international conference learning representations razvan pascanu yujia oriol vinyals nicolas heess david reichert theophane weber sebastien racaniere lars buesing daan wierstra peter battaglia. learning model-based planning scratch. arxiv preprint jürgen schmidhuber. learning think algorithmic information theory novel combinations reinforcement learning controllers recurrent neural world models. arxiv preprint arxiv. training rollout policy distillation details agent used paper deﬁnes stochastic policy i.e. categorical distribution discrete actions logits computed neural network parameters taking observation timestep input. training increase probability rewarding actions taken applies update parameters using policy gradient value function also computed output neural network parameters input value function network chosen second last layer policy network computes parameter updated towards bootstrapped k-step return numerical implementation express updates gradients corresponding surrogate loss surrogate loss entropy regularizer encourage exploration λent thoughout exλent periments. applicable loss policy distillation consisting cross-entropy scaling parameter λdist. denotes backpropagate gradients ldist wrt. parameters rollout policy behavioral policy finally even though pre-trained environment models principle also learn jointly agent adding appropriate log-likelihood term observations model. investigate future research. optimize hyperparameters separately agent standard model-free baseline agent taken multi-layer convolutional neural network taking current observation input followed fully connected hidden layer. layer feeds heads layer output action computing policy logits another layer single output computes value function sizes layers chosen follows model free path consists identical standard model-free baseline rollout encoder processes frame generated environment model another identically sized cnn. output concatenated reward prediction feature input lstm units. lstm used process rollouts last output lstm rollouts concatenated single vector length sokoban minipacman. vector concatenated output model-free path fully connected layers computing policy logits value function baseline agent described above. pre-train separate auto-regressive models order pixel observations minipacman sokoban environments cases input model consisted last observation broadcasted one-hot representation last action following previous studies outputs models trained predict next frame stochastic gradient decent bernoulli cross-entropy network outputs data ot+. sokoban model simpliﬁed case minipacman model; sokoban model nearly entirely local minipacman model needs deal nonlocal interaction input output frames size model depicted ﬁgure consisted size preserving multi-scale architecture additional fully connected layers reward prediction. order capture long-range dependencies across pixels also make layer call pool-and-inject applies global max-pooling feature broadcasts resulting values feature maps size concatenates result input. pool-and-inject layers therefore size-preserving layers communicate max-value layer globally next convolutional layer. sokoban model chosen residual additional fully-connected pathway predicting rewards. input size ﬁrst processed convolutions large kernel stride reduced representation processed size preserving layers outputting predicted frame convolutional layer. figure minipacman environment model. overview given right panel blowups basic convolutional building block pool-and-inject layer basic build block three hyperparameters determining number channels convolutions; numeric values given right panel. minipacman additional details minipacman played grid-world. characters ghosts pacman move maze. walls positions ﬁxed. start level power pills number ghosts pacman placed random world. food found every square maze. number ghosts level level− ghosts always move square time step. pacman usually moves square except eaten power pill makes move squares time. moving squares pacman position ends inside wall moved back square back corridor. pacman ghost meet either location path crosses pacman moves square food power pill eats eating power pill gives pacman super powers moving double speed able ghosts. effects eating power pill last time steps. pacman meets ghost either pacman dies eaten ghost pacman recently eaten power pill ghost dies eaten pacman. pacman eaten power pill ghosts pacman. otherwise chase pacman. precise algorithm movement ghost given pseudo code used different tasks available minipacman. share environment dynamics vary reward structure level termination. rewards associated various events tasks given table below. regular level cleared food eaten; avoid level cleared steps; hunt level cleared ghosts eaten steps. ambush level cleared ghosts eaten steps. rush level cleared power pills eaten. game sokoban random actions levels would solve levels vanishing probability leading extreme exploration issues solving problem reinforcement learning. alleviate issue shaping reward scheme version sokoban every time step penalty applied agent. whenever agent pushes target receives reward whenever agent pushes target receives penalty finishing level gives agent reward level terminates. ﬁrst reward encourage agents ﬁnish levels faster second encourage agents push boxes onto targets third avoid artiﬁcial reward loop would induced repeatedly pushing target fourth strongly reward solving level. levels interrupted steps identical levels nearly never encountered training testing note reward scheme always optimal solve level alternative strategy would agent play curriculum increasingly difﬁcult tasks; expect strategies work similarly. ﬁrst additional experiment compared without reward prediction trained longer horizon. reward prediction clearly converged shortly steps therefore interrupted training; however without reward prediction kept increasing performance steps recover performance level close levels solved fig. next investigated monte-carlo search agent solve levels times within internal model. base architecture solving around levels; mental retries boosted performance around levels solved. although agent allowed mental retries practice performance increase obtained within ﬁrst mental retries. exact percentage gain mental retry shown fig. note fig. levels solved ﬁrst mental attempt even though architecture could solve around levels. explained environment model although looks nearly perfect naked model actually equivalent environment. ﬁrst trained value network estimates value function trained model-free policy; this trained model-free agent environment steps. agent solved close episodes. using agent generated pairs trained value network predict value frame; training test error comparable don’t expect increasing number training points would signiﬁcantly improved quality value network. value network architecture residual network stacks convolution layer convolution blocks ﬁnal fully-connected layer hidden units. ﬁrst convolution convolution feature maps. three residual convolution block composed convolutional layers; ﬁrst convolution feature maps second convolution feature maps last layer feature maps. help value networks trained pixel representation symbolic representation. trained value network employed search evaluate leaf-nodes similar replacing role traditional random rollouts mcts. tree policy uses ﬁne-tuned exploration constant depth-wise transposition tables tree nodes used deal symmetries sokoban environment. external actions selected taking value root node. tree reused steps selecting appropriate subtree root node next step. reported results obtained averaging results episodes. detail procedural generation sokoban levels follow closely methods described generation sokoban level involves three steps room topology generation position conﬁguration room reverse-playing. topology generation given initial width*height room entirely constituted wall blocks topology generation consists creating ‘empty’ spaces boxes targets player placed. simple random walk algorithm conﬁgurable number steps applied random initial position direction chosen. afterwards every step position updated probability random direction selected. every ‘visited’ position emptied together number surrounding wall blocks selected randomly choosing following patterns indicating adjacent room blocks removed note room ‘exterior’ walls never emptied width×height room space actually converted corridors. random walk approach guarantees positions room principle reachable player. relatively small probability changing walk direction favours generation longer corridors application random pattern favours slightly convoluted spaces. position conﬁguration room topology generated target locations desired boxes player initial position randomly selected. obvious prerequisite enough empty spaces room place targets player constraints imposed step. reverse playing topology targets/player positions generated room reverseplayed. case step player eight possible actions choose from simply moving moving+pulling possible direction initially room conﬁgured boxes placed corresponding targets. position depth-ﬁrst search carried space possible moves ‘expanding’ reached player/boxes position iteratively applying possible actions entire tree explored different combinations actions leading repeated boxes/player conﬁgurations skipped. statistics collected boxes/player conﬁguration turn scored simple heuristic boxswaps represents number occasions player stopped pulling given started pulling different boxdisplacement represents manhattan distance initial ﬁnal position given box. also whenever player placed targets roomscore value scoring heuristic doesn’t guarantee complexity generated rooms it’s aimed favour room conﬁgurations overall boxes away original positions increase probability room requiring convoluted combination moves solution scoring mechanism empirically proved generate levels balanced combination difﬁculties. reverse playing ends available positions explore predeﬁned maximum number possible room conﬁgurations reached. room higher roomscore returned. defaul parameters room conﬁguration tree default limited maximum depth applied actions. total number visited positions default limited default random-walk steps", "year": 2017}