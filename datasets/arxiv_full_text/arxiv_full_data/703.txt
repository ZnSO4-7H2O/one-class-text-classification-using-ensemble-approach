{"title": "Improving Efficiency in Convolutional Neural Network with Multilinear  Filters", "tag": ["cs.CV", "cs.AI", "cs.NE"], "abstract": "The excellent performance of deep neural networks has enabled us to solve several automatization problems, opening an era of autonomous devices. However, current deep net architectures are heavy with millions of parameters and require billions of floating point operations. Several works have been developed to compress a pre-trained deep network to reduce memory footprint and, possibly, computation. Instead of compressing a pre-trained network, in this work, we propose a generic neural network layer structure employing multilinear projection as the primary feature extractor. The proposed architecture requires several times less memory as compared to the traditional Convolutional Neural Networks (CNN), while inherits the similar design principles of a CNN. In addition, the proposed architecture is equipped with two computation schemes that enable computation reduction or scalability. Experimental results show the effectiveness of our compact projection that outperforms traditional CNN, while requiring far fewer parameters.", "text": "abstract—the excellent performance deep neural networks enabled solve several automatization problems opening autonomous devices. however current deep architectures heavy millions parameters require billions ﬂoating point operations. several works developed compress pre-trained deep network reduce memory footprint possibly computation. instead compressing pre-trained network work propose generic neural network layer structure employing multilinear projection primary feature extractor. proposed architecture requires several times less memory compared traditional convolutional neural networks inherits similar design principles cnn. addition proposed architecture equipped computation schemes enable computation reduction scalability. experimental results show effectiveness compact projection outperforms traditional requiring fewer parameters. recent years deep neural network architectures excelled several application domains ranging machine vision natural language processing biomedical ﬁnancial data analysis important developments convolutional neural network evolved main workhorse solving computer vision tasks nowadays. architecture originally developed handwritten character recognition using convolutional layers years development graphical processing units efﬁcient implementation convolution operation depth cnns increased tackle complicated problems. nowadays prominent architectures residual network google inception hundreds layers become saturated. researchers started wonder whether millions parameters essential achieve performance. order extend beneﬁt deep nets embedded devices limited computation power memory recent works focused reducing memory footprint computation pre-trained network i.e. apply network compression post-training stage. fact recent works shown traditional network architectures alexnet inception highly redundant structures example simple heuristic based magnitude weights employed eliminate connections pre-trained network achieved considerable amount compression without hurting performance much. additionally representing network parameters bitwidth numbers like shown performance -bit network closely retained -bit representations. noted approaches complementary other. fact compression pipeline called deep compression consists three compression procedures i.e. weight pruning weight quantization huffman-based weight encoding achieved excellent compression performance alexnet vgg- architectures. along pruning quantization low-rank approximation convolutional layers fully connected layers also employed achieve computational speed viewed high-order tensors convolutional layers decomposed using traditional tensor decomposition methods decomposition tucker decomposition convolution operation approximated applying consecutive convolutions. overall efforts remove redundancy already trained neural networks shown promising results determining networks much simpler structure. results naturally pose following question compress already trained network seek compact network representation trained scratch?. subsequently could course exploit mentioned compression techniques decrease cost. perspective works utilizing low-rank approximation approach among ﬁrst report simpliﬁed network structures. success convolutional neural networks attributed four important design principles sparse connectivity parameter sharing pooling multilayer structure. sparse connectivity allows local interaction input neurons output neurons. design principle comes fact many natural data modalities images videos local/neighboring values often highly correlated. groups local values usually contain certain distinctive patterns e.g. edges color blobs images. parameter sharing mechanism cnns enables underlying model learn location invariant cues. words sliding ﬁlters input patterns detected regardless location. pooling multilayer structure design deep neural networks general particular captures compositional hierarchies embedded within many natural signals. example facial images lower level cues edges color texture patterns form discriminative higher level cues facial parts like nose eyes lips. similar compositional structure seen speech text composed phonemes syllables words sentences. although particular structure deep network evolved time important design principles remain unchanged. core convolution layer ﬁlter d×d×c elements operates micro feature extractor performs linear projection data patch/volume feature space dimensions real value. order enhance discrimination power micro feature extractor authors proposed replace model general nonlinear function approximator particularly multilayer perceptron resulting architecture dubbed network network since consists micro networks perform feature extractor functionality instead simple linear projection. paper instead seeking complex feature extractor propose replace linear projection traditional multilinear projection pursuit simplicity. great effort extend traditional linear methods multilinear ones attempt directly learn natural representation data high order tensors beauty multilinear techniques lies property input tensor projected simultaneously tensor mode allowing certain connections input dimensions output dimensions hence greatly reducing number parameters. previous works multilinear discriminant learning multilinear regression shown competitive results multilinear-based techniques. proposed architecture still inherits four fundamental design properties traditional deep network utilizing multilinear projection generic feature extractor. complexity feature extractor easily controlled rank hyper-parameter. besides fast computation scheme network compact also propose alternative computation method allows efﬁcient computation complexity increases. propose generic feature extractor performs multilinear mapping replace conventional linear ﬁlters cnns. complexity individual feature extractor easily controlled rank hyperparameter method. ability adjust individual ﬁlter’s complexity complexity entire network adjusted without need increasing number ﬁlters layer i.e. width layer. since proposed mapping differentiable entire network easily trained end-to-end using gradient descent-based training process. provide analysis computation memory requirements proposed structure. addition based properties proposed mapping propose efﬁcient computation strategies leading different complexity settings. remainder paper organized follows section provide overview related works focusing designing compact network structures. section gives necessary notations deﬁnitions presenting proposed structure analysis. section provide details experiment procedures results quantitative analysis. section concludes work discusses possible future extensions. research focusing design less redundant network architecture gained much attention recently. prominent design pattern bottleneck unit ﬁrst introduced resnet architecture bottleneck pattern formed convolution layers convolution layers between. ﬁrst convolution layer used reduce number input feature maps latter used restore number output feature maps. several works incorporated bottleneck units network structure reduce computation memory consumed. recently mobilenet architecture proposed replaced normal convolution operation depthwise separable convolution layers. constituted depthwise convolution pointwise convolution depthwise separable convolution layer performs ﬁltering combining steps independently. resulting structure many times efﬁcient terms memory computation. noted bottleneck design depthwise separable convolution layer design macro level network structure arrangements layers investigated reduce computation. micro level works assumed rank structure convolutional kernels order derive compact network structure. fact rank assumption incorporated several designs prior deep neural networks dictionary learning wavelet transform high dimensional data. ﬁrst incorporation rank assumption neural network compression proposed decomposition proposed decompose entire convolutional layer four convolutions. although effective depth network remains same replacing convolution operation four potentially lead difﬁculty training network scratch. carefully designed initialization scheme work able train mixture low-rank ﬁlters scratch competitive performances. improving idea different low-rank structure allows approximating already trained network training scratch proposed speciﬁcally denote convolution layer kernels rd×d×c×n number input feature maps spatial size kernel respectively. proposed approximate using vertical kernel rd××c×k horizontal kernel r×d×k×n approximation following form rd×d×c rd×d×c denote input patch centered spatial location convolution kernel respectively. core classic convolution kernel operates feature extractor sliding input tensor generate higher level representation. speciﬁcally kernel performs following linear mapping denotes response intercept term respectively. denotes dot-product tensors. linear projection nonlinearity applied using layer’s activation function. since mapping operates similar input patch yields scalar response linear mapping cnns proposed multilinear mapping acts generic feature extractor incorporated design topology alexnet inception resnet addition since mapping differentiable respect individual weight vector resulting network architecture trained end-to-end fashion back propagation algorithm. hereby denote layer employing proposed multilinear mapping mlconv. recently mode-k multiplication introduced tensor contraction layer project entire input layer high-order tensor another tensor. fundamentally different approach since tensor contraction layer global mapping incorporate sparse connectivity parameter sharing principles. general modek multiplication applied input patch/volume output another tensor instead scalar proposal. restrict multilinear projection form avoid increase output dimension leads computation overhead next layer. moreover tensor unfolding operation required perform multilinear projection transforms tensor another tensor potentially increase computation. contrary proposed mapping special case general multilinear mapping using mode-k superscript subscript denote index channel kernel respectively. hyper-parameter controlling rank matrix approximation. kernel weight n-th ﬁlter applied c-th channel input feature map; d-dimensional vectors. seen authors simplify convolutional layer types parameter sharing. ﬁrst sharing right singular vectors across input channels within n-th ﬁlter second enforces sharing left singular vectors across ﬁlters. work closely related since avoid designing particular initialization scheme including batch normalization step resulting structure easily trained scratch different network conﬁgurations. start section introducing notations deﬁnitions related work. denote scalar values either low-case upper-case characters vectors low-case bold-face characters matrices upper-case bold-face characters tensors calligraphic capital characters tensor multilinear matrix modes deﬁned ri×i×···×ik denotes dimension mode-k. entry ikth index mode-k denoted xii...ik deﬁnition mode-k ﬁber tensor ri×i×···×ik vector ik-dimensional given ﬁxing every index mode-k unfolding also known mode-k matricization transforms tensor matrix formed arranging mode-k ﬁbers columns. shape ×i¯k mode-k projection computational cost actually depends order projections. computationally efﬁcient ﬁrst perform projection mode- order reduce number input feature maps subsequent mode- mode- projection mode- projection corresponds applying convolutions input kernels size elements computational complexity output projection along third mode tensor size obvious advantages mapping requires fewer parameters estimate model compared linear mapping cnn. utilizing mapping layer kernels requires storage parameters. hand similar layer conﬁguration mappings utilizing projection requires parameters. gain ratio memory reduction utilizing mapping varies different layers. case leads gain ratio approximately equal d/r. experiments seen layers memory reduction approximately competitive performance compared similar network topology. denote rx×y rd×d×c×n input kernels l-th convolutional layer input feature maps output feature maps. addition assume zero-padding sliding window stride using linear projection case computational complexity layer evaluating computational cost layer using proposed method noted projection efﬁciently computed applying three consecutive convolution operations. details convolution operations depend order three modes. therefore although result mapping independent order scalability proposed mapping large previous subsection proposes efﬁcient computation scheme allows computation savings small. overall conclude computation proposed layer structure efﬁcient while shown experimental evaluation changing rank adopted tensor deﬁnitions increase performance. section provide experimental results support theoretical analysis section iii. experimental protocol datasets described ﬁrst followed discussion experimental results. expressed kruskal form outer-product corresponding projection three modes. calculating using modek product deﬁnition using dot-product equivalance found consequently convolution layer converted mlconv layer decomposing convolution ﬁlter kruskal form using decomposition method noted that since closed-form solution decomposition conversion corresponds approximation step. perspective pre-trained used initialize network structure speed training process. however show experimental section random initialization multilinear ﬁlters lead better performance. addition initialization scheme also complements proposed mapping efﬁcient computation strategy large. computation cost discussed previous subsection depends linearly parameter large efﬁcient compute mapping according ﬁrst calculating convolving input computational complexity ﬁrst step convolution step resulting overall complexity entire layer. ratio normal convolution layer mlconv layer using computation strategy clear usually much larger therefore increase computation compared normal convolution marginal. following calculation strategy rank network marginally slower rank network cnn. demonstrated experiment section. conclusion computation method discussed subsection allows traditional topology consists modules feature extractor module classiﬁer module. several convolution pooling layers stacked feature extractor fully-connected layers classiﬁer. order evaluate effectiveness proposed multilinear ﬁlter constructed network architecture feature extractor layers i.e. convolution layer mlconv layer together pooling layer skipping fully-connected layer. name suggests fully-connected layer dense connections accounting large number parameters network prone overﬁtting. moreover powerful effective feature extractor module expected produce highly discriminative latent space classiﬁcation task made simple. fullyconvolutional networks attracted much attention lately compactness excellent performance imagerelated problems like semantic segmentation object localization classiﬁcation conﬁguration baseline network adopted experiment benchmark shown table denotes kernels spatial dimension denotes batch normalization lrelu denotes leaky rectiﬁed linear unit baseline architecture similar proposed four differences. firstly choose retain proper pooling layer instead performing convolution stride proposed secondly batch normalization applied every convolution layer except last output goes softmax produce class probability. addition lrelu activation unit applied output batch normalization. shown adoption lrelu speeds learning process network tolerant learning rate possibility arriving better minimas based conﬁguration network topology compare performance standard linear convolution kernel proposed multilinear kernel low-rank structure proposed last convolution layers replaced mlconv layer. noted lrelu applied three competing structures experiments cifar- cifar- cifar dataset object classiﬁcation dataset consists color images training testing resolution pixels. cifar- refers -class classiﬁcation problem dataset class images training images testing cifar- refers ﬁne-grained classiﬁcation images classes. svhn svhn well-known dataset handwritten digit recognition problem consists images house numbers extracted natural scenes varying number samples class. dataset poses much harder character recognition problem compared mnist dataset used cropped images provided database individual image might contain distracting digits sides. networks trained using optimizer well adam proposed structure tends arrive better minimas adam case methods. optimizer momentum ﬁxed adopted sets learning rate schedule schedule initial learning rate decreases next value epochs cross-validated trained network maximum epochs cifar svhn respectively. batch size ﬁxed samples competing networks. regarding data augmentation cifar dataset random horizontally ﬂipped samples added well random translation images maximum pixels performed training process; svhn dataset random translation maximum pixels performed. dataset preprocessing step applied. regarding regularization weight decay max-norm individually together exploited experiments. max-norm regularizer introduced used together dropout. training process norm individual ﬁlter constrained inside ball given radius cross-validated weight decay hyper-parameter searched addition dropout applied input dropout applied output pooling layers optimal obtained differences three competing structures observed baseline networks work well weight decay applying weight decay proposed network structure tends drive weight values close zeros large regularization effect marginal using small value leading mlconv structures experimented several values rank parameter namely proposed mapping experiments made attempt optimize individual ﬁlter layer order maximal compact structure since approach impractical real cases. instead used rank value throughout layers. experiments hence different authors reported performance different values layer without discussing rank selection method. experiments conducted corresponding structures denoted mlconv mlconv mlconv mlconv. values selected number parameters network similar number parameters mlconv counterpart given corresponding structures denoted number denotes value perform experiments corresponds since training network computationally much slower falls objective paper. three competing structures training scratch initialized random initialization scheme proposed additionally trained mlconv structure weights initialized optimal pre-trained cifar dataset. aforementioned protocols also applied conﬁguration. weights mlconv initialized decomposition using canonical alternating least square method structure followed calculation proposed obtaining optimal hyper-parameter values network trained times median value reported. second tables shows classiﬁcation errors competing methods trained scratch cifar- cifar- respectively third shows performance initialized pre-trained cnn. last reports model size network. seen tables using proposed multi-linear ﬁlters leads reduction memory outperforming standard convolution ﬁlters coarse ﬁne-grained classiﬁcation cifar datasets. interestingly cifar- rank multilinear ﬁlter network attains improvement increase number projections mode i.e. using performance network increases small margin. cifar- cifar constraining gains memory reduction keeping performance relatively closed baseline less increment classiﬁcation error. limiting maximizes parameter reduction nearly cost increase error rate cifar- cifar- respectively. graphical illustration compromise number network’s parameters classiﬁcation error cifar- cifar illustrated figures respectively. classiﬁcation error competing network trained scratch svhn dataset shown table using proposed mlconv layers achieved reduction model size slightly outperforming cnn. compact conﬁguration mlconv structure i.e. mlconv observed small increment classiﬁcation error compared baseline. increased complexity mlconv layers little improvement seen mlconv comparing proposed multi-linear ﬁlter rank structure conﬁgurations mlconv network signiﬁcantly outperform counterparts. speciﬁcally compact conﬁguration mlconv better cifar- cifar- respectively. margin shrinks complexity increases proposed structure consistently outperforms training network scratch. similar comparison results observed svhn dataset using mlconv layers obtained lower classiﬁcation errors compared layers complexity conﬁgurations. opposed experimental results reported observed inferior results structure compared standard training scratch. difference might attributed main reasons incorporated batch normalization baseline could potentially improve performance baseline cnn; baseline conﬁguration fullyconnected layer solely benchmark efﬁciency different ﬁlter structures feature extractor. interesting phenomenon observed initialized mlconv pre-trained cnn. structure conﬁgurations enjoy substantial improvement initializing network weights decomposed pre-trained cifar dataset. contrary happens proposed mlconv structure since conﬁgurations observe degradation performance. explained fact structure designed approximate individual convolution ﬁlter every input feature resulting structure comes closed-form solution approximation. good initialization network easily arrived good minimum training low-rank setting scratch might difﬁculty achieving good local minimum. although proposed mapping viewed form convolution ﬁlter mapping embeds multi-linear structure hence possessing certain degree difference. initializing proposed mapping applying decomposition closed-form solution lead network suboptimal state. table reports average forward propagation time single sample measured three network structures cifar-. second third columns report theoretical actual speed-ups respectively measured number multiply-accumulate operations normalized respect convolution counterparts. proposed mlconv structure report computation cost calculation strategies discussed section iii. refer ﬁrst calculation strategy using separable convolution scheme latter using normal convolution scheme. results scheme denoted asterisk. networks implemented using keras library tensorﬂow backend. theoretical speed-up actual speed-up especially proposed structure implemented unoptimized separable convolution operation. fact time writing implementation separable convolution operation still missing libraries mention efﬁcient implementation. contrary results scheme using normal convolution show near perfect match theory implementation. fact normal convolution operation efﬁciently implemented optimized popular libraries. also explains computation gain structure inferior mlconv structure theory similar practice since structure realized normal convolution operation. last four columns table additionally prove scalability scheme respect hyper-parameter rank discussed section iii-d. paper proposed multilinear mapping replace conventional convolution ﬁlter convolutional neural networks. resulting structure’s complexity ﬂexibly controlled adjusting number projections mode hyper-parameter proposed mapping comes computation schemes either allow memory computation reduction small scalability large. numerical results showed fewer parameters architectures employing mapping could outperform standard cnns. promising results opens future research directions focusing optimizing parameter individual convolution layers achieve compact structure performance. girshick donahue darrell malik rich feature hierarchies accurate object detection semantic segmentation proceedings ieee conference computer vision pattern recognition hinton deng dahl a.-r. mohamed jaitly senior vanhoucke nguyen sainath deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine vol. graves a.-r. mohamed hinton speech recognition deep recurrent neural networks acoustics speech signal processing ieee international conference ieee zabihi kiranyaz gabbouj katsaggelos heart sound anomaly quality detection using ensemble neural networks without segmentation computing cardiology conference ieee tsantekidis passalis tefas kanniainen gabbouj iosiﬁdis using deep learning detect price change indications ﬁnancial markets european signal processing conference greece tsantekidis passalis tefas kanniainen gabbouj iosiﬁdis forecasting stock prices limit order book using convolutional neural networks business informatics ieee conference vol. ieee szegedy sermanet reed anguelov erhan vanhoucke rabinovich going deeper convolutions proceedings ieee conference computer vision pattern recognition denton zaremba bruna lecun fergus exploiting linear structure within convolutional networks efﬁcient evaluation advances neural information processing systems hubara courbariaux soudry el-yaniv bengio quantized neural networks training neural networks precision weights activations arxiv preprint arxiv. s.-c. zhou y.-z. wang q.-y. y.-h. balanced quantization effective efﬁcient approach quantized neural networks journal computer science technology vol. schonfeld multilinear discriminant analysis higherorder tensor data classiﬁcation ieee transactions pattern analysis machine intelligence vol. yang zhang tang h.-j. zhang discriminant analysis tensor representation computer vision pattern recognition cvpr ieee computer society conference vol. ieee wang foroosh design efﬁcient convolutional layers using single intra-channel convolution topological subdivisioning spatial bottleneck structure howard chen kalenichenko wang weyand andreetto adam mobilenets efﬁcient convolutional neural networks mobile vision applications arxiv preprint arxiv. ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift international conference machine learning ronneberger fischer brox u-net convolutional networks biomedical image segmentation international conference image computing computer-assisted intervention medical springer maas hannun rectiﬁer nonlinearities improve neural network acoustic models proc. icml vol. springenberg dosovitskiy brox riedmiller striving simplicity convolutional arxiv preprint arxiv. netzer wang coates bissacco reading digits natural images unsupervised feature learning nips workshop deep learning unsupervised feature learning vol. zhang delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation proceedings ieee international conference computer vision chollet keras. https//github.com/fchollet/keras abadi agarwal barham brevdo chen citro corrado davis dean devin ghemawat goodfellow harp irving isard jozefowicz kaiser kudlur levenberg man´e monga moore murray olah schuster shlens steiner sutskever talwar tucker vanhoucke vasudevan vi´egas vinyals warden wattenberg wicke zheng tensorflow large-scale machine learning heterogeneous systems software available tensorﬂow.org.", "year": 2017}