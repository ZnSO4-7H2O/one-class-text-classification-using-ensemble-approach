{"title": "Crowdsourcing Multiple Choice Science Questions", "tag": ["cs.HC", "cs.AI", "cs.CL", "stat.ML"], "abstract": "We present a novel method for obtaining high-quality, domain-targeted multiple choice questions from crowd workers. Generating these questions can be difficult without trading away originality, relevance or diversity in the answer options. Our method addresses these problems by leveraging a large corpus of domain-specific text and a small set of existing questions. It produces model suggestions for document selection and answer distractor choice which aid the human question generation process. With this method we have assembled SciQ, a dataset of 13.7K multiple choice science exam questions (Dataset available at http://allenai.org/data.html). We demonstrate that the method produces in-domain questions by providing an analysis of this new dataset and by showing that humans cannot distinguish the crowdsourced questions from original questions. When using SciQ as additional training data to existing questions, we observe accuracy improvements on real science exams.", "text": "science exam high-level task requires mastery integration information extraction reading comprehension common sense reasoning consider example question with force moon affect tidal movements oceans?. solve model must possess abstract understanding natural phenomena apply questions. transfer general domain-speciﬁc background knowledge scenarios poses formidable challenge modern statistical techniques currently struggle with. recent kaggle competition addressing grade science questions highest scoring systems achieved multiple choice test retrieval-based systems outperforming neural systems. major bottleneck applying sophisticated statistical techniques science lack large in-domain training sets. creating large multiple choice science dataset challenging since crowd workers cannot expected domain expertise questions lack relevance diversity structure content. furthermore poorly chosen answer distractors multiple choice setting make questions almost trivial solve. ﬁrst contribution paper general method mitigating difﬁculties crowdsourcing data particular focus multiple choice science questions. method broadly similar recent work relying mainly showing crowd present novel method obtaining high-quality domain-targeted multiple choice questions crowd workers. generating questions difﬁcult without trading away originality relevance diversity answer options. method addresses problems leveraging large corpus domain-speciﬁc text small existing questions. produces model suggestions document selection answer distractor choice human question generation process. method assembled sciq dataset multiple choice science exam questions. demonstrate method produces indomain questions providing analysis dataset showing humans cannot distinguish crowdsourced questions original questions. using sciq additional training data existing questions observe accuracy improvements real science exams. construction large high-quality datasets main drivers progress nlp. recent proliferation datasets textual entailment reading comprehension question answering allowed advances tasks particularly neural models need domainrelevant passages questions seek create multiple choice questions directanswer questions. two-step process solve problems ﬁrst using noisy classiﬁer relevant passages showing several options workers select generating question. second model trained real science exam questions predict good answer distractors given question correct answer. predictions crowd workers transforming question produced ﬁrst step multiple choice question. thus methodology leverage existing study texts science questions obtain relevant questions plausible answer distractors. consequently human intelligence task shifted away purely generative task reframed terms selection modiﬁcation validation task call sciq. figure shows ﬁrst four training examples sciq. dataset multiple choice version task select correct answer using whatever background information system given question several answer options direct answer version given passage question system must predict span within passage answers question. experiments using recent state-ofthe-art reading comprehension methods show useful dataset research. interestingly neural models beat simple information retrieval baselines multiple choice version dataset leaving room research applying neural models settings training examples number tens thousands instead hundreds thousands. also show using sciq additional source training data improves performance real grade exam questions proving method successfully produces useful in-domain training data. dataset construction. recent work focused constructing large datasets suitable training neural models. datasets assembled based freebase wikipedia articles search user queries reading comprehension based news children books novels recognizing textual entailment based image captions continue line work construct dataset science exam dataset differs aforementioned datasets consists natural language questions produced people instead cloze-style questions. also differs prior work narrower domain science exams produce multiple choice questions difﬁcult generate. science exam question answering. existing models multiple-choice science exam vary reasoning framework training methodology. sub-problems solution strategies outlined clark method described clark evaluates coherence scene constructed question enriched background information sachan train entailment model derives correct answer background knowledge aligned maxmargin ranker. probabilistic reasoning approaches include markov logic networks integer linear program-based model assembles proof chains structured knowledge aristo ensemble combines multiple reasoning strategies shallow statistical methods based lexical co-occurrence provide surprisingly strong baselines. much work applying neural networks task likely paucity training data; paper attempt address issue constructing much larger dataset previously available present results experiments using state-of-the-art reading comprehension techniques datasets. automatic question generation. transforming text questions tackled before mostly didactic purposes. approaches rely syntactic transformation templates others generate cloze-style questions. ﬁrst attempts constructing science question dataset followed techniques. found methods produce highseveral similarity measures employed selecting answer distractors including measures derived wordnet thesauri distributional context domainspeciﬁc ontologies phonetic morphological similarity probability scores question context context-sensitive lexical inference also used. contrast aforementioned similaritybased selection strategies method uses feature-based ranker learn plausible distractors original questions. several heuristics used features ranking model. feature-based distractor generation models used past agarwal mannem creating biology questions. model uses random forest rank candidates; agnostic towards taking cloze humanly-generated questions learned speciﬁcally generate distractors resemble real science exam questions. section present method crowdsourcing science exam questions. method two-step process ﬁrst present candidate passages crowd worker letting worker choose passages question second another worker takes question answer generated ﬁrst step produces three distractors aided model trained predict good answer distractors. result multiple choice science question consisting question passage correct answer distractors incorrect answer options {a}. example questions shown figure remainder section elaborates steps question generation process. falling speciﬁc lexical structural patterns. enforce diversity question content lexical expression inspire relevant in-domain questions rely corpus in-domain text crowd workers questions. however text large in-domain corpus textbook suitable generating questions. simple ﬁlter narrow selection paragraphs likely produce reasonable questions. base corpus. choosing relevant in-domain base corpus inspire questions crucial importance overall characteristics dataset. science questions corpus consist topics covered school exams linguistically complex speciﬁc loaded technical detail observed articles retrieved searches science exam keywords yield signiﬁcant proportion commercial otherwise irrelevant documents consider further. articles science-related categories simple wikipedia targeted factual often state highly speciﬁc knowledge chose science study textbooks base corpus directly relevant linguistically tailored towards student audience. contain verbal descriptions general natural principles instead highly speciﬁc example features particular species. number resources limited compiled list books various online learning resources including openstax share material creative commons license. books biology chemistry earth science physics span elementary level college introductory material. full list books used found appendix. document filter. designed rule-based document ﬁlter model individual paragraphs base corpus fed. system classiﬁes individual sentences accepts paragraph minimum number sentences accepted. small manually annotated dataset sentences labelled either relevant irrelevant ﬁlter designed iteratively adding ﬁlter rules ﬁrst improve precision recall held-out validation set. ﬁnal ﬁlter included lexical grammatical pragmatical complexity based rules. speciﬁcally sentences ﬁltered question exclamation verb phrase iii) contained modal verbs contained imperative phrases contained demonstrative pronouns contained personal pronouns third-person vii) began pronoun viii) contained ﬁrst names less tokens commas contained special characters punctuation three tokens beginning uppercase xii) mentioned graph table link xiii) began discourse marker xiv) contained absoulute wording contained instructional vocabulary question formulation task. actually generate in-domain pairs presented ﬁltered in-domain text crowd workers question could answered presented passage. although undesirable paragraphs ﬁltered beforehand non-negligible proportion irrelevant documents remained. circumvent problem showed worker three textbook paragraphs gave freedom choose reject irrelevant. paragraph chosen reused formulate questions speciﬁed desirable characteristics science exam questions yes/no questions requiring context querying general principles rather highly speciﬁc facts question length words answer length words ambiguous questions answers clear paragraph chosen. examples desirable undesirable questions given explanations good examples. furtherencouraged workers give feedback contact email provided address upcoming questions directly; multiple crowdworkers made opportunity. task advertised amazon mechanical turk requiring master’s status crowdworkers paying compensation hit. total workers participated whole crowdsourcing cases three documents rejected much fewer single document presented thus besides economical proposing several documents reduces risk generating irrelevant questions best case helps match crowdworker’s individual preferences. second task selecting distractors generating convincing answer distractors great importance since distractors make question trivial solve. writing science questions ourselves found ﬁnding reasonable distractors time-consuming part overall. thus support process crowdsourcing task model-generated answer distractor suggestions. primed workers relevant examples allowed suggested distractors directly good enough. next discuss characteristics good answer distractors propose evaluate model suggesting distractors describe crowdsourcing task uses them. distractor characteristics. multiple choice science questions nonsensical incorrect answer options interesting task study useful training model well real science exams model would need kind science reasoning answer training questions correctly. difﬁculty generating good multiple choice question then lies identifying expressions false answers generating expressions plausible false answers. concretely besides false answers good distractors thus grammatically consistent question when animals energy always produced? noun phrase expected. distrators takes account criteria. basic level ranks candidates large collection possible distractors selects highest scoring items. ranking function classiﬁer distinguishes plausible distractors random distractors based features classiﬁcation train actual in-domain questions observed false answers plausible distractors random expressions negative examples sampled equal proportion classiﬁer chose random forest robust performance small mid-sized data settings power incorporate nonlinear feature interactions contrast e.g. logistic regression. distractor model features. section describes features used distractor ranking model. features distractor model learn characteristics real distractors original questions suggest distractors deems realistic question. following features question correct answer tentative distractor expression used intuition knowledge-base link hypernymy indicator features reveal sibling structures respect shared property hypernym. example correct answer heart plausible distractor like liver would share hyponymy relation organ wordnet. model training. ﬁrst constructed large candidate distractor whose items ranked model. contained expressions consisting items glove vocabulary answer distractors observed training questions; list noun phrases simple wikipedia articles body parts; noun vocabulary expressions extracted primary school science texts. examples consisted multiple tokens added expression could obtained exchanging unigram another unigram model trained science exam questions separated training questions validation questions. question came four answer options providing three good distractor examples. used scikit-learn’s implementation random forests default parameters. used trees enforced least samples tree leaf. distractor model evaluation. model achieved training validation accuracy overall. example predictions distractor model shown table qualitatively predictions appear acceptable cases though quality high enough directly without additional ﬁltering crowd workers. many cases distractor semantically related correct type predictions misaligned level speciﬁcity multiword expressions likely unrelated ungrammatical despite inclusion part speech features. even predicted distractors fully coherent showing crowd worker still positive priming effect helping worker generate good distractors either providing nearly-good-enough candidates forcing worker think suggestion good distractor question. distractor selection task. actually generate multiple choice science question show result ﬁrst task pair crowd worker along distractors suggested previously described model. goal task two-fold quality control pair) validating predicted distractors writing ones necessary. ﬁrst instruction judge whether question could appear school science exam; questions could marked ungrammatical having false answer unrelated science requiring speciﬁc background knowledge. total proportion questions passing second instruction select suggested distractors write least distractor total three. requirement worker generate distractors instead allowed select three predicted distractors added initial pilot task found forced workers engage task resulted higher quality distractors. gave examples desirable undesirable distractors opportunity provide feedback before. advertised task amazon mechanical turk paying requiring master’s status. average crowd workers found predicted distractors good enough include ﬁnal question around half time resulting distractors ﬁnal dataset generated model decomposition energy membrane energy motion energy context energy distinct energy remainder paper investigate properties sciq dataset generated following methodology described section. present system human performance show sciq used additional training data improve model performance real science exams. sciq total multiple choice questions. randomly shufﬂed dataset split training validation test portions questions validation test portions remainder train. figure show distribution question answer lengths data. part questions answers dataset relatively short though longer questions. question also associated passage used generating question. multiple choice question trivial answer given correct passage multiple choice version sciq include passage; systems must retrieve background knowledge answering question. associated passage additionally created direct-answer version sciq passage question answer options. small percentage passages obtained unreleasable texts direct answer version sciq slightly smaller questions train test. qualitative evaluation. created crowdsourcing task following setup person presented original science exam question crowdsourced question. instructions choose questions likely real exam question. randomly drew original questions instances sciq training presented options random order. people identiﬁed science exam question cases falls signiﬁcance level null hypothesis random guess. multiple choice setting. used aristo ensemble individual components simple information retrieval baseline table-based integer linear programming model evaluate sciq. also evaluate competitive neural reading comprehension models attention reader gated attention reader reading comprehension methods require supporting text passage answer question. corpus aristo’s lucene component retrieve text passage formulating queries based question answer concatenating three results query passage. train reading comprehension models training hyperparameters recommended prior work reader reader) early stopping validation data. human accuracy estimated using sampled subset questions different people answering questions. answering questions people allowed query systems were. table shows results evaluation. aristo performance slightly better real science exams tableilp uses hand-collected background knowledge cover topics sciq performance substantially worse original test set. neural models perform reasonably well dataset though interestingly able outperform simple information retrieval baseline even using exactly background information. suggests sciq useful dataset studying reading comprehension models medium-data settings. direct answer setting. additionally present baseline direct answer version sciq. bidirectional attention flow model recently achieved state-of-the-art results squad trained bidaf training portion sciq evaluated test set. bidaf achieves exact match score model’s performance squad. using sciq answer exam questions last experiment sciq shows usefulness training data models answer real science questions. collected corpus grade science exam questions used reader reader answer questions. table shows model performances using real science questions training data augmenting training data sciq. adding sciq performance reader reader improves grade levels cases substantially. contrasts earlier attempts using purely synthetic data models overﬁt synthetic data overall performance decrease. successful transfer information sciq real science exam questions shows question distribution similar real science questions. particular focus science questions. using methodology constructed dataset science questions called sciq release future research. shown baseline evaluations dataset useful research resource investigate neural model performance medium-sized data settings augment training data answering real science exam questions. multiple strands possible future work. direction systematic exploration multitask settings best exploit dataset. possible extensions direction generating answer distractors could adaptation idea negative sampling e.g. population. another direction bootstrap data obtained improve automatic document selection question generation distractor prediction generate questions fully automatically. references manish agarwal prashanth mannem. automatic gap-ﬁll question generation text books. proceedings workshop innovative building educational applications. association computational linguistics stroudsburg iunlpbea pages http//dl.acm.org/citation.cfm?id=.. jonathan berant andrew chou frostig semantic parsing freepercy liang. proceedings base question-answer pairs. conference empirical methods natural language processing emnlp october grand hyatt seattle seattle washington meeting sigdat special interest group acl. pages http//aclweb.org/anthology/d/d/d-.pdf. samuel bowman gabor angeli christopher potts christopher manning. large annotated corpus learning natural language inferproceedings conference ence. empirical methods natural language processing association computational linguistics. peter clark. elementary school science math tests driver take aristo twentychallenge ninth aaai conference artiﬁcial intelligence. aaai press aaai’ pages http//dl.acm.org/citation.cfm?id=.. peter clark oren etzioni tushar khot ashish sabharwal oyvind tafjord peter turney combining retrieval daniel khashabi. statistics answer elementary science questions. thirtieth aaai conference artiﬁcial intelligence. aaai press aaai’ pages http//dl.acm.org/citation.cfm?id=.. peter clark philip harrison niranjan balasubramanian. study knowledge base requirements passing elementary science proceedings workshop test. automated knowledge base construction. york akbc pages https//doi.org/./.. correia jorge baptista nuno mamede isabel trancoso maxine eskenazi. automatic progeneration cloze question distractors. ceedings interspeech satellite workshop second language studies acquisition learning education technology waseda university tokyo japan. michael heilman noah smith. good question statistical ranking question generation. human language technologies annual conference north american chapter association computational linguistics. association computational linguistics stroudsburg pages http//dl.acm.org/citation.cfm?id=.. karl moritz hermann tom´aˇs koˇcisk´y edward grefenstette lasse espeholt mustafa teaching suleyman phil blunsom. advances machines read comprehend. neural information processing systems http//arxiv.org/abs/.. daniel hewlett alexandre lacoste llion jones illia polosukhin andrew fandrianto matthew kelcey david berthelot. wikireading novel large-scale language understanding task wikipedia. corr abs/.. http//arxiv.org/abs/.. felix hill antoine bordes sumit chopra goldilocks prinjason weston. ciple reading children’s books explicit memory representations. corr abs/.. http//arxiv.org/abs/.. daniel khashabi tushar khot ashish sabharwal peter clark oren etzioni roth. question answering integer programming proceedings semi-structured knowledge. twenty-fifth international joint conference artiﬁcial intelligence ijcai york july pages http//www.ijcai.org/abstract//. eric gribkoff ashish sabharwal peter clark oren etzioni. exploring markov logic networks proceedings question answering. conference empirical methods natural language processing emnlp lisbon portugal september pages http//aclweb.org/anthology/d/d/d-.pdf. mausam michael schmitz robert bart stephen soderland oren etzioni. open language proceedlearning information extraction. ings joint conference empirical methods natural language processing computational natural language learning. association computational linguistics stroudsburg emnlp-conll pages http//dl.acm.org/citation.cfm?id=.. ruslan mitkov computeraided generation multiple-choice tests. proceedings hlt-naacl workshop building educational applications using natural language processing volume association computational linguistics stroudsburg hlt-naacl-educ pages https//doi.org/./.. ruslan mitkov andrea varga semantic similarity disluz rello. tractors multiple-choice tests extrinsic evalworkshop uation. geometrical models natural language semantics. association computational linguistics stroudsburg gems pages http//dl.acm.org/citation.cfm?id=.. generating diagnostic multiple choice comprehension proceedings seventh cloze questions. workshop building educational applications using nlp. association computational linguistics stroudsburg pages http//dl.acm.org/citation.cfm?id=.. takeshi onishi wang mohit bansal kevin gimpel david mcallester. what large-scale person-centered cloze condataset. ference empirical methods natural language processing emnlp austin texas november pages http//aclweb.org/anthology/d/d/d-.pdf. andreas papasalouros konstantinos kanaris konstantinos kotis. automatic generation multiple choice questions domain ontologies. miguel baptista nunes maggie mcpherson editors e-learning. iadis pages denis paperno germ´an kruszewski angeliki lazaridou quan ngoc pham raffaella bernardi sandro pezzelle marco baroni gemma boleda raquel fern´andez. lambada dataset word prediction requiring broad discourse context. arxiv preprint arxiv. jeffrey pennington richard socher christopher manning. glove global vectors word representation. empirical methods natural language processing pages http//www.aclweb.org/anthology/d-. juan pino michael heilman maxine eskenazi. selection strategy improve cloze question quality. proceedings workshop intelligent tutoring systems ill-deﬁned domains. international conference intelligent tutoring systems.. keisuke sakaguchi yuki arase mamoru komachi. discriminative approach ﬁllin-the-blank quiz generation language learnst annual meeters. association computational linguistics august soﬁa bulgaria volume short papers. pages http//aclweb.org/anthology/p/p/p-.pdf. carissa schoenick peter clark oyvind tafjord peter turney oren etzioni. moving beyond turing test allen science challenge. arxiv preprint arxiv. seiichi yamamoto. measuring non-native speakers’ proﬁciency english using test automatically-generated quesproceedings second workshop tions. building educational applications using nlp. association computational linguistics stroudsburg edappsnlp pages http//dl.acm.org/citation.cfm?id=.. yang scott wen-tau chris meek. wikiqa challenge dataset open-domain question answering. association computational linguistics. https//www.microsoft.com/enus/research/publication/wikiqa-a-challenge-datasetfor-open-domain-question-answering/. auchallenging distractors tomatic generation inference using context-sensitive proceedings ninth workshop innovative building educational applications beaacl june baltimore maryland usa. pages http//aclweb.org/anthology/w/w/w.pdf. general chemistry principles patterns applications bruce averill strategic energy security solutions patricia eldredge r.h. hand llc; saylor foundation basics general organic biological chemistry david ball cleveland state university john hill university wisconsin rhonda scott southern adventist university. saylor foundation peoples physics book basic biology advanced concepts biology concepts biology chemistry basic chemistry concepts intermediate earth science concepts middle correspondence authors onishi hyperparameters reported original paper rest. reader three gated-attention layers multiplicative gating mechanism. character-level embedding features question-evidence common word features follow work using pretrained -dimension glove vectors initialize ﬁxed word embedding layer. gated attention layer apply dropout rate hyperparameters original work direct answer reading comprehension. implemented bidirectional attention flow model exactly described adopted hyperparameters used paper. multiple choice reading comprehension. during training reader reader monitored model performance epoch stopped training error validation increased hard limit epochs models reached peak validation accuracy ﬁrst second epoch. test evaluation applicable used model parameters epoch peak validation accuracy. implemented models keras theano backend tesla gpu.", "year": 2017}