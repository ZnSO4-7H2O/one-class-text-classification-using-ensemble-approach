{"title": "Recklessly Approximate Sparse Coding", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "It has recently been observed that certain extremely simple feature encoding techniques are able to achieve state of the art performance on several standard image classification benchmarks including deep belief networks, convolutional nets, factored RBMs, mcRBMs, convolutional RBMs, sparse autoencoders and several others. Moreover, these \"triangle\" or \"soft threshold\" encodings are ex- tremely efficient to compute. Several intuitive arguments have been put forward to explain this remarkable performance, yet no mathematical justification has been offered.  The main result of this report is to show that these features are realized as an approximate solution to the a non-negative sparse coding problem. Using this connection we describe several variants of the soft threshold features and demonstrate their effectiveness on two image classification benchmark tasks.", "text": "recently observed certain extremely simple feature encoding techniques able achieve state performance several standard image classiﬁcation benchmarks including deep belief networks convolutional nets factored rbms mcrbms convolutional rbms sparse autoencoders several others. moreover triangle soft threshold encodings extremely eﬃcient compute. several intuitive arguments forward explain remarkable performance mathematical justiﬁcation oﬀered. main result report show features realized approximate solution non-negative sparse coding problem. using connection describe several variants soft threshold features demonstrate eﬀectiveness image classiﬁcation benchmark tasks. image classiﬁcation several central problems computer vision. problem concerned sorting images categories based objects types objects appear them. important assumption make setting object interest appears prominently image consider possibly presence background clutter ignored. related problem object localization predict location extent object interest larger image considered report. neural networks common tool problem applied area since least late recently introduction contrastive divergence lead explosion work neural networks task. neural network models serve purposes setting provide method design dictionary primitives representing images. neural networks dictionary designed learning thus tailored speciﬁc data set. representations constructed classiﬁed using standard classiﬁer support vector machine. properly designed feature based representations classiﬁed much accurately using pixels directly. neural networks proved eﬀective tools constructing representations rameters required. designing features image using techniques requires learning parameters rapidly becomes intractable even small images. common solution problem construct features much eﬀort devoted designing elaborate feature learning methods shown recently provided dictionary reasonable encoding method greater eﬀect classiﬁcation performance speciﬁc choice dictionary particular demonstrate simple feature encoding methods outperform variety much sophisticated techniques. aforementioned works feature encoding methods motivated based computational simplicity eﬀectiveness. main contribution report provide connection features sparse coding; situate work broader theoretical framework oﬀer explanation success techniques. ings proved surprisingly eﬀective achieving state results popular image classiﬁcation benchmarks. however makes features especially appealing simplicity. given dictionary producing encoding requires single matrix multiply threshold operation. similar approaches feature encoding well known computer vision literature name vector quantization. approach data encoded hard assignment nearest dictionary element. gemert consider softer version idea using kernel function quantization rather hard assignment leads better performance. bourdeau consider similar soft quantization scheme sparse coding performs better still. following success triangle soft threshold features applied several settings. blum triangle features encoding work applying unsupervised feature learning rgb-d data using dictionary designed convolutional k-means approach. knoll apply triangle features image compression using paq. unthresholded version triangle features used low-level image features system extracts redesigns chart images documents. soft threshold features used detection recognition digits text natural images. features also employed part base learning model system selecting receptive ﬁelds higher layers deep network. consider triangle soft threshold features encoding image patches investigate eﬀects optimizing spatial pooling process order achieve better classiﬁcation accuracy. work similar found report work gregor lecun approximations sparse coding like interested designing feature encoders using approximations sparse coding; however technique diﬀerent consider here. chief diﬃculty sparse coding encoding step requires solving regularized problem closed form solution. example want extract features frame video real time sparse coding prohibitively slow. authors design trainable ﬁxed cost encoders predict sparse coding features. predictor designed taking iterative method solving sparse coding problem truncating speciﬁed number steps give ﬁxed complexity feed forward predictor. parameters predictor optimized predict true sparse codes training set. ﬁcation performance directly whereas focus predicting optimal codes. experiments show that least approach quantities surprisingly uncorrelated. cated iterative solutions without learning done coordinate descent based algorithm. experiments suggest methods vastly outperformed truncated proximal methods setting. soft threshold features sparse coding proximal gradient algorithm. using connection outline four possible variants soft threshold features based sparse encoding algorithms chapter chapter provides necessary optimization background support tools used report. focusing objective functions speciﬁc separable structure discuss proximal gradient algorithm ﬁxed step size extension method uses barzilai-borwein step size scheme. also consider variant proximal gradient algorithm operates dual space alteration method make tractable sparse coding problem. throughout chapter eschew generality favour developing tools directly relevant sparse coding problem. methods discuss general variants applicable broader class problems concerned with. citations chapter used expositions methods general settings. presentation chapter assumes familiarity common optimization tools speciﬁcally reader familiar lagrangian methods dualization used without justiﬁcation. extensive discussions supporting theory lagrangian dual found standard text convex optimization alternative step size selection methods line search iterate averaging also possible. scheme barzilai-borwein scheme picks step size approximates inverse hessian choice step size motivated form proximal gradient updates. consider approximating solution equation using quadratic model diagonal covariance α−i. approximating taylor expansion arbitrary point leads problem showing proximal gradient update equation amounts minimizing plus quadratic approximation step. calculation made repeated fact adding multiplying constant aﬀect location argmax. barzilai-borwein scheme adjusts step size ensure model minimize accurate possible. general step size selection rule found consider speciﬁc instance scheme specialized sparse coding problem section method known dual ascent shown converge certain conditions. unfortunately sparse coding problem conditions satisﬁed. chapter often interested problems minimized entire subspace problematic projection subspace non-zero minimization equation unbounded well deﬁned. lagrangian equation comparing equation shows dual space proximal ascent gradient ascent equivalent. actual derivation somewhat lengthy reproduced feature learning encoding process similar many ways neural network based methods discussed introduction. allows create accurate reconstructions input vectors data set. useful consider dictionaries overcomplete dictionary elements dimensions; however case minimizing reconstruction error alone provide unique encoding. sparse coding uniqueness recovered asking feature representation input sparse possible. report focus encoding phase assume dictionary provided external source. focus attention reasonable since shown experimentally long dictionary constructed reasonable encoding process eﬀect classiﬁcation performance. want make explicit considering encoding phase refer sparse encoding problem. exactly reasonable means context interesting question beyond scope report. results demonstrate wide variety dictionary construction methods lead similar classiﬁcation performance oﬀer conditions dictionary guarantee good performance. often useful consider non-negative version sparse coding leads optimization equation additional constraint elements must non-negative. ways formulate constraint useful following chapters indicator function positive orthant objective equation studies sparse coding learning encoding phases problem considered together. cases proceeds alternately optimizing convergence. ﬁxed optimization equation quadratic easily solved. optimization presented here matrix changes successive optimization. since setting dictionary ﬁxed need consider optimization diﬀerence focus leads terminological conﬂict literature. since sparse coding often refers learning encoding problem together term non-negative sparse coding typically refers slightly diﬀerent problem equation equation constrained non-negative whereas literature non-negative sparse coding typically implies constrained non-negative done cannot introduce constraint here since treat ﬁxed parameter generated external process. remainder chapter introduce four algorithms solving sparse encoding problem. ﬁrst three instances proximal gradient framework presented chapter fourth algorithm based diﬀerent approach solving sparse encoding problem works tracking solutions regularization parameter varies. equation non-smooth part proportional ||x||. name iterative soft thresholding arises proximal operator norm given soft threshold function ...) constant lipschitz constant referred statement equation which case sparse coding largest eigenvalue fast variant iterative soft thresholding modiﬁes iteration include specially chosen momentum term leading following iteration starting sparse reconstruction separable approximation optimization framework designed handling problems form considered chapter framework actually subsumes ista fista style algorithms discussed above consider speciﬁc instantiation framework sets step size using barzilai-borwein scheme making diﬀerent methods described above. development discusses sparsa full generality specializing sparse coding problem following iteration fact observed forcing objective value step. methods descend step signiﬁcantly degrade performance practice order guarantee convergence type scheme common approach force iterates larger largest objective value ﬁxed time window. approach allows objective value occasionally increase still ensuring iterates converge limit. boosted lasso diﬀerent approach solving sparse encoding problem considered above. rather solving equation directly blasso works alternative formulation sparse encoding problem value equation corresponding value causes equation solution although mapping values problem dependent. blasso works varying value maintaining corresponding solution equation step. name suggests blasso draws theory boosting cast problem functional gradient descent mixture parameters additive model composed weak learners. setting weak learners elements dictionary mixing parameters found similarly algorithms considered above blasso starts fully sparse solution instead applying proximal iterations proceeds taking types steps forward steps decrease quadratic term equation backward steps decrease regularizer. truth blasso also gives blasso used optimize arbitrary convex loss function convex regularizer; however case sparse coding forward backward steps especially simple. simplicity means iteration blasso much cheaper single iteration methods consider although advantage reduced fact several iterations blasso required produce reasonable encodings. another disadvantage blasso cannot easily cast allows multiple encodings computed simultaneously. chapter present main result report connection soft threshold features discussed introduction sparse encoding problem. insight show soft threshold features viewed approximate solution non-negative sparse encoding problem illustrate connection framework proximal gradient minimization. using tools presented chapter demonstrate connection writing proximal gradient iteration sparse encoding problem computing value ﬁrst iterate starting appropriately chosen initial point. summarize result proposition. given single step proximal gradient descent nonnegative sparse coding objective regularization parameter known dictionary starting fully sparse solution. stated proof proposition nearly immediate; however immediacy appears hindsight. soft threshold features sparse coding features treated separate competing entities triangle features sparse coding treated separate sparsity inducing objects. proposition provides nice explanation success soft threshold features classiﬁcation. sparse coding well studied problem widely known features sparse coding models eﬀective classiﬁcation tasks. even optimizers specially designed sparse coding problem typically take many iterations converge solution reconstruction error single iteration proximal gradient descent suﬃcient give features shown highly discriminative. investigate answers questions experimentally chapter examining variants soft threshold features preform. develop variants truncating proximal descent based optimization algorithms sparse coding problem. remainder chapter presents one-step features algorithms presented chapter fast iterative soft thresholding described section ﬁrst iteration fista step ordinary proximal gradient descent since fista iteration requires iterates adjust step size. sparse reconstruction separable approximation described section sparsa like fista adaptive step size selection scheme proximal gradient descent chooses step size based previous iterates. ﬁrst iteration information available authors suggest step size case. choice gives following formula step sparsa features alternating direction method multipliers described section since admm operates dual space computing iterations requires choosing starting value dual variable since choice essentially parameter expression penalty parameter admm. long interested taking single step optimization matrix precomputed encoding cost admm soft threshold features. unfortunately want perform iterations admm forced solve linear system iteration although cache appropriate factorization order avoid full inversion step. choice allows make interesting connections between step admm features optimization problems. instance consider second order variant proximal gradient following step features sparse encoding problem thus interpret step admm features smoothed step proximal version newton’s method. smoothing admm features important typical sparse coding problems dictionary overcomplete thus rank deﬁcient. although possible replace inverse expression pseudoinverse found numerically unstable. admm iteration smooths inverse ridge term recovers stability. blasso described brieﬂy section unlike algorithms consider iteration blasso updates exactly element feature vector means taking step blasso leads feature vector exactly non-zero element contrast proximal methods described update elements iteration meaning step features arbitrarily dense. evaluate classiﬁcation performance using features obtained approximately solving sparse coding problem using diﬀerent optimization methods discussed chapter report results classifying cifar- stl- data sets using experimental framework similar images stl- pixels scale match size cifar- experiments. diﬀerent optimization algorithm produce features running diﬀerent numbers iterations examine eﬀect classiﬁcation accuracy. experimented methods constructing dictionaries well including using dictionary built omitting k-means step using whitened patches directly. also considered dictionary normalized random noise well smoothed version random noise obtained convolving noise features guassian ﬁlter. however found dictionary created using whitened patches k-means together gave uniformly better performance report results choice dictionary. extensive comparison diﬀerent dictionary choices appears testing phase build representation image cifar- stl- data sets using dictionary obtained training. build representations image using patches follows sparse encoding problems encode cifar- alone repeated number iterations algorithm consider. since iterations diﬀerent algorithms diﬀerent computational complexity compare classiﬁcation accuracy time required produce encoded features rather number iterations. performed procedure using features obtained non-negative sparse coding well regular sparse coding found projecting features positive orthant always gives better performance reported results features obtained way. parameter selection performed separately algorithm data set. algorithm select algorithm speciﬁc parameters admm blasso well equation order maximize classiﬁcation accuracy using features obtained single step optimization. parameter values used experiment shown table results experiment cifar- summarized figure corresponding results stl- shown figure results similar data sets; discussion applies cifar- stl-. ﬁrst notable feature results blasso leads features give relatively poor performance. although approximate blasso able exact solutions equation running algorithm limited number iterations means regularization strong. also small numbers iterations performance features obtained fista admm sparsa nearly identical. table shows highest accuracy obtained algorithm data runs. another interesting feature blasso performance that optimized produce one-step features algorithms signiﬁcantly faster iterations blasso. unexpected iterations blasso complexity reason blasso features take longer compute comes fact possible vectorize blasso iterations across diﬀerent problems. solve many sparse encoding problems simultaneously replace vectors equation matrices containing corresponding vectors several problems aggregated columns. form updates described chapter replace without aﬀecting solution problem. allows take advantage optimized blas libraries compute matrix-matrix multiplications required solve problems batch. possible take advantage optimization blasso. notable feature figures large numbers iterations performance fista sparsa actually drops single iteration ﬁrst blush seems obviously wrong. important interpret implications plots carefully. ﬁgures parameters chosen optimize classiﬁcation performance one-step features. particular reason expect parameters also lead optimal performance many iterations. found parameters obtains optimizing one-step classiﬁcation performance generally parameters gets optimizing performance optimization converged. also noted parameter constellations found table universally small value. contributes drop accuracy fista especially since algorithm becomes unstable many iterations small observation consistent experiments found diﬀerent optimal values sparse coding regularizer threshold parameter soft threshold features comparisons. also stated that reported performance soft threshold features sparse coding often signiﬁcantly diﬀerent. refer reader cited work discussion factors governing diﬀerences. experiment designed answer third question chapter relating relationship reconstruction classiﬁcation accuracy. experiment measure reconstruction accuracy encodings obtained previous experiment. encoding dictionary used experiment constructed using method described section order ensure results comparable previous experiment actually dictionary both. results experiment shown figure comparing results previous experiment surprisingly little correlation reconstruction error classiﬁcation performance. figure one-step features give best classiﬁcation performance methods considered features also lead worst reconstruction. another interesting feature experiment parameters found give best features admm actually lead optimizer makes progress reconstruction beyond ﬁrst iteration. conﬁrm merely artifact implementation also included reconstruction error admm alternative setting gives lowest reconstruction error tested methods producing inferior performance classiﬁcation. figure classiﬁcation accuracy versus computation time cifar- using diﬀerent sparse encoding algorithms. fista admm sparsa markers show performance measured budget iterations. left-most marker lines shows performance using implementation optimized perform exactly step optimization. line blasso shows performance measured budget iterations. cases early stopping allowed termination criterion figure classiﬁcation accuracy versus computation time stl- using diﬀerent sparse encoding algorithms. fista admm sparsa markers show performance measured budget iterations. left-most marker lines shows performance using implementation optimized perform exactly step optimization. line blasso shows performance measured budget iterations. cases early stopping allowed termination criterion figure mean reconstruction error versus computation time small sample patches cifar-. fista admm sparsa iterations each blasso iterations. admm corresponds admm parameters gave best one-step classiﬁcation performance. admm diﬀerent parameter leads better reconstruction worse classiﬁcation. report shown soft threshold features enjoyed much success recently arise single step proximal gradient descent non-negative sparse encoding objective. result serves situate surprisingly successful feature encoding method broader theoretical framework. using connection proposed four alternative feature encoding methods based approximate solutions sparse encoding problem. experiments demonstrate approximate proximal-based encoding methods lead feature representations similar performance image classiﬁcation benchmarks. sparse encoding objective based around minimizing error reconstructing image patch using linear combination dictionary elements. given degree approximation techniques would expect features lead accurate reconstructions. second experiment demonstrates intuition correct. obvious extension work would preform thorough empirical exploration interaction value regularization parameter degree approximation. experimentation interaction exists glean insight structure. concrete suggestions along line full parameter search would make possible properly asses usefulness performing iteration optimization constructing feature encoding. report evaluate performance diﬀerent feature encoding methods using single dictionary; examining variability performance across multiple dictionaries would lend credibility results reported chapter sparse encoding problem. addition indicator function regularizer equation appears essential good performance proximal methods found eﬀective setting work adding quadratic smoothing term objective function step. connection one-step admm elastic also notable regard. understanding eﬀects diﬀerent regularizers empirically better theoretical framework reasoning eﬀects diﬀerent regularizers setting would quite valuable. work looked unstructured variants sparse coding. possible extend ideas presented structured case regularizer includes structure inducing terms potential launching points authors investigate proximal optimization methods structured variants sparse coding problem. blum springenberg w¨ulﬁng riedmiller. applicability unsupervised feature learning object recognition rgbdata. nips workshop deep learning unsupervised feature learning boyd parikh peleato eckstein. distributed optimization statistical learning alternating direction method multipliers. foundations trends machine learning coates carpenter case satheesh suresh wang text detection character recognition scene images unsupervised feature learning. proceedings international conference document analysis recognition pages ieee courville bergstra bengio. spike slab restricted boltzmann machine. proceedings fourteenth international conference artiﬁcial intelligence statistics volume jenatton mairal obozinski bach proximal methods sparse hierarchical dictionary learning. proceedings international conference machine learning ranzato monga devin corrado chen dean building high-level features using large scale unsupervised learning. international conference machine learning lecun jackel boser denker graf guyon henderson howard hubbard. handwritten digit recognition applications neural chips automatic learning. fogelman herault burnod editors neurocomputing algorithms architectures applications arcs france springer. netzer wang coates bissacco reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning ngiam chen bhaskar sparse ﬁltering. shawe-taylor r.s. zemel bartlett f.c.n. pereira k.q. weinberger editors advances neural information processing systems pages ranzato hinton. modelling pixel means covariances using factored third-order boltzmann machines. ieee conference computer vision pattern recognition ranzato krizhevsky hinton factored -way restricted boltzmann machines modelling natural images. international conference artiﬁcial intelligence statistics rifai vincent muller glorot bengio. contracting auto-encoders explicit invariance feature extraction. proceedings twenty-eight international conference machine learning june savva kong chhajta fei-fei agrawala heer. revision automated classiﬁcation analysis redesign chart images. proceedings anual symposium user interface software technology", "year": 2012}