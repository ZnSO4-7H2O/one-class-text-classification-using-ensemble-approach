{"title": "An improvement to k-nearest neighbor classifier", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "K-Nearest neighbor classifier (k-NNC) is simple to use and has little design time like finding k values in k-nearest neighbor classifier, hence these are suitable to work with dynamically varying data-sets. There exists some fundamental improvements over the basic k-NNC, like weighted k-nearest neighbors classifier (where weights to nearest neighbors are given based on linear interpolation), using artificially generated training set called bootstrapped training set, etc. These improvements are orthogonal to space reduction and classification time reduction techniques, hence can be coupled with any of them. The paper proposes another improvement to the basic k-NNC where the weights to nearest neighbors are given based on Gaussian distribution (instead of linear interpolation as done in weighted k-NNC) which is also independent of any space reduction and classification time reduction technique. We formally show that our proposed method is closely related to non-parametric density estimation using a Gaussian kernel. We experimentally demonstrate using various standard data-sets that the proposed method is better than the existing ones in most cases.", "text": "abstract k-nearest neighbor classifier simple little design time like finding values k-nearest neighbor classifier hence suitable work dynamically varying data-sets. exists fundamental improvements basic k-nnc like weighted k-nearest neighbors classifier using artificially generated training called bootstrapped training etc. improvements orthogonal space reduction classification time reduction techniques hence coupled them. paper proposes another improvement basic k-nnc weights nearest neighbors given based gaussian distribution also independent space reduction classification time reduction technique. formally show proposed method closely related non-parametric density estimation using gaussian kernel. experimentally demonstrate using various standard data-sets proposed method better existing ones cases. keywords pattern classification nearest neighbor classifier gaussian weights introduction nearest neighbor classifier variants like knearest neighbor classifier simple often shows good performance shown infinite numbers training patterns available k-nnc equivalent bayes classifier error less twice bayes error performance k-nnc larger training better smaller training set. significant design phase hence highly adoptive dynamically varying data sets. hence knnc variants suitable work data mining applications training data sets large. prominent problems nearest neighbor based classifiers space number training patterns requirement classification time also needs store entire training search classify given pattern. space requirement problem solved extent prototype selection reducing dimensionality using feature selection extraction methods using compact representation training data like pc-tree fp-tree cf-tree etc. else using editing techniques reduce training size without affecting performance much. classification time requirement problem solved finding index training like rtree fundamental improvements k-nnc areweighted k-nearest neighbor classifier weight training pattern assigned used classification generating artificial training applying bootstrapping method bootstrapped training place original training set. bootstrap method given hamamoto shown work well nearest neighbor based classifiers. methods coupled space reduction indexing methods. exists literature another bootstrap method used k-nnc vijaya saradhi methods called rest mrest given basically reduces training size improve performance. hence space reduction technique. present paper proposes novel fundamental improvement k-nnc called gaussian weighted k-nearest neighbor classifier gwk-nnc assigns weight neighbors based gaussian probability density function. formally shown that like closely related non-parametric density estimation using gaussian kernel. experimentally using several standard data-sets shown proposed method better cases. further proposed method also independent space reduction classification reduction technique hence easily coupled them. rest paper organized follows. section describes notation definitions used throughout paper. section describes k-nnc hamamoto’s bootstrap method. section deals proposed classifier viz. gaussian weighted k-nearest neighbor classifier section gives empirical results finally section gives conclusions along future directions research. performance classifier test set. classification accuracy accuracy classifier test set. normally measured percentage. percentage patterns test correctly classified classifier. nearest neighbors based classifters nearest neighbor classifier assigns class given test pattern class nearest neighbor training according distance function. test pattern. pattern nearest neighbor. need unique. pattern training equal distance class class assigned test pattern case several patterns qualifying occurs. ties broken arbitrarily. actual class label available test then pattern correctly classified; otherwise wrongly classified. k-nearest neighbor classifier integer generalization nnc. nearest neighbors given test pattern yare found training set. class information nearest neighbors preserved. nearest neighbors nearest neighbors along class information belongs belongs class label assigned example assume occurred time majority vote winner majority vote winner occurs. ties broken arbitrarily. weighted k-nearest neighbor classifier also known modified k-nearest neighbor classifier improvement k-nnc. achieved giving weights nearest neighbors. total weight class found summing weights nearest neighbors belonging class. nearest neighbors particular class total weight assigned class test pattern assigned class maximum total weight. k-nnc seen special case wk-nnc weights nearest neighbors equal another improvement found literature generate artificial training given training set. k-nnc uses artificial training instead originally given training set. process generating training given training often called bootstrapping. bootstrapping method given hamamoto generates bootstrap samples locally combining original training samples. given original training pattern first finds nearest neighbors within class given pattern finds weighted average neighbors. training pattern nearest neighbors class. bootstrapped pattern generated either patterns original considered else random sample taken replacement generating bootstrapped patterns. either chosen equal else randomly weights four different techniques generating assigned value proposed improvement k-nnc section describes proposed improvement k-nnc. proposed improvement orthogonal space reduction techniques classification time reduction techniques. space reduction well classification time reduction techniques applied along proposed improvement. similarly improvement independent bootstrap methods. improved method called gaussian weighted k-nearest neighbor classifier experimentally demonstrated give better classification accuracy applied standard datasets. approximately found non-parametric methods like parzenwindows method assume draw hyper-sphere feature space whose volume keeping centre small enough encompass neighbors nearest neighbors among neighbors maximum. k-nnc hence approximate bayes classifier. reason calling k-nnc non-parametric classifier. intrinsically estimates probability densities non-parametrically. k-nnc class conditional densities found approximately. pattern falls hyper-sphere counted otherwise falls outside counted. shown improvement type density estimation kernel function whose general form ften gaussian kernel functions chosen reasons. first gaussian function smooth hence estimated density function also varies smoothly. second assume special form gaussian family function radially symmetrical function completely specified variance parameter expressed mixture radially symmetrical gaussian kernels thus common variance equation used find class conditional densities used find posterior probabilities. difficulty uses patterns class. index structure like r-tree used select first nearest neighbors efficiently small amount time. useless considering training patterns. hence propose take account patterns nearest neighbors calculating class-conditional density. proposed gaussian weighted k-nearest neighbor classifier similar weighted k-nnc except weight nearest neighbor given according equation classifier assigns class label performed experiments five different data-sets viz. winethyroid pendigits respectively. except data-set others repository dataset also used properties data-sets given table thyroid pendigits data-sets training test sets separately available. wine dataset patterns chosen randomly training patterns remaining test patterns. data-sets numeric valued features. data-sets normalized zero mean unit variance feature. table compares classification accuracies obtained various classifiers. classifiers compared k-nnc wk-nnc k-nnc using hamamoto’s bootstrapping proposed classifier gaussian weighted k-nnc parameters like value value found employing -fold cross validation obtain standard deviation measured experiments done different test sets generated employing sampling replacement given test set. table shows average along standard deviation obtained. conclusions future work paper presented novel improvement conventional k-nnc weights nearest neighbors found based gaussian distribution contrast wknnc weights given neighbors based linear function. proposed method gwk-nnc performs better related classifiers like k-nnc wk-nnc k-nnc using hamamoto’s bootstrap method. future directions research show theoretically reasons behind good performance gwk-nnc carry bias variance analysis proposed method clearly shows part error viz. bias variance reduced gwk-nnc compared related classifiers. belur dasarathy. data mining tasks methods classification nearest-neighbor approaches. handbook data mining knowledge discovery pages oxford university press york q.b. c.a. laszlo r.k.ward. vector quantization technique nonparametric classifier design. ieee transactions pattern analysis machine intelligence zhang tian ramakrishnan raghu livnymicon. birch efficient data clustering method large databases. proceedings sigmod international conference management data pages yoshihiko hamamoto shungi uchimura shingo tomita. bootstrap technique nearest neighbor classifier design. ieee transactions pattern analysis machine intelligence p.viswanathm.narasimhamurty shalabh bhatnagar. partition based pattern synthesis technique efficient algorithms nearest neighbor classification. pattern recognition letters p.viswanath m.narasimha murty satish kambala. efficient parzen-window based network intrusion detector using pattern synthesis technique. sankar editor pattern recognition machine intelligence proceedings premi lncs pages springerverlag berlin heidelberg", "year": 2013}