{"title": "The loss surface and expressivity of deep convolutional neural networks", "tag": ["cs.LG", "cs.AI", "cs.CV", "math.OC", "stat.ML"], "abstract": "We analyze the expressiveness and loss surface of practical deep convolutional neural networks (CNNs) with shared weights and max pooling layers. We show that such CNNs produce linearly independent features at a \"wide\" layer which has more neurons than the number of training samples. This condition holds e.g. for the VGG network. Furthermore, we provide for such wide CNNs necessary and sufficient conditions for global minima with zero training error. For the case where the wide layer is followed by a fully connected layer, we show that almost every critical point of the empirical loss is a global minimum with zero training error. Our analysis suggests that both depth and width are very important in deep learning. While depth brings more representational power and allows the network to learn high level features, width smoothes the optimization landscape of the loss function in the sense that a sufficiently wide network has a well-behaved loss surface with potentially no bad local minima.", "text": "analyze expressiveness loss surface practical deep convolutional neural networks shared weights pooling layers. show cnns produce linearly independent features wide layer neurons number training samples. condition holds e.g. network. furthermore provide wide cnns necessary sufﬁcient conditions global minima zero training error. case wide layer followed fully connected layer show almost every critical point empirical loss global minimum zero training error. analysis suggests depth width important deep learning. depth brings representational power allows network learn high level features width smoothes optimization landscape loss function sense sufﬁciently wide network well-behaved loss surface potentially local minima. well known optimization problem training neural networks exponentially many local minima np-hardness shown many cases however empirically observed training state-of-the-art deep cnns often overparameterized hampered suboptimal local minima. order explain apparent hardness results practical performance many interesting theoretical results recently developed order identify conditions guarantee local search algorithms like gradient descent converge globally optimal solution. however turns approaches either practical require e.g. knowledge data generating measure modiﬁcation network structure objective quite restricted network structures mostly hidden layer networks thus able explain success deep networks general. deep linear networks achieved quite complete picture loss surface shown every local minimum global minimum randomizing nonlinear part feedforward network relu activation function making additional simplifying assumptions choromanska relate loss surface neural networks certain spin glass model. model objective local minima close global optimum number local minima decreases quickly distance global optimum. interesting result based number unrealistic assumptions recently nguyen hein analyzed deep fully connected networks general activation functions could show almost every critical point global minimum layer neurons number training points. result holds networks practice requires quite extensively overparameterized network. paper overcome restriction previous work several ways. paper ﬁrst ones studies deep cnns. cnns high practical interest learn useful representations small number parameters. aware cohen shashua study expressiveness cnns max-pooling layer relu activation rather unrealistic ﬁlters shared weights. setting allow well pooling general activation functions. moreover arbitrary number ﬁlters study general convolutions ﬁlters need applied regular structures like patch-based condition patches size ﬁlter. convolutional layers fully connected layers max-pooling layers combined almost arbitrary order. study paper expressiveness loss surface layer wide sense neurons number training points. assumption sounds ﬁrst quite strong want emphasize network cnns table fulﬁlls condition. show wide cnns produce linearly independent feature representations wide layer thus able training data exactly even true bottom layers chosen randomly probability one. think explains partially results zhang show experimentally several cnns able random labels. moreover provide necessary sufﬁcient conditions global minima zero squared loss show particular class cnns almost critical points globally optimal extent explains wide cnns optimized efﬁciently. ﬁrst introduce notation deﬁnition cnns. number training samples denote rn×d rn×m input resp. output matrix training data input dimension number classes. number layers network layer either convolutional maxpooling fully connected layer. layers indexed corresponds input layer hidden layer output layer. width layer function computes every input feature vector layer convolutional layer consists patches equal length every patch subset neurons layer. throughout paper assume patches every layer cover whole layer i.e. every neuron belongs least patches patches contain exactly subset neurons. means patch covers whole layer must patch layer. number patches resp. size denote patches input layer rlk−×tk denote number convolutional ﬁlters integers integers deﬁnition layer called convolutional layer output deﬁned every value neuron layer computed ﬁrst taking inner product ﬁlter layer patch layer adding bias applying activation function. number neurons layer thus tkpk− denote width layer deﬁnition convolutional layer quite general every patch arbitrary subset neurons layer thus covers existing variants practice. deﬁnition includes fully connected layer special case using rnk− rnk−×nk thus patch whole feature vector layer. reformulation convolutional layers convolutional fully connected layer denote rlk−×tk rnk−×nk linear returns every parameter matrix rlk−×tk corresponding full weight matrix rnk−×nk convolutional layers seen counterpart weight matrix fully connected layers. deﬁne layer fully connected. note mapping depends patch structure convolutional layer example suppose layer convolutional layer determined equation ordering rows calculated means every given particular index patch loop ﬁlters compute corresponding value output unit whole feature vector previous layer. taking inner product h-th assume throughout paper non-linearity output layer. ignoring max-pooling layers moment feature maps written straightforward assumption satisﬁed every neuron convolutional layer belongs least patch identical patches. lemma assumption holds every convolutional layer rlk−×tk rnk−×nk full rank lebesgue measure zero. proof since rnk−×nk linear every entry linear function entries rank matrices sub-matrices zero. determinant polynomial entries matrix thus real analytic function composition analytic functions analytic determinant real analytic function assumption exists least determinants non-zero. thus lemma determinant zero lebesgue measure zero. sub-matrices need rank order rank rank lebesgue measure zero. section show class standard architectures convolutional layers fully connected layers max-pooling layers plus standard activation functions like relu sigmoid softplus able learn linearly independent features hidden layer layer neurons number training samples. assumption training data following. assumption quite weak especially size input patches larger. assumption hold small perturbation training samples {\u0001i}n assumption fulﬁlled dataset lebesgue measure zero. moreover {\u0001i}n chosen arbitrarily small inﬂuence perturbation negligible. main assumptions activation function hidden layers following. exist parameters ﬁrst layers vectors linearly independent. moreover weight matrices full rank every theorem implies large class cnns employed practice convolutional fully connected max-pooling layers standard activation functions like relu sigmoid softplus produce linearly independent features hidden layer width larger number training samples. figure shows example architecture satisﬁes conditions theorem ﬁrst convolutional layer note vectors linearly independent also linearly separable. sense theorem suggests deep wide cnns produce linearly separable features every wide hidden layer. linear separability neural networks recently studied authors show two-hidden-layer fully connected network relu activation function transform training linearly separable approximately preserving distances training data output layer. compared theorem derived cnns wider range activation functions. moreover result shows even linear independence features stronger linear separability. recently nguyen hein shown similar result deep fully connected networks analytic activation functions. note that contrast fully connected networks cnns condition theorem necessarily imply network huge number parameters layers chosen convolutional. particular condition tkpk− fulﬁlled increasing number ﬁlters using large number patches however possible small otherwise condition patches cannot fulﬁlled. interestingly architecture used vgg-net small ﬁlters minimal stride ﬁrst layer thus fulﬁll condition table imagenet. also note state-of-the-art-networks fulﬁll condition table overall theorem seen theoretical support usage small ﬁlters strides practical architectures increases chance achieving linear separability early hidden layers network also reduces total number training parameters. reason linear separability helps discussed section analyze loss surface cnns. note also condition sufﬁcient condition necessary prove results. particular conjecture linear separability might hold less number neurons practical applications. might difﬁcult parameters generate linearly independent features hidden layer? next result shows analytic activation functions e.g. sigmoid softplus used ﬁrst hidden layers network linear independence features layer holds probability even draws parameters ﬁrst layers randomly distribution absolutely continuous w.r.t. lebesgue measure. note theorem much stronger statement theorem shows almost weight conﬁgurations gets linearly independent features wide layer. theorem hold relu activation function analytic function want note approximate relu function arbitrarily well using softplus function analytic function thus theorem applies. open question result holds also relu activation function itself. interesting note theorem explains previous empirical observations. particular czarnecki shown empirically linear separability often obtained already ﬁrst hidden layers trained networks. done attaching linear classiﬁer probe every hidden layer network training whole network backpropagation. fact theorem holds even parameters bottom layers wide layer chosen randomly also line recent empirical observations architectures little loss performance weights initial layers chosen randomly without training application theorem yields following universal ﬁnite sample expressivity cnns. particular deep architecture scalar output perfectly express values scalar-valued function ﬁnite number inputs long width last hidden layer larger number training samples. corollary assumption hold training samples. consider standard scalar output satisﬁes conditions theorem last hidden layer output network given proof since network satisﬁes conditions theorem exist rank fl]t parameters l−)−y holds fl−λ follows fl−λ. pick expressivity neural networks well-studied literature particular universal approximation theorems hidden layer networks recently many results shown deep networks superior shallow networks terms expressiveness results derived fully connected networks seems cohen shashua ﬁrst ones study expressivity cnns. particular show cnns max-pooling relu units universal sense approximate given function size networks unlimited. however number convolutional ﬁlters result grow exponentially number patches allow shared weights result standard feature cnns. corollary shows universal ﬁnite sample expressivity instead universal function approximation even single convolutional layer network perfectly training data long number hidden units smaller number samples. best knowledge ﬁrst result universal ﬁnite sample expressivity large class practical cnns. fully connected networks universal ﬁnite sample expressivity studied zhang nguyen hein hardt show network single hidden layer hidden units express training size number training parameters single hidden layer hidden units scalar output number convolutional ﬁlters length ﬁlter fully connected networks. width hidden layer order fulﬁll condition corollary number training parameters becomes less compared fully connected case. practice almost always typically small integer table width ﬁrst convolutional layer maximum width hidden layers state-of-the-art architectures comparison size imagenet numbers lower bounds true width. obviously practice important network generalizes rather ﬁtting training data. using shared weights sparsity structure cnns seem implicitly regularize model class order achieve good generalization performance. thus even though also random labels noise universal ﬁnite sample expressivity shown corollary seem still able generalize well section restrict analysis least squares loss. however show later network produce exactly target output choice parameters results also extended loss function global minimum attained instance squared hinge-loss analyzed nguyen hein denote space parameters network. ﬁnal training objective given assumption every layer network convolutional layer fully connected layer output layer fully connected. moreover exists hidden layer following holds width layer larger number training samples tkpk− activation functions hidden layers satisfy assumption strictly increasing strictly decreasing differentiable network pyramidal layer till output layer parameters feature vectors layer linearly independent weight matrices layer till output layer full rank. following examine conditions global optimality important note covers almost whole parameter space additional mild condition activation function. lemma assumption hold training sample architecture satisfy assumption layer assume activation functions ﬁrst layers real analytic. lebesgue measure zero. proof also measure zero. thus lebesgue measure zero. next lemma bound objective function terms gradient magnitude w.r.t. weight matrix layer every matrix rm×n σmin σmax denote smallest largest singular value |aij| amax |aij|. equations follows seen function thus layer fully connected thus otherwise layer convolutional note true gradient training objective true optimization parameter case true gradient w.r.t. true parameter matrix consists convolutional ﬁlters computed chain rule please note even though write partial derivatives respect matrix elements resp. matrices dimension resp. following. lemma consider standard deep satisﬁes assumption hidden layer holds next main result motivated fact empirically training over-parameterized neural networks shared weights sparsity structure like cnns seem problems sub-optimal local minima. many cases even training labels completely random local search algorithms like stochastic gradient descent converge solution almost zero training error understand better phenomenon ﬁrst characterize following theorem points parameter space zero loss analyze theorem loss surface special case network. emphasize results hold standard deep cnns convolutional layers shared weights fully connected layers. follows upper bound lemma ∇uk+φ proof suppose ∇uk+φ since holds rank full rank every thus holds σmin σmin every moreover non-zero derivative assumption thus lmin every combined lower bound lemma leads thus lemma shows points covered theorem measure zero mild condition. necessary sufﬁcient condition theorem rather intuitive requires gradient training objective vanish w.r.t. full weight matrix layer regardless architecture layer. turns layer fully connected condition always satisﬁed critical point case obtain every critical point global minimum exact zero training error. shown next theorem consider classiﬁcation task classes. rm×m full rank class encoding matrix e.g. identity matrix training sample whenever belongs class every theorem indicates loss surface type cnns rather simple structure sense almost every critical point global minimum zero training error. remains interesting open problem result transferred case layer also convolutional. case whether layer fully connected might still assume solution zero training error still exists. however note theorem shows points loss zero gradient w.r.t. must zero well. interesting special case theorem network fully connected case results theorem hold without modiﬁcations. corollary seen formal proof implicit assumption used recent work exists global minimum zero training error class fully connected deep wide networks. analyzed expressiveness loss surface cnns realistic practically relevant settings. state-of-the-art networks fulﬁll exactly approximately condition sufﬁciently wide convolutional layer think results help understand current cnns trained effectively. would interesting discuss loss surface cross-entropy loss currently analysis global minimum exist data linearly separable. proof since assumption holds training inputs ﬁrst pick assumption holds ﬁrst layer. given assumption satisﬁed ﬁrst layer selection parameters higher layers done similarly. since deﬁnition convolutional layer includes fully connected layer special case sufﬁcient prove result general convolutional structure. since ﬁrst layer convolutional layer assumption denote rl×t matrix basically true parameter matrices ﬁrst layer corresponding weight matrix rank exists patches different training samples inner product corresponding ﬁlters. assumption holds every thus right hand side formula union ﬁnite number hyperplanes lebesgue measure zero. left hand side follows lemma full rank measure zero. thus left hand side measure zero. since union measure zero sets also measure zero thus complementary rl×t must non-empty choose rl×t since continuous non-constant function exists interval bijective select matrix rl×t select free variable choose sufﬁciently small positive holds every construction show every entry must different every entry indeed compare holds sufﬁcient small last inequality follows three facts. first holds since second chosen values arguments activation function within third since bijective maps different inputs different outputs. since entries already pairwise different other corresponding patches must also different matter patches organized architecture means assumption also holds features layer thus follows similar construction done layer considering output layer input layer obtains exist full rank similar inequality holds layer thus implies holds proved holds every thus follows every layer exists parameters ﬁrst layers patches layer different training samples pairwise different other every moreover except max-pooling layers weight matrices layer chosen full rank. proof theorem rn×nk since deﬁnition convolutional layer includes fully connected layer special case sufﬁcient prove result convolutional structure. theorem’s assumption layer convolutional thus holds deﬁnition every following show exists parameters network rank weight matrices full rank. first observes subnetwork consisting layers input layer till layer satisﬁes conditions lemma thus applying lemma subnetwork obtains exist except max-pooling every layers full rank holds main idea parameters layers pick full rank holds rank deﬁne every construction holds thus union ﬁnite number hyperplanes thus measure zero. denote rlk−×tk parameter matrix contains convolutional ﬁlters layer columns. pick rlk− every holds full rank. note matrix always exists. indeed chosen positive measure columns picked positive measure set. moreover matrices rank measure zero lemma thus always exists least matrix columns belong full rank. rest proof value matrix ﬁxed. free parameter. since continuous non-constant function exist value ﬁxed well. construct convolutional ﬁlters biases layer follows. every deﬁne holds detnn thus ranknn implies rank) thus corresponding feature vectors linearly independent. case argue similarly. difference considers limit particular lead upper bound goes zero goes combined leibniz formula limα→∞ detnn since detnn continuous function exists every holds detnn thus ranknn implies rank) thus feature vectors layer linearly independent. overall shown always exist feature vectors layer linearly independent. moreover chosen weight matrices except max-pooling layers full rank every linear function real analytic real analytic functions closed addition multiplication composition e.g. prop. prop. krantz parks assume activation functions ﬁrst layers real analytic function real analytic function composition real analytic functions. recall deﬁnition fk]t rn×nk output matrix layer training samples. observes rank matrices sub-matrices zero. determinant polynomial entries matrix thus analytic function entries composition analytic functions analytic conclude determinant analytic function network parameters ﬁrst layers. theorem exists least parameters ﬁrst layers determinant functions identically zero thus lemma network parameters determinant zero lebesgue measure zero. sub-matrices need rank order rank follows parameters rank lebesgue measure zero. prove lemma ﬁrst derive standard backpropagation lemma following hadamard product rm×n deﬁned rm×n aijbij. ∂gkj derivative w.r.t. value unit layer evaluated single sample arrange vectors training samples single matrix δk]t rn×nk following lemma slight modiﬁcation lemma provide proof completeness. lemma given hidden layer differentiable. following hold following straightforward inequalities also helpful prove lemma λmin λmax denotes smallest largest eigenvalue matrix. lemma rm×n holds σmaxx σminx every note last conditions fulﬁlled fact assumption subnetwork consisting ﬁrst layers satisﬁes condition theorem thus applying theorem subnetwork obtains exist rank following layers show pick l=k+ main idea make output training samples class become identical layer thus network output. since classes would distinct outputs training samples layer thus make distinct outputs become linearly independent layer always exists weight matrix realizes target output last layer linear assumption. moreover show that except max-pooling layers parameters layers network chosen weight matrices achieve full rank. proof details follows. case holds rank pick since output layer fl−wl proof. case holds rank rm×nl− full rank matrix range. note assumption rn×nl− matrix satisfying whenever belongs class every construction full rank thus pick since layer fully connected assumption thus follows deﬁnition σl−i whenever belongs class construction ﬁrst layers fact training samples belonging class identical output layer since full rank construction pick last layer rm×m class embedding matrix rank easily check fl−wl fl−wl later follows deﬁnition output layer fully connected. verify indeed whenever belongs class moreover since rank rank rank−z) holds case holds rank rm×nk+ matrix range every nk+. rn×nk+ satisﬁes every class pick note matrix invertible chosen full rank. since layer fully connected assumption follows deﬁnition satisfy assumption assumption applying theorem subnetwork training data obtain must exist l=k+ full rank corresponding outputs layer linearly independent. particular rm×nl− corresponding outputs subnetwork holds rank intuitively feeds input layer would output layer combined leads fact feeds input layer would layer output whenever belongs class last pick follows since output layer fully connected holds deﬁnition fl−wl fl−wl. overall shown addition holds rank construction ﬁrst layers. matrices l=k+ full rank construction subnetwork till moreover also full rank since rank rank−z) therefore holds clear network structure satisﬁes conditions theorem every layer fully connected. moreover input patches also satisfy assumption every input patch simply training sample case since identical training samples derives input patches must different other.", "year": 2017}