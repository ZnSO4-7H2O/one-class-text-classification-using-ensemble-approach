{"title": "Geometric Enclosing Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Training model to generate data has increasingly attracted research attention and become important in modern world applications. We propose in this paper a new geometry-based optimization approach to address this problem. Orthogonal to current state-of-the-art density-based approaches, most notably VAE and GAN, we present a fresh new idea that borrows the principle of minimal enclosing ball to train a generator G\\left(\\bz\\right) in such a way that both training and generated data, after being mapped to the feature space, are enclosed in the same sphere. We develop theory to guarantee that the mapping is bijective so that its inverse from feature space to data space results in expressive nonlinear contours to describe the data manifold, hence ensuring data generated are also lying on the data manifold learned from training data. Our model enjoys a nice geometric interpretation, hence termed Geometric Enclosing Networks (GEN), and possesses some key advantages over its rivals, namely simple and easy-to-control optimization formulation, avoidance of mode collapsing and efficiently learn data manifold representation in a completely unsupervised manner. We conducted extensive experiments on synthesis and real-world datasets to illustrate the behaviors, strength and weakness of our proposed GEN, in particular its ability to handle multi-modal data and quality of generated data.", "text": "training model generate data increasingly attracted research attention become important modern world applications. propose paper geometry-based optimization approach address problem. orthogonal current state-of-the-art density-based approaches notably present fresh idea borrows principle minimal enclosing ball train generator training generated data mapped feature space enclosed sphere. develop theory guarantee mapping bijective inverse feature space data space results expressive nonlinear contours describe data manifold hence ensuring data generated also lying data manifold learned training data. model enjoys nice geometric interpretation hence termed geometric enclosing networks possesses advantages rivals namely simple easy-to-control optimization formulation avoidance mode collapsing efﬁciently learn data manifold representation completely unsupervised manner. conducted extensive experiments synthesis realworld datasets illustrate behaviors strength weakness proposed particular ability handle multi-modal data quality generated data. density estimation dominant approach statistical machine learning since inception sound theoretical underpinnings ability explain data. translates estimation distribution ‘close’ possible true unknown data distribution pdata among advantages important consequence approach ability generate data sampling recent years data generation need growing rapidly scale magnitude modern applications. however computational aspect several existing density estimation approaches becomes problematic becomes ‘daring’ idea recent success approach aims generate data directly without estimating analytical forms pioneered notably variational autoencoder generative adversarial nets differences used generate data ﬁrst sampling i.i.d noise space feeding generator parameterized neural distribution values although directly modeled gan’s objective functions minimize suitable distance true pdata. enjoyed enormous recent popularity scalability importantly extremely efﬁcient generate data using generator single shot. nonetheless training still challenging problem taming model generate meaningful outputs still like black art. particular suffers issues convergence mode collapsing several recent attempts address problems. example modifying criteria measure distance pdata yields different variations gans surging interest along line research infogan wasserstein others tried address convergence mode collapsing modifying optimization process kernel methods mature principle broadly applied wide range applications machine learning data analytics. principle kernel methods simple geometric shape feature space mapped back input space forms non-linear contours characterizing data. principle exploited support vector clustering wherein learning minimal enclosing ball feature space induce nonlinear contours capture data manifolds clusters. recent attempts leverage kernel methods generative model. nature works base kernel methods deﬁne maximum mean discrepancy frequentist estimator measure mean square difference statistics sets samples minimizing diminish divergence pdata. paper takes radical departure density estimation view data generation problem. goal remains same train generator used generate data efﬁciently i.i.d lying arbitrary uniform noise space. however unlike existing work approach model relation pdata instead work directly geometric structure data. particular depart original idea support vectors formulate optimization framework ensure generated value contained within data manifold learned training data. approach completely unsupervised hence richly characterize data manifold borrow idea constructing minimal enclosing ball feature space whose theory guarantees inverse mapping data space produces nonlinear contours rich enough describe data manifold high-level intuition learn generator values mapped feature space also enclosed ball consequently guaranteed data manifolds. directly using primal form construct however tractable purpose since need mapping function input space feature space explicit. furthermore function must facilitate efﬁcient parameter estimation procedure backpropagation. overcome obstacle using recent fourier random feature representation proposed approximate explicit ﬁnite-dimensional space. enable efﬁcient learning gradient descent backpropagation idea reparameterization reformulate altogether arrive optimization framework efﬁciently solved stages learn enclosing ball data random feature reparameterization followed backpropagation train generator term approach geometric enclosing networks reﬂect fact spirit approach indeed corresponding geometric intuition. addition using approximate mapping could however potentially result technical problem since theory requires bijective. provide theoretical analysis guarantee approximate mapping bijection. addition construction random feature reparameterization bijection result construction minimal enclosing ball also novel contributions knowledge. conducted experiments demonstrate behaviors proposed approach using synthetic real-world datasets. demonstrate avoid mode collapsing problem result better data generation quality comparison true data visual inspection mnist frey face cifar- celeba datasets. compared implicit density estimation approaches epitomized proposed possesses advantages. first presents orthogonal fresh idea problem data generation geometric intuition opposed density estimation view. second optimization simpler much easier control enjoys maturity optimization ﬁeld hand. third easily avoid mode collapsing problem efﬁciently learn manifold data completely unsupervised manner. lastly opens various promising future work geometry-based approach data generation. related closely technical development model theory support vectors minimal enclosing ball shall brieﬂy describe section original primal form however uses implicit inﬁnitely-dimensional mapping hence impeding backpropagation gradient descent training. overcome obstacle brieﬂy revise fourier random feature representation section minimal enclosing ball optimization given unlabeled training formulated optimization learn data description minimal enclosing ball feature vectors feature input space feature space interest important result minimal enclosing ball feature space mapped back input space generates nonlinear contours interpreted clusters data manifold training set. principle proven able learn nested complicated clusters data manifolds high dimensional space fourier random feature representation mapping implicitly deﬁned inner product evaluated kernel construct explicit representation idea approximate symmetric positive semi-deﬁnite kernel using kernel induced random ﬁnite-dimensional feature mathematical tool behind approximation bochner’s theorem states every shift-invariant p.s.d kernel represented inverse fourier transform proper distribution below popular shift-invariant kernels include gaussian laplacian cauchy. work employ rd×d. choice substituting yields closed-form probability distribution accurately efﬁciently resulting approximate kernel approximate original kernel explicit mapping explicit function hence facilitate learning kernel parameter gradient descent. section develop reparameterized version random feature representation becomes explicit function kernel parameter hence learned data. geometric enclosing networks present proposed section starting high-level geometric intuition. followed detailed description framework algorithm implementation details. finally theoretical analysis presented. strengthen intuition described earlier figure summarizes high-level intuition approach. given training data assume explicit feature input space feature space network ﬁrst learns minimal enclosing ball training data samples random feature space i.e. ball minimal radius encloses next train generator generated samples mapped produce also fall drawn i.i.d arbitrary noise space. long bijective mapping generated must locate contours characterizing true data samples input space. geometrically points feature space including mapped training data generated generator enclosed ball lies unit hypersphere random feature space mapping training data points generated data points also must surface unit hypersphere inside points section unit hypersphere called support vectors. optimization framework consists sub-optimization problems learn explicit mapping minimal enclosing ball ﬁrst step train generator second step detail sections respectively. learning random feature reparameterization recall ﬁrst goal minimal enclosing ball solving optimization problem however primal form feature unknown approach seeks approximate explicit mapping random feature representation although explicit form represented indirectly samples drawn hence still possible learn gradient descent yet. this need direct function kernel parameter adopting reparameterization method proposed shift source randomness kernel re-written eiude center random feature space radius. apply solve optimization problem summarized algorithm worth noting learn diagonal matrix efﬁcient computation. training generator backprop recruit neural network parameterized model generator. noise sample i.i.d sampled noise distribution could arbitrary noise distribution given sample train generator falls ball random feature space obtained previous step. reduces following optimization stretch data manifolds. however minimizing objective function could lead fact generator simply maps small region inside ball induce negligible cost. overcome issue augment objective function using quantity encourages generator produce samples spread ball employ backpropagation learn minimizing augmented objective function. steps presented algorithm ﬁrst phase learn ball using training data mapped onto random feature space. worth noting update variables using mini-batches. however sake simplicity present single-point updates second phase keep ﬁxed ball kernel train generator every random feature image falls ball theoretical analysis follows present theoretical analysis regarding tightness kernel approximation using fourier random feature reparameterization trick conditions original random feature maps bijections. figure geometric structure proposed network explicit mapping formulated learned reparameterized random feature representation together minimal enclosing ball generator trained back propagation image contained within ensures generated sample belongs contours characterizing data manifold input space. points intersection unit sphere called support vectors. representation random feature obvious verify expressed lemma helps construct geometric view gen. lemma every random feature lies unit hypersphere center origin radius able prove bijective feature rank{e rank{e random bijection feature map. theorem reveals order random feature bijection either random feature dimension sufﬁciently large value random feature kernel sufﬁciently approximate original kernel scales data samples experimental results conduct extensive experiments using synthetic real-world datasets demonstrate properties proposed network; particular ability deal multi-modal data distribution model data manifold quality generated samples. unless otherwise speciﬁed experiments stochastic gradient descent used adam optimizer learning rate empirically turned around employed. synthetic data first well deal multiple modes data generate samples drawn mixture univariate gaussians visualized blue curve left figure baseline gan. neural network speciﬁcation generator includes hidden layers softplus units uni. used common setting layer softplus hidden units generator layers tanh hidden units discriminator. figure shows pdfs generated true distribution clearly seen data generated distribute around three mixture components demonstrating ability deal multi-modal data case; whereas expected data generated concentrate single mode reﬂecting known mode collapsing problem gan. addition empirically observe generated stable tends jump ﬂuctuate around x-axis training whereas much stable. next compare quality generated data quantitatively. since know true distribution pdata case employ measures namely symmetric wasserstein distances. measures compute distance normalized histograms generated true pdata. figure clearly demonstrates superiority approach w.r.t. distances; wasserstein metric distance true distribution almost reduces zero. ﬁgure also demonstrates stability training much less ﬂuctuated compared finally figure displays mapping noise space input data space learned conﬁrms vulnerable mode collapse whilst case method. test model beyond univariate case repeat similar setting case mixture gaussian whose means centered respectively. figure conﬁrms model generate data multiple locations whereas stuck mode illustrate idea modeling data manifold synthesize data manifold s-shape shown figure blue points represent true data samples. using samples train generate data shown points. seen generated samples spread across manifold expected. addition discriminator recognize region s-shape experiment real datasets last experiment four real-world image datasets handwritten digits object images human face datasets popular mnist dataset contains images digits trained three subsets including images. noise space dimensions; generator architecture sigmoid output units; random features used construct visually inspected figure digits generated good quality improved training data. note even training images quality already sufﬁciently good. observation concurs synthetic experiments demonstrated figure geometric nature could usually generalize data manifold well moderate training data size. frey face dataset contains approximately images brendan’s face taken sequential frames small video. used smaller network generator dimensions single layer softplus hidden units sigmoid outputs. figure shows faces generated model observe good visual quality. experiments extended generating color images real-life objects human faces resizing original images size used convolutional generator network sigmoid output units trained leaky rectiﬁed linear discriminator network layers random feature number latent dimension respectively. show images generated network figure although generated images blurry contrast contain meaningful objects faces various shapes colors poses. results conﬁrm potential power proposed method generative network. conclusion paper presented approach called geometric enclosing networks construct data generator geometric view. instead examining effectiveness generator density generated data true distribution approach directly captures nonlinear data manifolds input space principle minimal enclosing ball. result model enjoys nice geometric interpretation possesses advantages namely simple easy-to-control optimization formulation avoidance mode collapse efﬁciently learning data manifold representation. established experiments demonstrate behaviors proposed approach using synthetic real-world datasets. experimental results show avoid mode collapsing problem result better data generation quality comparison true data visual inspection mnist frey face cifar- celeba datasets.", "year": 2017}