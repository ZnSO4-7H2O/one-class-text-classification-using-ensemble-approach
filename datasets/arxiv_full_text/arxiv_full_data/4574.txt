{"title": "Approximate Stochastic Subgradient Estimation Training for Support  Vector Machines", "tag": ["cs.LG", "cs.AI"], "abstract": "Subgradient algorithms for training support vector machines have been quite successful for solving large-scale and online learning problems. However, they have been restricted to linear kernels and strongly convex formulations. This paper describes efficient subgradient approaches without such limitations. Our approaches make use of randomized low-dimensional approximations to nonlinear kernels, and minimization of a reduced primal formulation using an algorithm based on robust stochastic approximation, which do not require strong convexity. Experiments illustrate that our approaches produce solutions of comparable prediction accuracy with the solutions acquired from existing SVM solvers, but often in much shorter time. We also suggest efficient prediction schemes that depend only on the dimension of kernel approximation, not on the number of support vectors.", "text": "subgradient algorithms training support vector machines quite successful solving largescale online learning problems. however restricted linear kernels strongly convex formulations. paper describes efﬁcient subgradient approaches without limitations. approaches make randomized low-dimensional approximations nonlinear kernels minimization reduced primal formulation using algorithm based robust stochastic approximation require strong convexity. experiments illustrate approaches produce solutions comparable prediction accuracy solutions acquired existing solvers often much shorter time. also suggest efﬁcient prediction schemes depend dimension kernel approximation number support vectors. support vector machines highly successful machine learning data mining. derivation implementation analysis efﬁcient solution methods svms subject great deal research past years. broadly categorize algorithms proposed follows. decomposition methods based dual formulation including libsvm svm-light gpdt online variant lasvm dual formulation allows nonlinear kernels introduced neatly formulation kernel trick cutting-plane methods using special primal formulations successively violated constraints formulation. ocas handle linear kernels former approach extended nonlinear kernels cpny cpsp subgradient methods particular interest since well suited large-scale online learning problems. iteration methods consists simple computation usually involving tiny subset training data. although large number iterations might required high accuracy solutions solutions moderate accuracy often enough learning purposes. despite beneﬁts subgradient algorithms proposed svms nonlinear kernels mainly lack explicit representations feature mappings interesting kernels required primal formulations. paper aims provide practical subgradient algorithms training svms nonlinear kernels. unlike pegasos vapnik’s original formulation without modifying objective strongly convex. main algorithm takes steplengths size online convex programming rather steplength scheme pegasos. although schemes slower convergence rate theory signiﬁcant performance difference practice methods. discuss later optimal choices tuning parameter objective often lead nearly weakly convex thus nearly breaking assumption underlies derive following result convex analysis showing solution used derive solution result regarded special case representer theorem. proposition solution deﬁne solution nonlinear-kernel formulation lowdimensional approximations nonlinear feature mappings whose dimension chosen users. obtain approximations either approximating gram matrix constructing subspaces random bases approximating feature spaces induced kernels. approximations computed applied data points iteratively thus suited online context. further suggest efﬁcient make predictions test points using approximate feature mappings without recovering potentially large number support vectors. ﬁrst analyze structure primal formulation nonlinear feature mappings. unveil details apply tools convex analysis rigorously rather appealing representer theorem chapelle idea ﬁrst introduced. discuss techniques ﬁnding satisﬁes ﬁrst uses randomized linear algebra calculate lowrank approximation gram matrix second approach uses random projections construct approximate feature mappings explicitly. ﬁrst approach makes nystr¨om sampling idea good approximation speciﬁed rank matrix approach specify integer choose elements random index form subset best rank-d approximation pseudoinverse indicate expectation high probability rank-d approximation obtained process error made close wish best rank-d approximation choosing sufﬁciently large. clearly symmetric positive semideﬁnite proof makes assumption nonsingularity matrix uniqueness solution however suggests without loss generality constrain restricted max{ results clarify connection expansion coefﬁdual variable introduced cient chapelle fully explicated there. similar arguments regression -insensitive loss function max{|d leads choose threshold sample approach requires operations creation factorization assuming evaluation kernel entry takes time. since algorithm requires single iteration computation cost amortized iterations cost iteration corresponding available; otherwise. approximation method less expensive previous approach requiring operations data point time). observe section however approach tends give lower prediction accuracy ﬁrst approach ﬁxed value. given solution describe prediction data point made efﬁciently without recovering support vector coefﬁcient imposed dimensionality approximate kernel approach lead signiﬁcantly lower cost prediction fraction cost exact-kernel approach. subgradient estimate constructed subgradient term summation empirical loss term. table summarizes subgradients classiﬁcation regression tasks hinge loss -insensitive loss functions respectively. feasible sets deﬁne feasible cartesian product ball component interval component. following shows classiﬁcation radius ball derived using strong duality note objective strongly convex convex pegasos requires strongly convex variables thus modiﬁes formulation property. approach describe suitable original formulation. suppose omit intercept linear formulation objective function becomes strongly convex variables. special case apply different steplength achieve faster convergence theory. algorithm remains averaged iterates solution estimated iterates rather weighted ﬁnal iterates. speciﬁcally deﬁne total number iterates used point start averaging ﬁnal reported solution estimate would estimation steplength requires knowledge subgradient estimate deviation deﬁned small random sample training data indexed ﬁrst iterate estimate summarize framework algorithm refer asset. integer speciﬁes iterate algorithm starts averaging iterates average iterates predetermined maximum iteration number output last iterate without averaging number between. approxima nemirovski yudin tion algorithm provides theoretical support above. considering algorithm applied general formulation denoting algorithm’s output following result. algorithm except averaging longer needed faster convergence rate proved essentially rate rather general proof) theorem given output optimal function value algorithm note strong convexity weak convergence approach slow unless well. without intercept feasible simpliﬁed component update steps changed accordingly. resulting algorithm refer asset∗ pegasos except extensions nonlinear kernels. implemented algorithms based open-source pegasos code. refer algorithms kernel matrix approximation assetm asset∗m feature mapping approximation assetf asset∗f interests making direct comparisons codes include intercept terms experiments since codes allow terms used without penalization. experiments load-free -bit linux systems processors memory. kernel cache size applicable. experiments randomness repeated times unless otherwise speciﬁed. table summarizes binary classiﬁcation tasks experiments. adult data randomly split training/validation/test sets. mnist data obtain binary problem classifying digits versus ccat collection original test training divide original training validation test sets. ijcnn constructed random splitting ijcnn challenge data set. covtype binary problem classify type forest cover types. finally mnist-e extended mnist generated elastic deformation original digits. table also indicates values regularization parameter gaussian kernel parameter selected using svm-light solver maximize classiﬁcation accuracy validation set. ﬁrst moderate-size tasks compare algorithms four publicly available codes. cutting-plane methods cpny cpsp implemented version svm-perf. search solution linear combination approximate basis functions approximation based nystr¨om sampling constructing optimal bases comparison codes svm-light solves dual formulation succession small subproblems lasvm makes single pass data selecting pairs examples optimize algorithm. original svm-perf ocas included comparison cannot handle nonlinear kernels. ﬁrst experiment investigates effect kernel approximation dimension classiﬁcation accuracy. dimension parameter section values range eigenvalue threshold note upper bound actual dimension approximation asset equal case asset cpsp cpny parameter similar compared setting parameter values ﬁrst moderate-size tasks algorithms epochs converged near-optimal value small variation among different randomization. obtained baseline performance tasks running svm-light. svm-light figure shows results. since assetm asset∗m yield similar results experiments plot asset∗m. value small figure adult data codes achieve good classiﬁcation performance small dimension. data sets chosen values larger intrinsic rank kernel matrix higher classiﬁcation performance continues improve increases. curacy. however practice specify larger dimension assetf assetm since former requires less computation latter. given dimension overall performance assetf worse methods especially ccat experiment. cutting plane method cpsp generally requires lower dimension others achieve prediction performance. cpsp spends extra time construct optimal basis functions whereas methods depend random sampling. however approximate-kernel methods including cpsp suffer considerably restriction dimension covtype task. table training time test error rate parentheses. kernel approximation dimension varied setting assetm asset∗m cpsp cpny. decomposition methods depend results tables. algorithms default stopping criteria. assetm asset∗m checked classiﬁcation error test sets times epoch terminating error matched performance cpny. test error measured using iterate averaged iterations immediately preceding checkpoint. results ﬁrst data sets shown table methods fastest cases. although best classiﬁcation errors among approximate codes obtained cpsp runtimes cpsp considerably longer methods. fact compare performance assetm cpsp assetm achieves similar test accuracy cpsp faster factor forty. cpny requires abnormally long time adult data set; surmise code affected numerical difﬁculties. noteworthy assetm shows similar performance asset∗m despite less impressive theoretical convergence rate former. values optimal regularization parameter near zero experiments thus objective function lost strong convexity condition required asset∗m work. observed similar slowdown pegasos approaches zero linear svms. figure shows progress single algorithms various approximation dimensions range vertical bars graphs indicate completion training. assetf tends converge faster shows smaller test error values asset∗f despite theoretical slower convergence rate former. assetf asset∗f required hours ﬁnish solution test error rate respectively. lasvm produced better solution test error rate required days computation complete single pass training data. proposed stochastic gradient framework training large-scale online svms using efﬁcient approximations nonlinear kernels. since approach require strong convexity objective function dual reformulations kernelization extended easily kernel-based learning problems. platt. fast training support vector machines using sequential minimal optimization. advances kernel methods support vector learning pages press seraﬁni zanghirati zanni. gradient projection methods large quadratic programs applications training support vector machines. optimization methods software authors acknowledge support grants dms- german research foundation grant collaborative research center providing information resourceconstrained data analysis.", "year": 2011}