{"title": "Non-Deterministic Policy Improvement Stabilizes Approximated  Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "This paper investigates a type of instability that is linked to the greedy policy improvement in approximated reinforcement learning. We show empirically that non-deterministic policy improvement can stabilize methods like LSPI by controlling the improvements' stochasticity. Additionally we show that a suitable representation of the value function also stabilizes the solution to some degree. The presented approach is simple and should also be easily transferable to more sophisticated algorithms like deep reinforcement learning.", "text": "wendelin b¨ohmer∗ rong klaus obermayer neural information processing group technische universit¨at berlin marchstraße berlin germany. corresponding author paper investigates type instability linked greedy policy improvement approximated reinforcement learning. show empirically non-deterministic policy improvement stabilize methods like lspi controlling improvements’ stochasticity. additionally show suitable representation value function also stabilizes solution degree. presented approach simple also easily transferable sophisticated algorithms like deep reinforcement learning. keywords provement least-squares policy iteration slow-feature-analysis representation paper investigates type instability linked greedy policy improvement approximated reinforcement learning. show empirically non-deterministic policy improvement used achieve stability large discount factors. presented approach simple also easily transferable sophisticated algorithms. recently deep reinforcement learning successful solving complex tasks large often continuous state spaces approaches gradient based q-learning policy gradient methods gradients neural networks must based i.i.d. distributed samples though deep uses therefore mini-batches sampled i.i.d. ﬁxed experiences collected training diﬀerence online algorithms often guaranteed converge limit inﬁnite training sequence batch learning long known vulnerable choice training sets depending batch training samples hand algorithm either converge almost optimal arbitrarily policy. practice depends strongly discount factor example figure demonstrate figure navigation performance policies learned lspi environments varying discount factors error bars indicate mean standard deviation fraction successful test-trajectories random-walk training sets samples each. agent either move forward rotate left right reaching goal area rewarded crashing wall punished policies learned least-squares policy iteration yield diﬀerent performances discount factor varied left plot shows unpredictable drop performance simple navigation experiment continuous states discrete actions right plot failure lspi learn suitable policy complicated environment. young discipline deep reported eﬀects like these reasonable assume happen batch algorithms sophisticated architectures well. authors attribute instability lack convergence guarantees oﬀ-policy batch value estimation distribution training samples batch also profound impact policy improvement approximate example perkins precup show algorithm similar lspi greedy policy improvement cause instability shown figure although analysis carry lspi show sequence non-deterministic policies converge reliably changed slowly enough. conservative policy iteration follows similar line thought slows policy improvement considerably guarantee convergence. safe policy iteration extends concept determining speed perkins precup open-ended on-policy online value estimation. training samples drawn every time policy improved errors observed samples thus average time. next policy combination previous policy greedy policy i.e. converges small update rate determined maximizing lower bound policy improvement converges much faster cpi. change lower bound policy improvement. algorithm improves convergence speed signiﬁcantly computationally expensive even ﬁnite state spaces. approaches suggest actor-critic architecture avoid oscillations optimize parameterizable softmax-policy directly paper evaluate idea perkins precup empirically lspi continuous navigation tasks. surprisingly stochasticity improved policy stabilizes solution rather slowness policy change. requires small modiﬁcation policy improvement scheme. although approach heuristic theoretically well-grounded algorithms fast simple implement applied algorithms used deep evaluated algorithm estimate corresponding q-value function converge optimal policy policy must also improved either q-value estimation additional step. improvement state usually chooses action maximizes current q-value estimate instead greedy improvement propose produce improved non-deterministic policy. examples softmax denotes inverse stochasticity operator. example nondeterministic version td-error q-learning observation ˆγβ−q matrix irm×m inverted during non-deterministic least-squares temporal diﬀerence learning would computed training batch rt}n softmax policies information ǫ-greedy situations better choice. however stochasticity softmax depends strongly diﬀerences between q-values. away reward q-values become similar softmax policies become almost uniform distributions. level stochasticity turns reliable stabilizer lspi used experiments normalized q-values non-deterministic policy improvement ˆγβ. normalizes stochasticity states normalizing diﬀerence q-values evaluated eﬀects non-deterministic policy improvement example simple navigation experiment us-shaped environment three dimensional state space consisted agent’s two-dimensional position orientation. action space contained actions forward movement rotations. crashing wall stopped movement would take agent unimpeded moves traverse environment spatial dimension. reaching goal area yielded reward crashes incurred punishment u-shaped s-shaped environment. represent q-value function chose fourier basis constructed basis functions space states actions. bases contained combinations cosine functions spatial dimension; constant cosine sine functions orientation; discrete kronecker-delta functions actions. irrespective policy improvement policies evaluated greedily remain comparable. performance measured fraction successful trajectories estimated running greedy policy random starting positions/orientations. successful trajectories reach goal within actions without hitting wall. started test idea perkins precup lspi using nondeterministic policy improvement slowly growing inverse stochasticity however observed annealing process improve learned policy. performance always comparable soft-lspi annealing’s ﬁnal stochasticity figure plots performance greedy-lspi soft-lspi varying discount factors face sparse rewards determines reward propagated drowned inevitable approximation errors. yields policies correct close reward therefore performance. hand close lead nearly optimal policies everywhere performance strongly aﬀected instability investigated paper. note large standard deviations plots stem training sets producing near optimal others producing nonsensical policies. reason refer regimes instable. first observe increasing stochasticity drastically stabilizes soft-lspi policies. secondly note seems trade-oﬀ inverse stochasticity discount factor reduces performance increasing stability left plot performance becomes near optimal larger figure lspi greedy softmax policy improvement compared navigation tasks figure large standard deviations usually caused mixture excellent horrible policies. therefore call regimes instable. stochastic improvements decrease performance small stabilize convergence large signiﬁcantly. figure lspi policies based diﬀerent representations navigation tasks figure better representations generally improve performance non-deterministic policy improvement still needed stabilize lspi complex tasks also note pronounced trade-oﬀ instabilities demonstrated lspi. could argue sophisticated approaches must aﬀected way. deep neural networks example lower layers provide representation state-action space stabilizes policy improvement. want investigate choosing basis functions known represent value functions well. b¨ohmer show features learned non-linear slow feature analysis approximate optimal encoding value functions tasks environment. used regularized sparse kernel gaussian kernels learn features training data. figure shows results comparison trigonometric fourier basis functions introduced above. using representation completely avoided instability simpler task left plot performance improves s-shaped environment large standard deviations indicate greedy lspi stable large discount factors soft-lspi stabilizes solution though. using deep architecture therefore reduce instability probably remove together. nonetheless results suggest non-deterministic policy improvement able stabilize deep architectures too. shown lspi become instable unpredictable regimes discount factor small diﬀerences training lead large diﬀerences policy performance. exactly clear solutions become instable show learned policies stabilized using non-deterministic policy improvement scheme. presented experiments became signiﬁcantly stable increasing stochasticity discount factor time. future works extend approach adjusting parameters policy iteration better representations state-action space also improved stability extend. sophisticated approaches learn representations implicitly lower layers therefore stable lspi. nonetheless instabilities probably occur non-deterministic policy improvement likely employed stabilize learned policy deep too. conclusion success failure learned policies depends crucially training consider non-deterministic policy improvement scheme. scheme presented paper computationally cheap easy implement ﬁne-tuned inverse stochasticity", "year": 2016}