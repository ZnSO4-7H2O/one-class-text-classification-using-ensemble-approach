{"title": "Deep metric learning using Triplet network", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.", "text": "deep learning proven successful models learning useful semantic representations data. these however mostly implicitly learned part classiﬁcation task. paper propose triplet network model aims learn useful representations distance comparisons. similar model deﬁned wang tailor made learning ranking image information retrieval. demonstrate using various datasets model learns better representation immediate competitor siamese network. also discuss future possible usage framework unsupervised learning. past years deep learning models used extensively solve various machine learning tasks. underlying assumptions deep hierarchical models convolutional networks create useful representation data hinton used distinguish available classes. quality contrast traditional approaches requiring engineered features extracted data used separate learning schemes. features extracted deep networks also shown provide useful representation sermanet turn successfully used tasks despite importance representations corresponding induced metrics often treated side effects classiﬁcation task rather explicitly sought. also many interesting open question regarding intermediate representations role disentangling explaining data notable exceptions explicit metric learning preformed siamese network variants chopra hadsell contrastive loss metric induced representation used train network distinguish similar dissimilar pairs examples. contrastive loss favours small distance pairs examples labeled similar large distances pairs labeled dissimilar. however representations learned models provide sub-par results used features classiﬁcation compared deep learning models including ours. siamese networks also sensitive calibration sense notion similarity dissimilarity requires context. example person might deemed similar another person dataset random objects provided might deemed dissimilar respect person wish distinguish individuals individuals only. model calibration required. fact experiments here experienced hands difﬁculty using siamese networks. follow similar task chechik samples chosen rough similarity measure given training oracle wish learn similarity function induced normed metric. unlike experiment metric embedding multi-class labeled dataset. always take class different class although general complicated choices could made. accordingly notation instead focus ﬁnding embedding learning function inspired recent success deep learning deep network embedding function call approach triplet network. similar approach proposed wang purpose learning ranking function image retrieval. compared single application proposed wang make comprehensive study triplet architecture shall argue below interesting itself. fact shall demonstrate triplet approach strong competitor siamese approach obvious competitor. triplet network comprised instances feedforward network samples network outputs intermediate values distances embedded representation inputs representation third. denote inputs embedded representation network last layer vector training preformed feeding network samples where explained above class different class. network architecture allows task expressed -class classiﬁcation problem objective correctly classify class stress general setting objective might learn metric embedding label determines example closer simply interpret closeness sharing label. order output comparison operator model softmax function applied outputs effectively creating ratio measure. similarly traditional convolutional-networks training done simple negative-loglikelihood loss regard -class problem. later examined better results achieved loss function replaced simple soft-max result compared vector loss experimented datasets. ﬁrst cifar consisting color images classes second dataset original mnist consisting gray-scale images handwritten digits corresponding test images. third street-view-house-numbers netzer consisting color images house-number digits fourth dataset coates similar cifar consisting object classes training images bigger image size. important note data augmentation whitening applied preprocessing global normalization zero mean unit variance. training instance uniformly sampled images class third different class. training epoch consisted instances ﬁxed instances used test. emphasize test instance involves images test images excluded training. cifar svhn used convolutional network consisting convolutional max-pooling layers followed fourth convolutional layer. relu non-linearity applied consecutive layers. network conﬁguration consists ﬁlter sizes feature dimensions vector ﬁnal embedded representation network. usually convolutional networks subsequent fully-connected layer used classiﬁcation. layer removed interested feature embedding only. training datasets done initial learning-rate learning rate decay regime. used momentum value also used dropout regularization technique avoid over-ﬁtting. training dataset epochs network reached ﬁxed error triplet comparisons. used embedding network extract features full dataset trained simple -layer network model full -class classiﬁcation task test measured accuracy. results comparable state-of-the-art results deep learning models without using artiﬁcial data augmentation goodfellow noteworthy dataset tripletnet achieved best known result non-augmented data. conjecture data augmentation techniques provide similar beneﬁts described previous works. also note similar results achieved embedded representations classiﬁed using linear model classiﬁcation deviance results figure another side-affect noticed representation seems sparse non-zero values. helpful used later features classiﬁcation computationally respect accuracy class characterised zero elements. order examine main premise network embeds images representation meaningful properties project embedding euclidean space easily visualized signiﬁcant clustering semantic meaning conﬁrming network useful embedding images euclidean space according content. similarity objects easily found measuring distance embedding shown results reach high classiﬁcation accuracy using simple subsequent linear classiﬁer. siamese network obvious competitor approach. implementation siamese network consisted embedding network contrastive loss pair samples instead three generated features used classiﬁcation using similar linear model used tripletnet method. measured lower accuracy mnist dataset compared results gained using tripletnet representations tried similar comparison three datasets unfortunately could obtain meaningful result using siamese network. conjecture might related problem context described above leave resolution conjecture future work. triplet model allows learning comparisons samples instead direct data labels usage unsupervised learning model possible. future investigations performed several scenarios using spatial information. objects image patches spatially near also expected similar semantic perspective. therefore could geometric distance patches image rough similarity oracle unsupervised setting. using temporal information. applicable time domain consecutive video frames expected describe object frame taken minutes later less likely triplet provide better embedding improve past attempts solving classiﬁcation tasks unsupervised environment also well known humans tend better accurately providing comparative labels. framework used crowd sourcing learning environment. compared tamuz used different approach. furthermore easier collect data trainable triplet network comparisons similarity measures much easier attain work introduced triplet network model tool uses deep network learn useful representation explicitly. results shown various datasets provide evidence representations learned useful classiﬁcation comparable network trained explicitly classify samples. believe enhancement embedding network network-in-network model inception models others beneﬁt triplet similarly beneﬁted classiﬁcation tasks. considering fact method requires know three images sampled class rather knowing class think inquired further provide insights deep networks learn general. also shown model learns using comparative measures instead labels future leverage data sources clear labels known make sense bromley jane bentz james bottou l´eon guyon isabelle lecun yann moore cliff s¨ackinger eduard shah roopak. signature veriﬁcation using siamese time delay neural network. international journal pattern recognition artiﬁcial intelligence coates adam andrew honglak. analysis single-layer networks unsupervised feature learning. international conference artiﬁcial intelligence statistics hadsell raia chopra sumit lecun yann. dimensionality reduction learning invariant mapping. computer vision pattern recognition ieee computer society conference volume ieee mobahi hossein collobert ronan weston jason. deep learning temporal coherence video. proceedings annual international conference machine learning razavian sharif azizpour hossein sullivan josephine carlsson stefan. features off-the-shelf astounding baseline recognition. arxiv http//arxiv. org/abs/.. sermanet pierre eigen david zhang xiang mathieu michael fergus lecun yann. overfeat integrated recognition localization detection using convolutional networks. arxiv preprint arxiv. http//arxiv.org/abs/. szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. going deeper convolutions. corr abs/. http//arxiv.org/abs/.. tamuz omer belongie serge shamir ohad kalai adam. adaptively learning crowd kernel. getoor lise scheffer tobias proceedings international conference machine learning icml york june acm. isbn ----.", "year": 2014}