{"title": "Thompson Sampling For Stochastic Bandits with Graph Feedback", "tag": ["cs.LG", "cs.AI"], "abstract": "We present a novel extension of Thompson Sampling for stochastic sequential decision problems with graph feedback, even when the graph structure itself is unknown and/or changing. We provide theoretical guarantees on the Bayesian regret of the algorithm, linking its performance to the underlying properties of the graph. Thompson Sampling has the advantage of being applicable without the need to construct complicated upper confidence bounds for different problems. We illustrate its performance through extensive experimental results on real and simulated networks with graph feedback. More specifically, we tested our algorithms on power law, planted partitions and Erdo's-Renyi graphs, as well as on graphs derived from Facebook and Flixster data. These all show that our algorithms clearly outperform related methods that employ upper confidence bounds, even if the latter use more information about the graph.", "text": "demonstrated performance exceeding state art. prompted surge interest thompson sampling ﬁrst theoretical results industrial adoption appearing recently. however still theoretical results many simplest settings. however easy implement effective many different settings complex feedback structures thus great need extend theoretical results wider settings. russo argue thompson sampling effective versatile strategy different information structures. paper focuses speciﬁc examples extreme cases full information mentioned case linear bandits combinatorial feedback. consider case feedback deﬁned graph speciﬁcally arms vertices graph chosen reward well neighbours. hand clean model theoretical experimental analysis hand also corresponds realistic settings social networks example advertisement settings provide problem-independent regret bound parametrized clique cover number graph naturally generalizes extreme cases zero full information. present variants thompson sampling easy implement computationally efﬁcient. ﬁrst straightforward thompson sampling draws according probability best also uses graph feedback update posterior distribution. second seen sampling cliques graph according probability containing best choosing empirically best clique. neither algorithm requires knowledge complete graph. almost previous algorithms require full structure feedback graph order operate. require entire graph performing updates round others actually need description graph beginning round make present novel extension thompson sampling stochastic sequential decision problems graph feedback even graph structure unknown and/or changing. provide theoretical guarantees bayesian regret algorithm linking performance underlying properties graph. thompson sampling advantage applicable without need construct complicated upper conﬁdence bounds different problems. illustrate performance extensive experimental results real simulated networks graph feedback. speciﬁcally tested algorithms power planted partitions erd˝os–r´enyi graphs well graphs derived facebook flixster data. show algorithms clearly outperform related methods employ upper conﬁdence bounds even latter information graph. sequential decision making problems uncertainty appear modern applications automated experimental design recommendation systems optimisation. common structure applications that time step decision-making agent faced choice. decision obtains problem-dependent feedback so-called bandit problem choices different arms feedback consists single scalar reward obtained time prediction problem obtains reward chosen also observes rewards choices time cases problem maximise total reward obtained time. however dealing speciﬁc types feedback require specialised algorithms. paper show thompson sampling algorithm applied successfully range sequential decision problems whose feedback structure characterised graph. algorithm extension thompson sampling introduced although easy implement effective practice remained unpopular until relatively recently. interest grew empirical studies copyright association advancement artiﬁcial intelligence rights reserved. graph feedback model model assume existence undirected graph vertices corresponding arms. taking receive reward played also observe rewards neighbouring arms precisely time-step observe reward still graph empty setting equivalent bandit problem. graph fully connected equivalent prediction problem. however many practical graphs derived social networks intermediate connectivity. cases amount information obtain picking characterised graph properties clique cover number deﬁnition clique covering graph partition vertices sets sub-graph formed clique i.e. vertices connected smallest number cliques nodes partitioned called clique cover number. denote minimum clique cover size omitting clear context. optimal policies stochastic multi-armed bandit problem ﬁrst characterised index-based optimal policies general non-parametric problems given later proved ﬁnite-time regret bounds number index policies proved ﬁnite-time bounds index policies similar problem-dependent bounds recently number policies based sampling posterior distribution analysed frequentist bayesian setting shown obtain order regret bound stochastic case. adversarial bandit problem bounds order analysis full information case generally results bounds regret i.e. much lower dependence number arms. decision almost none algorithms previously proposed literature able provide non-trivial regret guarantees without feedback graphs disclosed. however cohen hazan koren argue assumption entire observation system revealed learner round even making prediction rather unnatural. principle learner need even aware fact graph underlying feedback model; feedback graph merely technical notion specify observations possible arms. ideally signal would like agent receive following round observations corresponds taken round algorithms work setup need whole graph disclosed either selecting updating beliefs local neighborhood needed. furthermore underlying graph allowed change arbitrarily step. detailed proofs main results available full version paper. stochastic bandit model stochastic k-armed bandit problem well known sequential decision problem involving agent sequentially choosing among arms round agent plays receives reward ytat random variable deﬁned probability space reward function. mean reward goal maximize expected cumulative reward rounds. equivalent notion minimize expected regret oracle knows formally expected regret agent policy bandit problem deﬁned maxi∈v mean optimal policy agent deﬁning probability distribution next given history previous arms rewards. main challenge model agent know observes reward played. consequence agent must trade-off exploitation exploration bayesian setting offers natural model uncertainty assuming underlying probability parametrised deﬁne prior probability distribution case deﬁne bayesian regret intermediate cases full information bandit feedback obtained graph feedback introduced focus paper. particular analysed graph feedback problems stochastic adversarial reward sequences respectively. speciﬁcally caron analysed variants upper conﬁdence bound policies obtained problem-dependent bounds. recent work also introduced algorithms graphs structure never fully revealed showing large regret adversarial stochastic cases. particular show adversarial setting cannot better ignore additional feedback provide action-elimination algorithm stochastic setting. finally obtain problem-dependent bound form linear programming relaxation minimum degree contributions. paper provide much simpler strategies based thompson sampling matching regret bound. unlike previous work also applicable graphs whose structure unknown changing time. speciﬁcally extend graph-structured feedback obtain problem-independent bound consider algorithms based thompson sampling. ﬁrst uses standard thompson sampling select arms. also reveals rewards neighbouring arms posterior conditioned well. second algorithm uses thompson sampling select chooses empirically best within arm’s clique. ts-n policy ts-n policy adaptation thompson sampling graph-structured feedback. thompson sampling maintains distribution problem parameters. step selects according probability mean largest. observes rewards uses update probability distribution parameters. case reward generated bernoulli distributions. simply beta prior illustrated ts-n policy algorithm note algorithm trivially extends priors families. ts-maxn policy ts-n policy fully exploit graphical structure. example noted instead exploration could explore apparently better neighbour would give information. precisely instead picking pick best empirical mean. intuition behind that take going observe anyway reward always better exploit best resulting policy ts-maxn summarized algorithm although theoretical results apply policy better performance uses information. analysis ts-n policy russo introduced elegant approach analysis thompson sampling. deﬁne information ratio quantity analysing information structures proposition almost surely then denotes entropy. thus analyse performance thompson sampling speciﬁc problem focus bounding information ratio k-armed bandit case show full-information case show give simple useful extension results intermediate cases. proposition equivalence relation deﬁned arms denoting equivalence class sequence random variables |k/≡ half number equivalence classes. direct generalisation propositions reduces equivalence relation trivial full lemma graph corresponding arms suppose played observe rewards observe rewards corresponding neighbours. clique cover i.e. partition cliques. applying proposition lemma performance guarantee thompson sampling graphstructured feedback theorem thompson sampling feedback graph clique cover number remark bandit expert cases special cases corresponding empty graph complete graph respectively since empty graph complete graph. remark planted partition models stochastic block models graphs deﬁned follows ﬁrst ﬁxed partition vertices parts chosen edge vertices within class exists probability vertices different classes exists probability independently high probability clique cover number resulting graph thus class graphs regret grows theorem explored section large planted partition graph considered good model structure network communities. underlying graph changes time step compared proposed algorithms terms actual expected regret number algorithms take advantage graph structure. comparison performed synthetic graphs networks derived real-world data. algorithms hyperparameters. experiments tested ucb-maxn ucb-n algorithms introduced analogues algorithms using upper conﬁdence bounds instead thompson sampling. \u0001-greedy-d \u0001-greedy-lp. real-world networks also evaluated algorithms variant \u0001-greedy-lp based linear program formulation ﬁnding lower bound size minimum dominating set. observe ﬁrst analysis holds ﬁxed dominating bound obtained particular simple greedy algorithm compute near–optimal dominating maximum degree graph. using near optimal dominating place relaxation choosing arms uniformly random obtain variant original algorithm call \u0001-greedy-d much computationally efﬁcient enjoys similar regret bound theorem regret \u0001-greedy-d \u0001-greedy-d \u0001-greedy-lp hyper-parameters control amount exploration. found performance highly sensitive choice. experiments optimal values parameters performing separate grid search problem reporting best results. since obvious tune parameters online leads favourable bias results algorithm. similar observation made noted optimally tuned \u0001-greedy performs almost always best performance degrade signiﬁcantly parameters changed. although suggests method selecting parameters using leads near-linear regret. general experimental setup. experiments performed independent trials reported median-of-means estimator cumulative regret. partitions trials equal groups return median sample means group. number groups conﬁdence interval holds probability least also reported deviation algorithm using gini’s mean difference j-th order statistics sample shown provides superior approximation true deviation standard one. account fact cumulative regret algorithms might follow symmetric distribution computed separately values median-of-means. simulated graphs synthetic problems unless otherwise stated rewards drawn bernoulli distribution whose mean generated uniformly randomly except optimal whose mean generated randomly number nodes graph tested sparse graph edges also dense graph edges. ﬁrst observation policies take advantage large number edges cumulative regret better using dense graph rather sparse conﬁrms theoretical result dense graph smaller clique cover number sparse one. policy ts-maxn outperforms sparse dense graph model. however performance ts-n close ts-maxn near complete graph. explained fact near complete graph many cliques. revealing ts-n outperforms ucb-n ucb-maxn policies. power graph graphs commonly used generate static scale-free networks experiment generated non-growing random graph expected power-law degree distribution. show results respectively dense sparse graph figure show results respectively dense sparse graph. again policy ts-maxn clearly outperforms other. sparse graph model ts-n beaten ucb-maxn beginning rounds catches ended beating ucb-maxn. planted partition model experiment model check dependency number cliques policy. figure shows results x-axis parameter planted partition graph graph nodes. y-axis relative regret policy i.e. ratio regret policy regret best policy groups ease comparison. methods’ regret scales similarly. thus theoretical bounds appear hold practice somewhat pessimistic. larger number nodes would expect plots ﬂatten later. social networks datasets experiments real world datasets follow methodology described ﬁrst infer graph data deﬁne reward function movie recommendation user ratings. missing ratings predicted using matrix factorization. enables generate rewards graph. explain datasets reward function graph inference full version. results figure shows results facebook graph figure flixster graph. again thompson sampling strategies dominate strategies facebook matched optimised \u0001-greedy-d policy flixster graph. notice setting policies rest much larger overall regret policies. attributed larger size graphs. presented ﬁrst thompson sampling algorithms sequential decision problems graph feedback observe reward select also neighbouring arms graph. thus graph feedback allows ﬂexibility model different types feedback information bandit feedback expert feedback. since structure graph need known advance algorithms directly applicable problems changing and/or unknown topology. analysis leverages information-theoretic construction bounding expected information gain terms fundamental graph properties. although problem-independent bound directly comparable believe problem-independent version latter case results would represent improvement practice variants always outperform ucb-n ucb-maxn also graph feedback rely upper conﬁdence bounds. also favourably compared \u0001-greedy-d even tune parameters latter post hoc. would interesting extend techniques types feedback. example bayesian foundations thompson sampling render algorithms applicable arbitrary dependencies arms. future work analytically experimentally consider problems acknowledgments. research partially supported snsf grants adaptive control approximate bayesian computation differential privacy swiss sense synergy people programme european union’s seventh framework programme grant agreement number future life institute.", "year": 2017}