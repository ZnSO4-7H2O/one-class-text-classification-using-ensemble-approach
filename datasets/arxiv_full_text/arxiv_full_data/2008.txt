{"title": "SparkNet: Training Deep Networks in Spark", "tag": ["stat.ML", "cs.DC", "cs.LG", "cs.NE", "math.OC"], "abstract": "Training deep networks is a time-consuming process, with networks for object recognition often requiring multiple days to train. For this reason, leveraging the resources of a cluster to speed up training is an important area of work. However, widely-popular batch-processing computational frameworks like MapReduce and Spark were not designed to support the asynchronous and communication-intensive workloads of existing distributed deep learning systems. We introduce SparkNet, a framework for training deep networks in Spark. Our implementation includes a convenient interface for reading data from Spark RDDs, a Scala interface to the Caffe deep learning framework, and a lightweight multi-dimensional tensor library. Using a simple parallelization scheme for stochastic gradient descent, SparkNet scales well with the cluster size and tolerates very high-latency communication. Furthermore, it is easy to deploy and use with no parameter tuning, and it is compatible with existing Caffe models. We quantify the dependence of the speedup obtained by SparkNet on the number of machines, the communication frequency, and the cluster's communication overhead, and we benchmark our system's performance on the ImageNet dataset.", "text": "philipp moritz∗ robert nishihara∗ stoica michael jordan electrical engineering computer science university california berkeley {pcmoritzrknistoicajordan}eecs.berkeley.edu training deep networks time-consuming process networks object recognition often requiring multiple days train. reason leveraging resources cluster speed training important area work. however widely-popular batch-processing computational frameworks like mapreduce spark designed support asynchronous communication-intensive workloads existing distributed deep learning systems. introduce sparknet framework training deep networks spark. implementation includes convenient interface reading data spark rdds scala interface caffe deep learning framework lightweight multidimensional tensor library. using simple parallelization scheme stochastic gradient descent sparknet scales well cluster size tolerates high-latency communication. furthermore easy deploy parameter tuning compatible existing caffe models. quantify dependence speedup obtained sparknet number machines communication frequency cluster’s communication overhead benchmark system’s performance imagenet dataset. deep learning advanced state number application domains. many recent advances involve ﬁtting large models larger datasets given scale optimization problems training timeconsuming often requiring multiple days single using stochastic gradient descent reason much effort devoted leveraging computational resources cluster speed training deep networks many attempts speed training deep networks rely asynchronous lock-free optimization paradigm uses parameter server model master nodes hold latest model parameters memory serve worker nodes upon request. nodes compute gradients respect parameters minibatch drawn local data shard. gradients shipped back server updates model parameters. time batch-processing frameworks enjoy widespread usage gaining popularity. beginning mapreduce number frameworks distributed computing emerged make easier write distributed programs leverage resources cluster frameworks greatly simpliﬁed many large-scale data analytics tasks. however state-of-the-art deep learning systems rely custom implementations facilitate asynchronous communication-intensive workloads. reason popular batch-processing frameworks designed support workloads existing deep learning systems. sparknet implements scalable distributed algorithm training deep networks lends batch computational frameworks mapreduce spark works well out-of-the-box bandwidth-limited environments. beneﬁts integrating model training existing batch frameworks numerous. much difﬁculty applying machine learning obtaining cleaning processing data well deploying models serving predictions. reason convenient integrate model training existing data-processing pipelines engineered today’s distributed computational environments. furthermore approach allows data kept memory start ﬁnish whereas segmented approach requires writing disk operations. user wishes train deep network output query output graph computation feed resulting predictions distributed visualization tool done conveniently within single computational framework. emphasize hardware requirements approach minimal. whereas many approaches distributed training deep networks involve heavy communication approach gracefully handles bandwidth-limited setting also taking advantage clusters low-latency communication. reason easily deploy algorithm clusters optimized communication. implementation works well out-of-the ﬁve-node cluster broadcasting collecting model parameters takes order seconds performing single minibatch gradient computation requires seconds achieve providing simple algorithm parallelizing involves minimal communication lends straightforward implementation batch computational frameworks. goal outperform custom computational frameworks rather propose system easily implemented popular batch frameworks performs nearly well accomplished specialized frameworks. rddlayer) rddlayer) convlayer kernel= numfilters=) poollayer pool=max kernel= stride=) convlayer kernel= numfilters=) poollayer pool=max kernel= stride=) linearlayer numoutputs=) activationlayer activation=relu) linearlayer numoutputs=) softmaxwithloss) class wraps caffe exposes simple containing methods shown listing netparams type speciﬁes network architecture weightcollection type layer names lists weights. allows manipulation network components storage weights outputs individual layers. facilitate manipulation data weights without copying memory caffe implement ndarray class lightweight multi-dimensional tensor library. beneﬁt building caffe existing caffe model deﬁnition solver automatically compatible sparknet. large community developing caffe models extensions easily used sparknet. building spark inherit advantages modern batch computational frameworks. include high-throughput loading preprocessing data ability keep data memory operations. listing give example network architectures speciﬁed sparknet. addition model speciﬁcations weights loaded directly caffe ﬁles. example sketch code uses perform distributed training given listing perform well bandwidth-limited environments recommend parallelization scheme requires minimal communication. approach speciﬁc sgd. indeed sparknet works caffe solver. parallelization scheme described listing spark consists single master node number worker nodes. data split among spark workers. every iteration spark master broadcasts model parameters worker. worker runs model subset data ﬁxed number iterations ﬁxed length time resulting model parameters worker sent master averaged form model parameters. recommend initializing network running small number iterations master. similar sophisticated approach parallelizing minimal communication overhead discussed zhang standard approach parallelizing gradient computation requires broadcasting collecting model parameters every update occurs tens thousands times training. cluster broadcast collection takes twenty seconds putting bound speedup expected using approach without better hardware without partitioning models across machines. approach broadcasts collects parameters factor times less number iterations. experiments values seem work well. note caffe supports parallelism across multiple gpus within single node. competing form parallelism rather complementary one. experiments caffe handle parallelism within single node parallelization scheme described listing handle parallelism across nodes. section benchmark performance sparknet measure speedup system obtains relative training single node. however outcomes experiments depend number different factors. addition depend communication overhead cluster section instructive measure speedup idealized case zero communication overhead idealized model gives upper bound maximum speedup could hope obtain real-world cluster allows build model speedup function benchmarking system determine maximum possible speedup could obtained principle cluster communication overhead. determine dependence speedup parameters begin with consider theoretical limitations naive parallelism scheme parallelizes distributing minibatch computation multiple machines number serial iterations required obtain accuracy training batch size suppose computing gradient batch size requires units time. running time required achieve accuracy serial training naive parallelization scheme attempts distribute computation iteration dividing minibatch machines computing gradients separately aggregating results node. scheme cost computation done single node single iteration satisﬁes system communication overhead overhead summing gradients approach could principle achieve accuracy time nac/k. represents linear speedup number machines practice several important considerations. first approximation hold must much smaller limiting number machines effectively parallelize minibatch computation. might imagine circumventing limitation using larger batch size unfortunately beneﬁt using larger batches relatively modest. batch size increases decrease enough justify large value furthermore beneﬁts approach depend greatly degree communication overhead. aggregating gradients broadcasting model parameters requires units time time required approach least iteration na/k achieve accuracy therefore maximum achievable speedup c//k c/s. expect increase modestly increases suppress effect here. performance naive parallelization scheme easily understood behavior equivalent serial algorithm. contrast sparknet uses parallelization scheme equivalent serial analysis complex. sparknet’s parallelization scheme proceeds rounds round machine runs iterations batch size rounds models workers gathered together master averaged broadcast workers. denote number rounds required achieve accuracy number parallel iterations sparknet’s parallelization scheme required achieve accuracy wallclock time measure sensitivity sparknet’s parallelization scheme parameters consider grid values pair parameters sparknet using modiﬁed version alexnet subset imagenet total parallel iterations. training runs compute ratio ma/na. speedup achieved relative training single machine figure plot heatmap speedup given sparknet parallelization scheme different values figure exhibits several trends. heatmap corresponds case worker. since multiple workers synchronize number iterations synchronizations matter squares grid behave similarly exhibit speedup factor rightmost column heatmap corresponds case synchronize every iteration sgd. equivalent running serial batch size batchsize worker column speedup increase sublinearly note slightly surprising speedup increase monotonically left right decreases. intuitively might expect synchronization strictly better however experiments suggest modest delays synchronizations beneﬁcial. experiment capture speedup expect sparknet parallelization scheme case zero communication overhead measured numbers straightforward compute speedup expect function communication overhead. figure plot speedup expected naive parallelization sparknet ﬁve-node cluster function expected naive parallelization gives maximum speedup zero communication overhead gives speedup communication overhead comparable greater cost minibatch computation. contrast sparknet gives relatively consistent speedup even communication overhead times cost minibatch computation. ﬁgure depicts parallel machines naive parallelization scheme. iteration batch size divided among machines gradients subsets computed separately machine updates aggregated model broadcast workers. algorithmically approach exactly equivalent serial figure number iterations required achieve accuracy value ﬁgure depicts parallel machines sparknet’s parallelization scheme. step machine runs batch size iterations models aggregated averaged broadcast workers. quantity number rounds required obtain accuracy total number parallel iterations sparknet’s parallelization scheme required obtain accuracy speedup given naive parallelization scheme computed exactly given c//k +s). formula essentially amdahl’s law. note naive parallelization scheme slower computation single machine. speedup obtained sparknet nac/ s)ma] speciﬁc value numerator time required serial achieve accuracy equation denominator time required sparknet achieve accuracy equation choosing optimal value gives speedup maxτ nac/ s)ma]. practice choosing difﬁcult problem. ratio na/) degrades figure ﬁgure shows speedup ma/na given sparknet’s parallelization scheme relative training single machine obtain accuracy grid square corresponds different choice show speedup zero communication overhead setting. experiment uses modiﬁed version alexnet subset imagenet note numbers dataset speciﬁc. nevertheless trends capture interest. figure ﬁgure shows speedups obtained naive parallelization scheme sparknet function cluster’s communication overhead consider data plot applies training modiﬁed version alexnet subset imagenet speedup obtained naive parallelization scheme c//k speedup obtained sparknet nac/ s)ma] speciﬁc value numerator time required serial achieve accuracy denominator time required sparknet achieve accuracy optimal value speedup maxτ nac/ s)ma]. plot sparknet speedup curve maximize values values experiments ﬁfth figure experiments figure ﬁgure shows performance sparknet -node -node -node cluster node gpu. experiments baseline obtained running caffe single communication. experiments performed imagenet using alexnet. figure ﬁgure shows performance sparknet -node cluster -node cluster node gpus. experiments baseline uses caffe single node gpus communication overhead. experiments performed imagenet using googlenet. corresponding ﬁfth figure including values would increase sparknet speedup. distributed training deep networks typically thought communication-intensive procedure. however figure demonstrates value sparknet’s parallelization scheme even bandwidth-limited settings. explore scaling behavior algorithm implementation perform experiments using clusters g.xlarge nodes. node four nvidia grid gpus memory. train default caffe model alexnet imagenet dataset sparknet plot results figure comparison also caffe cluster single communication overhead obtain plot. experiments single node. measure speedup compare wall-clock time required obtain accuracy communication overhead takes hours. gpus sparknet takes hours giving speedups also train default caffe model googlenet imagenet. sparknet plot results figure experiments caffe’s multi-gpu support take advantage four gpus within node sparknet’s parallelization scheme handle parallelism across nodes. comparison train caffe single node four gpus communication overhead. measure speedup compare wall-clock time required obtain accuracy relative baseline caffe four gpus sparknet nodes gives speedups note speedup roughly caffe four gpus gets caffe speedups sparknet obtains caffe single roughly furthermore explore dependence parallelization scheme described section parameter determines number iterations worker synchronizing workers. results shown figure note presence stragglers sufﬁces replace ﬁxed number iterations ﬁxed length time experimental setup timing sufﬁciently consistent stragglers arise. single experiment figure trained single node communication overhead. figure ﬁgure shows dependence parallelization scheme described section experiment workers. ﬁgure shows good performance achieved without collecting broadcasting model every update. much work done build distributed frameworks training deep networks. coates build model-parallel system training deep networks cluster using inﬁniband. dean build distbelief distributed system capable training deep networks thousands machines using stochastic batch optimization procedures. particular highlight asynchronous batch l-bfgs. distbelief exploits data parallelism model parallelism. chilimbi build project adam system training deep networks hundreds machines using asynchronous sgd. build parameter servers exploit model data parallelism though systems better suited sparse gradient updates could well applied distributed training deep networks. recently abadi build tensorflow sophisticated system training deep networks generally specifying computation graphs performing automatic differentiation. iandola build firecaffe data-parallel system achieves impressive scaling using naive parallelization high-performance computing setting. minimize communication overhead using tree reduce aggregating gradients supercomputer cray gemini interconnects. custom systems numerous advantages including high performance ﬁne-grained control scheduling task placement ability take advantage low-latency communication machines. hand demanding communication requirements unlikely exhibit scaling cluster. furthermore nature custom systems lack beneﬁts tight integration general-purpose computational frameworks spark. systems preprocessing must done separately mapreduce style framework data written disk segments pipeline. sparknet preprocessing training done spark. training machine learning model deep network often step many real-world data analytics pipelines obtaining cleaning preprocessing data often expensive operations transferring data systems. training data machine learning model derived streaming source query graph computation. user wishing train deep network custom system output query would need separate engine. sparknet training deep network output query graph computation streaming data source straightforward general purpose nature support graph computations data streams attempts made train deep networks general-purpose computational frameworks however existing work typically hinges extremely low-latency intra-cluster communication. noel train deep networks spark yarn using leverage cluster resources parallelize computation gradient minibatch. achieve competitive performance remote direct memory accesses inﬁniband exchange model parameters quickly gpus. contrast sparknet tolerates low-bandwidth intra-cluster communication works amazon separate line work addresses speeding training deep networks using single-machine parallelism. example caffe troll modiﬁes caffe leverage resources within single node. approaches compatible sparknet used conjunction. many popular computational frameworks provide support training machine learning models linear models matrix factorization models. however demanding communication requirements larger scale many deep learning problems libraries extended include deep networks. various authors studied theory averaging separate runs sgd. bandwidth-limited setting zinkevich analyze simple algorithm convex optimization easily implemented mapreduce framework tolerate high-latency communication machines. zhang deﬁne parallelization scheme penalizes divergences parallel workers provide analysis convex case. zhang jordan propose general abstraction parallelizing stochastic optimization algorithms along spark implementation. described approach distributing training deep networks communicationlimited environments lends implementation batch computational frameworks like mapreduce spark. provide sparknet easy-to-use deep learning implementation spark based caffe enables easy parallelization existing caffe models minimal modiﬁcation. machine learning increasingly depends larger larger datasets integration fast general engine data processing spark allows researchers practitioners draw rich ecosystem tools develop deploy models. build models incorporate features variety data sources like images distributed system results query graph database query streaming data sources. using smaller version imagenet benchmark quantify speedup achieved sparknet function size cluster communication frequency cluster’s communication overhead. demonstrate approach effective even highly bandwidth-limited settings. full imagenet benchmark showed system achieves sizable speedup single node experiment even gpus. code sparknet available https//github.com/amplab/sparknet. invite contributions hope project help bring diverse deep learning applications spark community. would like thank cyprien noel andy feng tomer kaftan evan sparks shivaram venkataraman valuable advice. research supported part grant number dge. research supported part cise expeditions award ccf- award de-sc darpa xdata award fa--- gifts amazon services google thomas stacey siebel foundation adatao adobe apple blue goji bosch cisco cray cloudera ericsson facebook fujitsu guavus huawei informatica intel microsoft netapp pivotal samsung schlumberger splunk virdata vmware. references abadi mart´ın agarwal ashish barham paul tensorflow large-scale machine learning heterogeneous systems http//tensorflow.org/. software available tensorﬂow.org. armbrust michael reynold lian cheng huai davies bradley joseph meng xiangrui kaftan tomer franklin michael ghodsi spark relational data proceedings sigmod international conference processing spark. management data chilimbi trishul suzue yutaka apacible johnson kalyanaraman karthik. project adam building efﬁcient scalable deep learning training system. usenix symposium operating systems design implementation dean jeffrey corrado greg monga rajat chen devin matthieu mark ranzato marc’aurelio senior andrew tucker paul yang quoc andrew large advances neural information processing systems scale distributed deep networks. gonzalez joseph reynold dave ankur crankshaw daniel franklin michael stoica ion. graphx graph processing distributed dataﬂow framework. proceedings osdi qirong cipar james henggang seunghak gibbons phillip gibson garth ganger greg xing eric effective distributed stale synchronous parallel parameter server. advances neural information processing systems iandola forrest ashraf khalid moskewicz mattthew keutzer kurt. firecaffe near-linear acceleration deep neural network training compute clusters. arxiv preprint arxiv. isard michael budiu mihai yuan birrell andrew fetterly dennis. dryad distributed data-parallel programs sequential building blocks. proceedings sigops/eurosys european conference computer systems yangqing shelhamer evan donahue jeff karayev sergey long jonathan girshick ross guadarrama sergio darrell trevor. caffe convolutional architecture fast feature emproceedings international conference multimedia bedding. krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems andersen david park smola alexander ahmed josifovski vanja long james shekita eugene bor-yiing. scaling distributed machine learning parameter server. usenix symposium operating systems design implementation meng xiangrui bradley joseph yavuz burak sparks evan venkataraman shivaram davies freeman jeremy tsai amde manish owen sean mllib machine learning apache spark. arxiv preprint arxiv. murray derek mcsherry frank isaacs rebecca isard michael barham paul abadi mart´ın. naiad timely dataﬂow system. proceedings twenty-fourth symposium operating systems principles noel cyprien feng andy. large scale distributed deep learning hadoop clusters http//yahoohadoop.tumblr.com/post// large-scale-distributed-deep-learning-on-hadoop. russakovsky olga deng krause jonathan satheesh sanjeev sean huang zhiheng karpathy andrej khosla aditya bernstein michael berg alexander fei-fei international journal computer vision szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. going deeper convolutions. computer vision pattern recognition zaharia matei chowdhury mosharaf franklin michael shenker scott stoica ion. spark cluster computing working sets. proceedings usenix conference topics cloud computing volume zaharia matei tathagata haoyuan hunter timothy shenker scott stoica ion. discretized streams fault-tolerant streaming computation scale. proceedings twentyfourth symposium operating systems principles", "year": 2015}