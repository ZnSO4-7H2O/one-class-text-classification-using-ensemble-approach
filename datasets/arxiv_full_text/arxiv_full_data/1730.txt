{"title": "On Learning Vector Representations in Hierarchical Label Spaces", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "An important problem in multi-label classification is to capture label patterns or underlying structures that have an impact on such patterns. This paper addresses one such problem, namely how to exploit hierarchical structures over labels. We present a novel method to learn vector representations of a label space given a hierarchy of labels and label co-occurrence patterns. Our experimental results demonstrate qualitatively that the proposed method is able to learn regularities among labels by exploiting a label hierarchy as well as label co-occurrences. It highlights the importance of the hierarchical information in order to obtain regularities which facilitate analogical reasoning over a label space. We also experimentally illustrate the dependency of the learned representations on the label hierarchy.", "text": "jinseok johannes f¨urnkranz knowledge engineering group department computer science technische universit¨at darmstadt knowledge discovery scientiﬁc literature german institute educational research {namcs juffike}.tu-darmstadt.de important problem multi-label classiﬁcation capture label patterns underlying structures impact patterns. paper addresses problem namely exploit hierarchical structures labels. present novel method learn vector representations label space given hierarchy labels label co-occurrence patterns. experimental results demonstrate qualitatively proposed method able learn regularities among labels exploiting label hierarchy well label co-occurrences. highlights importance hierarchical information order obtain regularities facilitate analogical reasoning label space. also experimentally illustrate dependency learned representations label hierarchy. multi-label classiﬁcation area machine learning aims learn function maps instances label spaces. contrast multi-class classiﬁcation instance assumed associated label. goals multi-label classiﬁcation model underlying structures label space many problems occurrence labels independent other. many attempts made capture exploit label structures number possible conﬁgurations labels grows exponentially respect number labels required multi-label classiﬁer handle many labels efﬁciently multi-label classiﬁers also human annotators need handle large number labels efﬁciently. thus human experts effort maintaining updating hierarchies labels hierarchies used generate ground truth training classiﬁers many methods developed hierarchical output structures machine learning particular several researchers looked utilizing hierarchical structure label space improved predictions multi-label classiﬁcation work present novel method efﬁciently learn hierarchical structures labels well occurrences labels investigate importance hierarchical structures identify internal structures labels. rest paper organized follows. section deﬁne multi-label classiﬁcation hierarchical structures labels graph. introduce proposed method learns label spaces taking account label hierarchies. section experiments present empirical analysis section finally discuss ﬁndings experiments provide directions future work section throughout work present method learn representations labels multi-label classiﬁcation. firstly section deﬁne multi-label classiﬁcation notation label hierarchies. notation. multi-label classiﬁcation refers task learning function maps instances label sets given training dataset number instances number labels. consider multi-label problems label hierarchies exist. label graphs natural represent hierarchical structures. possible label parent node represent hierarchy labels directed acyclic graph consider graph denotes node represent connection nodes. node corresponds label edge represents parent-child relationship labels hierarchy. node corresponds label. exists directed path ancestor descendant ancestors denoted descendants denoted large-scale multi-label classiﬁcation problems high dimensionality label spaces makes learning classiﬁers difﬁcult. literature several work proposed tackle problem reducing dimensionality label spaces label co-occurrence patterns play crucial role. language modeling another area research high dimensionality issues arise severely deal problem representing words dense vectors learn representations maximizing conditional probability word given context word recently learning word representations mikolov proposed efﬁcient log-linear model relying co-occurrence patterns words given ﬁxed-length contexts shown interesting properties word representations analogical reasoning. following section introduce novel approach reduce dimensionality label spaces adapting log-linear models capable handling large-scale problems efﬁciently. stated section several work making hierarchical structures label spaces improving predictive performance multi-label classiﬁers. label hierarchies often built maintained human experts used additional source information identify internal structures. thus propose log-linear model reduce dimensionality label spaces exploiting label co-occurrences hierarchical relations labels. formally basic idea maximize probability predicting ancestor given particular label hierarchy well predicting co-occurring labels given labels training instances {yy··· objective function maximize average probability given j-th column vector rd×l i-th column vector rd×l. here correspond continuous vector representations input labels output labels respectively used parameters equation enables labels share ancestors co-occurring labels similar representations. softmax function equation viewed objective function neural network consisting linear activation function hidden layer weights connects input layer hidden layer used convey hidden activations output layer. thus probability predicting label given label i.e. ej|i obtained neural network framework dimensional vector whose i-th element zero elsewhere hidden activation label note since i-th element equal probability predicting label assigned together label label computed meaning parameterized well. parameters updated follows α∇ul denotes learning rate. hierarchical softmax hinton reduces gradient computing cost similar mikolov order make hierarchical softmax binary tree constructed huffman coding giving binary codes variable length label according |sd|. words descendants label hierarchy shorter codes assigned. note deﬁnition huffman coding labels correspond leaf nodes binary tree namely huffman tree internal nodes huffman tree. instead computing outputs hierarchical softmax computes probability binary decisions path root node tree leaf node corresponding target label equation speciﬁcally codeword label huffman coding either number bits codeword label. l-th codeword. unlike softmax computing hierarchical softmax output label representations vector representations inner nodes huffman tree. hierarchical softmax given vector representation l-th node path root node node corresponding label huffman tree. inner products required compute normalization term equation hierarchical softmax needs times computations. hence hierarchical softmax allows substantial improvements computing gradients carried experiments bioasq task dataset millions training documents approximately index terms given parent-child pairs deﬁned index terms. index terms known medical subject headings controlled updated continually national library medicine used index articles medline pubmed databases. representing parent-child pairs dag. often case represent parent-child pairs labels graph real-world problems contains cycles. sometimes reasonable assume cycles result wrong annotations complex relationship labels different abstract levels. additionally cycles graph also introduce difﬁculties learning algorithms. instance want visit ancestors given node node direct edge ancestors graph traversal never ends unless stopping criteria used. hence remove edges resulting cycles follows preprocessing steps left parent-child pairs removing ones obtain dag-structured label hierarchy. training details. dimension vector representations learning rate ﬁxed training. took hours iterate times millions training instances intel xeon equipped threads. hierarchy mesh vocabulary consists major categories. index term belongs least major category. training log-linear model bioasq dataset selected labels corresponding terminals belonging single major category order visualize learned representations. figure shows using hierarchy co-occurrences labels model separates reasonably well major categories deﬁned mesh vocabulary. please note whereas tsne better visualization figure principal component analysis used project label representations rest ﬁgures paper. following section investigate label representations learn hierarchy co-occurrences. illustration results focus subgraph related health care shown figure consider leaf nodes ﬁgure. according objective equation urban health suburban health rural health trained representations predicting common ancestors i.e. health population characteristics health care category. child nodes population also trained similarly. although urban suburban rural health separated urban suburban rural population representations tend similar since share ancestors. words urban health rural population example somewhat similar representations part increase probability predicting common ancestors even though rarely assigned instance. figure part hierarchy related health care. learned vector representations index terms leaf without hierarchy learned vector representations index terms hierarchy. perform experiment whether learning co-occurrences yields meaningful structures. case objective function limited right term equation co-occurrence information enables learn internal structures consider hierarchical relationship labels well co-occurrences them interesting property proposed method observed. major difference figure interintra-group relationships among labels leaves. speciﬁcally child nodes health located left side population appear opposite side addition relationship groups relations labels belonging health resemble found among children population figure shows important hierarchical information capturing regularities labels. likely label pairs co-occur frequently close particular figure illustrates urban population close urban health rural population urban population often assigned together urban population rural population document. likewise shown figure therapy close disorder therapy effective treatment. make hierarchical information training model reveals interesting relationship observed model trained co-occurrences. figure shows strong directed relation urban rural represented direction vector pointing upper-left. learned vector representations identiﬁed therapy-disorders/diseases relationship evaluate representation quality analogical reasoning shown example could represent analogy king queen like woman qualitative equation king woman queen. upon observations section performed analogical reasoning representations trained hierarchy ones without hierarchy speciﬁcally regarding therapy-disorders/diseases relationship expected seems like label representations trained hierarchy clearly advantageous ones trained without hierarchy analogical reasoning. speciﬁc consider ﬁrst example want know kinds therapies effective respiration disorders relationship diet therapy cardiovascular diseases. perform analogical reasoning using learned representations hierarchy probable answers analogy question therapies used treat respiration disorders including nutritional therapy. unlike learned representations hierarchy likely label representations learned without hierarchy perform poorly type tasks. however could observe regularities analogical reasoning questions terms parent-child relationships. instance expected either urban health health population urban health health -population characteristics population results representations close urban population answers questions something close urban population. conjecture proposed method encodes one-way hierarchical dependence labels. words index term trained predict ancestors predict descendants. previous section shown proposed method capable capturing hierarchical structures labels co-occurrences them. case expected learned representations change part hierarchy changed label co-occurrences remains same. answer question ﬁrstly modiﬁed original hierarchy obtain hierarchy originally population characteristics child nodes health population. contrary population three child nodes health dozens child nodes. instead removing nodes hierarchy kept hierarchy. however population removed hierarchy. created three internal nodes urban suburban rural representing types developed environments. finally nodes interest grouped according developed types. figure shows learned vector representations modiﬁed hierarchy compares learned representations previous results. unlike previous result figure healthrelated nodes population-related nodes form clusters urban health urban population clustered together since share common parent node urban. also observe similar patterns suburban rural. besides relative distance population health within cluster identical direction vector health population around deﬁned. please note that case learning model co-occurrences yields always similar label representations since modiﬁed hierarchy presented method learns vector representation labels taking hierarchical structures account. empirical results demonstrate using hierarchical structures labels label co-occurrences impact identifying regularities labels using label cooccurrences only. evaluated label representations qualitatively observing label representations even though interintra-group relations found learned label space still desired evaluate method using quantitative measures compared analyzed learned representations limited number examples chosen arbitrarily. hence currently attempting evaluate model quantitatively check whether model brings better prediction results multi-label classiﬁcation means pre-training joint embedding space also interested extending model capable analogical reasoning parent-child relationship. sake readability used repeated results left graph taken figure order clearly show difference representations learned original hierarchy modiﬁed well learned co-occurrences. figure modiﬁed hierarchy original obtained grouping types developed environments. text explanation. learned vector representations without using hierarchy. learned vector representations using original hierarchy. learned vector representations using modiﬁed hierarchy. dembczy´nski krzysztof waegeman willem cheng weiwei h¨ullermeier eyke. label dependence loss minimization multi-label classiﬁcation. machine learning mikolov tomas sutskever ilya chen corrado greg dean jeff. distributed representations words phrases compositionality. advances neural information processing systems morin frederic bengio yoshua. hierarchical probabilistic neural network language model. proceedings international workshop artiﬁcial intelligence statistics rousu juho saunders craig szedm´ak s´andor shawe-taylor john. kernel-based learning hierarchical multilabel classiﬁcation models. journal machine learning research hsiang-fu jain prateek purushottam dhillon inderjit. large-scale multi-label learning missing labels. proceedings international conference machine learning zimek arthur buchwald fabian frank eibe kramer stefan. study hierarchical classiﬁcation proteins. ieee/acm transactions computational biology bioinformatics", "year": 2014}