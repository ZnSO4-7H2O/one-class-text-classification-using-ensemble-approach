{"title": "Noise2Noise: Learning Image Restoration without Clean Data", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We apply basic statistical reasoning to signal reconstruction by machine learning -- learning to map corrupted observations to clean signals -- with a simple and powerful conclusion: under certain common circumstances, it is possible to learn to restore signals without ever observing clean ones, at performance close or equal to training using clean exemplars. We show applications in photographic noise removal, denoising of synthetic Monte Carlo images, and reconstruction of MRI scans from undersampled inputs, all based on only observing corrupted data.", "text": "reported several applications including gaussian denoising de-jpeg text removal super-resolution colorization image inpainting obtaining clean training targets often difﬁcult tedious. noise-free photograph requires long exposure; full sampling slow enough preclude dynamic subjects etc. work observe suitable common circumstances learn reconstruct signals corrupted examples without ever observing clean signals often well using clean examples. show below conclusion almost trivial statistical perspective practice signiﬁcantly eases learning signal reconstruction lifting requirements availability clean data. assume unreliable measurements room temperature. common strategy estimating true unknown temperature number smallest average deviation measurements according loss function loss absolute deviations turn optimum median observations. general class deviation-minimizing estimators known m-estimators statistical viewpoint summary estimation using common loss functions seen estimation interpreting loss function negative likelihood. training neural network regressors generalization point estimation procedure. observe form typical training task input-target pairs network function parameterized apply basic statistical reasoning signal reconstruction machine learning learning corrupted observations clean signals simple powerful conclusion certain common circumstances possible learn restore signals without ever observing clean ones performance close equal training using clean exemplars. show applications photographic noise removal denoising synthetic monte carlo images reconstruction scans undersampled inputs based observing corrupted data. signal reconstruction corrupted incomplete measurements important subﬁeld statistical data analysis. recent advances deep neural networks sparked signiﬁcant interest avoiding traditional explicit priori statistical modeling signal corruptions instead learning corrupted observations unobserved clean versions. happens training regression model e.g. convolutional neural network large number pairs corrupted inputs clean targets minimizing empirical risk parametric family mappings loss function notation underline fact corrupted input random variable distributed according clean target. training data include example pairs short long exposure photographs scene incomplete complete k-space samplings magnetic resonance images fast-but-noisy slow-but-converged monte carlo renderings synthetic scene etc. signiﬁcant advances ﬁnite data variance estimate average variance corruptions targets divided number training samples note typical denoising tasks effective number training samples large even number image pairs limited every pixel neighborhood contributes. many image restoration tasks expectation corrupted input data clean target seek restore. low-light photography example long noise-free exposure average short independent noisy exposures. ﬁndings indicate cases potential numerical issues dispose clean targets entirely long able observe source image twice task often signiﬁcantly less costly acquiring clean target. similar observations made loss functions. loss recovers median targets meaning neural networks trained repair images signiﬁcant outlier content requiring access pairs corrupted images. next sections present wide variety examples demonstrating theoretical capabilities also efﬁciently realizable practice allowing blindly learn signal reconstruction models state-of-theart methods make clean examples using precisely training methodology often without appreciable drawbacks training time performance. experimentally study practical properties noisy-target training identify various cases clean targets needed. start standard simple noise distributions sections continue much harder analytically intractable monte carlo noise image synthesis section section observe image reconstruction sub-nyquist spectral samplings magnetic resonance imaging learned corrupted observations only. ﬁrst study effect corrupted targets using additive gaussian noise. simple distribution draw samples thus generate unlimited amount synthetic training data corrupting clean images. noise zero mean loss training recover mean. indeed remove dependency input data trivial merely outputs learned scalar task reduces conversely full training task decomposes minimization problem every training sample; simple manipulations show equivalent network theory minimize loss solving point estimation problem separately input sample. hence properties underlying loss inherited neural network training. usual process training regressors equation ﬁnite number input-target pairs hides subtle point instead mapping inputs targets implied process reality mapping multiple-valued. example superresolution task natural images low-resolution image explained many different high-resolution images knowledge exact positions orientations edges texture lost decimation. words highly complex distribution natural images consistent low-resolution training neural network regressor using training pairs lowhigh-resolution images using loss network learns output average plausible explanations results spatial blurriness network’s predictions. significant amount work done combat well known tendency example using learned discriminator functions losses observation certain problems tendency unexpected beneﬁt. trivial ﬁrst sight useless property minimization expectation estimate remains unchanged replace targets random numbers whose expectations match targets. easy equation holds matter particular distribution drawn from. consequently optimal network parameters equation also remain unchanged input-conditioned target distributions replaced arbitrary distributions conditional expected values. implies principle corrupt training targets neural network zero-mean noise without changing network learns. combining corrupted inputs equation left empirical risk minimization task figure denoising performance function training epoch additive gaussian noise. clean noisy targets lead similar convergence speed eventual denoising quality. furthermore blur noisy targets using gaussian ﬁlter different widths observe lower frequency noise persistent cases network converges towards similar denoising quality. noisy target target noise blurred pixel ﬁlter. table psnr results three test datasets kodak gaussian poisson bernoulli noise. comparison methods inverse anscombe transform deep image prior strated effective wide range image restoration tasks including gaussian noise. train network using ×-pixel crops drawn images imagenet validation set. furthermore randomize noise standard deviation separately training example i.e. network estimate magnitude noise removing three well-known datasets kodak. summarized table behavior qualitatively similar three sets thus discuss averages. trained using standard clean targets network achieves average quality conﬁdence interval computed training networks different random initializations. widely used benchmark denoiser gives worse results. modify training noisy targets instead denoising performance remains equally good. furthermore training converges quickly shown figure leads conclude clean targets completely unnecessary application. perhaps surprising observation holds also different networks network capacities. figure shows example result. tests switch shallower u-net roughly faster train gives similar results u-net architecture training parameters described supplemental material. convergence speed clearly every training example asks network perform impossible task could succeed transforming instance noise another. consequently training loss actually decrease training signiﬁcant loss gradients continue quite large. larger noisier gradients affect convergence speed? activation gradients indeed noisy weight gradients fact relatively clean gaussian noise independent identically distributed pixels weight gradients averaged pixels fully convolutional network. figure makes situation artiﬁcially harder introducing inter-pixel correlation corruption. achieve training series networks gaussian noise targets blurred using progressively larger gaussian ﬁlters followed normalization restore variance. corruption remains zero-mean. example shown figure correlation increases effective averaging weight gradients decreases weight updates become noisier. makes convergence slower even extreme blur eventual denoising quality similar experiment types synthetic noise. training setup described above. poisson noise dominant source noise photographs. zero mean like gaussian harder remove signal-dependent. loss vary noise magnitude training. figure example results gaussian poisson bernoulli noise. result computed using noisy targets corresponding result clean targets omitted virtually identical three cases discussed text. different comparison method used noise type. actual image resulting image would incorrectly tend towards linear combination right answer average text color however reasonable amount overlaid text pixel retains original color often therefore median correct statistic. hence |fθ− loss function. figure shows example result. random-valued impulse noise replaces pixels noise retains colors others. instead standard salt pepper noise study harder distribution pixel replaced random color drawn uniform distribution probability retains color probability pixels’ color distributions dirac original color plus uniform distribution relative weights given replacement probability case neither mean median yield correct result; desired output mode distribution distribution remains unimodal. approximate mode seeking annealed version clean target images used average noisy targets give equally good similar convergence speed. comparison method based ascombe transform ﬁrst transforms input poisson noise turns gaussian noise removes using ﬁnally inverse transform gives lower quality method. forms noise cameras e.g. dark current quantization small compared poisson noise made zero-mean hence pose problems training noisy targets. conclude noise-free training data unnecessary application. said saturation break assumptions parts noise distribution discarded expectation remaining part longer correct. saturation unwanted reasons well signiﬁcant practical limitation. multiplicative bernoulli noise constructs random mask valid pixels zeroed/missing pixels. avoid backpropagating gradients missing pixels exclude loss using mask probability corrupted pixels denoted training vary testing training clean targets gives average noisy targets give slightly higher possibly noisy targets effectively implement form dropout network output. almost worse learning-based solution different approach shares property neither clean examples explicit model corruption needed. used image reconstruction setup described supplemental material. text removal figure demonstrates blind text removal. corruption consists large varying number random strings random places also other furthermore font size color randomized well. font string orientation remain ﬁxed. network trained using independently corrupted input target pairs. probability corrupted pixels approximately training testing. test mean correct answer overlaid text colors unrelated figure psnr noisy-target training relative clean targets varying percentage target pixels corrupted impulse noise. test separate network trained corruption level graph averaged kodak dataset. train network using noisy inputs noisy targets probability corrupted pixels randomized separately pair figure shows inference results input pixels randomized. training loss biases results heavily towards gray result tends towards linear combination correct answer mean uniform random corruption. predicted theory loss gives good results long fewer pixels randomized beyond threshold quickly starts bias dark bright areas towards gray hand shows little bias even extreme corruptions possible pixel values correct answer still common. physically accurate renderings virtual environments often generated process known monte carlo path tracing. amounts drawing random sequences scattering events scene connect light sources virtual sensors integrating radiance carried possible paths monte carlo integrator constructed intensity pixel expectation random path sampling process i.e. sampling noise zero-mean. however despite decades research importance sampling techniques little else said distribution. varies pixel pixel heavily depends scene conﬁguration rendering parameters arbitrarily multimodal. lighting effects figure comparison various loss functions training monte carlo denoiser noisy target images rendered samples pixel high-dynamic range setting custom relative loss lhdr clearly superior applying non-linear tone inputs beneﬁcial applying target images skews distribution noise leads wrong visibly dark results. effects make removal monte carlo noise much difﬁcult removing e.g. gaussian noise. hand problem somewhat alleviated possibility generating auxiliary information empirically found correlate clean result data generation. experiments denoiser input consists per-pixel luminance values also average albedo normal vector surfaces visible pixel. high dynamic range even adequate sampling pixel luminances differ several orders magnitude thus typically represented using ﬂoating-point values. order construct image suitable generally -bit display devices high dynamic range needs compressed ﬁxed range using tone mapping operator. large variety operators proposed work consider variant reinhard’s global operator scalar luminance value possibly pre-scaled image-wide exposure constant. operator maps range combination virtually unbounded range luminances nonlinearity operator poses problem. attempt train denoiser outputs luminance values standard loss dominated long-tail effects targets training converge. hand denoiser output tonemapped values nonlinearity would make expected value noisy target images different clean training target leading incorrect predictions. metric often used measuring quality images relative squared difference divided square approximate luminance pixel i.e. ˆy)/. however metric suffers nonlinearity problem comparing tonemapped outputs. therefore propose network output tends towards correct value limit denominator lhdr ˆy)/ shown lhdr converges correct expected value long consider gradient denominator zero. finally observed experimentally beneﬁcial tone input image supplied network instead using input image network continues output non-tonemapped luminance values retaining correctness expected value. figure evaluates different loss functions. denoising monte carlo rendered images trained denoiser monte carlo path traced images rendered using samples pixel training consisted architectural images validation done using images different scenes. three versions training images rendered using different random seeds validation images rendered versions. images pixels size mentioned earlier also saved albedo normal buffers input images. even small dataset rendering clean images strenuous effort example figure took minutes render high-end graphics server nvidia tesla gpus -core intel xeon cpu. average psnr validation inputs respect corresponding reference images network trained epochs using clean target images reached average psnr validation whereas similarly trained network using noisy target images gave lower quality. example results corresponding runs shown figure training took hours single nvidia tesla gpu. figure denoising monte carlo rendered image. image rendered samples pixel. denoised input denoiser network trained using target images. previous denoiser trained using reference images clean target images. reference image rendered samples pixel. psnr values refer images shown here text averages entire validation set. figure shows convergence plots experiment trained denoiser scratch duration frames scene ﬂythrough. nvidia titan path tracing single pixel image took rendered images input target. single network training iteration random pixel crop took performed eight frame. finally denoised rendered images taking averaged result produce ﬁnal denoised image shown user. total frame time thus rendering training inference. seen figure denoiser trained clean output images perform appreciably better denoiser trained noisy images. rendering single clean target image takes approximately minutes scene obvious quality-vs-time tradeoff favors training noisy target images instead. magnetic resonance imaging produces volumetric images biological tissues applying carefully controlled magnetic ﬁelds vary space time measuring radio frequency emissions molecules precessing ﬁelds coils. mathematically process amounts sampling fourier transform signal. modern techniques long relied compressed sensing cheat nyquist-shannon limit undersample k-space perform non-linear reconstruction removes aliasing exploiting sparsity image suitable transform domain effective compressed sensing requires sampling patterns yield measurements incoherent errors tend magnify would phase alignments standard regular sampling grids. figure online training psnr -frame ﬂythrough scene figure noisy target images almost good learning clean targets faster render denoisers offer substantial improvement noisy input. matched i.e. noisy targets took approximately twice long converge. however methods narrowed appreciably leading believe quality difference remain even limit. indeed would expect since training dataset contained limited number training pairs cost generating clean target images comparison method wanted test methods using matching data. said given noisy targets times faster produce could trivially produce larger quantity training data train even higher quality network still realize vast gains data collection plus training. online training since difﬁcult collect sufﬁciently large corpus monte carlo rendered images training generally applicable denoiser another possibility train denoiser speciﬁc single scene e.g. game level movie shot context desirable train denoiser dynamically walking scene. order maintain interactive frame rates afford samples pixel thus input target images inherently noisy. figure reconstruction example. input image spectrum samples retained scaled reconstruction network trained noisy target images similar input image. previous training done clean target images similar reference image. original uncorrupted image. psnr values refer images shown here text averages entire validation set. line super-resolution approaches observe smoothing effect training clean corrupted targets. many recent techniques attempt increase apparent resolution example generative adversarial networks however terms psnr results quite closely match reported results. shown simple statistical arguments lead surprising capabilities learned signal recovery; possible recover signals complex corruptions without observing clean signals performance levels equal close using clean target data. several realworld situations obtaining clean training data difﬁcult low-light photography physically-based image synhesis magnetic resonance imaging. proof-of-concept demonstrations point signiﬁcant potential beneﬁts applications removing need potentially strenuous collection clean data. course free lunch cannot learn pick features input data applies equally training clean targets. ambientgan recent idea allows principled training generative adversarial networks using corrupted observations. stark contrast approach need perfect explicit computational model corruption; requirement knowledge appropriate statistical summary signiﬁcantly less limiting. combining ideas along paths intriguing. process known probability density frequencies main idea applies. particular model k-space sampling operation bernoulli process individual frequency probability e−λ|k| selected acquisition. frequencies retained weighted inverse selection probability non-chosen frequencies zero. clearly expectation russian roulette process correct spectrum. parameter controls overall fraction k-space retained; following experiments choose samples retained relative full nyquist-shannon sampling. undersampled spectra transformed primal image domain standard inverse fourier transform. example undersampled input/target picture corresponding fully sampled reference spectra shown figure simply regression problem form train convolutional neural network using pairs independent undersampled images volume. spectra input target correct expectation fourier transform linear loss. additionally improve result slightly enforcing exact preservation frequencies present input image fourier transforming result replacing frequencies input transforming back primal domain computing loss ﬁnal loss reads denotes replacement non-zero frequencies input. process trained end-to-end. perform proof-of-concept experiments slices extracted brain scan dataset. simulate spectral sampling draw random samples inverse images dataset. hence deviation actual samples data real-valued periodicity discrete built-in. training contained images resolution subjects validation chose random images different subjects. baseline psnr sparsely-sampled input images reconstructed directly using ifft. network trained epochs noisy targets reached average psnr validation data network trained clean targets reached training clean targets similar prior training took hours nvidia tesla gpu. figure shows example reconstruction results convolutional networks trained noisy clean targets respectively. simpliﬁed example deviates practical sense sample spectra along trajectories. however believe designing pulse sequences lead similar pseudo-random sampling characteristics straightforward. bill dally david luebke aaron lefohn discussions supporting research; nvidia research staff suggestions discussion; runa lober gunter sprenger synthetic off-line training data; jacopo pantaleoni interactive renderer used on-line training; samuli vuorinen initial photography test data; koos zevenhoven discussions basics. chaitanya chakravarty alla kaplanyan anton schied christoph salvi marco lefohn aaron nowrouzezahrai derek aila timo. interactive reconstruction monte carlo image sequences using recurrent denoising autoencoder. trans. graph. goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial networks. nips hasinoff sharlet dillon geiss ryan adams andrew barron jonathan kainz florian chen jiawen levoy marc. burst photography high dynamic range low-light imaging mobile cameras. trans. graph. ledig christian theis lucas huszar ferenc caballero jose aitken andrew tejani alykhan totz johannes wang zehan wenzhe. photo-realistic single image super-resolution using generative adversarial network. proc. cvpr maas andrew hannun awni andrew. rectiﬁer nonlinearities improve neural network acoustic models. proc. international conference machine learning volume martin fowlkes malik database human segmented natural images application evaluating segmentation algorithms measuring ecological statistics. proc. iccv volume name input conv conv pool conv pool conv pool conv pool conv pool conv upsample concat conva convb upsample concat conva convb upsample concat conva convb upsample concat conva convb upsample concat conva convb convc function convolution convolution maxpool convolution maxpool convolution maxpool convolution maxpool convolution maxpool convolution upsample concatenate output pool convolution convolution upsample concatenate output pool convolution convolution upsample concatenate output pool convolution convolution upsample concatenate output pool convolution convolution upsample concatenate input convolution convolution convolution linear act. table network architecture used experiments. nout denotes number output feature maps layer. number network input channels output channels depend experiment. convolutions padding mode same except last layer followed leaky relu activation function layers linear activation. upsampling nearest-neighbor. table shows structure u-network used tests exception ﬁrst test section used network basic noise text removal experiments images number input output channels monte carlo denoising i.e. input contained pixel color albedo normal vector pixel. reconstruction done monochrome images input images represented range network weights initialized following batch normalization dropout regularization techniques used. training done using adam parameter values learning rate kept constant value training except brief rampdown period smoothly brought zero. learning rate used experiments except monte carlo denoising found provide better stability. minibatch size used experiments. compute expected error norm minimization task corrupted targets {ˆyi}n used place clean targets {yi}n ﬁnite number. arbitrary random variables e{ˆyi} usual point least deviation found respective mean. expected squared difference means across realizations noise then either case variance estimate average variance corruptions divided number samples therefore error approaches zero number samples grows. estimate unbiased interestingly norm could intuitively expected converge exact mode i.e. local maximum probability density function data theoretical analysis reveals recovers slightly different point. actual mode zero-crossing derivative norm minimization recovers zero-crossing hilbert transform instead. veriﬁed behavior variety numerical experiments practice estimate typically close true mode. explained fact hilbert transform approximates differentiation latter multiplication fourier domain whereas hilbert transform multiplication sgn. continuous data density norm minimization task amounts ﬁnding point minimal expected p-norm distance points", "year": 2018}