{"title": "Sample-efficient Deep Reinforcement Learning for Dialog Control", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Representing a dialog policy as a recurrent neural network (RNN) is attractive because it handles partial observability, infers a latent representation of state, and can be optimized with supervised learning (SL) or reinforcement learning (RL). For RL, a policy gradient approach is natural, but is sample inefficient. In this paper, we present 3 methods for reducing the number of dialogs required to optimize an RNN-based dialog policy with RL. The key idea is to maintain a second RNN which predicts the value of the current policy, and to apply experience replay to both networks. On two tasks, these methods reduce the number of dialogs/episodes required by about a third, vs. standard policy gradient methods.", "text": "practice policy gradient methods often sample inefﬁcient limiting real-world settings explorational interactions conducting dialogs expensive. contribution paper present family methods increasing sample efﬁciency policy gradient methods policy represented recurrent neural network speciﬁcally make changes standard policy gradient approach. first estimate second predicts expected future reward policy attain current state; updates value network reduces error gradient step expense additional computation maintaining value network. second experience replay networks allowing gradient steps taken dialog. paper organized follows. next section reviews policy gradient approach section presents methods sections present results tasks section covers related work section brieﬂy concludes. reinforcement learning problem agent interacts stateful environment maximize concretely numeric reward signal. timestep awarded real-valued reward receives observation vector ot+. goal agent choose actions maximize discounted future rewards called return representing dialog policy recurrent neural network attractive handles partial observability infers latent representation state optimized supervised learning reinforcement learning policy gradient approach natural sample inefﬁcient. paper present methods reducing number dialogs required optimize rnn-based dialog policy idea maintain second predicts value current policy apply experience replay networks. tasks methods reduce number dialogs/episodes required third standard policy gradient methods. study problem using reinforcement learning optimize controller represented recurrent neural network rnns attractive accumulate sequential observations latent representation state thus naturally handle partially observable environments dialog also robot navigation autonomous vehicle control others. among many methods optimization adopt policy gradient approach policy gradient approaches natural recurrent neural networks make updates stochastic gradient descent. also strong convergence characteristics compared value-function methods q-learning diverge using function approximation form paper consider policies represented recurrent neural network internally maintains vector representing latent state latent state begins ﬁxed state timestep takes input observation vector updates internal state according differentiable function outputs distribution actions according differentiable function parameterize functions. chosen implement long short-term memory gated recurrent unit recurrent models. denotes output timestep throughout paper also importance sampling ratios perform off-policy updates. assume behavior policy used generate dialogs general different target policy wish optimize. timestep deﬁne importance sampling ratio estimate value starting parameterizes method allows gradient step taken light value current state. implement second update parameters dialog using supervised learning method increases learning speed reusing past dialogs better estimate since policy changes dialog past dialogs off-policy respect correction needed. precup showed following off-policy update equal on-policy update expectation second method applying experience replay policy network. speciﬁcally degris shows samples following expectation behavior policy used estimate gradient policy network representing policy test approach created dialog system initiating phone calls contact address book taken microsoft internal employee directory. full details given williams zweig brieﬂy dialog system three entity types name phonetype yesno. contact’s name synonyms contact phone types turn synonyms large numbers experiments created stateful goal-directed simulated user goal sometimes covered dialog system behavior stochastic example simulated user usually answers questions instead ignore system provide additional information give user simulation parameterized around probabilities. used incentivize system complete dialogs faster rather slower. policy network deﬁned implement lstm hidden units dense output layer softmax activation. value network identical structure except single output linear activation. used batch size update networks completion single dialog. used adadelta stepsize dialogs took timesteps. every dialogs policy frozen dialogs measure average task completion. figures show mean variance task completion independent runs. compared benchmarks methods require third fewer dialogs attain asymptotic performance lower variance. since neural networks naturally lend policy gradient-style updates much past work adopted broad approach. however work studied fully observable case whereas study partially observable case. example alphago applies policy gradients fully observable state board. several papers study fully-observable related work ways. degris investigates off-policy policy gradient updates limited linear models. experience replay also off-policy optimization apply neural networks. like work fatemi also estimates value network uses experience replay optimize value network evaluates conversational system task. however unlike work experience replay policy network networks rely external state tracking process render state fully-observable learn feed-forward networks rather recurrent networks. hausknecht stone applies rnns partially observable problems adopts q-learning approach rather policy gradient approach. whereas policy gradient methods strong convergence properties q-learning diverge observed attempted optimize represented lstm dialog problem. also policy network pre-trained directly expert trajectories using classical supervised learning real-world applications trajectories often available introduced methods increasing sample efﬁciency policy-gradient dialog task partially observable state best method improved sample efﬁciency third. second fully-observable task observed similar gain sample efﬁciency despite using different network architecture activation function optimizer. result shows method robust variation task network design thus seems promising generalize dialog domains well. future work apply method dialog system real human users dialog task. however knowledge none exists instead applied method public non-dialog task called lunar lander openai gym. domain continuous state space dimensions discrete actions reward safe landing designated area crash. using engines also result negative cost explained link below. episodes ﬁnish spacecraft crashes lands. used since domain fully observable chose deﬁnitions policy network corresponding fully connected neural networks hidden layers followed softmax normalization. chose relu activations hidden units based limited initial experimentation. value network architecture except output layer single node linear activation. used batch episode size found batch divergence appears frequently. used stepsize used adam algorithm default parameters. methods performs off-policy updates on-policy update value network. method performs off-policy updates on-policy update policy network. sutskever timothy lillicrap madeleine leach koray kavukcuoglu thore graepel demis hassabis. mastering game deep neural networks tree search. nature richard sutton david mcallester satinder singh yishay mansour. policy gradient methods learning function approximation. advances neural information processing systems denver pages geoffrey zweig. end-to-end control optimized supervised reinforcement learning. corr abs/. http//arxiv.org/abs/.. statistical gradient-following algorithms connectionist reinforcement learning. machine learning", "year": 2016}