{"title": "Fast Generation for Convolutional Autoregressive Models", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Convolutional autoregressive models have recently demonstrated state-of-the-art performance on a number of generation tasks. While fast, parallel training methods have been crucial for their success, generation is typically implemented in a na\\\"{i}ve fashion where redundant computations are unnecessarily repeated. This results in slow generation, making such models infeasible for production environments. In this work, we describe a method to speed up generation in convolutional autoregressive models. The key idea is to cache hidden states to avoid redundant computation. We apply our fast generation method to the Wavenet and PixelCNN++ models and achieve up to $21\\times$ and $183\\times$ speedups respectively.", "text": "convolutional autoregressive models recently demonstrated state-of-the-art performance number generation tasks. fast parallel training methods crucial success generation typically implemented na¨ıve fashion redundant computations unnecessarily repeated. results slow generation making models infeasible production environments. work describe method speed generation convolutional autoregressive models. idea cache hidden states avoid redundant computation. apply fast generation method wavenet pixelcnn++ models achieve speedups respectively. autoregressive models powerful class generative models factorize joint probability data sample product conditional probabilities. autoregressive models wavenet bytenet pixelcnn video pixel networks shown strong performance audio textual image video generation. unfortunately generating na¨ıve fashion typically slow practical use. example generating batch images using pixelcnn++ takes minutes commodity hardware tesla gpu. ability fast generation useful many applications. production environments tight latency constraints real-time speech generation machine translation image super-resolution require fast generation. furthermore quick simulation environment dynamics important fast training model-based reinforcement learning however slow generation hampers convolutional autoregressive models situations. present general method enable fast generation autoregressive models caching. describe speciﬁc implementations method wavenet pixelcnn++ demonstrate fast generation achieves wavenet pixelcnn++ na¨ıve counterparts. open-source implementation fast generation wavenet pixelcnn++. generation code compatible open-source implementations models also implement training. na¨ıve generation convolutional autoregressive models recalculates entire receptive ﬁeld every iteration salimans details). results exponential time space complexity respect receptive ﬁeld. section propose method avoids cost caching previously computed hidden states using subsequent iterations. ﬁrst start describing method speed generation models dilated convolutions. discuss generalize method strided convolutions. finally discuss details applying method speed wavenet pixelcnn++ generate single output computations must performed entire receptive ﬁeld exponential respect number layers. na¨ıve generation method repeats computation entire receptive ﬁeld every step illustrated figure however wasteful many hidden states receptive ﬁeld re-used previous iterations. na¨ıve approach used open-source implementations wavenet. instead recomputing hidden states every iteration propose caching hidden states previous iterations. figure illustrates idea layer maintains cache previously computed hidden states. generation step hidden states popped cache perform convolutions. therefore computation space complexity linear number layers instead exponential. figure comparison na¨ıve implementation generation process proposed method. orange nodes computed current timestep blue nodes previously cached states gray nodes involved current timestep. notice generating single sample requires operations na¨ıve implementation number layers network. meanwhile implementation requires operations generate single sample. figures demonstrate caching mechanism detail. layer takes current input hidden state cache compute layer output. cache queue oldest hidden state popped current layer. size cache equivalent dilation layer. thus oldest hidden state cache exactly inputs layer process. finally output current layer pushed cache next layer used input future computation. caching algorithm dilated convolutions straightforward number hidden states layer equal number inputs. thus layer simply maintain cache updated every step. however strided convolutions pose additional challenge since number hidden states layer different number inputs. figure phase dilated convolutions. hidden states popped cache input corresponding location generation model. hidden states current input used compute current output hidden states figure fast generation network strided convolutions. show example model convolutional transposed convolutional layers stride stride layer fewer states network inputs. orange nodes computed current timestep blue nodes previously cached states gray nodes involved current timestep. ﬁrst timestep ﬁrst input used compute cache nodes sufﬁcient information generate including ﬁrst four outputs. nodes sufﬁcient information computed output already computed node sufﬁcient information computed although output also computed scenario similar enough information compute multiple hidden states generate next four outputs. analogous scenario. analogous cycle followed future time steps. downsampling layer necessarily generate output timestep even skip inputs hand upsampling layer produce hidden states outputs multiple timesteps result cache cannot updated every timestep. thus cache additional property cache every cache updated every cache every steps. every downsampling layer increases cache every property layer downsampling factor variety modiﬁcations including using strided convolutions transposed convolutions instead dilation speed. method scales changes. caches layer height equal ﬁlter height width equal image width. entire generated oldest cache popped pushed. strided convolutions used cache every idea detailed section furthermore pixelcnn++ uses vertical horizontal stream details). since vertical stream depend horizontal stream efﬁcient compute vertical stream entire time cached vertical stream every computation horizontal stream. full details please refer code. implemented methods wavenet pixelcnn++ tensorflow compare proposed method na¨ıve implementation wavenet na¨ıve implementation pixelcnn++. case pixelcnn++ vary number images generated parallel mirroring batching production environments. batch size increases method signiﬁcantly outperforms na¨ıve implementation runtime. example batch sizes method orders magnitude faster. wavenet experiment batch size ﬁxed order highlight effect adding layers performance. results indicate signiﬁcant speedups wavenet pixelcnn++. figure wavenet timing experiments. generated model sets dilation layers each using na¨ıve implementation ours. results averaged repeats. small na¨ıve implementation performs better expected parallelization convolution operations. large difference performance pronounced. figure pixelcnn++ timing experiments. generated images using model architecture described huge number convolution operations na¨ıve implementation utilization always high room parallelization across batch. since method avoids redundant computations larger batch sizes result larger speedups. references mart´ın abadi paul barham jianmin chen zhifeng chen andy davis jeffrey dean matthieu devin sanjay ghemawat geoffrey irving michael isard tensorﬂow system large-scale machine learning. proceedings usenix symposium operating systems design implementation savannah georgia kalchbrenner lasse espeholt karen simonyan aaron oord alex graves koray kavukcuoglu. neural machine translation linear time. arxiv preprint arxiv. junhyuk xiaoxiao honglak richard lewis satinder singh. action-conditional video prediction using deep networks atari games. advances neural information processing systems salimans andrej karpathy chen diederik kingma. pixelcnn++ improving pixelcnn discretized logistic mixture likelihood modiﬁcations. arxiv preprint arxiv. aaron oord sander dieleman heiga karen simonyan oriol vinyals alex graves kalchbrenner andrew senior koray kavukcuoglu. wavenet generative model audio. arxiv preprint arxiv.", "year": 2017}