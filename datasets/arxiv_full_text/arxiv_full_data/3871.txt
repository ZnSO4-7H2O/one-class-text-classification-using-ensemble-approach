{"title": "Sparse Subspace Clustering: Algorithm, Theory, and Applications", "tag": ["cs.CV", "cs.IR", "cs.IT", "cs.LG", "math.IT", "math.OC", "stat.ML"], "abstract": "In many real-world problems, we are dealing with collections of high-dimensional data, such as images, videos, text and web documents, DNA microarray data, and more. Often, high-dimensional data lie close to low-dimensional structures corresponding to several classes or categories the data belongs to. In this paper, we propose and study an algorithm, called Sparse Subspace Clustering (SSC), to cluster data points that lie in a union of low-dimensional subspaces. The key idea is that, among infinitely many possible representations of a data point in terms of other points, a sparse representation corresponds to selecting a few points from the same subspace. This motivates solving a sparse optimization program whose solution is used in a spectral clustering framework to infer the clustering of data into subspaces. Since solving the sparse optimization program is in general NP-hard, we consider a convex relaxation and show that, under appropriate conditions on the arrangement of subspaces and the distribution of data, the proposed minimization program succeeds in recovering the desired sparse representations. The proposed algorithm can be solved efficiently and can handle data points near the intersections of subspaces. Another key advantage of the proposed algorithm with respect to the state of the art is that it can deal with data nuisances, such as noise, sparse outlying entries, and missing entries, directly by incorporating the model of the data into the sparse optimization program. We demonstrate the effectiveness of the proposed algorithm through experiments on synthetic data as well as the two real-world problems of motion segmentation and face clustering.", "text": "abstract—many real-world problems deal collections high-dimensional data images videos text documents microarray data more. often high-dimensional data close low-dimensional structures corresponding several classes categories data belong. paper propose study algorithm called sparse subspace clustering cluster data points union low-dimensional subspaces. idea that among inﬁnitely many possible representations data point terms points sparse representation corresponds selecting points subspace. motivates solving sparse optimization program whose solution used spectral clustering framework infer clustering data subspaces. since solving sparse optimization program general np-hard consider convex relaxation show that appropriate conditions arrangement subspaces distribution data proposed minimization program succeeds recovering desired sparse representations. proposed algorithm efﬁcient handle data points near intersections subspaces. another advantage proposed algorithm respect state deal directly data nuisances noise sparse outlying entries missing entries incorporating model data sparse optimization program. demonstrate effectiveness proposed algorithm experiments synthetic data well real-world problems motion segmentation face clustering. index terms—high-dimensional data intrinsic low-dimensionality subspaces clustering sparse representation -minimization convex programming spectral clustering principal angles motion segmentation face clustering. machine learning signal image processing computer vision pattern recognition bioinformatics etc. instance images consist billions pixels videos millions frames text documents associated hundreds thousands features etc. high-dimensionality data increases computational time memory requirements algorithms also adversely affects performance noise effect insufﬁcient number samples respect ambient space dimension commonly referred curse dimensionality however high-dimensional data often low-dimensional structures instead uniformly distributed across ambient space. recovering low-dimensional structures data helps reduce computational cost memory requirements algorithms also reduce effect high-dimensional noise data improve performance inference learning recognition tasks. fact many problems data class category well represented low-dimensional subspace highdimensional ambient space. example feature trajectories rigidly moving object video face images subject varying illumination multiple instances handwritten digit different rotations translations thicknesses low-dimensional subspace ambient space. result collection data multiple classes categories union low-dimensional subspaces. subspace clustering references therein) refers problem separating data according underlying subspaces ﬁnds numerous applications image processing computer vision motion segmentation temporal video segmentation illustrated figures since data subspace often distributed arbitrarily around centroid standard clustering methods take advantage spatial proximity data cluster general applicable subspace clustering. therefore need clustering algorithms take account multi-subspace structure data. prior work subspace clustering existing algorithms divided four main categories iterative algebraic statistical spectral clustering-based methods. iterative methods. iterative approaches k-subspaces median k-ﬂats alternate assigning points subspaces ﬁtting subspace cluster. main drawbacks approaches generally require know number dimensions subspaces sensitive initialization. algebraic approaches. factorization-based algebraic approaches initial segmentation thresholding entries similarity matrix built factorization data matrix. methods provably correct subspaces independent fail assumption violated. addition sensitive noise outliers data. algebraic-geometric approaches generalized principal component analysis data polynomial whose gradient point gives normal vector subspace containing point. gpca fig. motion segmentation given feature points multiple rigidly moving objects tracked multiple frames video goal separate feature trajectories according moving objects statistical methods. iterative statistical approaches mixtures probabilistic multi-stage learning assume distribution data inside subspace gaussian alternate data clustering subspace estimation applying expectation maximization main drawbacks methods generally need know number dimensions subspaces sensitive initialization. robust statistical approaches random sample consensus subspace dimension randomly chosen subsets points number inliers large enough. inliers removed process repeated second subspace ransac deal noise outliers need know number subspaces. however dimensions subspaces must known equal. addition complexity algorithm increases exponentially dimension subspaces. information-theoretic statistical approaches agglomerative lossy compression look segmentation data minimizes coding length needed points mixture degenerate gaussians given distortion. minimization problem np-hard suboptimal solution found ﬁrst assuming point forms group iteratively merging pairs groups reduce coding length. handle noise outliers data. while principle need know number dimensions subspaces number subspaces found algorithms dependent choice distortion parameter. addition theoretical proof optimality agglomerative algorithm. spectral clustering-based methods. local spectral clusteringbased approaches local subspace afﬁnity locally linear manifold clustering spectral local best-ﬁt flats local information around point build similarity pairs points. segmentation data obtained applying spectral clustering similarity matrix. methods difﬁculties dealing points near intersection subspaces neighborhood point contain points different subspaces. addition sensitive right choice neighborhood size compute local information point. global spectral clustering-based approaches resolve issues building better similarities data points using global information. spectral curvature clustering uses multi-way similarities capture curvature collection points within afﬁne subspace. deal noisy data requires know number dimensions subspaces assumes subspaces dimension. addition complexity building multi-way similarity grows exponentially dimensions subspaces hence practice sampling strategy employed reduce computational cost. using advances sparse low-rank recovery algorithms sparse subspace clustering low-rank recovery low-rank subspace clustering algorithms pose clustering problem ﬁnding sparse low-rank representation data dictionary data itself. solution corresponding global optimization algorithm used build similarity graph segmentation data obtained. advantages methods respect state-of-the-art algorithms handle noise outliers data need know dimensions principle number subspaces priori. paper contributions paper propose study algorithm based sparse representation techniques called sparse subspace clustering cluster collection data points lying union low-dimensional subspaces. underlying idea behind algorithm call self-expressiveness property data states data point union subspaces efﬁciently represented linear afﬁne combination points. representation unique general inﬁnitely many ways data point assume know priori bases subspaces know data points belong subspace. subspace clustering problem refers problem ﬁnding number subspaces dimensions basis subspace segmentation data address subspace clustering problem propose algorithm consists steps. ﬁrst step data point points belong subspace. propose global sparse optimization program whose solution encodes information memberships data points underlying subspace point. second step information spectral clustering framework infer clustering data. eliminates trivial solution writing point linear combination itself. words matrix data points self-expressive dictionary point written linear combination points. however representation dictionary unique general. comes fact number data points subspace often greater dimension i.e. result consequently non-trivial nullspace giving rise inﬁnitely many representations data point. observation proposed algorithm among speciﬁcally data point lies d-dimensional subspace written linear combination points general directions result ideally sparse representation data point ﬁnds points subspace number nonzero elements corresponds dimension underlying subspace. expressed combination points. observation sparse representation data point ideally corresponds combination points subspace. motivates solving global sparse optimization program whose solution used spectral clustering framework infer clustering data. result overcome problems local spectral clustering-based algorithms choosing right neighborhood size dealing points near intersection subspaces since given data point sparse optimization program automatically picks points necessarily close belong subspace. since solving sparse optimization program general np-hard consider relaxation. show that mild conditions arrangement subspaces data distribution proposed -minimization program recovers desired solution guaranteeing success algorithm. theoretical analysis extends sparse representation theory multisubspace setting number points subspace arbitrary possibly much larger dimension. unlike blocksparse recovery problems bases subspaces known given bases subspaces know data points belong subspace making case challenging. sparsifying dictionary union subspaces given matrix data points. proposed -minimization program solved efﬁciently using convex programming tools require initialization. algorithm directly deal noise sparse outlying entries missing entries data well general class afﬁne subspaces incorporating data corruption subspace model sparse optimization program. finally experimental results show algorithm outperforms state-of-the-art subspace clustering methods real-world problems motion segmentation face clustering paper organization. section motivate introduce algorithm clustering data points union linear subspaces. section generalize algorithm deal noise sparse outlying entries missing entries data well general class afﬁne subspaces. section investigate theoretical conditions minimization program recovers desired sparse representations data points. section discuss connectivity similarity graph propose regularization term increase connectivity points subspace. section verify theoretical analysis experiments synthetic data. section compare performance state real-world problems motion segmentation face clustering. finally section concludes paper. sparse subspace clustering section introduce sparse subspace clustering algorithm clustering collection multi-subspace data using sparse representation techniques. motivate formulate algorithm data points perfectly union linear subspaces. next section generalize algorithm deal data nuisances noise sparse outlying entries missing entries well general class afﬁne subspaces. fig. three subspaces data points subspace ordered last points belong respectively. solution q-minimization program lying shown. note value decreases sparsity solution increases. solution corresponds choosing points lying different choices different effects obtained solution. typically decreasing value inﬁnity toward zero sparsity solution increases shown figure extreme case corresponds general np-hard problem ﬁnding sparsest representation given point -norm counts number nonzero elements solution. since interested efﬁciently ﬁnding non-trivial sparse representation dictionary consider minimizing tightest convex relaxation -norm i.e. solution corresponds subspace-sparse representations data points next infer clustering data. section study conditions convex optimization program guaranteed recover subspace-sparse representation data point. clustering using sparse coefﬁcients solving proposed optimization program obtain sparse representation data point whose nonzero elements ideally correspond points subspace. next step algorithm infer segmentation data different subspaces using sparse coefﬁcients. address problem build weighted graph denotes nodes graph corresponding data points denotes edges nodes. rn×n symmetric non-negative similarity matrix representing weights edges i.e. node connected node edge whose weight equal wij. ideal similarity matrix hence ideal similarity graph nodes correspond points subspace connected edges nodes correspond points different subspaces. note sparse optimization program ideally recovers subspace-sparse representation point i.e. representation whose nonzero elements correspond points subspace given data point. provides immediate choice similarity matrix |c|. words node connects node edge whose weight equal |cij| +|cji|. reason symmetrization that general data point write linear combination points including however necessarily choose sparse representation. particular choice weight make sure nodes connected either sparse representation other. similarity matrix data points clustering data subspaces follows applying spectral clustering graph speciﬁcally obtain clustering data applying kmeans algorithm normalized rows matrix whose columns bottom eigenvectors symmetric normalized laplacian matrix graph. remark optional step prior building similarity graph normalize sparse coefﬁcients ci/ci∞. helps better deal different norms data points. speciﬁcally data point large euclidean norm selects points small euclidean norms values nonzero coefﬁcients generally large. hand data point small euclidean norm selects points large euclidean norms values nonzero coefﬁcients generally small. since spectral clustering puts emphasis keeping stronger connections graph normalization step make sure largest edge weights nodes scale. algorithm summarizes algorithm. note advantage spectral clustering shown experimental results provides robustness respect errors sparse representations data points. words long edges points different subspaces weak spectral clustering correct segmentation. obtain symmetric similarity matrix directly impose constraint optimization program. however results increasing complexity optimization program practice perform better post-symmetrization described above. also processing approaches similarity matrix. remark principle need know number subspaces. speciﬁcally conditions theoretical results section similarity graph connections points different subspaces. thus determine number subspaces ﬁnding number graph components obtained analyzing eigenspectrum laplacian matrix however connections points different subspaces model selection techniques employed practical extensions real-world problems data often corrupted noise sparse outlying entries measurement/process noise adhoc data collection techniques. cases data perfectly union subspaces. instance motion segmentation problem malfunctioning tracker feature trajectories corrupted noise entries large errors similarly clustering human faces images corrupted errors specularities cast shadows occlusions hand data points missing entries e.g. tracker loses track feature points video occlusions finally data union afﬁne subspaces general model includes linear subspaces particular case. section generalize algorithm clustering data lying perfectly union linear subspaces deal aforementioned challenges. unlike state-of-the-art methods require separate algorithm ﬁrst correct errors data deal problems uniﬁed framework incorporating model corruption sparse optimization program. thus sparse coefﬁcients encode information memberships data subspaces used spectral clustering framework before. note equation sparse solution since expressed linear combination points using terms corrupted point rewriting sparse outlying entries vector noise vector substituting obtain since sparse solution also correspond vectors sparse outlying entries noise respectively. precisely nonzero vector sparse outlying entries since linear combination vectors outlying entries similarly nonzero signiﬁcantly large magnitudes vector noise since linear combination noise vectors collecting columns matrices objective solution corresponds sparse coefﬁcient matrix corresponds matrix sparse outlying entries noise matrix. propose solve following optimization program -norm promotes sparsity columns frobenius norm promotes small entries columns parameters balance three terms objective function. note optimization program convex respect optimization variables hence solved efﬁciently using convex programming tools. data corrupted noise eliminate optimization program hand data corrupted sparse outlying entries eliminate practice however also deal small errors noise. following proposition suggests setting αz/µz αe/µe i-th data point obtained corrupting error perfectly lies subspace vector free point large nonzero sparse outlying entries integer noise elements i.e. whose norm bounded since error-free data points perfectly union subspaces proposition consider optimization program withleast show that broad conditions |cij| bounded square root dimension underlying subspace theoretical guarantees proposed optimization program case corrupted data subject current research. data point optimal solution also without term exists least data point solving proposed optimization programs build similarity graph infer clustering data using spectral clustering. thus incorporating corruption model data sparse optimization program deal clustering corrupted data before without explicitly running separate algorithm correct errors data missing entries consider clustering incomplete data entries subset data points missing. note small fraction entries data point missing clustering incomplete data cast clustering data sparse outlying entries. precisely missing entries data point random values hence obtain data points sparse outlying entries. clustering data follows solving applying spectral clustering graph built using obtained sparse coefﬁcients. however drawback approach disregards fact know locations missing entries data matrix. possible cases cast clustering data missing entries clustering complete data. this consider collection data points {yi}n denote indices known entries deﬁne thus every index data points known entries. size denoted small relative ambient space dimension project data hence original subspaces subspace spanned columns identity matrix indexed apply algorithm obtained complete data. words keep rows indexed obtain optimization program infer clustering data applying spectral clustering graph built using sparse coefﬁcient matrix. note approach described based assumption nonempty. addressing problem subspace clustering missing entries empty small size subject future research. afﬁne subspaces real-world problems data union afﬁne rather linear subspaces. instance motion segmentation problem involves clustering data union -dimensional afﬁne subspaces naive deal case ignore afﬁne structure data perform clustering case linear subspaces. comes fact d-dimensional afﬁne subspace considered subset )-dimensional linear subspace includes origin. however drawback possibly increasing dimension intersection subspaces cases result indistinguishability subspaces other. example different lines plane form dimensional linear subspace including origin hence become indistinguishable. fig. left three -dimensional subspaces independent span -dimensional space dimensions also right three -dimensional disjoint subspaces intersect origin. which comparison case linear subspaces includes additional linear equality constraints. note deal linear subspaces well since linear subspace also afﬁne subspace. subspace-sparse recovery theory underlying assumption success algorithm proposed optimization program recovers subspacesparse representation data point i.e. representation whose nonzero elements correspond subspace given point. section investigate conditions which data points union linear subspaces sparse optimization program recovers subspace-sparse representations data points. investigate recovery conditions classes subspace arrangements independent disjoint subspace models independent independent since span -dimensional space dimensions also hand subspaces shown figure independent since span -dimensional space dimensions said disjoint every pair subspaces intersect origin. words every pair subspaces dim. example subspace arrangements shown figure disjoint since pair subspaces intersect origin. note that based deﬁnitions notion disjointness weaker independence independent subspace model always disjoint converse necessarily true. independent subspace model section consider data points union independent subspaces underlying model many subspace clustering algorithms. show -minimization program generally q-minimization always recover subspace-sparse representations data points. speciﬁcally show following result. theorem consider collection data points drawn independent subspaces {si}n denote data points rank denote data points subspaces except then every every nonzero q-minimization program recovers subspace-sparse representation i.e. note subspace-sparse recovery holds without assumption distribution data points subspace rank comes price restrictive model subspace arrangements. next show general class disjoint subspaces appropriate conditions relative conﬁguration subspaces well distribution data subspace -minimization recovers subspace-sparse representations data points. disjoint subspace model consider general class disjoint subspaces investigate conditions optimization program recovers subspace-sparse representation data point. consider vector intersection ⊕j=isj optimal solution -minimization restrict dictionary data points theorem consider collection data points drawn disjoint subspaces {si}n denote data points rank denote data points subspaces except -minimization recovers subspace-sparse representation every nonzero i.e. necessary sufﬁcient condition guarantees successful subspace-sparse recovery -minimization program explicitly show relationship subspace arrangements data distribution success -minimization program. establish relationship show depends singular values data points depends subspace angles subspaces. then sufﬁcient condition establishes relationship subspace angles data distribution minimization successful subspace-sparse recovery since implies holds every nonzero -minimization recovers subspace-sparse solution i.e. loosely speaking sufﬁcient condition theorem states smallest principal angle subspace larger certain value depends data distribution subspace-sparse recovery holds. bound rather high norms data points oddly distributed e.g. maximum norm data points much smaller maximum norm data points subspaces. since segmentation data change data points scaled apply linear subspaces normalizing data points unit euclidean norms. case sufﬁcient condition reduces remark independent subspaces intersection subspace direct subspaces origin hence condition always holds. result theorem -minimization always recovers subspace-sparse representations data points independent subspaces. fig. left nonzero intersection polytope reaches smaller hence subspace-sparse recovery holds. middle subspace angle decreases polytope reaches smaller right distribution data becomes nearly degenerate case close line polytope reaches smaller cases middle right subspace-sparse recovery hold points intersecion. remark condition closely related nullspace property sparse recovery literature difference however require inequality hold optimal solutions instead feasible solution. thus inequality violated many feasible solutions still hold optimal solutions guaranteeing successful subspace-sparse recovery theorem thus result thought generalization nullspace property multi-subspace setting number points subspace arbitrary. geometric interpretation section provide geometric interpretation subspace-sparse recovery conditions necessary recall relationship -norm optimal solution -norm optimal solution corresponds smallest scaled polytope reaches denote symmetrized convex polytopes respectively. condition following geometric interpretation subspace-sparse recovery holds nonzero intersection ⊕j=isj reaches αp−i i.e. smaller shown left plot figure intersection polytope reaches hence subspace-sparse recovery condition holds. hand principal angles subspaces decrease shown middle plot figure subspacesparse recovery condition hold since polytope reaches also shown right plot figure distribution data becomes nearly degenerate case close -dimensional subspace orthogonal direction subspace-sparse recovery condition hold since reaches note sufﬁcient condition translates relationship polytopes mentioned above explicitly terms relationship subspace angles singular values data. fig. coefﬁcient matrix obtained solution data points subspaces. left right increasing results concentration nonzero elements rows coefﬁcient matrix hence choosing common data points. graph connectivity previous section studied conditions proposed -minimization program recovers subspace-sparse representations data points. result similarity graph points different subspaces connected other. hand extensive experimental results synthetic real data show data points subspace always form connected component graph hence subspaces similarity graph connected components. theoretically veriﬁed connectivity points subspace dimensional subspaces. however shown that subspaces dimensions greater equal distribution data possible points subspace form multiple components graph. section consider regularization term sparse optimization program promotes connectivity points within subspace. idea data points subspace choose common points subspace sparse representations form single component similarity graph. thus sparse optimization program regularization term denotes indicator function denotes i-th hence minimizing corresponds minimizing number nonzero rows i.e. choosing common data points sparse representation point. another approach deal connectivity issue analyze subspaces corresponding components graph merge components whose associated subspaces small distance other i.e. small principal angle. however result sensitive choice dimension subspaces component well threshold value principal angles merge subspaces. fig. left three -dimensional subspaces normalized data points. middle corresponds solution similarity graph three components corresponding three subspaces. right corresponds solution -norm. example consider three -dimensional subspaces shown figure data points unit euclidean norms angle well equal note example sufﬁcient condition result solution holds values recovers subspace-sparse representation data point example uniquely given shown figure hence similarity graph exactly connected components corresponding data points subspace. another feasible solution given shown figure points choose points representations. hence similarity graph connected component. note large range subspace angles cannot recover subspace-sparse representations data points. suggests using row-sparsity regularizer small value experiments synthetic data section showed success -minimization subspace-sparse recovery depends principal angles subspaces distribution data subspace. section verify relationship experiments synthetic data. dimension embedded d-dimensional ambient space. make problem hard enough every data point subspace also reconstructed linear combination points fig. subspace-sparse recovery error subspace clustering error three disjoint subspaces. increasing number points smallest principal angle decreases errors. subspaces generate subspaces smallest principal angles equal thus verify effect smallest principal angle subspace-sparse recovery changing value investigate effect data distribution subspace-sparse recovery generate number data points subspace random change value typically number data points subspace increases probability data close degenerate subspace decreases. generating three d-dimensional subspaces associated solve -minimization program data point measure different errors. first denoting sparse representation corresponding points measure subspacesparse recovery error term inside summation indicates fraction -norm comes points subspaces. error zero corresponds choosing points subspace error equal corresponds choosing points subspaces. second building similarity graph using sparse coefﬁcients applying spectral clustering measure subspace clustering error multipliers framework whose derivation provided supplementary material. motion segmentation experiments noisy variation optimization program i.e. without term afﬁne constraint choose experiments face clustering experiments sparse outlying entries variation optimization program i.e. without term choose experiments also worth mentioning performs better admm approach general interior point solvers typically return many small nonzero coefﬁcients degrading spectral clustering result. state-of-the-art algorithms codes provided authors. nearest neighbors dimension local subspaces motion segmentation nearest neighbors dimension face clustering. dimension subspaces motion segmentation face clustering. motion segmentation face clustering. note algorithm according similar applies spectral clustering similarity graph built directly solution proposed optimization program. however code algorithm applies heuristic post-processing step similar low-rank solution prior building similarity graph thus compare effectiveness sparse versus low-rank objective function investigate effect post-processing step report results cases without heuristic post-processing step. lrsc method parameter motion segmentation variant method parameters face clustering. finally need know number subspaces priori estimation number subspaces eigenspectrum graph laplacian noisy setting often unreliable order fair comparison provide number subspaces input algorithms. datasets statistics. motion segmentation problem consider hopkins dataset consists video sequences motions corresponding low-dimensional subspaces video face clustering problem consider extended yale dataset consists face images human subjects images subject low-dimensional subspace describing problem detail presenting experimental results present statistics datasets help better understand challenges subspace clustering performance different algorithms. first compute smallest principal angle pair subspaces motion segmentation problem corresponds pair motions video face clustering problem corresponds pair subjects. then compute percentage subspace pairs whose smallest principal angle certain value ranges degrees. figure shows corresponding graphs datasets. original published code contains function compacc.m computing misclassiﬁcation rate erroneous. used correct code computing misclassiﬁcation rate result reported performance lrr-h different published results fig. left percentage pairs subspaces whose smallest principal angle smaller given value. right average percentage data points pairs subspaces k-nearest neighbors subspace. fig. left singular values several motions hopkins dataset. motion corresponds subspace dimension right singular values several faces extended yale dataset. subject corresponds subspace dimension around compute average errors trials results shown figure note either small subspace-sparse recovery error clustering error large predicted theoretical analysis. hand increases errors decrease sufﬁciently large obtain zero errors. results also verify success clustering relies success -minimization recovering subspace-sparse representations data points. note small increase subspacesparse recovery error large slightly decreases clustering error increases. fact increasing number points number undesirable edges different subspaces similarity graph increases making spectral clustering difﬁcult. note also that values subspace-sparse recovery error zero i.e. points different subspaces connected similarity graph clustering error also zero. implies that cases similarity graph exactly three connected components i.e. data points subspace form single component graph. experiments real data section evaluate performance algorithm dealing real-world problems segmenting multiple motions videos clustering images human faces compare performance best stateof-the-art subspace clustering algorithms lrsc shown subspaces datasets relatively small principal angles. hopkins- dataset principal angles subspaces always smaller degrees extended yale dataset principal angles subspaces degrees. second pair subspaces compute percentage data points k-nearest neighbors subspace. figure shows average percentages possible pairs subspaces dataset. shown hopkins- dataset almost data points nearest neighbors belong subspace. hand extended yale dataset relatively large number data points whose nearest neighbors come subspace. percentage rapidly increases number nearest neighbors increases. result plots figure conclude hopkins dataset challenge subspaces small principal angles extended yale dataset beside principal angles subspaces small challenge data points subspace close subspaces. motion segmentation motion segmentation refers problem segmenting video sequence multiple rigidly moving objects multiple spatiotemporal regions correspond different motions scene problem often solved extracting tracking feature points frames video. data point also called feature trajectory corresponds -dimensional vector obtained stacking feature points video motion segmentation refers problem separating feature trajectories according underlying motions. afﬁne projection model feature trajectories associated single rigid motion afﬁne subspace dimension equivalently linear subspace dimension therefore feature trajectories rigid motions union low-dimensional subspaces hence motion segmentation reduces clustering data points union subspaces. section evaluate performance algorithm well state-of-the-art subspace clustering methods problem motion segmentation. consider hopkins dataset consists video sequences videos motions videos three motions. average dataset sequence motions feature trajectories frames sequence motions feature trajectories frames. left plot figure shows singular values several motions dataset. note ﬁrst four singular values nonzero rest singular values close zero corroborating dimensionality underlying linear subspace motion. addition shows feature trajectories video well modeled data points almost perfectly union linear subspaces dimension results applying subspace clustering algorithms dataset original -dimensional feature trajectories project data n-dimensional subspace using shown table table respectively. results make following conclusions cases obtains small clustering error outperforming algorithms. suggests separation different motion subspaces terms principal angles distribution feature trajectories motion subspace sufﬁcient success sparse optimization program hence clustering. numbers inside parentheses show clustering errors without normalizing similarity matrix i.e. without step algorithm notice that explained remark normalization step helps improve clustering results. however improvement small i.e. performs well without post-processing without post-processing coefﬁcient matrix higher errors algorithms. hand postprocessing low-rank coefﬁcient matrix signiﬁcantly improves clustering performance lrsc tries noise-free dictionary data ﬁnding low-rank representation. helps improve lrr. also note errors lrsc higher reported ones comes fact used erroneous compacc.m function compute errors. clustering performances different algorithms using -dimensional feature trajectories n-dimensional projections close. comes fact feature trajectories motions video almost perfectly n-dimensional linear subspace -dimensional ambient space. thus projection using onto n-dimensional subspace preserves structure subspaces data study effect number subjects clustering performance different algorithms devise following experimental setting divide subjects groups ﬁrst three groups correspond subjects fourth group corresponds subjects ﬁrst three groups consider choices subjects last group consider choices finally apply clustering algorithms trial i.e. subjects. shown plot face data figure face images perfectly linear subspace corrupted errors. fact errors correspond cast shadows specularities face images modeled sparse outlying entries. result important subspace clustering algorithm effectively deal data sparse corruptions. order validate fact corruption faces sparse outlying errors show importance dealing corruptions clustering start following experiment. apply robust principal component analysis algorithm remove sparse outlying entries face data subject. note practice know clustering data beforehand hence cannot apply rpca faces subject. however show experiment illustrates challenges face clustering validates several conclusions performances different algorithms. table shows clustering error different algorithms applying rpca data points subject removing sparse outlying entries i.e. bringing data points back low-dimensional subspaces. results make following conclusions clustering error close zero different number subjects suggesting deal well face clustering face images corruption free. words data different subspaces close other shown figure performance dependent principal angles subspaces which small large enough success ssc. lrsc algorithms also clustering errors showing effectiveness removing sparse outliers clustering performance. hand lrr-h clustering error subjects relatively large error subjects showing post processing step obtained low-rank coefﬁcient matrix always improves result lrr. clustering error relatively large error increases number subjects increases. comes fact that shown figure face images neighborhood data point contains points belong subjects addition number neighbors subjects increases increase number subjects. figure show effect regularization parameter αz/µz clustering performance entire hopkins dataset. note clustering errors function follow similar pattern using dimensional data n-dimensional data. moreover cases clustering error less cases large range values finally notice results tables coincide reported mainly fact used random projections dimensionality reduction original -dimensional data. addition used solver compute subspacesparse representation admm solver. also notice improved overall clustering error case n-dimensional data reported using nearest neighbors instead given face images multiple subjects acquired ﬁxed pose varying illumination consider problem clustering images according subjects shown that lambertian assumption images subject ﬁxed pose varying illumination close linear subspace dimension thus collection face images multiple subjects close union -dimensional subspaces. section evaluate clustering performance well state-of-the-art methods extended yale dataset dataset consists pixel cropped face images individuals frontal face images subject acquired various lighting conditions. reduce computational cost memory requirements algorithms downsample images pixels treat -dimensional vectorized image data point hence right plot figure shows singular values data points several subjects dataset. note singular value curve knee around corroborating approximate -dimensionality face data subject. addition singular values gradually decay zero showing data corrupted errors. thus face images subjects modeled corrupted data points lying close union -dimensional subspaces. applying rpca simultaneously subjects practice cannot apply rpca separately data subject clustering unknown. section deal sparse outlying entries data applying rpca algorithm collection data points trial prior clustering. results shown table make following conclusions clustering error different number subjects. speciﬁcally obtains clustering data points subjects respectively. applying rpca data points simultaneously effective applying rpca data points subject separately. comes fact rpca tends bring data points common low-rank subspace result decreasing principal angles subspaces decreasing distances data points different subjects. explain increase clustering error clustering algorithms respect results table using original data points finally apply clustering algorithms original data points without pre-processing data. results shown table make following conclusions algorithm obtains clustering error numbers subjects obtaining clustering error subjects respectively. fact error smaller applying rpca data points. fact directly incorporates corruption model data sparse outlying entries sparse optimization program giving ability perform clustering corrupted data. also regularization term deal corrupted data clustering error relatively large especially number subjects increases. fact clear relationship corruption data point regularization term general hand post processing step lrr-h lowrank coefﬁcient matrix helps signiﬁcantly reduce clustering error although larger error. lrsc tries recover error-free data points ﬁnding low-rank representation obtains smaller errors lrr. explicit deal corrupted data. together fact face images subject relatively large number neighbors subjects shown figure result performances algorithms. computational time comparison average computational time algorithm function number subjects shown figure note computational time drastically higher algorithms. comes fact complexity increases exponentially dimension subspaces case hand lrsc fast efﬁcient convex optimization techniques keeps computational time lower algorithms. exact computational times provided supplementary materials. conclusions future work studied problem clustering collection data points close union low-dimensional subspaces. proposed subspace clustering algorithm based sparse representation techniques called ﬁnds sparse representation point dictionary points tron vidal motion segmentation presence outlying incomplete corrupted trajectories ieee transactions pattern analysis machine intelligence vol. pollefeys general framework motion segmentation independent articulated rigid non-rigid degenerate non-degenerate european conf. computer vision zelnik-manor irani degeneracies dependencies implications multi-body multi-sequence factorization ieee conf. computer vision pattern recognition vol. weiss jordan spectral clustering analysis algorithm neural information processing systems luxburg tutorial spectral clustering statistics comput donoho large underdetermined systems linear equations minimal -norm solution also sparsest solution communications pure applied mathematics vol. clustering disjoint subspaces sparse representation ieee international conference acoustics speech signal processing soltanolkotabi candes geometric analysis subspace builds similarity graph using sparse coefﬁcients obtains segmentation data using spectral clustering. showed that appropriate conditions arrangement subspaces distribution data algorithm succeeds recovering desired sparse representations data points. advantage algorithm ability directly deal data nuisances noise sparse outlying entries missing entries well general class afﬁne subspaces incorporating corresponding models sparse optimization program. experiments real data face images motions videos showed effectiveness algorithm superiority state art. interesting avenues research currently investigating include theoretical analysis subspace-sparse recovery presence noise sparse outlying entries missing entries data. extensive experiments synthetic real data show points subspace general form single component similarity graph. theoretical analysis connectivity similarity graph points subspace probabilistic framework would provide better understanding observation. finally making steps solving sparse optimization program spectral clustering applicable large datasets interesting practical subject future work. references bellman dynamic programming. princeton university press tomasi kanade shape motion image streams orthography international journal computer vision vol. yang wright sastry unsupervised segmentation natural images lossy data compression computer vision image understanding vol. costeira kanade multibody factorization method independently moving objects. int. journal computer vision vol. parvaresh vikalo misra hassibi recovering sparse signals using sparse measurement matrices compressed microarrays ieee journal selected topics signal processing vol. jun. eldar kuppinger bolcskei compressed sensing blocksparse signals uncertainty relations efﬁcient recovery ieee trans. signal processing vol. june boyd parikh peleato eckstein distributed optimization statistical learning alternating direction method multipliers foundations trends machine learning vol. gribonval nielsen sparse representations unions bases ieee trans. information theory vol. dec. berg friedlander theoretical empirical results recovery multiple measurements ieee trans. information theory vol. tropp. algorithms simultaneous sparse approximation. part convex relaxation signal processing special issue sparse approximations signal image processing vol. lauer schn¨orr spectral clustering linear subspaces motion segmentation ieee international conference computer vision tron vidal benchmark comparison motion segmentation algorithms ieee conference computer vision pattern recognition words achieve minimum cost among feasible solutions hence exists {··· maxi solution optimization program consider optimization program without term which using rewritten section prove theorem paper showed data points union independent subspaces solution q-minimization recovers subspace-sparse representations data points. theorem consider collection data points drawn independent subspaces {si}n denote data points rank denote data points subspaces except then every every nonzero q-minimization program note left hand side equation corresponds point subspace right hand side corresponds point subspace ⊕j=isj. independence assumption subspaces ⊕j=isj also independent hence disjoint intersect origin. thus must −ic∗ obtain ic∗. proof theorem section prove theorem paper provide necessary sufﬁcient condition subspace-sparse recovery union disjoint subspaces. consider vector intersection ⊕j=isj optimal solution -minimization restrict dictionary points also optimal solution minimization restrict dictionary points subspaces except show every nonzero intersection ⊕j=isj -norm solution strictly smaller -norm solution i.e. theorem consider collection data points drawn denote disjoint subspaces {si}n data points rank denote data points subspaces except -minimization hence proving desired result. prove result using contradiction. assume exists nonzero condition hold intersection ⊕j=isj result solution minimization program corresponds selecting points subspaces except contradicts subspace-sparse recovery assumption. note proposed convex programs solved using generic convex solvers cvx. however generic solvers typically high computational costs scale well dimension number data points. section study efﬁcient implementations proposed sparse optimizations using alternating direction method multipliers method consider general optimization program overall procedure admm algorithm introduce appropriate auxiliary variables optimization program augment constraints objective function iteratively minimize lagrangian respect primal variables maximize respect lagrange multipliers. abuse notation throughout section denote diag vector whose elements diagonal entries diagonal matrix whose diagonal elements diagonal entries whose solution coincides solution shortly introducing helps obtain efﬁcient updates optimization variables. next using parameter objective function penalty terms three steps repeated convergence achieved number iterations exceeds maximum iteration number. convergence achieved denotes error tolerance primal dual residuals. practice choice works well real experiments. summary algorithm shows updates admm implementation optimization program note adding penalty terms change optimal solution i.e. solutions since feasible solution satisﬁes constraints penalty terms vanish. however adding penalty terms makes objective function strictly convex terms optimization variables allows using admm approach. denotes trace operator given matrix. admm approach consists iterative procedure follows denote optimization variables iteration lagrange multipliers iteration subjects. note computational times based codes algorithms used authors. important mention implemented using faster optimization solvers. speciﬁcally made faster using ladmap method proposed linearized alternating direction method adaptive penalty low-rank representation nips also made faster using ladm method proposed yang zhang. alternating direction algorithms problems compressive sensing. siam scientiﬁc computing", "year": 2012}