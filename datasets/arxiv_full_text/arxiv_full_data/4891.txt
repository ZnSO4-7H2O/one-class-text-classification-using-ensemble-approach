{"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "tag": ["cs.LG", "cs.AI"], "abstract": "Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of algebraic and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.", "text": "combining abstract symbolic reasoning continuous neural reasoning grand challenge representation learning. step direction propose architecture called neural equivalence networks problem learning continuous semantic representations algebraic logical expressions. networks trained represent semantic equivalence even expressions syntactically different. challenge semantic representations must computed syntax-directed manner semantics compositional time small changes syntax lead large changes semantics difﬁcult continuous neural architectures. perform exhaustive evaluation task checking equivalence highly diverse class symbolic algebraic boolean expression types showing model signiﬁcantly outperforms existing architectures. combining abstract symbolic reasoning continuous neural reasoning grand challenge representation learning. particularly important dealing exponentially large domains source code logical expressions. symbolic notation allows abstractly represent large states perceptually different. although symbolic reasoning powerful also tends hard. example problems satisﬁablity boolean expressions automated formal proofs tend np-hard worse. raises exciting opportunity using pattern recognition within symbolic reasoning learn patterns datasets symbolic expressions approximately represent semantic relationwork started allamanis edinburgh. work done kohli microsoft. microsoft research cambridge university edinburgh deepmind london alan turing institute london correspondence miltiadis allamanis <t-miallamicrosoft.com>. ships. however apart notable exceptions area received relatively little attention machine learning. work explore direction learning continuous semantic representations symbolic expressions. goal expressions similar semantics similar continuous representations even syntactic representation different. representations potential allow class symbolic reasoning methods based heuristics depend continuous representations example guiding search procedure symbolic solver based distance metric continuous space. paper make ﬁrst essential step addressing problem learning continuous semantic representations symbolic expressions. given access training pairs expressions semantic equivalence known assign continuous vectors symbolic expressions semantically equivalent syntactically diverse expressions assigned identical continuous vectors. important hard problem; learning composable semvecs symbolic expressions requires learn semantics symbolic elements operators continuous representation space thus encapsulating implicit knowledge symbolic semantics recursive abstractive nature. show evaluation relatively simple logical polynomial expressions present signiﬁcant challenges semantics cannot sufﬁciently represented existing neural network architectures. work similar spirit work zaremba focus learning expression representations search computationally efﬁcient identities. recursive neural networks modeling homogenous single-variable polynomial expressions. present impressive results treenn model fails applied complex symbolic polynomial boolean expressions. particular experiments treenns tend assign similar representations syntactically similar expressions even semantically different. underlying conceptual problem develop continuous representation follows syntax much tackle problem propose architecture called neural equivalence networks eqnets learn syntactic composition recursively composes semvecs like treenn also designed model large changes semantics network progresses syntax tree. equivalence transitive formulate objective function training based equivalence classes rather pairwise decisions. network architecture based composing residual-like multi-layer networks allows ﬂexibility modeling semantic mapping syntax tree. encourage representations within equivalence class tightly clustered also introduce training method call subexpression autoencoding uses autoencoder force representation subexpression predictable reversible syntactic neighbors. experimental evaluation highly diverse class symbolic algebraic boolean expression types shows eqnets dramatically outperform existing architectures like treenns rnns. summarize main contributions work formulate problem learning continuous semantic representations symbolic expressions develop benchmarks task. present neural equivalence networks neural network architecture learns represent expression semantics onto continuous semantic representation space perform symbolic operations space. provide extensive evaluation boolean polynomial expressions showing eqnets perform dramatically better state-of-the-art alternatives. code data available groups.inf.ed.ac.uk/cup/semvec. work interested learning semantic compositional representations mathematical expressions call semvecs learning generate identical representations expressions semantically equivalent i.e. belong equivalence class. equivalence stronger property similarity focus previous work neural network learning since equivalence additionally transitive relationship. problem hardness. finding equivalence arbitrary symbolic expressions np-hard problem worse. example focus boolean expressions reducing expression representation false equivalence class amounts proving non-satisﬁability npcomplete problem. course expect circumvent np-complete problem neural networks. network solving boolean equivalence would require exponential number nodes size expression instead goal develop architectures efﬁciently learn solve equivalence problems expressions similar smaller number expressions given training set. supplementary material shows sample expressions illustrate hardness problem. notation framework. allow representations compositional employ general framework recursive neural networks case operating tree structures syntactic parse formula. given tree treenns learn distributed representations node tree recursively combining representations subtrees using neural network. denote children node ordered tuple nodes. also refer parent node node tree type e.g. terminal node could type referring variable type referring node logical operation. refer type node pseudocode treenns retrieve representation tree rooted node invoking function treenn returns vector representation general framework treenn allows points variation implementation lookupleafembedding combine. traditional treenns deﬁne lookupleafembedding simple lookup operation within matrix embeddings combine single-layer neural network. discussed next prove serious limitations setting. train networks learn semvecs supervised objective based known equivalence relations domain requires network learns abstract away syntax assigning identical representations expressions syntactically different semantically equivalent also assigning different representations expressions syntactically similar nonequivalent. work standard neural architectures handle well challenge. represent semantics syntax need learn recursively compose decompose semantic representations remove syntactic noise. syntactic operation signiﬁcantly change semantics reach semantic state many possible operations. necessitates using high-curvature operations semantic representation space. furthermore operations semantically reversible thus need learn reversible semantic representations based these deﬁne neural equivalence networks learn compose representations equivalence classes equivalence classes network follows treenn architecture i.e. implemented using treenn model compositional nature symbolic expressions adapted based domain requirements. extensions introduce aims ﬁrst improve network training; second interestingly encourage learned representations abstract away surface level information retaining semantic content. ﬁrst extension introduce network structure layer tree. traditional treenns single-layer neural network tree node. preliminary investigations section found single layer networks adequately expressive capture operations transform input semvecs output semvec maintain semantic equivalences requiring high-curvature operations. part problem stems fact within euclidean space semvecs operations need non-linear. example simple boolean operator requires high-curvature operations continuous semantic representation space. instead turn multi-layer neural networks. particular deﬁne network shown function combine figure uses twolayer residual-like connection compute semvec parent node syntax tree given children. node type e.g. logical operator different weights. experimented deeper networks yield improvements. however treenns become deeper suffer optimization issues diminishing exploding gradients. essentially highly compositional nature tree structures network used recursively causing echo errors producing unstable feedback loops. observe problem even two-layer mlps overall network become quite deep using layers node syntax tree. resolve issue training procedure constraining semvec unit norm. lookupleafembedding /cτn normalize output ﬁnal layer combine figure normalization step ¯lout somesimilar weight normalization vaguely resembles layer normalization normalizing semvecs partially resolves issues diminishing exploding gradients removes spurious degree freedom semantic representation. simple modiﬁcation seem found vital obtaining good performance multi-layer treenns converged low-performing settings without although modiﬁcations seem improve representation capacity network ability trained found sufﬁcient good performance. early experiments noticed networks primarily focusing syntax instead semantics i.e. expressions nearby continuous space primarily ones syntactically similar. time observed networks learn unify representations equivalence class observing multiple syntactically distinct semantically equivalent expressions distant semvecs. therefore modify training objective order encourage representations become abstract reducing dependence surface-level syntactic information. regularization term semvecs call subexpression autoencoder design regularization encourage semvecs properties abstraction reversibility. abstraction arguably means removing irrelevant information network bottleneck layer seems natural want training objective encourage bottleneck discard syntactic information rather semantic information. achieve this introduce component aims encourage reversibility explain example. observe given semantic representation three nodes subexpression often possible completely determine least place strong constraints semantics third. example consider boolean formula arbitrary propositional formulae variables clearly know implies true must imply true. generally belongs equivalence class belongs different class want continuous representation reﬂect strong constraints equivalence class subexpression autoencoding encourages abstraction employing autoencoder bottleneck thereby removing irrelevant information representations encourages reversibility autoencoding parent child representations together encourage dependence representations parents children. speciﬁcally given node tree children deﬁne parent-children tuple containing semvecs children parent nodes. subexpae autoencode representation tuple low-dimensional space denoising autoencoder. seek minimize reconstruction error child representations well reconstructed parent representation computed reconstructed children. formally minimize return value subexpae figure binary noise vector percent elements zero. note encoder speciﬁc parent node type although subexpae seem similar recursive autoencoders socher differs major ways. first subexpae autoencodes entire parent-children representation tuple rather child representations alone. second encoding used compute parent representation serves regularizer. subexpression autoencoding several desirable effects. first forces parent-children tuple lowdimensional space requiring network compress information individual subexpressions. second denoising autoencoder reconstructing parent child representations together encourages child representations predictable parents siblings. putting together goal information discarded autoencoder bottleneck syntactic semantic assuming semantics child node predictable parent sibling syntactic realization. goal nudge network learn consistent reversible semantics. additionally subexpression autoencoding potential gradually unify distant representations belong equivalence class. illustrate point imagine semantically equivalent child nodes different expressions combine cases autoencoder noise differences input tuple contain non-existent decoder predict single location then minimizing reconstruction error attracted eventually merge. train eqnets dataset expressions whose semantic equivalence known. given training parse trees expressions assume training partitioned equivalence classes ej}. supervised objective similar classiﬁcation; difference classiﬁcation setting whereas standard classiﬁcation problems consider ﬁxed class labels setting number equivalence classes training vary given expression tree belongs equivalence class compute probability model parameters interpret representations equivalence class appears training class scalar bias terms. note work information equivalence class whole expression ignoring available information subexpressions. without loss generality know equivalence class subexpression simply subexpression training set. train model max-margin equivalence classes split datasets training validation test sets. create test sets measure generalization performance equivalence classes seen training data measure generalization unseen equivalence classes easiest describe unseeneqclass ﬁrst. create unseeneqclass randomly select equivalence classes place expressions test set. select equivalence classes contain least expressions less three times average number expressions equivalence class. thus avoid selecting common equivalence classes testset. then create seeneqclass take remaining equivalence classes randomly split expressions class training validation seeneqclass test proportions %–%–%. provide datasets online groups.inf.ed.ac.uk/cup/semvec. baselines. compare performance model train following baselines. tf-idf learns representation given expression tokens captures topical/declarative knowledge unable capture procedural knowledge. refers token-level gated recurrent unit encoder bahdanau encodes token-sequence expression distributed representation. stack-augmented refers work joulin mikolov used learn algorithmic patterns uses stack memory operates expression tokens. also include recursive neural networks layer treenn original treenn also used zaremba also include -layer treenn combine classic two-layer without residual connections. shows effect semvec normalization subexpression autoencoder. hyperparameters. tune hyperparameters models using bayesian optimization boolean dataset variables maximum tree size using average k-nn statistics selected hyperparameters detailed supplementary material. metrics. evaluate quality learned representations count proportion nearest neighbors expression belong equivalence class. formally given test query expression equivalence class nearest neighbors across expressions deﬁne semantics. epoch instead supervised objective propose alternative option training eqnet would siamese objective learns similarities expressions. practice found optimization unstable yielding suboptimal performance. believe compositional recursive nature task creates unstable dynamics fact equivalence stronger property similarity. evaluation datasets. generate datasets expressions grouped equivalence classes domains. datasets bool domain contain boolean expressions poly datasets contain polynomial expressions. domains expression either variable binary operator combines expressions unary operator applied single expression. deﬁning equivalence interpret distinct variables referring different entities domain that e.g. polynomials equivalent. domain generate simple datasets smaller possible operators standard datasets larger complex operators. generate dataset exhaustively generating parse trees maximum tree size. expressions symbolically simpliﬁed canonical order determine equivalence class grouped accordingly. table shows datasets generated. supplementary material present sample expressions. polynomial domain also generated onev-poly datasets polynomials single variable since similar setting considered zaremba although onev-poly still little general restricted homogeneous polynomials. learning semvecs boolean expressions already hard problem; boolean variables table dataset statistics results. simp datasets contain simple operators rest contain operators operator. number dataset name indicates expressions’ maximum tree size. refers larger number variables. entropy equivalence classes. report results given testset simply average scorek expressions testset. also report precision-recall curves problem clustering semvecs appropriate equivalence classes. evaluation. figure presents average per-model precision-recall curves across datasets. table shows score unseeneqclass. detailed plots found supplementary material. eqnet performs better datasets large margin. exception poly treenn performs better. however small size dataset. reader observe simple datasets easier learn. understandably introducing variables increases size represented space reducing performance. tf-idf method performs better settings variables captures well variables operations used. similar observations made sequence models. layer treenns mixed performance; believe exploding diminishing gradients deep highly compositional nature treenns. although zaremba consider different problem data similar onev-poly datasets traditional treenn architecture. evaluation suggests eqnets perform much better within onev-poly setting. evaluation compositionality. evaluate whether eqnets successfully learn compute compositional representations rather overﬁtting expression trees small size. consider type transfer setting train simpler datasets test complex ones; example training training bool testing testset bool. average different train-test pairs show results figure figure graphs show eqnets better methods indeed performance worse non-transfer setting. impact eqnet components eqnets differ traditional treenns major ways analyze here. first subexpae improves performance. training network without subexpae average area curve scorek decreases seeneqclass unseeneqclass. difference smaller transfer setting decreases average. however even setting observe subexpae helps large diverse datasets. second difference traditional treenns output normalization residual connections. comparing model one-layer two-layer treenns again output normalization results important improvements note combination residual connections output normalization improve performance whereas used separately signiﬁcant improvements two-layer treenns. table shows expressions whose semvec nearest neighbor expression another equivalence class. manually inspecting boolean expressions eqnet confusions happen implication operator table semantically equivalent ﬁrst nearest-neighbors bool poly. checkmark indicates method correctly results nearest neighbor equivalence class. figure visualization score expression nodes three bool four poly test sample expressions using eqnet. darker color lower score i.e. white implies score dark score compose expressions achieve good score even subexpressions achieve worse score. suggests common expressions network tends select unique location without merging equivalence classes affecting upstream performance network. larger scale interactive t-sne visualizations found online. figure presents visualizations semvecs simple expressions negations/negatives. discerned black dots negations discriminated semantic representation space. figure shows property clear manner left-right discriminates polynomials topbottom polynomials diagonal parellelt observe similar behavior figure boolean expressions. involved. fact fail confused expressions eqnet involving operations bool expressions bool. expected tf-idf confuses expressions others contain operators variables ignoring order. contrast treenn tend confuse expressions similar symbolic representations i.e. differ deeply nested variables operators. contrast eqnet tends confuse fewer expressions confused expressions tend syntactically diverse semantically related. mann consider serialized version resulting neural network representation expression. however clear could compare serialized representations corresponding expressions whether mapping preserves semantic distances. recursive neural networks successfully used multiple applications. socher show treenns learn compute values simple propositional statements. eqnet’s subexpae resemble recursive autoencoders differs form function encoding whole parent-children tuple force clustering behavior. addition encoding expression architecture pooling layer directly produces single representation expression. design tree convolutional networks classify code student submission tasks. although learn representations student tasks representations capture task-speciﬁc syntactic features rather code semantics. piech also learn distributed matrix representations student code submissions. however learn representations input output program states test program equivalence. additionally representations necessarily represent program equivalence since learn representations possible input-outputs. allamanis learn variable-sized representations source code snippets summarize short function-like name learn summarization features code rather representations symbolic expression equivalence. closely related work zaremba treenn guide search efﬁcient mathematical identities limited homogeneous singlevariable polynomial expressions. contrast eqnets consider much wider expressions employ subexpression autoencoding guide learned semvecs better represent equivalence search looking equivalent expressions. alemi rnns convolutional neural networks detect features within mathematical expressions speed search premise selection automated theorem proving explicitly account semantic equivalence. future semvecs useful within area. work also related recent work neural network architectures learn controllers/programs contrast work learn evaluate expressions execute programs neural network architectures learn continuous semantic representations expression semantics irrespectively syntactically expressed evaluated. work presented eqnets ﬁrst step learning continuous semantic representations procedural knowledge. semvecs potential bridging continuous representations symbolic representations useful multiple applications artiﬁcial intelligence machine learning programming languages. show eqnets perform signiﬁcantly better state-of-the-art alternatives. improvements needed especially robust training compositional models. addition even relatively small symbolic expressions exponential explosion semantic space represented. fixed-sized semvecs like ones used eqnet eventually limit capacity available represent procedural knowledge. future represent complex procedures variable-sized representations would seem required. work supported microsoft research scholarship programme engineering physical sciences research council thank university edinburgh data science epsrc centre doctoral training providing additional computational resources. references alemi alex chollet francois irving geoffrey szegedy christian urban josef. deepmath deep sequence models premise selection. arxiv preprint arxiv. piech chris huang jonathan nguyen andy phulsuksombati mike sahami mehran guibas leonidas learning program embeddings propagate feedback student code. icml salimans kingma diederik weight normalization simple reparameterization accelerate training deep neural networks. advances neural information processing systems socher richard pennington jeffrey huang eric andrew manning christopher semi-supervised recursive autoencoders predicting sentiment distributions. emnlp socher richard perelygin alex jean chuang jason manning christopher andrew potts christopher. recursive deep models semantic compositionality sentiment treebank. emnlp complex ones essentially evaluating learned compositionality models. figure show performance varies across datasets based characteristics. expected number variables increase performance worsens expressions complex operators tend worse performance results unseeneqclass look similar plotted here. optimized hyperparameters detailed table hyperparameters optimized using spearmint bayesian optimization package. range values used common model hyperparameters. figure evaluation scorex respective seeneqclass unseeneqclass model trained markers shown every ticks x-axis make graph clear. treenn refers model socher hyperparameters learning rate rmsprop momentum minibatch size representation size autoencoder size autoencoder noise gradient clipping initial parameter standard deviation dropout rate hidden layer size curriculum initial tree size curriculum step epoch objective margin -layer-treenn learning rate rmsprop momentum minibatch size representation size gradient clipping initial parameter standard deviation dropout curriculum initial tree size curriculum step epoch objective margin -layer-treenn learning rate rmsprop momentum minibatch size representation size gradient clipping initial parameter standard deviation dropout hidden layer size curriculum initial tree size curriculum step epoch objective margin learning rate rmsprop momentum minibatch size representation size gradient clipping token embedding size initial parameter standard deviation dropout rate learning rate rmsprop momentum minibatch size representation size gradient clipping token embedding size parameter weights initialization standard deviation embedding weight initialization standard deviation dropout stack count", "year": 2016}