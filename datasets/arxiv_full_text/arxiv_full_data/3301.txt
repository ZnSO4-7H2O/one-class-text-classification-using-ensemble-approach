{"title": "Banzhaf Random Forests", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Random forests are a type of ensemble method which makes predictions by combining the results of several independent trees. However, the theory of random forests has long been outpaced by their application. In this paper, we propose a novel random forests algorithm based on cooperative game theory. Banzhaf power index is employed to evaluate the power of each feature by traversing possible feature coalitions. Unlike the previously used information gain rate of information theory, which simply chooses the most informative feature, the Banzhaf power index can be considered as a metric of the importance of each feature on the dependency among a group of features. More importantly, we have proved the consistency of the proposed algorithm, named Banzhaf random forests (BRF). This theoretical analysis takes a step towards narrowing the gap between the theory and practice of random forests for classification problems. Experiments on several UCI benchmark data sets show that BRF is competitive with state-of-the-art classifiers and dramatically outperforms previous consistent random forests. Particularly, it is much more efficient than previous consistent random forests.", "text": "abstract. random forests type ensemble method makes predictions combining results several independent trees. however theory random forests long outpaced application. paper propose novel random forests algorithm based cooperative game theory. banzhaf power index employed evaluate power feature traversing possible feature coalitions. unlike previously used information gain rate information theory simply chooses informative feature banzhaf power index considered metric importance feature dependency among group features. importantly proved consistency proposed algorithm named banzhaf random forests theoretical analysis takes step towards narrowing theory practice random forests classiﬁcation problems. experiments several benchmark data sets show competitive state-of-the-art classiﬁers dramatically outperforms previous consistent random forests. particularly much efﬁcient previous consistent random forests. ensemble methods learning algorithms construct classiﬁers combine classify unseen data random forests type ensemble method based combination several independent decision trees recent years random forests framework variants successfully applied practice general classiﬁcation regression tool. particularly random forests widely used computer vision pattern recognition applications promotes state-of-the-art performance. despite successful applications theoretical analysis random forest models still difﬁcult even basic mathematical properties hard understood. biau colleagues tries narrow theory practice paper introduce novel random forests algorithm based cooperative game theory. adopt banzhaf power index evaluate power feature traversing possible coalitions. this call proposed algorithm banzhaf random forests different previously used information gain rate information theory simply chooses informative feature banzhaf power index measures importance feature dependency among group features importantly reasonable proved consistency forest made contribution narrow theory practice random classiﬁcation forests problems. rest paper organized follows. section provide brief overview existing random forests models analyze advantage disadvantage. section introduce general random forests framework including construction trees randomness injection. section describes proposed algorithm banzhaf random forests detail section devoted justiﬁcation consistency brf. section shows experimental results benchmark data sets section concludes paper. classic random forests introduced breiman combine several decision trees bagging main idea random forests based early work random subspace method feature selection work random split selection based seminal work breiman suggests best average across sets trees different structures constituent trees. criminisi present uniﬁed efﬁcient model random decision forests applied number machine learning computer vision medical image analysis tasks. development random forests recent years applied wide variety real world problems despite successful applications random forests practice mathematical properties behind well understood. example early theoretical work essentially based mathematical heuristics formalized rigorous theory. theory main properties theoretical interests related random forests. consistency models whether converge optimal solution data grows inﬁnitely large. rate convergence. paper mainly focuses consistency proved breiman’s random forests cannot guarantee. design consistent random forests many researchers struggled trend. meinshausen shown algorithm random forests quantile regression consistent; ishwaran kogalur shown consistency survival forests model; denil show consistency online version random forests presents random regression forests. consistent models applied either regression survival online settings batch classiﬁcation settings training data used together learning. paper propose novel random forests model based cooperative game theory multi-class classiﬁcation problems. consistency proposed algorithm also proved. closely related papers work proves consistency popular averaging classiﬁers including random forests. speciﬁcally authors take weighted layered nearest neighbor classiﬁer perspective taxonomy proposed unfortunately property prevents consistency random tree classiﬁers. remedy inconsistency tree classiﬁers authors suggest technique introduced moreover also proposed scale-invariant version random forests consistency. recently presents model random forests similar original algorithm main difference models random features selected. requires second independent data evaluate importance index feature uses property prove consistency algorithm model doesn’t need second data set. paper banzhaf power index evaluate power feature traversing possible feature coalitions employing second data set. consistency proposed algorithm theoretically guaranteed. section brieﬂy review random forests framework. typically random forests built combining predictions several trees trained isolation. unlike boosting base models trained combined using dynamic weighting scheme trees trained independently predictions trees combined averaging majority voting. comprehensive review please refer construct random tree three core steps required ﬁrst method splitting tree nodes; second type predictor leaf third method injecting randomness trees. typical method splitting nodes splitting depends whether exceed threshold value chosen feature. alternatively linear splits linear combination features compared threshold make decision. threshold value either case chosen randomly optimizing function data. example gini index information gain rate commonly used. paper choose midpoint feature splitting threshold leads proposed algorithm efﬁcient especially case large scale applications. order split node tree candidate features data generated criterion evaluated choose them. simple strategy models analyzed choose among features uniformly random. common approach choose candidate split optimizes purity function nodes would created. particularly typical choices maximize information gain minimize gini index. banzhaf random forests choice predictors propose several different leaf predictors regression tasks. common consideration average predictors training points fall leaf. consideration based majority voting points leaf. work take last strategy. important inject randomness trees random forests. achieved several ways. choice features split node; coefﬁcients random combinations features. common method build tree using bootstrapped sub-sampled data set. tree forest trained slightly different data introduces differences trees. similar work uses bootstrapped method inject randomness tree. section describe proposed algorithm banzhaf random forest detail. firstly introduce basic concepts cooperative game theory. secondly based banzhaf power index introduce construct randomized trees. thirdly combine banzhaf trees formulate banzhaf random forests. finally present prediction method banzhaf random forests. cooperative game theory mainly studies ‘acceptable’ distributing gains collectively achieved group cooperating agents cooperative proﬁt game consists player characteristic function subset interpreted proﬁt achieved players usual goal cooperative game distribute total gain global coalition among player fair reasonable ways. different requirements fairness rationality derive different solution concepts cooperative game. core banzhaf power index related concepts approximate core. among various solution concepts concept banzhaf power index motivated fairness. game monotone i.e. satisﬁed every pair coalitions characteristic function takes value i.e. game called simple game. simple game coalitions value called ‘winning’ value called ‘losing’ i.e. respectively. coalition wins loses called swing player membership player coalition crucial ’winning’. fact banzhaf power index count number winning coalitions player joining losing coalitions crucial player majority coalitions winning. banzhaf power index yields unique outcome coalitional games proposed measure marginal contribution players game simple games banzhaf power index particular attractive interpretation measures power player i.e. probability inﬂuence outcome game. paper banzhaf power index measure power feature. figure shows structure banzhaf decision tree. root node feature selected information gain rate. nodes features selected banzhaf power index. idea banzhaf decision tree mainly motivated game theory especially cooperative game theory. take features data players game original tree construction problem transformed cooperative ‘feature’ game. node features form coalition selected best split. next ﬁrst present compute banzhaf power index work. original deﬁnition banzhaf power index described given cooperative game banzhaf power index player probability swings play denote banzhaf power index given banzhaf power index measures distribution power among players cooperative games. here apply decision tree construction attempting estimate power feature tree node. power feature measured averaging contributions makes subset belongs coalition candidate feature subset feature estimated. deﬁne ratio µi/ρi represent impact feature coalition interpreted number features fall interdependence relationship feature number features coalition therefore deﬁne threshold value call coalition ‘losing’ otherwise ‘winning’ i.e. here means feature make coalition exhibit better performance. threshold value means half features interdependent join coalition make ‘winning’. hence simplicity computation deﬁne clarity here give example show compute banzhaf power index. given cooperative ‘feature’ game feature player suppose currently goal calculate banzhaf power index total number possible coalitions feature subsets assume winning coalitions respect i.e. half coalitions interdependent featuref. banzhaf power index computed similarly value banzhaf power index features computed way. generally banzhaf power index hardly zero large scale high dimensional applications. order evaluate impact feature needs calculate proportion ‘winning’ coalitions. lead high computational complexity model randomly selected small group features compute banzhaf power index node. hence computational complexity fairly low. calculate proportion ‘winning’ coalitions conditional mutual information information theory evaluate interdependent single feature player half feature players interdependent paper condition mutual information deﬁned amount interdependent feature player feature player given feature player colation formally deﬁned banzhaf random forests algorithm given training data includes samples dimensionality data procedures banzhaf random forests algorithm described follows. construction banzhaf decision tree randomly draw samples replacement using bootstrap randomly select features withreplacement training data. base data grow recursive banzhaf tree. root node feature selected information gain rate. nodes features selected banzhaf power index. feature associated corresponding node split midpoint feature values generate left right branches. algorithm similar original algorithms used bootstrap aggregating i.e. bagging ensemble algorithm. main difference algorithm feature associated node selected. uses banzhaf power index breiman’s method gini index. another difference splits node midpoint feature values breiman’s algorithm not. importantly shown next section consistency theoretically guaranteed breiman’s algorithm not. also tested model pure banzhaf random forests i.e. feature root node also selected banzhaf power index. performance generally worse algorithm described above. reason result feature selected information gain rate root node present important invariant information data. prediction denote recursive tree created algorithm based data i.i.d. pairs random variables takes value multiclass random variable. make prediction query point banzhaf decision tree computes section prove consistency banzhaf random forests. denote banzhaf tree created banzhaf random forests trained data consistency sequence {gn} deﬁned follows. deﬁnition sequence classiﬁer {gn} consistent given distribution probability prediction error converges probability bayesian risk here denotes randomness tree-building algorithm training data probability convergence random selection bayesian risk probability prediction error bayesian classiﬁer makes predictions choosing class highest posterior probability order reduce complexity issue consider multi-class classiﬁer transformed combination several binary-class classiﬁer. need prove consistency estimators posterior distribution class. similar result shown denil estimates consistent. classiﬁer function margin function measures much better best choice second best choice. function measures margin probability error bayes classiﬁer. assumption guarantees using lemma allows prove consistency multiclass classiﬁer transformed prove consistency several class posterior estimates. i.e. given classes re-assign labels using order class problem problem equal original multiclass problem. then inspired following lemma allows focus atlemma assume sequence {gn} randomized classiﬁers consistent certain distribution voting classiﬁer obtained taking majority vote copies {gn} also consistent. proof. denote bayes classiﬁer. consistency {gn} equivalent saying fact since consistency {gn} means µ-almost according lemma conclude consistency banzhaf random forests implied consistency trees composed addition bagging ensemble method construct brf. theorem know consistency voting banzhaf random forests follows consistency base classiﬁer. here biau introduce parameter bootstrap sample data pair present probability independent other. theorem {gn} sequence classiﬁer consistency distribution consider banzhaf random forests using parameter classiﬁers consistent. proof. theorem lemma theorem established remainder effort goes proving consistency banzhaf tree construction. tree banzhaf forests established based banzhaf index. show classiﬁer condition consistency consists small group random variable uses banzhaf power index sampling sample process random variable generates acceptable sequences probability resulting classiﬁer unconditionally consistent. theorem suppose {gn} sequence classiﬁers whose probability error converges conditionally probability bayes risk speciﬁed distribution i.e. random sequence produced banzhaf power index distribution means produce acceptable sequence probability value probability error converges unconditionally probability i.e. {gn} consistent speciﬁed distribution. proof. sequence question uniformly integrable sufﬁcient show |i)] implies result expectation taken random selection training speciﬁc structure tree {gn}. write |i)] desired result. fact banzhaf power index equal income distribution function tree construction game i.e. chose maximize banzhaf power index node tree. obtain acceptable random variable sequence maximize banzhaf power index. random variable sequence cooperative obtain best result. sufﬁcient show banzhaf tree consistent conditioned sequence. evaluate proposed algorithm tested several data sets machine learning repository including iris wine ecoli thyroid soybean shuttle dermatology sonar musk. compare breiman’s random forests model proposed implemented breiman’s random forest generally performs well classiﬁcation problems. mentioned above model proposed consistent. comparison also listed classiﬁcation results yielded k-nearest neighbor classiﬁer support vector machines evaluate effect number trees conducted experiments three data sets iris ecoli shuttle. fig. shows obtained classiﬁcation accuracy number trees brf. that basically robust number trees. particularly number trees equals performs slightly better values. test running speed performed experiments seven data sets iris wine ecoli soybean thyroid dermatology shuttle. compared model table that running slower model mainly calculation banzhaf power index needs time constructing trees. however efﬁcient model state-of-the-art consistent random forests model. evaluate multi-class classiﬁcation problems compared knns svms model model nine data sets used. iris wine ecoli thyroid soybean shuttle dermatology sonar musk. data sets used -fold cross validation test models. average classiﬁcation accuracies reported. model used number trees random features. following breiman’s suggestion classiﬁcation problems number trees round dimensionality features. fair termination conditions random forests models i.e. percentage incorrectly assigned samples termination node greater number classes data set. knns svms selected parameter -fold cross validation. table shows results obtained compared models brf. performs slightly better knns svms model consistently better model demonstrates using interdependent features construct randomized trees lead better results using independent features random forests. paper propose novel random forests model called banzhaf random forests based concepts cooperative game theory. it’s consistency proved takes step towards narrowing theory practice random forest. work probably ﬁrst apply cooperative game theory random forests tested veriﬁed feasibility idea. experiments data sets show slightly outperforms state-of-the-art classiﬁers including knns svms random forests model breiman much efﬁcient existing consistent random forests.", "year": 2015}