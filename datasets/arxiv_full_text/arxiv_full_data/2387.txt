{"title": "Learning Bayesian Network Parameters with Prior Knowledge about  Context-Specific Qualitative Influences", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We present a method for learning the parameters of a Bayesian network with prior knowledge about the signs of influences between variables. Our method accommodates not just the standard signs, but provides for context-specific signs as well. We show how the various signs translate into order constraints on the network parameters and how isotonic regression can be used to compute order-constrained estimates from the available data. Our experimental results show that taking prior knowledge about the signs of influences into account leads to an improved fit of the true distribution, especially when only a small sample of data is available. Moreover, the computed estimates are guaranteed to be consistent with the specified signs, thereby resulting in a network that is more likely to be accepted by experts in its domain of application.", "text": "present method learning parameters bayesian network prior knowledge signs inﬂuences variables. method accommodates standard signs provides context-speciﬁc signs well. show various signs translate order constraints network parameters isotonic regression used compute order-constrained estimates available data. experimental results show taking prior knowledge signs inﬂuences account leads improved true distribution especially small sample data available. moreover computed estimates guaranteed consistent speciﬁed signs thereby resulting network likely accepted experts domain application. constructing bayesian network often knowledge acquired experts domain application. experience shows domain experts quite easily reliably specify graphical structure network tend problems coming probabilities numerical part data every-day problem solving domain available therefore would like data estimating probabilities required graphical structure arrive fully speciﬁed network. many applications unfortunately available data sample quite small giving rise inaccurate estimates. inaccuracies involved lead reasoning behaviour resulting network violates common domain knowledge network easily accepted experts domain. building feel comfortable providing qualitative knowledge probabilistic inﬂuences variables concerned qualitative knowledge provided domain experts moreover tends robust numerical assessments. demonstrated expert knowledge signs inﬂuences variables bayesian network used improve probability estimates obtained small data samples extend previous work accommodate wider range contextspeciﬁc signs context-speciﬁc independences speciﬁcally. argue signs impose order constraints probabilities required network. show problem estimating probabilities under order constraints special case isotonic regression. building upon property present estimator guaranteed produce probability estimates reﬂect qualitative knowledge speciﬁed experts. resulting network consequence less likely exhibit counterintuitive reasoning behaviour likely accepted network unconstrained estimates. paper organised follows. next section brieﬂy review qualitative inﬂuences. section discuss isotonic regression provide algorithm computation. show section problem learning constrained network parameters special case isotonic regression; also discuss different constraints result qualitative inﬂuences handled order constraints used bayesian context. section report experiments performed small artiﬁcial bayesian network real-life network medical domain. finally draw number conclusions work indicate interesting directions research. approach obtaining parameter estimates bayesian network satisfy signs inﬂuences speciﬁed experts special case isotonic regression section review isotonic regression general; next section discuss application parameter estimation bayesian networks. nonempty ﬁnite constants quasi-order real-valued function isotonic respect implies assume element associated real number real numbers typically estimates function values unknown isotonic function furthermore element associated positive weight typically indicates precision estimate. isotonic function isotonic regression respect weight function order minimizes isotonic regression provides solution many statistical estimation problems prior knowledge order parameters estimated. example suppose want estimate binomial parameters denotes probability success population denote number observations sampled population number successes sample binomially distributed isotonic regression estimates yi/ni weights provides maximum-likelihood estimate given isotonic note example suggests order-constrained estimates obtained ﬁrst computing unconstrained estimates performing isotonic regression basic estimates appropriate weights. isotonic regression problems solved quadratic programming methods. various dedicated algorithms often restricted particular type order proposed well. linearly ordered example pool assume variables network binary. parents variable consist vectors values variables parent conﬁgurations slightly abusing terminology sometimes occurs present value one. write sub-vector containing variables subset write xk\\a. qualitative inﬂuence variables expresses observing value variable affects probability distribution variable. positive inﬂuence along means occurrence increases probability occurs regardless direct inﬂuences similarly negative inﬂuence along occurrence decreases probability occurs posx−i itive inﬂuence denoted inﬂuence either negative inﬂuence positive negative sign called signed inﬂuence. sign speciﬁed call inﬂuence unsigned. idea signs inﬂuences readily extended include context-speciﬁc signs positive inﬂuence within context means whenever occurrence increases probability occurs x−c∪{i} −c∪{i} contextx speciﬁc negative inﬂuence deﬁned analogously. zero inﬂuence within context models local context-speciﬁc independence −c∪{i} x−c∪{i} signed context-speciﬁc inﬂuence along denoted note ordinary signed inﬂuences special cases context-speciﬁc inﬂuences note signed inﬂuence essence speciﬁes constraint parameters associated variable. assume throughout paper domain expert provides signs qualitative inﬂuences variables network. would like mention real-life applications signs quite readily obtained experts using special-purpose elicitation technique tailored acquisition signs qualitative inﬂuences adjacent violators algorithm well-known. application however require algorithm applicable sets constants arbitrary quasi-orders. purpose minimum lower sets algorithm proposed brunk algorithm builds upon concept lower set. subset lower imply weighted average function nonempty subset deﬁned algorithm takes input constants quasi-order associated weight real number algorithm returns isotonic regression respect algorithm resolves violations order constraints averaging suitably chosen subsets ﬁnal solution partitions number subsets isotonic regression constant. ﬁrst subset ﬁnal solution lower second subset lower obtained removing order relations involving elements process continued exhausted. iteration lower minimum weighted average selected; case multiple lower sets attain minimum union taken. section address maximum-likelihood estimation parameters bayesian network subject constraints imposed signs inﬂuences show viewed special case isotonic regression. note presence signs parameters associated different parent conﬁgurations variable longer unrelated. combinations parameter values isotonic respect quasi-order imposed speciﬁed signs feasible. parameters associated different variables still unrelated however sign imposes constraints parameters single variable only. restrict attention therefore parameters associated single variable. cast problem constrained parameter estimation isotonic regression framework proceed follows. parents construct order corresponds order parameters implied speciﬁed signs. speciﬁcally qualitative impose inﬂuence following order −c∪{i} since positive sign implies ordering statements follow transitivity reﬂexivity properties quasi-orders. speciﬁed inﬂuences constrain parameters nondecreasing suppose available data would like estimate parameters unconstrained maximum-likelihood estimate given denotes number observations following observation links isotonic regression problem currently considered isotonic regression weights provides maximum-likelihood estimate subject constraint estimates must non-decreasing also page illustrate observation consider fragment bayesian network. parents variable qualitative inﬂuences shown ﬁgure fragment expresses prior knowledge positive inﬂuence that absent negative inﬂuence models present absent thermore estimates second equal context-speciﬁc independence speciﬁed. clearly constraint violated well. finally estimates non-increasing ﬁrst contextspeciﬁc negative inﬂuence constraint satisﬁed. algorithm resolves identiﬁed constraint violations averaging unconstrained estimates conﬂicting cells table. starts computation weighted average lower sets table shows resulting averages. minimum average achieved algorithm sets element removed lower sets weighted averages recomputed. minimum weighted average element removed lower sets remaining lower set. weighted average algorithm sets component order solved. note constraint violations resolved simultaneously averaging pair violators next consider parent conﬁgurations table shows parent conﬁguration counts obtained available sample associated maximum-likelihood estimates note observations sample parent conﬁguration cases give cell arbitrary small weight. consequence estimate dominated parameter estimates soon pooled resolve conﬂicts. speciﬁed signs estimates non-decreasing within column non-increasing ﬁrst row. constraint satisﬁed column constraints not. table gives lower sets weighted averages. achieves minimum algorithm sets note constrained estimate empty cell simply equal estimate conﬂicting cell elements removed lower sets weighted averages recomputed. minimum weighted average achieved step violation order constraint ﬁrst column resolved averaging parameter estimates conﬂicting cells. note constrained joint estimate closer unconstrained estimate cell unconstrained estimate cell observations former latter former thus larger associated weight. figure shows quasi-order parent conﬁgurations imposed speciﬁed inﬂuences arrow indicates immediately precedes order. inﬂuence speciﬁed parent conﬁgurations different value incomparable. consequence order consists components estimates computed components separately order constraints parent conﬁgurations contained different components. also note component depicted left part ﬁgure contains cycle result context-speciﬁc independence speciﬁed independence constraint must satisﬁed. modelled considering parent conﬁgurations single element ordering shown ﬁgure table shows parent conﬁguration counts obtained given sample well associated maximum-likelihood estimates note estimates non-decreasing columns table positive inﬂuence constraint violated ﬁrst column. furwould also like note that although algorithm computes empty cell value interval would actually satisﬁed constraints. alternative proposed procedure would remove empty cells application algorithm estimates cells computed determine feasible estimates empty cells. since exhausted step algorithm halts. observe resulting parameter estimates indeed satisfy constraints imposed qualitative inﬂuences. also note parameters involved constraint violations retained original estimates. argued section number lower sets dominant factor runtime complexity minimum lower sets algorithm. determine number start simple case parents variable context-independent signed inﬂuence. without loss generality assume signs positive. since parents binary parent conﬁguration uniquely determined parents value alternatively subset partial order therefore isomorphic partial order generated inclusion every lower corresponds uniquely antichain partial order. hence number distinct lower sets equals number distinct nonempty antichains subsets k-set adheres well-known sloane sequence example network fragment noted unsigned inﬂuences serve partition parent conﬁgurations disjoint subsets element subset order related element subsets. argued constrained estimates computed subsets separately thereby effectively decomposing parameter learning problem number independent smaller problems. decomposition yields considerable improvement efﬁciency computations involved. general denote number parents signed inﬂuence denote number parents unsigned inﬂuence. number conﬁgurations parents unsigned inﬂuence equals order graph thus consists components. number lower sets entire order given number grows rapidly. example algorithm would need compute weighted average lower sets. treating component order separate problem algorithm initially compute weighted average lower sets. amounts lower sets. presence context-speciﬁc signs analysis algorithm’s runtime complexity becomes complicated. restrict following observations. first absence signs particular contexts also lead decomposition order hence similar reduction computations involved case unsigned inﬂuences. hand absence signs particular contexts also lead increase number lower sets. secondly context-speciﬁc independences lead reduction number elements ordering number parameters estimated hence reduction number lower parameter learning method described previous sections require expert speciﬁes numerical values parameters concerned. expert provide signs various inﬂuences. uncertain prior knowledge numeric values parameters available addition knowledge signs inﬂuences accommodate information. suppose expert willing specify beta prior parameters assume chooses hyperparameters experience equivalent seen value total times trials; called prior precision. denote modal value priori considered likely value assume expert’s values modes isotonic respect order imposed signs speciﬁed. forming joint prior assume local parameter independence except parameter values must isotonic. means prior density non-isotonic value combinations parameters proportional product beta distribution isotonic value combinations. isotonic estimates given isotonic regression order-constrained estimation amounts performing isotonic regression basic estimates appropriately chosen weights. basic estimates unconstrained estimates parameters. weight number actual observations parent conﬁguration prior precision speciﬁed expert. note case prior orderconstrained maximum likelihood estimates returned. study behaviour isotonic estimator slightly involved setting compare standard maximum-likelihood estimator well-known brain tumour network network signs inﬂuences depicted ﬁgure network speciﬁed probabilities consistent constraints generate data samples experiments. note that even though true distribution satisﬁes constraints need implementation ﬁrst generates variable quasi-order parent conﬁgurations corresponds speciﬁed signs. order ﬁnds separate components; parent conﬁgurations contained cycle mapped single element order adjusted accordingly. component order parameters parent conﬁgurations contained component estimated using algorithm. drew samples various sizes network using logic sampling; sample size data sets drawn. data standard maximumlikelihood estimates constrained estimates network parameters calculated. given parameter estimates joint distribution deﬁned resulting network computed. distribution compared true joint distribution deﬁned original network. comparing distributions used well-known kullback-leibler divergence. kullbackleibler divergence deﬁned denote joint distribution obtained unconstrained maximum-likelihood estimation. illustrate beneﬁts modeling context-speciﬁc independences ﬁrst estimated various parameters without taking embedded zeroes account used resulting distribution denoted table. finally pr∗∗ denotes distribution obtained isotone estimator using embedded zeroes. averages reported table computed data sets divergence smaller inﬁnity maximum-likelihood estimates isotone estimates always divergence smaller inﬁnity cases well. number data sets averages computed sample sizes respectively. sample sizes used bayesian estimation beta prior parameters prior mode parameters prior precision equal note setting parameters value never violate order constraints. unconstrained isotonic estimates used point estimates parameters. used bayesian estimation smallest sample sizes otherwise divergence would almost always equal inﬁnity. results reveal isotonic estimator consistently yields better true distribution compared unconstrained maximum-likelihood estimator although differences small. note smaller data sets differences marked larger data sets. conforms expectations since smaller data sets standard maximum-likelihood estimation higher probability yielding estimates violate speciﬁed constraints. larger data sets standard estimator isotonic estimator expected often result estimates. note using contextspeciﬁc independences gives additional improvement expected. conclude applied isotonic estimator real-life bayesian network ﬁeld oncology. oesoca network provides establishing stage patient’s oesophageal cancer based upon results number diagnostic tests. network constructed help gastroenterologists netherlands cancer institute antoni leeuwenhoekhuis; experts provided knowledge conﬁguration network’s structure also provided probability assessments network’s parameters. original oesoca network constructed binary network experiment carefully building upon knowledge domain. resulting network includes variables total parameters. values various parameters established signs qualitative inﬂuences variables. network includes inﬂuences; ﬁgure depicts binary oesoca network. signs qualitative inﬂuences shown corresponding arcs; readibility context-independent signs shown question mark used denote ambiguous inﬂuence. denotes joint distribution resulting unconstrained estimates joint distribution resulting isotonic estimates. results line results obtained brain tumour network isotonic estimator consistently better difference becomes smaller sample size increases. taking prior knowledge signs inﬂuences between variables account upon estimating parameters bayesian network results improved true distribution. improvement relatively large small samples since likely give rise maximum-likelihood estimates violate constraints. since constrained parameter estimates accordance prior knowledge speciﬁed experts resulting network likely accepted domain application. interesting extension method would allow non-binary variables linearly-ordered discrete values. signed inﬂuence variable deﬁned terms stochastic order distributions given different parent conﬁgurations. learning parameters network subject resulting constraints opinion merits research.", "year": 2012}