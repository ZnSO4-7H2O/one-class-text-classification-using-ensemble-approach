{"title": "Neural Tree Indexers for Text Understanding", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Recurrent neural networks (RNNs) process input text sequentially and model the conditional transition between word tokens. In contrast, the advantages of recursive networks include that they explicitly model the compositionality and the recursive structure of natural language. However, the current recursive architecture is limited by its dependence on syntactic tree. In this paper, we introduce a robust syntactic parsing-independent tree structured model, Neural Tree Indexers (NTI) that provides a middle ground between the sequential RNNs and the syntactic treebased recursive models. NTI constructs a full n-ary tree by processing the input text with its node function in a bottom-up fashion. Attention mechanism can then be applied to both structure and node function. We implemented and evaluated a binarytree model of NTI, showing the model achieved the state-of-the-art performance on three different NLP tasks: natural language inference, answer sentence selection, and sentence classification, outperforming state-of-the-art recurrent and recursive neural networks.", "text": "recurrent neural networks process input text sequentially model conditional transition word tokens. contrast advantages recursive networks include explicitly model compositionality recursive structure natural language. however current recursive architecture limited dependence syntactic tree. paper introduce robust syntactic parsing-independent tree structured model neural tree indexers provides middle ground sequential rnns syntactic treebased recursive models. constructs full n-ary tree processing input text node function bottom-up fashion. attention mechanism applied structure node function. implemented evaluated binarytree model showing model achieved state-of-the-art performance three different tasks natural language inference answer sentence selection sentence classiﬁcation outperforming state-of-the-art recurrent recursive neural networks recurrent neural networks successful modeling sequence data rnns equipped gated hidden units internal short-term memories long shortterm memories achieved notable success several tasks including named entity recognition constituency parsing textual entailment recognition question answering machine translation however lstm models explored sequential. encodes text sequentially left right vice versa naturally support compositionality language. sequential lstm models seem learn syntactic structure natural language however generalization unseen text relatively poor comparing models exploit syntactic tree structure unlike sequential models recursive neural networks compose word phrases syntactic tree structure shown improved performance sentiment analysis however dependence syntactic tree architecture limits practical applications. study introduce neural tree indexers class tree structured models tasks. takes sequence tokens produces representation constructing full n-ary tree bottom-up fashion. node associated node transformation functions leaf node mapping non-leaf node composition functions. unlike previous recursive models tree structure relaxed i.e. require input sequences parsed syntactically; therefore ﬂexible directly applied wide range tasks beyond sentence modeling. furthermore propose different variants node composition function attention tree models. sequential leaf node transformer lstm chosen network forms sequence-tree hybrid model taking advantage conditional compositional powers sequential recursive models. figure figure binary tree form neural tree indexers context question answering natural language inference. insert empty tokens input text form full binary tree. produces answer representation root node. representation along question used answer. learns representations premise hypothesis sentences attentively combines classiﬁcation. dotted lines indicate attention premise-indexed tree. shows binary-tree model nti. although model follow syntactic tree structure empirically show achieved state-ofthe-art performance three different applications natural language inference answer sentence selection sentence classiﬁcation. rnns model input text sequentially taking single token time step producing corresponding hidden state. hidden state passed along next time step provide historical sequence information. although great success variety tasks rnns limitations among them efﬁcient memorizing long distant sequence frequently called information bottleneck. approaches therefore developed overcome limitations. example mitigate information bottleneck bahdanau extended rnns soft attention mechanism context neural machine translation leading improved results translating longer sentences. rnns linear chain-structured; limits potential natural language represented complex structures including syntactic structure. study propose models mitigate limitation. tree structure predeﬁned syntactic parser non-leaf tree node associated node composition function combines children nodes produces representation. model trained back-propagating error structures node composition function varied. single layer network tanh non-linearity adopted recursive auto-associate memories recursive autoencoders socher extended network additional matrix representation node augment expressive power model. tensor networks also used composition function sentencelevel sentiment analysis task recently introduced s-lstm extends lstm units compose tree nodes recursive fashion. paper introduce novel attentive node composition function based slstm. model rely either parser output ﬁne-grained supervision nonleaf nodes required previous work. supervision target labels provided root node. such model robust applicable wide range tasks. introduce attention tree overcome vanishing/explode gradients challenges shown rnns. full n-ary tree non-leaf node functransformation function tion node leaf node function leaf leaf computes transformation input word embedding node function child nodes representation total number child nodes non-leaf node. implemented different tree structures. study implemented evaluated binary tree form non-leaf node take direct child nodes therefore function node composes left child node right child node figure illustrates model applied question answering natural language inference tasks note node leaf node functions neural networks training parameters nti. lstm-based non-leaf node function initiate node lstm. non-leaf node adopt s-lstm extension lstm tree structures learn node representation children nodes. vector representations cell states left right children. s-lstm computes parent node representation node cell state rk×k biases training parameters. denote elementwise sigmoid function element-wise vector multiplication. extension s-lstm nonleaf node function compose children straightforward. however number parameters increases quadratically s-lstm child nodes. attentive non-leaf node function applications would beneﬁt dynamic query dependent composition function. introduce non-leaf node function. unlike slstm composes child nodes attentively respect another relevant input vector input vector learnable representation sequence representation. given matrix resulted concatenating child node representations third input vector deﬁned relu rk×k learnable matrix attention score attention weight vector child. score attention scoring function implemented multi-layer perceptron attention tree comparing sequential lstm models less recurrence deﬁned tree depth binary tree length input sequence. however still needs compress input information single representation vector root. imposes practical difﬁculties processing long sequences. address issue attention classiﬁer handcrafted features lstms encoders dependency tree encoders nti-slstm spinn-pi encoders nti-slstm-lstm lstms attention lstms word-by-word attention nti-slstm node-by-node global attention nti-slstm node-by-node tree attention nti-slstm-lstm node-by-node tree attention nti-slstm-lstm node-by-node global attention mlstm word-by-word attention lstmn deep attention fusion tree matching nti-slstm-lstm tree attention decomposable attention model tree matching nti-slstm-lstm global attention full tree matching nti-slstm-lstm global attention mechanism tree. addition attention mechanism used matching trees carry different sequence information. ﬁrst deﬁne global attention introduce tree attention considers parent-child dependency calculation attention weights. global attention attention neural network global attention takes node representations input produces attentively blended vector whole tree. neural similar anf. particularly given matrix rk×n− resulted concatenating node representations relevant input representation global attention deﬁned rk×k training rameters attention weight vector node. attention mechanism robust globally normalizes attention score tmax obtain weights however consider tree structure producing ﬁnal representation htree. compares parent children nodes produce representation assuming node representations constructed. given matrix resulted concatenating parent node representation relevant input representation right child every non-leaf node simply updates representation using following equation bottom-up manner. equation similarity global attention. however non-leaf node attentively collects children representations passes towards root ﬁnally constructs attentively blended tree representation. note unlike global attention tree attention locally normalizes attention scores tmax. describe section experiments three different tasks natural language inference question answering sentence classiﬁcation demonstrate ﬂexibility effectiveness different settings. trained using adam hyperparameters selected development set. pre-trained glove vectors obtained word embeddings. word embeddings ﬁxed training. embeddings out-ofvocabulary words zero vector. input sequence form full binary tree. padding vector inserted padding. analyzed effects padding size found inﬂuence performance size hidden units modules models regularized using dropouts weight decay. conducted experiments stanford natural language dataset consists premise-hypothesis pairs train/dev/test sets target label indicating relation. unless otherwise noted follow setting previous work classiﬁcation takes outputs computes concatenation absolute difn− ference elementwise product sentence representations. also input layer units relu activation tmax output layer. explored nine different task-oriented models varying complexity described below. model batch size initial learning regularization strength number epoch trained varied model. nti-slstm model rely leaf transformer uses s-lstm units non-leaf node function. initial learning rate regularizer strength train model epochs. neural regularized input dropouts output dropouts. nti-slstm-lstm lstm leaf node function leaf concretely lstm output vectors given nti-slstm memory cells lowest level s-lstm initialized lstm memory states. hyper-parameters previous nti-slstm node-by-node global attention model learns inter-sentence relation global attention premise-indexed tree similar word-by-word attention model rockt¨aschel attends premise tree nodes every time step hypothesis encoding. weight parameters nti-slstms premise hypothesis leaf transformer used. initial learning rate regularizer strength train model epochs. neural regularized input dropouts output dropouts. nti-slstm-lstm node-by-node global attention model include lstm leaf node function leaf initialize memory cell s-lstm lstm memory hidden/memory state hypothesis lstm premise lstm initial learning rate regularizer strength train model epochs. neural regularized input dropouts output dropouts. tree matching nti-slstm-lstm global attention model ﬁrst constructs premise hypothesis trees simultaneously ntislstm-lstm model computes matching vector using global attention additional lstm. attention vectors produced hypothesis tree node given lstm model sequentially. lstm model compress attention vectors outputs single matching vector passed classiﬁcation. tree matching setting input layer units relu activation tmax output layer. tree matching nti-slstm-lstm global attention model produces sets attention vectors attending premise tree regarding hypothesis tree node another attending hypothesis tree regarding premise tree node. attention vectors given lstm model achieve full tree matching. last hidden states lstm models concatenated classiﬁcation. training weights shared among lstm models hyper-parameters previous model. table shows results models. comparison include results published state-of-the-art systems. sentence encoder models rely solely word embeddings dependency tree spinn-pi models make sentence parser output; present strong baseline systems. last methods designs inter-sentence relation soft attention best score task accuracy obtained full tree matching model. previous best performing model task performs phrase matching using attention mechanism. results show nti-slstm improved performance sequential lstm encoder approximately surprisingly using lstm leaf node function helps learning better representations. nti-slstm-lstm hybrid model encodes sequence sequentially leaf node function hierarchically composes output representations. node-by-node attention models improve performance indicating modeling inter-sentence interaction important element nli. aggregating matching vector between trees sequences separate lstm model effective. global attention seems answer sentence selection task model trained identify correct sentences answer factual question candidate sentences. experiment wikiqa dataset constructed wikipedia dataset contains pairs train/dev/test sets. question answer encoded vectors denotes output hidden layer mlp. task nti-slstm-lstm encode answer candidate sentences nti-anf-lstm encode question sentences. note nti-anf-lstm relied non-leaf node function. vector nti-anf-lstm answer representation produced answer encoding ntislstm-lstm model. batch size initial learning rate train model epochs. used input dropouts figure node-by-node attention visualizations. phrases shown nodes hypothesis-indexed tree premise tokens listed along x-axis. adjacent cells composed cell representing binary tree resulting longer attention span. table presents results model previous models task. classiﬁer handcrafted features model trained features. bigram-cnn model simple convolutional neural net. deep lstm lstm attention models outperform previous best result large margin nearly nasm improves result sets strong baseline combining variational autoencoder soft attention. nasm adopt deep three-layer lstm introduced latent stochastic attention mechanism answer sentence. model exceeds nasm approximately task. lastly evaluated stanford sentiment treebank dataset comes standard train/dev/test sets subtasks binary sentence classiﬁcation ﬁne-grained classiﬁcation classes. trained model text spans corresponding labeled phrases training evaluated model full sentences. nti-slstm nti-slstm-lstm models learn sentence representations task. sentence representations passed two-layer classiﬁcation. batch size initial learning rate regularizer strength train model epochs. nti-slstm model regularized input/output input/output dropouts ntislstm-lstm model input input/output dropouts binary ﬁnegrained settings. nurses puppies. golden retriever nurses dogs puppies. golden retriever nurses puppies. mother checking baby puppy. girl petting dog. wearing girl petting cat. constituency tree-based counter part ctlstm model. ct-lstm model composes phrases according output sentence parser uses node composition function similar s-lstm. transformed input lstm leaf node function achieved best performance task. attention compositionality help analyzing results output attention weights nti-slstm node-by-node global attention model. figure shows attention heatmaps sentences snli test set. shows model semantically aligns single multiword expressions addition model able re-orient attention different parts hypothesis expression complex. example rock wall autumn mostly focuses nodes depth representing contexts related stone leaves. stone wall surrounded. surprisingly attention degree single word expression like stone wall leaves lower compare multiword phrases. sequence models lack property explicit composition module produce mufinally interesting pattern model attends higher level tree nodes rich semantics considering longer phrase full sentence. shown model aligns root node representing whole hypothesis sentence higher level tree nodes covering larger sub-trees premise. certainly ignores lower level single word expressions starts attend words collectively form rich semantics. using cosine similarity representations produced nti-slstm model show able capture paraphrases snli test data. shown table seems distinguish plural singular forms addition captures non-surface knowledge. example phrases similar park tend align semantic content park including people play frisbee outdoors. model able relate santa claus christmas snow. interestingly learned representations also able connect implicit semantics. example found depressed hatred close phrases like obama supporter upset. overall model robust length phrases matched. given short phrase retrieve longer semantically coherent sequences snli test set. table show nearest-neighbor sentences snli test set. note sentences listed ﬁrst columns sound semantically coherent ones last column. query sentence sells women actually represent common-sense knowledge sentence seem confuse model. result retrieved sentence arbitrary coherent. introduced special padding character order construct full binary tree. padding character inﬂuence performance models? figure show relationship between padding size accuracy stanford sentiment analysis data. sentence padded form full binary tree. x-axis represents number padding characters introduced. padding size less nti-slstm-lstm model performs better. however model tends perform poorly equally padding size large. overobserve signiﬁcant performance drop models padding size increases. suggests learns ignore special padding character processing padded sentences. scenario also observed analyzing attention weights. attention padded nodes nearly zero. introduced neural tree indexers class tree structured recursive neural network. models achieved state-of-the-art performance different tasks. models form deep neural networks think reason works well even lacks direct linguistic motivations followed syntactictree-structured recursive models topologically related cnns hierarchical. however current implementation operates non-overlapping subtrees cnns slide input produce higher-level representations. ﬂexible selecting node function attention mechanism. like computation tree-depth parallelized effectively; therefore scalable suitable large-scale sequence processing. note seen generalization lstm. construct left-branching trees bottom-up fashion model acts like sequential lstm. different branching factors underlying tree structure explored. extended learns select compose dynamic number nodes efﬁciency essentially discovering intrinsic hierarchical structure input. would like thank anonymous reviewers insightful comments suggestions. work supported part grant national institutes health opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect sponsor.", "year": 2016}