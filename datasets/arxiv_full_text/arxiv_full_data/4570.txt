{"title": "Transfer from Multiple MDPs", "tag": ["cs.AI", "cs.LG"], "abstract": "Transfer reinforcement learning (RL) methods leverage on the experience collected on a set of source tasks to speed-up RL algorithms. A simple and effective approach is to transfer samples from source tasks and include them into the training set used to solve a given target task. In this paper, we investigate the theoretical properties of this transfer method and we introduce novel algorithms adapting the transfer process on the basis of the similarity between source and target tasks. Finally, we report illustrative experimental results in a continuous chain problem.", "text": "transfer reinforcement learning methods leverage experience collected source tasks speed-up algorithms. simple effective approach transfer samples source tasks include training used solve target task. paper investigate theoretical properties transfer method introduce novel algorithms adapting transfer process basis similarity source target tasks. finally report illustrative experimental results continuous chain problem. objective transfer reinforcement learning speed-up algorithms reusing knowledge obtained source tasks. underlying assumption transfer methods source tasks somehow similar target task transferred knowledge useful learning solution. wide range scenarios methods transfer studied last decade thorough survey). paper focus simple transfer approach trajectory samples transferred source mdps increase size training used solve target mdp. approach particularly suited problems possible interact environment long enough collect samples solve task hand. samples available sources solution target task beneﬁt larger training includes also source samples. approach already investigated case transfer tasks different state-action spaces source samples used build model target task whenever number target samples large enough. sophisticated sample-transfer method proposed authors introduce algorithm estimates similarity source target tasks selectively transfers source tasks likely provide samples similar generated target mdp. although empirical results encouraging proposed method based heuristic measures theoretical analysis performance provided. hand supervised learning number theoretical works investigated effectiveness transfer reducing sample complexity learning process. domain adaptation solution learned source task transferred target task performance depends similar tasks are. different distance measures proposed shown connected performance transferred solution. case transfer samples multiple source tasks studied interesting ﬁnding transfer performance beneﬁts using larger training cost additional error average distance source target tasks. implies existence transfer tradeoff transferring many samples possible limiting transfer sources similar target task. result transfer samples expected outperform single-task learning whenever negative transfer limited w.r.t. advantage increasing size training set. also opens question whether possible design methods able automatically detect similarity tasks adapt transfer process accordingly. algorithmic contribution. introduce three sample-transfer algorithms based ﬁtted qiteration ﬁrst algorithm simply transfers source samples. also design adaptive methods whose objective solve transfer tradeoff identifying best combination source tasks. theoretical contribution. formalize setting transfer samples derive ﬁnite-sample analysis highlights importance average obtained combination source tasks. also report analysis shows advantage identifying best combination source tasks additional cost terms auxiliary samples needed compute similarity tasks. empirical contribution. report results simple chain problem conﬁrm main theoretical ﬁndings support idea sample transfer signiﬁcantly speed-up learning process adaptive methods able solve transfer tradeoff avoid negative transfer effects. rest paper organized follows. section introduce notation deﬁne transfer problem. section reports theoretical analysis ast. described section along theoretical analysis. challenging setting introduced section together btt. section reports experimental results section concludes paper. finally appendix report proofs additional experimental analysis. notation mdps. deﬁne discounted markov decision process tuple state space bounded closed subset euclidean space ﬁnite action space deterministic reward function uniformly bounded rmax transition kernel distribution discount factor. denote probability measures space bounded measurable functions domain bounded deﬁne optimal action-value function unique ﬁxed-point optimal bellman operator deﬁned notation function spaces. measure obtained combination distribution uniform distribution discrete measurable function deﬁne l-norm ||f|| supremum norm deﬁned ||f||∞ supx∈x finally deﬁne standard feature vector features φ⊤α} linear space action-value functions spanned basis functions given state-action pairs corresponding feature matrix. deﬁne orthogonal projection operator minf f||µ. finally denote truncation function range problem setup. consider transfer problem tasks {mm}m available objective learn solution target task transferring samples source tasks {mm}m deﬁnition input built samples drawn arbitrary sampling distribution i.e. task transition reward sample generated state-action pairs input i.e. indexes drawn i.i.d. multinomial distribution parameters training available learner assumption samples generated practice single realization samples task indexes available. consider case condition implies number target samples much less source samples usually enough learn accurate solution target task. also consider pure transfer case finally notice def. implies existence generative model mdps since stateaction pairs generated according arbitrary sampling distribution ﬁrst consider case source samples generated beforehand according def. designer access source tasks. study algorithm called all-sample transfer simply runs linear space whole training case linear spaces minimization problem solved closed form fig. following report ﬁnite-sample analysis performance ast. similar ﬁrst study prediction error iteration propagate iterations. deﬁne average average mdps hand. deﬁne reward function transition kernel weighted average reward functions transition kernels basic mdps weights determined proportions multinomial distribum= λmpm). remark ﬁrst notice previous bound reduces standard bound bound composed three main terms approximation error estimation error transfer error. approximation error ||fαk teqk−||µ smallest error functions approximating target function teqk− independent transfer algorithm. estimation error ﬁnite random samples used learn depends fast rate linear spaces instead finally transfer error accounts target might bias towards wrong solution thus resulting poor approximation target function teqk−. interesting notice transfer error depends difference target task average obtained taking linear combination source tasks weighted parameters means even source tasks different target exists suitable combination similar target task transfer process still likely effective. furthermore considers difference result operators reward functions even transition distributions different application bellman operators given function eqk−. result λ||tv large) corresponding averages eqk− might still similar similar maxa′ remark iteration samples source task performance bounds +peλ bounds share exactly approximation error. main uses samples result much bigger estimation error takes advantage samples transferred source tasks. time thus conclude expected perform better single-task learning whenever advantage using samples greater bias samples coming tasks different target task. introduces transfer tradeoff including many source samples reduce estimation error ﬁnding source tasks whose combination leads small transfer error. section show possible deﬁne adaptive transfer algorithm selects proportions keep transfer error small possible. finally section consider different setting maximum number samples source ﬁxed. study previous error propagated iterations. evaluation norm ﬁrst report assumptions. assumption given arbitrary sequence policies {πp}p≥ assume future-state distribution supπ···πp ||d. bound reported previous theorem displays differences w.r.t. single-iteration bound. additional term accounts error ﬁnite number iterations decreases exponentially base approximation error supg g||µ. term referred inherent bellman error space related well bellman images functions approximated itself. possible show particular classes mdps large enough number carefully designed features available term small. bounded using linear independency features estimation error norm ||αk term inverse dependency smallest eigenvalue tends small whenever gram matrix well-deﬁned transfer error supα characterizes difference target average bellman operators space result even mdps signiﬁcantly different rewards transitions might small transfer error functions introduces tradeoff design large enough space containing functions able approximate small function space q-functions induced closer term also displays interesting similarities notion discrepancy introduced domain adaptation. discussed previous section transfer error plays crucial role comparison single-task learning. particular related proportions inducing average bellman operator deﬁnes target function approximated iteration. consider case designer direct access source tasks deﬁne arbitrary proportion particular propose method adapts iteration minimize transfer error consider case ﬁxed parameter algorithm iteration need estib iteration algorithm best average transfer ﬁrst computes minλ∈λbeλ -dimensional simplex runs iteration samples generated according proportionsbλk. denote probability remark analysis bound shows outperforms whenever advantage achieving smallest possible transfer error larger additional estimation error auxiliary training set. also interesting compare single-task learning. fact performs better single-task learning whenever best possible combination source tasks small transfer error additional estimation error related auxiliary training smaller estimation error singletask learning. particular means smaller number calls generative model order fair comparison single-task learning obtain condition constrains number tasks smaller dimensionality function space remark dependency auxiliary estimation error fact vectors belong simplex dimensionality hence previous condition suggests that general adaptive transfer methods signiﬁcantly improve transfer performance cost additional sources errors depend dimensionality search space used adapt transfer process remark recomputes proportionsbλk iteration fact combinanecessarily small transfer error ||eq||µ second iteration. investi cost additional estimation error scales size remark previous theorem shows achieves smallest transfer error auxiliary training o/)+o/). notice ﬁrst term estimation error depends well approximated using ﬁnite number state-action pairs slower rate w.r.t. terms. second term depends number next states simulated state-action pair used estimate value bellman operators. result order reduce estimation error need increase number next states state-action pair. interesting notice similar estimation errors appear optimal bellman operator approximated monte-carlo estimation. remark implicit assumption deﬁnition auxiliary training possible generate series next states rewards tasks stateaction pairs. source training sets ﬁxed advance designer access source tasks assumption veriﬁed possible test similarity target task. nonetheless generative model source tasks available learning time auxiliary training could generated learning phase actually begins. furthermore theoretical analysis samples auxiliary training learning time. trivial improvement include auxiliary samples training set. remark method compute similarity mdps proposed. particular authors introduce deﬁnition compliance average probability target samples generated sample-based estimation source mdps. compliance later used determine proportion samples transferred source tasks. although algorithm shares similar objective different notions similarity. particular method tries identify source tasks individually similar target task transfer error minimized considers average obtained transfer process. furthermore notion compliance tries measures overall distance mdps always measures distance images function different bellman operators. previous algorithm proved successfully estimate combination source tasks better approximates bellman operator target task. nonetheless relies implicit assumption samples always generated source task cannot applied case number source samples limited. consider challenging case designer still access source tasks limited number samples available them. case adaptive transfer algorithm solve tradeoff selecting many samples possible reduce estimation error choosing proportion source samples properly control transfer error. solution tradeoff return non-trivial results source tasks similar target task samples removed favor pool tasks whose average roughly approximate target task provide larger number samples. introduce best tradeoff transfer algorithm similar relies auxiliary training solve tradeoff. denote maximum number samples available source task weight vector fraction unlike heuristic algorithm motivated performance bound theorem provide theoretical guarantee performance. main technical difﬁculty w.r.t. previous algorithms setting considered match random task design assumption since number source samples constrained result given proportion vector cannot assume samples drawn random according multinomial parameters without assumption open question whether similar bound could derived. nonetheless experimental results reported section show effectiveness solving transfer tradeoff. section report discuss preliminary experimental results transfer algorithms introduced previous sections. main objective illustrate functioning algorithms compare results theoretical ﬁndings. thus focus simple problem leave challenging problems future work. moves toward left toward right. probability action makes step length affected noise intended direction probability moves opposite direction. target task state–transition model deﬁned following parameters uniform interval reward function provides system state reaches regions elsewhere. furthermore evaluate performance transfer algorithms previously described considered eight source tasks whose state–transition model parameters reward functions reported tab. approximate q-functions linear combination radial basis functions. particular action consider gaussians means uniformly spread interval variance equal plus constant feature. number iterations algorithm empirically ﬁxed samples collected sequence episodes starting state actions chosen uniformly random. experiments average runs report standard deviation error bars. ﬁrst consider pure transfer problem target samples actually used learning training objective study impact transfer error source samples effectiveness ﬁnding suitable combination source tasks. left plot fig. compares performances without transfer samples ﬁrst four tasks listed tab. case single-task learning number target samples refers samples used learning time represents size auxiliary training used estimate transfer error. thus single-task learning performance increases target samples make estimation accurate. number source samples added auxiliary target sample empirically ﬁxed ﬁrst noticed looking models tab. combination different target model learn good policy. hand even small auxiliary target samples able learn good policies. result existence linear combinations source tasks closely approximate target task iteration fqi. example proportion coefﬁcients computed iteration shown right plot fig. ﬁrst iteration produces approximation reward function. given ﬁrst four source tasks figure transfer left comparison single-task learning right comparison single-task learning addition target samples samples source task. improve readability plot truncated target samples. result coefﬁcient drops zero combination among source tasks found. note signiﬁcantly improves single-task learning particular target samples available. general case target task cannot obtained combination source tasks happens considering second source tasks impact situation learning performance shown left plot fig. note that target samples available transfer samples combination source tasks using algorithm still beneﬁcial. hand performance attainable bounded transfer error corresponding best source task combination result single-task quickly achieves better performance. results presented transfer algorithm assume trained samples obtained combinations source tasks. since number target samples already available auxiliary training trivial improvement include training together source samples shown plot right side fig. leads signiﬁcant improvement. behavior clear small target samples better transfer many samples possible source tasks number target samples increases preferable reduce number samples obtained combination source tasks actually match target task. fact much better performance beginning outperformed single-task learning. hand initial advantage small performance remains close single-task large number target samples. experiment highlights tradeoff need samples reduce estimation error resulting transfer error target task cannot expressed combination source tasks algorithm provides principled address tradeoff shown right plot fig. exploits advantage transferring source samples target samples available reduces weight source tasks samples target task enough. interesting notice increasing number samples available source task improves performance ﬁrst part graph keeping unchanged ﬁnal performance. capability algorithm avoid transfer source samples need them thus avoiding negative transfer effects. paper formalized studied sample-transfer problem. ﬁrst derived ﬁnitesample analysis performance simple transfer algorithm includes source samples training used solve given target task. best knowledge ﬁrst theoretical result transfer algorithm showing potential beneﬁt transfer single-task learning. then case designer direct access source tasks introduced adaptive algorithm selects proportion source tasks minimize bias source samples. finally considered challenging setting number samples available source task limited tradeoff amount transferred samples similarity source target tasks must solved. setting proposed principled adaptive algorithm. finally report detailed experimental analysis simple problem conﬁrms supports theoretical ﬁndings. transfer transformations. many problems exist simple transformations source tasks dynamics reward would increase similarity w.r.t. target task thus making transfer process effective. afﬁne transformations could used adaptive transfer algorithms presented paper interesting direction future work. particular open question whether cost ﬁnding suitable transformation would effectively counter-balanced transferring similar samples. transfer tasks different state-action spaces. many real applications source target tasks might different number state variables different actions. thus current work extended general case tasks different stateaction spaces integrated inter-task mapping transfer methods transfer ﬁxed tasks design. deﬁnition prescribes process used generate training used learning target task. state-action pair sample generated source task chosen random according multinomial distribution. designer access source tasks samples generated beforehand generative model reasonable. different model deﬁned sample coming speciﬁc source ﬁxed advance. interesting direction future work understand different generative model affects performance transfer algorithm whether possible deﬁne effective adaptive algorithms case. acknowledgments work supported french national research agency projects explo-ra anr--cosi- ministry higher education research nordpas calais regional council feder contrat projets ´etat region pascal european network excellence. research leading results also received funding european community’s seventh framework programme grant agreement", "year": 2011}