{"title": "Dynamic Policy Programming", "tag": ["cs.LG", "cs.AI", "cs.SY", "math.OC", "stat.ML"], "abstract": "In this paper, we propose a novel policy iteration method, called dynamic policy programming (DPP), to estimate the optimal policy in the infinite-horizon Markov decision processes. We prove the finite-iteration and asymptotic l\\infty-norm performance-loss bounds for DPP in the presence of approximation/estimation error. The bounds are expressed in terms of the l\\infty-norm of the average accumulated error as opposed to the l\\infty-norm of the error in the case of the standard approximate value iteration (AVI) and the approximate policy iteration (API). This suggests that DPP can achieve a better performance than AVI and API since it averages out the simulation noise caused by Monte-Carlo sampling throughout the learning process. We examine this theoretical results numerically by com- paring the performance of the approximate variants of DPP with existing reinforcement learning (RL) methods on different problem domains. Our results show that, in all cases, DPP-based algorithms outperform other RL methods by a wide margin.", "text": "paper propose novel policy iteration method called dynamic policy programming estimate optimal policy inﬁnite-horizon markov decision processes. prove ﬁnite-iteration asymptotic ℓ∞-norm performance-loss bounds presence approximation/estimation error. bounds expressed terms ℓ∞-norm average accumulated error opposed ℓ∞-norm error case standard approximate value iteration approximate policy iteration suggests achieve better performance since averages simulation noise caused monte-carlo sampling throughout learning process. examine theoretical results numerically comparing performance approximate variants existing reinforcement learning methods diﬀerent problem domains. results show that cases dpp-based algorithms outperform methods wide margin. many problems robotics operations research process control represented control problem solved ﬁnding optimal policy using dynamic programming based estimating measures value state-action bellman equation. high-dimensional discrete systems continuous systems computing value function intractable. common approach make computation tractable approximate value function using function-approximation monte-carlo sampling examples approximate dynamic programming methods approximate policy iteration approximate value iteration methods successfully applied many real world problems theoretical results derived form ﬁnite iteration asymptotic performance guarantee induced policy asymptotic ℓ∞-norm performance-loss bounds denotes discount factor ℓ∞-norm w.r.t. state-action pair also control policy approximation error round algorithms respectively. many problems interest however supremum normed-error kǫkk large hard control large variance estimation caused monte-carlo sampling. cases bound instead depends j=ǫj preferable. fact errors associated variance estimation considered instances zero-mean random variables. therefore show making large numbers argument errors asymptotically averaged accumulating approximation errors iterations. paper propose mathematically-justiﬁed approach estimate optimal policy called dynamic policy programming prove ﬁnite-iteration asymptotic performance loss bounds policy induced presence approximation. asymptotic bound approximate expressed terms average accumulated error k¯ǫkk opposed kǫkk case api. result suggests perform better presence large variance estimation since average estimation errors throughout learning process. dependency average error k¯ǫkk follows naturally incremental policy update round policy update unlike accumulates approximation errors previous iterations rather minimizing approximation error current iteration. article organized follows. section present notations used paper. introduce investigate convergence properties section section demonstrate compatibility method approximation techniques. generalize bounds case function approximation monte-carlo simulation. also introduce convergent algorithm called dpprl relies approximate sample-based variant estimate optimal policy. section presents numerical experiments several problem domains including optimal replacement problem stochastic grid world. section brieﬂy review related work. finally discuss implications work section section introduce concepts deﬁnitions theory markov decision processes reinforcement learning well standard notations. begin deﬁnition ℓ-norm ℓ∞-norm discounted quintuple respectively state space action space. shall denote state transition distribution denotes reward kernel. denotes discount factor. transition probability kernel next state upon taking action state shall denote real-valued numbers. reward associated state action keep representation succinct shall denote joint state-action space policy kernel determines distribution control action given past observations. policy called stationary markovian distribution control action independent time depends last state given last state shall denote stationary policy stationary policy called deterministic state exists action concentrates action. given policy corresponding value function denotes expected value long-term discounted rewards state action chosen policy denote often convenient associate value functions states state-action pairs. therefore introduce expected total discounted reward upon choosing action state following policy shall denote deﬁne bellman operator action-value functions section derive algorithm starting bellman equation. ﬁrst show adding relative entropy term reward control deviations induced policy baseline policy. derive iterative double-loop approach combines value policy updates. reduce double-loop iteration single iteration introducing algorithm. emphasize purpose following derivations motivate rather provide formal characterization. subsequently subsection section theoretically investigate ﬁnite-iteration asymptotic behavior prove convergence. positive constant rt+k reward time also expected value taken w.r.t. state transition probability distribution policy optimal value function satisﬁes following bellman equation equation modiﬁed version where addition maximizing expected reward optimal policy also minimizes distance baseline policy maximization performed closed form. following todorov state proposition optimal policy function base policy optimal value function state transition probability ﬁrst obtain optimal value function following ﬁxed-point iteration compute using maximizes value function however principle interested quantifying solving original problem computing idea improve policy towards replace baseline policy newly computed policy policy regarded base-line policy process repeated again. leads doubleloop algorithm optimal policy outer-loop inner-loop would consist policy update equation value function update equation respectively. follow following steps derive ﬁnal algorithm introduce extra smoothness policy update rule replacing double-loop algorithm direct optimization value function policy simultaneously using following ﬁxed point iterations finally would like emphasize important diﬀerence double-loop algorithm resulted solving notice algorithm regardless choice always incremental even goes whereas case double-loop update policy value function algorithm reduced standard value iteration apparently incremental policy reason diﬀerence extra smoothness introduced update rule replacing double-loop update single loop optimize bound choice soft-max policy soft-max operator replaced greedy policy max-operator immediate consequence theorem obtain following result algorithm applies small problems states actions. generalize algorithm problems practical scale using function approximation techniques. also compute optimal policy explicit knowledge model required. many real world problems information available instead possible simulate state transition monte-carlo sampling approximately estimate optimal policy using samples. section provide results performance-loss presence approximation/estimation error. compare ℓ∞-norm performance-loss bounds standard results api. finally introduce approximate algorithms implementing monte-carlo sampling linear function approximation consider sequence action preferences that round action preferences result approximately applying operator means function approximation monte-carlo simulation i.e. oψk. error term deﬁned diﬀerence approximation begin ﬁnite iteration analysis approximate dpp. following theorem establishes upper-bound performance loss presence approximation error. proof based generalization bound established taking account error εmax supk→∞ kǫkk. diﬀerence supremum norm error εmax replaced supremum norm average error words unlike size error iteration critical factor performance long size average error remains close achieve near-optimal performance even error arbitrary large. gain better understanding result consider case which algorithm sequence errors i.i.d. zero-mean random variables bounded obtain following asymptotic bound approximate applying large numbers corollary words approximate manages cancel i.i.d. noise asymptotically converges optimal policy whereas guarantee case convergence optimal solution. result suggests average simulation noise caused monte-carlo sampling eventually achieve signiﬁcantly better performance presence large variance estimation. show next subsection sampling-based variant manages cancel simulation noise asymptotically converges almost surely optimal policy compute optimal policy needs explicit knowledge model. many problems access information instead generate samples simulating model. optimal policy learned using samples. section introduce algorithm called dpp-rl relies sampling-based variant update policy. update rule dpp-rl similar diﬀerence that dpp-rl replace bellman operator sample estimate next sample drawn equation approximation update rule therefore convergence result corollary hold dpp-rl. however algorithm still converges optimal policy since show errors associated approximating asymptotically averaged dpp-rl postulated corollary following theorem establishes asymptotic convergence policy induced dpp-rl optimal policy. theorem assumption hold. assume initial action-value function uniformly bounded vmax policy induced algorithm round then w.p. following holds notice update rule dpp-rl unlike incremental methods q-learning sarsa involve decaying learning step. important diﬀerence since known convergence rate incremental methods like q-learning sensitive choice learning step choice learning step lead signiﬁcantly slow rate convergence. dpp-rl seems suﬀer problem since dpp-rl update rule empirical estimate update rule dpp. therefore expect rate convergence dpp-rl remains close fast rate convergence established theorem subsection consider linear function approximation leastsquares regression. given basis functions bounded real valued function sequence action preferences deﬁned linear combination basis functions column vector entries {φi}i=m vector parameters. action preference function approximation operator oψk. case common approach approximate operator vector projects column space spanned minimizing loss function expectation taken w.r.t. principle compute least squares solution equation requires compute states actions. large scale problems becomes infeasible. instead make sample estimate least-squares solution minimizing empirical loss {}n=n i.i.d. samples drawn distribution also onψk denotes single sample estimate deﬁned onψk γmηψk. further avoid overﬁtting small size data quadratic regularization term loss function. empirical least-squares solution minimizes given section analyze empirically eﬀectiveness proposed algorithms different problem domains. ﬁrst examine convergence properties dpp-rl several discrete state-action problems compare standard algorithms synchronous variant q-learning model-based -value iteration kearns singh next investigate ﬁnite-time performance sadpp presence function approximation limited sampling budget iteration. case consider variant optimal replacement problem described munos szepesv´ari compare method regularized least-squares ﬁtted q-iteration source code tested algorithms freely available http//www.mbfys.ru.nl/~mazar/research top.html. linear problem consists states arranged one-dimensional chain possible actions every state accessible state except ends chain absorbing states. state called absorbing figure linear illustration linear problem. nodes indicate states. states absorbing states state example interior state. arrows indicate possible transitions three nodes only. node reachable transition probability proportional inverse distance transition probability interior state state inversely proportional distance direction selected action zero states corresponding opposite direction. formally consider following quantity assigned non-absorbing states every combination lock combination lock problem considered stochastic variant reset state space models introduced koenig simmons reset state possible case consider before states arranged one-dimensional chain possible actions problem however absorbing state associated reward state reached all-ones sequence entered correctly. otherwise state action taken lock automatically resets previous state randomly illustration combination lock problem. nodes indicate states. state goal state state example interior state. arrows indicate possible transitions nodes only. previous state reachable transition probability proportional inverse distance among future states reachable every intermediate state rewards actions respectively. transition probability upon taking wrong action before inversely proportional distance states. grid world consists grid states. four actions {right down left} assigned every state location state grid determined coordinates integers absorbing ﬁrewall states surrounding grid another center grid reward assigned. reward ﬁrewalls means top-left absorbing state central state least possible reward remaining absorbing states reward increases proportionally distance state bottom-right corner transition probabilities deﬁned following taking action non-absorbing state results one-step transition direction action probability random move state probability inversely proportional euclidean distance cyk. optimal policy survive grid long possible avoiding absorbing ﬁrewalls center grid. note describe experimental setting. convergence properties dpp-rl compared algorithms synchronous variant q-learning which like dpp-rl updates action-value function state-action pairs iteration model-based q-value iteration kearns singh batch reinforcement learning algorithm ﬁrst estimates model using whole data performs value iteration learned model. algorithms evaluated terms ℓ∞-norm performance loss action-value function qπkk obtained policy induced iteration choose performance measure order consistent performance measure used section discount factor ﬁxed optimal action-value function computed high accuracy value iteration. consider polynomial learning step linear learning step note needs larger otherwise asymptotically diverge achieve best rate convergence dpp-rl replaces soft-max operator dpp-rl update rule operator resulting greedy policy fair comparison three algorithms since algorithm requires different number computations iteration total computational budget algorithms value benchmark. computation time constrained seconds case linear combination lock problems. grid world twice many actions benchmarks maximum time ﬁxed seconds. also total number samples state-action samples problems algorithms. signiﬁcantly less number samples leads dramatic decrease quality obtained solutions using approaches. algorithms implemented ﬁles intel core processor memory. time acquired using system function times provides process-speciﬁc time. randomization implemented using uniform function library superior standard rand. sampling time algorithms included time. figure shows performance-loss terms elapsed time three problems algorithms. results averages runs beginning action-value function action preferences randomly initialized interval samples generated results correspond average error computed small ﬁxed amount iterations. first dpp-rl converges fast achieving near optimal performance seconds. dpp-rl outperforms three benchmarks. minimum maximum errors attained linear problem grid world respectively. also observe diﬀerence dpp-rl signiﬁcant orders magnitude linear combination lock problems. grid world dpp-rl’s performance times better standard deviations performance-loss give indication robust solutions obtained algorithms. table shows ﬁnal numerical outcomes dpprl variance estimation dpp-rl substantially smaller table comparison dpp-rl q-learning model-based value iteration given ﬁxed computational sampling budget. table shows error means standard deviations simulations three diﬀerent algorithms three diﬀerent benchmarks results show that suggested theorem dpp-rl manages average simulation noise caused sampling converges rapidly near optimal solution robust. addition conclude dpp-rl performs signiﬁcantly better three presented benchmarks choice experimental setup. subsection illustrate performance sadpp algorithm presence function approximation limited sampling budget iteration. compare sadpp modiﬁcation regularized ﬁtted q-iteration make ﬁxed number basis functions. rfqi regarded monte-carlo sampling implementation approximate value iteration action-state representation. compare sadpp rfqi since methods make ℓ-regularization. purpose subsection analyze numerically sample complexity number samples required achieve near optimal performance variance sadpp. benchmark consider variant optimal replacement problem presented munos szepesv´ari following subsection describe problem subsequently present results. problem inﬁnite-horizon discounted mdp. state measures accumulated certain product represented continuous one-dimensional variable. time-step either product kept replaced whenever product replaced state variable reset zero additional cost state chosen according exponential distribution possible values starting zero current state value depending latest action sadpp rfqi state-action space using radial basis functions parameter values chosen munos szepesv´ari results also upper bound states xmax modify problem deﬁnition next state happens outside domain product replaced immediately state drawn action chosen previous time step. action selected algorithm. note that unlike rfqi selects action choosing action highest action-value function sadpp induces stochastic policy distribution actions. select sadpp choosing probable action induced soft-max policy compute algorithms implemented matlab executed hardware speciﬁcations previous section. analyze eﬀect using diﬀerent number samples iteration results averages runs beginning vector initialized interval algorithms. rest parameters including regularization factor optimized best asymptotic performance independently. figure shows averages standard deviations errors. first observe large initial transient sadpp rfqi reach near optimal solution. observe sadpp asymptotically outperforms rfqi average cases. average error variance estimation resulting solutions decreases approaches. comparison variances transient suggests sample complexity sadpp signiﬁcantly smaller rfqi. remarkably variance sadpp using samples comparable provided rfqi using samples. further variance sadpp reduced faster increasing results allow conclude sadpp positive eﬀects reducing eﬀect simulation noise postulated section methods rely incremental update policy. wellknown algorithm kind actor-critic method actor uses value function computed critic guide policy search important extension policy-gradient actor critic extends idea problems practical scale areas indicate averages plus/minus standard deviations error starting uniformly distributed initial conditions pgac actor updates parameterized policy direction gradient performance provided critic. gradient update ensures pgac asymptotically converges local maximum given unbiased estimate gradient provided critic incremental methods include qlearning sarsa considered incremental variants value iteration optimistic policy iteration algorithms respectively algorithms shown converge optimal value function tabular case also studies literature concerning asymptotic convergence q-learning presence function approximation however best knowledge preceding literature asymptotic ﬁnite-iteration performance loss bounds incremental methods study appears ﬁrst prove bound incremental algorithm. work proposed paper relation recent work kappen todorov formulate stochastic optimal control problem conditional probability distribution given uncontrolled dynamics control cost relative entropy exp). diﬀerence work restricted class control problems considered optimal solution computed directly terms without requiring bellman-like iterations. instead present approach general requires bellman-like iterations. likewise formalism superﬁcially similar power saem rely algorithm maximize lower bound expected return iterative fashion. lower-bound also written kl-divergence distributions. another relevant study relative entropy policy search relies idea minimizing relative entropy control size policy update. main diﬀerences reps algorithm actor-critic type algorithm policy iteration type method. reps inverse temperature needs optimized converges optimal solution inverse temperature provide convergence analysis convergence analysis reps. presented approach dynamic policy programming compute optimal policy inﬁnite-horizon discounted-reward mdps. theoretically proven convergence optimal policy tabular case. also provided performance-loss bounds presence approximation. bounds expressed terms supremum norm average accumulated error opposed standard results expressed terms supremum norm errors. introduced incremental model-free algorithm called dpp-rl relies sample estimate instance update rule estimate optimal policy. proven asymptotic convergence dpp-rl optimal policy compared numerically standard methods. experimental results various mdps provided showing that cases dpp-rl superior methods terms convergence rate. fact dpp-rl unlike incremental methods rely stochastic approximation estimating optimal policy therefore suﬀer slow convergence caused presence decaying learning step stochastic approximation. work interested estimation optimal policy problem exploration. therefore compared algorithms pac-mdp methods choice exploration policy impacts behavior learning algorithm. also paper compared results since rely diﬀerent kind sampling strategy dpprl sadpp rely generative model sampling whereas makes trajectories state-action pairs generated monte-carlo simulation estimate optimal policy. study provide ℓ∞-norm performance-loss bounds approximate dpp. however supervised learning regression algorithms rely minimizing form ℓp-norm error. therefore natural search kind performance bound relies ℓp-norm approximation error. following munos ℓp-norm bounds approximate established providing bound performance loss component value function policy induced dpp. would topic future research. another direction future work provide ﬁnite-sample probably approximately correct bounds sadpp dpp-rl spirit previous theoretical results available ﬁtted value iteration ﬁtted q-iteration update rule since computing exact summations become expensive. idea sample estimate mηψk using monte-carlo simulation since mηψk expected value soft-max policy derive needs relate optimal unfortunately ﬁnding direct relation easy task. instead relate auxiliary action-value function deﬁne below. remainder section take following steps express terms lemma obtain upper bound normed error lemma finally results derive bound normed error qπkk. sake readability skip formal proofs lemmas section since prove general case section further sequel repress state dependencies notation wherever dependencies clear e.g. becomes becomes consider sequence action preferences iterates goal establish ℓ∞-norm performance loss bound policy induced approximate dpp. main result iteration approximate have cumulative approximation error step here denotes action-value function policy soft-max policy associated proof theorem relate auxiliary action-value sequel ﬁrst express terms lemma then function obtain upper bound normed error lemma finally results derive deﬁne auxiliary action-value function sequence auxiliary action-value function resulted iterating initial action-value function following recursion deﬁne ﬁltration generated sequence random variables drawn distribution property e|fk−) means sequence estimation errors martingale diﬀerence sequence w.r.t. ﬁltration asymptotic converge dpp-rl optimal policy follows extending result case bounded martingale diﬀerences. need show sequence estimation errors {ǫj}j=k uniformly bounded thus prove convergence dpp-rl need show kekk asymptotically converges w.p. rely strong large numbers martingale diﬀerences states average sequence martingale diﬀerences asymptotically converges almost surely second moments entries sequence bounded case sequence martingales since already proven boundedness kǫkk lemma thus deduce", "year": 2010}