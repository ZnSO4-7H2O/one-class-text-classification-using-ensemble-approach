{"title": "On the Reliable Detection of Concept Drift from Streaming Unlabeled Data", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Classifiers deployed in the real world operate in a dynamic environment, where the data distribution can change over time. These changes, referred to as concept drift, can cause the predictive performance of the classifier to drop over time, thereby making it obsolete. To be of any real use, these classifiers need to detect drifts and be able to adapt to them, over time. Detecting drifts has traditionally been approached as a supervised task, with labeled data constantly being used for validating the learned model. Although effective in detecting drifts, these techniques are impractical, as labeling is a difficult, costly and time consuming activity. On the other hand, unsupervised change detection techniques are unreliable, as they produce a large number of false alarms. The inefficacy of the unsupervised techniques stems from the exclusion of the characteristics of the learned classifier, from the detection process. In this paper, we propose the Margin Density Drift Detection (MD3) algorithm, which tracks the number of samples in the uncertainty region of a classifier, as a metric to detect drift. The MD3 algorithm is a distribution independent, application independent, model independent, unsupervised and incremental algorithm for reliably detecting drifts from data streams. Experimental evaluation on 6 drift induced datasets and 4 additional datasets from the cybersecurity domain demonstrates that the MD3 approach can reliably detect drifts, with significantly fewer false alarms compared to unsupervised feature based drift detectors. The reduced false alarms enables the signaling of drifts only when they are most likely to affect classification performance. As such, the MD3 approach leads to a detection scheme which is credible, label efficient and general in its applicability.", "text": "classiﬁers deployed real world operate dynamic environment data distribution change time. changes referred concept drift cause predictive performance classiﬁer drop time thereby making obsolete. real classiﬁers need detect drifts able adapt them time. detecting drifts traditionally approached supervised task labeled data constantly used validating learned model. although eﬀective detecting drifts techniques impractical labeling diﬃcult costly time consuming activity. hand unsupervised change detection techniques unreliable produce large number false alarms. ineﬃcacy unsupervised techniques stems exclusion characteristics learned classiﬁer detection process. paper propose margin density drift detection algorithm tracks number samples uncertainty region classiﬁer metric detect drift. algorithm distribution independent application independent model independent unsupervised incremental algorithm reliably detecting drifts data streams. experimental evaluation drift induced datasets additional datasets cybersecurity domain demonstrates approach reliably detect drifts signiﬁcantly fewer false alarms compared unsupervised feature based drift detectors. time produces performance comparable fully labeled drift detector. reduced false alarms enables signaling drifts likely aﬀect classiﬁcation performance. such approach leads detection scheme credible label eﬃcient general applicability. machine learning ushered data deluge increasing scale reach modern applications classiﬁcation adopted popular technique providing data-driven prediction/detection capabilities core several otherwise complicated intractable tasks. ability generalize extrapolate data made usage attractive general approach data driven problem solving. however generalization ability classiﬁers relies important assumption stationaritystates training test data identically independently distributed derived distribution assumption often violated real world dynamic changes occur constantly. changes data distribution called concept drift cause predictive performance classiﬁers degrade time. ensure classiﬁers operating dynamic environments useful obsolete adaptive machine learning strategy warranted detect changes data update models data becomes available. several adaptive techniques proposed literature rely unhindered unbounded supply human expertise form labeled data detect adapt drifting data. streaming environment data ﬂows constantly constant human intervention impractical labeling time consuming expensive cases possibility highlight problem label dependence consider task detecting hate speech live tweets using classiﬁcation system facing twitter stream tweets requested labeled using crowd sourcing websites amazon’s mechanical turk would imply daily expenditure continuous availability crowd sourced workers every single particular task alone. scale velocity modern data applications makes dependence labeled data practical economic limitation. streaming data applications need able operate detect drifts unlabeled atmost sparsely labeled data real use. although labeled data retraining updating models largely unavoidable purpose drift detection superﬂuous. need constant validation learned model leads wasted labels discarded model found stable motivated development unlabeled drift detection techniques monitor changes feature distribution early indicator drift. however existing methods using unlabeled data essentially change detection techniques detect change data distribution irrespective eﬀects classiﬁcation process task classiﬁcation change relevant causes model performance degrade. relevance function learned model illustrated figure data shift resulted diametrically opposite results. figure model performance unaﬀected complete failure prediction capabilities diﬀerence lies classier models result learning different views data. existing unlabeled techniques fail make distinction cases totally exclude classiﬁer detection process make decisions solely distribution characteristics unlabeled data. results increased sensitivity change large number generated false alarms. false alarms drift detection makes algorithm overly paranoid leads wasted labeling eﬀort spent verify relevance probabilistic perspective concept drift seen change joint probability distribution data samples corresponding class labels equation unlabeled change detection techniques track changes labeled drift detection approaches directly track paper unlabeled drift detection methodology proposed vicariously track changes without needing explicit labeled samples. changes tracked based distribution sample relative learned classiﬁer’s boundary make robust towards irrelevant changes distribution data. margin density drift detection methodology proposed paper monitors number samples classiﬁer’s region uncertainty detect drifts. robust classiﬁers support vector machines feature bagged ensemble after training regions uncertainty called margins depicted figure regions result classiﬁer’s attempt generalize unseen data represent model’s best guess data space. large margin width density core optimization based classiﬁcation process while explicit information class distribution learned training classiﬁer additional auxiliary information also learned often overlooked margin characteristics expected margin density. information representative data state change could indicate non-stationarity. margin crucial generalization process changes margin density worthy veriﬁcation. since margin density computed unlabeled data only could used substitute explicit labeled drift detection techniques monitoring changes motivation methodology proposed application independent classiﬁer independent unlabeled incremental approach reliably signal concept drift streaming data. false alarms change detection hassle increased labeling expenditure need frequent veriﬁcation behavior especially undesirable cybersecurity applications becausea) frequent false alarms annoys experts provide model veriﬁcation causing detection process loose credibility overly reactive system used adversary manipulate learning cause spend excessive amount money labeling increased labeling false alarms expensive cause delay detection attacks. such evaluate approach domain independent methodology ﬁrst also evaluate applicability reliable drift detection approach adversarial streaming domains. research work area dealing concept drift motivated need unlabeled approach drift detection initial evaluation work presented paper extend earlier version with extending signal general classiﬁer independent approach allowing usage classiﬁers having margins also ensemble classiﬁers presenting evaluating novel drift induction scheme controlled testing eﬀects various drift scenarios iii) evaluating approach cybersecurity domain datasets benchmark concept drift datasets demonstrating generality eﬃcacy approach irrespective underlying classiﬁer used performing additional sensitivity analysis comparison state margin based unlabeled drift detection techniques. main contributions paper state developing margin density drift detection algorithm incremental streaming algorithm detecting drifts unlabeled data. formulation margin density classiﬁers explicit margins withexplicit margins comparison empirical evaluation demonstrating equivalence. novel drift induction framework introducing concept drift datasets controlled experimentation variety data domains. drift induction process provides reusable testing framework enabling addition concept drift static datasets better experimentation analysis drift detection methodologies. experimental evaluation framework datasets cybersecurity domains highlighting eﬃcacy proposed approach providing reliable robust adversarial drift detection. rest paper organized follows section presents detailed review existing work drift detection background proposed work. section introduces margin density metric signal change section develops streaming algorithm. experimental results drift induced datasets datasets drift cybersecurity domains presented section additional discussion analysis eﬃcacy approach motivation future work presented section conclusion avenues future work presented section detecting change essential trigger based stream adaptation strategies. several methods literature proposed recently goncalves summarized table techniques divided categories based reliance labeled data explicit/supervised drift detectors implicit/unsupervised drift detectors. explicit drift detectors rely labeled data compute performance metrics accuracy f-measure monitor online time. detect drop performance eﬃcient signaling change matters. implicit drift detectors rely properties unlabeled data’s feature values signal deviations. prone false alarms ability function without labeling makes useful applications labeling expensive time consuming available. table shows taxonomy drift detection techniques popular techniques category. techniques continuously monitor sequence performance metrics accuracy f-measure precision recall; signal change event signiﬁcant drop values. cumulative approach signals alarm mean sequence signiﬁcantly deviates cusum test monitors metric time incoming sample’s performance using parameters acceptable deviation change threshold. function equation used test changes positive direction. reverse effect function used. test memory-less used incrementally. variant approach pagehinckley test originally developed signal processing domain detect deviation mean gaussian signal. monitor metric accumulated diﬀerence mean current values shown where initial metric time current metric computed accumulation metric sample’s performance time parameter denotes acceptable deviation mean change detection threshold. cusum best suited univariate change detection sequence performance measures tracked online algorithms. related statistical change detection proposed deal imbalanced streaming data monitors multiple performance metrics. technique monitors true positive rate false positive rate true negative rate false positive rate obtained confusion matrix classiﬁcation. traditional metrics accuracy biased towards majority class confusion matrix presents detailed view suitable imbalance class problems. statistical process control based methodologies probably approximately correct learning model machine learning states error rate trained model decrease increasing number samples data distribution remains stationary drift detection techniques based statistical process control monitor online trace error rates detects deviations based ideas taken control charts. signiﬁcantly increased error rate violates model assumed result concept drift. drift detection method early drift detection methodology popular techniques category. approach monitors probability error time standard deviation pt/i. when reaches minimum value corresponding values stored pmin smin. warning signaled pmin smin drift signaled pt+st pmin+∗smin. eddm developed extension made suitable slow moving gradual drifts previously failed. eddm monitors number samples classiﬁcation errors metric tracked online drift detection. based model assumed that stationary environments distance subsequent errors would increase. violation condition seen indicative drift. statistical test equal proportions computes accuracy chunk recent samples compares overall accuracy beginning stream using chi-squares test check deviation. incremental approach proposed exponentially weighted moving average used signal deviation average error rate terms number standard deviations mean. metric time updated obtained training data random sampling. error rate time given acceptable deviation terms number standard deviation mean forgetting factor controls eﬀect previous data current sample. eddm stepd ewma also employ initial warning subsequent drift signaling system approach. unlike methods mentioned thus operate incremental fashion sample time window based approaches chunk based sliding window approach recent samples detect changes. deviations computed comparing current chunk’s distribution reference distribution obtained start stream training dataset window based approaches provide precise localization change point robust noise transient changes. however need extra memory store distributions time. adaptive windowing algorithm uses variable length sliding window whose length computed online according observed changes. case change present window shrunk vice-versa. whenever large enough windows current chunk exhibit distinct averages performance metric drift detected. hoeﬀding bounds used determine optimal change threshold window parameters. adwin methodology shown provide rigorous performance guarantees eﬃcient memory time complexities freedom specify cumbersome parameter values. another window based approach degree drift detects drifts computing distance samples current chunk nearest neighbors previous chunk degree drift metric computed based distance distance increases parameter drift signaled. paired learners approach uses pair reactive learner trained recent chunk data stable learner trained previously seen data. diﬀerences accuracies approaches indicative drift. disagreement captured binary valued circular list. increase number ones beyond change threshold signaled concept drift. drift managed replacing stable model reactive setting circular disagreement list recently proposed permutation based method relies observation randomly choosing training testing samples chunk data lead similar accuracy prediction unless window non-stationary data. method based idea commonly used classiﬁer’s cross validation evaluation cross validation entire training dataset split bands iteration fold cross validation approach band chosen test data bands form training dataset. generating model training dataset testing test dataset gives performance fold cross validation process. process repeated times average performance reported. cross validation provides good estimate generalization error data stationary. permutation approach splits current window parts train model ﬁrst half test second half. window shuﬄed process repeated. signiﬁcant change performance indicates drift concept drift sensitive sequence samples. method shown better precision-recall values robustness compared eddm stepd methods described section novelty detection methods relies using distance and/or density information detect previously unseen data distribution patterns. methods capable identifying uncertain suspicious samples need evaluation. deﬁne additional ’unknown’ class label suspicious samples existing view data clustering outlier based approaches popular implementation strategies detecting novel patterns summarize current data dissimilarity metrics identify samples online novelty drift detection algorithm uses k-means data clustering continuously monitor adapt emerging data distributions unknown samples stored short term memory queue periodically clustered either merged existing similar cluster proﬁles added novel proﬁle pool clusters. minas algorithm uses micro clusters obtains using incremental stream clustering algorithmclustream extends olinlindas approach used multi class problem. detectnod algorithm uses clustering model deﬁne boundaries existing known data. relies discrete cosine transform build compact representation clusters uses information provide nearest neighbor approximation incoming test samples. samples falling normal mode clustered clusters based similarity values existing clusters either termed ’novelties’ ’drifts’. figure illustrates process identiﬁed drift existing normal mode clusters identiﬁed novel pattern. ensemble ecsminer techniques rely concept micro clusters. ensemble clusters data assigns classiﬁer clusters. sample falling outside boundary existing cluster marked suspicious density monitored. increased number samples within radius suspicious samples indicates concept triggers retraining models readjustment cluster centroid. ecsminer uses concept filtered outliers refers samples fall outside boundary existing clusters works similar ensemble. approach extends idea micro clusters used grid density based clustering algorithm novelty determined newly appearing dense grids data space. novelty detection techniques rely clustering recognize regions space previously unseen. such suﬀer curse dimensionality distance dependent also problem dealing binary data spaces. additionally suitable detect speciﬁc type cluster-able drifts. drift manifest itself cluster novel region space make drift detection computationally eﬃcient high dimensional data streams principal component analysis based feature reduction used techniques reduce features monitored. shown monitoring reduced feature space allowed detect drifts original features. kuncheva faithfull advocates semi parametric likelihood criterion monitor changes data projected principal components. proposed monitoring principal components lowest eigenvalues suﬃcient detecting eﬀective drifts. however work presented contrasting results. shown analyzing principal components large eigenvalues valuable data characteristics original feature space best summarized component vectors retain maximum variance reduction. although based approached eﬃcient reducing number features tracked still suﬀer signiﬁcant false alarms multivariate distribution approaches. methods sensitive changes features irrespective importance classiﬁcation task. methods also suitable detecting concept drifts cases drift manifested feature distribution changes changes equation furthermore classiﬁcation imbalanced datasets methods eﬀective tracking changes minority class samples. changes samples signal signiﬁcant deviation minority class samples comprise small percentage original dataset. methodologies section explicitly track deviations feature distribution unlabeled data. such essentially change detection methodologies assume change data distribution lead changes classiﬁcation performance methods attractive independence type classiﬁer used methods lead detecting large number false alarms i.e. changes lead degradation classiﬁcation performance. false alarms lead wasted human intervention eﬀort undesirable. model dependent approaches directly consider classiﬁcadetection techniques fail. nevertheless techniques suitable multi-class classiﬁcation problems many classes appear disappear during course stream. multivariate distribution monitoring approaches directly monitor feature distribution unlabeled data. approaches primarily chunk based store summarized information training data chunk reference distribution monitor changes current data chunk. hellinger distance kl-divergence commonly used measure diﬀerences chunk distributions signal drift event signiﬁcant change. change concept technique considers feature independent stream data monitors correlation current chunk reference training chunk. change average correlation features used signal change. pearson correlation used makes normality assumption distribution. parametric widely applicable unlabeled approach proposed called hellinger distance drift detection methodology chunk based approach uses hellinger distance measure change distribution time. increased hellinger distance current stream chunk training reference chunk used signal drift. chunk distribution computed bins making histogram feature number samples chunk. hellinger distance reference chunk current chunk computed using equation here data dimensionality number bins computed hellinger distance averaged features results number range value indicates completely overlap used detect drifts conjunction incremental learning algorithm trigger resetting model. eﬃcacy approach indicated intion process tracking posterior probability estimates classiﬁers detect drift. used probabilistic classiﬁers output class probabilities thresholding generate ﬁnal class label. monitoring posterior probability estimates drift detection task reduced monitoring univariate stream values making process computationally eﬃcient. kolmogorov-smirnov test wilcoxon rank test sample t-test suggested monitor stream posterior probability estimates. idea margin introduced average uncertainty samples monitored lieu multivariate feature values. idea extended task detecting domain shifts high dimensional text classiﬁcation applications. reduced false positive rate obtained tracking ’a-distance’ proposed measure histogram diﬀerence obtained binning margin distribution samples reference current margin samples. conﬁdence distribution batch detection approach used kl-divergence perform similar analysis classiﬁer conﬁdence output values additionally combining active learning reduce amount labeled data requested text data streams. methods attractive signiﬁcantly reduce false alarms. however dependence using probabilistic models limit applicability. also methods trigger change posterior distribution margin samples. changes away margin classiﬁer less critical classiﬁcation process none mentioned approaches provide robustness them. domain adversarial classiﬁcation concept drift initiated attacker intended subvert system unlabeled drift detection extremely helpful automated early warning system. ensemble based techniques proposed disagreement scores ensemble models signal changes data distribution. ensemble classiﬁers used perform email spam classiﬁcation. average pairwise mutual agreement classiﬁers ensemble used signal change drive retraining. however methodology also relies periodic checking using labeled samples ensure high agreement result undetected changes aﬀecting classiﬁers. recent work also indicates relationship drift classiﬁer agreement scores. feature bagging found eﬀective characterizing adversarial activity task malicious classiﬁcation. drifts caused classiﬁer agreements shift disproportionately towards center range instead concentrated peripheries. work concentrates empirical analysis eﬀect provides initial experimentation speciﬁc malware domain. visual inspection change presented. however harnessing signal change context streaming data explored. proposed margin density drift detection technique paper provides signal drift unlabeled data reliable manner. unlike implicit drift detection techniques proposed approach less susceptible raising false alarms. actively including learned classiﬁer’s information decision process approach able discern changes could adversely aﬀect classiﬁcation performance. such embodies beneﬁts classes drift detectors like explicit drift detectors detects drifts impact classiﬁcation results using unlabeled data saving labeling budget implicit drift detectors. approach bridges categories drift detectors providing ﬁrst kind domain independent cost-eﬀective model independent drift detection scheme reliably signaling change high dimensional data streams. ability generalize training dataset core classiﬁcation technique. generalization eﬀort leads regions space known margins classiﬁer uncertain tries present best guess based learned information. margin portion prediction space vulnerable misclassiﬁcation. intuition used existing works active learning develop labeling strategies based uncertainty data samples. uncertainty sampling technique query committee technique methodologies select samples based distance classiﬁcation boundary disagreement ensemble models given samples respectively. approaches explore informativeness margin samples task managing labeling budget eﬃciently. proposed margin density approach explores margin tracking unlabeled drift detection. change number samples margin indicative drift depicted figure distribution samples respect distances classiﬁer boundary shown ﬁxed margin. sudden increase decrease number samples within margin makes stationary assumption data suspect. classiﬁers deﬁne margin width acceptable mis-classiﬁcations training process avoid ﬁtting. such tracking sudden change margin characteristics indicate distribution changes. changes data distribution relative classiﬁcation boundary enable tracking posterior probability distribution space withusing labeled data. done tacitly involving classiﬁer detection process thereby making change detection process relevant task hand. using ﬁxed margin tracking density samples closest classiﬁcation boundary false alarms caused changes away boundary avoided seldom result performance degradation. idea margin intuitive probabilistic classiﬁers logistic regression linear support vector machines explicit notion uncertainty. however motivation behind usage general. classiﬁer’s boundary embodiment features deems important task hand. monitoring changes close boundary enables limit tracking important features only. unlabeled drift detection approaches section suﬀer false alarms diﬀerentiate changes features giving equal weights features. margin density approach tracks margin changes summarizes important features interaction resulting formation classiﬁer’s separating boundary. real world high dimensional datasets multiple sets features provide high classiﬁcation performance. robust classiﬁer hinge loss feature bagged ensemble utilize majority features evenly distributing weights among them create better generalization data classiﬁer model serves committee experts multiple independent perspectives data. change perspective cause increased disagreement consequent uncertainty predicted results. since relevant features provide good perspective data uncertainty indicative drift requires attention. robust classiﬁer functions self-contained self-monitoring prediction unit aware capabilities deviations. figure coupled classiﬁer leads monitoring scheme constantly monitoring other. coupled detection strategy extended high dimensional spaces provides unsupervised approach changes relevant features detected observing invariance relevant features. using idea margin density approach extended classiﬁers decision trees knearest neighbors provide explicit class labels probabilistic values using feature bagged ensemble. ensemble setup trains multiple base models diﬀerent subset features combines results. eﬀect distributing classiﬁcation importance weights diﬀerent features features averaged produce ﬁnal prediction. central idea behind eﬃcacy margin density approach lies ability track changes aﬀect important features only. margin density computed svms computed ensemble disagreements would perform similarly track lack consensus cost generalization paid learned model. margin density metric univariate measure tracked time detect drifts unlabeled data. margin density deﬁned deﬁnition margin density expected number data samples fall within robust classiﬁer’s region uncertainty i.e. margin. margin width /||w|| shown figure trained model expected number samples margin soft constraints. margin density given ratio samples fall inside margin equation signal function checks given sample falls within margin parameters unlabeled samples given distance hyperplane given |w.x distance threshold sign function produce ﬁnal class label distance hyperplane sample within margin signal function emits denoting contributes margin density. probabilistic classiﬁers logistic regression return probability class conﬁdence computed threshold θmargin used specify cutoﬀ uncertain samples. samples conﬁdence less θmargin contribute margin density. classiﬁers decision trees k-nearest neighbors return discrete class labels intuitive notion margin. models considered unstable reduce number features necessary build models. order make robust distribute weights across features used feature bagging ensemble technique. feature bagging improves generalization unexplicit notions margins linear kernel svm. also computed disagreement scores feature bagged ensembles. here term margin taken notation regions uncertainty robust classiﬁer classiﬁcation importance weights distributed among features. margin well deﬁned algorithm classiﬁers decision trees notion pseudo-margin given region space high disagreement based feature bagged ensemble. section presents former section presents latter scenario. term margin used refer region uncertainty cases notation. classiﬁer support vector machines logistic regression explicitly deﬁne margins setup soft-margin linearkernel ﬁnds optimal maximum width separating hyperplane classes allowing samples enter margin better generalization capability. made possible addition slack variables svm’s objective function allow separable cases robustness noisy samples. optimization function linear kernel soft margin given equation normal vector separating hyperplane given gives oﬀset origin class label sample regularization cost parameter controls misclassiﬁcation cost. stable classiﬁers training multiple base models classiﬁer diﬀerent subset features original d-dimensional data space random subspace implementation strategy feature bagging given algorithm entire feature space features divided randomly chosen subspaces features each classiﬁer trained subspaces. resulting ensemble classiﬁers majority voting used predict ﬁnal label given sample employing random subspace ensemble base classiﬁer type made robust margin density signal extracted algorithm models ensemble trained diﬀerent views feature space serve committee experts independent views prediction problem. increased disagreement models indicative high uncertainty sample. margin density type models computed measuring number samples high uncertainty given equation signal function checks sample certainty less θmargin equation below refers voted mean predicted class probabilities base estimators ensemble. unlabeled samples collected ratio samples critical uncertainty given margin density. ensemble comprised classiﬁer type making margin density approach applicable irrespective choice classiﬁcation algorithm used. understand behavior margin density metric eﬃcacy indicator drift evaluated synthetic dataset diﬀerent change scenarios. change scenario setup generating initial distribution samples used training model generating additional samples changed distribution testing model. change margin density evaluated difference margin densities training test data |mdtrain mdtest|. comparing changes training testing error representative metric used fully labeled drift detectors eﬀectiveness detect true drifts evaluated. similarly comparison traditional feature based unlabeled drift detection techniques evaluated checking hellinger distance distributions since ∆err within range values normalized range dividing values following experiments. margin density metric evaluated linear kernel random subspace ensemble orthogonal decision trees. experiments synthetic dataset classes using random subspace model depicted figure represents initial training distribution samples represent diﬀerent change situations. drift scenario represent changes directly aﬀects classiﬁcation boundary drift features. causes error increase shown table correspondingly changes average changes since hellinger distance computed unlabeled data independently learned classiﬁer values given scenario shown table change scenarios represent shift equal magnitude opposite direction. shift away margin shift towards margin shown figure models. scenarios result change indicating shortcomings traditional feature tracking approaches diﬀerentiating false alarms relevant changes. metric shows change scenario detects consistent error tracking approach. using ﬁxed margin tracking density changes away margin eﬀectively ignored rarely cause performance degradation. property margin density approach makes resistant false alarms compared margin based methods extreme data distribution shift seen figure representative drastic drift aﬀecting features simultaneously; situation rare real world applications. change occur away margin goes unnoticed metric. however tracked ∆err methods. changes rare real world operational environments eﬀectively caught specialized novelty detection methods developed cases hellinger distance metric attempt provide completeness drift detection leads excessive false alarms. eﬀect exacerbated high dimensional datasets many irrelevant features contribute classiﬁcation process. illustrated figure z-dimension useful prediction additional scenario presented indicate need tracking drop well rise margin density. scenario shown figure common case linear tightly packed class distributions causes initial margin density high. drift leads drop margin density indicated negative values table indicating need track absolute value margin density change |∆md|. contrast ∆err metric spike error rate hellinger distance considered relevant change detection. analysis section indicates ability change margin density signal drift detection. metric signals change relevant classiﬁcation process providing high robustness stray changes. therefore used surrogate labeled error tracking apconﬁrmation retraining. metric reduces need frequent conﬁrmation owing robustness toward irrelevant changes making labeling requested essentially retraining phase only. algorithm begins initial trained classiﬁer obtained learning initial labeled training dataset model made online. initial training dataset reference distribution summarizing margin performance characteristics dataset learned. reference distribution comprises expected margin density mdre acceptable deviation margin density metricσre expected accuracy training dataset accre deviation σacc. values learned training dataset using k-fold cross validation technique commonly used evaluating classiﬁers cross valiproach provide reliable drift detection. change irrelevant features high dimensional spaces regions away classiﬁer’s margin eﬀectively ﬁltered. eﬀect observed classiﬁers explicit margin using random subspace ensemble cases margin explicit. algorithm streaming data algorithm needs operate limited memory past information needs continuously process data indefinitely single pass provide quick response time. change margin density used metric detecting drift streaming environment. incremental classiﬁcation process continuously receives unlabeled samples predicts class labels based classiﬁcation model shown figure given time signal function computes sample lies within margin computation performed using equations based type model used. signal used update expected margin density. signiﬁcant change margin density time signals change requires inspection. following this next ntrain samples requested labeled external oracle. oracle entity provide true labels unlabeled sample given cost. performance ntrain labeled samples found degraded drift conﬁrmed model retrained using labeled samples collected. approach need continuous monitoring using labeled samples drift detection process unsupervised. labeling requested drift suspected dation method entire dataset sequentially divided bands samples. ﬁrst iteration ﬁrst bands used training dataset learn model band used test model. process repeated times band functions test exactly once. accuracy margin density values test sets considered results random experimentation. average values standard deviation test accuracy margin density iterations used form reference distribution. cross validation allows create population metric values better estimate expected values acceptable deviation. values used signal change based desired level sensitivity given parameter change signaled margin density time given deviates standard deviations reference margin density value mdre given sensitivity parameter used detect signiﬁcant drop performance obtained labeled samples reference accuracy values here labeledsamples ntrain samples requested labeled signiﬁcant drift suspected drop accuracy conﬁrms change indeed result concept drift model retraining necessary update classiﬁer retraining performed reference distribution learned labeledsamples based k-fold cross validation technique described above. allowing users specify intuitive parameter sensitivity suggested picked range entire change detection process made ﬂexible used diﬀerent streaming environments. larger value frequent signaling desired alternatively lower value could used critical applications small changes could harmful undetected. drift detection process around tracking margin density signal made incremental using moving average formulation equation here margin density time given computed incrementally using forgetting factor mdt− combining signal funcparameter forgetting actor stream computed specifying chunk inﬂuence parameter-n. computed formulation makes applicable stream monitoring system making incremental updates margin density metric. noted incremental formulation speciﬁed drift monitoring process only. irrespective stream classiﬁcation algorithm used could process data either incrementally chunk using sliding window. separating detection classiﬁcation schemes makes metric general implementation used diﬀerent classiﬁcation setups also enables controlled testing eﬃcacy detecting meaningful concept drifts. section presents experimental analysis results proposed approach sets experiments section presents results drift induced datasets better understand drift detection characteristics framework controlled environment; experiments real world drifting data presented section section demonstrate practicality. experimental comparisons fully labeled drift detection technique -the acctr approach based ewma unlabeled drift detection approach ofhdddm performed. variants approach evaluatedmd-svm uses linear kernel base model md-rs uses random subspace implementation margin density approach. details experimental methods setup presented section eﬀects varying margin width parameter θmargin varying detection model presented section fully labeled accuracy tracking model forms upper baseline drift handling mechanisms. data assumed labeled explicit tracking accuracy used signal change. unlabeled drift detection mechanism eﬀective performance close acctr approach. acctr approach illustrated figure every predicted sample’s correct label requested oracle accuracy checked signiﬁcantly deviated training accuracy. accuracy tracked incrementally using ewma formulation change tracking given equation using model approach uses based implementation linear kernel hinge loss used. margin density computed tracking number samples classiﬁer’s margin. using random subspace model proposed random subspace drift detection technique used. ensemble comprises decision trees features randomly picked feature space. margin density given number samples regions high uncertainty ensemble. threshold critical uncertainty chosen samples conﬁdence less considered margin. hellinger distance drift detection methodology approach obtained representative traditional unlabeled feature tracking approaches found literature track changes feature space. particular hdddm approach tracks average hellinger distance samples within distributions signals change distance increases beyond threshold. hellinger distance popular metric streaming data research comparison using enable highlight fundamental diﬀerences unlabeled approaches approach. ensure bias underlying classiﬁcation process aﬀect analysis detection scheme approaches implemented incremental manner using moving average formulation equation metric time depends signal function deﬁned based detection method used. acctr approach uses error signal. predicted label diﬀerent correct label signal otherwise approaches signal obtained margin inclusion test given equation md-svm equation md-rs methods. hdddm approach incremental nature. chunk based approach computing histograms data needs entire chunk data. modiﬁed approach chunk deﬁned incrementally sliding rate sample. given time chunk comprises latest samples chunk size. forgetting factor equation taken acctr approaches ensure equivalence drift initial stream assumed labeled. forms initial training classiﬁer model learned along reference distribution metrics reference distribution obtained fold cross validation training data described section acctr approaches. hdddm approach reference distribution acceptable deviation obtained using sliding window unlabeled samples slide rate samples. hellinger distance subsequent chunks sliding window formed population learning expected hellinger distance standard deviation. sensitivity chosen balance robustness reactivity drift detection methods. experiments performed using python scikit-learn machine learning library support vector machine linear kernel regularization constant chosen prediction model experiments. order ensure diﬀerences detection techniques md-svm md-rs result diﬀerent training capabilities classiﬁers task prediction detection separated md-rs approach. training dataset used train models linear model. while used provide prediction online classiﬁer model used solely purpose detection drift triggering retraining setup enables analyze drift detection properties approaches controlled environment blocking diﬀerent prediction behavior. section presents experimental evaluation approach static datasets induced concept drift controlled manner. controlling location nature drifts datasets better understanding drift detection capabilities diﬀerent approaches real world setting obtained. datasets chosen machine learning library preprocessed numeric binary values normalized range multi-class datasets reduced binary class problems data instances shuﬄed randomly remove unintended concept drifts already data. characteristics datasets shown table chunk size parameter shown table used process stream based number instances present datasets. drift induction process explained next followed experimental results analysis drift induced data. drift induction process provides include single concept drift static datasets particular location data stream. allows controlled drift analysis time retaining properties real world applications dataset derived. dataset ﬁrst shuﬄed remove unwanted concept drift prepare drift induction process. drift induction process induces feature drift dataset point stream called changepoint. drift induced randomly picking subset features rotating values particular class. example feature picked class label changepoint instances belonging class features shufﬂed basic approach ensures feature drifts induced also maintaining original data properties dataset. approach however dependent features selected rotation provides erratic results ’right’ features selected. drift induction approach proposed here extends basic idea allows greater control nature change. instead randomly picking features rotated pick features based importance classiﬁcation task. done ranking features based information gain metric selecting features form bottom list based nature change desired. sets experiments performed here detectability experiments choose features ranked list false alarm experiments choose bottom done test detection capabilities robustness irrelevant changes respectively. ranked features high impact classiﬁcation task ranked based information content modifying features results model degradation necessary detected ﬁxed. modifying bottom features less impact classiﬁcation process results false alarms ignored detectors. eﬀect changing bottom features datasets shown table datasets changepoint induced stream. model trained data changepoint tested samples after point. similarity accuracy model training original test indicates initial static dataset. detectability experiments features rotated results signiﬁcant drop test accuracy after changepoint seen table indicates true drifts need detected drift detection algorithm. rotating bottom features false alarm experiments show signiﬁcant drop test accuracy. although number features rotated cases features diﬀerent levels relevance comes classiﬁcation task. perform experimental analysis bottom datasets analyze behavior algorithms diﬀerent change conditions. nochange acctr md-svm md-rs hdddm methodologies analyzed detectability false alarm experiments. number drifts detected false alarms raised accuracy stream reported table drift detected signiﬁcant change metric being tracked. results requesting labeled samples conﬁrm deviation leads drop accuracy. false alarm result change signal obtaining labeled samples found signiﬁcant eﬀect classiﬁcation performance. case false alarm reported retraining classiﬁer takes place labeled samples discarded. detectability experiments exactly true drift induced causes accuracy drop midpoint stream. case false alarm experiments induced change aﬀect performance exactly relevant change introduced experiments. detectability experiment causes model’s performance drop time evident accuracy nochange model seen table indicates need drift detection methodology deal induced drift. acctr approach directly monitors classiﬁcation performance labeled samples serves gold standard detecting drifts experiments. approach detect exactly drift cases indicating robustness false alarms. acctr hdddm md-svm md-rs techniques able detect atleast drift experiments able reach similar ﬁnal accuracies. md-rs md-svm unsupervised methods still able reach accuracy similar fully labeled acctr approach indicates ability approaches used instead labeled approaches without signiﬁcantly compromising prediction performance. resistance false alarms shown number drifts detected false alarm table changes experiments cause signiﬁcant performance degradation. such drift detected lead retraining classiﬁer resulting false alarm. acctr approaches resistant changes hdddm approach signals relevant change needs further inspection. hdddm diﬀerentiate between change features change bottom features classiﬁer agnostic technique relies solely tracking changes feature values distribution. hdddm approach causes higher false alarm rate approaches experiments. another observation md-svm md-rs show similar behavior average deviation accuracy exact number drifts detected. shows generic applicability margin density signal irrespective implementation technique used compute nochange acctr md-svm md-rs hdddm nochange acctr md-svm md-rs hdddm nochange acctr md-svm md-rs hdddm nochange acctr md-svm md-rs hdddm nochange acctr md-svm md-rs hdddm nochange acctr md-svm md-rs hdddm shown figure changepoint significant drop accuracy seen cases. drift detection approaches swift recognizing change retraining able provide high prediction performance. nochange approach detect drift performance continues degrade case. approaches accuracy trajectories close acctr detector indicating surrogate fully labeled approach. accuracy higher cases hdddm approach. approach false alarm musk dataset only hdddm approach seen trigger changes multiple times course stream. changes frequent often occur withcorrelation change accuracy values. approaches detect drifts close proximity acctr approach seen minimal green diamonds orange blue diamonds. md-rs approach robust change causes detect drifts delay figure however drift detection approaches close changepoint illustrating ability detect true drift eﬀectively. progression accuracy margin density metrics time detectability experiment shown figure drop accuracy seen changepoint. accompanied signiﬁcant spike md-rs metric. metric md-svm shows either signiﬁcant spike drop margin density thereby justifying need compute absolute deviation signal change. also margin density metric similar accuracy metric shows high signal-to-noise ratio. stays stable changepoint retraining performed signiﬁcant deviation drift occurs. conﬁrms drift detection result figure accuracy time nochange acctr md-svm md-rs hdddm approach detectability experiments. true drifts detected shown diamonds squares represent false alarms. machine learning models deployed real world applications operate dynamic environment concept drift occur time. drifts plausible fact expected rampant domain cybersecurity attackers constantly trying generate data degrades classiﬁer. section real world concept drift datasets chosen domain cybersecurity presented table datasets high dimensional popularly used machine learning literature test online binary classiﬁcation models concept drifting environment. spam spamassassin datasets taken represent task separating malicious spam email legitimate ones. phishing contains data malicious pages nsl-kdd dataset derived task intrusion detection systems ﬁlters malicious network traﬃc. datasets preprocessed converting feature numeric/binary types only normalizing feature value range ﬁnal data characteristics shown table datasets exhibit concept drift exact nature location drifts known advance. nochange acctr md-svm md-rs hdddm methodologies evaluated datasets. metrics used evaluation accuracy stream number drifts signaled number false alarms total percentage samples requested labeled. accuracy determines predictive performance online system. number drifts signaled indicates sensitivity change. false alarms occur drift signaled upon obtaining labeled samples found performance signiﬁcantly degraded thus requiring retraining. since location number drifts known advance false alarm refers cases lead retraining models causing figure accuracy margin density margin density time detectability experiments. drifts detected denoted diamonds circles denote retraining point. requested labels wasted. false alarms case acctr approach approach directly tracks drop accuracy unlike unlabeled techniques. false alarms refers situation equation triggered suspected drift upon receiving labeled samples conﬁrm retraining needed accuracy degraded signiﬁcantly. labeling% indicates cost expended methodology directly related number false alarms every alarm leads requesting chunk sizen samples labeled. high accuracy high drift detection false alarm labeling% desirable. cases accuracy nochange approach signiﬁcantly lower drift detection techniques observed table conﬁrms drifting nature datasets need drift detection. accuracy obtained margin density methodologies close fully labeled acctr approach md-svm average deviation md-rs deviation only. indicates ability techniques detect drift good labeled drift detection mechanism. labeling requirement approaches less acctr approach relies totally labeled stream performing computation. hdddm approach average performs poorly compared approaches also needs labeling. result high false alarm rate. false alarms harmful causes system react every change data distribution noise otherwise making adaptiveness hassle rather solution. especially domain streaming cybersecurity applications overly responsive system serious problem vulnerable malicious manipulation training process. also labeling time consuming expensive task. system frequently requires manual intervention less likely trusted cause experts disregard warnings. hdddm approach needed labeled samples approaches average increased false alarm rate. approach hand signals change would affect system performance directly making suitable self-guided automatic monitoring system drift detection malicious activity models. progression accuracy datasets shown figure seen methodologies acctr approach converge behave similarly time. indicates could used replacement fully labeled drift detection approach. methodology accuracy drifts signaled nochange acctr md-svm md-rs hdddm nochange acctr md-svm md-rs hdddm nochange acctr md-svm md-rs hdddm nochange acctr md-svm md-rs hdddm similarity md-svm mdrs approaches terms accuracy progression number drifts/false alarms detected indicates general applicability margin density metric drift indicator metric. margin density metric provides good performance irrespective type machine learning technique implemented making classiﬁcation algorithm independent. hdddm approach performs worse methods spam nsl-kdd datasets. attributed delay drift detection. datasets number false alarms high hdddm approach translates increased labeling expenditure shown table approaches signal false alarm spam spamassassin datasets. spam dataset md-svm md-rs approaches signal change timestamps respectively reported false alarms. however withbasis small drop accuracy points albeit enough warrant concept drift recovery. margin density metric sensitive changes accuracy still robust compared feature tracking approaches seem signal changes without correlation accuracy degradation. machine learning based cybersecurity systems beneﬁt increased reliability experiments detection model taken linear regularization parameter md-svm technique random subspace ensemble decision trees base models md-rs technique. moreover margin width kept ﬁxed based intuition. eﬀects varying settings detection capabilities framework presented section. real world datasets section evaluated using following additional detection models linear linear logistic regression l-penalty random subspace ensemble logistic regression models random subspace ensemble multinomial naive bayes models. results experiments presented figure models evaluated using scikit-learn machine learning library figure accuracy time nochange acctr md-svm md-rs hdddm approach real world concept drift datasets. true drifts detected shown diamonds squares represent false alarms. experimental results varying detection models presented figure observed varying underlying model signiﬁcant eﬀect detection capabilities methodology. friedman’s parametric test ﬁnal accuracy values datasets showed statistically signiﬁcant diﬀerence performance diﬀerent models p-value number drifts detected relative position detection also observed close majority cases. results inline intuition section postulate margin density signal could general model independent indicator change could applied robust classiﬁer model distributes classiﬁcation importance weights among features. models experiments described robust classiﬁers distribute feature weights perform similarly framework. results applying methodology non-robust classiﬁer shown table logistic regression model lpenalty used. margin logistic regression model deﬁned described section probability obtained posterior estimates classiﬁer. classiﬁer tends minimize number features used ﬁnal models making unsuitable methodology. results table show model detect drifts phishing dataset also performs signiﬁcantly worse spam spamassassin dataset. because l-penalty model tends minimize number features used violates central premise coupled features detection model relies however seen figure l-penalty based logistic regression model used random subspace ensemble eﬀective usage framework. methodology ability models explicit non-explicit margins therecase md-rs approach concept margin deﬁned margin width parameter θmargin taken experiments thus far. experiments previous section demonstrate varying underlying detection model significant eﬀect detection capabilities framework. section evaluate eﬀect varying parameter θmargin md-rs model results varying θmargin shown figure seen choice parameter θmargin signiﬁcant eﬀect ﬁnal results accuracy plots follow similar trajectory time. failure case seen case θmargin=. phishing dataset gray). margin width samples captured insuﬃcient detect drift eﬀectively. margin density signal depicted figure phishing nsl-kdd dataset. seen that phishing dataset θmargin=. signal fails detect drift. margin values although absolute signal magnitude different eﬀective detecting change nearly location. attributed reference distribution learning component algorithm learns expected margin density cross validation training dataset. subsequent changes tracked relative reference distribution making eﬀective even margin width changes. maintain robustness change ensure drift detection eﬀective θmargin range suggested. tuning done based desired level sensitivity required application. methodology evaluated popular real world data streams electricity market dataset covertype dataset widely used datasets benchmark test concept drift handling systems electricity market dataset represents pricing data collected south wales australia ﬂuctuates based supply demand components market gama goncalves covertype dataset consists forest cover data multi-class dataset bifet dataset reduced binary class prediction problem considering class labels datasets preprocessed converting features numeric values normalized range chunk size chosen evaluation datasets. datasets unknown type location drift serve real world benchmarks evaluating concept drift techniques. results table show although datasets unknown concept drift beneﬁt drift handling accuracy nochange approach worse techniques approaches found similar accuracy fully labeled acctr approach approach also seen signal fewer drifts hdddm approach leading half labeling budget case case covtype dataset resulting accuracy average. ability margin density signal eﬀectively ignore changes unlabeled data affect classiﬁcation performance makes usage attractive unsupervised drift indicator. because unlike feature based change detectors like hdddm margin density approach implicitly includes model drift detection process. unlabeled drift detection techniques developed literature described section also incorporate notion margin. however techniques diﬀer approach signal tracked. technique tracks margin density signal expected number samples uncertain regions classiﬁer. margin based techniques section essentially track average change uncertainty samples time. diﬀerence paradigms subtle ability specify ﬁxed margin deployment responsible eﬃcacy margin density approach. elucidate diﬀerence paradigms implications synthetic experiment designed section. order understand eﬀects diﬀerent margin based techniques experiment similar section performed. synthetic dimensional dataset generated. dataset samples concept drift occurring midpoint characteristics data shown table dataset dimensions ﬁrst made irrelevant classiﬁcation task assigning distributions classes. midpoint drift induced dataset changing class feature distribution. feature distributions incrementally changed upto feature changing mean values. remaining features relevant classiﬁcation task eﬀect changing gradually analyzed experiment. eﬀects changes evaluated training model ﬁrst samples evaluating change metrics remaining samples drift. chose random subspace ensemble decision trees base models experiments here. eﬀects changing feature values synthetic -dimensional dataset presented table change training testing error margin density metric average uncertainty hellinger distance compared. changing feature signiﬁcant eﬀect classiﬁcation error features irrelevant classiﬁer’s perspective. metric owing model agnostic measurements unable distinction change seen metric table margin density technique uncertainty tracking techniques robust changes. advantage technique traditional uncertainty tracking techniques seen case changes relevant features robust classiﬁer random subspace ensemble inherently capable providing high predictive performance even relevant features drift. unless majority features change time robust classiﬁer unaﬀected changes features. traditional uncertainty tracking techniques fail distinguish critical changes changes inherently managed robust classiﬁer. tracking posterior probability estimates classiﬁer models uncertainty based techniques could eﬀectively ignore changes irrelevant data still lead additional false alarms compared margin density approach. approach limits critical area uncertainty monitored limits tracking critical drifts only. seen table metric aﬀected features drift simultaneously opposed features uncertainty tracking techniques. actual fully labeled technique table results change detection metrics ∆err ∆uncertain varying intensities drift synthetic data. features irrelevant classiﬁcation. bold entries represent ﬁrst indication change metrics. would signal drift features aﬀected. signal comes closest tracking actual drift using unlabeled data high robustness false alarms. margin density approach robust changes irrelevant features also changes relevant features critical robust classiﬁer’s performance. uncertain metrics presented here representations paradigms unlabeled drift detection techniques literature. metric represents fundamental behavior feature distribution tracking techniques uncertainty metric represents behavior model based posterior probabilities tracking techniques although actual usage implementation metrics nuanced diﬀerent literature purpose demonstrate underlying principle makes margin density signal robust false alarms. purpose section provide motivation future work improving reliability unlabeled drift detection techniques. generation analysis real world datasets similar characteristics synthetic data presented here could also useful contribution better enable research paper margin density drift detection methodology presented capable reliably detecting concept drift unlabeled streaming data. proposed methodology uses number samples classiﬁer’s region uncertainty metric detecting drift. signiﬁcant deviation margin density metric used signal need labeled samples subsequent veriﬁcation retraining classiﬁer. shown independent classiﬁcation algorithm used robust stray changes data distribution reliable substitute supervised drift detectors. furthermore development incremental drift detection algorithm makes amenable usage variety data stream environments. experimental analysis performed drift induced datasets real world concept drift datasets cybersecurity domain benchmark concept drift datasets. results indicated high detection rate prediction performance coupled false alarm rate approach. margin density approach tested drift induced datasets resulted diﬀerence accuracy average compared fully labeled drift detection methodology. tested real world cybersecurity datasets average diﬀerence indicating eﬃcacy used surrogate fully labeled approaches. compared state unlabeled feature tracking approach hellinger distance drift detection methodology algorithm resulted fewer false alarms smaller labeling percentage accuracy. additionally experimentation using classiﬁers explicit margins without demonstrated signiﬁcant diﬀerences indicating generality margin density approach model independent signal change. evaluation approach cybersecurity datasets domains spam detection phishing websites network intrusion detection demonstrates applicability proposed work detection adversarial activity. approach general domain independent applicability particularly highlight beneﬁts adversarial domains envision immediately beneﬁt reduced number false alarms increased reliability change detection. approach used less labeling supervised drift detection approaches producing statistically equivalent classiﬁcation performance. also approach outperformed unsupervised hdddm approach reducing false alarm rate average. research adversarial drift handling systems utilize approach develop unsupervised dynamic machines capable constantly changing subvert adversarial activity. approach saves labeling expenditure signaling change lead drop predictive performance classiﬁer. future work concentrate label eﬃcient techniques relearning drift detected. using margin density information along active learning strategies selectively label samples conﬁrmation retraining phase warrants research. would allow additional savings terms reduced labeling online classiﬁcation process.", "year": 2017}