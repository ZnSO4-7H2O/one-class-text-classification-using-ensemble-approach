{"title": "Lifelong Metric Learning", "tag": ["cs.LG", "cs.AI", "I.5.2"], "abstract": "The state-of-the-art online learning approaches are only capable of learning the metric for predefined tasks. In this paper, we consider lifelong learning problem to mimic \"human learning\", i.e., endowing a new capability to the learned metric for a new task from new online samples and incorporating previous experiences and knowledge. Therefore, we propose a new metric learning framework: lifelong metric learning (LML), which only utilizes the data of the new task to train the metric model while preserving the original capabilities. More specifically, the proposed LML maintains a common subspace for all learned metrics, named lifelong dictionary, transfers knowledge from the common subspace to each new metric task with task-specific idiosyncrasy, and redefines the common subspace over time to maximize performance across all metric tasks. For model optimization, we apply online passive aggressive optimization algorithm to solve the proposed LML framework, where the lifelong dictionary and task-specific partition are optimized alternatively and consecutively. Finally, we evaluate our approach by analyzing several multi-task metric learning datasets. Extensive experimental results demonstrate effectiveness and efficiency of the proposed framework.", "text": "similarities datasets different types speakers adapting speciﬁcs particular users. therefore speech recognition library delivered coming speaker’s recognition default speech recognition capabilities speaker-speciﬁc metric models need added. another motivating example image classiﬁcation system metric learning system identify whether image contains apple banana however user wishes expand ability task e.g. detecting orange. achieve goal state-of-the-arts storage training data tasks retrain models time consuming way. therefore challenge lies learn accumulate knowledge continuously early samples accessible online scenario. depicted fig. paper propose framework called lifelong metric learning intends learn shared metric parameters ones without degrading performance accessing training data tasks. based assumption tasks retained low-dimensional common subspace learns library called lifelong dictionary shared basis metric models learned model tasks considered sparse combination discriminative lifelong dictionary. speciﬁcally lifelong dictionary initialized extracting efﬁciently ﬁrst training task different regions clustering. task arrives transfers knowledge shared base lifelong dictionary learn metric model sparsity regularization reﬁnes lifelong dictionary ﬁrstorder information task previous tasks. updating lifelong dictionary continuously fresh knowledge incorporated existing lifelong dictionary thereby improving performance previously learned models. therefore model +-th task obtained without accessing previous training data. evaluate framework state-of-the-art multi-task metric learning methods several datasets. experimental results validate encouraging performances proposed framework. contributions paper include best knowledge ﬁrst work online metric learning perspective lifelong learning adopts previous experience knowledge tasks incorporate learn task improve performance classiﬁcation accuracy reduce training time accordingly. support discriminative lifelong dictionary proposed lifelong metric learning framework model task sparse combination reduce storage burden without saving training data previous tasks ﬁrst-order information. abstract—the state-of-the-art online learning approaches capable learning metric predeﬁned tasks. paper consider lifelong learning problem mimic human learning i.e. endowing capability learned metric task online samples incorporating previous experiences knowledge. therefore propose metric learning framework lifelong metric learning utilizes data task train metric model preserving original capabilities. speciﬁcally proposed maintains common subspace learned metrics named lifelong dictionary transfers knowledge common subspace metric task task-speciﬁc idiosyncrasy redeﬁnes common subspace time maximize performance across metric tasks. model optimization apply online passive aggressive optimization algorithm solve proposed framework lifelong dictionary task-speciﬁc partition optimized alternatively consecutively. finally evaluate approach analyzing several multi-task metric learning datasets. extensive experimental results demonstrate effectiveness efﬁciency proposed framework. markable success variety applications data mining information retrieval computer vision mainly high efﬁciency scalability large-scale dataset. different conventional batch learning methods learn metric model ofﬂine training samples online learning aims exploit group samples time update metric model iteratively ideally appropriate tasks data arrives sequentially. however state-of-the online metric learning models achieve online learning ﬁxed predeﬁned metric tasks cannot task. paper consider lifelong learning problem mimic human learning i.e. extend current metric tasks current functionality metric remains. example speech recognition different people pronouncing word differs greatly based gender accent nationality individual leverage characteristics state laboratory robotics shenyang institute automation chinese academy sciences university chinese academy sciences china e-mail sungansia.cn fig. demonstration difference lifelong metric learning traditional multi-task metric learning lifelong metric learning. knowledge learned library transferred metric task knowledge task used update lifelong dictionary traditional metric learning utilizes training data update knowledge library. different shapes colors denote different metric base weights respectively. rest paper structured follows. section gives brief review related works. section introduces proposed lifelong metric learning formulation. section proposes solve proposed model efﬁciently online passive aggressive optimization algorithm. section report experimental results conclude paper section metric learning related methods long history. depending whether metric learning incorporates multi-task learning metric learning roughly categorized single metric learning multi-task metric learning. best knowledge seeking better distance metric learning training dataset issue state-of-the-art single metric learning models distance metric based researches representative approaches categorized issues batch metric learning online metric learning. batch metric learning models divided categories models based nearest neighbors optimizes expected leave-one-out error stochastic nearest classiﬁer projection space proposes widelyused mahalanohis distance learning large margin nearest neighbors learning mahalanobis distance metric classiﬁcation labeled training examples; models based pairs/triplets instance searches clustering puts similar pairs clusters dissimilar pairs different clusters; promotes input sparsity imposing group sparsity penalty learned metric trace constraint encourage output sparsity; proposes novel low-rank metric learning algorithm yield bilinear similarity functions applicable highdimensional data domains. however batch metric learning models assume training samples available prior learning phase cannot applied many practical applications fact small amount training samples available beginning others would come sequentially. therefore researchers focus online metric learning intend train classiﬁer coming data. online metric learning designs online algorithm scalable image similarity learning learning pairwise similarity fast scales linearly number objects number non-zero features. however oasis suffer over-ﬁtting difﬁcult applied case high dimensions. furthermore computational complexity learning full-rank metric ranging metric learner lies high-dimensional sample space dimension training dataset. order overcome over-ﬁtting problem omllr proposes novel online metric learning model rank constraint low-rank metric enables reduce storage metric matrices. incorporates large-scale high-dimensional dataset sparse online metric learning explore application image retrieval. addition loreta describes iterative online learning procedure consisting gradient step followed second-order retraction back manifold. incorporate beneﬁts online learning mahalanobis distance lego using log-det regularization instance loss guaranteed yield positive semideﬁnite matrix. furthermore details also found surveys based assumption relationships information shared among different tasks taken account multi-task learning aims improve generalization performance learning multiple related tasks simultaneously. furthermore multi-task metric learning methods designed make metric learning beneﬁt training tasks simultaneously. assumption multiple tasks share common mahalanobis metric task task-speciﬁc metric mtlmnn adopts lmnn formulation multi-task learning. however mtlmnn computationally complicated especially case high dimensions. speciﬁcally parameters optimized. based low-rank based assumptions presents transformation matrix problem multi-task metric learning learning common subspace tasks individual metric task individual metric restricted common subspace. addition mtscml constructs common basis multi-metric regularized relevant across tasks however storage computation become cumbersome large scale tasks. therefore order address situation total number tasks large task coming consecutively employ common subspace lifelong dictionary build robust lifelong metric learning framework. notations matrix rm×n entry i-th j-th column deﬁne norms number nonzero entries |wij| maxij |wij| -norm ∞-norm respectively. denote elementwise sign positive part elementwise absolute value matrix respectively. elementwise multiplication. assume related tasks. denotes training t-th task {xti dimension number traint= total number samples total number tasks similarity distance metric t-th learning tasks. assumed deﬁned based linear transformation obtain dimensional representation) fig. demonstration formulation given t-th task represented series atoms lifelong dictionary corresponding weights element ij-th element respectively. original intention multi-task metric learning learn appropriate distance metric t-th task utitraining lizing suppose loss involved t-th task determined distance function pairs appearing arbitrary loss function t-th task. however learning metric task without accessing previously used training data considered traditional multi-task metric learning. context multi-task metric learning lifelong metric learning system encounters series metric learning tasks task deﬁned convenience assume learner knows information tasks e.g. total number tasks distribution tasks etc. time step lifelong system receives batch training data metric learning task either metric task previously learning task system asked make predictions samples previous task. goal establish task models that feature vectors xtij xtj. must positive semi-deﬁnite satisfy properties similarity distance metric. triplets used deﬁne side-information denote similar dissimilar pairs respectively. example implies similar data pairs stay closer dissimilar pairs depending similarity distance metric without specially specifying similarity distance function denoted following. order model correlation among different metric tasks assume metric matrix t-th task represented using combination shared common subspace knowledge repository. moreover motivated theorem gives detail mathematical description. theorem denotes similarity distance deﬁned transformation matrix exists dimensional subspace spanned orthonormal basis ptd} metric matrix deﬁned rd×d therefore metric matrix t-th task theorem explicitly decomposed low-dimensional metric part subspace part lifelong metric learning framework simply represented learn individual metric task common subspace furthermore shown fig. parameter matrix metric task expressed optimization steps optimized ﬁxing wt’s another wt’s optimized holding ﬁxed. however shown approach inefﬁcient inapplicable lifelong learning many tasks data samples. order optimize problem recompute value wt’s address problem approximate applying online passive aggressive optimization strategy i.e. approximated large samples online learning small samples ofﬂine minibatch learning t-th task. moreover optimization problem also roughly divided subproblems alternating direction optimization strategy. initializing lifelong dictionary ﬁrst subproblem compute optimal coming task second subproblem update lifelong dictionary ﬁxing wt’s. rd×d denotes weight matrix. therefore metric task represented linear combination lifelong dictionary composed lilj∀i generally since diagonal elements represents selfcorrelation transformed feature off-diagonal element represents correlation among different transformed features diagonal elements dense offdiagonal elements. encourage off-diagonal elements wt’s sparse order ensure learned metric model captures maximal reusable chunk knowledge. given training data task optimize metrics minimize loss function tasks encouraging metrics share common knowledge lifelong dictionary. therefore framework formulated |wtij| used convex approximation true matrix sparsity frobenius norm matrix avoid overﬁtting. trade-off parameter controls regularization wtoﬀ task-speciﬁc matrices wt’s become selfcorrelation diagonal matrices. deﬁnition ﬁnal optimization problem lifelong metric learning formulated section provides detail procedure optimize proposed framework. since problem convex respect wt’s jointly objective function arrive local optimum. common approach computing local optimum objective functions alternately perform convex high-quality lifelong dictionary plays important role model. order generate discriminative basis vectors ﬁrst divide data different clusters. clutter select nearest neighbors class apply fisher denotes elementwise multiplication. notice fista amounts using sequences search points. moreover proximal method solving summarized algorithm solve given tt’s wt’s order evaluate lifelong dictionary modify formulation remove minimization besides also remove second term used keep similarity distance matrix close current one. further accomplish exploiting sideinformation learned tasks. following adopt gradient descent method solve gradient respect calculated different singletasklearner function using side-information framework summarized algorithm singletasklearner learned using base metric models. problem update system begins base metric learning models compute assume step complexity triplets number paper. next update requires solving instance lasso i.e. ||woﬀ. iteration problem begins computation gradient computational complexity therefore cost achieving \u0001-accuracy determined convergence property accelerated gradient method i.e. wtoﬀ. non-smooth nature propose proximal gradient method fast global convergence rate solve optimization problem. speciﬁcally proximal operator oﬀ-norm applied solve subproblem experiments compare framework single metric learning models multi-task metric learning models. single metric learning model includes euclidean distance standard euclidean distance feature space; oasis classical online metric learning model given table iteration number paper; lmnn large margin nearest neighbor classiﬁcation learns mahalanobis distance k-nearest neighbor classiﬁcation; scml-global simply combine local basis elements higher-rank global metric; lmnn-union lmnn metric obtained union training data tasks scmlunion scml metric obtained union training data metric tasks. multi-task scml multi-task metric learning model considers learned metrics expressed combinations basis subset though different weights task. i-th labeled training samples t-th task known loss function. speciﬁcally ella maintains sparsely shared basis vector regression logistic task models transfers knowledge basis learn t-th task. models implemented matlab codes available supplement website. notice parameters models tuned selected -fold cross validation. although model allows different weights task throughout paper adjust parameters section brieﬂy review learning method related proposed learning algorithm. perhaps relevant work context multi-task metric learning frames metric learning learning sparse combination locally discriminative metrics generated training data clustering. however motivation scml signiﬁcantly different scml aims cast metric learning learning sparse combination basis elements taken basis {bi}k bi’s ˆd-dimensional column vectors. instead ﬁxing metric task number focuses transfer knowledge preciously learned tasks metric task using shared basis i.e. lifelong task learning. scml metric matrix represented using basis matrices induced -norm constraint formulation scml achieve batch learning. encourages communication among different basis elements oﬀ-norm constraint resulting formulation integrate online sample learning adopting sinlgetasklearner online metric learning. furthermore also conducted extensive experiments effect oﬀ-norm experiment section. optimization algorithm scml local solution training samples. proposed algorithm learn metric task without accessing historical data gradient information previous training data adopted next iteration section carry empirical comparisons state-of-the-art single multi-task metric learning models. ﬁrst give learning lifelong metric learning framework table different mtxj belong class different according whether label consistent categorize real datasets different scenarios labelconsistent label-inconsistent. following demonstrate effectiveness proposed framework different datasets. label-consistent datasets label shared metric tasks roughly categorized metric task different metric tasks label set. therefore depending whether task adopt datasets paper. shown table sentiment consists amazon reviews four product types randomly split dataset training validation testing sets. isolet dataset popular dataset multi-task learning consists disjoint subjects called isolet randomly split task dataset training validation testing sets. moreover basis number stscml sentiment isolet respectively. compared competing methods proposed framework outperforms state-of-the-arts average error achieving improvement term classiﬁcation error using sentiment isolet datasets veriﬁes effectiveness framework lifelong learning manner. furthermore performance framework also better existing lifelong learning model fact adopt lifelong dictionary framework keeps learning step-by-steps. real datasets table also shows comparison time consumption framework single multi-task metric models. framework efﬁcient state-of-the-arts need retrain previous tasks. however framework little slower multi-task metric model isolet faster lmnn. high dimensional transformed features. similarity metric function outperforms distance metric function sentiment dataset implies similarity metric important different tasks; meanwhile distance metric outperforms similarity metric isolet dataset implies distance metric important metric task. fig. effect -norm. horizontal vertical axes index task classiﬁcation error isolet dataset respectively. addition line black line corresponding standard deviation stscml ours respectively. consists gray-scale images digits automatically. features grayscale values. split classiﬁcation problem tasks{ respectively. therefore number classes task randomly selected samples training samples remaining test. presented result table notice performance framework leads second best average error worse best mtscml outperforms state-of-the-arts gap. train model using data corresponding task instead mtscml adopting data tasks together model training. efﬁcient mtscml shown table subsection conduct comparisons proposed lifelong metric learning formulation study learned task impact generalization performance. effect woﬀ-norm regularization order study woﬀ-norm regularization affect performance single metric task compare stscml method proposed framework isolet dataset. speciﬁcally remove regularization term i.e. employ fista efﬁciently optimize convex problem. also randomly split task isolet dataset training validation testing sets performance presented fig. general model outperforms stscml single learned task expect task isolet. observation verities correlation information among different transformed features enables improve learning efﬁcacy effectiveness woﬀ-norm. dimension transformed features affect performance framework term classiﬁcation error. speciﬁcally also randomly split dataset training validation testing sets. varying number transformed feature present performance shown fig. notice average classiﬁcation error changes different number transformed features veriﬁes metric task embedded low-dimensional subspace namely lifelong dictionary framework. addition error average classiﬁcation minimum i.e. classiﬁcation error decreasing increase size larger size redundant feature information involved lifelong dictionary subsection also adopt sentiment dataset study number learned tasks affect classiﬁcation performance framework. splitting dataset training validation testing sets sequence learned tasks books electronics kitchen; present classiﬁcation performance fig. obviously metric task imposed step-by-step error decreased i.e. performance framework improved gradually justiﬁes fig. effect dimension transformed features horizontal vertical axes dimension transformed features classiﬁcation error sentiment dataset respectively. framework accumulate knowledge continuously achieve lifelong learning like human learning. addition performance early learned tasks improved obviously succeeding task i.e. early tasks beneﬁt accumulated knowledge. paper study metric task original metric system without retraining whole system time consuming state-of-the-art online metric learning models. speciﬁcally propose lifelong metric learning framework learns lifelong dictionary shared basis metric models based assumption metric tasks retained low-dimensional common subspace. metric task mensink verbeek perronnin csurka distance-based image classiﬁcation generalizing classes near-zero cost ieee transactions pattern analysis machine intelligence vol. s.-f. chang semi-supervised distance metric learning collaborative image retrieval clustering transactions multimedia computing communications applications vol. shalit weinshall chechik online learning embedded manifold low-rank matrices journal machine learning research vol. kulis metric learning survey foundations trends kumar learning task grouping overlap multitask learning international conference machine learning obozinski taskar jordan joint covariate selection joint subspace selection multiple classiﬁcation problems statistics computing vol. fig. effect number learned tasks. horizontal vertical axes number learned tasks classiﬁcation error task respectively. initial classiﬁcation error task achieved using sinlge metric learning. notice average error decreased increasing number tasks arrives transfer knowledge shared lifelong dictionary learn coming metric model sparsity regularization redeﬁne basis metrics knowledge metric task. converting convex problem subproblems online passive aggressive optimization adopt proximal gradient method solve proposed framework. extensive experiments carried several multi-task datasets verify proposed framework well suited lifelong learning problem exhibit prominent performance effectiveness efﬁciency. baghshah shouraki semi-supervised metric learning using pairwise constraints. international joint conference artiﬁcial intelligence vol. citeseer cong yuan self-supervised online metric learning rank constraint scene categorization ieee transactions image processing vol. parameswaran weinberger large margin multi-task metric learning advances neural information processing systems", "year": 2017}