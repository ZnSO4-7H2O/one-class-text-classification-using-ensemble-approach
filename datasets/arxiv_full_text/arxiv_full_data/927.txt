{"title": "Layer-wise learning of deep generative models", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "When using deep, multi-layered architectures to build generative models of data, it is difficult to train all layers at once. We propose a layer-wise training procedure admitting a performance guarantee compared to the global optimum. It is based on an optimistic proxy of future performance, the best latent marginal. We interpret auto-encoders in this setting as generative models, by showing that they train a lower bound of this criterion. We test the new learning procedure against a state of the art method (stacked RBMs), and find it to improve performance. Both theory and experiments highlight the importance, when training deep architectures, of using an inference model (from data to hidden variables) richer than the generative model (from hidden variables to data).", "text": "using deep multi-layered architectures build generative models data difficult train layers once. propose layer-wise training procedure admitting performance guarantee compared global optimum. based optimistic proxy future performance best latent marginal. interpret autoencoders setting generative models showing train lower bound criterion. test learning procedure state method find improve performance. theory experiments highlight importance training deep architectures using inference model richer generative model deep architectures multiple-layer neural networks recently object interest shown provide state-of-the-art performance many problems aspect deep learning help learning better representations data thus reducing need hand-crafted features time-consuming process requiring expert knowledge. difficulty training whole deep network once socalled layer-wise procedure used approximation however long-standing issue justification layer-wise training although method shown merits practice theoretical justifications fall somewhat short expectations. frequently cited result proof adding layers increases so-called variational lower bound log-likelihood model therefore adding layers improve performance. reflect validity layer-wise training procedures discuss assumptions construed equivalent non-layer-wise whole-network training. leads approach training deep generative models using criterion optimizing layer starting bottom transferring problem upwards next layer. right conditions first step section re-introduce general form deep generative models derive gradient log-likelihood deep models. gradient seldom ever considered considered intractable requires sampling complex distributions. hence need simpler layer-wise training procedure. show optimistic criterion upper bound used train optimal lower layers provided subsequent training upper layers successful discuss criterion transfer learning problem upper layers. leads discussion relation procedure stacked restricted boltzmann machines auto-encoders justification found auto-encoders optimizing lower part deep generative model. section spell theoretical advantages using model hidden variable form ùëûùëÉdata looking hidden-variable generative models data scheme close auto-encoders. finally discuss applications perform experiments validate approach compare state-of-the-art methods deep datasets synthetic real. particular introduce auto-encoders rich inference auto-encoders modified according framework. indeed theory experiments strongly suggest that using stacked auto-associators similar deep architectures inference part much richer model generative part fact rich possible. using richer inference helps find much better parameters given generative model. back basic formulation training deep architecture traditional learning problem optimizing parameters whole architecture seen probabilistic generative model data. time interested step decomposition. thus simplicity consider distribution interest observed variable latent variable results extend layers decomposition renaming variables. target distribution latent variables training reproduce target distribution recursively layers till reaches layer which hopefully simple probabilistic generative model used. indeed often done practice except objective replaced surrogate objective. instance architectures made stacked rbms level likelihood single maximized ignoring fact used part deep architecture moreover often using approximation likelihood contrastive divergence specific conditions shown adding layer improves lower bound performance address section following questions possible compute estimate optimal value parameters bottom layer without training whole model? possible compare values without training whole model? latter would particularly convenient hyper-parameter selection would allow compare lowerlayer models upper layers trained thus significantly reducing size hyper-parameter search space exponential linear number layers. propose procedure aimed reaching global optimum layer-wise fashion based optimistic estimate log-likelihood best latent marginal upper bound. study theoretical guarantees section section make experimental comparison stacked rbms auto-encoders modified according scheme vanilla auto-encoders simple deep datasets. present training procedure works successively layer. first train together conditional model latent variable knowing data. step involves bottom part model thus often tractable. allows infer target distribution upper layers trained. procedure theorem states possible obtain validation parameter bottom layer optimal provided rest training goes well. namely target distribution realized well approximated value parameters layers obtained using rich enough conditional model guaranteed globally optimal. theorem suppose parameters bottom layer trained note layers fail approximate perfectly loss performance depends observed difference ùëÉ^ùúÉùêΩ unknown global optimum beware that unfortunately bound relies perfect layer-wise training bottom layer i.e. optimum criterion optimized possible conditional distributions otherwise priori valid. practice supremum always taken restricted conditional distributions rather possible distributions thus theorem idealized version practice still suggests clear strategy separate deep optimization problem subproblems solved sequentially auto-encoders shown implement approximation procedure terms kept scheme designed mind situation upper layers progessively simpler. indeed layer wide layer theorem strongly suggests decoupling inference generative models using rich conditional model contrary e.g. common practice auto-encoders. indeed experiments section confirm using expressive yields improved values importantly used auxiliary prop solving optimization problem part final generative model using richer reach better value simply changing larger model. thus using richer inference model pose much risk overfitting regularization properties model come mainly choice generative model family criterion proposed particular relevance representation learning goal learn generative model learn useful representation data. setting training upper layer model becomes irrelevant interested generative model itself. matters representation learning lower layer optimal model left unspecified. proceed steps proof theorem occasion introduce concepts used later experimental setting. evaluate parameter bottom layer without training whole architecture optimistic assume layers able produce probability distribution gives best results used together leads following. definition value bottom layer parameters. best latent marginal probability distribution maximizing log-likelihood attempts prevent auto-encoders learning identity often result even constrained inference model e.g. tied weights sparsity constraints hidden representation. upper bound least upper bound loglikelihood deep generative model dataset used bottom layer. upper bound actual performance subsequent training suboptimal best latent marginal ÀÜùëÑùúÉùêº representable model training converge best solution. note concave typical situations unique‚Äîexcept degenerate cases values define proposition criterion used theorem training bottom layer coincides upper bound argument distribution form look disappointing choose particular form? section show writing distributions conditional distribution help maximize data log-likelihood quantifiably incorporating information data moreover bound loss performance upper layers match crucially relies properties ÀÜùëûùíü. practical argument using optimizing full distribution hidden variable time difficult optimizing whole network whereas deep architectures currently already train model knowing knowing time. remark theorem hold necessary optimize possible conditional probability distributions seen proof enough optimize family every distribution represented definition global optimum parameters whole model obviously value model since argmax taken restricted set. then turn definition ÀÜùúÉùêº. implies model model maximizes model give bound loss performance case training upper layers fails reproduce blm. complete proof theorem make special optimality property distributions form namely proposition whose proof postponed section importantly bound valid perfectly optimized possible conditional distributions thus used blindly performance bound since heuristics always used find therefore limited practical relevance. practice real loss larger bound optimized smaller smaller comparing upper bound optimistic assessment. definition optimum distribution better ùê∑kl. consequently difference performance smaller difference performance difference kullback‚Äìleibler divergences appearing proposition. given probability obtained taking according taking according generally equal however equal proposition exactly property thus thanks proposition uses single parameters represent distribution pairs similarly approach stacked rbms trained greedy layer-wise fashion starts training distribution bottom approximate distribution distributions learned jointly using single parameters final generative model full used layer provide distribution bottom rbms used generation knowing thus contrast approach trained maximize least upper bound likelihood full deep generative model trained maximize likelihood single rbm. procedure shown equivalent maximizing likelihood deep generative model infinitely many layers weights tied latter interpreted assumption future value unknown learning first layer. such srbms make different assumption future made respect this comparison gradient ascents instructive gradient ascent training bottom takes form reminiscent gradient ascent global generative model dependency upper layers ignored instead distribution tied uses single parameter both. adding layer trained initialization upside version current deep model still matches special infinite deep generative model tied weights mentioned above. starting training upper layer initialization guarantees layer increase likelihood however result known hold layers; layers known adding layers increases bound likelihood approach perspective different. training lower layers consider best possible model hidden variable. errors bound occur approximation optimization training model likelihood associated optimal upper model expected decrease time actually take another lower layer account layer errors approximation optimization occur final likelihood training smaller upper bound. since introduction deep neural networks auto-encoders considered credible alternative stacked rbms shown almost identical performance several tasks auto-encoders trained stacking auto-associators trained backpropagation. namely start three-layer network trained backpropagation reproduce data; provides conditional distributions turn another auto-associator trained three-layer network reproduce distribution etc. note auto-encoder trained deep generative model incomplete lacks generative model distribution deepest hidden variable auto-encoder provide. possibility learn layer completes generative model. concerning theoretical soundness stacking auto-associators training deep generative models known training auto-associators approximation training rbms largest term expansion log-likelihood kept sense srbm stacked auto-associator training approximate approach gives understanding auto-encoders lower part deep generative model trained maximize lower bound follows. taking likelihoods dataset corresponds criterion maximized auto-associators considered probabilistic perspective. since moreover optimizing general optimizing particular class conclude criterion optimized auto-associators lower bound criterion proposed theorem keeping justified assume inference approximation inverse generative process soon thus assumption criteria close theorem provides justification auto-encoder training case. hand assumption strong implies shared different instance observations cannot come underlying latent variable random choice. depending situation might unrealistic. still using training criterion might perform well even assumption fully satisfied. show imposing layer-wise consistency constraint stacked training leads training criterion used auto-encoders tied weights. material already appears fairness training auto-associators backpropagation probabilistic terms consists maximization output function neural network. perspective hidden variable considered random variable intermediate value form here introduce intermediate random variable criterion training backpropagation done sampling instead using activation value practice sample significantly affect performance. also equal distribution auto-encoder tied weights. layer-wise consistency criterion rbms coincides tied-weights auto-encoder training approximation practice auto-encoders retain terms thus stacked training tied-weight auto-encoder training seen approximations layer-wise consistent optimization problem using full distribution using nonetheless clear whether layer-wise consistency always desirable property. srbm training replacing distribution obtained data seemingly breaks layer-wise consistency time always improves data log-likelihood non-layer-wise consistent training procedures fine-tuning layers trained would improve performance. layer-wise consistent procedures require well case upper layers match target distribution approach presented section used recursively train deep generative models several layers using criterion irrecoverable losses incurred step first optimization problem imperfectly solved second layer trained using assumption upper layers able subsequent upper layer training match blm. consequently parameters used layer optimal respect other. suggests using fine-tuning procedure. case auto-encoders fine-tuning done backpropagation layers shown improve performance several contexts confirms expected gain performance recovering earlier approximation losses. principle limit number layers auto-encoder could trained backpropagation practice training many layers results difficult optimization problem many local minima. layer-wise training seen dealing issue local minima providing solution close good optimum. optimum reached global fine-tuning. fine-tuning described framework follows finetuning maximization upper bound layers considered single complex layer case autoencoders approximation used help optimization explained above. data incorporation properties clear interesting work conditional distribution define distribution rather working directly distribution first answer practical optimizing distribution simultaneously optimizing global network hand currently used deep architectures provide time. second answer mathematical defined dataset thus working concentrate correspondence full distribution either hopefully correspondence easier describe. dataset provide rather directly crafting distribution distribution automatically incorporates aspects data distribution even simple hopefully better; formalize argument. candidate distribution. might build better reflecting data namely defines distribution distribution turn defines conditional distribution knowing standard general ùëÑcondùíü coincide original distribution definition former involves data whereas arbitrary. show operation always improvement ùëÑcondùíü always yields better data log-likelihood proposition data incorporation sending distribution ùëÑcondùíü defined fixed. following properties data incorporation always increases data log-likelihood best latent marginal ÀÜùëÑùúÉùêº fixed point transformation. precisely distributions fixed points data incorporation exactly critical points data log-likelihood critical points maxima value). particular uniquely defined unique) fixed point data incorporation. putting observation several times data.) hidden variable algorithm parameter optimizes distribution itself. particular keep fixed. distributions define distribution pairs extends distribution ùëõ-tuples observations proves last statement proposition. mentioned above implies first general properties first statement proven best latent marginal ÀÜùëÑùúÉùêº fixed point data incorporation otherwise would even better distribution thus contradicting definition blm. point left prove equivalence critical points log-likelihood fixed points ùëÑcondùíü simple instance maximization constraints follows. critical points data loglikelihood log-likelihood change first order replaced small constraint elementary linear since since along probability distribution sums finds defined namely definition ùëÑcond condition exactly means ùëÑcondùíü hence equivalence intractability log-likelihood deep networks makes direct comparison several methods difficult general. often evaluation done using latent variables features classification task direct visual comparison samples generated model instead introduce datasets simple enough true loglikelihood computed explicitly complex enough relevant deep learning. introduce deep datasets dimension. order datasets give reasonable picture happens general case first make sure relevant deep learning using following approach comparison equal number parameters justified main hypotheses deep learning namely deep architectures capable representing functions compactly shallow architectures hyper-parameters taken account hyper-parameter random search hidden layers sizes learning rate number epochs. corresponding priors given table order give obvious head start deep networks possible layer sizes chosen maximum number parameters single deep network close possible. cmnist dataset cmnist dataset low-dimensional variation mnist dataset containing samples dimension full dataset split training validation test sets samples each. dataset obtained taking image center mnist sample using values probabilities. first samples cmnist dataset shown figure parameter hidden layer size deep hidden layer size deep hidden layer size inference hidden layer size learn rate learn rate epochs epochs init independent bernoulli model pixel given independent bernoulli probability. model trained training set. log-likelihood validation nats sample. dataset dataset based idea learning invariance amount liquid several containers teapot teacups. contains distinct samples distributed training validation test samples each. dataset consists images left part image represents teapot size right part image represents teacups size liquid represented ones always lies bottom container. total amount liquid always equal capacity teapot i.e. always ones zeros given sample. first samples dataset shown figure figure checking cmnist deep log-likelihood validation dataset rbms srbm deep detworks selected hyper-parameter random search function number parameters dim. figure checking deep log-likelihood validation dataset rbms srbm deep networks selected hyper-parameter random search function number parameters dim. first application approach training deep generative model using auto-associators. propose train lower layers using auto-associators generative layer model. compare three kinds deep architectures standard auto-encoders auto-encoders rich inference suggested framework also comparison stacked restricted boltzmann machines models used study final generative model class comparison focuses training procedure equal ground. srbms considered state ‚Äîalthough performance increased using richer models focus model layer-wise training procedure given model class. ideal circumstances would compared log-likelihood obtained training algorithm optimum deep learning procedure full gradient ascent procedure instead since ideal deep learning procedure intractable srbms serve reference. without risking overfit loss generalization power part final generative model used tool optimization generative model parameters. would suggest complexity could greatly increased positive consequences performance model. aeries exploit possibility having layer modified autoassociator hidden layers instead generative part equivalent regular auto-associator inference part greater representational power includes hidden layer models deep architecture depth stacked rbms made ordinary rbms. aeries vanilla lower part made single auto-associator generative part rbm. aeries vanilla lower part model trained using usual backpropagation algorithm cross-entropy loss performs gradient ascent probability trained maximize competitiveness model evaluated comparison log-likelihood validation distinct training set. comparisons made given identical number parameters generative model. model given equal chance find good optimum terms number evaluations hyper-parameter selection procedure random search. implementing training procedure proposed section several approximations needed. important compared theorem distribution really trained possible conditional distributions knowing next training upper layers course fail reproduce perfectly. moreover auto-associators approximation study effect approximations. stacked rbms. comparisons stacked rbms trained using procedure used random search hyper-parameters sizes hidden layers learning rate number epochs. because consider generative models obtained never taken account number parameters auto-encoder srbm. however parameters taken account necessary part generative model. vanilla auto-encoders. general training algorithm vanilla autoencoders depicted figure first auto-associator trained maximize adaptation upper bound auto-associators presented maximization procedure done backpropagation algorithm cross-entropy loss. inference weights tied generative weights wgen often case practice. ordinary used generative model layer. deep generative auto-encoders trained using random search hyper-parameters. deep generative auto-encoders layer hyper-parameters stacked rbms also backpropagation learning rate epochs init auto-encoders rich inference model training scheme aeries represented figure vanilla auto-encoders backpropagation algorithm cross-entropy loss maximize auto-encoder version upper bound training set. weights tied course make sense auto-associator different models pareto front composed models subsumed models according number parameters expected log-likelihood. model said subsumed another strictly parameters worse likelihood. figure pareto fronts average validation log-likelihood number parameters srbms deep generative auto-encoders modified deep generative auto-encoders dataset. srbms vanilla aeries perform better single seen evidence cmnist deep datasets. among deep models vanilla auto-encoders achieve lowest performance outperform single rbms significantly validates generative models also deep generative models. compared srbms vanilla auto-encoders achieve almost identical performance algorithm clearly suffers local optima instances perform poorly handful achieve performance comparable srbms aeries. auto-encoders rich inference able outperform single rbms vanilla auto-encoders also stacked rbms consistently. validates general deep learning procedure section arguably also understanding auto-encoders framework. seem surprising. considering average log-likelihood validation even sample validation happens given probability model average log-likelihood arbitrarily low. fact roundoff errors computation log-likelihood models measured performance affect comparison models affects instances performance already low. figure pareto fronts average validation log-likelihood number parameters srbms deep generative auto-encoders modified deep generative auto-encoders cmnist dataset. rich-inference auto-encoders. insist rich-inference autoencoders vanilla auto-encoders optimize exactly generative models structure thus facing exactly optimization problem clearly modified training procedure yields improved values generative parameter seen section upper bound least upper bound log-likelihood deep generative models using given lower part model. raises question whether good indicator final performance setting approximations w.r.t. need discussed. another point intractability upper bound models many hidden variables leads propose test estimator section though experiments considered small enough need unless otherwise specified. look turn upper bound applied log-likelihood estimation hyper-parameter selection‚Äîwhich considered part training procedure. first discuss various possible effects measuring empirically. consider maximization practice perform specific maximization obtain rely training procedure maximize thus resulting training procedure generally globally optimal theorem experiments course upper bound value resulting actual training. first question validity approximation upper bound obtained maximization possible course untractable. learned inference distribution used practice approximation reasons first model cover possible conditional distributions second training imperfect. effect ÀÜùí∞ùíüùëû lower bound upper bound ÀÜùí∞ùíüùëû second question relationship upper bound final log-likelihood model. bound optimistic tight upper part model manages reproduce perfectly. check tight practical applications upper layer model imperfect. addition estimate training final performance validation test sets might different. performance model validation generally lower training set. hand situation specific regularizing effect imperfect training layers. indeed refers universal optimization possible distributions might therefore overfit more hugging training closely. thus manage reproduce perfectly training could well decrease performance validation set. hand training layers approximate within model class introduces regularization could relationship empirical upper bound used training final log-likelihood real data results several effects going directions. might affect whether empirical upper bound really used predict future performance given bottom layer setting. context deep architectures hyper-parameter selection difficult problem. involve much hyper-parameters relevant conditionally others make matters worse evaluating generative performance models often intractable. evaluation usually done w.r.t. classification performance sometimes complemented visual comparison samples model rare instances variational approximation log-likelihood considered methods consider evaluating models layers fully trained. however since training deep architectures done layer-wise fashion criterion greedily maximized step would seem reasonable perform layer-wise evaluation. would advantage reducing size hyper-parameter search space exponential linear number layers. propose first evaluate performance lower layer trained according upper bound validation dataset ùíüvalid. measure performance obtained used part larger hyper-parameter selection evaluating layer less problematic definition layer always shallow model true likelihood becomes easily tractable. instance although rbms well known intractable partition function prevents evaluation several methods able compute close approximations true likelihood dataset evaluated perform hyper-parameter selection upper layer evaluating true likelihood validation data samples transformed inference distribution model layer. hyper-parameter selection used experiments simply used hyper-parameter random search. mentioned earlier context representation learning layer irrelevant objective train generative model better representation data. assumption good latent variables make good representations suggests upper bound used directly select best possible lower layers. present series tests check whether selection lower layers higher values actually results higher log-likelihood final deep generative models assess quantitative importance approximations discussed earlier. training algorithm comparison done using models trained hyper-parameters selected random search before. empirical upper bound computed using above. this could lead stopping criterion training model arbitrarily many layers upper layer compare likelihood best upper-model best possible next layer. next layer significatively higher likelihood upper-model another layer would help achieve better performance. training upper bound training log-likelihood. first compare value empirical upper bound ÀÜùí∞ùúÉùêº training actual log-likelihood trained model training set. evaluation optimistic given dataset checking closely training upper layers manages match target distribution also occasion check effect using resulting actual learning instead best possible conditional distributions. results given figures srbms aeries. empirical upper bound good predictor future log-likelihood full model training set. shows approximations w.r.t. optimality layer universality dealt practice. aeries models performance poor estimation upper bound presumably approximation learning affect model selection procedures concerns models performance discarded. test intuition compare upper bound bottom layer log-likelihood obtained shallow architecture layer; difference would give indication much could gained adding layers. figures compare expected log-likelihood training rbms previously trained upper bound generative model using first layer. results contrast previous ones confirm final performance upper bound model enough layers. alignment figures therefore seen confirmation training upper bound validation log-likelihood. compare training upper bound log-likelihood validation distinct training tests whether obtained training good indication final performance bottom layer parameter. regularization using training upper bound predict performance validation could optimistic therefore expect validation log-likelihood somewhat lower training upper bound. results reported figures confirm training upper bound validation log-likelihood. regularization cmnist dataset samples generalization difficult optimal training used fact almost optimal validation too. dataset picture somewhat different training upper-bound validation log-likelihood. attributed increased importance regularization dataset training contains samples. although training upper bound therefore considered good predictor validation log-likelihood still monotonous function validation log-likelihood still used comparing parameter settings hyper-parameter selection. cases examined predictivity obtained training final performance validation dataset. seen training imperfect predictor performance notably lack regularization optimistic assumption inference distribution maximized training set. effects easily predicted feeding validation inference part model hyper-parameter selection follows. call validation upper bound upper bound obtained using validation dataset instead note values still obtained training training dataset. parallels validation step auto-encoders which course reconstruction performance validation dataset done feeding dataset network. better overall approximation validation log-likelihood seems indicate performing hyper-parameter selection validation upper bound better account generalization regularization. experimental setting considered small enough allow exact computation various bounds summing possible states hidden variable however exact computation upper bound using ÀÜùí∞ùíüùëû always possible number terms exponential dimension hidden layer assess accuracy approximation take compare values ÀÜÀÜùí∞ùíüùëû ÀÜùí∞ùíüùëû cmnist training datasets. results reported figures three models superimposed showing good agreement. layer-wise approach propose train deep generative models based optimistic criterion upper bound suppose learning successful upper layers model. provided optimism justified posteriori good enough model found upper layers resulting deep generative model provably close optimal. optimism justified provide explicit bound loss performance. framework training deep generative models highlights importance using richer models performing inference contrary current practice. consistent intuition much harder guess underlying structure looking data derive data hidden structure known. possibility tested empirically auto-encoders rich inference completed top-rbm create deep generative models able outperform current state different deep datasets. upper bound also found good layer-wise proxy evaluate log-likelihood future models given lower layer setting relevant means hyper-parameter selection. opens avenues research instance design algorithms learn features lower part model possibility consider feature extraction partial deep generative model upper part model temporarily left unspecified.", "year": 2012}