{"title": "Auxiliary Objectives for Neural Error Detection Models", "tag": ["cs.CL", "cs.LG", "cs.NE", "I.2.7; I.2.6; I.5.1"], "abstract": "We investigate the utility of different auxiliary objectives and training strategies within a neural sequence labeling approach to error detection in learner writing. Auxiliary costs provide the model with additional linguistic information, allowing it to learn general-purpose compositional features that can then be exploited for other objectives. Our experiments show that a joint learning approach trained with parallel labels on in-domain data improves performance over the previous best error detection system. While the resulting model has the same number of parameters, the additional objectives allow it to be optimised more efficiently and achieve better performance.", "text": "investigate utility different auxiliary objectives training strategies within neural sequence labeling approach error detection learner writing. auxiliary costs provide model additional linguistic information allowing learn general-purpose compositional features exploited objectives. experiments show joint learning approach trained parallel labels in-domain data improves performance previous best error detection system. resulting model number parameters additional objectives allow optimised efﬁciently achieve better performance. automatic error detection systems learner writing need identify various types error text ranging incorrect uses function words articles prepositions semantic anomalies content words adjective– noun combinations. tackle scarcity errorannotated training data previous work investigated utility automatically generated ungrammatical data well explored learning native well-formed data work investigate utility supplementing error detection frameworks additional linguistic information extracted available error-annotated learner data. construct neural sequence labeling system error detection allows learn better representations language composition detect errors context accurately. addition predicting binary error labels experiment also predicting additional information token including token frequency speciﬁc error type extracted existing data well part-of-speech tags dependency relations generated automatically using readily available toolkits. auxiliary objectives provide sequence labeling model additional linguistic information allowing learn useful compositional features exploited error detection. seen type multi-task learning model learns better compositional features shared representations related tasks. common approaches multitask learning require randomly switching different tasks datasets demonstrate joint learning approach trained in-domain data parallel labels substantially improves error detection performance different datasets. addition auxiliary labels required training process resulting better model number parameters. following sections describe approach task systematically compare informativeness various auxiliary loss functions investigate alternative training strategies examine effect additional training data. addition scarcity errors training data recent research highlighted variability manual correction writing errors re-annotation conll shared task test annotators demonstrated even humans great difﬁculty agreeing correct writalso make character-level extension described token separated individual characters mapped character embeddings. using bidirectional lstm hidden feedforward component character vectors composed characterbased token representation. finally dynamic gating function used combine representation regular token embedding taking advantage approaches. component allows model capture useful morphological character-based patterns addition learning individual token-level vectors common tokens. model section learns assign error labels tokens based manual annotation available training data. however nearly limitless ways making writing errors learning explicitly hand-annotated examples feasible. addition writing errors sparse leaving system little useful training data learning error patterns. order train models generalise well limited training examples would want encourage learn generic patterns language grammar syntax composition exploited error detection. multi-task learning allows models learn multiple objectives shared representations using information related tasks boost performance tasks limited target data. example plank explored option using word frequency auxiliary loss function part-of-speech tagging. describe semi-supervised framework multi-task learning integrating language modeling additional objective. following work adapt auxiliary objectives task error detection experiing errors given challenges all-errors correction task previous research demonstrated detection models detect errors systems focusing correction therefore provide extensive feedback learner. following yannakoudakis treat error detection sequence labeling task token input sentence assigned label indicating whether correct incorrect given current context construct bidirectional recurrent neural network detecting writing errors. model given sequence tokens input mapped sequence distributed word embeddings embeddings given input bidirectional lstm moving sentence directions. step lstm calculates hidden representation based current token embedding hidden state previous step. network includes tanh-activated feedforward layer using hidden states lstms input allowing model learn complex higher-level features. combining hidden states directions able vector represents speciﬁc token also takes account context sides layer calculates label predictions based layer softmax activation function used output normalised probability distribution possible labels token predicted probability t-th value label correct otherwise; weight task since main goal develop accurate error detection models allows control much model depends n-th auxiliary task. example setting value means updates n-th task times less importance. tune speciﬁc weight task trying values choosing ones achieved highest result development data. main goal system classify tokens correct incorrect objective included conﬁgurations. addition experiment number auxiliary loss objectives required training frequency plank propose using word frequency additional objective tagging since words certain tags likely belong speciﬁc frequency groups. frequency token training corpus discretized int) used auxiliary label. ment larger possible objectives. instead predicting correctness token context extend system predict additional information labels every token. information auxiliary objectives propagated weights model during training without requiring extra labels testing time. common neural approaches multi-task learning switch randomly different tasks datasets joint learning approach trained in-domain data only. lower parts model function similarly system described section token representations ﬁrst passed bidirectional lstm order build context-speciﬁc representations. that separate objective assigned individual hidden layer weight matrices speciﬁc n-th task. recurrent components shared objectives hidden layers allow parts model customised speciﬁc task learning higher-level features controlling information forwardbackward-moving lstms combined. contains ﬁne-grained labels error. example training different labels individual error types missing determiners incorrect verb forms. giving model access labels system learn ﬁne-grained error patterns based individual error types. ﬁrst language previous work experimentally demonstrated distribution writing errors depends ﬁrst language learner investigate usefulness auxiliary objective training. part-of-speech tagging wellestablished sequence labeling task requiring model disambiguate word types based contexts. rasp parser automatically generate labels training data include additional objectives. grammatical relations include auxiliary objective type grammatical relation current token dependent order incentivise model learn semantic composition. rasp parser unlexicalised therefore suitable learner data spelling grammatical errors common. hidden output layers. however components required training process; testing time removed resulting model architecture number parameters baseline difference parameters optimised. yannakoudakis investigate number compositional architectures error detection present state-of-the-art results using bidirectional lstm. follow experimental setup investigate impact auxiliary loss functions datasets first certiﬁcate english dataset conll- shared task test contains texts written non-native learners english response exam prompts eliciting free-text answers. texts manually annotated error types error spans professional examiners yannakoudakis convert binary correct/incorrect token-level labeling error detection. missing-word errors error label assigned next word sequence. released version contains sentences training sentences development sentences testing. development randomly sampled training data test contains texts different examination year. conll- test contains texts annotated experts. compared texts technical written higherproﬁciency learners. order make results comparable yannakoudakis also evaluate models conll test annotations train models public dataset. corresponds fce-public model treats conll- dataset out-of-domain test corpus. following conll- shared task also report main evaluation metric. however shared task focused correction calculated error spans using multiple annotations evaluate token-level error detection performance. following recommendations chodorow also report counts predicted correct tokens. pre-processing texts lowercased digits replaced zeros tokenlevel representations although character-based component access original version token. tokens occur mapped single token used represent previously unseen tokens testing. word embeddings size initialised publicly available wordvec embeddings trained google news. lstm hidden layers size task-speciﬁc hidden layers size tanh activation. model optimised using adadelta training stopped based error detection score development set. implement proposed framework using theano make code publicly available online. nakoudakis trained data. main system experiments bi-directional lstm error detection model character-based representations described section model test effect performance adding auxiliary loss functions described section training objective. auxiliary frequency loss improves performance tagging however error detection objective help. certain tags likely belong speciﬁc frequency classes less reason believe word frequency provides useful error detection. similar drop performance observed auxiliary loss involving ﬁrst language learner. likely system learns speciﬁc types features identiﬁcation auxiliary task directly useful error detection. investigating different architectures incorporating auxiliary task avenue future work. types auxiliary loss function gives absolute improvement test set. baseline differentiates correct incorrect tokens auxiliary loss allows system learn feature detectors specialised individual error types thereby also making features available binary error detection component. inclusion tags gives consistent improvements basic conﬁguration. tasks require system understand token behaves senlearn highertence quality compositional representations. architecture able predict tags type based context features identify irregular sequences error detection. added advantage loss functions ﬁne-grained error types automatically generated require additional manual annotation. know ﬁrst time automatically generated labels explored objectives multi-task sequence labeling setting. finally evaluate combination system integrating auxiliary loss functions performed best development set. combination architecture includes four different loss functions main binary incorrect/correct label ﬁne-grained error type type. left frequency lowered performance development set. resulting system achieves absolute improvement baseline without auxiliary loss functions absolute improvement current state-of-the-art error detection system yannakoudakis trained dataset. table contains evaluations conll- shared task annotations. word frequency nearly effect whereas ﬁne-grained error labels lead roughly absolute improvement basic system. inclusion tags auxiliary objective consistently leads highest also improve performance main system overall contribution less compared test explained different writing style conll data. effectiveness approach implement alternative multi-task learning strategies error detection. experiments make three established sequence labeling datasets manually annotated different tasks conll dataset chunking containing sections wall street journal annotated different labels. conll- dataset identiﬁed bingel søgaard useful additional training resource multi-task setting; conll- dataset similar label density error detection task; corpus chosen tags gave consistently good performance error detection development test sets demonstrated previous section. ﬁrst setting datasets used train sequence labeling model respective tasks resulting model used initialise network training error detection system. common preload word embeddings different model strategy extends idea compositional components network. results table show performance error detection model without pre-training. slight improvement pre-training model conll dataset increase considerably smaller compared results section main advantages multi-task learning regularisation actively encouraging model learn general-purpose features something exploited setting since training happens separate stages. second experiments explore possibility training second domain task time error detection. similar collobert weston randomly sample sentence datasets update model parameters speciﬁc task. alternating tasks model able retain regularisation beneﬁts. however shown table type training improve error detection performance. possible explanation domain writing style auxiliary datasets different learner writing corpus model ends optimising unnecessary direction. including alternative labels dataset section model able extract information domain-relevant training data thereby achieve better results. main beneﬁts multi-task learning expected scenarios available taskspeciﬁc training data limited. however also investigate effect auxiliary objectives training substantially larger training set. speciﬁcally follow yannakoudakis also experimented augmenting publicly available datasets training data large proprietary corpus. total train large model tokens cambridge learner corpus corpus learner english lang- corpus model architecture yannakoudakis adding auxiliary objective predicting automatically generated successful additional objective based development experiments. table contains results evaluating model trained large training set. test data auxiliary objective provide improvement model performance comparable results yannakoudakis since large training comes quite similar dataset likely available training data sufﬁcient auxiliary objective offer additional beneﬁt. however considerable improvements conll test sets absolute improvements corresponding benchmarks. small amounts training data similar conll dataset including auxiliary objective provided robust model delivers better performance different writing styles. error detection early error detection systems based manually constructed error grammars mal-rules recent approaches exploited errorannotated learner corpora primarily treated task classiﬁcation problem vectors contextual lexical syntactic features extracted ﬁxed window around target token. work focused error-type speciﬁc detection models particular models detecting preposition article errors among frequent ones non-native english learner writing maximum entropy models along rule-based ﬁlters account substantial proportion utilized techniques. error detection models also integral component essay scoring systems writing instruction tools helping shared task error detection correction focused different errors though systems type speciﬁc targeted closed-class errors. following year shared task focused correcting preposition determiner errors recent conll shared tasks focused error correction rather detection conll- targeted correcting noun number verb form subjectverb agreement errors addition preposition determiner errors made non-native learners english whereas conll- expanded correction errors regardless type. core components systems across conll correction shared tasks include average perceptrons error correction priors naive bayes models joint inference capturing interactions errors well phrase-based statistical machine translation hypothesis incorrect source sentences translated correct target sentences work closely related yannakoudakis investigate number compositional architectures error detection propose framework based bidirectional lstms. work used system architecture baseline compared model results sections showed multi-task learning improve performance allow model generalise better. multi-task learning multi-task learning ﬁrst proposed caruana since applied many language processing tasks neural network architectures. example collobert weston constructed convolutional architecture shared weights between tasks tagging chunking. whereas model shared word embeddings approach focuses learning better compositional features shared bidirectional lstm. luong explored multi-task architecture sequence-to-sequence learning encoders decoders different languages trained jointly using semantic representation space. klerke used tracking measurements secondary task order improve model sentence compression. bingel søgaard explored beneﬁcial task relationships training multitask models different datasets. architectures trained randomly switching different tasks updating parameters based corresponding dataset. contrast treat alternative tasks auxiliary objectives dataset beneﬁcial error detection research using auxiliary training objectives context tasks. cheng described system detecting out-of-vocabulary names also predicting next word sequence. plank predicted frequency word together showed improve tagging accuracy low-frequency words. however ﬁrst explore auxiliary objectives described section context error detection. described method integrating auxiliary loss functions neural sequence labeling framework order improve error detection learner writing. predicting binary error labels model also learns predict additional linguistic information token allowing discover compositional features exploited error detection. performed systematic comparison possible auxiliary labels either available existing annotations generated automatically. experiments showed tags grammatical relations error types gave largest beneﬁt error detection combining together improved results further. compared training method multi-task approaches learning sequence labeling models related tasks using initialise error detection model; training multiple tasks datasets randomly switching them. methods outperformed proposed approach using auxiliary labels dataset latter beneﬁt regularising model different task also keeping training data in-domain. main beneﬁts multi-task learning expected scenarios available taskspeciﬁc training data limited found error detection beneﬁts additional labels even large training sets. successful error detection systems learn language composition introducing additional objective encourages model train general composition functions better word representations. error detection model also learns predict automatically generated tags achieved improved performance conll- benchmarks. useful direction future work would investigate dynamic weighting strategies auxiliary objectives allow network initially beneﬁt various available labels specialise performing main task.", "year": 2017}