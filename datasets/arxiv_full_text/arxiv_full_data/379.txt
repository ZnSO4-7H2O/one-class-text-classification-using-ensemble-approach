{"title": "ZhuSuan: A Library for Bayesian Deep Learning", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.NE", "stat.CO"], "abstract": "In this paper we introduce ZhuSuan, a python probabilistic programming library for Bayesian deep learning, which conjoins the complimentary advantages of Bayesian methods and deep learning. ZhuSuan is built upon Tensorflow. Unlike existing deep learning libraries, which are mainly designed for deterministic neural networks and supervised tasks, ZhuSuan is featured for its deep root into Bayesian inference, thus supporting various kinds of probabilistic models, including both the traditional hierarchical Bayesian models and recent deep generative models. We use running examples to illustrate the probabilistic programming on ZhuSuan, including Bayesian logistic regression, variational auto-encoders, deep sigmoid belief networks and Bayesian recurrent neural networks.", "text": "jiaxin jianfei chen shengyang yucen yihong yuhao zhou department computer science technology tnlist cbicr center state intelligent technology systems department electronic engineering tsinghua university beijing china shijxmails.tsinghua.edu.cn chenjianmails.tsinghua.edu.cn dcszjtsinghua.edu.cn ssycs.toronto.edu luoycmails.tsinghua.edu.cn gyhmails.tsinghua.edu.cn zhouyhmails.tsinghua.edu.cn paper introduce zhusuan python probabilistic programming library bayesian deep learning conjoins complimentary advantages bayesian methods deep learning. zhusuan built upon tensorﬂow. unlike existing deep learning libraries mainly designed deterministic neural networks supervised tasks zhusuan featured deep root bayesian inference thus supporting various kinds probabilistic models including traditional hierarchical bayesian models recent deep generative models. running examples illustrate probabilistic programming zhusuan including bayesian logistic regression variational auto-encoders deep sigmoid belief networks bayesian recurrent neural networks. keywords bayesian inference deep learning probabilistic programming deep generative models years seen great advances deep learning success many applications speech recognition computer vision language processing computer games good lesson learned practice deeply architected model well leveraging advanced optimization algorithms large training powerful computing devices although great expressiveness deep neural networks made ﬁrst choice many complex ﬁtting problems especially tasks involving mapping input space output space results given usually point estimates. deterministic approach account uncertainty essential every part learning machine randomness physical world incomplete information measurement noise. demonstrated deterministic neural networks vulnerable adversarial attacks hinder applications scenarios representing uncertainty crucial importance automated driving healthcare hand probabilistic view machine learning oﬀers mathematically grounded tools dealing uncertainty i.e. bayesian methods bayesian methods also provide theoretically sound approach incorporating structural bias domain knowledge prior posterior constraints achieve eﬃcient learning small number training samples. thus beneﬁcial combine complimentary advantages deep learning bayesian methods fact drawing tremendous attention recent years moreover success deep learning comes supervised tasks require large number labeled data research area paying attention unsupervised learning long-standing goal artiﬁcial intelligence probabilistic models bayesian methods commonly treated principled approaches modeling unlabeled data pure unsupervised learning hybrid supervised learning recently popularity deep generative models demonstrates promise combining deep neural networks probabilistic modeling shown superior results image generation semi-supervised classiﬁcation one-shot learning call arising direction conjoins advantages bayesian methods deep learning bayesian deep learning scope covers traditional bayesian methods deep learning methods probabilistic inference plays role intersection. unique feature deterministic transformation random variables automatically learned data expressive parametric formulation typically using deep neural networks traditional bayesian models transformation tends simple analytical form challenge bayesian deep learning posterior inference typically intractable models needs sophisticated approximation techniques. although much progress made variational inference monte carlo methods still diﬃcult practitioners pick-up. moreover although variational inference monte carlo methods general recipes still quite involved experienced researcher derive equations implement every detail particular model. procedure error prone takes long time debug. paper present zhusuan probabilistic programming library bayesian deep learning. build zhusuan upon tensorﬂow leverage computation graphs ﬂexible modeling. zhusuan users enjoy powerful ﬁtting recent eﬀorts made towards improving robustness deep neural networks adversarial samples adversarial training reverse cross-entropy training bayesian averaging multi-gpu training deep learning time probabilistic models model complex world exploit unlabeled data deal uncertainty applying principled bayesian inference. provide running examples illustrate intuitive program zhusuan including bayesian logistic regression variational auto-encoders deep sigmoid belief networks bayesian recurrent networks. examples found code repository https//github.com/thu-ml/zhusuan. diﬀerence zhusuan probabilistic programming libraries lies ﬂexibility beneﬁted deep learning paradigm library built treatment model reuse. detailed comparisons closely related works edward pymc provided paper. conjunction graph theory probability theory probabilistic graphical models provide powerful language probabilistic machine learning. deﬁnes joint distribution random variables using graph intuitive compact semantic meaning read conditional independence structures. major types pgms namely directed ones undirected ones though bayesian networks mrfs diﬀerent expressiveness sometimes transform bayesian network vice versa zhusuan concentrate bayesian networks provide intuitive data generation process choice also reﬂected literature bayesian deep learning. example modern deep generative models make ﬁrst remarkable step undirected graphs directed models dominated area cheap generation fast inference. consider general formulation kinds nodes bayesian network deterministic nodes stochastic nodes deterministic node calculates value deterministic function parents stochastic node represented standard probabilistic distribution. although deterministic nodes often represented explicitly make graph structure concise beneﬁcial especially consider bayesian deep learning models. transformation traditional model typically simple analytical form bayesian deep learning model learns ﬂexible family hence explicitly representing deterministic nodes helps model deﬁnition inference shall clear soon. variables observed dataset easy learning inference bayesian network. however reality common partially observed data physical randomness missing information measurement noise thereby variables hidden. models known latent variable models provide suite useful tools unveil underlying factors figure illustrates generic latent variable model gray nodes represent observed variables rest latent variables number observed data samples. class models include global local latent variables. local mean latent variable impact paired observation global means inﬂuences observations analyzed vanilla bayes’ rule generalized regularized bayesian inference introducing posterior regularization information theoretical formulation. general posterior inference using bayes’ rule regbayes formulation intractable except simple examples. therefore resort approximate bayesian inference methods. many years developments area many fast variational inference optimization-based method posterior approximation parametric distribution family chosen approximate true posterior minimizing divergence klevidence lower bound generally steps first choose parametric family used variational posterior. second step solve optimization problem respect variational parameters recent years beneﬁted joint eﬀort bayesian deep learning community variational inference undergoing many signiﬁcant changes algorithm choices variational families. algorithm side stochastic approximation data sub-sampling enabled scale large datasets meanwhile direct optimization variational lower bounds gradient descent replacing traditional analytic updates makes applicable broader range models non-conjugate dependencies challenge draws attention direction gradient estimates. many gradient estimators developed diﬀerent variance reduction techniques specially continuous latent variable models algorithm named stochastic gradient variational bayes successful clever trick pass gradient stochastic node well-known reparameterization trick. besides better lower bounds elbo also developed optimized gradient descent well applied discrete continuous latent variable models side considerable eﬀorts also design variational posteriors. using neural networks parameterize variational posterior common practice adopted aforementioned works serving technologies eﬃciently train deep generative models. network usually observations expected amortize inference cost data point learning inference. type scheme often referred amortized inference summary bayesian deep learning context stochastic diﬀerentiable amortized before. unlike reformulates inference optimization problem monte carlo methods direct ways simulate samples estimate properties particular distribution. main techniques extensively used bayesian inference importance sampling markov chain monte carlo. importance sampling basic idea importance sampling follows. estimate probability density deﬁned probability distribution called proposal introduced. required whenever bayesian inference context often case access unnormalized version denote ˜p/z normalizing constant intractable. self-normalized importance sampling particularly useful situation gives estimate introduced above importance sampling strict sense sampling method directly draw samples target distribution. instead provides method estimating properties certain distribution thus importance sampling everywhere needs estimate integral probability measure. though computation eqs. rather direct application scenarios obvious users. focus application scenarios bayesian deep learning concentrate model learning evaluation. model learning shown self-normalized importance sampling used estimate gradients marginal likelihoods respect model parameters technique proposed train deep generative models. even used learning importance sampling still simple eﬃcient method evaluating marginal likelihoods latent variable models drawbacks using importance sampling estimation large variance proposal good enough therefore people exploring adaptive proposals idea recently reformed technique called neural adaptive proposals i.e. neural network used parameterize proposal distribution adapted towards optimal proposal gradient descent technique proves successful applications like dynamic systems classic method generate samples posterior distribution bayesian statistics. unlike variational inference mcmc known asymptotically unbiased allows user trade computation accuracy without limit. basic idea mcmc design markov chain whose stationary distribution target distribution samples generated simulating chain convergence. speciﬁcally markov chain speciﬁed common throw away samples initial stage chain converges. stage often referred burn-in stage. samples mcmc methods many ways them. parameter estimation lvms samples used monte-carlo algorithm throughout literature many ways design transition kernel. example simplest form metropolis hastings algorithm combines gaussian random walk proposal accept-reject test correction scales poorly increasing dimension complexity target distribution. gibbs sampling utilizes structure target distribution taking element-wise conditional distribution transition proposal. however requires conditionals analytically computable limits application scope especially bayesian deep learning. practice simuhamiltonian-conservation properties dynamics. late dynamics discrete time steps. leapfrog integrator widely used since keeps vital properties hamiltonian dynamics volume-preserving approximately hamiltonian-conserving. combined metropolis-hastings algorithm correct approximating error hamiltonian discrete-time simulation comes satisfactory markov chain converges people exploring bayesian deep learning. early work radford neal applied inference bayesian neural networks representative models apply bayesian methods capture uncertainty deep learning. recent works deep generative models applied improve variational posterior well used hmc-based mcem algorithm directly learn model parameters zhusuan designed towards basic needs bayesian deep learning which stated last section mainly include parts modeling inference. modeling part follow principle code reads like model deﬁnition. requires model primitives support reasons explained section zhusuan’s model primitives designed bayesian networks. since basic structure introduce node primitives ﬁrst zhusuan stores fetches information graph. deterministic nodes zhusuan users enabled tensorﬂow operation deterministic transformations. includes various arithmetic operators neural network layers even control ﬂows tensorﬂow computation graph output operation named tensor. zhusuan higher-level abstraction tensors directly treat deterministic nodes bayesian networks. primitives work well directly tensors. stochastic nodes stochastic nodes zhusuan provides abstraction called stocha stictensor named following deterministic counterpart. many commonly used probabilistic distributions implemented wrapped stochastictensors together recently developed variants bayesian deep learning gumbel-softmax concrete stochastictensors inherit behaviors tensors. former directly tensorﬂow operations automatically cast tensors computing them. cast tensors values choose samples observations automatically determined according states given context introduced next. graph context bayesian network built transparently mixes tensorﬂow operations stochastictensors. large sophisticated models generally common today still painful deal nodes individually. help users manage graph convenient zhusuan provides graph context called bayesiannet keeps track named stochastictensors constructed bayesiannet context supports making queries inner stochastic nodes. query options include current-state outputs local probabilities certain nodes. several representative examples illustrate process building models zhusuan ranging simple bayesian logistic regression modern bayesian deep models. general programming probabilistic model zhusuan intuitive reading corresponding graphical model provided side-by-side. example deterministic transformation part linear model implemented tensorﬂow operations tf.expand_dimstf.reduce_sum multiply enable batch processing inputs random variables created zs.bernoulli zs.normal respectively stochastictensor bernoulli normal distributions. group_ndims argument passed specify stochastictensor means last dimension treated group whose probabilities computed together. starts latent representation sampled simple distribution samples forwarded deep neural network capture complex generative process high dimensional observations images finally noise added output tractable likelihood model. binarized mnist observation noise chosen bernoulli parameters output neural network. zhusuan built upon tensorﬂow full support deep neural networks implementation extremely easy intuitive shown figure example sigmoid belief networks directed discrete latent variable model close connections feed-forward neural networks boltzmann machines recent years return neural networks brought life model. fact well-known deep belief networks earliest work deep learning inﬁnite-layer tied-weight bottom layers untied generative process dsbn layers sigmoid function; layer parameters; hidden weights; latent variables observations. deﬁnition dsbn model multi-layer stochastic nodes. implementation two-layer dsbn shown figure provide interesting example utilizes powerful control operations tensorﬂow describe bayesian recurrent neural network below also intuitively programmed zhusuan. example mentioned previously deterministic neural networks lack ability account uncertainty predictions. solution given bayesian methods model named bayesian neural network treats network weights random variables infers posterior distribution given data. generative process bayesian classiﬁcation tasks neural network weights predicted unnormalized probabilities classes represents categorical distribution class label. chosen recurrent network process describes bayesian rnn. graphical model bayesian shown figure consider model two-class sequence classiﬁcation task. part uses long short-term memory network. code splits three parts. first build bayesianlstmcell applies transformations inputs hidden states time step. diﬀerence tf.nn.rnn_cell.basiclstmcell tensorﬂow weights generated normal stochastictensor. tf.scan operation used apply cell function input sequences. part utilizes power control ﬂows deal variable-length sequences finally bernoulli stochastictensor generates class label given outputs rnn. model reuse unlike supervised neural networks feature probabilistic graphical models polymorphism i.e. stochastic node states observed latent. major diﬃculty designing programming primitives. example consider case. tensor sampled normal distribution model used case observed common practice tensorﬂow programming write another piece code builds graph observation input. stochastic nodes large process painful. reusability problem shared problem probabilistic programming libraries based computation graph toolkits e.g. theano tensorﬂow libraries pymc edward address problem directly manipulating created computation graph. speciﬁcally pymc relies oﬃcially supported theano.clone function copy recreate subgraphs inﬂuenced state node changes. edward implement copy function looking nonpublic low-level apis tensorﬂow computation graph. later solutions manipulating created graphs induce problems limitations libraries. zhusuan carefully designed towards reusability rely altering created graphs. speciﬁcally observation passed stochastictensor directly enables model reuse repeated calls function diﬀerent arguments passed. illustration example however practice often found redundant makes hard automatic inference algorithms model. bayesiannet context makes diﬀerence. bayesiannet accepts argument named observed dictionary mapping names stochastictensors observations. context responsible determining states stochastictensors. makes easier users handle state changes large sets stochastic nodes importantly enables uniﬁed form model building functions makes automatic inference possible. example shown below. section covered major inference methods used bayesian deep learning. although gradient-based stochastic black-box suitable building general inference framework currently software provides complete support them. bridging zhusuan leverages recent advances diﬀerentiable inference bayesian deep learning provides wide ﬂexible support traditional modern inference algorithms. algorithms provided well deep learning paradigms makes inference typical implementations probabilistic programming languages mostly restricted simple variational posteriors. example advi algorithm serves main method stan uses gaussian variational posteriors. contrast zhusuan supports building ﬂexible variational posteriors leveraging bayesian networks. opens door rich variational families user-speciﬁed dependency structures. optimization side mentioned section many gradient-based variational methods emerged recent progress bayesian deep learning methods diﬀer variational objectives gradient estimators use. make automatic easier handle zhusuan wrapped single functions computes surrogate cost users directly take derivatives means optimizing surrogate costs equally optimizing corresponding variational objectives using well-developed gradient estimators. currently supported varia) build variational posterior. true posterior intractable correlations across dimensions. follow common practice make mean-ﬁeld dimension weights. code using factorized normal distribution variational posterior choose gradient estimator surrogate cost minimize. continuous reparameterized mean standard deviation normal distribution choose sgvb gradient estimator computed surrogate costs batch data averaged. call tensorﬂow optimizers gradient descent surrogate cost. previously explained optimizing elbo objective sgvb gradient estimator. also fetch elbo value verify simple example using factorized variational posteriors mentioned previously zhusuan supports building ﬂexible variational posteriors. following example amortized inference leveraging neural networks variational posterior. example consider model example diﬀerence compared vae’s latent variables local instead global. number local latent variables scales linearly number observations hard aﬀord separate variational posterior them. amortized style inference becomes useful. speciﬁcally neural network input generate parameters variational posterior corresponding network often referred decoder recognition model vae. code shown figure deep hierarchical models often conditional dependencies latent variables need consideration inference. dsbn model example illustrate introduce structured dependencies building variational posteriors hierarchical models. example also demonstrates applying discrete latent variables zhusuan. conditional dependency structure original model given observed. graphical model well code structured posterior shown figure seen code directly depends concrete relaxation gumbel-softmax trick namely concrete random variables replace bernoulli ones training. concrete distribution continuous reparameterizable keep using sgvb estimator. test time switch back bernoulli random variables using input parameters. model posterior modiﬁed little approach directly gradient estimator applies discrete latent variables. includes reinforce vimco. present code using vimco. vimco optimizing importance weighted bound multisample bound model variational posterior need changed multi-sample versions shown below. addition variational inference eﬀorts also made towards unifying monte carlo methods bayesian deep learning context. covered basics section walk zhusuan’s support importance sampling black-box mcmc method hamiltonian monte carlo. importance sampling reviewed section major application scenarios importance sampling bayesian deep learning include model learning evaluation. model learning introduced self-normalized importance sampling used estimate gradients marginal likelihoods respect model parameters turns gradients estimated equivalent exact gradients importance weighted variational objective respect model parameters. model learning procedure importance sampling implemented directly optimizing importance weighted objective table respect model parameters besides learning model parameters mentioned importance sampling extensively used model evaluation. towards need zhusuan’s evaluation module provides is_loglikelihood function estimating marginal likelihoods given observations using simple importance sampling haven’t covered build proposal distribution importance sampling. fact procedure exactly building variational posterior. zhusuan’s modeling primitives neural adaptive proposals mentioned section easy implement leveraging neural networks proposal distribution. adapting proposal scenarios also straightforward true posterior distribution often ideal choice. adaptation turns variational inference problem solved choosing appropriate method table specially pling recovers method used reweighted wake-sleep estimator named zhusuan. illustrate process training dsbn importance sampling following example. example example reproduce algorithm proposed reweighted wake-sleep paper learns dsbn importance sampling neural adaptive proposal. code snippet follows multi-sample version model posterior example omit code draw samples compute probabilities proposal distribution. code clearly parts. ﬁrst part learning model parameters optimizing importance weighted objective respect model parameters. second part adapting proposal minimizing inclusive divergence true posterior current proposal. training proceeds adaptation part help maintain good proposal reducing variance marginal likelihood estimate importance sampling. hamiltonian monte carlo section brieﬂy analyzed existing mcmc methods identiﬁed powerful tool address posterior inference problem high-dimensional spaces non-conjugate models perfect bayesian deep learning. however practice algorithm involves lots technical details hard implement fast eﬃcient way. besides despite tuning parameters clear physical meanings still hard users tune hand optimal choice always depends unknown statistics underlying distribution. example mass matrix describes variance underlying distribution hard know draw samples recent years rise practical algorithms high-performance softwares target problems. no-u-turn sampler nuts proposes automatically determine number leapfrog steps. also comes along dual averaging scheme automatically tuning step size. implementation stan also includes procedure estimates mass matrix samples drawn warm-up stage. provide options automatically tuning parameters including step size mass matrix. nuts algorithm determining leapfrog steps included it’s recursive algorithm separate chain diﬀerent leapfrog steps thus hard parallel implementation static computation graphs. compare zhusuan representatives python probabilistic programming libraries namely pymc edward detailed feature comparison shown table seen table three libraries build upon modern computation graph libraries transparently gpus. features mainly divided three categories modeling capabilities inference support well architecture design. modeling capabilities three libraries primitives computation graph toolkits base designed directed graphs pymc solves model reuse problem inference theano.copy copy related subgraphs. edward also rely oﬃcial tensorﬂow api. zhusuan avoids altering created graphs build reuse purely function reuse context management. result pymc zhusuan correctly deal control primitives like theano.scan tf.while_loop edward faces challenges applying variational inference kind models require graph copying replacing latent variables samples variational posterior troublesome control operations given oﬃcial support. name. much eﬀorts sampling algorithms made applicable broad class models bayesian statistics. hand pymc’s support variational inference limited speciﬁc several independent algorithms edward general inference abstraction implements large number algorithms subclasses. however form abstraction induces constraints many implemented algorithms limited behaviors make strong assumptions constructed models zhusuan emphasizes scenarios bayesian deep learning thus puts eﬀorts modern diﬀerentiable algorithms uniﬁed deep learning paradigm. edward zhusuan support customizable variational posteriors methods pymc made speciﬁc reparameterizable settings. architecture design design philosophy zhusuan introduced section emphasizes principles—modularity transparency. principles diﬀerent traditional designs probabilistic programming pymc stereotype. pymc programs strictly follow form model deﬁnition inference function call predictive checks. little customization made except model deﬁnition provided inference options. edward also roughly follows framework made ﬂexible general programmable variational posteriors. however inference procedure also hidden reliance lots internal manipulations created computation graphs hard plain users understand. libraries build modeling inference functionalities tightly coupled way. implies model cannot described using modeling primitives little possibility using inference features. zhusuan draws beneﬁts deep learning paradigms makes inference gradient descent cost term. parts library used independently. proves much transparent compositional especially models large stochastic activities various observation behaviors deep hierarchies. described zhusuan python probabilistic programming library bayesian deep learning built upon tensorﬂow. zhusuan bridges bayesian methods deep learning providing deep learning style primitives algorithms building probabilistic models applying bayesian inference. many examples provided illustrate intuitiveness programming zhusuan. open-sourced zhusuan github aiming accelerate research applications bayesian deep learning. thank haowen helpful discussions design chang advices paper writing shizhen dong initial attempts zhusuan multi-card distributed systems. would like acknowledge support project national basic research program china national china national youth top-notch talents support program nvidia nvail program tsinghua tiangong institite intelligent technology.", "year": 2017}