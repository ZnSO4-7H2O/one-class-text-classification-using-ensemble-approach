{"title": "Discrete Independent Component Analysis (DICA) with Belief Propagation", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We apply belief propagation to a Bayesian bipartite graph composed of discrete independent hidden variables and discrete visible variables. The network is the Discrete counterpart of Independent Component Analysis (DICA) and it is manipulated in a factor graph form for inference and learning. A full set of simulations is reported for character images from the MNIST dataset. The results show that the factorial code implemented by the sources contributes to build a good generative model for the data that can be used in various inference modes.", "text": "apply belief propagation bayesian bipartite graph composed discrete independent hidden variables discrete visible variables. network discrete counterpart independent component analysis manipulated factor graph form inference learning. full simulations reported character images mnist dataset. results show factorial code implemented sources contributes build good generative model data used various inference modes. bi-directional information belief propagation networks becoming popular framework many signal processing applications inference learning easily manipulated small rules. generally bayesian models capturing hidden structure underly observed data assumption network random variables partially occasionally visible independent component analysis popular signal processing framework observed data mapped generated from independent hidden sources variables variables typically continuous transformation sources visible variables linear. used many applications signal separation analyzing signals images ﬁlters trained real images seem converge patterns resemble receptive ﬁelds found neural visual cortex work partially supported ponpe-- cisr ministero dell’istruzione dell’universit`a della ricerca; ponpe-- mar.te. consorzio nazionale interuniversitario telecomunicazioni sources feed visible variables also discrete. even computational difﬁculties naturally emerge dealing product space discrete alphabets even limiting attention tractable small sizes dica framework clearly shows potential applications perhaps building block complex architectures. discrete component analysis also discussed buntine reference different models. reduce dica architecture bayesian factor graph so-called reduced normal form reference therein) includes simple interconnected blocks. experiment belief propagation architecture using images extracted mnist dataset show dica network nicely converges learning generative model reproduces accurately image set. section bayesian model presented section discrete version transformed factor graph belief propagation. various modes inference discussed section learning section simulations unsupervised mapping mnist images reported section addition label variable section conclusions sections unfortunately used generative model hard produce realistic images even experimental densities used density sources structured patches easy obtain resemble complex structures found natural images. reason independent continuous sources carry necessary structure assemble complex structures found natural images. report simulation following seems conﬁrm results. attempts made two-layer architectures however clear properly include linearities investigations direction still progress. fig. dica model factor graph reduced normal form. shaded boxes represent ﬁxed matrices unshaded boxes represent conditional probability matrices work experiment unconstrained model discrete variables. speciﬁcally assume sources visible variables take values ﬁnite discrete alphabets .... ....xn sizes |s||s| ....|sm| |x||x| ....|xn|. difﬁculties dealing model clearly related computational complexity manipulation product space size |s||s| |sm| however even limiting attention small dimensionalites i.e. paper focus generative model depicted bi-partite graph figure independent source variables .... main variables .... connected source variables factorization note .... conditionally independent must conditioned whole sources even marginal distribution factorizes appears general model independent hidden sources underly dependent variables .... system degenerates single-variable latent model solving probability functions involved bayesian model group source variables figure note bayesian graph show source variables marginally independent. made explicit factor graph representation follow. independent component analysis obtained variables conditional probability density functions constrained depend linear combinations speciﬁcally typical assumption linear combinations contribute means dispersion around mean spherical follows gaussian distribution source variables small alphabets framework applied natural images reveals quite interesting results. furthermore basic architecture used building block complicated multi-layer bayesian architectures probability propagation learning graph figure handled ﬂexible transform model factor graph figure graph so-called reduced normal form references therein) composed one-to-one blocks source blocks diverters one-to-one blocks characterized conditional probability matrix sources probability vector. often advocated representation handled block diagram amenable distributed implementations. also designed simulink library rapid prototyping denotes kronecker product k-dimensional column vector ones identity matrix. conditional probability matrix variable contributes product space value uniform components compete source variables. blocks bottom figure represent |xj| conditional probability matrices source prior distributions typically learned data. information ﬂows network bi-directionally branch variable forward backward message discrete probability vectors. messages usually kept normalized numerical stability. variables connected diverter represent replicated version variable carry different forward backward messages combined product rule propagation one-to-one block follows rule variable direction matrix multiplication fout ﬂexibility framework allows factor graph figure various inference modes. information bi-directional assuming parameters learned unspeciﬁed messages initialized uniform distributions dica graph generation source values picked injected forward delta distributions three steps message propagation forward distributions collected terminal variables decoded version source values. note distributions typically displayed means argmaxes encoding observed values injected delta backward distributions bottom. three steps message propagation backward distributions multiplied forward normalized result factorial code input. argmaxes distribution decoding input. pattern completion subset values available available values injected bottom delta backward distributions. missing values uniform densities usually injected. after three steps message propagation forward distributions collected bottom variables. observed variables forward-backward products return deltas observations provides information. unknown variables forward distribution best knowledge variable. means argmaxes used ﬁnal result. inference erasures synthesis information coming observations priors. error correction available values contain errors. presented backward delta distributions bottom variables. three steps message propagation forward distributions collected used corrections. product backward applied know component reliable. similar scheme values known softly distributions injected bottom backward messages. train dica system assume examples available visible variables x...xn learning system matrices bottom blocks vectors sources performed using search. various algorithms used inspired localized maximum likelihood cost function. iterations conﬁned block locally available forward backward messages. details learning algorithms factor graph reduced normal form reported elsewhere omitted space reasons references therein). extracted images training set. ﬁrst experiments train architecture figure binary variables various number sources learning images training presented backward delta distributions time cycles inside block therefore order obtain conditional probability matrices prior distributions generation figure shows increasing means sources inject binary conﬁgurations forward messages reported picture also learned priors. note that larger number sources product space corresponds increasingly accurate pattern memorization. characters different shape system builds separate representations. source variables independent deﬁnition learn marginal distributions progressively less uniform number sources increases kronecker product individual binary distributions even small uniformities priors cause highly uniform). encoding figure shows typical results presenting dica graph figure images test backward delta distributions third column posterior distributions sources shown dica graph acts encoder binary conﬁgurations factorial code presented images. note codes sharp. second column mean forward distributions also shown. decoding figure dica graph used soft decoder smooth sharp distributions injected sources. pattern completion figure shows results network backward present images pixels removed. erased pixels backward uniform distribution presented. third fourth columns report mean forward posterior distributions respectively. network ﬁlls-in rather well missing parts. natural question point whether continuous icas would possible obtain similar results. model clearly different data attempted comparison. mnist images training computed icas using fast algorithm available matlab retained fig. encoding images test set. col. images presented delta backward distributions. col. means forward distributions. col. posterior probabilities sources ...ps ﬁrst components estimated output densities using average histograms. random samples densities used generate images though inverse figure shows masks generated images. results conﬁrm that even nicely represent bases data unconstrained independent samples sources average structures generated. also tried larger number components obtained images look similar. results seem consistent experiments presented literature patches natural images average textures obtained. linear independent unconstrained sources seem generative model preserves structured composition training set. great ﬂexibility factor graph framework allows extend easily architecture dica graph shown figure also label variable included. variable belong ﬁnite alphabet attached directly conditional probability matrix product space diverter. diverters reduced normal form like probability pipelines label information backward delta distribution. blocks including probability matrix trained learned network typical recognition task images test shown figure graph represents simultaneously classiﬁcation encoding. note ﬁrst network naturally confused generative experiment also performed architecture backward delta distributions injected results shown figure images mean forward distributions could considered prototypes labels. graphs corresponding simultaneous encoding sources. simulations mnist dataset binary sources show belief propagation dica architecture also addition label variable provides uniﬁed framework image data coded generated corrected ﬂexible way. also experimented natural images quantized patches obtaining similar results also sources alphabet sizes greater two. results reported elsewhere. currently pursuing framework building multi-layer architectures. jordan sudderth wainwright willsky major advances emerging developments graphical models ieee signal processing magazine vol. november special issue. aapo hyvrinen jarmo hurri patrick hoyer natural image statistics probabilistic approach early computational vision. springer publishing company incorporated edition wray buntine aleks jakulin discrete component analysis subspace latent structure feature selection craig saunders marko grobelnik steve gunn john shawe-taylor eds. vol. lecture notes computer science springer berlin heidelberg", "year": 2015}