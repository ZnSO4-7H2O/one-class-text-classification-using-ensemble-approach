{"title": "Run, skeleton, run: skeletal model in a physics-based simulation", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In this paper, we present our approach to solve a physics-based reinforcement learning challenge \"Learning to Run\" with objective to train physiologically-based human model to navigate a complex obstacle course as quickly as possible. The environment is computationally expensive, has a high-dimensional continuous action space and is stochastic. We benchmark state of the art policy-gradient methods and test several improvements, such as layer normalization, parameter noise, action and state reflecting, to stabilize training and improve its sample-efficiency. We found that the Deep Deterministic Policy Gradient method is the most efficient method for this environment and the improvements we have introduced help to stabilize training. Learned models are able to generalize to new physical scenarios, e.g. different obstacle courses.", "text": "cheap accessible. interesting researches work schulman simulated robot learned ground another paper heess authors trained several simulated bodies diverse challenging terrains obstacles using simple reward function based forward progress solve problem continuous control simulation environments become generally accepted adapt reward signal speciﬁc environment. still lead unexpected results reward function modiﬁed even slightly advanced behaviors appropriate reward function often non-obvious. address problem community came several environment-independent approaches unsupervised auxiliary tasks unsupervised exploration rewards suggestions trying solve main challenge reinforcement learning agent learn itself directly limited reward signal achieve best performance. besides difﬁculty deﬁning reward function physically realistic environments usually stochasticity computationally expensive high-dimensional action spaces. support learnnecessary reliable settings scalable sample-efﬁcient reinforcement learning algorithm. paper evaluate several existing approaches improve upon best performing approach physical simulator setting. present approach used solve learning nips competition challenge objective learn control physiologically-based human model make quickly possible. model present third place challenge https//www.crowdai.org/challenges/ nips--learning-to-run/leaderboards. paper proceeds follows ﬁrst review basics reinforcement learning describe environment used challenge models used experiment present results experiments ﬁnally discuss results conclude work. paper present approach solve physicsbased reinforcement learning challenge learning objective train physiologically-based human model navigate complex obstacle course quickly possible. environment computationally expensive high-dimensional continuous action space stochastic. benchmark state policy-gradient methods test several improvements layer normalization parameter noise action state reﬂecting stabilize training improve sample-efﬁciency. found deep deterministic policy gradient method efﬁcient method environment improvements introduced help stabilize training. learned models able generalize physical scenarios e.g. different obstacle courses. reinforcement learning signiﬁcant subﬁeld machine learning artiﬁcial intelligence along supervised unsupervised subﬁelds numerous applications ranging trading robotics medicine. already achieved high levels performance atari games board games navigation tasks tasks feature common always well-deﬁned reward function example game score optimized produce required behaviour. nevertheless many tasks environments still unclear correct reward function optimize. even harder problem talk continuous control tasks physics-based environments robotics recently substantial interest directed research employing physics-based based environment. environments signiﬁcantly interesting challenging realistic well deﬁned games; time still simpler real conditions physical agents copyright association advancement artiﬁcial intelligence rights reserved. state coordinates velocities various body parts obstacle locations. coordinates absolute. improve generalization controller data efﬁciently modiﬁed original version environment making coordinates relative coordinate pelvis. ments steps environment could take seconds. environments fast three orders magnitudes faster. crucial train agent using sample-efﬁcient method. section brieﬂy describe models evaluated task learning challenge. also describe improvements model best performing competition deep deterministic policy gradient on-policy methods on-policy methods update agent’s behavior data generated current policy. consider popular on-policy algorithms namely trust region policy optimization proximal policy optimization baseline algorithms environment solving. trust region policy optimization notable state-of-the-art algorithms developed schulman theoretical monotonic improvement guarantee. basis trpo using reinforce algorithm estimates gradient expected return likelihood ratio approach problem basic setup agent interacting environment. learning environment fully observable thus modeled markov decision process deﬁned states actions distribution initial states reward function transition probabilities time horizon discount factor policy parametrized denoted policy either deterministic stochastic. agent’s goal maximize expected discounted return γtr] denotes trajectory environment musculoskeletal model includes body segments pelvis segment single segment represent upper half body figure clarifying screenshot. segments connected joints motion joints controlled excitation muscles. muscles model complex paths muscle actuators also highly nonlinear. purpose navigate complex obstacle course quickly possible. agent operates world. obstacles balls randomly located along agent’s way. simulation done using opensim library relies simbody physics engine. environment described table detailed description environment found competition github page. parameter noise another improvement recently proposed parameters noise perturbs network weights encouraging state dependent exploration. used parameter noise actor network. standard deviation gaussian noise chosen according original work measure training episode switched action noise parameter noise choosing probability respectively. layer norm henderson showed layer normalization stabilizes learning process wide range reward scaling. investigated claim settings. additionally layer normalization allowed perturbation scale across layers despite parameters noise normalized output layer except last critic actor standardizing activations sample. also give neuron adaptive bias gain. applied layer normalization nonlinearity. actions states reﬂection symmetry model bilateral body symmetry. state components actions reﬂected increase sample size factor sampled transitions replay memory reﬂected states actions used original states actions well reﬂected batch training step. procedure improves stability learned policy. dont step model learned suboptimal policies example muscles active follows ﬁrst leg. section presents experiments setup. experiments used environment obstacles random strengths psoas muscles. tested models setups running threads. comparing different trpo ddpg settings used threads model conﬁguration. compared various combinations improvements ddpg identical settings differed number threads used conﬁguration goal determine whether model rankings consistent number threads changes. threads used threads sampling transitions thread training thread testing. models used identical architecture actor critic networks. hyperparameters listed table code used competition described experiments found github repo. number episodes number steps episode cumulative reward variance reducing baseline that ascent step taken along estimated gradient. trpo improves upon reinforce computing ascent direction ensures small change policy distribution. baseline trpo used agent described proximal policy optimization trpo tries estimate ascent direction gradient expected return restricts changes policy small values. used clipped surrogate objective variant proximal policy optimization modiﬁcation trying compute update step minimizes following cost function off-policy methods contrast on-policy algorithms off-policy methods allow learning based data arbitrary policies. signiﬁcantly increases sample-efﬁciency algorithms relative on-policy based methods. simulation speed litimations environment consider deep deterministic policy gradient deep deterministic policy gradient consists actor critic networks. critic trained using bellman equation off-policy data actor policy. actor trained maximize critic’s estimated q-values back-propagating critic actor networks. original article used replay buffer target network stabilize training efﬁciently samples environment. ddpg improvements present improvements ddpg method. used standard reinforcement learning techniques action repeat reward scaling. several attempts choose scale factor experiments. exploration used ornstein-uhlenbeck process generate temporally correlated noise efﬁcient exploration physical environments. ddpg implementation parallelized follows processes collected samples ﬁxed weights processed learning process episode updated weights. since ddpg off-policy method stale weights samples improved performance providing sampling process weights thus improving exploration. benchmarking different models comparison winning model baseline approaches presented figure among methods ddpg signiﬁcantly outperformed trpo. environment time expensive method utilized experience effectively possible. ddpg experience replay uses sample environment many times making effective method environment. testing improvements ddpg evaluate component used ablation study done rainbow article ablation removed component full combination. results experiments presented figure figure threads respectively. ﬁgures demonstrate modiﬁcation leads statistically signiﬁcant performance increase. model containing modiﬁcations scores highest reward. note substantially lower reward case parameter noise employed without layer norm. reasons perturbation scale across layers work well without normalization. also note behavior quite stable across number threads well model ranking. expected increasing number threads improves result. maximal rewards achieved given time threads cases combinations modiﬁcations summarized table main things observe substantial improvement effect number threads results opensim experiments indicate computationally expensive stochastic environments high-dimensional continuous action space best performing method off-policy ddpg. tested modiﬁcations ddpg turned important learning. action states reﬂection doubles size training data improves stability learning encourages agent learn left right muscles equally well. approach agent truly learns run. examples learned policies without reﬂection present https//tinyurl.com/ycvfqcv. parameter layer noise additionally improves stability learning introduction state dependent exploration. general believe investigation human-based agents physically realistic environments promising direction future research. figure comparing test reward various modiﬁcations ddpg algorithm threads conﬁguration threads conﬁguration although number threads signiﬁcantly affects performance model ranking approximately stays same.", "year": 2017}