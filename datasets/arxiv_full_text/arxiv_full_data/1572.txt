{"title": "Generalization without systematicity: On the compositional skills of  sequence-to-sequence recurrent networks", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb \"dax,\" he or she can immediately understand the meaning of \"dax twice\" or \"sing and dax.\" In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply \"mix-and-match\" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the \"dax\" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.", "text": "limited data inﬂuential controversial papers jerry fodor researchers argued neural networks plausible models mind associative devices cannot capture systematic compositionality last years neural network research made astounding progress practical domains success crucially depends generalization. perhaps strikingly end-to-end recurrent neural networks currently dominate state-of-the-art machine translation since overwhelming majority sentences even word sequences language occur once even large corpus points strong generalization abilities. still commonly observed neural networks extremely sample inefﬁcient requiring large training sets suggests lack algebraic compositionality humans exploit might sensitive broad patterns lots accumulated statistics paper introduce grounded navigation environment learner must translate commands given limited form natural language sequence actions. problem naturally framed sequence-to-sequence task simplicity ideal study systematic generalization novel examples controlled setup. thus test wide range modern recurrent network architectures terms compositional abilities. results suggest standard architectures lstms attention generalize well novel examples feature mixture constructions observed training. however models catastrophically affected systematic differences training test sentences sort would trivial agent equipped algebraic mind humans understand produce utterances effortlessly thanks compositional skills. person learns meaning verb immediately understand meaning twice sing dax. paper introduce scan domain consisting simple compositional navigation commands paired corresponding action sequences. test zero-shot generalization capabilities variety recurrent neural networks trained scan sequence-to-sequence methods. rnns make successful zero-shot generalizations differences training test commands small apply mix-and-match strategies solve task. however generalization requires systematic compositional skills rnns fail spectacularly. conclude proof-of-concept experiment neural machine translation suggesting lack systematicity might partially responsible neural networks’ notorious training data thirst. human language thought characterized systematic compositionality algebraic capacity understand produce potentially inﬁnite number novel combinations known components example person knows meaning usage words twice again learns verb immediately understand produce instructions twice again. type compositionality central human ability make strong generalizations dept. psychology center data science york university facebook artiﬁcial intelligence research. correspondence brenden lake <brendennyu.edu> marco baroni <mbaronifb.com>. scan tasks call data scan simpliﬁed version commai navigation tasks learner goal translate commands presented simpliﬁed natural language sequence actions. since command unambiguously associated single action sequence scan straightforwardly treated supervised sequenceto-sequence semantic parsing task input vocabulary given words used commands output actions available learner. several examples scan presented fig. formally scan consists commands generated phrase-structure grammar corresponding sequence actions produced according semantic interpretation function intuitively scan grammar licenses commands denoting primitive actions jump walk lturn refer primitive commands. also accepts modiﬁers conjunctions compositionally build expressions referring action sequences. left right modiﬁers take commands denoting undirected primitive actions input return commands denoting directed counterparts opposite modiﬁer produces action sequence turns agent backward speciﬁed direction executing target action around makes agent execute action step turning around speciﬁed direction twice/thrice modiﬁers trigger repetition command take scope over and/after combine action sequences. although scan examples fig. focus jump/jump primitive instance jump replaced either walk look generate commands. many combinations possible licensed grammar. scan grammar lacking recursion generates ﬁnite large unambiguous commands commands decoded compositionally applying corresponding interpretation function. means that discovers right interpretation function learner understand commands seen training. example learner might observed primitive jump command training scan available https//anonymized introducing primitive turning actions lturn rturn considerably simpliﬁes interpretation function compared capturing orientation specifying arguments movement actions jump). approach scan successful sequence-tosequence framework recurrent networks work together learn mapping input sequences output sequences fig. illustrates application seqseq approach scan example. first recurrent network encoder receives input sequence word-by-word forming lowdimensional representation entire command. second low-dimensional representation passed recurrent network decoder generates output sequence action-by-action. decoder’s output compared ground truth backpropagation algorithm used update parameters encoder decoder. note although encoder decoder share network structure otherwise share weights/parameters other. details regarding encoder-decoder provided appendix. using seqseq framework tested range standard recurrent neural network models literature simple recurrent networks long short-term memory networks gated recurrent units recurrent networks attention become increasingly popular last years thus also tested network without attentional mechanism using model bahdanau finally make evaluations systematic possible large-scale hyperparameter search conducted varied number layers number hidden units layer amount dropout varying hyperparameters leads different network architectures experiment replicated times different random initializations. reporting results focus overall-best architecture determined extensive hyperparameter search. winning architecture -layer lstm hidden units layer attention dropout applied level. although detailed analyses follow focus particular model top-performing arjump jump left jump around right turn left twice jump thrice jump opposite left walk thrice jump opposite left walk around left lturn walk lturn walk lturn walk lturn walk following experiments recurrent networks trained large commands scan tasks establish background knowledge outlined above. training networks evaluated commands designed test generalization beyond background systematic compositional ways. evaluating commands networks must make zero-shot generalizations produce appropriate action sequence based solely extrapolation background training. experiment scan tasks randomly split training test training provides broad coverage task space test examines networks decompose recombine commands training set. instance network asked perform command jump opposite right walk around right thrice zero-shot generalization test set. although conjunction whole novel parts training features many examples parts contexts e.g. jump opposite right turn opposite right jump right twice walk around right thrice succeed network needs generalize recombining pieces existing commands interpret ones. overall networks highly successful generalization. top-performing network experiment achieved correct test topperforming architecture lstm attention layers hidden units dropout. best-overall network achieved correct. interestingly every architecture successful classic srns performed poorly best achieved less correct test time however attention-augmented srns learned commands much better achieving correct average test ends ﬁrst <eos> symbol decoder begins <sos>. networks trained following speciﬁcations. training consisted trials presenting input/output sequence updating networks weights. adam optimization algorithm used default parameters including learning rate gradients norm larger clipped. finally decoder requires previous step’s output next step’s input computed different ways. training half time network’s self-produced outputs passed back next step half time groundtruth outputs passed back next step networks implemented pytorch based standard seqseq implementation. study next systematic form generalization models must bootstrap commands requiring longer action sequences seen training. training contains commands requiring sequences actions whereas test includes remaining commands split example test time network must execute command jump around left twice walk opposite right thrice requiring sequence actions. although elements used command observed training network never asked produce sequence length ever seen around twice command conjoined opposite thrice command thus must productively generalize familiar verbs modiﬁers conjunctions generate longer action sequences. fair task system correctly translating input commands. know walk around jump function conjunction immediately able walk around jump even never performed action sequence length. fig. shows partial success almost entirely explained generalization shortest action sequence lengths test set. although might expect even humans able generalize long action sequences sharp drop extrapolating actions striking. bottom panel fig. shows accuracy test organized command length model gets right longest commands training longest action sequences invariably associated commands containing tokens. thus model correctly generalizing cases similar training instances. focus action sequence length rather command length since former exhibits variance longest commands given conjunction directed primitives modiﬁed twice e.g. jump around left twice opposite right thrice. hand relatively short command jump around left thrice demands actions. figure zero-shot generalization training random subset scan tasks. overall-best network trained varying proportions distinct tasks generalization measured tasks shows mean training runs corresponding sem. indicated above main split quite generous providing commands training time total distinct examples next re-trained best-overall network varying numbers distinct examples results shown fig. commands shown training network performs poorly correct. coverage performance improves correct test set. coverage performance correct. results show networks generalize random subsets tasks relatively sparse coverage compositional command space. well line success seqseq architectures machine translation test sentences likely never encountered training. still even sparser coverage differences training test instances dramatic. example consider commands without conjunction commands sort occur test training coverage split also occur corresponding training average occurrences. even split conjunction-less test command also occur training split frequency occurrence commands training non-negligible average next test closest thought experiment presented introduction. training phase model exposed primitive command denoting certain basic action model also exposed primitive composed commands actions test time model execute composed commands action primitive context according classic thought experiments fodor colleagues easy know meaning jump twice also understand jump twice means. variants experiment generalizing turn left jump respectively. since turn right distributionally identical turn left walk look distributionally identical jump redundant test commands. moreover ensure networks highly familiar target primitive command overrepresented training roughly training presentations command. obtain strikingly different results turn left jump. turn left many models generalize well composed commands. best performance achieved network attention layer hidden units dropout overallbest model achieved accuracy. hand jump models almost completely incapable generalize composed commands. best performance accuracy overall-best model reached accuracy. case turn left although models exposed primitive command training action denotes many times used accomplish many directed actions. example training example walk left jump left ground-truth interpretation lturn walk lturn jump. apparently seeing action sequences containing lturn sufﬁces model understand composed commands turn left probably model receives direct evidence figure zero-shot generalization commands action sequence lengths seen training. accuracy distribution action sequence length. bottom accuracy distribution command length bars show means runs overall-best model sem. finally studied whether difﬁculty long sequences mitigated proper length provided oracle evaluation time. difﬁculty relatively straightforward issue decoder terminating early provide difﬁculty symptomatic deeper problems generalization change small effect. oracle overall-best network performance improved correct notable insufﬁcient master long sequences. top-performing model showed substantial improvement although improved networks perfect still exhibited difﬁculties long sequences output actions take closer look results focusing median-performance overall-best model observe even successful turn left case model errors surprising. would expect errors randomly distributed perhaps pertain longest commands action sequences. instead errors made model conjunctions components simple turn left turn left thrice particularly striking network produced correct mapping turn left training well turn left thrice test time gets many conjunctions right conclude that even network apparently learned systematic composition almost perfectly non-human-like way. it’s hard conceive someone understood meaning turn left jump right turn left twice jump right turn left jump experiment network could correctly decode composite cases starting execution primitive jump conjoined different action jump opposite right jump walk around left thrice. instructive look representations network induced various commands latter experiment. table reports nearest neighbours sample commands. command similarity measured cosine ﬁnal encoder hidden state vectors computed respect commands present training set. provided example primitive command model exposed full composed paradigm training. would expect close primitive commands well short conjoined commands contain primitive conjuncts instead since jump different training distribution primitive commands model capture similarity them shown cosines nearest commands. since fails establish link basic commands model generalize modiﬁer application jump. although twice similar primitive tasks composed twice jump twice isolated representational space nearest neighbours look arbitrary. tested systematicity purest form model exposed jump isolation asked bootstrap compositional paradigm based behaviour primitive commands walk look run. although suspect humans would problems setup arguably opaque computational model could lack evidence jumping sort action walking. suppose give network evidence jumping composes like walking showing composed jump command training. network able generalize full composed paradigm? question answered figure again primitive command over-sampled training make presentations. here even shown different composed commands jump training time network generalize composed commands weak generalization starts appearing network presented composed tasks training signiﬁcant generalization shows training contains especially distinct composed commands conclude network failing generalize simply because original setup evidence jump dropped less point). tested network embedding daxy following constructions daxy daxy daxy daxy daxy daxy daxy daxy training model constructions occurring distinct predicates average still model could translations right comparison adjective tired occurred different constructions training corpus network accuracy testing constructions daxy although small-scale machine translation problem preliminary result suggests models similarly struggle systematic compositionality larger data sets adding word vocabulary ways people clearly not. thirty years since inception systematicity debate many tested ability neural networks solve tasks requiring compositional generalization mixed results however best knowledge ﬁrst study testing systematicity modern seqseq models results conﬁrm mixed picture. hand experiment turn left results experiment show standard recurrent models reach high zeroshot accuracy relatively training examples. would like stress important positive result showing controlled experiments seqseq models make powerful zero-shot generalizations. indeed interesting direction future work understand precisely generalization mechanisms subtend networks’ success experiments. human language plenty generalization patterns easily accounted algebraic compositionality figure zero-shot generalization adding primitive jump compositional jump commands. overall-best network trained different numbers composed jump commands generalization measured composed jump commands shows mean runs varying training commands along corresponding sem. behave like commands. hand runs composed examples conﬁrm that found experiment network display powerful generalization abilities. simply conform all-or-nothing rule-based behaviour would expect systematically compositional device–and consequence require positive examples emerge. ﬁnal experiment proof-of-concept ﬁndings broadly applicable; limitations recurrent networks regards systematic compositionality extend beyond scan sequence-to-sequence problems machine translation. first trained standard seqseq code short english-french sentence pairs begin english phrases they contractions informal hyperparameter search pick lstm attention layers hidden units dropout. hyperparameters training procedure used scan tasks network reached respectable bleu test score steps. second examine compositionality introduction word trained fresh network adding repetitions sentence daxy training data translate translate; translate translate translate. then meaning command learned training time acts variable rules applied learning needed test time. represented abstract training test distributions quite similar even differ terms shallower statistics word frequency. encourage seqseq models extract rules data rather exploiting shallower pattern recognition mechanisms? think several non-mutually exclusive avenues explored. first learning-to-learn approach network exposed number different learning environments regulated similar rules. objective function requiring successful generalization environments might encourage learners discover shared general rules. another promising approach structure neural networks. taking inspiration recent neural program induction modular network models could endow rnns manually-encoded learned functions interpreting individual modiﬁers connectives primitives. would learn apply compose functions appropriate interpreting command. similarly differentiable stacks tapes random-access memory could equip seqseq models quasi-discrete memory structures enabling separate storage variables turn might encourage solutions ad-hoc copying mechanisms special ways initialize embeddings novel words might help solve scan tasks speciﬁcally. unlikely help general seqseq problems. remains seen course proposed approaches offer truly general solution. nonetheless suggestions directions worth pursuing perhaps simultaneously complementary ways goal achieving human-like systematicity scan beyond. given astounding successes seqseq models challenging tasks machine translation might argue failure generalize systematic composition indicates neural networks poor models aspects human cognition little practical import. however systematicity extremely efﬁcient generalize. person learns english adjective daxy immediately produce understand inﬁnity sentences containing scan experiments proof-of-concept machine translation experiment suggest ability still beyond grasp state-of-the-art neural networks likely contributing striking need large training sets. results give hope neural networks capable systematic compositionality could greatly beneﬁt machine translation language modeling applications. thank germ´an kruszewski adam liska tomas mikolov kristina gulordava gemma boleda michael auli matt botvinick bowman jeff dean jonas gehring david grangier angeliki lazaridou gary marcus commai team audiences facebook dialogue summit paris syntax semantics colloquium clic-it feedback advice. scan tasks based navigation tasks available https// github.com/facebookresearch/commai-env references bahdanau dzmitry kyunghyun bengio yoshua. neural machine translation jointly learning align translate. proceedings iclr conference track diego published online http//www. iclr.cc/doku.php?id=iclrmain. graham yvette haddow barry huck matthias jimeno yepes antonio koehn philipp logacheva varvara monz christof negri matteo neveol aurelie neves mariana popel martin post matt rubino raphael scarton carolina specia lucia turchi marco verspoor karin zampieri marcos. findings conference machine translation. proceedings first conference machine translation berlin germany botvinick matthew plaut david. empirical computational support context-dependent representations serial order reply bowers damian davis psychological review bowers jeffrey damian markus david colin. fundamental limitation conjunctive codes learned models cognition comment botvinick plaut psychological review frank stefan. getting real systematicity. calvo paco symons john architecture cognition rethinking fodor pylyshyn’s systematicity challenge press cambridge bowman samuel manning christopher potts christopher. tree-structured composition neural networks without tree-structured architectures. arxiv preprint chung junyoung gulcehre caglar kyunghyun bengio yoshua. empirical evaluation gated recurprorent neural networks sequence modeling. ceedings nips deep learning representation learning workshop montreal canada published online http//www.dlworkshop.org/ accepted-papers. graves alex wayne greg reynolds malcolm harley danihelka grabska-barwinska agnieszka colmenarejo sergio gomez grefenstette edward ramalho tiago agapiou john badia adri`a puigdom`enech hermann karl moritz zwols yori ostrovski georg cain adam king helen summerﬁeld christopher blunsom phil kavukcuoglu koray hassabis demis. hybrid computing using neural network dynamic external memory. nature velde frank voort kleij gwendid kamps marc. lack combinatorial productivity language processing simple recurrent networks. connection science wong francis wang william. generalisation towards combinatorial productivity language acquisition simple recurrent networks. proceedings kimas waltham yonghui schuster mike chen zhifeng quoc norouzi mohammad macherey wolfgang krikun maxim yuan macherey klaus klingner jeff shah apurva johnson melvin xiaobing kaiser lukasz gouws stephan kato yoshikiyo kudo taku kazawa hideto stevens keith kurian george patil nishant wang young cliff smith jason riesa jason rudnick alex vinyals oriol corrado greg hughes macduff dean jeffrey. google’s neural machine translation system bridging human machine translation. http// arxiv.org/abs/. ronghang andreas jacob rohrbach marcus darrell trevor saenko kate. learning reason end-toend module networks visual question answering. proceedings iccv venice italy johnson justin hariharan bharath maaten laurens hoffman judy fei-fei zitnick lawrence girshick ross. inferring executing programs visual reasoning. international conference computer vision kingma diederik welling max. efﬁcient gradientbased inference transformations bayes nets neural nets. international conference machine learning reed scott freitas nando. neural programmerinterpreters. proceedings iclr juan puerto rico published online http//www.iclr. cc/doku.php?id=iclrmain. risi sebastian vanderbleek sandy hughes charles stanley kenneth. novelty search escapes deceptive trap learning learn. proceedings gecco montreal canada", "year": 2017}