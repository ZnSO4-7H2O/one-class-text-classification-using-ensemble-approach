{"title": "On the Origin of Deep Learning", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "This paper is a review of the evolutionary history of deep learning models. It covers from the genesis of neural networks when associationism modeling of the brain is studied, to the models that dominate the last decade of research in deep learning like convolutional neural networks, deep belief networks, and recurrent neural networks. In addition to a review of these models, this paper primarily focuses on the precedents of the models above, examining how the initial ideas are assembled to construct the early models and how these preliminary models are developed into their current forms. Many of these evolutionary paths last more than half a century and have a diversity of directions. For example, CNN is built on prior knowledge of biological vision system; DBN is evolved from a trade-off of modeling power and computation complexity of graphical models and many nowadays models are neural counterparts of ancient linear models. This paper reviews these evolutionary paths and offers a concise thought flow of how these models are developed, and aims to provide a thorough background for deep learning. More importantly, along with the path, this paper summarizes the gist behind these milestones and proposes many directions to guide the future research of deep learning.", "text": "paper review evolutionary history deep learning models. covers genesis neural networks associationism modeling brain studied models dominate last decade research deep learning like convolutional neural networks deep belief networks recurrent neural networks. addition review models paper primarily focuses precedents models above examining initial ideas assembled construct early models preliminary models developed current forms. many evolutionary paths last half century diversity directions. example built prior knowledge biological vision system; evolved trade-oﬀ modeling power computation complexity graphical models many nowadays models neural counterparts ancient linear models. paper reviews evolutionary paths oﬀers concise thought models developed aims provide thorough background deep learning. importantly along path paper summarizes gist behind milestones proposes many directions guide future research deep learning. deep learning dramatically improved state-of-the-art many diﬀerent artiﬁcial intelligent tasks like object detection speech recognition machine translation deep architecture nature grants deep learning possibility solving many complicated tasks result researchers extending deep learning variety diﬀerent modern domains tasks additional traditional tasks like object detection face recognition language models example osako uses recurrent neural network denoise speech signals gupta uses stacked autoencoders discover clustering patterns gene expressions. gatys uses neural model generate images diﬀerent styles. wang uses deep learning allow sentiment analysis multiple modalities simultaneously etc. period witness blooming deep learning research. however fundamentally push deep learning research frontier forward needs thoroughly understand attempted history current models exist present forms. paper summarizes evolutionary history several diﬀerent deep learning models explains main ideas behind models relationship ancestors. understand past work trivial deep learning evolved long time history showed table therefore paper aims oﬀer readers walk-through major milestones deep learning research. cover milestones showed table well many additional works. split story diﬀerent sections clearness presentation. paper starts discussion research human brain modeling. although success deep learning nowadays necessarily resemblance human brain ambition build system simulate brain indeed thrust initial development neural networks. therefore next section begins connectionism naturally leads shallow neural network matures. maturity neural networks paper continues brieﬂy discuss necessity extending shallow neural networks deeper ones well promises deep neural networks make challenges deep architecture introduces. establishment deep neural network paper diverges three different popular deep learning topics. speciﬁcally section paper elaborates deep belief nets construction component restricted boltzmann machine evolve trade-oﬀ modeling power computation loads. section paper focuses development history convolutional neural network featured prominent steps along ladder imagenet competition. section paper discusses development recurrent neural networks successors like lstm attention models successes achieved. paper primarily discusses deep learning models optimization deep architecture inevitable topic society. section devoted brief summary optimization techniques including advanced gradient method dropout batch normalization etc. paper could read complementary schmidhuber’s paper aimed assign credit contributed present state paper focuses every single incremental work along path therefore cannot elabcontribution introduced associationism started history human’s attempt understand brain. introduced neural groupings earliest models neural network inspired hebbian learning rule. introduced model considered ancestor artiﬁcial neural model. considered father neural networks introduced hebbian learning rule lays foundation modern neural network. introduced ﬁrst perceptron highly resembles modern perceptron. introduced backpropagation introduced self organizing introduced neocogitron inspired convolutional neural network introduced hopﬁeld network introduced boltzmann machine introduced harmonium later known restricted boltzmann machine deﬁned introduced recurrent neural network introduced lenet showed possibility deep neural networks practice introduced bidirectional recurrent neural network introduced lstm solved problem vanishing gradient recurrent neural networks introduced deep belief networks also introduced layer-wise pretraining technique opened current deep learning era. orate well enough them. hand paper aimed providing background readers understand models developed. therefore emphasize milestones elaborate ideas help build associations ideas. addition paths classical deep learning models also discuss recent deep learning work builds classical linear models. another article readers could read complementary authors conducted extensive interviews well-known scientiﬁc leaders topic neural networks’ history. study deep learning artiﬁcial neural networks originates ambition build computer system simulating human brain. build system requires understandings functionality cognitive system. therefore paper traces back origins attempts understand brain starts discussion aristotle’s associationism around b.c. when therefore accomplish reminiscence pass certain series precursive movements arrive movement quest habitually consequent. hence hunt mental train excogitating present other similar contrary coadjacent. process reminiscence takes place. movements cases sometimes time sometimes parts whole subsequent movement already half accomplished. remarkable paragraph aristotle seen starting point associationism associationism theory states mind conceptual elements organized associations elements. inspired plato aristotle examined processes remembrance recall brought four laws association back then aristotle described implementation laws mind common sense. example feel smell taste apple naturally lead concept apple common sense. nowadays surprising laws proposed years still serve fundamental assumptions machine learning methods. example samples near clustered group; explanatory variables frequently occur response variables draw attention model; similar/dissimilar data usually represented similar/dissimilar embeddings latent space. contemporaneously similar laws also proposed zeno citium epicurus augustine hippo. theory associationism later strengthened variety philosophers psychologists. thomas hobbes stated complex experiences association simple experiences associations sensations. also believed association exists means coherence frequency strength factor. meanwhile john locke introduced concept association ideas. still separated concept ideas sensation ideas reﬂection stated complex ideas could derived combination simple ideas. david hume later reduced aristotle’s four laws three resemblance contiguity cause eﬀect. believed whatever coherence world seemed matter three laws. dugald stewart extended three laws several principles among obvious accidental coincidence sounds words. thomas reid believed original quality mind required explain spontaneous recurrence thinking rather habits. james mill emphasized frequency learning similar later stages research. david hartley physician remarkably regarded made associationism popular addition existing laws proposed argument memory could conceived smaller scale vibrations regions brain original sensory experience. vibrations link represent complex ideas therefore material basis stream consciousness. idea potentially inspired hebbian learning rule discussed later paper foundation neural networks. besides david hartley alexander bain also contributed fundamental ideas hebbian learning rule book bain related processes associative memory distribution activity neural groupings proposed constructive mode storage capable assembling required contrast alternative traditional mode storage prestored memories. illustrate ideas bain ﬁrst described computational ﬂexibility allows neural grouping function multiple associations stored. hypothesis bain managed describe structure highly resembled neural networks today individual cell summarizing stimulation selected linked cells within grouping showed figure joint stimulation triggers stimulation triggers stimulation triggers original illustration stand simulations outcomes cells. establishment associative structure neural grouping function memory bain proceeded describe construction structures. followed directions associationism stated relevant impressions neural groupings must made temporal contiguity period either occasion repeated occasions. further bain described computational properties neural grouping connections strengthened weakened experience changes intervening cell-substance. therefore induction circuits would selected comparatively strong weak. following section hebb’s postulate highly resembles bain’s description although nowadays usually label postulate hebb’s rather bain’s according omission bain’s contribution also bain’s lack conﬁdence theory eventually bain convinced doubted practical values neural groupings. when axon cell near enough excite cell repeatedly persistently takes part ﬁring growth process metabolic change takes place cells eﬃciency cells ﬁring increased. stands change synaptic weights neuron input signal denotes postsynaptic response denotes learning rate. words hebbian learning rule states connection units strengthened frequency co-occurrences units increase. although hebbian learning rule seen laying foundation neural networks seen today drawbacks obvious co-occurrences appear more weights connections keep increasing weights dominant signal increase exponentially. known unstableness hebbian learning rule fortunately problems inﬂuence hebb’s identity father neural networks. erkki extended hebbian learning rule avoid unstableness property also showed neuron following updating rule approximating behavior principal component analyzer long story short introduced normalization term rescue hebbian learning rule showed learning rule simply online update principal component analyzer. present details argument following paragraphs. took step show neuron updated rule eﬀectively performing principal component analysis data. show this ﬁrst re-wrote equation following forms additional assumptions covariance matrix input proceeded show property many conclusions another work linked back fact components eigenvectors ﬁrst component eigenvector corresponding largest eigenvalues covariance matrix. intuitively could interpret property simpler explanation eigenvectors solution maximize rule updating function. since eigenvectors covariance matrix donald hebb seen father neural networks ﬁrst model neuron could trace back years ahead publication hebbian learning rule neurophysiologist warren mcculloch mathematician walter pitts speculated inner workings neurons modeled primitive neural network electrical circuits based ﬁndings model known neural model linear step function upon weighted linearly interpolated data could described stands output stands input signals stands corresponding weights stands inhibitory input. stands threshold. function designed activity inhibitory input completely prevents excitation neuron time. success neural model frank rosenblatt substantialized hebbian learning rule introduction perceptrons theorists like hebb focusing biological system natural environment rosenblatt constructed electronic device named perceptron showed ability learn accordance associationism. figure illustrates explanation perceptron. left right four units sensory unit projection unit association unit response unit respectively. projection unit receives information sensory unit passes onto association unit. unit often omitted description similar models. omission projection unit structure resembles structure nowadays perceptron neural network sensory units collect data association units linearly adds data diﬀerent weights apply non-linear transform onto thresholded pass results response units. distinction early stage neuron models modern perceptrons introduction non-linear activation functions originates argument linear threshold function softened simulate biological neural networks well consideration feasibility computation replace step function continuous rosenblatt’s introduction perceptron widrow introduced followmodel called adaline. however diﬀerence rosenblatt’s perceptron adaline mainly algorithm aspect. primary focus paper neural network models skip discussion adaline. perceptron fundamentally linear function input signals; therefore limited represent linear decision boundaries like logical operations like sophisticated decision boundary required. limitation highlighted minski papert attacked limitations perceptions emphasizing perceptrons cannot solve functions like nxor. result little research done area show concrete example introduce linear preceptron inputs therefore decision boundary forms line two-dimensional space. choice threshold magnitude shifts line horizontally sign function picks side line halfspace function represents. halfspace showed figure figure present nodes denote input well node denote situation trigger node denote situation neither triggers. figure figure show clearly linear perceptron used describe operation inputs. however figure interested operation operation longer described single linear decision boundary. next section show representation ability greatly enlarged perceptrons together make neural network. however keep stacking neural network upon make deep learning model representation power necessarily increase. section introduce important properties neural networks. properties partially explain popularity neural network gains days also motivate necessity exploring deeper architecture. speciﬁc discuss universal approximation properties property condition. then show although shallow neural network universal approximator deeper architecture signiﬁcantly reduce requirement resources retaining representation power. last also show interesting properties discovered backpropagation inspire related research today. step perceptrons basic neural networks placing perceptrons together. placing perceptrons side side single one-layer neural network stacking one-layer neural network upon other multi-layer neural network often known multi-layer perceptrons remarkable property neural networks widely known universal approximation property roughly describes represent functions. discussed property three diﬀerent aspects discuss three properties detail following paragraphs. suit diﬀerent readers’ interest ﬁrst oﬀer intuitive explanation properties oﬀer proofs. approximation property straightforward. previous section shown every linear preceptron perform either according morgan’s laws every propositional formula converted equivalent conjunctive normal form multiple functions. therefore simply rewrite target boolean function multiple operations. design network input layer performs operations hidden layer simply operation. continuing linear representation power perceptron discussed previously want represent complex function showed figure linear perceptrons describing halfspace. perceptrons shown figure need perceptrons. perceptrons bound target function showed figure numbers showed figure represent number subspaces described perceptrons fall corresponding region. appropriate selection threshold bound target function out. therefore describe bounded continuous function hidden layer; even shape complicated figure conclusion drawn proof contradiction hahn-banach theorem riesz representation theorem fact closure subspace contradicts assumption activation function. till today property drawn thousands citations. unfortunately many later works cite property inappropriately equation widely accepted form one-hidden-layer neural network deliver thresholded/squashed output linear output instead. years later property shown castro concluded story showing ﬁnal output squashed universal approximation property still holds. move explain property need ﬁrst show major property regarding combining linear perceptrons neural networks. figure shows number linear perceptrons increases bound target function area outside polygon close threshold shrinks. following trend large number perceptrons bound circle achieved even without knowing threshold area close threshold shrinks nothing. left outside circle fact area sums therefore neural network hidden layer represent circle arbitrary diameter. further introduce another hidden layer used combine outputs many diﬀerent circles. newly added hidden layer used perform operation. figure shows example extra hidden layer used merge circles previous layer neural network used approximate function. target function necessarily continuous. however circle requires large number neurons consequently entire function requires even more. property showed respectively. looking back property today arduous build connections property fourier series approximation which informal words states every function curve decomposed many simpler curves. linkage show universal approximation property show one-hiddenlayer neural network represent simple surface second hidden layer sums simple surfaces approximate arbitrary function. know hidden layer neural network simply performs thresholded operation therefore step left show ﬁrst hidden layer represent simple surface. understand simple surface linkage fourier transform imagine cycle sinusoid one-dimensional case bump plane two-dimensional case. could easily generalized n-dimensional case need sigmoid functions simple surface. simple surface contributes ﬁnal function neuron added onto second hidden layer. therefore despite number neurons need never need third hidden layer approximate function. universal approximation properties showed great potential shallow neural networks price exponentially many neurons layers. followed-up question reduce number required neurons maintaining representation power. question motivates people proceed deeper neural networks despite shallow neural networks already inﬁnite modeling power. another issue worth attention that although neural networks approximate functions trivial parameters explain data. next sections discuss questions respectively. universal approximation properties shallow neural networks come price exponentially many neurons therefore realistic. question maintain expressive power network reducing number computation units asked years. intuitively bengio delalleau suggested nature pursue deeper networks human neural system deep architecture humans tend represent concepts level abstraction composition concepts lower levels. nowadays solution build deeper architectures comes conclusion states representation power layer neural network polynomial many neurons conclusion could trace back three decades showed limitations shallow circuits functions. hastad later showed property parity circuits there functions computable polynomial size depth requires application demorgan’s states rewritten ands vice versa. therefore simpliﬁed circuit ands appear rewriting layer ands therefore merge operation neighboring layer ors. repeating procedure able represent function fewer layers computations. moving circuits neural networks delalleau bengio compared deep shallow sum-product neural networks. showed function could expressed neurons network depth required least neurons two-layer neural network. further bianchini scarselli extended study general neural network many major activation functions including tanh sigmoid. derived conclusion concept betti numbers used number describe representation power neural networks. showed shallow network representation power grow polynomially respect number neurons deep architecture representation grow exponentially respect number neurons. also related conclusion vc-dimension neural networks tanh number parameters. recently eldan shamir presented thorough proof show depth neural network exponentially valuable width neural network standard popular activation functions. conclusion drawn weak assumptions constrain activation functions mildly increasing measurable able allow shallow neural networks approximate univariate lipschitz function. finally well-grounded theory support fact deeper network preferred shallow ones. however reality many problems arise keep increasing layers. among them increased diﬃculty learning proper parameters probably prominent one. immediately next section discuss main drive searching parameters neural network backpropagation. proceed need clarify name backpropagation originally referring algorithm used learn parameters neural network instead stands technique help eﬃciently compute gradient parameters gradient descent algorithm applied learn parameters however nowadays widely recognized term refer gradient descent algorithm technique. compared standard gradient descent updates parameters respect error backpropagation ﬁrst propagates error term output layer back layer parameters need updated uses standard gradient descent update parameters respect propagated error. intuitively derivation backpropagation organizing terms gradient expressed chain rule. derivation neat skipped paper extensive resources available instead discuss interesting seemingly contradictory properties backpropagation. gori tesi studied problem local minima backpropagation. interestingly society believes neural networks deep learning approaches believed suﬀer local optimal proposed architecture global optimal guaranteed. weak assumptions network needed reach global optimal including hand brady studied situations backpropagation fails linearly separable data sets. showed could situations data linearly separable neural network learned backpropagation cannot boundary. also showed examples situation occurs. illustrative examples hold misclassiﬁed data samples signiﬁcantly less correctly classiﬁed data samples words misclassiﬁed data samples might outliers. therefore interesting property viewed today arguably desirable property backpropagation typically expect machine learning model neglect outliers. therefore ﬁnding attracted many attentions. however matter whether data outlier neural network able overﬁt training data given suﬃcient training iterations legitimate learning algorithm especially considering brady showed inferior algorithm able overﬁt data. therefore phenomenon played critical role research improving optimization techniques. recently studying cost surfaces neural networks indicated existence saddle points explain ﬁndings brady back late backpropagation enables optimization deep neural networks. however still long optimize well. later section brieﬂy discuss techniques related optimization neural networks. background modern neural network proceed visit prominent branch current deep learning family. ﬁrst stop branch leads popular restricted boltzmann machines deep belief nets starts model understand data unsupervisedly. figure summarizes model covered section. horizontal axis stands computation complexity models vertical axis stands representation power. milestones focused section placed ﬁgure. discussion starts self organizing invented kohonen powerful technique primarily used reducing dimension data usually dimensions reducing dimensionality also retains topological similarity data points. also seen tool clustering imposing topology clustered representation. figure illustration self organizing dimension hidden neurons. therefore learns dimension representation data. upper shaded nodes denote units used position node ﬁxed. representation viewed numerical value. instead position also matters. property diﬀerent widely-accepted representation criterion. example compare case one-hot vector one-dimension used denote colors denote green long specify green correspondingly. however one-dimensional vectors possible that since aims represent data retaining similarity; purple much similar green green purple green represented splits purple. notice that example used demonstrate position unit matters. practice values unit restricted integers. learned usually good tool visualizing data. example conduct survey happiness level richness level country feed data two-dimensional som. trained units represent happiest richest country corner represent opposite country furthest corner. rest corners represent richest unhappiest poorest happiest countries. rest countries positioned accordingly. advantage allows literature example notice connections illustrations models. however connections used represent neighborhood relationship nodes information ﬂowing connections. paper show many models rely clear illustration information decide save connections denote that. understanding representation power proceed parameter learning algorithm. classic algorithm heuristic intuitive shown below two-dimensional example indexes units; weight algorithm explains used learn representation similarities retained always selects subset units similar data sampled adjust weights units match data sampled. however algorithm relies careful selection radius neighbor selection good initialization weights. otherwise although learned weights local property topological similarity loses property globally sometimes similar clusters similar events separated another dissimilar cluster similar events. simpler words units green actually separate units units purple network appropriately trained. hopﬁeld network historically described form recurrent neural network ﬁrst introduced recurrent context refers fact weights connecting neurons bidirectional. hopﬁeld network widely recognized content-addressable memory property. content-addressable memory property simulation spin glass theory. therefore start discussion spin glass. group dipoles placed together space. dipole forced align ﬁeld generated dipoles location. however aligning itself changes ﬁeld locations leading dipoles causing ﬁeld original location change. eventually changes converge stable state. hopﬁeld network fully connected neural network binary thresholding neural units. values units either units fully connected bidirectional weights. state unit denotes bias; denotes bidirectional weights indexes units. energy function closely connects potential energy function spin glass showed equation hopﬁeld network typically applied memorize state data. weights network designed learned make sure energy minimized given state interest. therefore another state presented network weights ﬁxed hopﬁeld network search states minimize energy recover state memory. example face completion task image faces presented hopﬁeld network network calculate weights minimize energy given faces. later image corrupted distorted presented network again network able recover original image searching conﬁguration states minimize energy starting corrupted input presented. term energy remind people physics. explain hopﬁeld network works physics scenario clearer nature uses hopﬁeld network memorize equilibrium position pendulum because equilibrium position pendulum lowest gravitational potential energy. therefore whenever pendulum placed converge back equilibrium position. learning procedure simple still worth mentioning essential step hopﬁeld network applied solve practical problems. however many online tutorials omit step make worse refer inference states learning/training. remove confusion paper similar terms used standard machine learning society refer calculation weights model parameter learning training. refer process applying existing model weights known onto solving real-world problem inference testing network invert state proceed test next unit. procedure called asynchronous update procedure obviously subject sequential order selection units. counterpart known synchronous update network ﬁrst tests units inverts unit-to-invert simultaneously. methods lead local optimal. synchronous update even result increasing energy converge oscillation loop states. distinct disadvantage hopﬁeld network cannot keep memory eﬃcient network units store memory bits. network units edges. addition storing memories connection integer value range thus number bits required store units therefore safely draw conclusion although hopﬁeld network remarkable idea enables network memorize data extremely ineﬃcient practice. follow-ups invention hopﬁeld network many works attempted study increase capacity original hopﬁeld network despite attempts made hopﬁeld network still gradually fades society. replaced models inspired immediately following section discuss popular boltzmann machine restricted boltzmann machine study models upgraded initial ideas hopﬁeld network evolve replace boltzmann distribution named ludwig boltzmann investigated extensively originally used describe probability distribution particles system various possible states following stands state corresponding energy. boltzmann’s constant thermodynamic temperature respectively. naturally ratio distribution characterized diﬀerence energies following figure illustration boltzmann machine. introduction hidden units model conceptually splits parts visible units hidden units. dashed line used highlight conceptual separation. mentioned previously boltzmann machine stochastic with-hidden-unit version hopﬁeld network. figure introduces idea hidden units introduced turns hopﬁeld network boltzmann machine. boltzmann machine visible units connected data hidden units used assist visible units describe distribution data. therefore model conceptually splits visible part hidden part still maintains fully connected network among units. stochastic introduced boltzmann machine improved hopﬁeld network regarding leaping local optimum oscillation states. inspired physics method transfer state regardless current energy introduced state state regardless current state following probability es=. stands temperature. idea inspired physics process higher temperature likely state transfer. addition probability higher energy state transferring lower energy state always greater reverse process. idea highly related popular optimization algorithm called simulated annealing back then simulated annealing hardly relevant nowadays deep learning society. regardless historical importance term introduces within section assume constant sake simpliﬁcation. stands visible units stands hidden units. equation also connects back equation except boltzmann machine splits energy function according hidden units visible units. boltzmann machine trained stable state called thermal equilibrium distribution probabilities remain constant distribution energy constant. however probability visible unit hidden unit vary energy minimum. related thermal equilibrium deﬁned constant factor distribution part system. thermal equilibrium hard concept understand. imagine pouring water bottle pouring cold water onto water. start bottle feels bottom feels cold gradually bottle feels mild cold water water heat transferred. however temperature bottle becomes mild stably necessarily mean molecules cease move respect joint distribution variable states. however calculating expectations generally infeasible realistically-sized model involves summing huge number possible states/conﬁgurations. general approach solving problem markov chain monte carlo approximate sums equation diﬀerence expectation value product states data visible states expectation product states data fed. ﬁrst term calculated taking average value energy function gradient visible hidden units driven observed data samples. practice ﬁrst term generally straightforward calculate. calculating second term generally complicated involves running markov chains reach current models equilibrium distribution taking average energy function gradient based samples. restricted boltzmann machine originally known harmonium invented smolensky version boltzmann machine restriction connections either visible units hidden units. figure illustration restricted boltzmann machine achieved based boltzmann machine connections hidden units well connections visible units removed model becomes bipartite graph. restriction introduced energy function much simpler figure illustration restricted boltzmann machine. restriction connections hidden units connections visible units boltzmann machine turns restricted boltzmann machine. model bipartite graph. still trained boltzmann machine trained. since energy function much simpler sampling method used infer second term equation becomes easier. despite relative simplicity learning procedure still requires large amount sampling steps approximate model distribution. denote data distribution denote model distribution. notations remain unchanged. therefore diﬃculty mentioned methods learn parameters requires potentially inﬁnitely many sampling steps approximate model distribution. hinton overcame issue magically introduction method named contrastive divergence. empirically found perform inﬁnitely many sampling steps converge model distribution ﬁnite steps sampling enough. therefore equation eﬀectively re-written into biased algorithm ﬁnite cannot represent model distribution. however empirical results suggested ﬁnite approximate model distribution well enough resulting small enough bias. addition algorithm works well practice strengthened idea contrastive divergence. reasonable modeling power fast approximation algorithm quickly draws great attention becomes fundamental building blocks deep neural networks. following sections introduce distinguished deep neural networks built based rbm/boltzmann machine namely deep belief nets deep boltzmann machine. figure shows structure three-layer deep belief networks. diﬀerent stacking allows bi-directional connections layer following bottom layers top-down connections. probably better understand think multi-layer generative models. despite fact generally described stacked quite diﬀerent putting other. probably appropriate think one-layer extended layers specially devoted generating patterns data. layerwise pre-training success deep belief network largely introduction layer-wised pretraining. idea simple reason works still attracts researchers. pre-training simply ﬁrst train network component component bottom treating ﬁrst layers train treat second layer third layer another train parameters. addition deep belief networks pretraining mechanism also inspires pretraining many classical models including autoencoders deep boltzmann machines models inspired classical models like pre-training performed ﬁne-tuning carried optimize network search parameters lead lower minimum. deep belief networks diﬀerent tuning strategies dependent goals network. fine tuning generative model fine-tuning generative model achieved contrastive version wake-sleep algorithm algorithm intriguing reason designed interpret brain works. scientists found sleeping critical process brain function seems inverse version learn awake. wake-sleep algorithm also steps. wake phase propagate information bottom adjust top-down weights reconstructing layer below. sleep phase inverse wake phase. propagate information adjust bottom-up weights reconstructing layer above. contrastive version wake-sleep algorithm contrastive divergence phase wake phase sleep phase. wake phase goes visible layer sample contrastive divergence sleep phase starts visible layer rbm. figure shows three layer deep boltzmann machine distinction between mentioned previous section allows bidirectional connections bottom layers. therefore represents idea stacking rbms much better although might clearer named deep restricted boltzmann machine. acronyms suggest deep boltzmann machine deep belief networks many similarities especially ﬁrst glance. deep neural networks originates idea restricted boltzmann machine. bidirectional structure grants possibility learn complex pattern data. also grants possibility approximate inference procedure incorporate top-down feedback addition initial bottom-up pass allowing deep boltzmann machines better propagate uncertainty ambiguous inputs. lake introduces bayesian program learning framework simulate human learning abilities large scale visual concepts. addition performance one-shot learning classiﬁcation task model passes visual turing test terms generating handwritten characters worlds alphabets. words generative performance model indistinguishable human’s behavior. deep neural model itself model outperforms several concurrent deep neural networks. deep neural counterpart bayesian program learning framework surely expected even better performance. conditional image generation also another interesting topic recently. problem usually solved pixel networks pixel however given part image seems simplify generation task. section start discuss diﬀerent family models convolutional neural network family. distinct family previous section convolutional neural network family mainly evolves knowledge human visual cortex. therefore section ﬁrst introduce important reasons account success convolutional neural networks vision problems bionic design replicate human vision system. nowadays convolutional neural networks probably originate design rather early-stage ancestors. background set-up brieﬂy introduce successful models make themselves famous imagenet challenge last present known problems vision task guide future research directions vision tasks. convolutional neural network widely known inspired visual cortex however except publications discuss inspiration brieﬂy resources present inspiration thoroughly. section focus discussion basics visual cortex lays ground study convolutional neural networks. visual cortex brain located occipital lobe located back skull part cerebral cortex plays important role processing visual information. visual information coming goes series brain structures reaches visual cortex. parts visual cortex receive sensory inputs known primary visual cortex also known area visual information managed extrastriate areas including visual areas four also visual areas paper primarily focus visual areas related object recognition known ventral stream consists areas inferior temporal gyrus higher levels ventral stream visual processing associated representation complex object features global shape like face perception deliberately discuss components connections established technologies convolutional neural network interested developing powerful models encouraged investigate components. figure brief illustration ventral stream visual cortex human vision system. consists primary visual cortex visual areas inferior temporal gyrus. sociation area. receives strong feedforward connections sends strong connections later areas. cells tuned extract mainly simple properties visual signals orientation spatial frequency colour complex properties. like simple geometric shapes addition orientation spatial frequency color. also shown strong attentional modulation also receives direct input form object comparing processed information stored memories objects identify object words performs semantic level tasks like face recognition. many descriptions functions visual cortex revive recollection convolutional neural networks readers exposed relevant technical literature. later section discuss details convolutional neural networks help build explicit connections. even readers barely besides convolutional neural networks visual cortex inspiring works computer vision long time. example built neural model inspired primary visual cortex another granularity serre introduced system feature detections inspired visual cortex. ladurantaye published book describing models information processing visual cortex. poggio serre conducted comprehensive survey relevant topic didn’t focus particular subject detail survey. section discuss connections visual cortex convolutional neural networks details. begin neocogitron borrows ideas visual cortex later inspires convolutional neural network. neocogitron proposed fukushima generally seen model inspires convolutional neural networks computation side. neural network consists diﬀerent kinds layers s-layer consists number s-cells inspired cell primary visual cortex. serves feature extractor. s-cell ideally trained responsive particular feature presented receptive ﬁeld. generally local features edges particular orientations extracted lower layers global features extracted higher layers. structure highly resembles human conceive objects. c-layer resembles complex cell higher pathway visual cortex. mainly introduced shift invariant property features extracted s-layer. parameter learning process parameters s-layer updated. neocogitron also trained unsupervisedly good feature extractor s-layers. training process s-layer similar hebbian learning rule strengthens connections s-layer c-layer whichever s-cell shows strongest response. training mechanism also introduces problem hebbian learning rule introduces strength connections saturate solution also introduced fukushima introduced name inhibitory cell. performed function normalization avoid problem. proceed neocogitron convolutional neural network. first introduce building components convolutional layer subsampling layer. assemble components present convolutional neural network using lenet example. convolution operation strictly mathematical operation treated equally operations like addition multiplication discussed particularly machine learning literature. however still discuss completeness readers familiar convolution mathematical operation functions produces third function integral expresses amount overlap function shifted function described formally following showed figure leftmost matrix input matrix. middle usually called kernel matrix. convolution applied matrices result showed rightmost matrix. convolution process element-wise product convoluted result invariant property plays critical role vision problem ideal case recognition result changed shift rotation features. critical property used solved elegantly lowe convolutional neural network brought performance level. convolution operation usually known kernels. diﬀerent choices kernels diﬀerent operations images could achieved. operations typically including identity edge detection blur sharpening etc. introducing random matrices convolution operator interesting properties might discovered. figure illustration example kernels applied ﬁgure. diﬀerent kernels applied fulﬁll diﬀerent tasks. random kernels also applied transform image interesting outcomes. figure shows edge detection central tasks primary visual cortex fulﬁlled clever choice kernels. furthermore clever selection kernels lead success replication visual cortex. result learning meaningful convolutional kernel central tasks convolutional neural networks applied vision tasks. also explains section devoted model widely recognized ﬁrst convolutional neural network lenet invented inspired neocogitron. section introduce convolutional neural network introducing lenet. figure shows illustration architecture lenet. consists pairs convolutional layer subsampling layer connected fully connected layer layer classiﬁcation. convolutional layer primarily layer performs convolution operation. discussed previously clever selection convolution kernel eﬀectively simulate task visual cortex. convolutional layer introduces another operation convolution assist simulation successful non-linearity transform. subsampling layer performs simpler task. samples input every region looks into. diﬀerent strategies sampling considered like max-pooling average-pooling even probabilistic pooling sampling turns input representations smaller manageable embeddings. importantly sampling makes network invariant small transformations distortions translations input image. small distortion input change outcome pooling since take maximum/average value local neighborhood. lenet known ability classify digits handle variety diﬀerent problems digits including variances position scale rotation squeezing digits even diﬀerent stroke width digit. meanwhile introduction lenet lecun also introduces mnist database later becomes standard benchmark digit recognition ﬁeld. success lenet convolutional neural network shown great potential solving vision tasks. potentials attracted large number researchers aiming solve vision task regarding object recognition cifar classiﬁcation imagenet challenge along path several superstar milestones attracted great attentions applied ﬁelds good performance. section brieﬂy discuss models. lenet starts convolutional neural networks alexnet invented krizhevsky starts used imagenet classiﬁcation. alexnet ﬁrst evidence perform well historically diﬃcult imagenet dataset performs well leads society competition developing cnns. success alexnet unique design architecture also clever mechanism training. avoid computationally expensive training process alexnet split streams trained gpus. also used data augmentation techniques consist image translations horizontal reﬂections patch extractions. recipe alexnet shown figure however rarely lessons learned architecture alexnet despite remarkable performance. even unfortunately fact particular architecture alexnet wellgrounded theoretical support pushes many researchers blindly burn computing resources blind competition exploring diﬀerent architectures simonyan zisserman showed simplicity promising direction model named vgg. although deeper models around time architecture extremely spatial size input volumes layer decrease result convolutional pooling layers depth volumes increases increased number ﬁlters behavior reinforces idea shrink spatial dimensions grow depth. winner imagenet competition year googlenet introduced several important concepts like inception module concept later used r-cnn arbitrary/creative design architecture barely contribute society especially considering residual following path imagenet challenge unprecedented level. residual layer network times deeper usually seen time invented following path introduces resnet explores deeper structure simple layer. however naively breakthrough resnet introduces allows resnet substantially deeper previous networks called residual block. idea behind residual block input certain layer passed component layers later either following traditional path involves convolutional layers relu transform succession going express directly passes there. result input component layers later instead typically seen idea residual block illustrated figure complementary work validated residual blocks essential propagating information smoothly therefore simpliﬁes optimization. also extended resnet -layer version success cifar data set. another interesting perspective resnet provided showed resnet behave behaves like ensemble shallow networks express introduced allows resnet perform collection independent networks network signiﬁcantly shallower integrated resnet itself. also explains gradient passed ultra-deep architecture without vanished. another work directly relevant resnet help understand conducted hariharan showed features lower layers informative addition summarized ﬁnal layer. resnet still completely vacant clever designs. number layers whole network number layers residual block allows identity bypass still choices require experimental validations. nonetheless extent resnet shown critical reasoning help development better blind experimental trails. addition idea residual block found actual visual cortex although resnet designed according ﬁrst place. introduction state-of-the-art neural models successful challenges canziani conducted comprehensive experimental study comparing models. upon comparison showed still room improvement fully connected layers show strong ineﬃciencies smaller batches images. resnet story. models techniques appear every push limit cnns further. example zhang took step residual block inside residual block. zagoruyko komodakis attempted decrease depth network increasing width. however incremental works kind scope paper. would like story convolutional neural networks current challenges fundamental vision problems able solved naively investigation machine learning techniques. convolutional neural networks reached unprecedented accuracy object detection. however still industry reliable application intriguing properties found szegedy szegedy showed could force deep learning model misclassify image simply adding perturbations image. importantly perturbations even observed naked human eyes. words objects look almost human recognized diﬀerent objects well-trained neural network also shown property likely modeling problem contrast problems raised insuﬃcient training. hand nguyen showed could generate patterns convey almost information human recognized objects neural networks high conﬁdence since neural networks typically forced make prediction surprising network classify meaningless patter something however high conﬁdence indicate fundamental diﬀerences neural networks human learn know world. figure shows examples aforementioned works. construction show neural networks misclassify object easily recognized human something unusual. hand neural network also classify weird patterns believed objects human something familiar with. properties restrict usage deep learning real world applications reliable prediction necessary. even without examples also realize reliable prediction neural networks could issue fundamental property matrix existence null space. long perturbation happens within null space matrix able alter image dramatically neural network still makes figure illustrations mistakes neural networks. adversarial images generated based original images. diﬀerences original ones un-observable naked neural network successfully classify original ones fail adversarial ones. patterns generated. neural network classify school guitar peacock pekinese respectively. blind spot discourage promising future neural networks. contrary makes convolutional neural network resemble human vision system deeper level. human vision system blind spots also exist interesting work might seen linking ﬂaws human vision system defects neural networks helping overcome defects future. last present misclassiﬁed images resnet imagenet challenge. hopefully examples could inspire methodologies invented fundamental vision problem. figure shows misclassiﬁed images resnet applied imagenet challenge. labels provided human eﬀort unexpected even many humans. therefore error rate resnet probably hitting limit since labeling preference annotator harder predict actual labels. example figure labeled tiny part image important contents expressed image. hand figure annotated background image image obviously centering object. improve performance resnet reached direction might modeling annotators’ labeling preference. assumption could annotators prefer label image make distinguishable. established work modeling human factors could helpful. however important question whether worth optimizing model increase testing results imagenet dataset since remaining misclassiﬁcations result incompetency model problems annotations. introduction data sets like coco flickr visualgenome open vision problems competitive challenges. however fundamental problems experiences section introduces never forgotten. section start discuss family deep learning models attracted many attentions especially tasks time series data sequential data. recurrent neural network class neural network whose connections units form directed cycle; nature grants ability work temporal data. also discussed literature like paper continue oﬀer complementary views surveys emphasis evolutionary history milestone models provide insights future direction coming models. discussed previously hopﬁeld network widely recognized recurrent neural network although formalization distinctly diﬀerent recurrent neural network deﬁned nowadays. therefore despite literature tend begin discussion hopﬁeld network treat member family avoid unnecessary confusion. model later referred jordan network. simple neural network hidden layer input denoted weights hidden layer denoted weights output layer denoted weights recurrent computation denoted hidden representation denoted output denoted jordan network formulated years later another introduced elman formalized recurrent structure slightly diﬀerently. later network known elman network. elman network formalized following diﬀerence whether information previous time step provided previous output previous hidden layer. diﬀerence illustrated figure diﬀerence illustrated respect historical contribution works. notice fundamental diﬀerence structures since wyht therefore diﬀerence lies choice nevertheless step jordan network elman network still remarkable introduces possibility passing information hidden layers signiﬁcantly improve ﬂexibility structure design later work. intuitively solution unfold recurrent structure expand feedforward neural network certain time steps apply traditional backpropagation onto unfolded neural network. solution known backpropagation time independently invented several researchers including however recurrent neural network usually complex cost surface naive backpropagation work well. later paper recurrent structure introduces critical problems example vanishing gradient problem makes optimization great challenge society. unfold structure feedforward neural network inﬁnite depth. therefore build conceptual connection feedforward network inﬁnite layers. since neural network history bidirectional neural networks playing important roles follow-up question recurrent structures correspond inﬁnite layer bidirectional models are. answer bidirectional recurrent neural network. figure unfolded structured brnn. temporal order left right. hidden layer unfolded standard rnn. hidden layer unfolded simulate reverse connection. bidirectional recurrent neural network invented schuster paliwal goal introduce structure unfolded bidirectional neural network. therefore applied time series data information passed following natural temporal sequences information also reversely provide knowledge previous time steps. figure shows unfolded structure brnn. hidden layer unfolded standard rnn. hidden layer unfolded simulate reverse connection. transparency applied emphasize unfolding concept used illustration purpose. actual model handles data diﬀerent time steps single model. introduction recurrent connections back future backpropagation time longer directly feasible. solution treat model combination rnns standard reverse apply bptt onto them. weights updated simultaneously gradients computed. another breakthrough family introduced year brnn. hochreiter schmidhuber introduced neuron family named long short-term memory invented term lstm used refer algorithm designed overcome vanishing gradient problem help special designed memory cell. nowadays lstm widely used denote recurrent network memory cell nowadays referred lstm cell. lstm introduced overcome problem rnns cannot long term dependencies overcome issue requires specially designed memory cell illustrated figure input gate forget gate computed described equation equation figure shows output gate computed described equation figure shows internal state updated described equation figure shows output hidden state updated described equation weights parameters need learned training. therefore theoretically lstm learn memorize long time dependency necessary learn forget past necessary making powerful model. important theoretical guarantee many works attempted improve lstm. example gers schmidhuber added peephole connection allows gate information internal state. introduced gated recurrent unit known simpliﬁed lstm merging internal state hidden state state merging forget gate input gate simple update gate. integrating lstm cell bidirectional also intuitive follow-up look interestingly despite novel lstm variants proposed then greﬀ conducted large-scale experiment investigating performance lstms conclusion none variants improve upon standard lstm architecture signiﬁcantly. probably improvement lstm another direction rather updating structure inside cell. attention models seem direction attention models loosely based bionic design simulate behavior human vision attention mechanism humans look image scan stare whole image focus major part gradually build context capturing gist. attention mechanisms ﬁrst discussed larochelle hinton denil attention models mostly refer models introduced machine translation soon applied many diﬀerent domains like speech recognition image caption generation. attention models mostly used sequence output prediction. instead seeing whole sequential data make single prediction model needs make sequential prediction sequential input tasks like machine translation image caption generation. therefore attention model mostly used answer question attention based previously predicted labels hidden states. output sequence linked one-to-one input sequence input data even sequence. therefore usually encoder-decoder framework necessary. encoder used encode data representations decoder used make sequential predictions. attention mechanism used locate region representation predicting label current time step. figure shows basic attention model encoder-decoder network structure. representation encoder encodes accessible attention model attention model selects regions pass onto lstm cell usage prediction making. figure unfolded structured attention model. transparency used show unfolding conceptual. representation encoder learns available decoder across time steps. attention module selects pass onto lstm cell prediction. formalize works denote encoded representation denote hidden states lstm cell. then attention module generate unscaled weights region encoded representation problem hard attention sampling multinoulli distribution diﬀerentiable. therefore gradient based method hardly applied. variational methods policy gradient based method considered. although recurrent neural network suﬀers many issues deep neural network recurrent connections current rnns still deep models regarding representation learning compared models families. pascanu formalizes idea constructing deep rnns extending current rnns. figure shows three diﬀerent directions construct deep recurrent neural network increasing layers input component recurrent component output component respectively. rnns improved variety diﬀerent ways like assembling pieces together conditional random field together components addition convolutional operation directly built lstm resulting convlstm convlstm also connected variety diﬀerent components fundamental problems training rnns vanishing/exploding gradient problem introduced detail problem basically states traditional activation functions gradient bounded. gradients computed backpropagation following chain rule error signal decreases exponentially within time steps bptt trace back long-term dependency lost. lstm relu known good solutions vanishing/exploding gradient problem. however solutions introduce ways bypass problem clever design instead solving fundamentally. methods work well practically fundamental problem general still solved. pascanu attempted solutions still done. primary focus paper deep learning models. however optimization inevitable topic development history deep learning models. section brieﬂy revisit major topics optimization neural networks. introduction models algorithms discussed along models. here discuss remaining methods mentioned previously. despite fact neural networks developed ﬁfty years optimization neural networks still heavily rely gradient descent methods within algorithm backpropagation. paper intend introduce classical backpropagation gradient descent method stochastic version batch version simple techniques like momentum method starts right topics. rprop introduced riedmiller braun unique method even studied back today fully utilize information gradient considers sign words updates parameters following unique formalization allows gradient method overcome cost curvatures easily solved today’s dominant methods. two-decade-old method worth study days. follows idea introducing adagrad introduced duchi adaptive learning rate mechanism assigns higher learning rate parameters updated mildly assigns lower learning rate parameters updated dramatically. measure degree update applied norm historical gradients therefore update rule adagrad showed great improvement robustness upon traditional gradient method however problem norm accumulates fraction norm decays substantial small term. adadelta extension adagrad aims reducing decaying rate learning rate proposed instead accumulating gradients time step adagrad adadelta re-weights previously accumulation adding current term onto previously accumulated result resulting adam stands adaptive moment estimation proposed adam like combination momentum method adagrad method component re-weighted time step formally time step have modern gradient variants published promising claim helpful improve convergence rate previous methods. empirically methods seem indeed helpful however many cases good choice methods seems beneﬁt limited extent. dropout introduced technique soon inﬂuential good performance also simplicity implementation. idea simple randomly dropping units training. formally training case hidden unit randomly omitted network probability suggested hinton dropout seen eﬃcient perform model averaging across large number diﬀerent neural networks overﬁtting avoided much less cost computation. actual performance introduces dropout soon became popular upon introduction work attempted understand mechanism diﬀerent perspectives including also applied train models like batch normalization introduced ioﬀe szegedy another breakthrough optimization deep neural networks. addressed problem named internal covariate shift. intuitively problem understood following steps learned function barely useful input changes layer function changes parameters layers change input current layer. change could dramatic shift distribution inputs. follow-up proposes technique layer normalization transpose batch normalization layer normalization computing mean variance used normalization summed inputs neurons layer single training case. therefore technique nature advantage applicable recurrent neural network straightforwardly. however seems transposed batch normalization cannot implemented simple batch normalization. therefore become inﬂuential batch normalization last section optimization techniques neural networks revisit methods attempted learn optimal model architecture. many methods known constructive network approaches. methods proposed decades raise enough impact back then. nowadays powerful computation resources people start consider methods again. remarks need made proceed obviously methods trace back counterparts non-parametric machine learning ﬁeld methods perform enough raise impact focusing discussion evolutionary path mislead readers. instead list methods readers seek inspiration. many methods exclusively optimization techniques methods usually proposed particularly designed architecture. technically speaking methods distributed previous sections according models associated. however methods barely inspire modern modeling research chance inspire modern optimization research list methods section. earliest important works topic proposed fahlman lebiere introduced model well corresponding algorithm named cascade-correlation learning. idea algorithm starts minimum network builds towards bigger network. whenever another hidden unit added parameters previous hidden units ﬁxed algorithm searches optimal parameter newly-added hidden unit. interestingly unique architecture cascade-correlation learning grants network grow deeper wider time every newly added hidden unit takes data together outputs previously added units input. important questions algorithm parameters current hidden units proceed tune newly added terminate entire algorithm. questions answered similar manner algorithm adds hidden unit signiﬁcant changes existing architecture terminates overall performance satisfying. training process introduce problems overﬁtting might account fact method seen much modern deep learning research. m´ezard nadal presented idea tiling algorithm learns parameters number layers well number hidden units layer simultaneously feedforward neural network boolean functions. later algorithm extended multiple class version parekh algorithm works every layer tries build layer hidden units cluster data diﬀerent clusters label cluster. algorithm keeps increasing number hidden units clustering pattern achieved proceed another layer. m´ezard nadal also oﬀered proof theoretical guarantees tiling algorithm. basically theorem says tiling algorithm greedily improve performance neural network. frean proposed upstart algorithm. long story short algorithm simply neural network version standard decision tree tree node replaced linear perceptron. therefore tree seen neural network uses core component neural networks tree node. result standard building tree advertised building neural network automatically. evolutionary algorithm family algorithms uses mechanisms inspired biological evolution search parameter space optimal solution. prominent examples family genetic algorithm simulates natural selection colony optimization algorithm simulates cooperation colony explore surroundings. oﬀered extensive survey usage evolution algorithm upon optimization neural networks introduced several encoding schemes enable neural network architecture learned evolutionary algorithms. encoding schemes basically transfer network architecture vectors standard algorithm take input optimize discussed representative algorithms aimed learn network architecture automatically. algorithms eventually fade modern deep learning research conjecture main reasons outcome algorithms tend overﬁt data. algorithms following greedy search paradigm unlikely optimal architecture. however rapid development machine learning methods computation resources last decade hope constructive network methods listed still inspire readers substantial contributions modern deep learning research. paper revisited evolutionary path nowadays deep learning models. revisited paths three major families deep learning models deep generative model family convolutional neural network family recurrent neural network family well topics optimization techniques. paper could serve goals first documents major milestones science history impacted current development deep learning. milestones limited development computer science ﬁelds. importantly revisiting evolutionary path major milestone paper able suggest readers remarkable works developed among thousands contemporaneous publications. brieﬂy summarize three directions many milestones pursue models layering architecture onto another hoping backpropagation optimal parameters history says masterminds tend think simple dropout widely recognized performance simplicity implementation intuitive reasoning. hopﬁeld network restricted boltzmann machine models simpliﬁed along iterations ready piled-up. model proposed substantially parameters contemporaneous ones must solve problem others solve nicely remarkable. lstm much complex traditional bypasses vanishing gradient problem nicely. deep belief network famous fact ﬁrst come idea putting onto another come algorithm allow deep architectures trained eﬀectively. learning statistics ﬁeld. human visual cortex greatly inspired development convolutional neural networks. even recent popular residual networks corresponding mechanism human visual cortex. generative adversarial network also connection game theory developed ﬁfty years ago. thanks demo http//beej.us/blog/data/convolution-image-processing/ quick generation examples figure thanks bojian carnegie mellon university examples figure thanks blog http//sebastianruder.com/optimizinggradient-descent/index.html summary gradient methods section thanks yutong zheng xupeng tong carnegie mellon university suggesting relevant contents.", "year": 2017}