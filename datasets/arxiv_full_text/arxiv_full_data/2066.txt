{"title": "Bounded Planning in Passive POMDPs", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In Passive POMDPs actions do not affect the world state, but still incur costs. When the agent is bounded by information-processing constraints, it can only keep an approximation of the belief. We present a variational principle for the problem of maintaining the information which is most useful for minimizing the cost, and introduce an efficient and simple algorithm for finding an optimum.", "text": "passive pomdp viewed inference quality measured cost function. examples passive pomdps include various gambling scenarios stock exchange horse racing betting aﬀect world state. settings reward depends directly amount information agent world state world state given observable history belief minimal suﬃcient statistic therefore keeps relevant information. computed sequentially forward algorithm starting step updating passive pomdps actions aﬀect world state still incur costs. agent bounded information-processing constraints keep approximation belief. present variational principle problem maintaining information useful minimizing cost introduce eﬃcient simple algorithm ﬁnding optimum. planning partially observable markov decision processes important task reinforcement learning models agent’s interaction environment discrete-time stochastic process. environment goes sequence standard pomdp agent chooses action aﬀects next world state incurs cost. consider passive pomdps action aﬀects cost world state. assume world markov chain states governed time-independent transition several works optimizing ﬁnite-state controller given size belief represented state controller posterior probability world state given memory state. diﬀerent approach explicitly select subset beliefs guide iterations another reduce dimension belief space principle components paper present novel setting planning passive pomdps constrained information capacities. setting allows treatment reinforcement learning information-theoretic framework. also provide principled method belief approximation general pomdps. ﬁxed action policy pomdp becomes passive pomdp bounded inference policy computed. reduces belief space turn guides action planning. decoupling similar chrisman explored future work. research treats pomdps cost distributions next world state controlled uncontrolled interesting analogies setting. information-rate constraints deﬁne eﬀect components cost between distribution next memory state marginals tishby polani combine similar information-rate constraints perception action together. future work explore exploit symmetry special case memory information rate unconstrained. minimal possible. however agent operates capacity constraints channels external cost parallels distortion rate-distortion theory internal costs information rates. agent actually needs minimize combination costs. note agent generally information next observation even seeing i.e. independent. agent therefore freedom choosing part information common remembers part forgets observes anew. suﬃciency exact belief allows agent minimize external cost incurs signiﬁcant internal costs. amount information agent needs keep memory large even observation agent grasp. anyway information equally useful reducing external costs. requirement agent keeps suﬃcient statistics exact beliefs unrealistic. rather agent’s memory must statistic sufﬁcient still good sense keeps external cost low. also want minimal level quality terms information-processing rates agent keeps information useful enough. step individually exactly notion captured rate-distortion theory sequential extension main results paper formulation setting described above introduction eﬃcient simple algorithm solve prove algorithm converges local optimum demonstrate simulations tradeoﬀ memory sensing intrinsic setting. application results previously studied problems comparison existing algorithms left future work. paper organized follows. section formulate setting information-theoretical terms. section solve problem step ﬁnding variational principle eﬃcient optimization algorithm. section analyze complete sequential problem introduce algorithm solve section show simulations solution. unconstrained planning passive pomdps easily done maintaining exact belief choosing action minimize subjective expected cost. planning general pomdps harder aspect size belief space. many algorithms plan eﬃciently approximately focusing subset space. dkl; ¯qn) dkl; ¯qn) dkl; ¯qn) mn−on normalization constraints. waive constraints non-negative probabilities essentially never active shall later. also note optimize distinct parameters. justiﬁed theorem states that optimum indeed marginals emergence three information-rate constraints channels similar spirit multiterminal source coding terminology agent needs implement lossy coding correlated sources capacity constraints minimize average expected distortion. main diﬀerence chose allow encoding distributed keeping ability memory interact perception biological agents consider long-term planning required agent problem focus choice ﬁnal step given transitions given joint distribution deﬁne joint belief joint distribution positive information-rate pair having independent makes information-rate constraints inactive. satisﬁes slater condition multipliers detailed theorem karush-kuhn-tucker multipliers necessary optimal. corollary boundaries rate-distortion regions obtained keeping three information-rate constraints. sake clarity rest paper strict convexity uniqueness minimum convergence taken respect events transitions positive probability justiﬁed theorem proof. ﬁxed convex since terms convex. non-zero terms involve prθn focusing parameters distortion terms linear information terms strictly convex. unique feasible follows theorem complementary slackness conditions suﬃcient optimality. table shows conditions information rates solution meets boundary subgradient boundary point. exnote equation linear backward recursion multipliers come constraints probability distribution function. consequence however since independent normalized used compute equation value function. expected cost ln−t+ ﬁxed policy suﬃx consists linear terms expected distortion also strictly convex terms. latter take form kullback-leibler dicost linear makes representation value function challenge greater diﬃculty size policy space ﬁnite discreteaction ﬁnite-horizon pomdps continuous here. minimizing yield piecewise-linear function joint belief although still continuous convex mixing policies shows still concave. unclear ﬁnitely represent resulting value function case. perhaps surprisingly determinism jointbelief allows deﬁne local criterion optimality. together iterations algorithm make local improvements guarantee convergence local optimum. algorithm simple forward-backward algorithm building block forward-backward. iteration compute recursively forward joint beliefs current policy compute recursively backward policy ﬁnding step policy suﬃx locally optimal criterion optimality step either previous step previous iteration whichever leads lower cost chosen. suggests viewing problem joint-belief mdp. states joint beliefs actions next state always follows deterministically according equation determinism allows time-dependent policy rather state-dependent prove useful ﬁnding solution. belief space standard pomdp looked state space belief actions observations linear transition function. memory states approximate beliefs model like abstraction state space distributions belief space. table summarizes main diﬀerences joint-belief belief-mdp representation discrete-action ﬁnite-horizon pomdps. algorithm runs algorithm initialize previous iteration. speed iteration particularly algorithm nearly converged. addition running algorithm diﬀerent sets multipliers converges much faster initialized previous result. empirically also leads much better local minima runs sorted order decreasing multipliers. clarify further ﬁgure shows colored contour boundary. lower distortion higher required information rates. tradeoﬀ memory perception illustrated negative slope contours. work motivated problem planning general pomdps beneﬁt belief approximation principled information theory. application current results problem left future work.", "year": 2012}