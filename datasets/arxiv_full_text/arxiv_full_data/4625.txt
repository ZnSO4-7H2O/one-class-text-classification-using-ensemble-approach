{"title": "Robustness and Regularization of Support Vector Machines", "tag": ["cs.LG", "cs.AI"], "abstract": "We consider regularized support vector machines (SVMs) and show that they are precisely equivalent to a new robust optimization formulation. We show that this equivalence of robust optimization and regularization has implications for both algorithms, and analysis. In terms of algorithms, the equivalence suggests more general SVM-like algorithms for classification that explicitly build in protection to noise, and at the same time control overfitting. On the analysis front, the equivalence of robustness and regularization, provides a robust optimization interpretation for the success of regularized SVMs. We use the this new robustness interpretation of SVMs to give a new proof of consistency of (kernelized) SVMs, thus establishing robustness as the reason regularized SVMs generalize well.", "text": "consider regularized support vector machines show precisely equivalent robust optimization formulation. show equivalence robust optimization regularization implications algorithms analysis. terms algorithms equivalence suggests general svm-like algorithms classiﬁcation explicitly build protection noise time control overﬁtting. analysis front equivalence robustness regularization provides robust optimization interpretation success regularized svms. robustness interpretation svms give proof consistency svms thus establishing robustness reason regularized svms generalize well. support vector machines originated boser traced back early vapnik lerner vapnik chervonenkis continue successful algorithms classiﬁcation. svms address classiﬁcation problem ﬁnding hyperplane feature space achieves maximum sample margin training samples separable leads minimizing norm classiﬁer. samples separable penalty term approximates total training error considered well known minimizing training error lead poor classiﬁcation performance unlabeled data; approach poor generalization error essentially overﬁtting variety modiﬁcations proposed combat problem popular methods minimizing combination training-error regularization term. latter typically chosen norm classiﬁer. resulting regularized classiﬁer performs better data. phenomenon often interpreted statistical learning theory view regularization term restricts complexity classiﬁer hence deviation testing error training error controlled paper consider diﬀerent setup assuming training data generated true underlying distribution non-i.i.d. disturbance added samples observe. follow robust optimization approach i.e. minimizing worst possible empirical error under disturbances. robust optimization classiﬁcation robust classiﬁcation models studied past considered box-type uncertainty sets allow possibility data skewed non-neutral manner correlated disturbance. made diﬃcult obtain non-conservative generalization bounds. moreover explicit connection regularized classiﬁer although high-level known regularization robust optimization related main contribution paper solving robust classiﬁcation problem class non-box-typed uncertainty sets providing linkage robust classiﬁcation standard regularization scheme svms. particular contributions include following permits ﬁner control adversarial disturbance restricting satisfy aggregate constraints across data points therefore reducing possibility highly correlated disturbance. classiﬁcation thus explicitly relating robustness regularization. provides alternative explanation success regularization also suggests physically motivated ways construct regularization terms. chance-constrained classiﬁer show robust formulation approximate less conservatively previous robust formulations could possibly also consider bayesian setup show used provide principled means selecting regularization coeﬃcient without cross-validation. useful standard learning setup using prove consistency standard classiﬁcation without using vc-dimension stability arguments. result implies generalization ability direct result robustness local disturbances; therefore suggests justiﬁcation good performance consequently allows construct learning algorithms generalize well robustifying non-consistent algorithms. comment explicit equivalence robustness regularization. brieﬂy explain observation diﬀerent previous work interesting. certain equivalence relationships robustness regularization established problems classiﬁcation results directly apply classiﬁcation problem. indeed research classiﬁer regularization mainly discusses eﬀect bounding complexity function class meanwhile research robust classiﬁcation attempted relate robustness regularization part robustness formulations used papers. fact consider robustiﬁed versions regularized classiﬁcations. bhattacharyya considers robust formulation box-type uncertainty relates robust formulation regularized svm. however formulation connection robustness regularization context important following reasons. first gives alternative potentially powerful explanation generalization ability regularization term. classical machine learning literature regularization term bounds complexity class classiﬁers. robust view regularization regards testing samples perturbed copy training samples. show total perturbation given bounded regularization term bounds classiﬁcation errors sets samples. contrast standard approach bound depends neither rich class candidate classiﬁers assumption samples picked i.i.d. manner. addition suggests novel approaches designing good classiﬁcation algorithms particular designing regularization term. structural-risk minimization approach regularization chosen minimize bound generalization error based training error complexity term. complexity term typically leads overly emphasizing regularizer indeed approach known often pessimistic problems structure. robust approach oﬀers another avenue. since noise robustness physical processes close investigation application noise characteristics hand provide insights properly robustify therefore regularize classiﬁer. example known normalizing samples variance among features roughly often leads good generalization performance. robustness perspective simply says noise anisotropic rather spherical hence appropriate robustiﬁcation must designed anisotropy. also show using robust optimization viewpoint obtain probabilistic results outside setup. section bound probability noisy training sample correctly labeled. bound considers behavior corrupted samples hence diﬀerent known bounds. helpful training samples testing samples drawn diﬀerent distributions adversary manipulates samples prevent correctly labeled finally connection need point several diﬀerent deﬁnitions robustness literature. paper well aforementioned robust classiﬁcation papers robustness mainly understood robust optimization perspective min-max optimization performed possible disturbances. alternative interpretation robustness stems rich literature robust statistics studies estimator algorithm behaves small perturbation statistics model. example inﬂuence function approach proposed hampel hampel measures impact inﬁnitesimal amount contamination original distribution quantity interest. based notion robustness christmann steinwart showed many kernel classiﬁcation algorithms including robust sense ﬁnite inﬂuence function. similar result regression algorithms shown christmann steinwart smooth loss functions christmann messem non-smooth loss functions relaxed version inﬂuence function applied. machine learning literature another widely used notion closely related robustness stability algorithm required robust speciﬁc perturbation deleting sample training set. well known stable algorithm desirable generalization properties statistically consistent mild technical conditions; example bousquet elisseeﬀ kutin niyogi poggio mukherjee details. main diﬀerence robust optimization robustness notions former constructive rather analytical. contrast robust statistics stability approach measures robustness given algorithm robust optimization robustify algorithm converts given algorithm robust one. example show paper version naive empirical-error minimization well known svm. constructive process approach also leads additional ﬂexibility algorithm design especially nature perturbation known well estimated. structure paper paper organized follows. section investigate correlated disturbance case show equivalence robust classiﬁcation regularization process. develop connections probabilistic formulations section prove consistency result based robustness analysis section kernelized version investigated section concluding remarks given section denote column vectors. given norm denote dual norm i.e. kzk∗ sup{z⊤x|kxk vector positive semi-deﬁnite matrix dimension kxkc denotes √x⊤cx. denote disturbance aﬀecting samples. superscript denote true value uncertain variable true noise sample. non-negative scalars denoted integers denoted brieﬂy explain four reasons motivate robust perturbation setup particular min-max form first explicitly incorporate prior problem knowledge local invariance example vision tasks desirable classiﬁer provide consistent answer input image slightly changes. second situations adversarial opponents manipulate testing samples avoid correctly classiﬁed robustness toward manipulation taken consideration training process alternatively training samples testing samples obtained diﬀerent processes hence standard i.i.d. assumption violated example real-time applications newly generated samples often less accurate time constraints. finally formulations based chance-constraints mathematically equivalent min-max formulation. closed set. second condition atomic uncertainty basically says uncertainty bounded symmetric. particular norm balls ellipsoids centered origin atomic uncertainty sets arbitrary polytope might atomic uncertainty set. sublinear aggregated uncertainty deﬁnition models case disturbances sample treated identically aggregate behavior across multiple samples controlled. interesting examples include non-separable arbitrary functheorem assume yi}m tion sublinear aggregated uncertainty corresponding atomic uncertainty following min-max problem xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxx taking inﬁmum sides establishes equivalence problem probw⊤δ supremum class aﬃne functions observe supδ∈n hence lower semi-continuous. therefore also lower semi-continuous. thus minimum achieved problem problem equivalence corollary explains widely known fact regularized classiﬁer tends robust. speciﬁcally explains observation disturbance noiselike neutral rather adversarial norm-regularized classiﬁer performance often superior box-typed robust classiﬁer hand observation also suggests appropriate regularize come disturbance-robustness perspective. equivalence implies standard regularization essentially assumes disturbance spherical; true robustness yield better regularization-like algorithm. eﬀective regularization term closer investigation data variation desirable e.g. examining variation data solving corresponding robust classiﬁcation problem. example regularize splitting given training samples subsets equal number elements treating disturbed copy other. analyzing direction disturbance magnitude total variation choose proper norm suitable tradeoﬀ parameter. although problem formulated without probabilistic assumptions section brieﬂy explain approaches construct uncertainty equivalently tune regularization parameter based probabilistic information. ﬁrst approach problem approximate upper bound chanceconstrained classiﬁer. suppose disturbance follows joint probability measure chance-constrained classiﬁer given following minimization probability simultaneously. vector η-quantile hinge-loss sample contrast formulation minimizes η-quantile average empirical error. controlling average quantity interest box-type noise formulation overly conservative. thus upper bounded gives additional probabilistic robustness property standard regularized classiﬁer. notice following similar approach constraint-wise robust setup i.e. uncertainty would lead considerably pessimistic approximations chance constraint. cross validation simply using expected value note equivalence corollary makes possible since diﬃcult imagine setting would prior regularization coeﬃcients. previous results easily generalized kernelized setting discuss detail section. particular similar linear classiﬁcation case give interpretation standard kernelized min-max empirical hinge-loss solution disturbance assumed feature space. relate setup disturbance lies sample space. relationship section prove consistency result kernelized svms. proved sch¨olkopf smola take increasing function regularization term optimal solution solved without knowing explicitly feature mapping evaluating kernel function only. theorem assume yi}m linearly separable arbitrary function sublinear aggregated uncertainty corresponding atomic uncertainty following min-max problem widely used feature mappings yi}m empirical error plus penalty term supδ∈nhw however easy show rkhs norm i.e. kzkh =phz noticing rkhs norm self corollary n|pm equation variant form standard squared rkhs norm regularization term shown formulations equivalent changing tradeoﬀ parameter since empirical hinge-loss rkhs norm convex. therefore corollary essentially means standard kernelized implicitly robust classiﬁer disturbance feature-space magnitude disturbance bounded. appendix prove result provides tighter relationship disturbance feature space disturbance sample space kernels. proof expanding rkhs norm yields lemma essentially says certain conditions robustness feature space stronger requirement robustness sample space. therefore classiﬁer achieves robustness feature space also achieves robustness sample space. notice condition lemma rather weak. particular next section consider foundational property robustness sample space show classiﬁer robust sample space asymptotically consistent. consequence result linear classiﬁers results imply consistency broad class kernelized svms. section explore fundamental connection learning robustness using robustness properties re-prove statistical consistency linear classiﬁer kernelized svm. indeed proof mirrors consistency proof found diﬀerence replace metric entropy vc-dimension stability conditions used there robustness condition. thus considered setup training-samples corrupted certain set-inclusive disturbances. turn standard statistical learning setup assuming training samples testing samples generated i.i.d. according probability i.e. exist explicit disturbance. proof brieﬂy explain basic idea proof going technical details. consider testing sample perturbed copy training sample measure magnitude perturbation. testing samples small samples testing samples form pairings exist sample pairs data reused. given training samples testing samples denote largest number pairings. prove theorem need establish following lemma. lemma given mmc/m almost surely uniformly w.r.t. proof make partition either form ···× ×{+} ···× ×{−} partition cartesian product rectangular cell singleton notice training sample testing sample fall form pairing. number training samples testing samples falling respectively. thus multinomially distributed random vectors following distribution. notice multinomially dis) hence borel-cantelli lemma probability event {mmc/m occurs ﬁnitely often mmc/m almost surely. since arbitrarily close zero mmc/m almost surely. observe convergence uniform since depends proceed prove theorem. given training samples testing samples sample pairs notice paired samples total testing error total testing hinge-loss upper bounded shown average testing error upper bounded. ﬁnal step show implies fact random variable given conditional expectation error bounded almost surely statement theorem. make things precise consider ﬁxed generate training samples testing samples respectively shorthand denote random variable ﬁrst training samples. denote imply statistical consistency linear kernelized svms. again bounded suppose training samples generated i.i.d. according unknown distribution supported theorem denote maxx∈x suppose exists continuous non-decreasing function satisfying that proof proof theorem generate testing samples training samples lower-bound number samples form sample pair feature-space; pair consisting training sample testing sample sample space feature space inﬁnite dimensional thus decomposition inﬁnite number bricks. case multinomial random variable argument used proof lemma breaks down. nevertheless able lower bound number sample pairs feature space number sample pairs sample space. deﬁne max{β since continuous notice lemma testing sample training sample belong brick length side min/√n) sample space hence number sample pairs feature sample space. cover ﬁnitely many bricks since then similar argument lemma shows ratio samples form pairs brick converges increases. notice paired samples total testing error hinge-loss upper-bounded notice condition theorem satisﬁed widely used kernels e.g. homogeneous polynominal kernels gaussian rbf. condition requires feature mapping smooth hence preserves locality disturbance i.e. small disturbance sample space guarantees corresponding disturbance feature space also small. easy construct non-smooth kernel functions generalize well. example consider following kernel equals sign provides meaningful prediction testing sample training samples. hence increases testing error remains large regardless tradeoﬀ parameter used algorithm training error made arbitrarily small ﬁne-tuning parameter. proof robustness condition place vc-dimension stability condition used proof main steps. show always exists minimizer expected regularized hinge loss; expected regularized hinge loss minimizer converges expected hinge loss regularizer goes zero; sequence functions asymptotically optimal expected hinge loss also optimal expected loss; expected hinge loss minimizer regularized training hinge loss concentrates around empirical regularized hinge loss. ﬁnal step accomplished using concentration inequalities derived vc-dimension considerations stability considerations. regularized expected hinge-loss. hence empirical distribution samples write respectively. notice objective function svm. denote solution i.e. classiﬁer running samples parameter denote minimizer existence minimizer proved lemma steinwart finally proposition shows step namely approximating hinge loss suﬃcient guarantee approximation bayes loss. thus equation implies risk function converges bayes risk. work considers relationship robust regularized classiﬁcation. particular prove standard norm-regularized classiﬁer fact solution robust classiﬁcation setup thus known results regularized classiﬁers extend robust classiﬁers. best knowledge ﬁrst explicit link regularization robustness pattern classiﬁcation. link suggests norm-based regularization essentially builds robustness sample noise whose probability level sets symmetric moreover structure unit ball respect dual regularizing norm. would interesting understand performance gains possible noise characteristics robust setup used place regularization appropriately deﬁned uncertainty set. based robustness interpretation regularization term re-proved consistency svms without direct appeal notions metric entropy vc-dimension stability. proof suggests ability handle disturbance crucial algorithm achieve good generalization ability. particular smooth feature mappings robustness disturbance observation space guaranteed hence svms achieve consistency. other-hand certain non-smooth feature mappings fail consistent simply kernels robustness feature-space imply robustness observation space.", "year": 2008}