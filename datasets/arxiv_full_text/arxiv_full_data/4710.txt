{"title": "Exchangeable Variable Models", "tag": ["cs.LG", "cs.AI"], "abstract": "A sequence of random variables is exchangeable if its joint distribution is invariant under variable permutations. We introduce exchangeable variable models (EVMs) as a novel class of probabilistic models whose basic building blocks are partially exchangeable sequences, a generalization of exchangeable sequences. We prove that a family of tractable EVMs is optimal under zero-one loss for a large class of functions, including parity and threshold functions, and strictly subsumes existing tractable independence-based model families. Extensive experiments show that EVMs outperform state of the art classifiers such as SVMs and probabilistic models which are solely based on independence assumptions.", "text": "assume variables identical shades exchangeable. evms especially beneﬁcial high-dimensional sparse domains text collaborative ﬁltering problems. exists work tractable models majority focusing tree-width graphical models framework ﬁnite partial exchangeability basic building block tractable probabilistic models seems natural exist. propose exchangeable variable models novel family probabilistic models classiﬁcation probability estimation. probabilistic models built notion conditional independence graphical representation evms ﬁnite partially exchangeable sequences basic components. show evms represent complex positive negative correlations large sets variables parameters without sacriﬁcing tractable inference. parameters evms estimated maximum-likelihood principle assume examples independent identically distributed. develop methods efﬁcient probabilistic inference maximum-likelihood estimation structure learning. introduce mixtures evms family models strictly expressive naive bayes family models efﬁcient learn. mevms represent classiﬁers optimal zero-one loss large class boolean functions including parity threshold functions. extensive experiments show exchangeable variable models combined notion conditional independence effective sequence random variables exchangeable joint distribution invariant variable permutations. introduce exchangeable variable models novel class probabilistic models whose basic building blocks partially exchangeable sequences generalization exchangeable sequences. prove family tractable evms optimal zeroone loss large class functions including parity threshold functions strictly subsumes existing tractable independence-based model families. extensive experiments show evms outperform state classiﬁers svms probabilistic models solely based independence assumptions. conditional independence crucial notion facilitates efﬁcient inference parameter learning probabilistic models. logical algorithmic properties well graphical representations advent graphical models discipline within artiﬁcial intelligence notion ﬁnite exchangeability hand explored basic building block tractable probabilistic models. sequence random variables exchangeable distribution invariant variable permutations. similar conditional independence partial exchangeability generalization exchangeability reduce complexity parameter learning concept facilitates high tree-width graphical models tractable inference. instance graphical models bernoulli variables figure depict typical tree-width models based notion independence. graphical models high tree-width tractable partial exchangeability using notion statistic. deﬁnition sequence random variables distribution domain ﬁnite set. sequence partially exchangeable respect statistic following theorem states joint distribution sequence random variables partially exchangeable respect statistic unique mixture uniform distributions. theorem sequence random variables distribution ﬁnite statistic. moreover uniform distribution partially exchangeable respect theorem provides implicit description distributions challenge speciﬁc families random variables lies ﬁnding statistic respect sequence variables partially exchangeable efﬁcient algorithm compute probabilities case exchangeable sequences discrete random variables particular exchangeable sequences binary random variables explicit description exist well-known statistics literature example three exchangeable binary variables joint distribution then sequence partially exchangeable respect statistic thus write classiﬁcation probability estimation. mevm classiﬁer signiﬁcantly outperforms state classiﬁers numerous high-dimensional sparse data sets. mevms also outperform several tractable graphical model classes typical probability estimation problems orders magnitudes efﬁcient. finite exchangeability best understood context ﬁnite sequence binary random variables ﬁnite number coin tosses. here ﬁnite exchangeability means number heads matters particular order. since exchangeable variables necessarily independent ﬁnite exchangeability model highly correlated variables graphical representation would fully connected graph high treewidth however later number parameters complexity inference remains linear number variables. deﬁnition sequence random variables joint distribution group permutations acting exchangeable paper concerned exchangeable variables examples. literature mostly focused exchangeability inﬁnite sequence random variables. case express joint distribution mixture sequences however ﬁnite sequences exchangeable variables representation inadequate ﬁnite exchangeable sequences approximated finetti style mixtures sequences approximations suitable ﬁnite sequences random variables extendable inﬁnite exchangeable sequence moreover negative correlations modeled ﬁnite case. interesting connections automorphisms graphical models ﬁnite exchangeability alternative approach exchangeability considers relationship sufﬁciency core work. indicator function. hence distribution parameterized unique mixture four processes value number black balls. figure illustrates mixture model. generative process follows. first choose four urns according mixing weights draw three consecutive balls chosen without replacement. propose exchangeable variable models novel family tractable probabilistic models classiﬁcation probability estimation. probabilistic graphical models built notion independence graphical representation evms built notion ﬁnite exchangeability. evms model negative positive correlations would high tree-width graphical models without losing tractability probabilistic inference. relate ﬁnite partial exchangeability tractable probabilistic inference assume every joint assignment computed time poly. proposition partially exchangeable respect poly partial assignment denotes agree variables intersection time poly every every decide exists proposition generalizes probabilistic models computed constant factor undirected graphical models. please note computing conditional probabilities tractable whenever conditions satisﬁed. statistic tractable either conditions fulﬁlled. proposition provides theoretical framework developing tractable non-local potentials. instance exchangeable bernoulli variables complexity marginal inference polynomial follows statistic satisfying conditions since related work cardinality-based potentials mostly focused inference finite exchangeability also speaks marginal inference tractability computing |ste|−. evms model unary potentials using singleton sets exchangeable variables. instances ﬁnite partial exchangeability result tractable probabilistic models exist several examples satisfying conditions beyond ﬁnite exchangeability. supplementary material addition proofs theorems propositions present examples tractable statistics different associated cardinality-based potentials parameters ﬁnite sequences partially exchangeable variables mixture weights parameterization given equation theorem estimating parameters basic components evms crucial task. derive maximum-likelihood estimates mixture weight vectors. theorem sequence random variables joint distribution statistic distinct values partially exchangeable respect estimates examples mle] sequence random variables examples drawn dataˆ generating distribution. order learn structure evms need address problems. problem find subsequences exchangeable respect given tractable statistic identiﬁes individual components tractable inference learning possible. utilize different tractable statistics different components. problem construct graphical models whose potentials previously learned tractable components. order preserve tractability global model restrict class possible graphical structures. present approaches problems learn expressive evms maintaining tractability. ﬁrst address problem focus evms ﬁnitely exchangeable components. fortunately exist several necessary conditions ﬁnite exchangeability sequence random variables. proposition following statements necessary conditions exchangeability ﬁnite sequence random variables necessary conditions exploited assess whether sequence variables ﬁnitely exchangeable. order learn components assume sequence variables exchangeable unless statistical test contradicts necessary conditions ﬁnite exchangeability. instance statistical test deemed expectations variables identical could assume exchangeable. wanted statistical test ﬁnite exchangeability speciﬁc less sensitive would also require conditions and/or hold. please note analogy structure learning conditional independence tests. instead identifying independencies identify ﬁnite exchangeability among random variables. sequence identically distributed variables assumption exchangeability weaker independence. testing whether discrete variables identical mean variance efﬁcient algorithmically. course application necessary conditions ﬁnite exchangeability possible approach learning components evms. turn problem ensure tractability global graphical structure restricted tractable classes chains trees. here focus mixture models where conditioned values latent variable partitioned exchangeable blocks hence value latent variable perform statistical tests problem estimates conditional expectations introduce class evms next section leave complex structures future work. context longitudinal studies repeatedmeasures experiments observation made different times different conditions exist several models taking account correlation between observations assuming identical similar figure combination exchangeable independent variables leads spectrum models. model where conditioned class variables independent model where conditioned class variables exchangeable partition variables exchangeable blocks vary class value. covariance structure subsets variables compound symmetry models however make assumption exchangeability therefore generally facilitate tractable inference. nevertheless ﬁnite exchangeability seen form parameter tying method also applied context hidden markov models neural networks notably statistical relational learning collective graphical models high-order potentials models based non-local potentials. proposition applied learning structure novel tractable instances cgms hops. position design model families combine notions exchangeability independence. instead specifying structure solely models independence characteristics probabilistic model evms also specify sequences variables exchangeable. previous results provide necessary tools learn structure parameters partially exchangeable sequences perform tractable probabilistic inference. possibilities building families exchangeable variable models vast. here focus family mixtures evms generalizing widely used naive bayes model. family probabilistic models therefore also related research extending naive bayes classiﬁer motivation behind novel class evms facilitates tractable maximum-likelihood learning tractable probabilistic inference. line existing work mixture models derive maximum-likelihood estimates fully observed setting examples missing class labels. also discuss expectation maximization algorithm case data partially observed examples missing class labels exist. deﬁnition mixture evms model consists class variable possible values binary attributes specifying partition attributes blocks exchangeable sequences. structure model therefore deﬁned {xi}k attribute partitions class value. model following parameters input number classes training examples parameter specifying stopping criterion. initialization assign random examples mixture component. class value partition variables exchangeable sequences iterate stopping criterion compute hence conditioned class attributes partitioned mutually independent disjoint blocks exchangeable sequences. figure illustrates model family naive bayes model positioned spectrum. here {{x} ...{xn}} spectrum model assumes full exchangeability conditioned class. here xn}} binary attributes number free parameters member mevm family. following theorem provides maximum-likelihood estimates parameters. theorem maximum-likelihood estimates mevm attributes structure {xi}k class variable conditioned assignments attribute variables. probability estimation class latent apply algorithm expectation maximization algorithm initialized assigning random examples mixture components. iteration examples fractionally assigned components block structure parameters updated. finally either previous current structure chosen based maximum likelihood. structure learning step instance apply conditions proposition conditional expectations statistical tests construct since structure chosen containing structure previous iteration convergence algorithm follows structural expectation maximization crucial question expressive novel model family provide analytic answer showing mevms globally optimal zero-one loss large class boolean functions namely conjunctions disjunctions attributes symmetric boolean functions. symmetric boolean functions boolean function whose value depends number ones input class includes threshold functions whose value inputs vectors ones ﬁxed exact-value functions whose value inputs vectors ones ﬁxed counting functions whose value inputs vectors number ones congruent ﬁxed parity functions whose value input vector number ones. deﬁnition bayes rate example lowest zero-one loss achievable classiﬁer example. classiﬁer locally optimal example zero-one loss example equal bayes rate. classiﬁer globally optimal sample locally optimal every example sample. classiﬁer globally optimal problem globally optimal possible samples problem. theorem striking parity function special case function instances linearly separable functions often used examples particularly challenging classiﬁcation problems. optimality symmetric boolean functions holds even model assumes full exchangeability attributes given value class variable known naive bayes classiﬁer globally optimal threshold functions despite linearly separable hence combining conditional independence exchangeability leads highly tractable probabilistic models globally optimal broader class boolean functions. conducted extensive experiments assess efﬁciency effectiveness mevms tractable probabilistic models classiﬁcation probability estimation. major objective comparison mevms naive bayes models. also compare mevms several state classiﬁcation algorithms. probability estimation experiments compare mevms latent naive bayes models several widely used tractable graphical model classes latent tree models. evaluated mevm classiﬁer using synthetic real-world data sets. synthetic data consists training test examples. number ones example parity data generated sampling uniformly random example assigning ﬁrst class second class otherwise. -of- data assigned example ﬁrst class second class otherwise. counting data assigned examples ﬁrst class second class otherwise. exact data assigned example ﬁrst class second class otherwise. used scikit functions load newsgroup train test samples. removed headers footers quotes training test documents. renders classiﬁcation problem difﬁcult leads signiﬁcantly higher zero-one loss classiﬁers. reuters- data considered reuters documents single topic classes least train test example. webkb text data considered classes project course faculty student. text data sets used binary bag-of-word representation resulting feature spaces dimensions. mnist data collection hand-written digits feature value original feature value greater otherwise. polarity data well-known sentiment analysis problem based movie reviews problem classify movie reviews either positive negative. used cross-validation splits provided authors. enron spam data collection e-mails enron corpus divided spam no-spam messages here applied randomized -fold cross validation. apply feature extraction algorithms data sets. table lists properties data sets mean standard deviation number blocks mevms. distinguished two-class multi-class problems. original data classes created two-class problems considering every pair classes separate cross-validation problem. draw distinction want compare classiﬁcation approaches independent particular multi-class strategies exploited necessary condition proposition learn block structure mevm classiﬁers. pair variables class value applied welch’s t-test test null hypothesis variables test’s pvalue less rejected null hypothesis placed different blocks conditioned applied laplace smoothing constant parameter values applied across data sets experiments. classiﬁers used scikit implementations naive bayes.bernoullinb tree.decisiontreeclassiﬁer svm.linearsvc neighbors.kneighborsclassiﬁer. used classiﬁers’ standard settings except naive bayes classiﬁer applied laplace smoothing constant ensure fair comparison standard setting classiﬁers available part scikit documentation. implementations data sets published. table lists results two-class problems. mevm classiﬁer best classiﬁers data sets. exception mnist data difference insigniﬁcant mevm signiﬁcantly outperformed naive bayes classiﬁer data sets. mevm classiﬁer outperformed svms data sets real-world text classiﬁcation problems achieved parity data mevm classiﬁer better random. table shows results multi-class problems. here mevm classiﬁer signiﬁcantly outperforms naive bayes data set. mevm classiﬁer outperformed classiﬁers newsgroup close second reuters- webkb data sets. mevm classiﬁer particularly suitable high-dimensional sparse data sets. hypothesize three reasons. first mevms model negative positive correlations variables. second mevms perform non-linear transformation feature space. third mevms cluster noisy variables blocks exchangeable sequences acts form regularization sparse domains. conducted experiments widely used collection data sets table lists number variables training test examples number blocks mevm models. latent variable’s domain size problem applied initialization mevms models. could compare mevm independent tuning parameters speciﬁc implemented exactly described algorithm step exploited proposition partitioned variables exchangeable blocks performing series welch’s t-tests expectations estimated assigning variables different blocks null hypothesis identical means could rejected signiﬁcance level mevm used laplace smoothing constant average log-likelihood increase iterations less restarted times chose model maximal log-likelihood training examples. validation data. applied four methods clrg clnj regclrg regclnj chose model highest validation log-likelihood. table lists average log-likelihood test data mevm latent naive bayes latent tree chow-liu tree model even without exploiting validation data model tuning mevm models outperformed models ltms data set. mevms achieve highest log-likelihood score data sets. exception jester data mevms either outperformed tied model. results indicate mevms effective higher-dimensional sparse data sets increase log-likelihood signiﬁcant mevms also outperformed models data sets less variables. mevm models exactly number free parameters. since results data sets available tractable model classes also compared mevms spns acmns here mevms outperformed complex spns acmns data sets. however mevms competitive outperform spns acmns data sets. following previous work applied wilcoxon signed-rank test. mevm outperforms models signiﬁcance level difference insignificant compared acmns spns compute probability example mevms require many steps blocks exchangeable variables. hence mevm signiﬁcantly efﬁcient single iteration reach stopping criterion. difference less signiﬁcant problems fewer variables algorithm mevm orders magnitude faster data sets variables. exchangeable variable models provide framework probabilistic models combining notions conditional independence partial exchangeability. result possible efﬁciently learn parameters structure tractable high tree-width models. evms model complex positive negative correlations between large numbers variables. presented theory evms showed particular subfamily optimal several important classes boolean functions. experiments large number data sets veriﬁed mixtures evms powerful highly efﬁcient models classiﬁcation probability estimation. evms potential components deep architectures sum-product networks light theorem exchangeable variable nodes complementing product nodes lead compact representations fewer parameters learn. evms also related graphical modeling perfect graphs addition evms provide insightful connection lifted probabilistic inference active research area concerned exploiting symmetries efﬁcient probabilistic inference. developed principled framework based partial exchangeability important notion structural symmetry. numerous opportunities crossfertilization evms perfect graphical models collective graphical models statistical relational models. directions future work include sophisticated structure learning evms continuous variables evms based instances partial exchangeability ﬁnite exchangeability novel statistical relational formalisms incorporating evms applications evms general theory graphical models exchangeable potentials. acknowledgments many thanks broeck hung daniel lowd helpful discussions. research partly funded grant wnf--- grants n--- n--- afrl contract fa---. views conclusions contained document authors interpreted necessarily representing ofﬁcial policies either expressed implied afrl united states government. references hung huynh tuyen salvo braz rodrigo. exact lifted inference distinct soft evidence every object. proceedings aaai conference artiﬁcial intelligence niepert mathias broeck guy. tractability exchangeability perspective efﬁcient probabilistic inference. proceedings aaai conference artiﬁcial intelligence pang lillian. sentimental education sentiment analysis using subjectivity summarization based minimum cuts. proceedings meeting association computational linguistics rennie jason shih lawrence teevan jaime karger david. tackling poor assumptions naive bayes text classiﬁers. proceedings international conference machine learning gupta rahul diwan ajit sarawagi sunita. efﬁcient inference cardinality-based clique potentials. proceedings international conference machine learning tarlow daniel givoni inmar zemel richard hopmap efﬁcient message passing high order potentials. proceedings conference artiﬁcial intelligence statistics tarlow daniel swersky kevin zemel richard adams ryan frey brendan fast exact inference recursive cardinality models. proceedings conference uncertainty artiﬁcial intelligence haaren davis jesse. markov network structure learning randomized feature generation approach. proceedings aaai conference artiﬁcial intelligence proof. ﬁrst prove statement given partial assignment assume want compute argmaxy construct argmaxxt assumption possible time poly. since argmaxy extract solution linear time. utilize proposition prove probabilistic inference sequence exchangeable binary variables tractable. example exchangeable sequence binary random variables. number partial assignment variables clearly exchangeable respect statistic values first prove every partial assignment variables every decide exists construct time poly. n−k+n exist otherwise possible generate linear time assigning exactly ones unassigned variables ste. hence inference tractable. forms ﬁnite partial exchangeability beyond notion full ﬁnite exchengeability therefore cardinality-based potentials example provide three examples. example ﬁxed constant. sequence binary random variables partition variables subsequences number assignment projected onto variables straight-forward verify poly. moreover arguments similar made example show conditions proposition met. hence marginal inference tractable statistic example sequence binary random variables number times occurs substring consider statistic example also poly. partial assignment variables value statistic. string characters encode assignments variables according character encodes unassigned variables. partition four sets substrings deﬁned s|s| |s|} denotes substring relation. complete partial assignment joint assignment proof. sequence variables consideration. write class value example conjunctions attributes sequence variables part conjunction. conditioned binary class variable either partition variables blocks parameters mevm follows. otherwise; otherwise; example array binary random variables. instance could represent binarized image rows columns. ﬁxed integer constant integer assume without loss generality integer exists. partition original array squares dimension variables square statistic deﬁned then log). second term free parameters hence ﬁnding estimates amounts maximizing ﬁrst sum. equivalent ﬁnding maximum likelihood estimate multinomial solved lagrange multipliers. hence |x|−| then otherwise. moreover otherwise. hence mevm classiﬁer always returns correct class value. similar argument made prove optimality disjunctions attributes. prove second statement consider mevm model binary class variable following block structure. class variable’s values xn}. conditioned class value attributes assumed exchangeable straightforward verify particular mevm learn arbitrary discrete distributions symmetric boolean function.", "year": 2014}