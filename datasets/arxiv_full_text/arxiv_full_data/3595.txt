{"title": "Batch Kalman Normalization: Towards Training Deep Neural Networks with  Micro-Batches", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "As an indispensable component, Batch Normalization (BN) has successfully improved the training of deep neural networks (DNNs) with mini-batches, by normalizing the distribution of the internal representation for each hidden layer. However, the effectiveness of BN would diminish with scenario of micro-batch (e.g., less than 10 samples in a mini-batch), since the estimated statistics in a mini-batch are not reliable with insufficient samples. In this paper, we present a novel normalization method, called Batch Kalman Normalization (BKN), for improving and accelerating the training of DNNs, particularly under the context of micro-batches. Specifically, unlike the existing solutions treating each hidden layer as an isolated system, BKN treats all the layers in a network as a whole system, and estimates the statistics of a certain layer by considering the distributions of all its preceding layers, mimicking the merits of Kalman Filtering. BKN has two appealing properties. First, it enables more stable training and faster convergence compared to previous works. Second, training DNNs using BKN performs substantially better than those using BN and its variants, especially when very small mini-batches are presented. On the image classification benchmark of ImageNet, using BKN powered networks we improve upon the best-published model-zoo results: reaching 74.0% top-1 val accuracy for InceptionV2. More importantly, using BKN achieves the comparable accuracy with extremely smaller batch size, such as 64 times smaller on CIFAR-10/100 and 8 times smaller on ImageNet.", "text": "figure illustrates distribution estimation conventional batch normalization minibatch statistics estimated based currently observed mini-batch k-th layer. clarity notation indicate mean covariance matrix respectively. note diagonal entries representation normalization. proposed batch kalman normalization provides accurate distribution estimation k-th layer aggregating statistics preceding layer. training inference dnns normalizes means variances internal representation hidden layer illustrated figure pointed enables using larger learning rate training leading faster convergence. although signiﬁcance demonstrated many previous works drawback cannot neglected i.e. effectiveness diminishing small mini-batch presented training. consider consisting number layers bottom top. traditional normalization step seeks eliminate change distributions internal layers reducing internal covariant shifts. prior normalizing distribution layer ﬁrst estimates statistics including means variances. however impractical expected bottom layer input data pre-estimated training representations internal layers keep changing network parameters updated training step. hence handles issue following schemes. model training approximates population statistics using batch indispensable component batch normalization successfully improved training deep neural networks mini-batches normalizing distribution internal representation hidden layer. however effectiveness would diminish scenario micro-batch since estimated statistics mini-batch reliable insufﬁcient samples. paper present novel normalization method called batch kalman normalization improving accelerating training dnns particularly context microbatches. speciﬁcally unlike existing solutions treating hidden layer isolated system treats layers network whole system estimates statistics certain layer considering distributions preceding layers mimicking merits kalman filtering. appealing properties. first enables stable training faster convergence compared previous works. second training dnns using performs substantially better using variants especially small mini-batches presented. image classiﬁcation benchmark imagenet using powered networks improve upon best-published model-zoo results reaching top- accuracy inceptionv. importantly using achieves comparable accuracy extremely smaller batch size times smaller cifar-/ times smaller imagenet. batch normalization recently become standard crucial component improving training deep neural networks successfully employed harness several state-of-the-art architectures residual networks inception nets sample statistics mini-batch. retains moving average statistics training iteration employs inference. however limitation limited memory capacity computing platforms especially network size image size large. case mini-batch size sufﬁcient approximate statistics making bias noise. errors would ampliﬁed network becomes deeper degenerating quality trained model. negative effects exist also inference normalization applied testing sample. furthermore mechanism distribution certain layer could vary along training iteration limits stability convergence model. recently extension called batch renormalization proposed improve performance mini-batch size small. training performs correction minibatch statistics using moving average statistics i.e. adopting short-term memory mini-batch statistics past make estimate robust. despite sheds light dealing mini-batches small sizes performance still undesirable following reasons. moving averages away actual statistics early stage training making correction statistics unreliable. implementation extra parameters introduced measure whether correction trusted need carefully tuned training. moreover probably fail handling mini-batches examples e.g. less samples. case estimates batch sample statistics moving statistics either instable means variances dramatically vary different training iterations. paper present normalization method batch kalman normalization improving accelerating training dnns particularly context micro-batches. advances existing solutions achieving accurate estimation statistics internal representations dnns. unlike statistics estimated measuring mini-batches within certain layer i.e. considered layer network isolated sub-system shows estimated statistics strong correlations among sequential layers. estimations accurately jointly considering preceding layers network illustrated figure analogy proposed estimation method shares merits compared kalman ﬁltering process observed batch sample means variances calculated within mini-batch. steps efﬁcient bkn. updating current estimation previous states brings negligible extra computational cost compared traditional speciﬁcally according comprehensive evaluations computational complexity increases compared leading marginal additional computation. paper makes following contributions. propose intuitive effective normalization method offering promise improving accelerating neural network training. proposed method enables training networks mini-batches small sizes resulting models perform substantially better using existing batch normalization methods. speciﬁcally makes method advantageous several memory-consuming problems training large-scale wide deep networks semantic image segmentation asynchronous sgd. classiﬁcation benchmark imagenet experiments show recent advanced networks strengthened method trained models improve leading results using less training steps additional computation complexity. whitening. decorrelating whitening input data demonstrated speed training dnns. following methods proposed whiten activations using sampled training data performing whitening every thousands iterations reduce computation. nevertheless operations would lead model blowing according instability training. recently whitened neural network generalizations presented practical implementations whiten internal representation hidden layer drew connections whitened networks natural gradient descent. although approaches theoretical guarantee achieved promising results reducing computational complexity singular value decomposition whitening computational costs still neglectable especially training plenty convolutional layers large-scale dataset many recent advanced deep architectures did. standardization. address issues instead whitening ioffe proposed normalize neurons hidden layer independently batch normalization calculated using mini-batch statistics. extension adapted recurrent neural networks using re-parameterization lstm. spite successes heavy dependence activations entire batch causes drawbacks methods. figure illustration proposed batch kalman normalization layer ﬁrst estimates statistics ˆµk−|k− ˆσk−|k−. estimations k-th layer based estimations layer estimations updated combining observed statistics k-th layer. process treats entire whole system different existing works estimated statistics hidden layer independently. shifted preserve modeling capacity network parameters opmizted training. however mini-batch moderately large size required estimate statistics compelling explore better estimations distribution accelerate training. state transition matrix transforms states previous layer current layer. bias following gaussian distribution zero mean unit variance. note could linear transition layers. reasonable purpose draw connection layers estimating statistics rather accurately compute hidden features certain layer given previous layer. example mini-batch size small batch statistics unreliable. hence several works proposed alleviate mini-batch dependence. normalization propagation attempted normalize propagation network using careful analysis nonlinearities rectiﬁed linear units. layer normalization standardized hidden layer activations invariant feature shifting scaling training sample. fixed normalization provided alternative solution employed separate ﬁxed minibatch compute normalization parameters. however methods estimated statistics hidden layers separately whereas treats entire network whole achieve better estimations. feature vector hidden neuron k-th hidden layer pixel hidden convolutional layer cnn. normalizes values using mini-batch samples mean covariance approximated ¯xk)t adopted normalize diag denotes diagonal entries matrix i.e. variances then normalized representation scaled values feature maps {x...m} layer; statics µk−|k− ˆσk−|k− layer; parameˆ ters moving mean moving variance moving momentum kalman gain transition matrix output current layer. fig. illustrates diagram bkn. unlike statistics computed within layer independently uses messages proceeding layers improve statistic estimations current layer. algorithm presents batch kalman normalization. gradients presented appendix. convolutional layer different elements feature different locations normalized way. therefore jointly normalize activations mini-batch locations following ˆµk−|k− denotes estimation mean layer ˆµk|k− estimation mean layer conditioned previous layer. call ˆµk|k− intermediate estimation k-th layer combined observed values achieve ﬁnal estimation. shown eqn. below estimation current layer ˆµk|k computed combining intermediate estimation bias term represents error observed values ˆµk|k−. indicates observed mean values gain value indicating much reply bias. estimations covariances achieved calculating ˆσk|k− σk|k represents deﬁnition covariance matrix. introducing combining deﬁnitions eqn. following update rules estimate statistics shown eqn.. proof given appendix. ˆσk|k− ˆσk|k denote intermediate ﬁnal estimations covaraince matrixes k-th layer respectively. covariance matrix bias eqn.. note identical layers. observed covariance matrix mini-batch k-th layer. eqn. transition matrix covariance matrix gain value parameters optimized training. employ ˆµk|k ˆσk|k normalize hidden representation. above unique characteristics distinguish well brn. first offers better estimation distribution. contrast existing normalization methods depth information explicitly exploited bkn. instance prior message distribution input image data leveraged improve estimation second layer’s statistics. contrary ignoring sequential dependence network requires larger batch size. second values feature across elements mini-batch spatial locations. mini-batch size feature maps spatial size effective mini-batch size parameters learned feature rather activation. taking resnet example size last feature batch size training sample size normalization reveals another beneﬁt bkn. procedure data-driven fusion current layer’s distribution previous layer. implies order compute statistics certain layer achieve implicitly using weighted feature maps layers below. instance compute mean variance last layer taking consideration mean variance feature maps entire network. speciﬁcally according eqn. mean last layer computed ˆµl|l plal ˆµl−|l− ¯xl. ˆµl−|l− decomposed using estimations previous layers. shares many beneﬁcial properties insensitivity respect initialization network parameters ability train efﬁciently large learning rate. unlike method ensure layers trained reliable estimation making training faster stable. comparison shortcuts resnet. although shortcut connection also incorporates information previous layers unique characteristics distinguish shortcut connection. provides better statistic estimation. shortcut connection informations previous layer current layer simply summed distribution estimation performed. theory applied shortcut connection structure whereas practice brings memory catastrophe. receives mean/variance previous layer shortcut connection receives entire feature maps previous layer computation cost differs. demonstrate generality model consider representative networks i.e. inceptionv resnet baseline models. models stacked convolution relu activation thus denote models inception+bn resnet+bn respectively. then apply proposed models simply replacing employed batch normalization operation denote inception+bkn resnet+bkn respectively. similarly inception+brn resnet+brn denote replacing method methods three image classiﬁcation datasets imagenet cifar- cifar- imagenet benchmark models trained million images tested validation images. regard top- accuracy evaluation metric. settings train inception+bn inception+bkn respectively using batch size gpus i.e. mini-batch note that normalizations accomplished within mini-batch gradients aggregated gpus update network parameters. table illustrates top- accuracy validation inception+bkn inception+bn reach respectively update steps. however terms reaching accuracy inception+bkn requires fewer steps inception+bn. particular inception+bkn achieves advanced accuracy training converged outperforming original network improvement attributed reasons. first leveraging messages previous layers estimation statistics stable bkn. makes training converged faster especially early stage. second procedure also reduces internal covariance shift leading discriminative representation learning hence improving classiﬁcation accuracy. similar phenomenon also observed resnet resnet+bkn achieves top- accuracy resnet+bn achieves computation complexity. table reports computation time inception+bkn compared inception+bn terms number samples processed second. fair comparison methods individually trained desktop titan-x gpus. table inception+bn inception+bkn similar computational costs i.e. inception+bkn performs slower next evaluate batch size relatively small different settings. speciﬁcally deﬁne statistics-batch-size gradient-batch-size distinguish settings. former indicates sample size used estimate statistics normalization latter indicates sample size used estimate gradients. therefore setting denoted example baselines moderate batch size referred micro-batch training statistics-batch-size typically smaller leading non-negligible optimization difﬁculty. training becomes challenging gradient-batch-size also small e.g. smaller indicating statistics normalization estimated using samples also gradients calculated using small mini-batch. setting experiment statistic-batchsize gradient-batch-size employ baseline comparison. fig. reports results. major observations fig.. first obtain substantial improvement proposed example inceptionv inception+bkn achieves top- accuracy outperforming inception+bn large margin similar phenomenon also observed resnet example k-th iteration resnet+bkn obtains top- gain compared resnet+bn. comparisons verify effectiveness micro-batches. second also observed setting validation accuracy normalization methods lower baseline slow training convergence facing statistic-batch-size however signiﬁcantly worse compared baseline. iterations inception+bkn achieves comparable baseline using times smaller batch size. indicates micro-batch training problem better addressed using rather setting setting extremely challenging normalization accomplished within training sample. adopted input image massive pixels sample video containing multiple frames. shown works well micro-batches shown fig.. next investigate necessary estimating distribution statistic-batch-size degrades fig. compare options including batch sample statistics used normalize layer’s inputs inference population statistics used normalization figure validation accuracy models trained bkn. setting allows model train faster achieve higher accuracy achieving comparable performance baseline setting presents evaluation inceptionv shows results resnet. observe major phenomenons figure first options signiﬁcantly better methods. option obtains increase compared respectively. second comparison option signiﬁcantly better ‘b’. example obtains top- accuracy option option ‘b’. note gain solely usage different statistics. attribute following reasons. first approaches fail estimate population statistics example-batch training. discussed section networks trained using batch sample statistics tested based population statistics obtained moving averages. natural statistic-batch-size larger things change statistic-batchsize equals every iteration statistics example calculated stored moving averages. information communication never happens examples. therefore moving averages difﬁcult represent population statistics. fact shows learning model approximate population statistics infeasible. possible solution moving averages normalize layer inputs training turns infeasible second indeed need population statistic case -example batches. recall moving averages introduced based following reasons. population statistic order evaluate example test need calculate statistics online either based unique example using temporary partners help. statistics based single example reliable. using temporary partners suffers problem uncertain results temporary partners changes. problems disappear statistic-batch-size equals ensures activations computed forward pass training step depend single example. also note better performance methods improving compared respectively. results verify effectiveness bkn. setting shown works better others micro-batch setting i.e. statistics-batchsize typically smaller leading nonnegligible optimization difﬁculty. next investigate challenging setting i.e. gradient-batch-size statistics-batch-size tiny indicating statistics normalization estimated using samples also gradients calculated using small minibatch. ﬁrst compare options major observations figure first achieve comparable performance baseline using times less gradient-batch-size times less statistics-batch-size cannot reach performance even using iterations. example inceptionv shown figure inception+bkn achieves second performances normalization methods setting lower setting however suffers less indicates limitation solving training problem micro-batches. contrary provides reasonable results settings. setting next evaluate challenging setting gradient-batch-size reduced times meanwhile statistics-batch-size reduced times. figure presents similar ﬁndings poor top- accuracy bkn. example obtains improvement compared k-th iteration inceptionv resnet relative improvements respectively. fig. horizontal respectively. axis represents values different batches vertical axis represents neurons different channels. observe values fig. indicating provides accurate statistic estimation. indirect evidence. inference ways calculate classiﬁcation accuracy i.e. using moving mean/variance batch mean/variance experimental results table show using batch mean/variance achieves accuracy using moving mean/variance. there’s using batch variance moving variance. proves proposed contributes provide accurate estimations. ablation extra parameters. show gain doesn’t come extra parameters provide evidences. first newly introduced parameters quite few. parameter numbers powered networks respectively. second also design experiment introduce compensatory parameters empowered networks fair comparison enlarging width. results table show operation gain. scalability bkn. provide experiments datasets cifar- cifar-. results table show beats large margin datasets moreover observe performance micro-batch encouraging compared typical size conclusion paper presented novel batch normalization method called batch kalman normalization normalize hidden representation deep neural network. unlike previous methods normalized hidden layer independently treats entire network whole. statistics current layer depends preceding layers using procedure like kalman ﬁltering. extensive experiments suggest capable strengthening several state-of-the-art neural networks improving training stability convergence speed. importantly handle training minibatches small sizes. future work theoretical analysis made disclose underlying merits proposed bkn. figure visualization variance batch sample variance moving variance respectively moving variances expected appropriate population statistics. mini-batces micro verifying bkn’s advantage. also note accuracy methods signiﬁcantly lower baseline normalized statistic-batch-size suggesting optimization difﬁculty fundamental problem. conducted studies cifar- cifar- dataset consist training images testing images classes classes respectively. considering focus behaviors extremely small batch size instead achieving state-ofthe-art results design simple architecture summarized following table fully connected layer output channels omitted. evidence accurate statistic estimations. show indeed provides accurate statistic estimation present evidences follows direct evidence. training stage ﬁnished exhaustively forward-propagated samples cifar obtain moving statistics batch sample statistics. gaps batch sample variance moving variance visualized fig.", "year": 2018}