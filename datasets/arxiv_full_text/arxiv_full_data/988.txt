{"title": "A review of learning vector quantization classifiers", "tag": ["cs.LG", "astro-ph.IM", "cs.NE", "stat.ML"], "abstract": "In this work we present a review of the state of the art of Learning Vector Quantization (LVQ) classifiers. A taxonomy is proposed which integrates the most relevant LVQ approaches to date. The main concepts associated with modern LVQ approaches are defined. A comparison is made among eleven LVQ classifiers using one real-world and two artificial datasets.", "text": "abstract work present review state learning vector quantization classiﬁers. taxonomy proposed integrates relevant approaches date. main concepts associated modern approaches deﬁned. comparison made among eleven classiﬁers using real-world artiﬁcial datasets. keywords learning vector quantization supervised learning neural networks margin maximization likelihood ratio maximization learning vector quantization family algorithms statistical pattern classiﬁcation aims learning prototypes representing class regions. class regions deﬁned hyperplanes between prototypes yielding voronoi partitions. late teuvo kohonen introduced algorithm years produced several variants. since inception algorithms researched small active community. search science november found journal articles keywords learning vector quantization titles abstracts. paper review progress made ﬁeld last years. estévez department electrical engineering advanced mining technology center faculty physical mathematical sciences university chile santiago chile e-mail pestevezing.uchile.cl algorithms related competitive learning algorithms self-organizing maps c-means. competitive learning algorithms based winner-take-all learning rule variants certain elements neighborhoods updated learning. original algorithms modern extensions supervised learning obtaining class-labeled prototypes however also trained without labels unsupervised learning clustering purposes paper focus review classiﬁers. classiﬁers particularly intuitive simple understand based notion class representatives class regions usually input space advantage multilayer perceptrons support vector machines considered black boxes. moreover support vectors extreme values datasets prototypes typical vectors. another advantage algorithms simple fast result based hebbian learning. computational cost algorithms depends number prototypes usually ﬁxed number. svms depend number training samples instead number support vectors fraction size training set. shown valuable alternative svms classiﬁers approximate theoretical bayesian border deal directly multi-class problems. initial learning rules heuristic showed sensitivity initialization slow convergence problems instabilities. however main approaches proposed deﬁning explicit cost functions derive learning rules steepest descent ascent solving problem convergence original algorithms. ﬁrst model generalization called generalized learning vector quantization glvq cost function deﬁned learning rule derived steepest descent. cost function shown related minimization errors maximization margin classiﬁer second approach called robust soft-lvq statistical objective function used derive learning rule gradient ascent. probability density function data assumed gaussian mixture class. given data point logarithm ratio correct class versus pdf’s incorrect classes serves cost function maximized. improvement deals initialization sensitivity original algorithms glvq recent extensions family algorithms substitute euclidean distance general metric structures weighted euclidean metrics adaptive relevance matrix metrics pseudo-euclidean metrics similarity measures kernel feature space lead kernelized versions paper present comprehensive review relevant supervised algorithms developed since original work teuvo kohonen. introduce taxonomy classiﬁers describe main algorithms. compare performance eleven algorithms empirically artiﬁcial real-world datasets. discuss advantages limitations method depending nature datasets. remainder paper organized follows section taxonomy classiﬁers presented. section main learning rules described. section results obtained eleven diﬀerent methods three diﬀerent datasets shown. section open problems presented. finally section conclusions drawn. c}|i training data d-dimensional input samples cardinality sample labels number classes. neural network consists number prototypes characterized vectors class labels c}|j classiﬁcation scheme based best matching unit receptive ﬁeld prototype deﬁned follows fig. shows taxonomy relevant classiﬁers developed since pioneer work kohonen. methods decomposed families kohonen’s methods methods based margin maximization methods based likelihood ratio maximization original algorithm associated cost function ensure convergence. improvements lvq. olvq achieving higher convergence speed better approximation bayesian borders. versions based hebbian learning i.e. heuristic. original corrected winner prototype. algorithm pulled prototypes away class borders. assumes good initial state network i.e. requires preprocessing method. also shows sensitivity overlapping data sets neurons never learn training patterns. algorithm updates vectors step winner runner-up. purpose estimate diﬀerentially decision border towards theoretical bayes decision border. algorithm makes corrections dependent error only present instabilities. corrects convergence problem consisting location prototypes changing continued learning adding stability factor. margin analysis original algorithm performed. deﬁnitions margin. ﬁrst sample-margin corresponds quantiﬁcation samples travel space withchanging classiﬁcation rate classiﬁer. deﬁnition used svms. second deﬁnition called hypothesis-margin corresponds quantiﬁcation distance classiﬁer altered without changing classiﬁcation rate. deﬁnition used adaboost. context sample margin hard compute numerically unstable small repositionings prototypes might create large changes sample margin. crammer showed decision borders original algorithms hypothesis margin maximizers. margin associated generalization error bounds. therefore maximizing margin equivalent minimizing generalization error. interestingly bound dimension free depends number prototypes. cost function aims margin maximization. approach solves limitations original algorithms slow convergence initialization sensitivity limitations multidimensional data correlations exist dimensions name few. euclidean distance data point closest prototype class label euclidean distance closest prototype diﬀerent class label. term constitutes hypothesis margin classiﬁer according winner-takes rule note since glvq includes margin cost function described margin optimization learning algorithm. generalization bounds show larger margin better generalization ability cost function extended distance metrics. generalization bound derived generalized relevance uses adaptive metric. section describe detail cost function proposed robust soft-learning vector quantization cost function gives origin right branch taxonomy illustrated fig. based gaussian probabilistic model data forms mixture models. assumed probability density function data described gaussian mixture model class. sample generated gaussian mixture model correct class compared sample generated gaussian mixture models incorrect classes. logarithm ratio correct mixture gaussian model incorrect mixture gaussian models probability densities maximized. labeled prototype vectors. probability density data given fig. shape receptive ﬁeld prototypes depending distance measure. receptive ﬁeld using euclidean distance measure. receptive ﬁeld using diagonal matrix relevance. receptive ﬁeld using full matrix relevance. generated component mixture chosen identically prototype conditional component generates particular data point function prototype following likelihood ratio proposed cost function maximized original glvq rslvq methods rely euclidean distance. choice metrics assumes spherical receptive ﬁeld prototypes shown fig. however heterogeneous datasets diﬀerent scaling correlations dimensions; high-dimensional datasets estimate errors accumulate disrupt classiﬁcation distance learning distance measure adaptive training allowing receptive ﬁelds prototypes shown figs. generalized matrix generalized distance metric proposed classiﬁers kernelized mapping function deﬁned order realize nonlinear transformation data space higher dimensional possibly linearly separable feature space follows problems allow vectorial representation e.g. alignment symbolic strings. cases data represented pairwise similarities dissimilarities corresponding matrices respectively. matrices symmetric i.e. zero diagonals. easy turn similarities dissimilarities vice-versa shown corresponds called ’relational data’ representation. data represented pairwise dissimilarities previously mentioned restrictions always embedded pseudo-euclidean space pseudo-euclidean space vector space equipped symmetric bilinear form composed parts euclidean correction bilinear form expressed diagonal matrix entries entries pseudoeuclidean space characterized signature ﬁrst components euclidean next components non-euclidean components zeros. prototypes assumed linear combinations data samples vector coeﬃcients describes prototype implicitly. unlike kernel approaches dis-/similarities approaches assume data euclidean. distances data points prototypes computed based pairwise data similarities follows fig. schematic lvq. updating rule. circles represent prototypes; square represents input sample; colors indicate diﬀerent class labels. example sample incorrectly classiﬁed nearest prototype correct class moves towards sample nearest prototype incorrect class moves away sample. among initial variants proposed kohonen popular lvq. described detail below. prototypes updated step winner runner-up. them belongs correct class other belongs incorrect class. either winner runner-up class label sample. furthermore current input must fall within window deﬁned around mid-plane vectors w+and updating rule follows closest prototype input sample class label sample class label rule behavior input sample class label presented. rule behavior input sample blue class label presented. adds neighborhood cooperativity glvq solving dependency initialization. prototypes class current input sample adapted towards data point according ranking closest prototype diﬀerent class label moved away data sample. sgng incorporates plasticity selecting adaptive number prototypes learning. another method harmonic minimum allows adapting prototypes simultaneously iteration. prototypes class label current sample updated. likewise prototypes diﬀerent label sample adjusted replaced harmonic average distances details reader referred margin maximization methods based information theoretic learning methods divergence measure proposed principe based cauchy-schwarz inequality. together consistently chosen parzen-estimator densities gives numerically well-behaved approach information optimization prototype-based vector quantization called cauchy-schwarz divergence methods fuzzy class labels train classiﬁer. extension glvq kernel glvq kglvq projects data space non-linearly higher dimensional feature space applies glvq algorithm newly formed feature space. prototypes represented implicit form shown updating rules glvq algorithm generalized data space feature space using following relative distance diﬀerence speciﬁes combinatorial coeﬃcient vectors coeﬃcient vector associated nearest prototype w+having class label coeﬃcient vector associated nearest prototype diﬀerent class label furthermore kglvq extended nystrom-approximation generalized sparsity imposed nystrom approximation technique used reduce learning complexity large data sets. method proposed using dis-/similarity data representing implicit form. approach extended margin maximization technique giving origin relational glvq relative distance diﬀerence. dissimilarity matrix elements rn×n; coeﬃcient vector elements associated nearest prototype class label nearest prototype diﬀerent class label respectively. equivalent updating rules parameters matrix learning scheme applied rslvq obtaining named matrix rslvq local version local mrslvq latter consists local distance adaptation prototype cases full matrix relevance factors. based rslvq approach uses conditional probability density function component generates particular data point function prototype assumed normalized exponential form methods based likelihood ratio maximization extended feature space using kernels. kernelized version rslvq proposed called kernel robust soft learning vector quantization prototypes represented linear combination data images feature space described rslvq gaussian mixture model assumed euclidean distance measured feature space using distance deﬁned consequently substituted dis-/similarities approaches allow extending likelihood-ratio maximization methods pseudo-euclidean space giving origin relational rslvq prototypes represented implicit form linear combination data points remember represents dissimilarity matrix symmetric diagonal elements dissimilarities computed using approach argument normalized exponential takes following form section results three diﬀerent experiments presented. ﬁrst employs multi-modal dataset used studying sensitivity initialization initial position prototypes. second dataset image segmentation used comparing performance diﬀerent methods nature features data heterogeneous. finally third dataset usps allows compare performance classiﬁers diﬀerent methods real-world problem. multi-modal dataset three classes training samples class. training samples class distributed three clusters classes multi-modal distributions. class consists sub-clusters number samples cluster respectively. class composed sub-clusters number samples cluster respectively. image segmentation dataset consists samples features correspond pixel regions extracted outdoor images. classes brick-face foliage cement window path grass features constants eliminated. -fold cross validation used comparing performance diﬀerent algorithms. addition multi-comparison statistical test used compare means pairs classiﬁers. test involves comparing many group means i.e. pairs simple t-test comparisons realized bonferroni adjustment done compensate critical value used multiple comparison procedure initial value learning rate multi-modal image segmentation dataset usps; start time learning rate tmax maximum number training epochs experiments. table shows results obtained multi-modal dataset using -fold cross validation. initialization prototypes random mean whole dataset. setting allows quantifying sensitivity initial conditions algorithm study. number prototypes class shown table algorithms update prototype iteration hm-lvq sgng obtained best results together glvq. multi-comparison statistical test performed showed hm-lvq signiﬁcantly better algorithms conﬁdence interval mean diﬀerence. statistical test indicates algorithms signiﬁcantly diﬀerent hm-lvq sgng glvq grlvq. however take algorithms reference statistical test shows statistically signiﬁcantly diﬀerent classiﬁers instead done hmlvq. results table show performance inferior algorithms. associated functional sensitive initial condition prototypes. practice needs pre-processing technique ﬁnding good initial position prototypes. methods using adaptive metric grlvq lgrlvq gmlvq lgmlvq classiﬁcation error higher obtained glvq methods similar glvq early iterations prone over-ﬁtting adaptive metric trying good performance. experiment prototypes initialized mean class avoid initialization sensitivity. number prototypes class algorithms based distance learning reached better performance shown third column table mean using -fold cross validation shown table. lowest classiﬁcation error obtained lgmlvq mean value signiﬁcantly better algorithms conﬁdence interval mean diﬀerence according multi-comparison statistical test using bonferroni adjustment. hand lgmlvq signiﬁcantly diﬀerent glvq grlvq gmlvq. discussed previous section means algorithms signiﬁcantly diﬀerent three algorithms instead done lgmlvq. gmlvq grlvq statistically signiﬁcant diﬀerence conﬁdence interval mean diﬀerence. case local distance learning matrix method obtained better performance relevance version signiﬁcantly diﬀerent conﬁdence interval methods based distance learning obtained better performance capacity modifying shape receptive ﬁeld prototypes locally globally robust dataset heterogeneous features image segmentation dataset. usps usps* datasets prototypes initialized mean class number prototypes class methods based likelihood ratio maximization rslvq krslvq obtained higher performance methods based margin maximization. usps* rslvq krslvq reached average classiﬁcation errors signiﬁcantly better algorithms conﬁdence interval. best performance obtained rslvq mean multi-comparison statistical test indicates means obtained rslvq krslvq signiﬁcantly diﬀerent conﬁdence interval. algorithms based likelihood ratio maximization achieved higher performance dataset best prototypes necessarily located centroids cluster allows obtaining better performance datasets overlapped. case usps best performance obtained followed hm-lvq methods based likelihood-ratio maximization rslvq krslvq appear closely third fourth places shown table section open problems challenges ﬁeld classiﬁers presented. principled approach lvq. although glvq provides cost function shown margin maximizer method cost function derived ﬁrst principles probabilistic information theory. sparsity. real world datasets becoming larger larger computational cost applying prototype-based method classiﬁer keeps growing. reason recently several sparsity approaches extended classiﬁers allow obtaining linear training time without losing classiﬁcation performance semi-supervised learning. real world increasing size datasets longer possible labels samples. reason necessary adapt classiﬁers semi-supervised learning active learning framework visualization classiﬁers. prototype-based methods classiﬁers allow prototypical representation data input space. advantage prototypes data visualized classiﬁer interpreted easily. classiﬁers work space diﬀerent data space kernelized relational variants classiﬁers longer easily visualized interpreted. losing natural capacity early classiﬁers interest active learning. improving generalization ability prototypebased methods classiﬁers active learning used. method gives learner capability selecting samples training. furthermore using active learning approach generalization ability model increased well learning speed presented review relevant classiﬁers developed last years. introduced taxonomy classiﬁers general framework. diﬀerent main cost functions proposed literature margin maximization likelihood ratio maximization. classiﬁers based margin maximization demonstrated good performance many problems. methods prototypes centroid data makes less ﬂexible overlapping data. classiﬁers based likelihood ratio maximization alternative uses probabilistic approach prototypes centroid classes gives ﬂexibility case overlapping data. hand classiﬁers based adaptive metric reached best performance heterogeneous feature datasets. also classiﬁers update prototype iteration less sensitive initial conditions better performance multi-modal datasets. recently classiﬁers kernelized relational classiﬁers based data representation improving performance classiﬁers data complex. relational classiﬁers general representation data allows working noneuclidean spaces. sense recent approaches based dis-/similarities capture inherent data structure naturally improve performance classiﬁer. experiments done work shown free lunch; method pros cons. diﬀerent methods designed dealing speciﬁc problems datasets. general point view classiﬁers demonstrated", "year": 2015}