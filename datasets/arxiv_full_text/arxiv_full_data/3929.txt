{"title": "Self-Organizing Multilayered Neural Networks of Optimal Complexity", "tag": ["cs.NE", "cs.AI"], "abstract": "The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics.", "text": "abstract. principles self-organizing neural networks optimal complexity considered unrepresentative learning set. method self-organizing multi-layered neural networks offered used train logical neural networks applied medical diagnostics. well-known principles self-organization used synthesize neural networks unrepresentative learning sets eliminate priori uncertainty structures self-organization realized next conditions firstly various structures neural network generated secondly best selected criterion efficiency. variety number training neural network states must adequate accordance general principle ashby complexity learned neural network optimal variety adequate minimal number nodes synaptic connections. known rosenblatt's perceptron consisting input associative adjustable layers nodes complexity optimal synaptic links layers randomly redundantly defined. stated supplement layers structure improves recognition capability redundancy neural network structure reduced random search methods selected ones decrease value lost function. within methods search desired structure consisting associative layer priori defined number neurons completed defined number unsuccessful attempts aimed decrease value lost function achieved limitations without introduced cause desired neural network structure conditionally optimal. within heuristic self-organization methods neural network structure evolutionary one. complexity neural network incrementally increased layer value lost function decreased. layer variety neural network candidate-structures generated defined number best selected. using principle exterior addition selection criteria lost function minimum points desired neural network function however complexity synthesized neural network optimal results heuristic self-organization depend defined configure selection criterion freedom candidate-structures selection below analyze possibility self-organizing multi-layered neural network optimal complexity suggested approach behavior neural network input variables output function describes. self-organization neural network made unrepresentative learning composed small number independent instances classified belong classes distinguished states objects. note case sequence dichotomy classifications used. within heuristic self-organization desired neural network described rosenblatt scheme consists sensor associative layers. synthesis associative layers made reference function arguments typically reference function belong arbitrary class function freedom selecting candidate-structures criterion value layer minimal. number combination variables second next ones typically number ranges first layer configure criterion supposes learning separated several non-conjunctive subset used self-organizing neural network. typically subsets length number equals two. within known approach heuristic realized supposes true function according desire neural network depend subset chose synthesize evaluating efficiency neural network candidate-structure synthesized subset instances whole used. heuristic formalized criteria unbias regularity criteria called exterior since evaluating function efficiency exterior instances subset used. using similar configure criteria principle beer's exterior addition realized allowed eliminate contradictories godel's theorem incompleteness axiomatic systems condition unbias regularity values depend undetermined component caused measurement errors input variables well influence uncontrolled variables. because increase robustness desired neural network convolution criteria computed minimum criterion corresponds best desired neural network layer number layers increased complexity neural network candidate-structures also done value criterion comes minimum pointed desired neural network however above-mentioned undetermined component minimum criterion local. eliminating possible bias desired neural network structure positive variable introduced stopping rule condition carried layer algorithm self-organization completed neural network whose value criterion equals crr- assigned desirable. note several neural network structure minimal value criterion. within known method final choosing structures used subsidiary criterion instances etc. since resulted neural network found under minimal number layers structure features also minimal. self-organizing neural network optimal complexity influence settings user without defines excluded. results selforganization depend settings variants next reasons. self-organizing neural network optimal complexity suggest exterior criteria whose above-mentioned drawbacks eliminated. criteria realized computing value empirical function introduced evaluate neural network accuracy loss occurs whole learning set. condition carried structural modifications reference function brings neural network layer ones belong previous neural network fjr-. definition modifications able form exterior addition neural network fjr- feature brings. indeed structural modification similar modification brought neural network fjr- condition carried out. consequently selecting best neural networks condition sufficient one. first condition carried number case neural networks efficiency compose collective. note number neural networks collective proportioned complexity decided task reasons appearing collective learning represented strongly restricted field input variable values. less value user without defined decision nonplausible. typically value defined analyzing ration values efficiency learning evaluated. case either structure input variables learning reconstructed. synthesized neural network clearly represented training steinbuch's matrix figure shows example logical neural network using reference function boolean variables. figure collective two-layered logical neural networks depicted trained matrix including sensors outputs .... solid circles logical functions variables. vertical lines sensor nodes boolean variables. horizontal lines hidden nodes described reference functions depicted solid circles. circles placed intersection vertical horizontal lines denote sensor lines connected input formal neuron implements reference function note number marked near solid circles logical functions boolean variables. thus horizontal lines activated certain states vertical lines. rule connections placed horizontal lines first layer. horizontal lines intersected vertical ones next layers formed. starting second layer rule allows connection points intersections. additionally horizontal lines split second figure finally horizontal lines second layer form variables outputs collective consisting equal efficiency neural networks. results self-organization connections horizontal vertical lines well number reference functions table represents logical reference functions used describe trained neural network. sing logical reference functions neural networks synthesized represented if-then rules. neural networks synthesized unrepresentative learning preferential robustness maximal. also evaluating decision plausibility values coherence preliminarily computed combinations boolean features using criterion so-called problem combinatorial \"explosion\" avoided. problem arises attempt search desired function among logical functions variables order prove this thirst find value number number combination number logical functions variables. find numbers etc. substituted values inequality inequality true since right-hand increases rapider left one. example numbers inequality carried equals substantially less eveloped method self-organization applied discovering diagnostic rules medicine. neural network rules synthesized distinguish pathologies clinically close example pathologies infectious endocarditis active rheumatism systemic lupus erythematosus also early postoperational complication abdominal surgery. diagnostic rules extracted unrepresentative learning sets composed instances doctors easily interpreted neural network rules synthesized logical reference function arguments. extracted rules represented diagnostic tables logical function if-then. generating neural network candidate-structures condition used. extracted rules ensured unerring classification learning sets. rules include features whose number less times doctors suggested. accuracy tested sets contained unseen examples. first three diagnostic rules able unerringly classify testing sets. fourth rule generated true decisions unseen examples. detail rule extracted distinguish analyzed. learning consists classified instances represented variables. among variables quantitative variables. synthesized neural network consisting layers input output nodes depicted figure training matrix. input nodes consist quantitative features boolean variables represented table table thresholds quantitative features functions quantization also brought. extracted rule represented truth table consisted rows. rows values coefficient computed evaluate plausibility taken decision. developed method self-organization able synthesize multilayered neural networks optimal complexity unrepresentative learning set. results self-organization depend settings user defined without. final decision taken value coefficient introduced evaluate coherence collective consisted neural networks efficiency. particular case neural network logical one. developed method successfully applied medical diagnostics. general suggested method decide wide class tasks knowledge extraction making decisions etc. foerster g.w. zopf principles self-organization. pergament press ashby w.r. introduction cybernetics. willey york rosenblatt princeples neurodinamics. york spartan books rastrigin l.a. adaptation complex systems. zinatne riga rastrigin l.a. yu.p. ponomarev extrapolation methods design control.", "year": 2005}