{"title": "Amortised MAP Inference for Image Super-resolution", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high-resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Furthermore, MAP inference is often performed via optimisation-based iterative algorithms which don't compare well with the efficiency of neural-network-based alternatives. Here we introduce new methods for amortised MAP inference whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.", "text": "casper kaae sønderby jose caballero lucas theis wenzhe shi& ferenc huszár casperkaaegmail.com {jcaballeroltheiswshifhuszar}twitter.com twitter london university copenhagen denmark image super-resolution underdetermined inverse problem large number plausible high resolution images explain downsampled image. current single image methods empirical risk minimisation often pixel-wise mean squared error loss. however outputs methods tend blurry over-smoothed generally appear implausible. desirable approach would employ maximum posteriori inference preferring solutions always high probability image prior thus appear plausible. direct estimation nontrivial requires build model image prior samples. introduce methods amortised inference whereby calculate estimate directly using convolutional neural network. ﬁrst introduce novel neural network architecture performs projection afﬁne subspace valid solutions ensuring high resolution output network always consistent resolution input. using architecture amortised inference problem reduces minimising cross-entropy distributions similar training generative models. propose three methods solve optimisation problem generative adversarial networks denoiser-guided backpropagates gradient-estimates denoising train network baseline method using maximum-likelihoodtrained image prior. experiments show based approach performs best real image data. lastly establish connection gans amortised variational inference variational autoencoders. image super-resolution underdetermined inverse problem estimating high resolution image given corresponding resolution input. problem recently attracted signiﬁcant research interest potential enhancing visual experience many applications limiting amount pixel data needs stored transmitted. many applications example medical diagnostics forensics primarily motivated improve perceptual quality applied natural images. current single image methods empirical risk minimisation often pixel-wise mean squared error loss however convex loss functions general known limitations presented uncertainty multimodal nontrivial distributions distributions natural images. large number plausible images explain input bayes-optimal behaviour trained model output mean plausible solutions weighted according posterior probability. natural images averaging behaviour leads blurry over-smoothed outputs generally appear implausible i.e. produced estimates probability natural image prior. idealised method applications would full-reference perceptual loss function describes sensitivity human visual perception system different distortions. however figure illustration problem example. two-dimensional data drawn swiss-roll distribution given observation downsampling modelled valid solutions along line shading illustrates magnitude posterior |x=.. bayes-optimal estimates well estimate given marked labels. estimates different values also trained model outputs estimated gradishown ents denoising function trained note affgan affdg values. affgan affdg achieves cross-entropy values close solution conﬁrming minimize desired quantity. models performs worse since minimize cross-entropy. models using afﬁne projections performs better soft constrained models. widely used loss functions related peak-signal-to-noise-ratio metric shown correlate poorly human perception image quality improved perceptual quality metrics proposed popular structural similarity multi-scale variants although correlation metrics human perception improved still provide fully satisfactory alternative training neural networks lieu satisfactory perceptual loss function leave empirical risk minimisation framework present methods based natural image statistics. paper argue desirable approach employ amortised maximum posteriori inference preferring solutions high posterior probability thus high probability image prior keeping computational beneﬁts amortised inference. motivate inference desirable consider problem figure data two-dimensional distributed according swiss-roll density. observation deﬁned average pixels consider observing data point possible solutions line generally afﬁne subspace shown dashed line figure posterior distribution thus degenerate corresponds slice prior along line shown shading. minimise mean absolute error bayes-optimal solution mean median along line respectively. example illustrates produce output probability data prior whereas inference would always mode deﬁnition high-probability region. section discussion possible limitations inference approach. ﬁrst contribution convolutional neural networks architecture designed exploit structure problem. image downsampling linear transformation modelled strided convolution. figure illustrates images compatible image span afﬁne subspace. show using speciﬁcally chosen linear convolution deconvolution layers implement projection afﬁne subspace. ensures cnns always output estimates consistent inputs. afﬁne projection layer added indeed trainable algorithm. using architecture show training model inference reduces minimising cross-entropy data distribution implied distribution model’s output evaluated random images. result don’t need corresponding image pairs more training becomes akin training generative models. however direct minimisation cross-entropy possible instead develop three approaches depending projecting model output afﬁne subspace valid solution approximate directly data present variant generative adversarial networks approximately minimises kullback–leibler divergence cross-entropy between analysis provides theoretical grounding using gans image also introduce trick call instance noise generally applied address instability training gans. employ denoising capture natural image statistics. bayes-optimal denoising approximately learn take gradient step along log-probability data distribution gradient estimates denoising directly backpropagated network minimise cross-entropy gradient descent. present approach probability density data directly modelled generative model trained maximum likelihood. differentiable generative model based pixelcnns mixture conditional gaussian scale mixtures whose performance believe close the-state-of-the-art category. section empirically demonstrate behaviour proposed methods dimensional dataset real image datasets. lastly appendix show stochastic version affgan performs amortised variational inference ﬁrst time establishes connection gans variational inference variational autoencoders framework introduced goodfellow also showed models minimise shannon-jensen divergence certain conditions. section present update rule corresponds minising klpy recently nowozin presented general treatment connects gans f-divergence minimisation. parallel contributions theoretical work mohamed lakshminarayanan presented unifying view learning gan-style algorithms variant regarded special case. focus several recent papers gans algorithmic tricks improve stability section introduce another trick call instance noise. discuss theoretical motivations compare one-sided label smoothing proposed salimans also refer parallel work arjovsky bottou proposing similar method. recently several attempts made improve perceptual quality using deep representations natural images. bruna wand measure euclidean distance nonlinear feature space deep pre-trained perform object classiﬁcation. dosovitskiy brox ledig similar approach also adversarial loss term. unpublished work garcia explored combining gans penalty input down-sampled output. note soft penalties used methods interpreted assuming gaussian laplace observation noise. contrast approach assumes observation noise satisﬁes consistency inputs outputs exactly using afﬁne projection explained section work larsen proposed replace pixel-wise used training variational autoencoders learned metric discriminator. denoiser based method exploits fundamental connection probabilistic modelling learning denoise bayes-optimal denoiser used estimate gradient probability data. knowledge work ﬁrst time output denoiser explicitly back-propagated train another network. lastly note denoising used solve inverse problems compressed sensing approximate message passing instead calculating separately perform amortised inference would like train function calculate estimate. natural loss function learning parameters average log-posterior expectation taken distribution observations loss depends unknown posterior distribution proceed decomposing log-posterior using bayes’ rule follows. linear transformation used image downsampling. general modelled strided two-dimensional convolution. therefore likelihood term eqn. degenerate eqn. rewritten constrained optimisation arbitrary mapping space projection afﬁne subspace moore-penrose pseudoinverse satisﬁes aa+a a+aa+ conveniently strided two-dimensional convolution becomes deconvolution up-convolution standard operation used deep learning important stress optimal deconvolution simply transpose figure illustrates upsampling kernel corresponds gaussian downsampling kernel deconvolution easily found used numerical methods detailed appendix intuitively thought baseline solution residual. operation projection null-space therefore downsample residual guaranteed matter using functions form turn eqn. unconstrained optimization problem. denotes cross-entropy used eˆy∼qθ minimise objective need matched input-output pairs empirical risk minimisation. instead need match marginal distribution reconstructed images distribution images. respect problem becomes akin unsupervised learning generative modelling. following sections present three approaches ﬁnding optimal utilising properties afﬁne projection. generative adversarial networks consist generator turns noise sampled distribution images parametric mapping discriminator learns distinguish real synthetic images. generator discriminator updated tandem resulting generative distribution moving closer distribution real data behaviour gans depends speciﬁcs generator discriminator trained. following objective functions algorithm iterates steps ﬁrst updates lowering keeping ﬁxed updates lowering keeping ﬁxed. shown amounts minimising klpy distribution samples generated appendix proof context afﬁne projected function takes role generator. instead noise generator low-resolution images leaving everything else unchanged deploy algorithm minimise klpy call algorithm afﬁne projected affgan short. similarly introduce notation softgan denote algorithm without afﬁne projection instead uses additional soft-constraint note difference cross-entropy divergence entropy klpy hence expect affgan favour approximate solutions lead higher entropy thus diverse solutions overall. theory suggests gans convergent algorithm. unique optimal discriminator exists reached optimising perfection step technically whole algorithm corresponds gradient descent estimate klpy respect practice however gans tend highly unstable. theory wrong? think main reason instability gans stems concentrated distributions whose support overlap. distribution natural images often assumed concentrate around low-dimensional manifold. cases degenerate manifold-like construction affgan. therefore odds especially convergence reached perfectly separated several violating condition convergence proof. remedy problem adding instance noise true image samples. amounts minimising divergence denotes convolution noise distribution noise level annealed training noise allows safely optimise convergence iteration. trick related one-sided label noise introduced salimans however without introducing bias optimal discriminator believe promising technique stabilising training general. details please appendix gradients function calculated back-propagation whereas requires estimation since unknown. results showing limit inﬁnitesimal gaussian noise optimal denoising functions used estimate gradient gaussian white noise bayes-optimal denoising function noise level using results maximise eqn. ﬁrst training neural network denoise samples backpropagate gradient estimates eqn. chain rule eqn. update well call method affdg uses afﬁne subspace projection guided gradient dae. similar we’ll call similar algorithm soft-enforcing eqn. softdg. direct baseline model amortised inference tractable powerful density model using maximum likelihood cross entropy respect generative model approximate eqn. deep generative model similar pixelcnn continuous mcgsm likelihood. type models state-of-the-art density estimation relatively fast evaluate produce visually interesting samples call method affll uses afﬁne projection guided log-likelihood density model. designed experiments address following questions methods proposed section successful minimising cross-entropy?→ section afﬁne projection layer hurt performance cnns image sr?→ section proposed methods produce perceptually superior results? sections initially illustrate behaviour proposed algorithms data exact inference computationally tractable. data drawn two-dimensional noisy swiss-roll distribution one-dimensional data simply average pixels. next tested proposed algorithm series experiments natural images using downsampling.. ﬁrst dataset took random crops images containing grass texture. random textures known hard using loss functions. finally tested proposed models real image data faces natural images models convolution neural networks implemented using theano lasagne refer appendix full experimental details. experiment wanted demonstrate affgan affdg indeed minimising objective eqn. used two-dimensional problem evaluated using brute-force monte carlo. figure shows outputs models trained different criterion. affgan affdg solutions largely dominant mode similar inference. models output generally falls regions prior density. table shows cross-entropy achieved different methods averaged independent trials random initialisation. cross-entropy values based models relatively close optimal solution case brute-force way. expected models perform worse models minimize also calculated average network input downsampled network output. afﬁne projected models error exactly soft constrained models approximately satisfy constraint even extensive training further observe afﬁne projected models generally found lower cross-entropy compared soft-constrained versions. figure celeba performance models training. distance model output true image using ssim space input down-sampled model output tuple legend indicate ixed rainable afﬁne projection rained andom initialised afﬁne projections). models using pre-trained afﬁne projections always performs better metrics compared models using either random initialized afﬁne projections further ﬁxed pre-trained afﬁne projection ensures best consistency input down-sampled output seen ﬁgure kernels afﬁne projection seen unconstrained architectures. test this trained cnns without afﬁne projections perform celeba dataset using objective function. results shown figure first note using afﬁne projections randomly initialised network starts learning lower initial loss low-frequency components network output already match target image. observed afﬁne projected networks generally train faster unconstrained ones. furthermore afﬁne projected networks tend better solution measured ssim investigate aspects network architecture responsible improved performance evaluated models variant initialise afﬁne projected implement correct projection treat trainable parameter. ﬁnal variant keep architecture same initialise ﬁnal deconvolution layer randomly allow trained. found initialising correct moore-penrose inverse important similar results irrespective whether ﬁxed training. figure shows error network input downsampled network output. exact afﬁne projected network keeps error virtually whereas network violate consistency. figure show downsampling kernel corresponding optimal kernel grass textures random textures known hard model using loss function. figure shows grass texture patches using identical afﬁne projected cnns trained different loss functions. randomly initialised afﬁne projected cnns always produce output correct lowfrequency componentsas illustrated third panel labelled affinit figure affgan model produces clearly sharpest images found images plausible given inputs. notice reconstruction perfect pixel-by-pixel correct statistical properties human visual system recognise grass texture. affdg affll models produced blurry results unable improve upon using various optimization methods. ﬁndings choose perform experiments models concentrate affgan instead. refer appendix discussion results models. figure results seen several models trained using different loss functions. trained models outputs somewhat generic over-smoothed images expected. models global content correct afﬁne projected soft constrained models. comparing affgan softgan outputs affgan model produces slightly sharper images figure grass textures. shows model input true image model outputs according ﬁgure legend. bottom shows zoom except images row. affgan image much sharper somewhat blurry affmse image. note affdg affll produces blurry results. affinit shows output untrained afﬁne projected model baseline solution illustrating effect upsampling using however also seem contain slightly high frequency noise. observed colour drifting soft constrained models. table shows quantitative results four models where terms psnr ssim model achieves best scores expected. consistency input output clearly shows models using afﬁne projections satisfy eqn. better soft constrained versions losses. table psnr ssim scores celeba dataset. terms psnr ssim space trained models achieves best scores expected affgan performs better softgan. considering models using afﬁne projections clearly show better consistency input sampled model output models using projection. figure celeba faces. model input target model outputs according ﬁgure legend. affgan softgan produces clearly shaper images blurry outputs. found affgan outputs slightly sharper images compared softgan however also slightly high-frequency noise. natural images figure show results pixels affgan trained natural images imagenet. images results sharp corresponds well input. however still high-frequency noise present results images. interestingly snake depicted third column super resolved water obviously wrong still plausible image considering input image. further water likely higher density image prior snakes suggests model dreams reasonable data. figure using affgan imagenet. affgan outputs true images model input generally affgan produces plausible outputs however still easily distinguishable true images. interestingly snake depicted third column super resolved water obviously wrong still plausible image considering input image. argument inference mode distribution dependent representation transforming variable invertible transformation performing inference transformed space lead different answers depending transformation. extreme example consider transforming continuous random scalar cumulative distribution function resulting variable uniformly distributed value interval mode. thus estimate unique allows alternative representations guarantee estimate -bit pixel representation seek paper special. arrive different solution performing estimation feature space convolutional neural network even merely alternative colour space used. interestingly affgan resilient coordinate transformations eqn. includes extra term effected transformations second argument relates assumption estimates appear plausible. although deﬁnition mode lies high-probability region guarantee appearance anything like random sample. consider example data drawn d-dimensional standard normal distribution. concentration measure increases norm typical sample approximately high probability. mode however norm sense mode distribution highly atypical. indeed human observers easily tell apart typical sample noise distribution mode would hard time noticing difference random samples. argument suggests sampling posterior good even preferable obtain plausible reconstructions. appendix establish connection variational inference varational autoencoders stochastic version affgan however leaving emperical studies further. work developed methods approximate inference ﬁrst introduced architectural restriction neural networks projecting model output afﬁne subspace valid solutions. proposed three methods based gans denoising density models amortised inference using afﬁne projection. high dimensions empirically found based approach affgan produced visually appealing results. work follows successful demonstrations gan-based algorithms image provide additional theoretical motivation approach makes sense. future work plan focus stochastic extension affgan seen performing amortised variational inference. sander dieleman schlüter colin raffel eben olson søren kaae sønderby daniel nouri eric battenberg and. lasagne first release. http//dx.doi.org/./zenodo.. goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems klaus greff antti rasmus mathias berglund tele hotloo jürgen schmidhuber harri valpola. tagger deep unsupervised perceptual grouping. advances neural information processing systems valero laparra johannes ballé alexander berardino eero simoncelli. perceptual image quality assessment using normalized laplacian pyramid. proc. is&t int’l symposium electronic imaging conf. human vision electronic imaging anders boesen lindbo larsen søren kaae sønderby winther. autoencoding beyond pixels using learned similarity metric. proceedings international conference machine learning christian ledig lucas theis ferenc huszár jose caballero andrew aitken alykhan tejani johannes totz zehan wang wenzhe shi. photo-realistic single image super-resolution using generative adversarial network. arxiv preprint arxiv. christopher metzler arian maleki richard baraniuk. optimal recovery compressive measurements denoising-based approximate message passing. international conference sampling theory applications wenzhe jose caballero ferenc huszar johannes totz andrew aitken bishop daniel rueckert zehan wang. real-time single image video super-resolution using efﬁcient sub-pixel convolutional neural network. proceedings ieee conference computer vision pattern recognition theano development team rami al-rfou guillaume alain amjad almahairi christof angermueller dzmitry bahdanau nicolas ballas frédéric bastien justin bayer anatoly belikov theano python framework fast computation mathematical expressions. arxiv preprint arxiv. pascal vincent hugo larochelle yoshua bengio pierre-antoine manzagol. extracting composing robust features denoising autoencoders. proceedings international conference machine learning zhou wang eero simoncelli alan bovik. multiscale structural similarity image quality assessment. conference record asilomar conference signals systems computers volume practice implement down-sampling projection strided convolution ﬁxed gaussian smoothing kernel stride corresponds down-sampling factor. implemented transposed convolution operation parameters optimised numerically stochastic gradient descent following objective function d-dimensional standard normal distribution dimensionality data thought monte carlo estimate spectral norm transformations respectively. monte carlo formulation advantage optimised stochastic gradient descent. operation thought three-layer fully linear convolutional neural network corresponds strided convolution ﬁxed kernels trainable deconvolution. note certain downsampling kernels exact would inﬁnitely large kernel although always approximated local kernel. convergence found depending down-sampling factor width gaussian kernel used ﬁlter sizes gans notoriously unstable train several papers exist improve convergence properties various tricks. consider following idealised algorithm iteration consisting following steps train discriminator logistic regression convergence extract estimate logarithmic likelihood ratio update taking stochastic gradient step objective function ey∼qθ well-conditioned distributions low-dimensional space algorithm performs gradient descent approximation divergence converge. highly unstable practical situations? crucially convergence algorithm relies assumptions don’t always hold log-likelihood-ratio tion bayes-optimal solution logistic regression problem unique. stipulate real-world situations neither holds mainly concentrated distributions whose support overlap. image modelling distribution natural images often assumed concentrated around lower-dimensional manifold. similarly often degenerate construction. odds distributions share support high-dimensional space especially early training small. non-overlapping support log-likelihood-ratio therefore divergence inﬁnite jensen-shannon divergence saturated maximum value locally constant large near-optimal discriminators whose logistic regression loss close bayes optimum possibly provides different gradients generator. thus training discriminator might different near-optimal solution time depending initialisation even ﬁxed main ways avoid pathologies involve making discriminator’s harder. example implementations discriminator partially updated iteration rather trained convergence. another cripple discriminator adding label noise equivalently one-sided label smoothing introduced salimans technique labels discriminator’s training data randomly ﬂipped. however believe techniques adequately address concerns described above. figure illustrate almost perfectly separable distributions. notice large distributions means large number possible classiﬁers tell distributions apart achieve similar logistic loss. bayes-optimal classiﬁer unique near-optimal classiﬁers large diverse. figure show effect sided label smoothing equivalently adding label noise. technique labels real data samples ﬂipped discriminator trained thinking samples discriminator indeed harder task classiﬁers penalised almost equally. result still large discriminators achieve near-optimal loss it’s near-optimal loss larger. label smoothing help bayes-optimal classiﬁer unique. instead propose noise samples rather labels denote instance noise. using instance noise support distributions broadened longer perfectly separable illustrated figure adding noise bayes-optimal discriminator becomes unique discriminator less prone overﬁtting wider training distribution log-likelihood-ratio becomes better behaved. jensen-shannon divergence noisy distributions non-constant function using instance noise easy construct algorithm minimises following divergence figure illustration samples non-overlapping perfectly distributions distributions one-sided label smoothing instance noise side-label smoothing shifts optimal decision boundary still covers areas support instance noise broadens support distributions without biasing optimal discriminator. parameter noise distribution. logistic regression noisy samples provides estimate pσ∗qθ updating generator minimise mean noisy samples pσ∗py know that gaussian bregman-divergence distributions equal. added noise less sensitive local features distribution. found experiments instance noise helped convergence affgan. tested instance noise generative modelling application. don’t worry over-training discriminator train convergence take gradient steps subsequent updates generator. critical hyper-parameter method noise distribution. used additive gaussian noise whose variance annealed training. propose heuristic annealing schedule noise adapted keep optimal discriminator’s loss constant training. possible noise distributions heavy-tailed spike-and-slab would work better investigated options. models generative discriminative parameters updated using eqn. models enforcing eqn. using soft-constraint added extra loss term generative parameters training anneal noise level continuously save model parameters trained increasingly smaller noise levels. learn parameters generator following gradient eqn. using estimate learning rate. training continuously load parameters trained increasingly noise levels gradients pointing approximate correct direction beginning training covering large data space precise gradients close data manifold training. joint density decomposed using chain rule runs pixels. similar continuously save parameters density model training. learn parameters generator directly minimising negative log-likelihood generated samples learned density model. input deﬁned cross-entropy calculated estimating probability density function using gaussian kernel density estimator ﬁtted samples noiseless swiss roll density i.e. setting bandwidth kernel generator discriminators -layered fully connected units layer. affdg model layered units layer trained annealing standard deviation gaussian noise image data image experiments convolution using gaussian smoothing kernel size using stride corresponding down-sampling. convolution operation kernels size followed reordering pixel output corresponding up-sampling convolution described parameters optimised numerically described appendix down-sampling done using projection. image models used convolutional models using relu nonlinearities batch normalization layers except output. generators used skip connections similar ﬁnal sigmoid non-linearity applied output model either used directly feed afﬁne transformation layers parameterised discriminators standard convolutional networks followed ﬁnal sigmoid layer. grass texture experiments used randomly extracted patches data high resolution grass texture images. generators used layers convolutions ﬁlter maps skip connections every second layer. discriminators four layers strided convolutions ﬁlter maps. affdg model four layer convolutional network ﬁlter maps layer trained annealing standard deviation gaussian noise density model implemented pixelcnn similar oord four layers convolution ﬁlter kernel sizes except ﬁrst layers used original pixelcnn uses non-differentiable categorical distribution likelihood model used gradient based optimization. instead used mcgsm likelihood model shown good density model images using mixture components quadratic features approximate covariance matrices. celeba experiments datasets split train validation test using standard splitting. images center cropped resized down-sampling using generators layer convolution networks four layers ﬁlter maps skip connections every fourth layer. discriminators layer convolution nets layers ﬁlter maps using stride every second layer. imagenet experiments dataset randomly split train validation test samples test validation sets. images discarded remove images resolution. images center cropped resized down-sampling using generator layer convolutional network layers ﬁlter maps skip connections every second layer. discriminators layer convolution nets layers ﬁlter maps using stride every second layer. stabilise training used gaussian instance noise linearly annealed initial standard deviation unable stable train models without extra regularization. figure show psnr ssim scores training affdg affll models trained grass textures. note models converging seen figure images blurry. models problems diverging training. models high noise levels gradients approximately correct covers large space around data manifold whereas small noise levels gradients accurate small space around data manifold. density model believe similar phenomenon making training diverge since accurate density models estimated density likely peaked around data manifold making learning beginning training difﬁcult. resolve issue started training using models high noise levels log-likelihood values loaded model parameters training continuously smaller noise levels better log-likelihood values. effect clearly seen training step like behavior affdg figure note density model used training affll achieved log-likelihood bits dimension comparable values obtained theis bethge texture dataset. affll model achieved high log-likelihood values model suggesting density model simply providing accurate enough representation provide precises scores training affll model. we’ll show stochastic extension affgan model approximately minimises amortised variational inference criterion variational autoencoders ﬁrst time establishes connection adversarial methods inferences variational inference. introduce variant affgan where addition data generator function also takes input independent noise variables establish connection gans amortised variational", "year": 2016}