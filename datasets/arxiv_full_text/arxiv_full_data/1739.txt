{"title": "Tag-Weighted Topic Model For Large-scale Semi-Structured Documents", "tag": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "abstract": "To date, there have been massive Semi-Structured Documents (SSDs) during the evolution of the Internet. These SSDs contain both unstructured features (e.g., plain text) and metadata (e.g., tags). Most previous works focused on modeling the unstructured text, and recently, some other methods have been proposed to model the unstructured text with specific tags. To build a general model for SSDs remains an important problem in terms of both model fitness and efficiency. We propose a novel method to model the SSDs by a so-called Tag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the tags and words information, not only to learn the document-topic and topic-word distributions, but also to infer the tag-topic distributions for text mining tasks. We present an efficient variational inference method with an EM algorithm for estimating the model parameters. Meanwhile, we propose three large-scale solutions for our model under the MapReduce distributed computing platform for modeling large-scale SSDs. The experimental results show the effectiveness, efficiency and the robustness by comparing our model with the state-of-the-art methods in document modeling, tags prediction and text classification. We also show the performance of the three distributed solutions in terms of time and accuracy on document modeling.", "text": "abstract—to date massive semi-structured documents evolution internet. ssds contain unstructured features metadata previous works focused modeling unstructured text recently methods proposed model unstructured text speciﬁc tags. build general model ssds remains important problem terms model ﬁtness efﬁciency. propose novel method model ssds so-called tag-weighted topic model twtm framework leverages tags words information learn document-topic topic-word distributions also infer tag-topic distributions text mining tasks. present efﬁcient variational inference method algorithm estimating model parameters. meanwhile propose three large-scale solutions model mapreduce distributed computing platform modeling large-scale ssds. experimental results show effectiveness efﬁciency robustness comparing model state-of-the-art methods document modeling tags prediction text classiﬁcation. also show performance three distributed solutions terms time accuracy document modeling. huge amount documents many applications. kinds documents plain text data document metadata called semi-structured documents characterize semi-structured document data becomes important issue addressed many areas information retrieval artiﬁcial intelligence data mining etc. tags important text data document mining. example imdb world’s popular authoritative source movie celebrity content movie lots tags like director writers stars country language storyline text data. given movie dick martin idea higher chance comedy without read full text storyline watch another example collection scientiﬁc articles document list tags. read main text paper would know talks authors keywords paper provides. shuangyin jiefei guan huang ruiyang rong pan’s e-mails shuangyinlicse.ust.hk {lijiefeimail. huanggmail. tanrymail. panr}sysu.edu.cn. submitted reviewed ieee transactions knowledge data engineering e.g. text classiﬁcation structural information exploiting. document modeling topic models used powerful method analyzing modeling document corpora using bayesian statistics machine learning discover thematic contents untagged documents. topic models discover latent structures documents establish links them latent dirichlet allocation however unsupervised method words documents modeled lda. thus could treat tags word features rather kind information document modeling. model semi-structured documents needs consider characteristics different kinds objects including word topic document relationship among them. problem topic kind hidden objects three observations. relative word document objective; either objective subjective similar topic models consider binary relationships pairs objects including topic-word document-topic. addition consider binary relationships like tag-word tag-topic tag-document tag-tag. tag-document relationship implies consider weights tags document. tag-topic tag-tag relationships complicated thus difﬁcult model. earlier works consider certain tags. example authortopic model considers authorship information documents modeled. work don’t limit types number tags document. extreme case document model degenerates lda. hand since tags created people relevant topics documents; however correlated redundant even noisy. therefore tag-topic relationships general enough also model weights tags document. past years researchers proposed approaches model documents tags labels example labeled assumes latent topics topic restricted associated given labels. plda assumes topic associated label however labeled plda implicit assumptions given labels strongly associated topics modeled labels independent other. another problem would trouble need deal large-scale semistructured documents. variety algorithms used estimate parameters proposed topic models mining documents monte carlo markov chain sampling techniques variational methods others methods sampling methods actually appeal tailored solution mcmc particular model would impede requirement convergence properties speed especially corpus comprise millions words. variational methods approximation solutions extent improve learning speed. however would also ineffective learning speed model accuracy comes large-scale corpus. paper propose framework tagweighted topic model represent text data various tags weights evaluate importance tags. besides learning topic distributions documents generating topic distributions words framework also infers topic distributions tags. weights observed tags document infer dataset give opportunity provide method rank tags. many applications documents corpora tags. lots documents consist words without tags maybe removed data preprocessing denoising. consider weights among tags would hold case. address problem also propose ﬂexible model called tag-weighted dirichlet allocation extended model. based twtm learns weights among dirichlet prior given tags among tags. therefore twda handles semi-structured documents also unstructured documents. unstructured documents twda degenerates latent dirichlet allocation hybrid corpora consist semistructured documents unstructured documents twda handle complex type corpora effectively easily. challenge modeling large-scale corpora propose three distributed schemes framework twtm model mapreduce programming framework proposed model four principal contributions. twtm leverages weights among observed tags document evaluate importance tags using function tagweighted topic assignment process. weights associated observed tags document providing rank tags. addition could used predict latent tags document. framework tag-weighted process easy extend many different real world applications. example extended model twda handle multi-tag documents non-tag documents simultaneously useful process complicated applications. rest paper organized follows. section ﬁrst analyze discuss related works. section introducing notations present novel topic modeling framework twtm give methods learning inference. section show extended model twda give process learning inference. section give theoretical analysis discuss differences twtm twda comparing topic models. section propose three distributed solutions twtm large-scale semistructured documents. section present experimental results three domains show performance proposed method document modeling text classiﬁcation effectiveness efﬁciency three large-scale solutions large scale semi-structured documents modeling. paper section related works topic models provide amalgam ideas drawn mathematics computer science cognitive science help users understand unstructured data. many topic models proposed shown powerful document analyzing applied many areas including document clustering classiﬁcation information retrieval extended many topic models different situation applications analyzing text data however models consider textual information treat information plain text well. tmbp cftm propose methods make contextual information documents topic modeling. tmbp topic model biased propagation leveraging contextual information authors venue. tmbp needs predeﬁne weights author venue information word assignment limits usefulness real applications. method cftm strong assumption word associated either author venue. many applications assumption hold. several models proposed take advantage tags labels labeled plda modeling relationships among several variables author-topic model labeled topic distribution document picking several hyperparameter components correspond labels draw topic components hyperparameter without inferring topic distribution labels. labeled assume existence latent topics plda provides anway modeling tagged text data assumes generation topics assignment limited given tags word training process plda assumes topic takes part exactly label optionally share global label present every document. author topic model obtains topic distributions authors without giving importance weights among given authors document. dirichlet-multinomial regression topic model includes log-linear prior documenttopic distributions exponential function given features document. however doesn’t output weights either useful ranking. work propose tag-weighted topic modeling framework leverages information given document list weight values model topic distribution document. meanwhile mixture collection semistructured documents unstructured documents present extended model called tag-weighted dirichlet allocation considers dirichlet prior tags weight values among them. based framework tag-weighted topic model also show three large-scale solutions similar formally deﬁne following terms. consider semi-structured corpus collection documents. deﬁne corpus -tuple denotes document bag-of-word representation document vector element binary indicator size corpus convenience inference paper expanded matrix number tags document number binary vector i-th document j-th corpus paper wish probabilistic model corpus assigns high likelihood documents corpus documents alike utilizing given information. twtm probabilistic graphical model describes process generating semi-structured document collection. previous topic models document typically characterized multinomial distribution topics topic represented words vocabulary. take example generative process topic distribution document assumed follows. hyperparameter topic distribution drawn hyperparameter without considering given tags. however information useful generation dirichlet prior. paper instead denote topic distribution document shown figure represent topic distribution matrix number topics. represent distribution matrix words dictionary number words dictionary similar twtm models document mixture underlying assume observed tags document make contributions infer topic distribution document expected different tags works corresponding weights. example blog application blog tags author blog’s date blog category blog’s url. clearly compared tags author plays important role constituting topic components blog. making information. topic models using information contextual take advantage different past. twtm assume made observed tags weights. figure shows twtm works probabilistic graphical model. shown figure controlled sides topic distributions tags weights given tags document important distinguish twtm author-topic model author-topic model words chose given tags’ distribution twtm word observed tags document would make contributions. topic distribution matrix given tags sub-components secondly weight vector observed tags dimension represents weight importance associated corresponding tag. thus mixed corresponding weight values. fig. graphical model representation twtm distribution matrix whole tags distribution matrix words represents weight vector tags indicates topic components document. dirichlet prior bernoulli prior. process designates dirichlet distribution mult multinomial distribution column vector dirichlet prior. note indicates weight vector observed tags constituting topic proportions document transpose furthermore drawn dirichlet prior obtained matrix multiplication clearly result vector whose dimension depended number observed tags document step document ﬁrst generate document’s tags using bernoulli coin toss prior probability shown step draw generate remaining part generative process familiar shown above twtm introduce novel model topic proportions semi-structured document documentspecial tags text data. discussed dimension parameter changed different documents. could difﬁcult compute expected probability topic assignment tag-weighted topic assignment used twtm. then maximize lower bound respect variational parameters using variational expectation-maximization procedure follows. next maximize respect γik. adding lagrange multipliers terms contain taking derivative respect setting derivative zero yields obtain update equation denotes index dictionary. e-step update document initialized model parameters. reason different document different number tags keep updated document m-step estimation. inference twtm topic models inferential problem need solve compute posterior distribution hidden variables given document given document easily posterior distribution latent variables proposed model work make mean-ﬁeld variational algorithm efﬁciently obtain approximation posterior distribution latent variables. mean-ﬁeld variational inference minimize divergence variational posterior probability true posterior probability maximizing evidence lower bound single document obtain using jensen’s inequality ld-dimensional dirichlet parameter vector vector variational parameters variational distribution shown figure indicates entropy variational distribution m-step needs update four parameters tagging prior probability dirichlet prior tags’ weights topic distribution tags corpus probability word topic. document’s tag-set observed bernoulli prior unused included model completeness. given corpus estimated adding number i-th appears corpus. weights among observed tags. proposed solution problem dirichlet prior topic distribution means learn weights among dirichlet prior given tags among tags. call solution tagweighted dirichlet allocation handling unstructured documents hybrid corpus twda degenerates draws topic proportions document dirichlet distribution. extended model twtm twda uses parameter notations. unlike twtm convenience inference twda expanded matrix number given tags document number binary vector i-th document j-th corpus note that last dimension last dimensions last equal documents. detail setting shown later. twda deﬁnes dirichlet prior latent topic distribution document mixes latent topic proportion topic distributions given tags importance weight form ﬁnal topic distribution document. figure shows graphical model representation twda generative process twda given following procedure tag-weighted dirichlet allocation real world application corpus likely contain semi-structured documents unstructured documents. many documents corpus tags unstructured text data. case twtm work generates topic distribution document leveraging note that number tags appeared corpora number topics. different twtm column vector column vector. dirichlet prior. vector drawn transpose drawn dirichlet prior obtained matrix multiplication clearly result vector whose dimension depended number observed tags document note that number tags given described above. words treat topic distribution latent dirichlet prior document controlled latent idea twda latent dirichlet allocation form augmented matrix represents vector matrix last becomes matrix. show above matrix form given tags document last binary vector last dimension equals others equal deﬁne idea tag-weighted dirichlet allocation model topic proportions semi-structured documents document-special tags text data. different topic proportion document assumed model controlled dirichlet prior also observed tags. generate normalized topic distribution document dirichlet allocation tags information weight vector thus function topic assignment obtain topic distribution topic distribution simpliﬁed shown above draw dirichlet prior means topic proportions document draw dirichlet distribution basic assumption others words handling unstructured documents twda degenerates lda. words topic distribution document twtm weighted average topic distributions given tags extent linear relation topic distribution document tags. while twda addition dirichlet prior equal generate latent document special topic distribution non-linear topic generation procedure document. currently many applications appear large scale tagged documents highlight issues large scale semi-structured documents many areas. paper propose compare three different distributed methods based framework twtm focus challenge working large scale mapreduce programming framework. ﬁrst solution tailored parallel algorithm twtm. learning inference proposed model based variational method algorithm. thus design parallel algorithm twtm using mapreduce programming framework. shown above need update global parameters corpus. every document associated corresponding variational parameters mapper computes variational parameters document uses generate sufﬁcient statistics update andψ. denotes index dictionary. e-step update variational parameters document initialized model parameters. show detailed derivation variational parameters twda appendix four model parameters need estimate m-step dirichlet prior tags’ weights topic distribution tags corpus probability word topic dirichlet prior model. twda estimate twtm. variational expectation maximization procedure twda update variational parameters eqs. respectively e-step. m-step besides update also update newton-raphson algorithm. detailed derivation model parameter estimation twda shown appendix analysis twda twda introduce better directly model semi-structured documents unstructured documents adding latent documents topic distribution document controlled observed tags latent tag. topic distribution document drawn hyperparameter without considering given tags twtm topic distribution controlled list given tags corresponding weight values. main difference among models handle unstructured text semi-structured documents label-lda plda driver driver program marshals entire inference process. beginning driver initializes model parameters topic number user speciﬁed; number tags determined data; initial value given user randomly initialized. mapreduce iteration driver normalizes global note that global parameter corpus update iteration driver. however lead large scale data migration compute since associated document different documents different tags affect different dimensions whole corpus data would migrate single driver node. could generate bottleneck driver. documents contain plurality cluster. means documents divided mutually independent space tags. show detailed process clustering appendix mapreduce procedure solution following procedure. solution exact solution twtm equivalent solution documents belong cluster. however solution provides efﬁcient method solution depends result document clustering would anther bottleneck real applications. although solution approximate method modeling semi-structured documents effectively avoids bottleneck brought solution solution iii. experiment results section show solution works better solution solution iii. worth note solutions need iterate mapreduce procedure driver function convergence maximum number iterations reached. section show experimental results comparisons three solutions document modeling efﬁciency. experiments work used three semi-structured corpora. ﬁrst document collection data internet movie database data includes movie storylines words removing stop words tags. movies belong genres. tags used contain directors stars time movie keywords. second consists technical papers digital bibliography library project data collection bibliographic information major computer science journals proceedings. paper subset dblp contains abstracts papers words vocabulary unique tags. tags used dblp include authors keywords. last corpus used contains wordpress blog posts kaggle. corpus tags words. used corpus test effectiveness performance twtm large scale dataset. implemented three distributed methods twtm using hadoop experiments cluster containing physical nodes; node cores threads could conﬁgured maximum mappers reducers tasks. conﬁguration build different scales distributed environments setting maximum mappers used node. parts experiments. first trained four latent variable models including twtm twda corpora movie documents imdb compare generalization performance four models. part trains text data without taking advantage information. removed stop words conducted experiments using -fold crossvalidation. figure demonstrates perplexity results imdb data set. clearly twtm twda excel signiﬁcantly consistently. second order compare performance twtm twda topic models take advantage information trained twtm twda plda author topic model corrlda movie documents imdb computed perplexity test data set. since could handle corpus tags easily experiment treated given tags word features them. corrlda used tags document represent image segments corrlda handle ssds. figure demonstrates perplexity results models imdb data. experiment results shows twtm twda better models increases corrlda running over-ﬁtting trend twtm twda keeps going perplexity signiﬁcantly lower baselines. plda assumes tags optionally denote latent present every document thus trained plda twtm twda topics imdb data tags since plda latent topic takes part exactly collection. shown plda builds labeled latent topic topic approximately equivalent labeled lda. case trained plda topics. figure shows perplexity results twtm twda plda. note twda less mean squared error twtm. results figure shown twtm twda work well compared topic models make information. tf-idf word features features induced topic model tf-idf word features features generated twda model number topics features induced -topic twda model tf-idf word features respectively. experiments conducted multi-class classiﬁcation experiments using imdb data contains genres. calculated evaluation metrics provided class tags movies’ genres using -fold cross-validation. report movie classiﬁcation performance different methods figure signiﬁcant improvement classiﬁcation performance using twda comparing using tf-idf features twda outperforms tf-idf terms order show classiﬁcation performance better also calculated evaluation metrics fmeasure results f-measure reported table twda provides substantially better performance f-measure. demonstrated performance work model robustness part experimental analysis. part measured compared perplexity added noise tags information test documents using dblp data set. respectively randomly added noise tags test document calculated perplexity. example paper document dblp authors adding noise randomly selected author author process paper collection dblp. addition predicting tags given document evaluate ability proposed model compared corrlda predict tags document conditioned words document. part treat authors paper tags abstract word features predict authors paper modeling paper abstract document data using corrlda twda. model evaluate likelihood authors given word features document rank possible author likelihood function author. first model topic distribution test document dtest given author then evaluate dtest author tags corrlda authors represent image regions used shown evaluate likelihood author given document. method deﬁne shown note likelihoods given author document necessarily comparable among topic models however interested ranking trained three models dblp data using -fold cross-validation shows recall topic corpora results shown figure figure twda ranks authors consistently higher models. next experiment test classiﬁcation performance utilizing feature sets generated twda baselines. base classiﬁer libsvm gaussian kernel default parameters. purpose comparison trained four svms real-world applications noise tags appeared document relevance real tags. experiment selected noise tags author-tag meet real applications extent. experiment dblp corpora contains tags noise tags added test document would sparse whole corpora. added different percentages noise tags test document show trend perplexity noise content increases. figure shows twda steady trend noise level increases compared dmr. table shows examples weights original tags noise tags. tags noise added test data values behind weights among tags inference twda model. note that showed weight values normalized. results shown twda good performance model robustness weight values noise tags much smaller original tags. applications proposed model rank tags given document would good approach recommendation annotation. demonstrated performance three proposed parallelized solutions twtm largescale dataset training time accuracy document modeling suitable twda well. firstly measured compared training time solutions using wordpress blog data system setting model parameters. used doc-indexed sparse storage mode matrix ξ-document matrix would huge large scale data set. figure shows performance average training time secondly sampled training dataset wordpress corpus different sample ratios show performance running time different size training dataset. addition limited maximum number mappers conﬁguration trained described section solution would spend great deal time data migration update driver process solution resources taken clustering process mapper especially corpus nonhomogeneous leads uneven loading mapper. while solution avoids problems approximation method. lastly measured generalization capability three solutions using perplexity conducted experiments. held data test trained three solutions remaining observe relatively little difference among solutions compared standard twtm terms perplexity shown figure number topic increases. three solutions good approximations terms model ﬁtness. worthy note solution almost performance solution solution iii. conclusion tag-weighted topic model proposed paper provide analyze probabilistic approach mining semi-structured documents. meanwhile three distributed solutions twtm presented handle large scale problems. besides twtm able obtain topics distribution tags corpus useful text classiﬁcation clustering data mining applications. time propose novel framework processing tagged text high extensibility uses novel function tag-weighted topic assignment documents. extended model twda shows capability handling mixture corpora semi-structured documents unstructured documents. second beneﬁt tag-weighted topic model allows incorporate different types tags modeling documents provides general framework multi-tag modeling level tags also level documents. helps provide different approach classiﬁcation clustering recommendation large scale semi-structured documents proposed solutions shown effective efﬁcient complex applications. future plan apply twtm different practical areas fig. average training time iteration wordpress corpus different number mappers solution iii. note horizontal axis repesents maximum number mappers used training task. model described section demonstrate comparison performance three solutions restricted resources. figure figure show results average training time iteration three solutions using different sample ratios mappers dataset training setting number topic respectively. part experiments solution better performance efﬁciency solution iii. meanwhile order compare model plda used wordpress dataset tags train plda model trained twtm solution table shows comparison plda twtm solution daniel ramage christopher manning susan dumais. partially labeled topic models interpretable text mining. proceedings sigkdd international conference knowledge discovery data mining pages york acm. susan dumais michele banko eric brill jimmy andrew question answering always better? proceedings annual international sigir conference research development information retrieval sigir pages york acm. einat minkov william cohen andrew contextual search name disambiguation email using graphs. proceedings annual international sigir conference research development information retrieval sigir pages york acm. daniel ramage david hall ramesh nallapati christopher manning. labeled supervised topic model credit attribution multi-labeled corpora. emnlp pages xing bruce croft. lda-based document models ad-hoc retrieval. proceedings annual international sigir conference research development information retrieval sigir pages york acm. shuangyin received master degree school information science technology yat-sen university china master’s program focused research large scale image retrieval system hadoop platform. currently active within ﬁeld text mining artiﬁcial intelligence continues research track yat-sen university. research focuses topic model deep neural networks published several research mainly focused semi-structured documents modeling. jiefei received bachelor’s degree department computer science yat-sen university currently studying master’s degree yatsen university. research focuses topic model. guan huang received bachelor’s degree department computer science yat-sen university currently studying master degree yatsen university ﬁled word embedding topic model learning rank. ruiyang studying bachelor’s degree department computer science yat-sen university. participated acm/icpc twice times asia regional champions. rong received degrees applied mathematics university china respectively. postdoctoral fellow hong kong university science technology labs since then faculty member department computer science yat-sen university. research interest includes text mining recommender systems data mining machine learning. topic models inferential problem need solve compute posterior distribution hidden variables given document given document easily posterior distribution latent variables proposed model work make mean-ﬁeld variational algorithm efﬁciently obtain approximation posterior distribution latent variables. mean-ﬁeld variational inference minimize divergence variational posterior probability true posterior probability maximizing evidence lower bound single document obtain using jensen’s inequality m-step needs update four parameters tagging prior probability dirichlet prior tags’ weights topic distribution tags corpus probability word topic. appendix tag-weighted dirichlet allocation twda treat unknown constants estimated variational expectationmaximization procedure carry approximate maximum likelihood estimation twtm. given document easily posterior distribution latent variables twda model ld-dimensional dirichlet parameter vector vector vector variational parameters variational distribution. unlike twtm ldin twda number observed tags document indicates entropy variational distribution variational e-step single document variational parameters include γik. first maximize respect variational parameters obtain estimate posterior. m-step needs update parameters tagging prior probability dirichlet prior tags’ weights topic distribution tags corpus probability word topic dirichlet prior model. worthy note update method twtm. appendix cluster algorithm solution shown associated documents contain tag. thus running twtm cluster documents several clusters condition documents contain tags cluster. means documents divided mutually independent space tags. show simple example shown figure left panel. fig. left example clustering result. represents document corpora column represents means given documents circle belong cluster documents blue circle belong another cluster. right illustration update combine different parts. document clustering tags contained cluster appeared clusters. case could assign cluster different computed nodes. update simply combine number document clusters shown figure right panel. show cluster process solution algorithm input semi-structured corpora corpora. output cluster contains clusters cluster contains documents. create cluster create document cluster create added docs store documents ready cluster create scanned tags store tags scanned. return", "year": 2015}