{"title": "Deep Kernel Learning", "tag": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "abstract": "We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost $O(n)$ for $n$ training points, and predictions cost $O(1)$ per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.", "text": "introduce scalable deep kernels combine structural properties deep learning architectures non-parametric ﬂexibility kernel methods. speciﬁcally transform inputs spectral mixture base kernel deep architecture using local kernel interpolation inducing points structure exploiting algebra scalable kernel representation. closed-form kernels used drop-in replacements standard kernels beneﬁts expressive power scalability. jointly learn properties kernels marginal tions including dataset million examples show improved performance scalable gaussian processes ﬂexible kernel learning models stand-alone deep architectures. gaussian processes possibly replace neural networks? thrown baby bathwater? questioned mackay late researchers grown frustrated many design choices associated neural networks regarding architecture activation functions regularisation lack principled framework guide choices. gaussian processes recently popularised within machine learning community neal shown bayesian neural networks inﬁnitely many hidden units converged gaussian processes particular kernel function. gaussian processes subsequently viewed ﬂexible interpretable alternatives neural networks straightforward learning procedures. neural networks used ﬁnitely many highly adaptive basis functions gaussian processes typically used inﬁnitely many ﬁxed basis functions. argued mackay hinton bengio neural networks could automatically discover meaningful representations highdimensional data learning multiple layers highly adaptive basis functions. contrast gaussian processes popular kernel functions used typically simple smoothing devices. recent approaches demonstrated develop expressive kernel functions indeed able discover rich structure data without human intervention. methods eﬀectively inﬁnitely many adaptive basis functions. relevant question becomes paradigm replaces other whether combine advantages approach. indeed deep neural networks provide powerful mechanism creating adaptive basis functions inductive biases proven eﬀective learning many application domains including visual object recognition speech perception language understanding information retrieval paper combine non-parametric ﬂexibility kernel methods structural properties deep neural networks. particular deep feedforward fullyconnected convolutional networks combination spectral mixture covariance functions inducing points structure exploiting algebra local kernel interpolation create scalable expressive closed form covariance kernels gaussian processes. non-parametric method information capacity model grows amount available data complexity automatically calibrated marginal likelihood gaussian process without need regularization cross-validation ﬂexibility automatic calibration provided non-parametric layer typically provides high standard performance reducing need extensive hand tuning user. point allowing fast prediction times. kiss-gp creates approximate kernel user speciﬁed kernel fast computations independently speciﬁc inference procedure view resulting kernel scalable deep kernel. demonstrate value scalability experimental results section large datasets provide greatest opportunities model discover expressive statistical representations. begin reviewing related work section providing background material gaussian processes section section derive scalable closed form deep kernels describe perform eﬃcient automatic learning kernels gaussian process marginal likelihood. section show substantially improved performance standard gaussian processes expressive kernel learning approaches deep neural gaussian process regression network replaces weight connections bayesian neural network gaussian processes allowing authors model input dependent correlations multiple tasks. alternatively damianou lawrence replace every activation function bayesian neural network gaussian process transformation unsupervised setting. promising models task speciﬁc require sophisticated approximate bayesian inference much demanding required standard gaussian processes deep learning models typically scale beyond thousand training points. similarly salakhutdinov hinton combine deep belief networks gaussian processes showing improved performance standard kernels context semi-supervised learning. however model heavily relying unsupervised pretraining dbns component unable scale beyond thousand training points. likewise calandra combine feedforward neural network transformation gaussian process showing ability learn sharp discontinuities. however similar many approaches resulting model scale thousand data points. frequentist setting yang combine convolutional networks parameters pre-trained imagenet scalable fastfood expansion kernel applied ﬁnal layer. resulting method scalable ﬂexible network parameters generally must ﬁrst trained separately fastfood features combined model remains parametric parametric expansion provided fastfood. careful attention must still paid training procedures regularization manual calibration network architecture. similar manner huang snoek combined deep architectures parametric bayesian models. huang pursue unsupervised pre-training procedure using deep autoencoders showing improved performance using standard kernels. snoek show promising performance bayesian optimisation tasks tuning parameters deep neural network. approach distinct combine deep feedforward convolutional architectures spectral mixture covariances inducing points kronecker toeplitz algebra local kernel interpolation derive expressive scalable closed form kernels trained jointly uniﬁed supervised objective part non-parametric gaussian process framework without requiring approximate bayesian inference. moreover simple joint learning procedure approach applied general settings. indeed show proposed model outperforms state stand-alone deep learning architectures gaussian processes advanced kernel learning procedures wide range datasets demonstrating practical signiﬁcance. achieve scalability retaining non-parametric model structure leveraging recent kiss-gp approach extensions wilson eﬃciently representing kernel functions produce scalable deep kernels. brieﬂy review predictive equations marginal likelihood gaussian processes associated computational requirements following notational conventions wilson example rasmussen williams comprehensive discussion gps. kernels correspond models inﬁnite basis expansion dual space compelling theoretical properties models universal approximators prior support within arbitrarily small epsilon band continuous function indeed properties distribution functions induced gaussian process controlled kernel function. example popular kernel structure data discovered learning interpretable kernel hyperparameters. marginal likelihood targets probability data conditioned kernel hyperparameters provides principled probabilistic framework kernel learning used shorthand given note expression marginal likelihood pleasingly separates automatically calibrated model complexity terms kernel learning achieved optimizing respect computational bottleneck inference solving linear system kernel learning computing determinant |kxx marginal likelihood. standard approach compute cholesky decomposition matrix requires operations storage. inference complete predictive mean costs predictive variance costs test point section show contruct kernels encapsulate expressive power deep architectures learn properties kernels part scalable probabilistic gaussian process framework. widths frequencies. spectral mixture kernel forms expressive basis stationary covariance functions discover quasi-periodic stationary structure interpretable succinct representation deep learning transformation captures non-stationary hierarchical structure. figure deep kernel learning gaussian process deep kernel maps dimensional inputs parametric hidden layers followed hidden layer inﬁnite number basis functions base kernel hyperparameters overall gaussian process deep kernel produces probabilistic mapping inﬁnite number adaptive basis functions parametrized deep kernel covariance function gaussian process model conditioned kernel hyperparameters interpret model applying gaussian process base kernel ﬁnal hidden layer deep network. since base kernel corresponds inﬁnite basis function representation network eﬀectively hidden layer inﬁnite number hidden units. overall model shown figure include weights network parameters base kernel indeed compartmentalizing model base kernel deep architecture pedagogical clarity. applying gaussian process deep kernel operates single unit drop-in replacement e.g. standard mat´ern kernels since learning inference follow procedures. absorbed noise covariance covariance matrix treat part base kernel hyperparameters derivatives deep kernel respect base kernel hyperparameters conditioned ﬁxed transformation inputs similarly implicit derivatives deep kernel respect holding ﬁxed. derivatives respect weight variables sparse matrix interpolation weights containing non-zero entries local cubic interpolation covariance matrix created deep kernel evaluated latent inducing points i=...m. place inducing points regular multidimensional lattice exploit resulting decomposition kronecker product toeplitz matrices extremely fast matrix vector multiplications without requiring grid structure data inputs transformed inputs kiss-gp operates creating approximate kernel admits fast computations independent speciﬁc inference learning procedure view kiss approximation applied deep kernels stand-alone kernel kdeep combined gaussian processes kernel machines scalable learning. table comparative rmse performance runtime regression datasets training points input dimensions. results averaged equal partitions data standard deviation. best denotes best-performing kernel according yang following yang exact gaussian processes intractable large data used here fastfood ﬁnite basis function expansions used approximation veriﬁed datasets exact kernels provide comparable performance fastfood expansions. datasets used fully-connected architecture used architecture. consider scalable deep kernel learning base kernels. base kernel datasets training instances larger datasets. practice depends conditioning kiss-gp covariance matrix rather number training points estimating determinant marginal likelihood follow approach described wilson nickisch extensions wilson kiss-gp training scales typically close linear versus conventional scalable approaches require computations need tractability results severe deteriorations predictive performance. ability large allows kiss-gp evaluate proposed deep kernel learning method wide range regression problems including large diverse collection regression tasks repository orientation extraction face patches magnitude recovery handwritten digits step function recovery show proposed algorithm substantially outperforms gaussian processes expressive kernel learning approaches deep neural networks without signiﬁcant increases computational overhead. deep kernel learning model ﬁrst train deep neural network using squared loss objective rectiﬁed linear activation functions. neural network pre-trained kiss-gp model ﬁtted using top-level features model inputs. using pre-training initialization joint deep kernel learning model section trained optimizing hyperparameters deep kernel backpropagating derivatives marginal likelihood gaussian process figure left randomly sampled examples training test data. right dimensional outputs convolutional network test cases. point shown using line segment orientation input face. consider large regression problems varying sizes properties. table reports test root mean squared error many scalable gaussian process kernel learning methods based fastfood stand-alone deep neural networks proposed combined deep kernel learning model using base kernels. table shows datasets method strongly outperforms gaussian processes standard kernel also best-performing kernels selected wide range alternative kernel learning procedures compared stand-alone deep neural networks exact architecture component dkl. combining kiss-gp dnns part joint procedure obtain consistently better results stand-alone deep learning datasets. moreover using spectral mixture base kernel create deep kernel provides notable additional performance improvements. interesting observe eﬀectively learning salient features data plain dnns generally achieve competitive performance compared expressive gaussian processes. combining complementary advantages approaches scalable deep kernels consistently brings substantial additional performance gains. next investigate runtime dkl. table right panel compares stand-alone terms runtime evaluating objective derivatives addition improving accuracy table rmse performance olivetti mnist. comparison face orientation extraction trained amount training instances dbn-gp used labels; whereas dbn-gp scaled labeled images modeled remaining data unsupervised pretraining dbn. used base kernel within gps. combining kiss-gp dnns deep kernels introduces negligible runtime costs kiss-gp imposes additional runtime runtime typically requires. overall results show general applicability practical signiﬁcance scalable approach. consider task predicting orientation face extracted grayscale image patch explored salakhutdinov hinton investigate procedure eﬃciently learning meaningful representations high-dimensional highlystructured image data. olivetti face data contains images forty diﬀerent people images total. following salakhutdinov hinton constructed datasets images randomly rotating cropping subsampling original images. randomly select people uniformly collect images training data using images remaining people test data. figure shows randomly sampled examples training test data. training olivetti face patches used convolutional network consisting convolutional layers followed fully-connected layers mapping face patch -dimensional feature vector base kernel. describe convolutional architecture detail appendix. table shows rmse predicted face orientations using four models. dbngp model proposed salakhutdinov hinton ﬁrst extracts features data using deep belief network applies gaussian process kernel. however approach could handle thousand labelled datapoints complexity standard gaussian processes. remaining data modeled unsupervised learning leaving large amount available labels unused. proposed deep kernel methods contrast scale linearly size training data capable directly modeling full labeled data accurately recover salient patterns. figure right panel shows deep kernel discovers features essential orientation prediction ﬁltering irrelevant factors identities scales. figure left rmse number training examples. middle runtime right total training time dashed line black indicates slope convolutional networks used within dkl. kernel. figure left panel validates beneﬁt scaling large data. training data used model continues increase accuracy. indeed large datasets provide greatest opportunities model discover expressive statistical representations. figure show spectral density base kernels learned deep kernel learning method. expressive spectral mixture kernel discovers structure peaks frequency domain. kernel able single gaussian spectral domain centred origin. attempt capture signiﬁcant mass near frequency kernel spectral density spreads across whole frequency domain missing important local correlations near frequency thus erroneously discarding much network features white noise since broad spectral peak corresponds short length-scale. result provides intuition spectral mixture base kernels generally perform much better base kernels despite ﬂexibility deep architecture. beneﬁt base kernel figure show learned covariance matrices constructed whole deep kernels base kernels. covariance matrix evaluated test inputs randomly sample instances test sort terms orientation angles input faces. deep kernels base kernels discover faces similar rotation angles highly correlated concentrating largest entries diagonal deep kernel learning base kernel captures correlations strongly base kernel somewhat diﬀuse. figure right panel also show learned covariance matrix kernel standard gaussian process applied data inputs. entries diﬀuse. essence deep kernel learning learn metric faces similar rotation angles highly correlated thus overcome fundamental limitations euclidean distance metric similar rotation angles particularly correlated regardless hyper-parameters learned euclidean kernels. next measure scalability model. figure middle panel shows runtimes seconds function training instances evaluating objective relevant derivatives. that scalable kiss-gp joint model achieves roughly linear asymptotic scaling slope figure right panel show total training time changes varying data size addition linear scaling necessary modeling large data added time combining kiss-gp cnns reasonable especially considering gains performance expressive power. figure left induced covariance matrix using dkl-sm kernel test cases test samples ordered according orientations input faces. middle respective covariance matrix using dkl-rbf kernel. right respective covariance matrix using regular kernel. models trained base kernel. handwritten digits used convolutional neural network similar architecture lenet table shows performs considerably better dbn-gp improves cnn. considered rmse comparison alternative methods posterior predictive distributions readily available problems rmse historically used benchmark. however advantage stand-alone deep architectures ability naturally produce posterior predictive distribution especially useful applications reinforcement learning bayesian optimisation. figure consider example learn posterior predictive distribution step function many challenging discontinuities. problem particularly diﬃcult conventional gaussian process approaches strong smoothness assumptions intrinsic popular kernels. kernels improve upon kernels neither properly adapt many sharp changes covariance structure. contrast dkl-sm model accurately encodes discontinuities function reasonable uncertainty whole domain. explored scalable deep kernels combine structural properties deep architectures non-parametric ﬂexibility kernel methods. particular transform inputs base kernel deep architecture leverage local kernel interpolation inducing points structure exploiting algebra scalable kernel representation. scalable kernels combined time. moreover spectral mixture covariances base kernel provides signiﬁcant additional boost representational power. overall scalable deep kernels used place standard kernels following inference learning procedures beneﬁts expressive power eﬃciency. show wide range experiments general applicability practical signiﬁcance approach consistently outperforming scalable expressive kernels stand-alone dnns. major challenge developing expressive kernel learning approaches euclidean absolute distance based metrics pervasive families kernel functions mat´ern kernels. indeed although intuitive cases cannot expect euclidean absolute distance measures similarity generally applicable especially problematic high dimensional input spaces modern approaches attempt learn ﬂexible parametric family example weighted combinations known kernels still fundamentally limited standard notions distance. seen olivetti faces examples approach allows whole functional form metric learned ﬂexible manner expressive transformations input space. expect metric learning particularly valuable high dimensional classiﬁcation problems view promising direction future research. hope work help bring together research neural networks kernel methods inspire many models unifying perspectives combine complementary advantages approaches.", "year": 2015}