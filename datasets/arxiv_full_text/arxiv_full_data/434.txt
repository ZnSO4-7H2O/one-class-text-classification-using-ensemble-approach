{"title": "Training Probabilistic Spiking Neural Networks with First-to-spike  Decoding", "tag": ["stat.ML", "cs.AI", "cs.IT", "cs.LG", "cs.NE", "math.IT"], "abstract": "Third-generation neural networks, or Spiking Neural Networks (SNNs), aim at harnessing the energy efficiency of spike-domain processing by building on computing elements that operate on, and exchange, spikes. In this paper, the problem of training a two-layer SNN is studied for the purpose of classification, under a Generalized Linear Model (GLM) probabilistic neural model that was previously considered within the computational neuroscience literature. Conventional classification rules for SNNs operate offline based on the number of output spikes at each output neuron. In contrast, a novel training method is proposed here for a first-to-spike decoding rule, whereby the SNN can perform an early classification decision once spike firing is detected at an output neuron. Numerical results bring insights into the optimal parameter selection for the GLM neuron and on the accuracy-complexity trade-off performance of conventional and first-to-spike decoding.", "text": "third-generation neural networks spiking neural networks harnessing energy efﬁciency spike-domain processing building computing elements operate exchange spikes. paper problem training two-layer studied purpose classiﬁcation generalized linear model probabilistic neural model previously considered within computational neuroscience literature. conventional classiﬁcation rules snns operate ofﬂine based number output spikes output neuron. contrast novel training method proposed ﬁrst-to-spike decoding rule whereby perform early classiﬁcation decision spike ﬁring detected output neuron. numerical results bring insights optimal parameter selection neuron accuracy-complexity trade-off performance conventional ﬁrst-to-spike decoding. current machine learning methods rely secondgeneration neural networks consist simple static non-linear neurons. contrast neurons human brain known communicate means sparse spiking processes. result mostly inactive energy consumed sporadically. third-generation neural networks spiking neural networks harnessing energy efﬁciency spike-domain processing building computing elements operate exchange spikes snns natively implemented neuromorphic chips currently developed within academic projects major chip manufacturers. proof-of-concept implementations shown remarkable energy savings multiple orders magnitude respect second-generation neural networks notwithstanding potential snns signiﬁcant stumbling block adoption dearth ﬂexible effective learning algorithms. existing algorithms based variations unsupervised mechanism spike-timing dependent plasticity updates synaptic weights based local input output spikes supervised variations leverage global feedback another common approach convert trained second-generation networks snns among learning methods attempt directly maximize spike-domain performance criterion techniques assume deterministic spike response model neurons propose various approximations cope non-differentiability neurons’ outputs references therein). probabilistic models spiking neurons standard context computational neuroscience probabilistic modeling sparsely considered machine learning literature snns. despite known increased ﬂexibility expressive power probabilistic models context snns example probabilistic models capability learning ﬁring thresholds using standard gradient based methods deterministic models instead treated hyperparameters using heuristic mechanisms homeostasis state supervised learning probabilistic models considers stochastic gradient descent binary signal emitted j-th presynaptic postsynaptic neurons respectively time also vector samples spiking process presynaptic neuron time interval similarly vector contains samples spiking process neuron interval seen fig. output postsynaptic neuron time bernoulli distributed ﬁring probability depends past spiking behaviors {xt− presynaptic neurons window duration samples well past spike timings neuron window duration samples. mathematically membrane potential postsynaptic neuron time given vector deﬁnes synaptic kernel applied synapse presynaptic neuron postsynaptic neuron feedback kernel bias parameter. vector variable parameters includes bias parameters deﬁne ﬁlters discussed below. accordingly log-probability entire spike train conditioned input spike trains {xj}nx written unlike prior work snns neurons adopt parameterized model introduced ﬁeld computational neuroscience. accordingly ﬁlters parameterized ﬁxed basis functions learnable weights. elaborate write paper study problem training two-layer illustrated fig. probabilistic neuron model purpose classiﬁcation. conventional decoding snns operates ofﬂine selecting output neuron hence corresponding class largest number output spikes contrast study ﬁrst-to-spike decoding rule whereby perform early classiﬁcation decision spike ﬁring detected output neuron. generally reduces decision latency complexity inference phase. ﬁrst-to-spike decision method investigated temporal rather rate coding deterministic neurons learning algorithm exists probabilistic neural models. ﬁrst propose ﬂexible computationally tractable generalized linear model introduced context computational neuroscience model derive novel sgd-based learning algorithm maximizes likelihood ﬁrst spike observed correct output neuron finally present numerical results bring insights optimal parameter selection neuron accuracy-complexity trade-off performance conventional ﬁrst-to-spike decoding rules. architecture. consider problem classiﬁcation using two-layer snn. shown fig. fully connected presynaptic neurons input sensory layer neurons output layer. output neuron associated class. order feed input example e.g. gray scale image converted discrete-time spike trains samples rate encoding. input spike trains postsynaptic neurons output discrete-time spike trains. decoder selects image class basis spike trains emitted output neurons. rate encoding method entry input signal e.g. pixel images converted discrete-time spike train generating independent identically distributed bernoulli vectors. probability generating i.e. spike proportional value entry. experiments sec. gray scale images pixel intensities yield spike probability neuron model. relationship input spike trains presynaptic neurons output spike train postsynaptic neuron follows illustrated fig. elaborate denote inference phase ﬁrst-to-spike decoding decision made ﬁrst spike observed output neuron. order train classiﬁcation rule propose follow criterion maximizing probability ﬁrst spike output neuron corresponding correct label logarithm probability given example written based resulting update considered neo-hebbian rule since multiplies contributions presynaptic neurons postsynaptic activity former depends latter potential uit. furthermore probabilities ﬁring time weighted probability probability correct neuron ﬁrst spike ﬁres time given ﬁrst spike time interval basis vectors; {wjik} {vik} learnable weights kernels respectively. parameterization generalizes previously studied models machine learning application. instance special case weights equation yields discrete-time approximation model considered another example all-zero vector except position yields unstructured model considered experiments discussed sec. adopt time-localized raised cosine basis functions introduced illustrated fig. note model ﬂexible enough include learning synaptic delays section brieﬂy review training based conventional rate decoding two-layer snn. inference phase decoding conventionally carried post-processing selecting output neuron largest number spikes. order facilitate success decoding rule training phase postsynaptic neuron corresponding correct label typically assigned desired output spike train number spikes zero output assigned postsynaptic neurons extended examples training set. parameter vector includes parameters {wi}ny negative log-likelihood convex respect minimized sgd. completeness report gradients appendix practical note order avoid vanishing values calculating weights compute probability term log-domain normalize resulting terms respect minimum probability section numerically study performance probabilistic fig. conventional ﬁrst-to-spike decoding rules. standard mnist dataset input data. result input neuron pixel images. following consider different number classes digits namely digits four digits digits samples class training number test set. desired spike train spike every three zeros training conventional decoding. minibatch size training epochs used schemes. ten-fold cross-validation applied selecting constant learning rates. model parameters randomly initialized uniform distribution evaluate performance schemes terms classiﬁcation accuracy test inference complexity. inference complexity measured total number elementary operations namely additions multiplications input image required inference. number arithmetic operations needed calculate membrane potential neuron time instant ﬁrst consider test classiﬁcation accuracy function number basis functions neural model. basis functions numbered fig. fig. observe conventional decoding requires large number order obtain best accuracy. need ensure correct output neuron ﬁres consistently neurons response input spikes. this turn requires larger temporal reception ﬁeld i.e. larger sensitive randomly located input spikes. note small values ﬁrstto-spike decoding obtains better accuracies conventional decoding. fig. depicts test classiﬁcation accuracy versus inference complexity conventional ﬁrst-to-spike decoding rules digits classiﬁcation accuracy conventional two-layer artiﬁcial neural network logistic neurons added comparison. ﬁgure ﬁrst-to-spike decoding seen offer signiﬁcantly lower inference complexity thanks capability early decisions without compromising accuracy. instance classiﬁcation accuracy equals complexity conventional decoding method times larger ﬁrst-to-spike method. note also conventional decoding generally requires large values perform satisfactorily. paper proposed novel learning method probabilistic two-layer operates according ﬁrstto-spike learning rule. demonstrated proposed method improves accuracy-inference complexity trade-off respect conventional decoding. additional work needed order generalize results multi-layer networks. diamond nowotny schmuker comparing neuromorphic solutions action implementing bio-inspired solution benchmark classiﬁcation task three parallel-computing platforms front. neurosci. vol. taherkhani belatreche maguire dl-resume delay learning-based remote supervised method spiking neurons ieee trans. neural netw. learn. syst. vol. anwani rajendran normad-normalized approximate descent based supervised learning rule spiking neurons proc. ieee int. joint conf. neural netw. pillow paninski uzzell simoncelli chichilnisky prediction decoding retinal ganglion cell responses probabilistic spiking model neurosci. vol. mozafari kheradpisheh masquelier nowzaridalini ganjtabesh first-spike based visual categorization using reward-modulated stdp arxiv preprint arxiv. wang belatreche maguire mcginnity spiketemp enhanced rank-order-based learning approach spiking neural networks adaptive structure ieee trans. neural netw. learn. syst. vol. pillow shlens paninski sher litke chichilnisky simoncelli spatio-temporal correlations visual signalling complete neuronal population nature vol.", "year": 2017}