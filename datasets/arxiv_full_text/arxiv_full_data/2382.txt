{"title": "Advances in Learning Bayesian Networks of Bounded Treewidth", "tag": ["cs.AI", "cs.LG", "stat.ML", "68T37"], "abstract": "This work presents novel algorithms for learning Bayesian network structures with bounded treewidth. Both exact and approximate methods are developed. The exact method combines mixed-integer linear programming formulations for structure learning and treewidth computation. The approximate method consists in uniformly sampling $k$-trees (maximal graphs of treewidth $k$), and subsequently selecting, exactly or approximately, the best structure whose moral graph is a subgraph of that $k$-tree. Some properties of these methods are discussed and proven. The approaches are empirically compared to each other and to a state-of-the-art method for learning bounded treewidth structures on a collection of public data sets with up to 100 variables. The experiments show that our exact algorithm outperforms the state of the art, and that the approximate approach is fairly accurate.", "text": "work presents novel algorithms learning bayesian network structures bounded treewidth. exact approximate methods developed. exact method combines mixed-integer linear programming formulations structure learning treewidth computation. approximate method consists uniformly sampling k-trees subsequently selecting exactly approximately best structure whose moral graph subgraph k-tree. properties methods discussed proven. approaches empirically compared state-of-the-art method learning bounded treewidth structures collection public data sets variables. experiments show exact algorithm outperforms state approximate approach fairly accurate. bayesian networks graphical models widely used represent joint probability distributions complex multivariate domains bayesian network comprises parts directed acyclic graph describing relationships among variables model collection conditional probability tables joint distribution reconstructed. number variables model increases specifying underlying structure becomes tedious difﬁcult task practitioners often resort learning bayesian networks directly data. here learning bayesian network refers inferring underlying graphical structure data task well-known np-hard ∗email niesrpi.edu. afﬁliation rensselaer polytechnic institute usa. †email denis.mauausp.br. afﬁliation universidade s˜ao paulo brazil. ‡email cassioidsia.ch. afﬁliation dalle molle institute artiﬁcial intelligence switzerland. §email jiqrpi.edu. afﬁliation rensselaer polytechnic institute usa. learned bayesian networks commonly used drawing inferences querying posterior probability variable evidence entered ﬁnding mode joint distribution selecting conﬁguration subset variables maximizes conditional probability inferences np-hard compute even approximately known algorithms worst-case time complexity exponential treewidth measure connectedness graph. polynomial-time algorithms inferences exist provide guarantees quality solution deliver raises doubts whether occasional results consequence suboptimal structure learning approximate inference. fact widely believed assumptions complexity theory exponential time complexity treewidth inevitable algorithm provides provably good inferences thus learning network structures small treewidth essential wishes perform reliable efﬁcient inference. particularly important presence missing data learning methods usually resort kind expectation-maximization procedure requires performing belief updating network every iteration cases inefﬁcient inference leads great computational cost learning; unreliable inference leads learning underﬁtted/overﬁtted structures. since estimating network’s treewidth np-hard task extending current methods learning bayesian networks case bounded treewidth maintaining relative efﬁciency accuracy trivial. comparison unconstrained bayesian network learning algorithms designed bounded treewidth case. korhonen parviainen showed learning bounded treewidth bayesian networks np-hard developed exact algorithm based dynamic programming learns optimal n-node structures treewidth time nnω+o time required best worst-case algorithms learning optimal bayesian networks constraint treewidth elidan gould combined several heuristics treewidth computation network structure learning order design approximate methods. others addressed similar problem learning undirected models bounded treewidth recently seems increase interest topic. berg showed problem learning bounded treewidth bayesian networks reduced weighted maximum satisﬁability problem subsequently solved weighted max-sat solvers. report experimental results showing approach outperforms korhonen parviainen’s dynamic programming approach. year parviainen showed problem reduced mixed-integer linear program solved off-the-shelf milp optimizers reduced milp problem however exponentially many constraints number variables. following work cussens authors avoid creating large programs cutting plane generation mechanism iteratively includes constraint optimum found. generation constraint requires solving another milp problem. works developed independently simultaneously work presented here; reason compare methods paper present novel ideas score-based bayesian network structure learning hard constraint treewidth. ﬁrst introduce mixed integer linear programming formulation problem builds existing milp formulations unconstrained structure learning bayesian networks computing treewidth graph designed formulation able score-maximizer bayesian network treewidth smaller given constant models containing many variables korhonen parviainen’s method empirically demonstrate section unlike milp formulation parviainen milp problem generate polynomial size number variables require cutting planes techniques. makes clean succinct formulation solved single call milp optimizer. better understanding cases approach preferred achieved. since linear programming relaxations used solving milp problem milp formulation used provide approximate solutions error estimates anytime fashion however milp formulations cannot cope large domains even agreed obtaining approximate solutions. minimum size milp problems cubic number variables probably little considerably improve situation limitation observed experiments reported section milp formulation requires much larger amount time obtain much poorer solutions networks variables. order deal large domains devise approximate method based uniform sampling k-trees achieved using fast computable bijection k-trees dandelion codes sampled k-tree either exact algorithm similar proposed learn scoremaximizing network whose moral graph subgraph k-tree resort much efﬁcient method takes partial variable orderings uniformly random space orderings compatible k-tree. discuss time sample complexity variants compare similar schemes learning unconstrained networks. show empirically double sampling scheme effective learning close optimal structures selected data sets. conclude section noting methods propose considered state-of-the-art suggesting possible improvements. start section presents background knowledge learning bayesian networks. bayesian network concise graphical representation multivariate domain random variable associated node underlying directed acyclic graph local conditional probability distributions speciﬁed variable given parents graph consider ﬁnite categorical random variables taking values ﬁnite sets formally bayesian network triple whose nodes one-to-one correspondence variables {θi} numerical parameters specifying probability values every node value assignment parents according structure represents stochastic independence assessments among variables particular represents graphical markov conditions every variable conditionally independent nondescendant nonparents given parents. consequence bayesian network uniquely deﬁnes joint probability distribution product parameters learning structure data challenging problem. approach identify variable minimal variables makes variable conditionally independent others usually done means statistical tests stochastic independence information theoretic measures alternatively structural learning posed combinatorial optimization problem seeks structure maximizes score function relates data likelihood avoiding excessive model complexity. commonly used score functions include minimum description length bayesian dirichlet equivalent uniform score functions follow different rationale satisfy properties written local score functions depend parent node data local score functions efﬁciently computed stored. score-based structure learning difﬁcult task research topic active score-based bayesian network learning seek structure class dags nodes local score functions depend parent given depends values take data set). assume local scores previously computed retrieved constant time. despite decomposability score functions optimization cycle undirected graph chord nodes cycle connected edge outside cycle. chordal graph undirected graph cycles length four chord. graph made chordal inserting edges process called chordalization treewidth chordal graph size largest clique minus one. treewidth arbitrary undirected graph minimum treewidth chordalizations moral graph undirected graph obtained connecting nodes common child dropping directions. treewidth treewidth corresponding moral graph. treewidth bayesian network treewidth elimination order linear ordering nodes graph. elimination order perfect every node order higher-ordered neighbors form clique graph admits perfect elimination order chordal. perfect elimination orders computed linear time exist. elimination node according elimination order process pairwise connecting higher-ordered neighbors. thus elimination nodes produces chordal graph elimination order used perfect. edges inserted elimination process called ﬁll-in edges. given perfect elimination order treewidth graph computed maximum number higher ordered neighbors graph. reason score functions penalize model complexity data likelihood always increases augmenting number parents variable leads overﬁtting poor generalization. scores penalize model complexity generally leads structures bounded in-degree helps preventing overﬁtting even bounded in-degree graphs large treewidth yields great problem subsequent probabilistic inferences model. least direct reasons learning bayesian networks bounded treewidth discussed previously known exact algorithms probabilistic inference exponential time complexity treewidth networks high treewidth usually challenging approximate methods; previous empirical results suggest bounding treewidth might improve model performance held-out data. also evidence bounding treewidth impose great burden expressivity model real data sets goal learning bayesian networks bounded treewidth search srebro’s complexity result markov networks show learning structure bayesian networks bounded treewidth strictly greater np-hard. dasgupta’s results also prove hardness score maximizes data likelihood ﬁrst contribution work mixed integer linear programming formulation design exactly solve problem structure learning bounded treewidth. milp formulations shown effective learning bayesian networks without treewidth bound surpassing attempts range data sets. moreover great language power milp problem allows encode treewidth constraint natural manner might easy structure learning approaches note computing treewidth graph np-hard problem even linear algorithms exponential treewidth hence hope enforce bound treewidth without machinery least powerful novel formulation based combining milp formulation structure learning milp formulation presented computing treewidth undirected graph. although crucial differences highlight later avoided sophisticated techniques milp context structure learning constraint generation interested providing clean succinct milp formulation using off-the-shelf solvers without additional coding. since formulation combination previous milp formulations distinct problems present formulation separately describe combine concise milp problem. formulation based encoding possible elimination orders nodes chordalization treewidth obtained feasible solution program setting constraint ensures treewidth bounding number higher-ordered neighbors every node variables take values partially deﬁne elimination order nodes node eliminated node order need linear cases multiple linearizations partial order equally good building chordalization cases nodes might assigned value indicating eliminating converse results chordal graphs treewidth. variables }-valued indicate whether node precedes order edge exists among resulting chordal graph although values forced integers formulation practice likely constraint allows appears order constraint ensures supergraph constraint guarantees elimination ordering induced perfect higher ordered neighbors also neighbors either must practical difference formulation respect lies fact allow partial elimination orders need integer variables enforce orders. bottleneck speciﬁcation constraint constraints. following result immediate conclusion reasoning. turn attention milp formulation structure learning part. consider chordal graph perfect elimination order }-valued variables contains eliminated node collection allowed parent sets node denote element |fi| following milp formulation speciﬁes class dags consistent scope constraint |fi|. obtained solution program setting fit}. variables take values partially specify topological order nodes ancestor variables |fi| }-valued indicate whether t-th parent chosen node constraint enforces exactly parent chosen node. constraint forces choices acyclic respect topological order induced variables order need linear. fact relative ordering nodes connected relevant constraints ensure arcs appear corresponding edges moral graph exist responsible moralization graph falling inside theorem |pi| variables satisfying constraints deﬁne directed graph s.t. fit}. acyclic consistent parents sets treewidth corollary |pi| maximize satisfy deﬁned solution optimization milp formulation directly off-the-shelf milp optimizer. according corollary outcome always optimum structure enough resources given. standard milp optimizers often employ branch-and-bound procedures able halted prematurely time still provide valid solution outer bound maximum score. hence milp formulation also provides anytime algorithm learning bayesian networks bounded treewidth procedure stopped time still provide approximate solutions error bound. moreover quality approximation solution returned increases time error bounds monotonically decrease eventually converge zero. validate practical feasibility milp formulation compare dynamic programming method proposed previously problem call table show time performance milp formulation collection reasonably small data sets repository small values treewidth bound. details data presented section experiments limit memory usage maximum number parents node equal three shall careful directly comparing times methods implementations different languages note milp formulation orders magnitude faster able solve many problems latter could time limit given milp case estimation error reported table computational time optimal bayesian network structure. empty cells indicate method failed solve instance excessive memory consumption. limit given milp case estimation error reported. mean seconds minutes hours respectively. results table show milp formulations largely outperforms able handle much larger problems. experiments algorithms scale poorly number variables. particular cannot cope data sets containing dozen variables. results suggest milp problems become easier treewidth bound increases. likely consequence increase space feasible solutions makes linear relaxations used solving milp problem tighter thus reducing computational load. probably aggravated small number variables data sets shall demonstrate empirically section quality solutions found milp approach reasonable amount time degrades quickly number variables reaches several dozens. indeed milp formulation unable reasonable solutions data sets containing variables surprising given number constraints cubic number variables; thus increases even linear relaxations milp problem become hard solve. next section present clever sampling algorithm space k-trees overcome limitations handle large domains. milp formulation described baseline performance approximate approach. section develop approximate method learning bounded treewidth bayesian networks based sampling graphs bounded treewidth subsequently ﬁnding dags whose moral graph subgraph graph. approach designed aiming data sets large domains cannot handled milp formulation. naive approach designing approximate method would extend sampling methods unconstrained bayesian network learning. instance could envision rejection sampling approach would sample structures using available procedure verify treewidth discarding structure test fails. great issues approach computation treewidth hard problem even linear-time algorithms perform poorly practice; virtually structures would discarded fact complex structures tend larger scores simple ones least used score functions empirically veriﬁed facts report here. another natural approach problem consider elimination order variables topological order straightforward uniformly sample space orderings combined deﬁnition k-tree deﬁned following recursive -clique k-tree. k-tree. denote k-trees nodes. fact bayesian network treewidth bounded closely related k-tree. k-trees exactly maximal graphs treewidth proposed linear time method coding decoding k-trees called dandelion codes moreover established bijective mapping codes k-trees tnk. code pair list pairs integers drawn arbitrary number example dandelion code -tree nodes dandelion codes sampled uniformly random trivial linear-time algorithm uniformly chooses elements build uniformly samples pairs integers {\u0001}. theorem bijection mapping elements comgiven dynamic programming algorithm proposed optimal structure whose moral graph subgraph implementation follows ideas also seen extending divide-and-conquer method account possible divisions nodes. results following theorem. combine linear-time sampling k-trees described theorem linear-time learning bounded structures consistent graph theorem obtain algorithm learning bounded treewidth bayesian networks. algorithm described algorithm input score function output gbest. initialize πbest repeat certain number iterations reached uniformly sample ank; decode tnk; find maximizes score function consistent running time algorithm linear computational complexity step uses method exponential treewidth n)). hence cannot hope moderately high treewidth bounds regarding sample space according theorem slightly higher order-based learning unconstrained bayesian networks especially however iteration step needs considerable effort corresponding iteration unbounded case explained main practical drawback algorithm step process sampled k-tree. sequel propose approach much faster price slight increase sampling space. empirically compare approaches next section. deﬁne partial order nodes. consistent directed path words constrains valid topological orderings nodes force linear order interested orderings specify edge k-tree ending points precedes multiple linear orderings achieve result goal sample smallest possible space orderings recursive process deﬁnition described algorithm procedure produces partial orders whose underlying graph exactly graph note treewidth corresponding might exceed treewidth affect correctness algorithm used specify node preceeds node order hence possible parents; actual parents chosen treewidth bound respect. done efﬁciently using algorithm sampling partial order within k-tree. input k-tree nodes output partial order deﬁned initialize arbitrarily choose -clique call root proof. sampling nodes root clique takes time sampling ways choose arcs without creating cycles. assume appropriate structure representing known steps done time. iteration step spend time ways direct edges equivalent placing relative order respect already ordered neighbors. hence total running time sampling space n−k− n−k. following result shows sampling space version sampling algorithm remains reasonably small especially proof. before decoding algorithm method uniformly sample dandelion code linear time algorithm samples ordering linear time too. finally ﬁnding best consistent k-tree greedy procedure nodes treewidth cannot exceed take subgraph cycles formed respect although sampling space version larger version version much faster iteration. allows explore much larger region space k-tress version within ﬁxed amount time. moreover version without pre-computing score function scores needed computed stored hash table accesses thus closely matching another desirable characteristic order-based learning methods unbounded treewidth empirically analyze accuracy algorithm comparing versions values obtained milp method. before collection data sets repository varying dimensionality variables discretized median value needed. number variables samples data described table columns original data sets audio community discarded variables audio always constant value variables community almost different value sample variables missing data experiments maximize bayesian dirichlet likelihood equivalent uniform score equivalent sample size equal treewidth bounds maximum parent size fair among runs pre-computed scores considered input problem. milp optimized cplex memory limit allowed three hours also collected incumbent solution minutes. algorithm given minutes figure performance methods relative solution found version algorithm treewidth limit four. milp results missing community hill able produce solution cases. figure performance methods relative solution found version algorithm treewidth limit ten. milp results minutes missing community hill able produce solution within time. account variability performance sampling methods respect sampling seed version algorithm times data different seeds. report minimum median maximum obtained values runs dataset. show relative scores approximate methods respect version median score treewidth bounds four relative score computed ratio obtained value median score version higher values better. moreover value higher shows method outperformed version whereas value smaller shows converse. data used ﬁgures appear tables exponential dependence treewidth version made intractable treewidth bound greater plot version largely superior version even former might suboptimal networks given k-tree. probably consequence much lower running times iteration allows version explore much larger k-trees. also suggests spending time ﬁnding good k-trees worthy optimizing network structures given k-tree. also milp formulation scales poorly number variables unable obtain satisfactory solutions data sets variables. hill data treewidth cplex running milp formulation able output solution within minutes solution obtained within hours left zoomed area graph figure community data treewidth cplex solution within hours. regarding treewidth bound observe version accurate outperforms milp formulation larger data sets. worth noting versions algorithm implemented matlab; hence comparison approximate solution running milp formulation amount time might unfair expect produce better results appropriate re-coding sampling methods efﬁcient language nevertheless results show version competitive even scenario. created exact approximate procedures learn bayesian networks bounded treewidth. perform well immediate practical use. designed mixed-integer linear programming formulation improves milp formulations related tasks especially regarding speciﬁcation treewidth-related constraints. solves problem exactly surpasses state-of-the-art method size networks treewidth handle. even results indicate better state milp accurate might fail large domains. purpose proposed double sampling idea provides means learn bayesian networks large domains high treewidth limits empirically shown perform well collection public data sets. scales well complexity linear domain size treewidth bound. certainly search methods integrated sampling approach instance local search every iteration sampling local permutations orderings compatible k-trees etc. leave study avenues future work. making work closely related works appeared literature. developed exact learning procedure based maximum satisﬁability. developed alternative milp formulation problem exponentially many constraints used cutting plane generation techniques improve performance. works developed independently simultaneously work presented here; future work compare performance empirically methods proposed here.", "year": 2014}