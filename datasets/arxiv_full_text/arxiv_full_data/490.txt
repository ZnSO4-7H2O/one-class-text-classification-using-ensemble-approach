{"title": "Conditional Generation and Snapshot Learning in Neural Dialogue Systems", "tag": ["cs.CL", "cs.NE", "stat.ML"], "abstract": "Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model architectures and different ways to represent and aggregate the source information in an end-to-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used.", "text": "recently variety lstm-based conditional language models applied across range language generation tasks. work study various model architectures different ways represent aggregate source information endto-end neural dialogue system framework. method called snapshot learning also proposed facilitate learning supervised sequential signals applying companion cross-entropy objective function conditioning vector. experimental analytical results demonstrate ﬁrstly competition occurs conditioning vector differing architectures provide different trade-offs two. secondly discriminative power transparency conditioning vector providing model interpretability better performance. thirdly snapshot learning leads consistent performance improvements independent architecture used. recurrent neural network -based conditional language models shown effective solving number real world problems machine translation image caption generation recently rnns applied task generating sentences explicit semantic representation attention-based methods long short-term memory -like gating mechanisms studied improve generation quality. although clear lstmbased conditional generate plausible natural language less effort comparing different model architectures. furthermore conditional generation models typically tested relatively straightforward tasks conditioned single source goal optimise single metric work study conditional lstms generation component neural network -based dialogue systems depend multiple conditioning sources optimising multiple metrics. neural conversational agents direct extensions sequence-to-sequence model conversation cast source target transduction problem. however models still real world applications because lack capability supporting domain speciﬁc tasks example able interact databases aggregate useful information responses. recent work however proposed end-to-end trainable neural dialogue system assist users complete speciﬁc tasks. system used distributed symbolic representations capture user intents collectively condition language generator generate system responses. diversity conditioning information sources best represent combine non-trivial. objective function learning dialogue policy language generator depends solely likelihood output sentences. however sequential supervision signal informative enough learn good conditioning vector representation resulting generation process dominated often lead inappropriate system outputs. paper therefore also investigate snapshot learning attempts mitigate problem heuristically applying companion supervision signals subset conditioning vector. idea similar deeply supervised nets ﬁnal cost output layer optimised together companion signals generated intermediary layer. found snapshot learning offers several beneﬁts consistently improves performance; learns discriminative robust feature representations alleviates vanishing gradient problem; appears learn transparent interpretable subspaces conditioning vector. machine learning approaches task-oriented dialogue system design cast problem partially observable markov decision process using reinforcement learning train dialogue policies online interactions real users order make tractable state action space must carefully designed understanding generation modules assumed available trained standalone supervised corpora. underlying hand-coded semantic representation conversation natural comprehension capability limited. motivates neural networks model dialogues conditional generation problem. interest generating natural language using attributed success large vocabulary speech recognition sutskever showed plausible sentences obtained sampling characters output layer rnn. conditioning lstm sequence characters graves showed machines synthesise handwriting indistinguishable human. later conditional generation idea tried several research ﬁelds example generating image captions conditioning convolutional neural network output translating source target language conditioning decoder lstm encoder lstm generating natural language conditioning symbolic semantic representation among methods attention-based mechanisms shown effective improving performance using dynamic source aggregation strategy. model dialogue conditional generation sequence-to-sequence learning framework adopted. vinyals trained model several conversation datasets showed model generate plausible conversations. however serban discovered majority generated responses generic maximum likelihood criterion latter addressed using maximum mutual information decoding strategy. furthermore lack consistent system persona also studied despite demonstrated potential major barrier line research data collection. many works investigated conversation datasets developing chat qa-like general purpose conversation agents. however collecting data develop goal oriented dialogue systems help users complete task speciﬁc domain remains difﬁcult. recent work problem addressed designing online parallel version wizard-of-oz data collection allows large scale cheap in-domain conversation data collected using amazon mechanical turk. nnbased dialogue model also proposed learn collected dataset shown able assist human subjects complete speciﬁc tasks. snapshot learning viewed special form weak supervision supervision signals heuristically labelled matching unlabelled corpora entities attributes structured database. widely applied relation extraction information extraction facts knowledge base used objectives train classiﬁers. recently self supervision also used memory networks improve discriminative power memory attention. conceptually snapshot learning related curriculum learning instead learning easier examples difﬁcult ones snapshot learning creates easier target example. practice snapshot learning similar deeply supervised nets companion objectives generated intermediary layers optimised altogether output objective. testbed work neural network-based task-oriented dialogue system proposed model casts dialogue source target sequence transduction problem augmented dialogue history current database search outcome model consists encoder decoder modules. details module given below. encoder module turn goal encoder produce distributed representation system action used condition decoder generate next system response skeletal form. consists four submodules intent network belief tracker database operator policy network. intent network intent network takes sequence tokens converts sentence embedding representing user intent using lstm network. hidden layer lstm last encoding step taken representation. mentioned representation viewed distributed version speech used traditional systems. belief trackers addition intent network neural dialogue system uses slot-based belief trackers track user requests. taking user input evidence task belief tracker maintain multinomial distribution values informable slot binary distribution requestable slot. probcalled belief states ability distributions system. belief states together intent vector viewed system’s comprehension user requests turn database operator based belief states query formed taking union maximum values informable slot. vector representing different degrees matching produced counting number matched entities expressing -bin -hot encoding. zero associated entity pointer maintained identiﬁes matching entities selected random. entity pointer updated current entity longer matches search criteria; otherwise stays same. policy network based vectors three modules policy network combines single action vector three-way matrix transformation decoder module conditioned system action vector provided encoder module decoder module uses conditional lstm generate required system output token token skeletal form. ﬁnal system response formed informable slots slots users constrain search food type price range; requestable slots slots users value phone number. information speciﬁed domain ontology. substituting actual values database entries skeletal sentence structure. conditional generation network paper study analyse three different variants lstm-based conditional generation architectures language model type straightforward condition lstm network additional source information concatenate conditioning vector together input word embedding previous hidden layer index generation step hidden layer size input forget output gates respectively proposed cell value true cell value step model parameters. model shown figure since differ signiﬁcantly original lstm call language model type conditional generation network. memory type memory type conditional generation network introduced shown figure conditioning vector governed standalone reading gate reading gate decides much information read conditioning vector directly writes memory cell another weight matrix learn. idea behind model isolates conditioning vector model ﬂexibility learn trade two. hybrid type continuing idea memory type network complete separation conditioning vector provided hybrid type network shown figure model motivated fact long-term dependency needed conditioning vector apply information every step anyway. decoupling conditioning vector attractive leads better interpretability results provides potential learn better conditioning vector attention belief representation attention attention-based mechanism provides effective approach aggregating multiple information sources prediction tasks. like matrix vector parameters learn. belief representation effect different belief state representations performance also studied. user informable slots full belief state original state containing categorical values; summary belief state contains three components summed value categorical probabilities probability user said don’t care slot probability slot mentioned. user requestable slots hand full belief state summary belief state slot values binary rather categorical. snapshot learning learning conditional generation models sequential supervision signals difﬁcult requires model learn long-term word dependencies potentially distant source encoding functions. mitigate difﬁculty introduce novel method called snapshot learning create vector binary labels snapshot remaining part output sentence ttj|tt| generation step element snapshot vector indicator function certain event happen future obtained either system response dialogue context training time. companion cross entropy error computed force subset conditioning vector close snapshot vector figure idea snapshot learning. snapshot vector trained additional supervisions indicator functions heuristically labelled using system response. indicator functions work forms whether particular slot value going occur whether system offered venue shown figure offer label snapshot produced checking delexicalised name token entire dialogue. occurred every label subsequent turns labelled otherwise labelled create snapshot targets particular slot value output sentence matched corresponding delexicalised token turn turn generation step. generation step target labelled delexicalised token generated; otherwise however models without attention targets turn condition vector able learn dynamically changing behaviour without attention. dataset dataset used work collected wizard-of-oz online data collection described task system assist users restaurant cambridge area. three informable slots users constrain search requestable slots shown bold face. slots) user value restaurant offered. dialogues dataset approximately turns total. database contains unique restaurants. training training procedure divided stages. firstly belief tracker parameters pre-trained using cross entropy errors between tracker labels predictions. ﬁxed tracker parameters remaining parts model trained using cross entropy errors generation network output token targets predictions respectively turn output step snapshot cost equation tradeoff parameter models trained snapshot learning. treated dialogue batch used stochastic gradient descent small regularisation term train model. collected corpus partitioned training validation testing sets ratio early stopping implemented based validation considering log-likelihoods. gradient clipping hidden layer sizes weights randomly initialised including word embeddings. vocabulary size around input output rare words words delexicalised removed. decoding order compare models trained different recipes rather decoding strategies decode trained models average probability tokens sentence. applied beam search beamwidth equal search stops end-of-sentence token generated. order consider language variability decoding candidates obtained performed evaluation them. metrics compared models trained different recipes performing corpus-based evaluation model used predict system response held-out test set. three evaluation metrics used bleu score slot matching rate objective task success rate dialogue marked successful both offered entity matches task speciﬁed user system answered associated information requests user. slot matching rate percentage delexicalised tokens appear candidate also appear reference. computed bleu scores skeletal sentence forms substituting actual entity values. results averaged random initialised networks. results table shows evaluation results. numbers left right table cell model trained snapshot learning. figure learned attention heat maps trackers. ﬁrst three columns ﬁgure informable slot trackers rest requestable slot trackers. generation model hybrid type lstm. ﬁrst observation snapshot learning consistently improves metrics regardless model architecture. especially true bleu scores. think attributed discriminative conditioning vector learned snapshot method makes learning conditional easier. ﬁrst block belief state representation compare effect different belief representations. seen using succinct representation better identity categorical value belief state help generation decisions done skeletal form. fact full belief state representation encourage model learn incorrect coadaptation among features data scarce. conditional architecture block compare three different conditional generation architectures described section result shows language model type memory type networks perform better terms bleu score slot matching rate hybrid type networks achieve higher task success. probably degree separation between conditioning vector coupling approach sacriﬁces conditioning vector learns better higher bleu; complete separation learns better conditioning vector offers higher task success. anism compare again. firstly characteristics three models observed also hold attention-based models. secondly found attention mechanism improves three architectures task success rate bleu scores. probably limitations using n-gram based metrics like bleu evaluate generation quality gate activations ﬁrst studied average activation individual gate models averaging running generation test set. analysed hybrid models reading gate output gate activation ratio shows clear tradeoff conditioning vector components. seen table found average forget gate activations ratio reading gate output gate activation strong correlations performance better performance seems come models learn longer word dependency better conditioning vector learned attention visualised learned attention heat models trained without snapshot learning figure attention informable slot trackers requestable slot trackers found model trained snapshot learning seems produce accurate discriminative attention heat comparing trained without contribute better performance achieved snapshot learning approach. snapshot neurons mentioned earlier snapshot learning forces subspace conditioning vector become discriminative interpretable. example generated sentences together snapshot neuron activations shown figure seen generating words neuron activations changing detect different events assigned snapshot training signals e.g. figure light blue orange neurons switched domination role token generated; offered neuron high activation state figure system offering venue figure activated system still helping user venue. examples found appendix. paper investigated different conditional generation architectures novel method called snapshot learning improve response generation neural dialogue system framework. results showed three major ﬁndings. firstly although hybrid type model rank highest metrics nevertheless preferred achieved highest task success also provided interpretable results. secondly snapshot learning provided gains virtually metrics regardless architecture used. analysis suggested beneﬁt snapshot learning mainly comes discriminative robust subspace representation learned heuristically labelled companion signals turn facilitates optimisation ﬁnal target objective. lastly results suggested making complex system interpretable different levels helps understanding also leads highest success rates. however still much work left work focused conditional generation architectures snapshot learning scenario generating dialogue responses. would helpful comparison could conducted application domains machine translation image caption generation wider view effectiveness approaches assessed.", "year": 2016}