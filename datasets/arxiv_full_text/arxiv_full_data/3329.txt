{"title": "A Latent-Variable Lattice Model", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Markov random field (MRF) learning is intractable, and its approximation algorithms are computationally expensive. We target a small subset of MRF that is used frequently in computer vision. We characterize this subset with three concepts: Lattice, Homogeneity, and Inertia; and design a non-markov model as an alternative. Our goal is robust learning from small datasets. Our learning algorithm uses vector quantization and, at time complexity O(U log U) for a dataset of U pixels, is much faster than that of general-purpose MRF.", "text": "markov random ﬁeld learning intractable approximation algorithms computationally expensive. target small subset used frequently computer vision. characterize subset three concepts lattice homogeneity inertia; design non-markov model alternative. goal robust learning small datasets. learning algorithm uses vector quantization time complexity dataset pixels much faster general-purpose mrf. computer vision commonly used model images videos. algorithms intractable general case approximation algorithms generally used. approximation algorithms computationally expensive. computer vision usually need full expressive power mrf. paper restrict small subset useful computer vision develop fast algorithms work restricted case. three restrictions denotes undirected graph. nodes undirected edges. denotes variable associated node giving random vector xp}. local markov property variable states given neighbors conditionally independent rest variables. markov random ﬁeld graph distributions satisﬁes markov properties qc∈c clique restrict domain d-dimensional lattices. without loss generality assume lattice length nodes dimension. node element denote lattice nodes latent variables hidden. associated visible variable counted among neighbors. value visible variable dependent solely latent variable. given values latent variables visible variables independent other. state node assignment observed symbol visible variable image. image essentially visible-variable lattice opposed latent-variable lattice. training data consists images lacks state conﬁguration corresponding latent variables. classiﬁcation images input classes speciﬁed images belonging each. objective classify test images classes. build model class learning class-conditional density lattice conﬁgurations. given prior probabilities class-conditional models bayesian classiﬁcation used classify test images. graphical models general networks introduced discussed applications computer vision. physics ising model widely used. potts model described generalization ising model closely related mrf. main benchmark algorithm fast inference associative pairwise solve slightly different problem solution useful approximating solution associative well. latent-variable model adjacent latent nodes share possible states likely state called associativity. intrinsic property process underlying many application domains. amount associativity lattice varies domain. one-dimensional case associativity speech recognition limited amount associativity chunking natural language almost associativity part-of-speech tagging natural language. associativity restricted dependency adjacent latent nodes. paper consider broader condition non-adjacent latent nodes dependent other. markov related nth-order markov model. call inertia restrict model lattices inertia. since inertial model approximate associative model used applications inertia lattice well approximation applications associativity lattice. sliding hypercube window propose index expectation taken hypercube centered node every node lattice. probability state window. nodes window state states equal probability within window. model applicable index atleast following ﬁgures circle node. thick circles latent-variable nodes. figure associative figure non-associative mrf. matrices pairwise potentials main diagonal represents adjacent nodes state. nodes latent visible boolean. differences mrfs highlighted bold. section describe variant model discrete output symbols. one-dimensional case variant used model instance proteins sequences twenty amino acids. signature discussed section maps node probability simplex. divide probability simplex partitions partition corresponds state mrf. potential partitions observation symbol probability distribution roughly data model. model markov. inertia. parameter model range probability mass multinomial distribution categories standard states considered points probability simplex. coordinates state observation symbol probability distribution sliding window methods popular sequences images. since model inertia nearby nodes probably state. therefore sliding window models likely work well. w-window hypercube might beneﬁcial different values evaluation decoding learning evaluation decoding learning. optimal size window related typical minimum number steps direction change state. increasing window size increases inductive bias reduces variance. reducing window size reduces inductive bias increases variance. ability vector maximum likelihood generating symbols sliding window. {xt} sample probability symbol w-window around node signature point probability simplex. algorithm since inertia results decoding algorithm reliable need consider possible lattice conﬁgurations. decode lattice conﬁguration image compute likelihood. decoding algorithm works steps characterize sliding window around current node signature signature assign latent state current node. signature observation probalgorithm decode lattice initialize sliding window compute symbol counts slide sliding window lattice every step update symbol counts send signature node signature probability distribution. assign subroutine assigns state closest norm. creates partitioning probability simplex partitions associated state. compute signature nodes using symbol counts vector quantization signatures partitions probability simplex states. coordinates centroids observation probability matrix sample transition probabilities lattice signatures state potential matrix. requirements clustering roughly spherical partitions representative point partition. solve clustering problem vector quantization since requirements similar. classical technique signal processing models probability density functions distribution prototype vectors. multiple algorithms different tradeoffs. fast pairwise nearest neighbor algorithm trades modeling accuracy reduced computational complexity. fast computational complexity need high modeling accuracy section describe variant model real-valued opposed discrete output symbols. two-dimensional case variant model digital images grayscale pixel value extracted features. previously state variable took values probability simplex embedded consists real variables representing mean vector i.e. -dimenpoint assigned closest state state space gets divided partitions corresponds state mrf. matrix remains probability transitioning partitions remains size sliding window. modify parameters model follows real-symbol discussed subsection iv-a switching linear dynamical systems extension state associated linear dynamical process. describes inference switching switching vector autoregressive processes. algorithm initialize sliding window compute output instances slide sliding window every step update send signature sliding window assign state lattice.", "year": 2015}