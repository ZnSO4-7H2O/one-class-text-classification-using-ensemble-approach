{"title": "Improving the Performance of Neural Networks in Regression Tasks Using  Drawering", "tag": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "abstract": "The method presented extends a given regression neural network to make its performance improve. The modification affects the learning procedure only, hence the extension may be easily omitted during evaluation without any change in prediction. It means that the modified model may be evaluated as quickly as the original one but tends to perform better.  This improvement is possible because the modification gives better expressive power, provides better behaved gradients and works as a regularization. The knowledge gained by the temporarily extended neural network is contained in the parameters shared with the original neural network.  The only cost is an increase in learning time.", "text": "improve learning. training done original neural network used standalone. knowledge extended neural network seems transferred original neural network achieves better results regression task. data consists pairs input ﬁxed size real valued vector target continuous value neural network architecture trained relation input target loss function used asses performance lower setup given neural network understood composition last part neural network i.e. applies matrix multiplication optionally non-linearity. words vector value last hidden layer input value written matrix function done squeeze information last hidden layer value. simple words neural network divided parts ﬁrst core performs majority calculations second tiny calculates single value prediction based output abstract—the method presented extends given regression neural network make performance improve. modiﬁcation affects learning procedure only hence extension easily omitted evaluation without change prediction. means modiﬁed model evaluated quickly original tends perform better. improvement possible modiﬁcation gives better expressive power provides better behaved gradients works regularization. knowledge gained temporarily extended neural network contained parameters shared original neural network. neural networks especially deep learning architectures become popular recently believe deep neural networks powerful tools majority classiﬁcation problems unfortunately neural networks regression tasks limited recently showed softmax distribution clustered values tends work better even target continuous cases seemingly continuous values understood categorical ones transformation types straightforward however sometimes transformation cannot simply incorporated furthermore forcing neural network predict multiple targets instead single makes evaluation slower. original target also additional depicts order magnitude original target. result extended neural network simultaneously solves regression task related classiﬁcation problem. training done using gradient descent hence sufﬁcient obtain gradients functions deﬁned i.e. given pair forward pass calculated afterwards backpropagations processed. backpropagation composition using loss function returns vector concatenation vectors gradg gradhg gradg gradient function point gradhg gradient function point similarly backpropagation using loss function gives gradients grads gradhs functions respectively. computed gradients parameters applied normal case functions takes part backpropagations. updating parameters belonging part complex obtaining different gradients gradhg gradhs. worth noting parameters common parameters compositions want take average gradients gradhg gradhs apply parameters). unfortunately orders magnitute different. therefore taking unweighted average result minimalizing loss functions address problem averages absolute values gradients calculated. values aproximately describe impacts loss functions respectively. ﬁnal vector gradh used gradient parameters gradient descent procedure equals figure sample extension function function always squeezes last hidden layer value. hand function hidden layers simplest architecture presented. main idea extend neural network every input value last hidden layer duplicated processed independent parameterized functions. ﬁrst second called original neural network trained minimize given loss function neural network trained loss function example extension described presended figure sake consistency loss function called note functions share parameters compositions inner function. since parameters shared. means learning inﬂuences want train functions jointly hard general function loss function constructed special presented below. function trained contains pair predict sets loss function deﬁned non-binary cross-entropy loss typically used classiﬁation problems. simplest form function multiplication matrix drawering basic form additional parallel layer takes input value last hidden layer original neural network modiﬁed neural network trained predict useful bear mind backpropagations also share calculations. extreme case known advance backpropagation ratio performed simultaneously loss function weighted loss function noticed ratio needed roughly constant batches iterations therefore calculated initial phase learning. afterwards checked updated time time. regular uneven. mentioned subsection simplest deﬁning drawers take intervals whose endings suitable percentiles distribute target values uniformly. case regular drawers deﬁned following however noticed alternative deﬁning ei’s tends support classical mean square error loss better proposed. loss penalizes difference given target prediction larger. address problem drawers deﬁned encourages learning procedure focus extreme values. drawers group middle values bigger clusters placing extreme values smaller ones. deﬁnition uneven drawers follows case every drawer contains approximately times target values compared drawer finally contain maximum target values. similarly asceding intervals ﬁrst half desceding i.e. contain less less target values. number drawers hyperparameter. bigger complex distribution modeled. hand drawers contain enough representants among targets training set. experiments drawer contained least target values. disjoint nested. observed sometimes better train predict whether target i=jei. case answer simpler question target higher given value? instead bounding target value sides. course case longer solves one-class classiﬁcation problem every value assesed independently binary cross-entropy loss. extension gives additional expressive power used predict given neural network. additional target since target closely related original believed gained knowledge transferred core given neural network since categorical distributions assume shape model arbitrary distribution ﬂexible. argue classiﬁcation loss functions provide better behaved gradients regression ones. result evolution classiﬁcation neural network smooth learning. effectiveness method presented established comparison. original drawered neural network trained dataset trainings completed neural networks performances given test measured. since drawering affects learning procedure comparision fair. learning procedures depend random initialization hence obtain reliable results learning procedures setups performed. adam chosen stochastic optimization. rossmann operates drug stores european countries. currently rossmann store managers tasked predicting daily sales weeks advance. store sales inﬂuenced many factors including promotions competition school state holidays seasonality locality. thousands individual managers predicting sales based unique circumstances accuracy results quite varied. since needed ground truth labels train part dataset used split data training validation test time. training consisted observations year validation contain observations january february march april finally test covers rest observations year version task target normalized logarithm turnover given day. logarithm used since turnovers exponentially distributed. input consisted information provided original dataset except promo related information. month extracted given date biggest challenge linked dataset overﬁt trained model dataset size relatively small encoding layers used cope categorical variables. differences scores train validation test sets signiﬁcant seemed grow learning. believe drawering prevents overﬁtting works regularization case. dataset describes states customers time impressions. state vector mainly continuous features like price last item seen value previous purchase number items basket etc. target price next item bought given user. price always positive since users clicked converted incorporated dataset. believe biggest challenge working conversion value task tame gradients vary lot. pairs dataset inputs close even identical targets even order magnitude. result gradients remain relatively high even last phase learning model tend predict last encountered target instead predicting average them. argue drawering helps general patterns providing better behaved gradients. case original neural network takes input produced values categorical continuous ones. categorical value encoded vector size number possible values given categorical variable. minimum applied avoid incorporating redundancy. continuous features normalized. concatenation encoded features continuous variables produces input vector size neural network sequential form drawered neural network roughly parameters. huge advantage additional parameters used calculate target additional calculations skipped evaluation. believe patterns found answer additional target related original transferred core part used dropout since incorporating dropout causes instability learning. work regression tasks noticed general issue investigated scope paper. fifty learning procedures original extended neural network performed. stopped ﬁfty iterations without progress validation iteration model performed best validation chosen evaluated test set. loss function used classic square error loss. minimal error test achieved drawered neural network better best original neural network. difference average scores also around favor drawering. analyzing average ﬁfty models method difference seems blurred. caused fact learning procedures overﬁted much achieved unsatisfying results. even case average drawered neural networks better. scores standard deviations showed table note extending additional parameters result ever better performance would drastically slow evaluation. however noticed simple extensions original neural netwok tend overﬁt achieve better results. train errors also investigated. case original neural network performs better supports thesis drawering works regularization. detailed results presented table dataset provides detailed users descriptions consisted categorical features continuous ones. encoding original neural network takes input vector size core part neural network layers outputs vector size function extension simple linear linear respectively. case conversion value task provide detailed model description since dataset private experiment reproduced. however decided incorporate comparison paper versions drawers tested dataset also want point invented drawering method working dataset afterwards decided check method public data. unable achieve superior results without drawering. therefore believe work done dataset presented. figure learning curves showed. three setups best worst ones chosen. means minimum eight representants showed. ﬁrst iterations skipped make ﬁgure lucid. learning procedure ﬁnished iterations without progress validation set. believe improvement possible drawered neural network bigger expressive power provided better behaved gradients model arbitrary distribution regularized. turned knowledge gained modiﬁed neural network contained parameters shared given neural network. since cost increase learning time believe cases better performance important training time drawering incorporated given regression neural network.", "year": 2016}