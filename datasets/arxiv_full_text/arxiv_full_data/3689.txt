{"title": "Doubly Robust Off-policy Value Evaluation for Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "cs.SY", "stat.ME", "stat.ML"], "abstract": "We study the problem of off-policy value evaluation in reinforcement learning (RL), where one aims to estimate the value of a new policy based on data collected by a different policy. This problem is often a critical step when applying RL in real-world problems. Despite its importance, existing general methods either have uncontrolled bias or suffer high variance. In this work, we extend the doubly robust estimator for bandits to sequential decision-making problems, which gets the best of both worlds: it is guaranteed to be unbiased and can have a much lower variance than the popular importance sampling estimators. We demonstrate the estimator's accuracy in several benchmark problems, and illustrate its use as a subroutine in safe policy improvement. We also provide theoretical results on the hardness of the problem, and show that our estimator can match the lower bound in certain scenarios.", "text": "roughly classes approaches off-policy value evaluation. ﬁrst model data regression evaluate policy model. regression based approach relatively variance works well model learned satisfactory accuracy. however complex real-world problems often hard specify function class regression efﬁciently learnable limited data time small approximation error. furthermore general impossible estimate approximation error function class resulting bias cannot easily quantiﬁed. second class approaches based idea importance sampling corrects mismatch distributions induced target policy behavior policy approaches salient properties unbiased independent size problem’s state space variance large method useful horizon long work propose off-policy value evaluation estimator achieve best regression based approaches importance sampling based approaches contributions three-fold estimator’s statistical properties analyzed suggests superiority previous approaches. furthermore certain scenarios prove estimator’s variance matches cramer-rao lower bound off-policy value evaluation. benchmark problems estimator much accurate importance sampling baselines remaining unbiased contrast regressionbased approaches. application show better estimator beneﬁt safe policy iteration effective policy improvement step. study problem off-policy value evaluation reinforcement learning aims estimate value policy based data collected different policy. problem often critical step applying real-world problems. despite importance existing general methods either uncontrolled bias suffer high variance. work extend doubly robust estimator bandits sequential decision-making problems gets best worlds guaranteed unbiased much lower variance popular importance sampling estimators. demonstrate estimator’s accuracy several benchmark problems illustrate subroutine safe policy improvement. also provide theoretical results inherent hardness problem show estimator match lower bound certain scenarios. study off-policy value evaluation problem aims estimate value policy data collected another policy problem critical many real-world applications reinforcement learning whenever infeasible estimate policy value running policy expensive risky unethical/illegal. robotics business/marketing applications instance often risky policy without estimate policy’s quality medical public-policy domains often hard controlled experiment estimate treatment effect off-policy paper focuses off-policy value evaluation ﬁnitehorizon problems often natural model real-world problems like dialogue systems. goal estimate expected return start states drawn randomly distribution. differs setting considered previous work often known off-policy policy evaluation aims estimate whole value function settings important different uses practice share core difﬁculty dealing distribution mismatch. technique ﬁrst studied statistics improve robustness estimation model misspeciﬁcation estimator developed dynamic treatment regime later applied policy learning contextual bandits ﬁnite-time variance shown typically lower estimator work extends work dud´ık sequential decision-making problems. addition show certain scenarios variance matches statistical lower bound estimation problem. important application off-policy value evaluation ensure policy deployed degenerate performance policy iteration; example algorithms purpose include conservative policy iteration safe policy iteration recently thomas incorporate lower conﬁdence bounds approximate policy iteration ensure computed policy meets minimum value guarantee. work compliments interesting conﬁdence intervals providing drop-in replacement show replacement agent accept good policies aggressively hence obtain higher reward maintaining level safety policies. background markov decision processes deﬁned state space action space transition function probability seeing state taking action state mean reward function immediate goodness discount factor. initial state distribution. policy assigns state distribution actions probability distribution h-step trajectory speciﬁed value conditioned deﬁne state value function discounted problems true horizon inﬁnite purpose policy value evaluation still ﬁnite approximates bounded error diminishes increases. simplicity assume data sampled using ﬁxed stochastic policy known behavior policy. goal estimate value given target policy data trajectories. review popular families estimators off-policy value evaluation. notation since interested value dependence value functions policy omitted. terms like πh−t+ also omit dependence horizon abbreviate assuming remaining steps. also expectations taken respect distribution induced unless stated otherwise. finally shorthand expectations however real-world problems usually large even inﬁnite state space many state-action pairs observed even data rendering necessity generalization model ﬁtting. generalize stone gr¨unew¨alder ﬁtting value function directly function approximation makes problem tractable introduce bias estimated value parameters value function cannot represented corresponding function class. bias general hard quantify data thus breaks credibility estimations given regression based approaches estimator provides unbiased estimate value averaging following function trajectory data deﬁne per-step importance ratio basic estimator improved step-wise version given follows given dataset estimator simply average estimate trajectories namely number trajectories applied i-th trajectory. typically suffers high variance easily grows exponentially horizon. variant weighted importance sampling biased consistent estimator given follows together t/|d| average cumulative important ratio horizon dataset trajectory estimates given trajectory-wise step-wise respectively contextual bandits considered mdps horizon sample trajectories take form suppose given estimated reward function possibly performing regression separate dataset doubly robust estimator contextual bandits deﬁned case importance ratio unknown estimates reward function data using parametric function classes. name doubly robust refers fact either function class properly speciﬁed estimator asymptotically unbiased offering chances ensure consistency. paper however interested dr’s variance-reduction beneﬁt. requirement independence practice target policy often computed data stay unbiased depend samples used eqn.; extension estimator view step-wise importance sampling estimator dealing bandit problem horizon context action taken observed stochastic return step-is whose expected value probability least value case number trajectories chosen conﬁdence level range estimate function maximal magnitudes application sophisticated bounds off-policy value evaluation found thomas practice however strict usually pessimistic normal approximations used instead experiments normally approximated lead effective reliable policy improvement eqn. ﬁrst terms variances different sources randomness time step state transition randomness action stochasticity reward randomness respectively; term contains variance future steps. conclusion dr’s variance mentioned introduction important motivation off-policy value evaluation guarantee safety deploying policy. purpose characterize uncertainty estimates usually terms conﬁdence interval calculation straight-forward since unbiased estimator applied i.i.d. trajectories standard concentration bounds apply. example hoeffding’s inequality states random variables bounded range deviation average independent samples expected long substantial stochasiticity rewards and/or state transitions. however possible address limitation. example modiﬁcation reduces variance state transitions variance original cost introducing small bias. bias bounded proposition whose proof deferred appendix section demonstrate estimator experiment. section showed possibility reducing variance state transition stochasticity special scenario. natural question whether exists estimator reduce variance without relying strong assumpstating results emphasize that estimation problems bound depends crucially parameterized parameterization captures prior knowledge problem. general structural knowledge encoded parameterization easier recover true parameters data lower bound strong assumptions often made training phase make problems tractable want count prior knowledge evaluation every assumption made evaluation decreases credibility value estimate. therefore ﬁrst present result hardest case assumptions especially markov assumption last observation state made ensure credible estimate. relaxed case discussed afterwards. deﬁnition discrete tree state represented history ot−at−ot. ot’s called observations. assume discrete observations actions. initial states take form upon taking action state transition next state form probability simpliﬁcation assume non-zero rewards occur trajectory. additional observation encodes reward randomness reward function deterministic solely parameterized transition probabilities. theorem discrete tree mdps variance unbiased off-policy value estimator lower bounded proof obs.. result follows directly unfolding recursion eqn. noticing implication minimal prior knowledge available lower bound theorem equals variance perfect q-value function hence part variance state transition stochasticity intrinsic problem cannot eliminated without extra knowledge. tends have. related hardness result given mdps known transition probabilities. relaxed case appendix discuss relaxed case directed acyclic graph structure allowing different histories length identiﬁed state making problem easier tree case. cases share almost identical proofs give concise proof theorem readers consult appendix fully expanded version. proof theorem proof convenient index rows columns matrix histories denotes entry matrix furthermore given real-valued function )]hh denotes matrix whose entry given parameterize discrete tree length convenience treat model parameters encoded vector θhao contains |ha| alternating observations actions. parameters subject normalization constraints taken consideration co∈o matrix form block-diagonal matrix block vector speciﬁcally fhahao ha}. note jacobian constraints. matrix whose column vectors consist orthonormal basis null space moore corollary obtain constrained cramer-rao bound fisher information matrix without taking constraints consideration jacobian quantity want estimate. calculation ccrb consists four main steps. calculation deﬁnition probability observing policy matrix takes entries indexed neither strings preﬁx other. entries without loss generality assume preﬁx hao; case similar symmetric. since ghaoghao preﬁx experiments compare accuracy point estimate given estimator. domain policy πtrain computed optimal policy model estimated training dataset dtrain target policy πtrain parameter controls similarity larger tends make off-policy evaluation easier cost yielding conservative policy πtrain potentially high quality. apply estimators separate dataset deval estimate value compare estimates groundtruth value take average estimation errors across multiple draws deval. note estimaaverage estimate subsets. since estimate subset unbiased overall average remains unbiased lower variance since trajectories sample average. show results -fold model ﬁtting time-consuming. domain description mountain popular benchmark problem -dimensional continuous state space discrete actions deterministic dynamics horizon time steps initial state distribution uniformly random. behavior policy uniformly random actions. model ﬁtting state aggregations state variables multiplied respectively rounded integers treated abstract state. estimate model data using tabular approach. data sizes details choose |dtrain| hard compute inverse directly. note however matrix matching dimensions deﬁnition orthogonal observing this design make diagonal easy invert. achieved letting xao) except preﬁx case xao) |ha). hard verify diagonal trick since ccrb invariant choice choose diag}) diagonal block columns forming orthonormal basis null space none-zero part easy verify exists column orthonormal also rewrite diag}) diagonal matrix diag ﬁnal step notice block expression simply times ccrb multinomial distribution diag calculation recall want estimate throughout section concerned comparison among following estimators. compactness drop preﬁx step-wise step-wise wis. experiment details found appendix figure comparison methods point estimators mountain car. trajectories generated off-policy evaluation results runs. subgraphs correspond target policies produced mixing πtrain different portions. x-axes show size dtest part data used is/wis/dr/dr-bsl. remaining data used |deval| dr-bsl uses step-dependent constant rmin/. results fig. errors is/wis/dr-bsl/dr dtest dreg. |dtest| increases is/wis gets increasingly better gets worse dreg contains less data. since depends halves data achieves best error intermediate |dtest| beats using data is/wis graphs. drdomain description sailing domain stochastic shortest-path problem agent sails grid. state contains integer variables representing location direction. actions rmin model ﬁtting apply kernel-based reinforcement learning smoothing kernel -distance data sizes details data sizes |dtrain| |deval| dr-bsl uses step-dependent results fig.. results qualitatively similar mountain results fig. except that good graph; graph split outperforms estimators signiﬁcant margin improvement achieved -fold last domain donation dataset records email interactions agent potential donators. state contains integer features discrete actions. trajectories -steps long discount. since groundtruth values available target policies simulator true data groundtruth everything henceforward true value target policy computed monte-carlo roll-outs simulator off-policy evaluation methods also data generated simulator among compared estimators replace dr-v -fold trick. state variable assumed evolve independently marginal transition probabilities estimated using tabular approach exactly simulator real data. reward function hand linear figure results donation dataset averaged runs. dr-v estimator given eqn. fold trick. whole dataset applied estimators. x-axis shows portion mixed πtrain. regression using ﬁrst state features consequently model almost perfect transition function relatively inaccurate reward function dr-v supposed work well situation. results shown fig. dr-v best estimator situations beats beats close. experiment apply off-policy value evaluation methods safe policy improvement. given batch dataset agent uses part candidate policies poor data insufﬁciency and/or inaccurate approximation. agent evaluates candidates remaining data chooses policy based evaluation. common scenario necessary hold part dtest regression. high variance variants acting greedily w.r.t. point estimate enough promote safety. typical approach select policy highest lower conﬁdence bound hold current behavior policy none bounds better behavior policy’s value. speciﬁcally bound point estimate empirical standard error controls conﬁdence level. placeholder method works averaging function sample trajectories; examples considered paper estimators. experiment conducted mountain setting section since address exploration-exploitation problem keep behavior policy ﬁxed uniformly random evaluate recommended policy figure safe policy improvement mountain car. x-axis shows size data y-axis shows true value recommended policy subtracted value behavior policy. agent gets data. candidate policies generated follows split |dtrain|/|d| split compute optimal πtrain model estimated dtrain πtrain rate compute conﬁdence bounds applying is/dr dtrain ﬁnally pick policy highest score splits α’s. results shown left panel fig.. ﬁgure clear dr’s value improvement largely outperforms primarily able accept target policy different however πtrain mostly good policy hence aggressive algorithm value gets. evidence algorithms achieve best value raising concern might make unsafe recommendations πtrain poor. falsify hypothesis conduct another experiment parallel πtrain minimize value instead maximizing resulting policies worse behavior policy results shown right panel. clearly becomes smaller algorithms become less safe safe better overall conclude drop-in replacement safe policy improvement. proposed doubly robust estimator off-policy value evaluation showed high accuracy point estimator usefulness safe policy improvement. hardness results problem also provided variance match lower bound certain scenarios. building preliminary version work thomas brunskill showed estimator viewed application control variates variance reduction designed advanced dr-based estimators. future work includes applying techniques real-world problems access effectiveness practice. thank xiujun help donation dataset experiment. also thank satinder singh michael littman susan murphy emma brunskill anonymous reviewers detailed helpful comments well philip thomas independent implementation veriﬁcation algorithm. work done jiang’s internship microsoft research. work partially supported grant jiang university michigan. opinions ﬁndings conclusions recommendations expressed authors necessarily reﬂect views sponsors. bottou l´eon peters jonas qui˜nonero-candela joaquin charles denis xavier chickering portugaly elon dipankar simard patrice snelson counterfactual reasoning learning systems example computational advertising. journal machine learning research fonteneau raphael murphy susan wehenkel louis ernst damien. batch mode reinforcement learning based synthesis artiﬁcial trajectories. annals operations research gr¨unew¨alder lever baldassarre pontil gretton modelling transition dynamics mdps proceedings rkhs embeddings. international conference machine learning jiang kulesza alex singh satinder. abstraction selection model-based reinforcement learning. proceedings international conference machine learning jong nicholas stone peter. model-based function approximation reinforcement learning. proceedings international conference autonomous agents multiagent systems kakade sham langford john. approximately optimal approximate reinforcement learning. proceedings international conference machine learning volume lihong langford john wang xuanhui. unbiased ofﬂine evaluation contextual-bandit-based news article recommendation algorithms. proceedings international conference search data mining lihong munos remi szepesv´ari csaba. toward proceedings minimax off-policy value estimation. eighteenth international conference artiﬁcial intelligence statistics mandel travis yun-en levine sergey brunskill emma popovic zoran. ofﬂine policy evaluation across representations applications educational games. proceedings international conference autonomous agents multi-agent systems thomas philip brunskill emma. data-efﬁcient offpolicy policy evaluation reinforcement learning. proceedings international conference machine learning thomas philip theocharous georgios ghavamzadeh mohammad. high conﬁdence off-policy evaluation. proceedings aaai conference artiﬁcial intelligence thomas philip theocharous georgios ghavamzadeh mohammad. high conﬁdence policy improvement. proceedings international conference machine learning pirotta matteo restelli marcello pecorino alessio calandriello daniele. safe policy iteration. proceedings international conference machine learning number precup doina sutton richard singh satinder eligibility traces off-policy policy evaluation. proceedings international conference machine learning precup doina sutton richard dasgupta sanjoy. off-policy temporal-difference learning funtion approximation. proceedings international conference machine learning here prove lower bound relaxed setting layered directed acyclic graph instead tree. mdps regions state space reachable different time steps disjoint trajectories separate early steps reunion state later. deﬁnition discrete directed acyclic graph completes proof. note eqn. next step used fact conditioned independent zero means terms constants. therefore square equals squares expectation. diagonal non-diagonal entries whose indexing column indexing tuples time step value cases suppose column without loss generality assume entry st+) corresponding entries st+statst+) +|st at)p+). then calculate avoid difﬁculty taking inverse non-diagonal matrix apply following trick diagonalize note matrix matching dimensions deﬁnition orthogonal design diagonal matrix achieved eliminate non-diagonal entries upper triangle without touching anything diagonal below symmetry deal lower triangle. particular take st+) at)p+)i hard verify construction diagonalizes diagonalization trick since ccrb invariant choice observe rows orthogonal choose follows number n×−) matrix remark compared theorem cumulative importance ratio replaced state-action occupancy ratio theorem ratios equal state reached unique sample path. general howproof theorem parameterize convenience treat parameters represented parameters subject normalization constraints taken consideration cramer-rao bound namely θstatst+ matrix left effectively jacobian constraints denote index rows entries matrix whose column vectors consist orthonormal basis null space moore corollary constrained cramer-rao bound being fact existing literature contrained cramer-rao bound deal situation unconstrained parameters break normalization constraints however easily tackled changing model slightly θhao resolves issue domain description mountain widely used benchmark problem -dimensional continuous state space deterministic dynamics state space discrete actions. agent receives reward every time step discount factor episode terminates ﬁrst dimension state reaches right boundary. initial state distribution uniformly random behavior policy uniformly random actions. typical horizon problem large variants therefore accelerate dynamics given next state obtained calling original transition function times holding ﬁxed horizon similar modiﬁcation taken thomas every steps compressed step. model construction model construct domain uses simple discretization state variables multiplied respectively rounded integers treated abstract state. estimate model parameters data using tabular approach. unseen aggregated state-action pairs assumed reward rmin self-loop transition. models produces πtrain used off-policy evaluation constructed way. data sizes details dataset sizes |dtrain| |deval| split deval dtest dr-bsl uses step-dependent constant function since estimators family typically highly skewed distribution estimates occasionally largely range crop outliers ensure statistically signiﬁcant experiment results within reasonable number simulations. treatment also applied experiment sailing. ’s); ﬁnally choose block diagonal matrix diag}) diagonal blocks easy verify column orthonormal similarly write diag}) diagonal matrix )st+st+ notice block eqn. simply times ccrb multinomial distribution ccrb multinomial distribution easily computed alternative formula gives diag have aiming terminal location top-right corner. state represented integer variables representing either location direction. step agent chooses move directions receives negative reward depends moving direction wind direction factors ranging rmin rmax problem non-discounting easy convergence computing πtrain. model construction apply kernel-based reinforcement learning supply smoothing kernel joint space states actions. kernel takes form exp·/b) -distance kernel bandwidth data sizes details data sizes |dtrain| |deval| split deval dtest drbsl uses step-dependent constant function γh−t+ difference directions deﬁned angle between divided computational efﬁciency kernel function cropped whenever stateaction pairs deviate dimensions.", "year": 2015}