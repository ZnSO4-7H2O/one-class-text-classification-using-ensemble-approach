{"title": "Using Mechanical Turk to Build Machine Translation Evaluation Sets", "tag": ["cs.CL", "cs.LG", "stat.ML", "I.2.7; I.2.6; I.5.1; I.5.4"], "abstract": "Building machine translation (MT) test sets is a relatively expensive task. As MT becomes increasingly desired for more and more language pairs and more and more domains, it becomes necessary to build test sets for each case. In this paper, we investigate using Amazon's Mechanical Turk (MTurk) to make MT test sets cheaply. We find that MTurk can be used to make test sets much cheaper than professionally-produced test sets. More importantly, in experiments with multiple MT systems, we find that the MTurk-produced test sets yield essentially the same conclusions regarding system performance as the professionally-produced test sets yield.", "text": "building machine translation test sets relatively expensive task. becomes increasingly desired language pairs domains becomes necessary build test sets case. paper investigate using amazon’s mechanical turk make test sets cheaply. mturk used make test sets much cheaper professionally-produced test sets. importantly experiments multiple systems mturk-produced test sets yield essentially conclusions regarding system performance professionally-produced test sets yield. machine translation research empirically evaluated comparing system output reference human translations typically using automatic evaluation metrics. method establishing translation test hold part training used testing. however practice typically overestimates system quality compared evaluating test drawn different domain. therefore it’s necessary make test sets language pairs also domains. creating reasonable sized test sets domains expensive. example workshop statistical machine translation uses non-professional professional translators create test sets annual shared translation tasks total cost creating test sets consisting roughly words across sentences seven european languages approximately slightly usd/word. creating test sets consisting sentences languages approximately slightly usd/word. paper examine amazon’s mechanical turk create translation test sets statistical machine translation research. snow showed mturk useful creating data variety tasks combination judgments non-experts attain expert-level quality many cases. callisonburch showed mturk could used low-cost manual evaluation machine translation quality suggested might possible mturk create test sets initial pilot study turkers produced translations sentences languages. paper explores detail asking turkers translate urdu sentences urdu-english test used nist machine translation evaluation workshop. evaluate multiple systems professionallyproduced nist test mturkproduced test mturk-produced test yields essentially conclusions system performance nist yields. paper published within proceedings naacl workshop creating speech language data amazon’s mechanical turk pages angeles california june association computational linguistics nist urdu-english test professionally produced machine translation evaluation containing four human-produced reference translations urdu sentences. posted urdu sentences mturk asked translations english. charged translation giving total translation cost usd. challenge encountered data collection many turkers would cheat giving fake translations. noticed many turkers pasting urdu online machine translation system giving output response even though instructions said this. manually monitored rejected responses blocked workers computing future work assignments. future plan combat principled manner converting urdu sentences image posting images. cheating turkers able paste machine translation system. also noticed many translations simple mistakes misspellings typos. wanted investigate whether would decrease value test second phase data collection posted translations gathered asked turkers correct simple grammar mistakes misspellings typos. postediting phase paid sentences giving total post-editing cost usd. summary built sets reference translations editing postediting. next section present results experiments test effective test sets evaluating systems. quality translations produced systems. therefore given system system outperforms another given system system high-quality professionally-produced test would want system also outperforms system mturk-produced test set. also desirable magnitudes differences performance systems also maintained. order measure differences performance using differences absolute magnitudes bleu scores work well because magnitudes bleu scores affected many factors test used number reference translations foreign sentence. determining performance differences systems especially comparing across different test sets percentage baseline performance. compute percentage baseline performance designate system baseline system percentage baseline system’s performance. example table shows absolute bleu scores percentage performance three systems tested different test sets. ﬁrst test table nist- four reference translations urdu sentence. next four test sets single reference translation urdu sentence note bleu scores single-reference translation test sets much lower test four reference translations difference absolute magnitudes bleu scores three different systems different different test sets. however percentage performance systems maintained across different test sets. another useful purpose would absolute sense quality translations seems reach currently values bleu scores difﬁcult precise levels translation quality. table table shows three systems evaluated different test sets. system-test pair numbers displayed. number bleu score system using test set. example isi-syntax tested nist- test bleu score bottom number percentage baseline system performance achieved. isi-syntax used baseline. thus always percentage performance test sets. illustrate computing percentage performance systems consider jhusyntax tested nist bleu score divided bleu score baseline system galley syntax augmented chosen represent stateof-the-art performance achieved highest scores nist knowledge. also similar performance nist want similar performance maintained evaluate mturk-produced test sets. third system open source implementation chosen though competitive system clear markedly lower performance nist systems want difference performance also maintained shift evaluation mturk-produced test sets. table table shows three systems evaluated using ofﬁcial nist test test sets constructed system-test pair numbers displayed. number bleu score system using test set. example isi-syntax tested nist- test bleu score bottom number percentage baseline system performance achieved. isi-syntax used baseline. nist test set. primarily nist four translations foreign sentence whereas mturk-produced sets translation foreign sentence. different scale bleu scores compare performances using percentage baseline performance. syntax system baseline since achieved highest results nist. main observation results table relative performance various systems amount differences performance maintained mturkproduced test sets nist test set. particular whether using nist test mturk-produced test sets would conclude syntax syntax perform joshua-hierarchical delivers performance syntax systems. post-edited test yield different conclusions non-edited test yielded value post-editing test creation remains open question. niﬁcantly reduced cost. large cost savings hamper utility test evaluating systems’ translation quality. experiments mturk-produced test sets lead essentially conclusions multiple systems’ translation quality much expensive professionally-produced test sets. it’s important able build test sets quickly cheaply need ones domains shown feasibility using mturk build test sets future plan build test sets speciﬁc domains release community spur work domain-adaptation also envision using mturk collect additional training data tune system domain. it’s shown active learning used reduce training data annotation burdens variety tasks therefore future work plan mturk combined active learning approach gather data domain investigate improving performance specialized domains. we’ll need test sets specialized domains able evaluate effectiveness line research therefore need able build test sets. light ﬁndings presented paper seems build test sets using mturk relatively costs without sacriﬁcing much utility evaluating systems. research supported euromatrixplus project funded european commission darpa gale program contract hr--- grant iis. thanks amazon mechanical turk providing credit.", "year": 2014}