{"title": "Blocks and Fuel: Frameworks for deep learning", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We introduce two Python frameworks to train neural networks on large datasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler with CUDA-support. It facilitates the training of complex neural network models by providing parametrized Theano operations, attaching metadata to Theano's symbolic computational graph, and providing an extensive set of utilities to assist training the networks, e.g. training algorithms, logging, monitoring, visualization, and serialization. Fuel provides a standard format for machine learning datasets. It allows the user to easily iterate over large datasets, performing many types of pre-processing on the fly.", "text": "introduce python frameworks train neural networks large datasets blocks fuel. blocks based theano linear algebra compiler cuda-support facilitates training complex neural network models providing parametrized theano operations attaching metadata theano’s symbolic computational graph providing extensive utilities assist training networks e.g. training algorithms logging monitoring visualization serialization. fuel provides standard format machine learning datasets. allows user easily iterate large datasets performing many types pre-processing blocks fuel developed montreal institute learning algorithms university montreal. focus lies quick prototyping complex neural network models. intended target audience researchers design experiment machine learning algorithms especially deep learning algorithms. several libraries built theano exist including pylearn groundhog lasagne keras. like mila-developed predecessors blocks maintains focus research rapid prototyping. blocks diﬀerentiates notably mentioned toolkits unique relationship theano. instead introducing abstract objects representing ‘models’ ‘layers’ blocks annotates theano computational graph maintaining ﬂexibility theano making large models manageable. data processing integral part training neural networks addressed many aforementioned frameworks. fuel aims gap. provides tools download datasets iterate/preprocess eﬃciently. blocks fuel developed beginning strong focus software engineering best practices. development teams strive high test coverage thorough documentation carefully considered apis. theano popular choice implementation neural networks pascanu blocks many libraries pylearn build theano providing reusable components common neural networks linear transformations followed non-linear activations complicated components lstm units. blocks components referred bricks parametrized theano operations. bricks consist theano shared variables example weight matrix linear transformation ﬁlters convolutional layer. bricks parameters transform symbolic theano variables. bricks contain bricks within them. introduces hierarchy computational graph deﬁned theano makes easier address conﬁgure complex models programmatically. parameters bricks initialized using variety schemes popular neural network literature sparse initialization orthogonal initialization recurrent weights etc. blocks comes large number ‘bricks’. besides standard activations transformations used feedforward networks also include variety advanced recurrent neural network components like lstm support attention mechanisms large neural networks often result theano computational graphs containing hundreds variables operations. blocks attempt abstract away complex graph make manageable annotating variables graph. input output parameter brick annotated such. variables also annotated role play model weights biases ﬁlters etc. series convenience tools written allow users ﬁlter symbolic computational graph based annotations apply transformations graph. many regularization methods weight decay weight noise dropout implemented generic model-agnostic way. furthermore complex query mechanism allows ﬁne-grained application apply weight noise weights belong lstm unit whose parent brick name foo. gradient descent training algorithm blocks composed diﬀerent ‘step rules’ modify descent direction variety algorithms adagrad adadelta adam rmsprop available step rules. experiment management performed using ‘main loop’ combines theano graph training algorithm fuel data stream. main loop ﬂexible extension interface used perform tasks monitoring validation serialization learning rate scheduling plotting printing saving logs etc. fuel’s goal provide common interface variety data formats published datasets mnist cifar- imagenet etc. making easy users write interface datasets. also provides variety on-the-ﬂy preprocessing methods random cropping images creating n-grams text ﬁles ability implement many methods easily. preprocessing steps chained together form complex transformations input data. sidestep python’s global interpreter lock ensure optimal performance fuel perform operations separate process transferring processed data training process using sockets. datasets distributed wide range formats. fuel simpliﬁes dataset storage converting built-in datasets annotated ﬁles addition eﬃcient format large datasets don’t memory easy organize document. data stored single following metadata attached integrating user data fuel straightforward simply requires data written metadata according speciﬁcations. finally standardizing convention fuel dataset independent users free implement dataset objects employing backends rest fuel’s components. fuel −download script used download data ﬁles. downloading mnist data ﬁles easy typing fuel −download mnist. fuel −convert script used convert data ﬁles hdf-format. reproducibility important feature fuel blocks fuel −convert script automatically tags ﬁles creates relevant module interface versions exact command used generate ﬁles. inspection metadata done fuel −info script. training large deep neural networks often take days even weeks. hence regular checkpointing training progress important. blocks aims make resumption experiments entirely transparent even across platforms ensuring reproducibility experiments. goal complicated shortcomings python’s pickle serialization module unable serialize many iterators fuel heavily depends order iterate large datasets eﬃciently. circumvent reimplemented itertools module python standard library serializable. blocks fuel well documented documentation tutorials available online. active mailing lists support users libraries. separate repository maintained users contribute non-trivial examples blocks. implementations neural machine translation models deep recurrent attentive writer model publicly available examples state-of-the-art models succesfully implemented using blocks. https//jaberg.github.io/skdata/ https//github.com/mila-udem/picklable-itertools https//groups.google.com/d/forum/blocks-users https//groups.google.com/d/forum/fuel-users https//github.com/mila-udem/blocks-examples authors would like acknowledge support following agencies research funding computing support nserc calcul qu´ebec compute canada canada research chairs cifar. bahdanau thanks planet intelligent systems gmbh ﬁnancial support. would also like thank developers theano.", "year": 2015}