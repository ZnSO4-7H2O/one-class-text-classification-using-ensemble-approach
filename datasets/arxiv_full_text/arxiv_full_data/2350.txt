{"title": "Maximum Likelihood Bounded Tree-Width Markov Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Chow and Liu (1968) studied the problem of learning a maximumlikelihood Markov tree. We generalize their work to more complexMarkov networks by considering the problem of learning a maximumlikelihood Markov network of bounded complexity. We discuss howtree-width is in many ways the appropriate measure of complexity andthus analyze the problem of learning a maximum likelihood Markovnetwork of bounded tree-width.Similar to the work of Chow and Liu, we are able to formalize thelearning problem as a combinatorial optimization problem on graphs. Weshow that learning a maximum likelihood Markov network of boundedtree-width is equivalent to finding a maximum weight hypertree. Thisequivalence gives rise to global, integer-programming based,approximation algorithms with provable performance guarantees, for thelearning problem. This contrasts with heuristic local-searchalgorithms which were previously suggested (e.g. by Malvestuto 1991).The equivalence also allows us to study the computational hardness ofthe learning problem. We show that learning a maximum likelihoodMarkov network of bounded tree-width is NP-hard, and discuss thehardness of approximation.", "text": "study problem tribution hood distribution bounded combinatorial maximum weight hypertree np-hard solve exactly vide approximation algorithm able performance fact study somewhat maximum likelihood minimizing pirical distribution problem cept class divergence class minimizes beyond target. projections applications finding maximum likelihood distribution. paper framework discuss projecting markov networks estimating ficult problem. search bayesian hard fact) narios guaranteed slightly solely attained \"concept\" class maximum likelihood sought. purely hardness mation accuracy. similarly formalize torial projecting bounded tree-width maximum-weight rise global tion algorithms contrasts heuristics lence also allows ness learning maximum likelihood width np-hard chow provided rigorous analysis casting problem tree thus providing rithm would like generalize chow problem likelihood section discuss appropriate analyze hood markov network chow showed that markov trees reduction empty graph additively contribution dent structure information composition point decomposition local elements bounding would also like limit putations. compact representation small clique size generally performing conditional even calculating markov networks likelihood might infeasible. would extremely graph minimizes calculating order work markov networks ticular abilities triangulated formed time linear marginals i.e. exponential cliques angulated network tions tractable triangulation note adding edges graph decrease divergence information since distribu­ tion markov network certainly one. projected triangulatedtriangulated tree-width lower equal divergence. consider weight candidate three­ clique three variables depen­ pairwise independent dence using calculate h+h+ non-negative. efit taking account dependency variables formation divergence. information marginal distribution three singleton marginal distributions. tempting adopt clean interpretation weights weight represents taking account additional tained dency. interpretation weight candidate d-clique information gence true d-way marginal maxi­ agrees consider total weight graph con­ surprise dependencies taining three-clique. markov chain already captured pair­ wise dependencies accord­ ingly reduction information diver­ gence captured however call weight function monotone weight function. weight function total summed weight graph containing single summed weight graph containing weight function number parameters therefore complexity resulting tempting least polynomial parameters. learning practically input size would generally data case. number therefore know construct sample specified mtional biases. however biases corresponding rational achieve approximate weights biases square roots rationals approximated although maximum hypertree problem hard present integer programming based approximation stant show polynomial-time finds triangulated total summed weight within constant factor maximum possible total summed weight. unfortunately pends heavily k-for width find graph recall decomposition gence presented distribution cross entropy relative figure viewed representing likelihoo pendent models) markov maximum attainable tropy empirical graph gain maximum likelihood fully independent model. constant factor approximation constant factor approximation likelihood. thus attain constant factor width) approximation means constant exponential factor approximation nately imum hypertree within additive constant np-hard approximate multiplicative constant. np-hard find markov network tree-width optimal likelihood tree-width work also scoring model selection. scores decomposed triangulated graphs. however longer monotone hold. might achieved weights hardness finding triangulated graph bounded tree-width minimizes demonstrated tribution cast combinatorial finding maximum weight hypertree. maximum hypertree projection problem approximation mance guarantee. weak large remains result ever maximum weight hypertree presented proved algorithms markov networks proximation factor constant information divergence likelihood). factor provide information tion narrow markov network entropy ical distribution) information useful tion divergence maximum likelihood approximating ever approximating interesting target narrow markov network. optimal information divergence much stringent factor reduction versus independent relevant even target ical distribution narrow markov network.", "year": 2013}