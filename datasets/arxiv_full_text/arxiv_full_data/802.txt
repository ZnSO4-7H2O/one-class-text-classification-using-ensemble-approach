{"title": "Neural Architecture Search with Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "text": "neural networks powerful ﬂexible models work well many difﬁcult learning tasks image speech natural language understanding. despite success neural networks still hard design. paper recurrent network generate model descriptions neural networks train reinforcement learning maximize expected accuracy generated architectures validation set. cifar- dataset method starting scratch design novel network architecture rivals best human-invented architecture terms test accuracy. cifar- model achieves test error rate percent better faster previous state-of-the-art model used similar architectural scheme. penn treebank dataset model compose novel recurrent cell outperforms widely-used lstm cell state-of-the-art baselines. cell achieves test perplexity penn treebank perplexity better previous state-of-the-art model. cell also transferred character language modeling task achieves state-of-the-art perplexity last years seen much success deep neural networks many challenging applications speech recognition image recognition machine translation along success paradigm shift feature designing architecture designing i.e. sift alexnet vggnet googlenet resnet although become easier designing architectures still requires expert knowledge takes ample time. neural network typically speciﬁed variable-length string. therefore possible recurrent network controller generate string. training network speciﬁed string child network real data result accuracy validation set. using accuracy reward signal compute policy gradient update controller. result next iteration controller give higher probabilities architectures receive high accuracies. words controller learn improve search time. experiments show neural architecture search design good models scratch achievement considered possible methods. image recognition cifar- neural architecture search novel convnet model better human-invented architectures. cifar- model achieves test error faster current best model. language modeling penn treebank neural architecture search design novel recurrent cell also better previous lstm architectures. cell model found achieves test perplexity penn treebank dataset perplexity better previous state-of-the-art. hyperparameter optimization important research topic machine learning widely used practice despite success methods still limited search models ﬁxed-length space. words difﬁcult generate variable-length conﬁguration speciﬁes structure connectivity network. practice methods often work better supplied good initial model bayesian optimization methods allow search ﬁxed length architectures less general less ﬂexible method proposed paper. modern neuro-evolution algorithms e.g. wierstra floreano stanley hand much ﬂexible composing novel models usually less practical large scale. limitations fact search-based methods thus slow require many heuristics work well. neural architecture search parallels program synthesis inductive programming idea searching program examples machine learning probabilistic program induction used successfully many settings learning solve simple sort list numbers learning examples controller neural architecture search auto-regressive means predicts hyperparameters time conditioned previous predictions. idea borrowed decoder end-to-end sequence sequence learning unlike sequence sequence learning method optimizes non-differentiable metric accuracy child network. therefore similar work bleu optimization neural machine translation unlike approaches method learns directly reward signal without supervised bootstrapping. also related work idea learning learn meta-learning general framework using information learned task improve future task. closely related idea using neural network learn gradient descent updates another network idea using reinforcement learning update policies another network following section ﬁrst describe simple method using recurrent network generate convolutional architectures. show recurrent network trained policy gradient method maximize expected accuracy sampled architectures. present several improvements core approach forming skip connections increase model complexity using parameter server approach speed training. last part neural architecture search controller generate architectural hyperparameters neural networks. ﬂexible controller implemented recurrent neural network. let’s suppose would like predict feedforward neural networks convolutional layers controller generate hyperparameters sequence tokens figure controller recurrent neural network samples simple convolutional network. predicts ﬁlter height ﬁlter width stride height stride width number ﬁlters layer repeats. every prediction carried softmax classiﬁer next time step input. experiments process generating architecture stops number layers exceeds certain value. value follows schedule increase training progresses. controller ﬁnishes generating architecture neural network architecture built trained. convergence accuracy network held-out validation recorded. parameters controller optimized order maximize expected validation accuracy proposed architectures. next section describe policy gradient method update parameters controller generates better architectures time. list tokens controller predicts viewed list actions design architecture child network. convergence child network achieve accuracy held-out dataset. accuracy reward signal reinforcement learning train controller. concretely optimal architecture controller maximize expected reward represented validation accuracy k-th neural network architecture achieves trained training dataset update unbiased estimate gradient high variance. order reduce variance estimate employ baseline function long baseline function depend current action still unbiased gradient estimate. work baseline exponential moving average previous architecture accuracies. accelerate training parallelism asynchronous updates neural architecture search gradient update controller parameters corresponds training child network convergence. training child network take hours distributed training asynchronous parameter updates order speed learning process controller parameter-server scheme parameter server shards store shared parameters controller replicas. controller replica samples different child architectures trained parallel. controller collects gradients according results minibatch architectures convergence sends parameter server order update weights across controller replicas. implementation convergence child network reached training exceeds certain number epochs. scheme parallelism summarized figure figure distributed training neural architecture search. parameter servers store send parameters controller replicas. controller replica samples architectures multiple child models parallel. accuracy child model recorded compute gradients respect sent back parameter servers. section search space skip connections branching layers used modern architectures googlenet residual section introduce method allows controller propose skip connections branching layers thereby widening search space. enable controller predict connections set-selection type attention built upon attention mechanism layer anchor point content-based sigmoids indicate previous layers need connected. sigmoid function current hiddenstate controller previous hiddenstates previous anchor points represents hiddenstate controller anchor point j-th layer ranges sample sigmoids decide previous layers used inputs current layer. matrices wprev wcurr trainable parameters. connections also deﬁned probability distributions reinforce method still applies without signiﬁcant modiﬁcations. figure shows controller uses skip connections decide layers wants inputs current layer. framework layer many input layers input layers concatenated depth dimension. skip connections cause compilation failures layer compatible another layer layer input output. circumvent issues employ three simple techniques. first layer connected input layer image used input layer. second ﬁnal layer take layer outputs connected concatenate sending ﬁnal hiddenstate classiﬁer. lastly input layers concatenated different sizes small layers zeros concatenated layers sizes. finally section predict learning rate also assume architectures consist convolutional layers also quite restrictive. possible learning rate predictions. additionally also possible predict pooling local contrast normalization batchnorm architectures. able types layers need additional step controller predict layer type hyperparameters associated section modify method generate recurrent cells. every time step controller needs functional form takes inputs. simplest tanh formulation basic recurrent cell. complicated formulation widely-used lstm recurrent cell computations basic lstm cells generalized tree steps take inputs produce ﬁnal output. controller needs label node tree combination method activation function merge inputs produce output. outputs inputs next node tree. allow controller select methods functions index nodes tree order controller visit node label needed hyperparameters. inspired construction lstm cell also need cell variables represent memory states. incorporate variables need controller predict nodes tree connect variables predictions done last blocks controller rnn. make process clear show example figure tree structure leaf nodes internal node. leaf nodes indexed internal node indexed controller needs ﬁrst predict blocks block specifying combination method activation function tree index. needs predict last blocks specify connect temporary variables inside tree. speciﬁcally figure example recurrent cell constructed tree leaf nodes internal node. left tree deﬁnes computation steps predicted controller. center example predictions made controller computation step tree. right computation graph recurrent cell constructed example predictions controller. controller predicts second element cell index relu relu. controller predicts elemm sigmoid tree index means need since maximum index tree controller predicts ﬁrst element cell index means output tree index activation i.e. apply method image classiﬁcation task cifar- language modeling task penn treebank benchmarked datasets deep learning. cifar- goal good convolutional architecture whereas penn treebank goal good recurrent cell. dataset separate held-out validation dataset compute reward signal. reported performance test computed network achieves best result held-out validation dataset. details experimental procedures results follows. dataset experiments cifar- dataset data preprocessing augmentation procedures line previous results. ﬁrst preprocess data whitening images. additionally upsample image choose random crop upsampled image. finally random horizontal ﬂips cropped image. search space search space consists convolutional architectures rectiﬁed linear units non-linearities batch normalization skip connections layers every convolutional layer controller select ﬁlter height ﬁlter width number ﬁlters learning rate weights controller initialized uniformly distributed training number parameter server shards number controller replicas number child replicas means networks trained gpus concurrently time. controller samples architecture child model constructed trained epochs. reward used updating controller maximum validation accuracy last epochs cubed. validation examples randomly sampled training remaining examples used training. settings training cifar- child models used huang momentum optimizer learning rate weight decay momentum used nesterov momentum training controller schedule increasing number layers child networks training progresses. cifar- controller increase depth child models every samples starting layers. results controller trains architectures architecture achieves best validation accuracy. small grid search learning rate weight decay batchnorm epsilon epoch decay learning rate. best model grid search convergence compute test accuracy model summarize results table seen table neural architecture search design several promising architectures perform well best models dataset. network network all-cnn deeply supervised highway network scalable bayesian optimization fractalnet dropout/drop-path resnet resnet resnet stochastic depth densenet huang densenet huang densenet huang densenet-bc huang neural architecture search stride pooling neural architecture search predicting strides neural architecture search pooling neural architecture search pooling ﬁlters first controller predict stride pooling design -layer architecture achieves error rate test set. architecture good balance accuracy depth. fact shallowest perhaps inexpensive architecture among performing networks table. architecture shown appendix figure notable feature architecture many rectangular ﬁlters prefers larger ﬁlters layers. like residual networks architecture also many one-step skip connections. architecture local optimum sense perturb performance becomes worse. example densely connect layers skip connections performance becomes slightly worse remove skip connections performance drops second experiments controller predict strides addition hyperparameters. stated earlier challenging search space larger. case ﬁnds -layer architecture achieves error rate test much worse ﬁrst experiments. finally allow controller include pooling layers layer layer architectures controller design -layer network achieves close best human-invented architecture achieves limit search space complexity model predict layers layer prediction fully connected block layers. additionally change number ﬁlters model predict result improved adding ﬁlters layer architecture. additionally model ﬁlters added fast densenet model achieves better performance. densenet model achieves error rate uses convolutions reduce total number parameters exact comparison. dataset apply neural architecture search penn treebank dataset well-known benchmark language modeling. task lstm architectures tend excel improving difﬁcult small dataset regularization methods needed avoid overﬁtting. first make embedding dropout recurrent dropout techniques proposed zaremba also combine method sharing input output embeddings e.g. bengio mnih hinton especially inan press wolf results method marked shared embeddings. search space following section controller sequentially predicts combination method activation function node tree. node tree controller needs select combination method activation method number input pairs cell called base number experiments. base number search space approximately architectures much larger number architectures allow controller evaluate. training details controller training almost identical cifar- experiments except modiﬁcations learning rate controller slightly smaller controller cifar- distributed training means networks trained cpus concurrently time asynchronous training parameter updates parameter-server gradients replicas accumulated. experiments every child model constructed trained epochs. every child model layers number hidden units adjusted total number learnable parameters approximately match medium baselines experiments controller predict cell structure hyperparameters. reward function results table provide comprehensive list architectures performance dataset. seen table models found neural architecture search outperform state-of-the-art models dataset best models achieves gain almost perplexity. cell better model achieves perplexity also times faster previous best network requires running cell times time step mikolov zweig mikolov zweig cache mikolov zweig mikolov zweig rnn-lda mikolov zweig rnn-lda cache pascanu deep cheng sum-prod zaremba lstm zaremba lstm variational lstm variational lstm variational lstm variational lstm charcnn press wolf variational lstm shared embeddings merity zoneout variational lstm merity pointer sentinel-lstm inan vd-lstm real zilly variational shared embeddings neural architecture search base neural architecture search base shared embeddings neural architecture search base shared embeddings newly discovered cell visualized figure appendix visualization reveals cell many similarities lstm cell ﬁrst steps likes compute several times send different components cell. transfer learning results understand whether cell generalize different task apply character language modeling task dataset. experimental setup similar variational dropout also train lstm setup fair lstm baseline. models trained steps best test perplexity taken according step validation perplexity best. results test method state-of-art methods reported table results small settings parameters conﬁrm cell indeed generalize better lstm cell. additionally carry larger experiment model parameters. model weight decay rate trained steps test perplexity taken validation perplexity highest. dropout rates described embedding dropout. adam optimizer learning rate input embedding size model layers hidden units. used minibatch size bptt length setting model achieves perplexity state-of-the-art result task. network layers encoder layers decoder. ﬁrst layer encoder bidirectional connections. attention module neural network hidden layer. lstm cell used number hidden units layer model trained distributed setting parameter sever workers. additionally worker uses gpus minibatch adam learning rate ﬁrst training steps learning rate steps. learning rate annealed dividing every steps reaches training stopped steps. details found experiment cell make change settings except dropping cell adjusting hyperparameters model computational complexity base model. result shows cell computational complexity achieves improvement test bleu default lstm cell. though improvement huge fact cell used without tuning existing gnmt framework encouraging. expect tuning help cell perform better. control experiment adding functions search space test robustness neural architecture search list combination functions list activation functions rerun experiments. results show even bigger search space model achieve somewhat comparable performance. best architecture shown figure appendix control experiment comparison random search instead policy gradient random search best network. although baseline seems simple often hard surpass report perplexity improvements using policy gradient random search training progresses figure results show best model using policy gradient better best model using random search also average models also much better. figure improvement neural architecture search random search time. plot difference average models controller ﬁnds random search every models run. paper introduce neural architecture search idea using recurrent neural network compose neural network architectures. using recurrent network controller method ﬂexible search variable-length architecture space. method strong empirical performance challenging benchmarks presents research direction automatically ﬁnding good neural network architectures. code running models found controller cifar- released https//github.com/tensorﬂow/models additionally added cell found using method name nascell tensorflow others easily marcin andrychowicz misha denil sergio gomez matthew hoffman david pfau schaul nando freitas. learning learn gradient descent gradient descent. arxiv preprint arxiv. geoffrey hinton deng dong george dahl abdel-rahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine hector mendoza aaron klein matthias feurer jost tobias springenberg frank hutter. towards automatically-tuned neural networks. proceedings workshop automatic machine learning jasper snoek oren rippel kevin swersky ryan kiros nadathur satish narayanan sundaram mostofa patwary mostofa ryan adams scalable bayesian optimization using deep neural networks. icml christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. cvpr yonghui mike schuster zhifeng chen quoc mohammad norouzi google’s neural machine translation system bridging human machine translation. arxiv preprint arxiv. figure convolutional architecture discovered method search space strides pooling layers. ﬁlter height ﬁlter width number ﬁlters. note skip connections residual connections. layer many input layers input layers concatenated depth dimension. figure comparison original lstm cell good cells model found. left lstm cell. right cell found model search space include sin. bottom cell found model search space includes", "year": 2016}