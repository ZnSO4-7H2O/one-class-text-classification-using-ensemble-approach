{"title": "Influence-Directed Explanations for Deep Convolutional Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We study the problem of explaining a rich class of behavioral properties of deep neural networks. Distinctively, our influence-directed explanations approach this problem by peering inside the net- work to identify neurons with high influence on the property and distribution of interest using an axiomatically justified influence measure, and then providing an interpretation for the concepts these neurons represent. We evaluate our approach by training convolutional neural net- works on MNIST, ImageNet, Pubfig, and Diabetic Retinopathy datasets. Our evaluation demonstrates that influence-directed explanations (1) identify influential concepts that generalize across instances, (2) help extract the essence of what the network learned about a class, (3) isolate individual features the network uses to make decisions and distinguish related instances, and (4) assist in understanding misclassifications.", "text": "figure images cars labeled sports imagenet model receptive ﬁelds inﬂuential feature comparative quantity characterizes model’s tendency predict sports convertible cases features within ﬁeld contain distinctive concept classes. proposed ﬁtting simpler interpretable models around test point predict relevant input regions. appealing feature approaches capture input inﬂuence. however approaches relate instance-speciﬁc features instance-speciﬁc predictions explanations produce generalize beyond single test point orthogonal approach visualize features learned networks identifying input instances maximally activate internal neuron either optimizing activation input space searching instances dataset importantly type explanation gives insight higher-level concepts learnt network naturally generalizes across instances classes. however approach relate higher-level concepts predictions cause. indeed examining activations alone sufﬁcient paper introduces inﬂuence-directed explanations deep networks combine positive attributes lines work. approach peers inside network identify neurons high inﬂuence model’s behavior uses existing techniques provide interpretation concepts represent. study problem explaining rich class behavioral properties deep neural networks. distinctively inﬂuence-directed explanations approach problem peering inside network identify neurons high inﬂuence property distribution interest using axiomatically justiﬁed inﬂuence measure providing interpretation concepts neurons represent. evaluate approach training convolutional neural networks mnist imagenet pubﬁg diabetic retinopathy datasets. evaluation demonstrates inﬂuence-directed explanations identify inﬂuential concepts generalize across instances help extract essence network learned class isolate individual features network uses make decisions distinguish related instances assist understanding misclassiﬁcations. study problem explaining class behavioral properties deep neural networks focus convolutional neural networks. problem received signiﬁcant attention recent years rise deep networks associated concerns opacity explanations provide insight reasons behind incorrect network behavior play important role mitigating opacity. growing body work explaining deep convolutional network behavior based mapping models’ prediction outputs back relevant regions input image. accomplished various ways visualizing gradients backpropagation recently carnegie mellon university tsinghua university. correspondence klas leino <kleinocs.cmu.edu> shayak <shayakscs.cmu.edu> anupam datta <danupamcmu.edu> matt fredrikson <mfredrikcmu.edu>. introduce novel distributional inﬂuence measure allows identify neurons inﬂuential determining model’s behavior given distribution instances. able identify learned concepts cause network behave characteristically example distribution instances share particular label. figure demonstrates model trained imagenet dataset capability inﬂuence-directed explanations extract meaningful insight network’s inner workings. measure inﬂuence feature maps conv_ layer network’s tendency predict sports convertible. images figure computed rendering receptive ﬁeld inﬂuential original feature space corresponding image results coincide intuitive understanding distinction classes depicted interpretation highlights portion image depicting car’s top. inﬂuenceempirical evaluation demonstrates directed explanations extract inﬂuential concepts generalize across instances whereas input-inﬂuence-based explanations fail reveal essence network views class isolate features network uses make predictions distinguish related instances assist understanding misclassiﬁcations case inﬂuence-directed explanations leverage ability measure internal inﬂuence produce useful explanations would possible otherwise. section propose distributional inﬂuence axiomatically justiﬁed family measures inﬂuence. distributional inﬂuence parameterized slice network quantity interest distribution interest. measure average partial derivative quantity interest distribution interest slice. describe measure parameters below. section justify family measures proving measures satisfy natural properties. slice parameter exposes internals network allows compute inﬂuence respect intermediate neurons signiﬁcant departure prior work. importantly opposed input pixels internal neurons represent higher-level concepts inﬂuential internal neurons allow explanations general rather speciﬁc single instances. section compare measure others used prior work. distribution quantity interest together capture aspects network behavior interested explaining. examples distributions interest single instance distribution ‘cat’ images overall distribution images. ﬁrst distribution interest focuses single instance classiﬁed particular second explains essence class third identiﬁes generally inﬂuential neurons entire population. fourth instance uniform distribution line segment scaled instances instance baseline yields measure called integrated gradients examples quantities interest outcome towards ‘cat’ class comparative outcome towards ‘cat’ versus ‘dog’ ﬁrst quantity interest answers question particular input classiﬁed whereas second helpful understanding network distinguishes ‘cat’ instances ‘dog’ instances. represent quantities interest networks continuous differentiable functions number inputs distributional inﬂuence measure denoted measures inﬂuence input quantity interest distribution interest distribution next deﬁne slice network. particular layer network viewed slice. generally slice partitioning network parts exposes internals. formally slice network tuple functions internal representation instance given setting elements viewed activations neurons particular layer. deﬁnition inﬂuence element internal representation deﬁned given inﬂuence measure deﬁned section parameterized distribution interest measure taken. selecting point mass resulting measurements characterize importance features components model’s behavior single instance. meaningful interpretation measurements refer instance thus reﬂect speciﬁc features concepts generalize across class. deﬁning distribution interest support larger instances yield explanations individual measure inﬂuence instance individually rank features units accordingly. labeled mean measure inﬂuence using distribution interest corresponding class question rank features units instance according resulting ordering. comparing individual mean results tells well explanations generalize across class. individual explanations signiﬁcantly outperform respective mean explanations conclude distributional inﬂuences failed identify concepts relevant many instances class. results figure show small performance instance-speciﬁc mean cases. suggests units deemed relevant class on-average also tend matter consistently across instances class. compared integrated gradients operates input features class-wide average inﬂuences computed internal nodes outperform signiﬁcantly. particularly surprising size illustrates additional explanatory power inﬂuential internal units. figure also shows using integrated gradients results rapid initial dropoff. regardless area curve respective internal inﬂuence approximately lower integrated gradients function value drops baseline value quickly factor observed inﬂuence distributed fairly evenly across fewer units layer question experiments whereas bottom input layer relatively small number features account inﬂuence longer tail features small amount inﬂuence. finally sharp dropoff additionally explained observation many mnist instances possible change classiﬁcation changing relatively pixels whereas phenomenon pronounced datasets. apparent figure often case relatively units highly inﬂuential towards particular class. cases refer essence class network’s behavior classes understood focusing units. validate claim demonstrate units isolated rest model extract classiﬁer proﬁcient distinguishing class instances rest data distribution original model. introduce technique compressing models using inﬂuence measurements yield class-speciﬁc expert models demonstrate essence class learned model. figure plot decrease function value features removed order inﬂuence. model based lenet trained mnist dataset. case inﬂuential feature hidden unit incrementally removed resulting value depicted vertical axis. vertical axis normalized average value class value dashed curves depict quantity inﬂuence measured instance individually solid curves inﬂuence empirical distribution entire class distribution interest. plots shown integrated gradients well internal inﬂuence slice ﬁrst fully-connected layer network. capture factors common network behaviors across corresponding population instances. explanations capture essence network learned population used identify concepts relevant network’s behavior figure quantiﬁes degree internal units identiﬁed using inﬂuence correspond relevant general concepts compared inﬂuence measurements obtained using integrated gradients curves computed turning either input features units network order determined inﬂuence. adapted approach hidden units intervening activation vertical axis depicts percentage dropoff network’s output prior softmax percentage features removed order inﬂuence. evaluated measure instances number mnist lenet model selected integrated gradients point comparison because found outperformed comparable instances discussed related work. calculated inﬂuence ways characterize difference instancespeciﬁc general measurements. cases labeled table model compression recall randomly-selected imagenet classes. columns marked orig. correspond original model act. experts computed using activation levels inﬂ. experts computed using inﬂuence measures. precision cases table shows recall experts found randomly selected imagenet classes well recall original model class experts computed using activations rather inﬂuence. precision shown cases shows bottom inﬂuential neurons sufﬁcient capture concepts embodied particular layer discriminate given class others. removing non-inﬂuential neurons yields signiﬁcantly higher recall baseline model moreover activation levels meaningful consistent indication relevance neuron. words measuring internal inﬂuence effective identify concepts network learned order discriminate classes other. results discussed demonstrate internal distributional inﬂuence measurements used identify relevant concepts generalize across instances distinguish classes. section show concepts identiﬁed often represent input-space features interpretable domain experts important correctly classifying instances identiﬁed even possible interpret concepts reliably pixel space. building work trained inception network diagnose severity diabetic retinopathy color retinal fundus images diabetic retinopathy medical condition characterized damage retina occurring diabetes. classiﬁed scale class corresponding absence symptoms class severe presentation. kaggle dataset used train model class- common remaining classes distributed relatively evenly. class- least severe positive diagnosis characterized presence visible mifigure score experts randomly-selected class imagenet using network. rows columns correspond respectively. results indicate selecting larger sets units compression lead increased performance returns diminish rapidly good experts identiﬁed using small units. given model softmax output slice vector. intuitively masks units layer wish retain locations corresponding units everywhere else. slice compression g∗mh) corresponds original model discarding units selected given model obtain binary classiﬁer class projecting softmax output addition outputs f|j) projection class-speciﬁc experts sake discussion deﬁne class-wise expert slice compression whose corresponding binary classiﬁer achieves better recall binary classiﬁer obtained also achieving approximately recall. demonstrate inﬂuence measurements taken slice distribution interest conditioned class yields efﬁcient heuristic extracting experts large networks. particular compute measuring slice inﬂuence using quantity interest g|i. given parameters select units layer largest positive inﬂuence units greatest negative inﬂuence deﬁned zero positions except corresponding units. experiments obtain concrete values parameter sweep ultimately selecting values yield best experts criteria deﬁned above. figure shows score obtained randomly-selected class function notably measures plateau relatively units compressed model indicating croaneurysms only symptoms present fundus image distinguish class-. small size pixel space validating identiﬁed inﬂuence measurement challenging possible visualize well. address challenge created dataset control presence microaneurysm features characterize class- instances. speciﬁcally preprocessed images minor gaussian blur remove corresponding visual features trained second model dataset generated intervention. model trained original dataset behaved expected achieved non-trivial recall class- instances also able extract expert class model improved recall original model demonstrating internal inﬂuence measures identiﬁed distinctive concepts case. model trained intervened dataset displayed classiﬁcation behavior consistent expectation applying small-radius gaussian blur removes microaneurysm features. namely intervened model classiﬁed none instances validation class- instead classiﬁed true class- instances class-. moreover applied strategy extracting expert class- unable inﬂuential neurons achieved better recall. case even relax criteria selecting experts allow reduced precision. summarize controlling presence important concept source data often possible characterize concept represented internal units testing disappearing experts retrained models. discussed section computing inﬂuence slice network lets determine relevant neurons intermediate layers particular network behavior. particular given image network’s prediction image inﬂuence measurements slice reveal features concepts present image relevant prediction. figure interpretation three inﬂuential units slice corresponding convolutional layer network. explanation based integrated gradients taken network image. interpretation cases computed scaling pixels original image using results either method. figure comparative explanation classes sports convertible taken top-three inﬂuential units conv layer explanation computed using quantity interest corresponding sports instance used ential units quantity interest corresponding correct classiﬁcation image. precisely quantity interest used example corresponds i.e. projection model’s softmax output coordinate corresponding correct label instance. visualization units obtained measuring inﬂuence input pixels units along color channel scaling pixels original image accordingly. convolutional units limited receptive ﬁeld resulting interpretation shows distinct regions original image case corresponding left mouth relevant model’s predicted classiﬁcation. compared explanation provided integrated gradients input features shown figure evident explanation based network’s internal units better localizing features used network prediction. inﬂuence-directed explanations parameterized quantity interest corresponding function equation changing quantity interest gives additional ﬂexibility characteristic explained inﬂuence measurements interpretation. class quantities particularly useful answering counterfactual questions instance classiﬁed rather given comparative quantity. precisely softmax classiﬁcation model predicts classes comparative quantity interest classes f|j. used equation quantity captures tendency model classify instances section used quantity show internal inﬂuences computed distribution capture high-level concepts learned network demonstrate also useful explaining individual predictions. continuing example discussed section figure shows example comparative explanation taken model trained imagenet dataset original instance shown left labeled sports leaf node imagenet hierarchy. measured inﬂuence using comparative quantity leaf class convertible using slice conv convolutional layer. interpretation computed top-three inﬂuential units layer discussed section examples figure receptive ﬁeld inﬂuential unit corresponds region containing hard vehicle understood distinctive feature according comparative quantity. figure shows interpretation instance computed using inﬂuence measurements taken quantity interest explanations capture features common cars comparative explanation isolates distinctive elements feature space. inﬂuential distributional concepts also lead insights misclassiﬁcation behavior particular instances. figure depicts example explanation. visualizations generated measuring inﬂuence slice corresponding bottom-most fully-connected layer diabetic retinopathy model quantity interest corresponds particular class outcome distribution conditioned corresponding class label. units layer sorted according inﬂuence conditioned distribution top-left corner corresponding largest positive inﬂuence bottom-right largest negative inﬂuence. speciﬁc instance inﬂuences layer measured magnitude sign corresponding unit class-wide ordering depicted size shape position large boxes denote larger magnitude whereas green boxes denote positive sign negative. figure depicts instance class correctly classiﬁed whereas instance class incorrectly classiﬁed. differences readily apparent inﬂuences correctly-classiﬁed instance align closely distributional order whereas incorrectly-classiﬁed case. suggests notion instance classiﬁed correct reason incorrect anomalous reason determined identifying inﬂuential high-level concepts. section justify family measures presented deﬁning natural properties inﬂuence measure satisfy proving tight characterization measures. ﬁrst address case inﬂuence measured respect inputs i.e. slice identity function. generalize measure general slices address case inﬂuence measured respect internal neurons. ﬁrst characterize measure measures inﬂuence input quantity interest distribution interest ﬁrst axiom linear agreement states linear systems coefﬁcient input inﬂuence. measuring inﬂuence linear models straightforward since unit change input corresponds change output given coefﬁcient. axiom linear models form second axiom distributional marginality states gradients points outside support distribution interest affect inﬂuence input. axiom ensures inﬂuence measure depends behavior model points within manifold containing input distribution. axiom bottom-most fully connected layer. compute grid distribution inﬂuence conditioned class class figure depicts instance class correctly classiﬁed such instance class incorrectly classiﬁed class inﬂuences depicted grid align closely class-wide ordering inﬂuences whereas visibly random. white space middle grid corresponds units inﬂuence quantity. stances third axiom distribution linearity states inﬂuence measure linear distribution interest. ensures inﬂuence measures properly weighted input space i.e. inﬂuence infrequent regions input space receive lesser weight inﬂuence measure compared frequent regions. axiom family distrigpada show inﬂuence measure satisﬁes three axioms weighted gradient input probability distribution theorem measure satisﬁes linear agreement distributional marginality distribution linearity given section generalize measure input inﬂuence measure used measure inﬂuence internal neuron. take axiomatic approach natural invariance properties structure network. ﬁrst axiom states inﬂuence measure agnostic network sliced long neuron respect inﬂuence measured unchanged. below notation refers vector element removed. slices j-equivalent appendix provided supplementary material proofs second axiom equates input inﬂuence input internal inﬂuence perfect predictor input. essentially encodes consistency requirement inputs internal neurons internal neuron exactly behavior input internal neuron inﬂuence input. axiom consider x−ihi slice prior work interpreting cnns focused answering questions given input image part instance relevant particular neuron? maximizes activation particular neuron? table comparison inﬂuence-directed explanations proposed prior related work including integrated gradients sensitivity analysis deconvolution layer-wise relevance propagation simple taylor decomposition ﬁrst three columns refer capabilities corresponding explanation framework ﬂexibility choice quantity interest distribution interest instance ability examine role internal neurons. latter columns describe properties inﬂuence measure used build explanations faithfulness refers distributional faithfulness; sensitivity requires instances differ feature yield different predictions feature assigned non-zero inﬂuence. denotes framework feature certain parameterizations denotes framework measures internal inﬂuence intermediary step computing feature inﬂuence. identifying inﬂuential regions approach interpreting predictions convolutional networks activations neurons back regions input image relevant outcomes neurons. possible approaches localizing relevance visualize gradients propagate activations back using gradients learning interpretable models predicting effect presence regions image approaches relate instance-speciﬁc features instance-speciﬁc predictions results generalize beyond single test point. visualization maximizing activation orthogonal approach visualize features learnt networks identifying input instances maximally activate neuron achieved either optimizing activation input space searching instances dataset examining causal inﬂuence neurons rather activations better identiﬁes neurons used network classiﬁcation. experiments section demonstrate examining activations fails identify important neurons. table presents detailed comparison inﬂuencedirected explanation framework presented related prior work measuring inﬂuence cnns. leftmost three columns describe framework properties measures allow ﬂexibility. first explanations parametric distribution interest allowing explain network behavior different levels granularity second explanations parametric quantity interest allow provide explanations different behaviors system opposed instance outcomes. cells marked columns denote measures offer limited form ﬂexibility along dimension choice appropriate baseline. third examining inﬂuence internal neurons plays important role capture general concepts demonstrate possible identify expert neurons certain distributions contrast frameworks proposed integrated gradients sensitivity analysis simple taylor decomposition based attributing relevance solely input features. deconvolution guided backpropogation cells marked internal inﬂuences used intermediary computing input inﬂuence. rightmost columns table reﬂect choice inﬂuence measures guided axiomatic choices different prior work notable difference stems distributional faithfulness criteria imposes weaker distribution marginality principle marginality principle imposed integrated gradients. practical consequence criteria integrated gradients certain choices baselines satisfy faithfulness notably several frameworks make inﬂuence measures satisfy sensitivity circumstances; matter described detail measures satisfy sensitivity problematic practice fail identify features components causally-relevant quantity interest leading blind spots focus irrelevant features. expect distributional inﬂuence measure introduced paper applicable broad deep neural networks. direction future work couple measure appropriate interpretation methods produce inﬂuence-directed explanations types deep networks recursive networks text processing tasks. another direction develop debugging tools models using inﬂuence-directed explanations building block. ribeiro marco tulio singh sameer guestrin carlos. trust you? explaining predictions classiﬁer. proceedings sigkdd international conference knowledge discovery data mining york acm. russakovsky olga deng krause jonathan satheesh sanjeev sean huang zhiheng karpathy andrej khosla aditya bernstein michael berg alexander fei-fei imagenet large scale visual recognition challenge. international journal computer vision ./s---y. samek binder montavon lapuschkin mller evaluating visualization deep neural network learned. ieee transactions neural networks learning systems simonyan karen vedaldi andrea zisserman andrew. deep inside convolutional networks visualising image classiﬁcation models saliency maps. arxiv e-prints szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. computer vision going deeper convolutions. pattern recognition http //arxiv.org/abs/.. bach sebastian binder alexander montavon gr´egoire klauschen frederick m¨uller klaus-robert samek wojciech. pixel-wise explanations non-linear classiﬁer decisions layer-wise relevance propagation. plos journal.pone.. girshick ross donahue jeff darrell trevor malik jitendra. rich feature hierarchies accurate object detection semantic segmentation. proceedings ieee conference computer vision pattern recognition cvpr isbn ----. nguyen dosovitskiy alexey yosinski jason brox thomas clune jeff. synthesizing preferred inputs neurons neural networks deep generator networks. corr abs/. http//arxiv.org/abs/.. pratt harry coenen frans broadbent deborah harding simon zheng yalin. convolutional neural networks diabetic retinopathy. procedia computer science ﬁrst characterize measure measures inﬂuence input quantity interest distribution interest ﬁrst axiom linear agreement states linear systems coefﬁcient input inﬂuence. measuring inﬂuence linear models straightforward since unit change input corresponds change output given coefﬁcient. axiom linear models form second axiom distributional marginality states gradients points outside support distribution interest affect inﬂuence input. axiom ensures inﬂuence measure depends behavior model points within manifold containing input distribution. axiom family distrigpada proof. choose function dirac delta function choose |axi. linearity agreement must case that distributional marginality pada. therefore distribution linearity axiom", "year": 2018}