{"title": "Data Augmentation Generative Adversarial Networks", "tag": ["stat.ML", "cs.CV", "cs.LG", "cs.NE"], "abstract": "Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13% increase in accuracy in the low-data regime experiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face (4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5% (from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%).", "text": "effective training neural networks requires much data. low-data regime parameters underdetermined learnt networks generalise poorly. data augmentation alleviates using existing data effectively. however standard data augmentation produces limited plausible alternative data. given potential generate much broader augmentations design train generative model data augmentation. model based image conditional generative adversarial networks takes data source domain learns take data item generalise generate within-class data items. generative process depend classes themselves applied novel unseen classes data. show data augmentation generative adversarial network augments standard vanilla classiﬁers well. also show dagan enhance few-shot learning systems matching networks. demonstrate approaches omniglot emnist learnt dagan omniglot vgg-face data. experiments increase accuracy low-data regime experiments omniglot emnist vgg-face matching networks omniglot observe increase increase emnist last decade deep neural networks produced unprecedented performance number tasks given sufﬁcient data. demonstrated variety domains spanning image classiﬁcation machine translation natural language processing speech recognition synthesis learning human play reinforcement learning among others. cases large datasets utilized case reinforcement learning extensive play. many realistic settings need achieve goals limited datasets; case deep neural networks seem fall short overﬁtting training producing poor generalisation test set. techniques developed years help combat overﬁtting dropout batch normalization batch renormalisation layer normalization however data regimes even techniques fall short since ﬂexibility network high. methods able capitalise known input invariances might form good prior knowledge informing parameter learning. figure learning generative manifold classes source domain help learn better classiﬁers shot target domain test point nearer orange point actually closer learnt grey data manifold. generate extra examples grey manifold nearest neighbour measure better match nearest manifold measure. figure graphical model dataset shift one-shot setting distribution class label changes extreme affecting distribution latent however generating distribution change. also possible generate data existing data applying various transformations original dataset. transformations include random translations rotations ﬂips well addition gaussian noise. methods capitalize transformations know affect class. technique seems vital low-data cases size dataset fact even models trained largest datasets imagenet beneﬁt practice. typical data augmentation techniques limited known invariances easy invoke. paper recognize learn model much larger invariance space training form conditional generative adversarial network different domain typically called source domain. applied low-data domain interest target domain. show data augmentation generative adversarial network enables effective neural network training even low-data target domains. dagan depend classes themselves captures cross-class transformations moving data-points points equivalent class. result applied novel unseen classes. illustrated figure also demonstrate learnt dagan used substantially efﬁciently improve matching networks few-shot target domains. augmenting data matching networks related models relevant comparator points class generated dagan. relates idea tangent distances involves targeting distances manifolds deﬁned learnt dagan. figure paper train dagan evaluate performance low-data target domains using standard stochastic-gradient neural network training speciﬁc one-shot meta-learning methods. datasets omniglot dataset emnist dataset complex vgg-face dataset. dagan trained omniglot used unseen omniglot terget domain also emnist domain demonstrate beneﬁt even transferring substantially different source target domains. vgg-face dataset provide substantially challenging test. present generated samples good quality also show substantial improvement classiﬁcation tasks vanilla networks matching networks. primary contributions paper using novel generative adversarial network learn representation process data demonstrate realistic data-augmentation samples single novel data point. application dagan augment standard classiﬁer low-data regime demonstrating application dagan meta-learning space showing performance better previous general meta-learning models. results showed performance beyond state-of-the omniglot case emnist case. background transfer learning dataset shift shot learning problem particular case dataset shift summarised graphical model figure term dataset shift generalises concept covariate shift multiple cases changes domains. shot learning extreme shift class distributions distributions share support classes zero probability ones move zero non-zero probability. however assumption class conditional distributions share commonality information transferred source domain one-shot target domain. generative adversarial networks speciﬁcally deep convolutional gans ability discriminate true generated examples objective approaches learn complex joint densities. recent improvements optimization process reduced failure modes learning process. data augmentation routinely used classiﬁcation problems. often non-trivial encode known invariances model. easier encode invariances data instead generating additional data items transformations existing data items. example labels handwritten characters invariant small shifts location small rotations shears changes intensity changes stroke thickness changes size etc. almost cases data augmentation priori known invariance. prior paper know works attempt learn data augmentation strategies. paper worthy note work authors learn augmentation strategies class class basis. approach transfer one-shot setting completely classes considered. few-shot learning meta-learning number approaches shot learning hierarchical boltzmann machine modern deep learning architectures one-shot conditional generation hierarchical variational autoencoders recently based one-shot generative model early effective approach oneshot learning involved siamese networks others used nonparametric bayesian approaches conditional variational autoencoders examples nearest neighbour classiﬁer kernel classiﬁer obvious choice. hence meta-learning distance metrics weighting kernels clear potential skip-residual pairwise networks also proven particularly effective various forms memory augmented networks also used collate critical sparse information incremental none approaches consider augmentation model basis meta-learning. models data augmentation know class label invariant particular transformation apply transformation generate additional data. know transformations might valid data related problems attempt learn valid transformations related problems apply setting example meta-learning; learn problems improve learning target problem. subject paper. generative adversarial methods approach learning generate examples density matching density training dataset xnd} size denoted learn minimizing distribution discrepancy measure generated data true data. generative model learnt generative adversarial network figure dagan architecture. left generator network composed encoder taking input image projecting lower dimensional manifold random vector transformed concatenated bottleneck vector; passed decoder network generates augmentation image. right adversarial discriminator network trained discriminate samples real distribution fake distribution adversarial training leads network generate images appear within class look different enough different sample. implemented neural network. here vectors generated latent gaussian variables provide variation generated. generative adversarial network thought learning transformations data manifold gives point manifold changing different direction continuously maps data manifold. data augmentation generative adversarial network generative adversarial network could also used data augmentation manifold. given data point could learn representation input along generative model before. combined model would take form neural network takes representation random inputs. given obtain generatively meaningful representation data point encapsulates generate extra augmentation data assumption gaussian without much loss generality usual generating distribution known nonlinear transformations gaussian; transformations fairly straightforward implement neural network. learning data augmentation model learnt source domain using adversarial approach. consider source domain consisting data xnd} corresponding target values tnd}. improved wgan critic either takes input data point second data point class input data point output current generator takes input. critic tries discriminate generated points real points generator trained minimize discriminative capability measured wasserstein distance importance providing original discriminator emphasised. want ensure generator capable generating different data related different from current data point. providing information current data point discriminator prevent simply autoencoding current data point. time provide class information learn generalise ways consistent across classes. architectures main experiments used dagan generator combination unet resnet henceforth call uresnet. uresnet generator total blocks block convolutional layers activations batch renormalisation followed downscaling upscaling layer. downscaling layers convolutions stride followed leaky relu batch normalisation dropout. upscaling layers stride replicators followed convolution leaky relu batch renormalisation dropout. omniglot emnist experiments layers ﬁlters. vgg-faces ﬁrst blocks encoder last blocks decoder ﬁlters last blocks encoder ﬁrst blocks decoder ﬁlters. addition block uresnet generator skip connections. standard resnet strided convolution also passes information blocks bypassing block non-linearity help gradient ﬂow. finally skip connections introduced equivalent sized ﬁlters network ﬁgure architecture pseudocode deﬁnition given appendix used densenet discriminator using layer normalization instead batch normalization; latter would break assumptions wgan objective function. densenet composed dense blocks transition layers deﬁned used growth rate dense block convolutional layers within discriminator also used dropout last convolutional layer dense block found improves sample quality. trained dagan epochs using learning rate adam optimizer adam parameters classiﬁcation experiment used densenet classiﬁer composed dense blocks transition layers dense block convolutional layers within classiﬁers total layers furthermore applied dropout last convolutional layer dense block. classiﬁer trained standard augmentation random gaussian noise added images random shifts along axis random degree rotations classiﬁers trained epochs learning rate adam optmizer datasets tested dagan augmentation datasets omniglot emnist vgg-faces. datasets split randomly source domain sets validation domain sets test domain sets. classiﬁer networks data character split test cases validation cases varying number training cases depending experiment. classiﬁer training done training cases examples domains hyperparameter choice made validation cases. finally test performance reported test cases target domain set. case splits randomized across test run. one-shot networks dagan training done source domain meta learning done source domain validated validation domain. results presented target domain data. target domain varying number training cases provided results presented test cases omniglot data split source domain target domain similarly split order classes shufﬂed source target domains contain diverse samples ﬁrst used source domain validation domain target domain test set. emnist data split source domain included classes validation domain included classes test domain included classes since emnist dataset thousands samples class chose subset class could make task low-data one. vgg-face dataset case randomly chose samples class uncorrupted images resulting full classes available dataset. shufﬂing split resulting dataset source domain included ﬁrst classes. test domain included classes validation domain included classes training dagan source domain dagan trained source omniglot domains using variety architectures standard u-nets resnet inspired architectures. increasingly powerful networks proved better generators uresnet described section generator model choice. figure shows improved variability generations achieved powerful architectures. dagan also trained vgg-faces source domains. examples generated faces given figure vanilla classifiers ﬁrst test well dagan augment vanilla classiﬁers trained target domain. densenet classiﬁer trained ﬁrst real data examples class. second case classiﬁer also passed dagan generated augmented data. real fake label also passed network enable network learn best emphasise true generated data. last step proved crucial maximizing potential dagan augmentations. training cycle varying numbers augmented samples provided real example best figure interpolated spherical subspace generation space using single real seed image real image ﬁgure top-left corner rest generated augment example using dagan. experiment emnist_standard emnist_dagan_augmented emnist_standard emnist_dagan_augmented emnist_standard emnist_dagan_augmented emnist_standard emnist_dagan_augmented table vanilla classiﬁcation results results averages independent runs. dagan augmentation improves classiﬁer performance cases. test accuracy result test cases test domain annotation rate selected performance validation domain. results held test cases target domain given table every case augmentation improves classiﬁcation. standard approach shot learning involves learning appropriate distance representations used nearest neighbour classiﬁer metric. focus matching networks learn representation space distances representation space produce good classiﬁers. networks used target domain nearest neighbour classiﬁcation. matching networks introduced matching network creates predictor support target domain using attention memory network generate appropriate comparison space comparing test example training examples. achieved simulating process small support sets source domain learning learn good mapping. however matching networks learn match based individual examples. augmenting support sets learning learn augmented data enhance classiﬁcation power matching networks apply augmented domain. beyond learn choose good examples dagan space best related test training manifolds. precisely train dagan source domain train matching networks source domain along sample-selector neural network selects best representative input used create additional datum augments case. quantities differentiable whole process trained full pipeline. networks tested validation domain best performing matching network sample-selector network combination selected test domain. technique name pixel distance pixel distance dagan augmentations matching nets neural statistician conv. prototypical networks siam-i siam-ii siam-i siam-ii srpn matching nets matching nets dagan augmentations table omniglot shot results results averages independent runs. note local implementation matching networks substantially outperform matching network results presented original paper however dagan augmentation takes matching networks level conv-arc note dagan augmentation even increase simple pixel distance nearest neighbour model non-negligible levels. training matching networks dagan augmentation augmentation used every matching network training episode simulate data augmentation process. used matching networks without full-context embedding version stacked generated images along original image. experiments classes sample class episode i.e. shot learning setting. ratio generated real data varied standard augmentations applied data dagan augmented standard settings. table showing omniglot one-shot results dagan enhancing even simple pixel distance improvement furthermore matching network experiments improvement state pushes matching network performance level conv technique used substantial prior knowledge data form achieve results. addition omniglot experimented emnist vgg-face. used omniglot dagan generate augmentation samples emnist stress test power network generalise dissimilar domains. one-shot results showed improvement baseline matching network training emnist vgg-face shot matching networks experiments dagan augmented system performed performance baseline one. however worth noting augmented matching network overﬁt dataset happened omniglot emnist experiments. indicates embedding network architecture used perhaps enough capacity learn complex task discriminating faces. embedding network architecture used described convolutional layers ﬁlters each. experiments larger embedding functions required better explore evaluate dagan performance vgg-face dataset. conclusions data augmentation widely applicable approach improving performance low-data setting dagan ﬂexible model automatic learn augment data. however beyond that demonstrate dagans improve performance classiﬁers even standard data-augmentation. furthermore meta-learning best choice augmentation one-shot setting leads better performance state meta learning methods. generality data augmentation across models methods means dagan could valuable addition data setting. deng dong richard socher li-jia fei-fei. imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference ieee jakob foerster yannis assael nando freitas shimon whiteson. learning communicate deep multi-agent reinforcement learning. advances neural information processing systems goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems annual conference neural information processing systems december montreal quebec canada http //papers.nips.cc/paper/-generative-adversarial-nets. jiuxiang zhenhua wang jason kuen lianyang amir shahroudy bing shuai ting xingxing wang gang wang. recent advances convolutional neural networks. http//arxiv.org/abs/.. shixiang timothy lillicrap ilya sutskever sergey levine. continuous deep q-learning model-based acceleration. international conference machine learning søren hauberg oren freifeld anders boesen lindbo larsen john fisher lars hansen. dreaming data class-dependent distributions diffeomorphisms learned data augmentation. artiﬁcial intelligence statistics kaiming xiangyu zhang shaoqing jian sun. delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation. http//arxiv.org/ abs/.. geoffrey hinton deng dong george dahl abdel-rahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutdinov. improving neural networks preventing co-adaptation feature detectors. http//arxiv.org/abs/.. sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. http//arxiv.org/abs/.. alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems akshay mehrotra ambedkar dukkipati. generative adversarial residual pairwise networks shot learning. arxiv preprint arxiv. http//arxiv.org/abs/ volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature alec radford luke metz soumith chintala. unsupervised representation learning deep convolutional generative adversarial networks. proceedings iclr volume abs/. http//arxiv.org/abs/.. danilo rezende danihelka karol gregor daan wierstra one-shot generalization deep generative models. proceedings international conference machine learning ruslan salakhutdinov joshua tenenbaum antonio torralba. one-shot learning hierarchical nonparametric bayesian model. icml unsupervised transfer learning david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature a.j. storkey. training test sets different characterising learning transfer. candela sugiyama schwaighofer lawrence dataset shift machine learning chapter press http//mitpress.mit.edu/catalog/item/default.asp? ttype=&tid=. yuxuan wang skerry-ryan daisy stanton yonghui weiss navdeep jaitly zongheng yang ying xiao zhifeng chen samy bengio quoc yannis agiomyrgiannakis clark saurous. tacotron fully end-to-end text-to-speech synthesis model. corr abs/. http//arxiv.org/abs/.. yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey google’s neural machine translation system bridging human machine translation. arxiv preprint arxiv. appendix appendix show form uresnet generator network figure full details uresnet model algorithm full code implementing aspects paper made available acceptance. notation follows encoder_multi_layer decoder_multi_layer layer multi_layer up_scale_layer down_scale_layer uslp up_scale_linear_projection dslp down_scale_linear_projection procedure u-resnet procedure encoder_multi_layer procedure decoder_multi_layer procedure multi_layer procedure down_scale_layer procedure down_scale_linear_projection procedure up_scale_layer procedure up_scale_linear_projection procedure layer", "year": 2017}