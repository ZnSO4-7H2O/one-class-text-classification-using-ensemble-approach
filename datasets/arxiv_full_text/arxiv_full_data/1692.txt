{"title": "Parsimonious Topic Models with Salient Word Discovery", "tag": ["cs.LG", "cs.CL", "cs.IR", "stat.ML", "I.7.0; I.5.3; G.3; I.5.2"], "abstract": "We propose a parsimonious topic model for text corpora. In related models such as Latent Dirichlet Allocation (LDA), all words are modeled topic-specifically, even though many words occur with similar frequencies across different topics. Our modeling determines salient words for each topic, which have topic-specific probabilities, with the rest explained by a universal shared model. Further, in LDA all topics are in principle present in every document. By contrast our model gives sparse topic representation, determining the (small) subset of relevant topics for each document. We derive a Bayesian Information Criterion (BIC), balancing model complexity and goodness of fit. Here, interestingly, we identify an effective sample size and corresponding penalty specific to each parameter type in our model. We minimize BIC to jointly determine our entire model -- the topic-specific words, document-specific topics, all model parameter values, {\\it and} the total number of topics -- in a wholly unsupervised fashion. Results on three text corpora and an image dataset show that our model achieves higher test set likelihood and better agreement with ground-truth class labels, compared to LDA and to a model designed to incorporate sparsity.", "text": "abstract—we propose parsimonious topic model text corpora. related models latent dirichlet allocation words modeled topic-speciﬁcally even though many words occur similar frequencies across different topics. modeling determines salient words topic topic-speciﬁc probabilities rest explained universal shared model. further topics principle present every document. contrast model gives sparse topic representation determining subset relevant topics document. derive bayesian information criterion balancing model complexity goodness here interestingly identify effective sample size corresponding penalty speciﬁc parameter type model. minimize jointly determine entire model topic-speciﬁc words document-speciﬁc topics model parameter values total number topics wholly unsupervised fashion. results three text corpora image dataset show model achieves higher test likelihood better agreement ground-truth class labels compared model designed incorporate sparsity. free parameters require model data. models proposed e.g. model-based clustering reducing number free parameters needed specify cluster’s shape clustering text documents great reduction number free parameters also achieved tying parameter values across many clusters e.g. parsimonious models less prone overﬁtting parameter-rich ones various modeling problems document modeling good target here bag-of-words model introduces parameter word topic word given lexicon. amount tens thousands word probability parameters every topic model. expected economization model representation possible. topics models latent dirichlet allocation topic probability mass function deﬁned given vocabulary. extension mixtures unigrams latter assuming document generated single topic randomly selected based corpus-level topic proportions contrast word given document independently modeled mixture topics. prone over-ﬁtting. moreover intuitively many words context-speciﬁc i.e. used roughly frequency different topics. thus words reasonably modeled using universal shared model across topics. approach exploit parsimony. similarly every topic principle present every document non-zero proportion. seems implausible since document expected main theme covered modest subset related topics. allowing topics nonzero proportions every document complicates model’s representation data. contrast proposed method identiﬁes sparse topics present document. model fact sparse abovementioned senses. first given topic word topic-speciﬁc feature probability parameter shared feature using parameter shared topics. second document sparse occurring topics non-zero proportions identiﬁed. sparsities understood fig. probability topic generating word proportion topic document example topics present document ﬁrst word’s probability generation shared topics. model learning consists main parts alternately iteratively applied structure learning parameter learning. assuming number topics known given current parameter estimates structure learning determines topic-speciﬁc words topic topics non-zero proportions document. likewise given ﬁxed structure parameter learning document-speciﬁc topic proportions topic-speciﬁc word probabilities re-estimated. priors topic distributions word probabilities control skewness word topic frequency distributions. asymmetric priors shown prevent common words dominating topics also help achieve sparser topic presence documents. however similar approach parsimonious. topics nonzero proportion every document words modeled topic-speciﬁc fashion. introduced spike slab model control sparsity word probabilities. unlike approach shared distribution. moreover provide subset relevant topics document. similar approach based indian buffet process used address sparsity topic proportions non-parametric topic model. global background models used information retrieval models probability word every topic mixture background model topic-speciﬁc word probabilities. similar idea used words well-modeled background model small topic-speciﬁc probabilities. mixing proportions models hyperparameters estimated cross-validation. proposed combination background general documentspeciﬁc topics improve information retrieval. authors argued over-generalizes thus effective matching queries contain highlevel semantics keywords. introduced huge free parameters adding document-speciﬁc topic every document. models similar word every topic free parameter. contrast model shared model heavily used topic possessing relatively topicspeciﬁc words. also unlike approaches model sparse topic proportions. presented sparse topical coding nonprobabilistic topic model gives parsimony topic-proportions models words topic-speciﬁcally. moreover method three hyper-parameters must determined cross-validation. approach also viewed standpoint unsupervised feature selection. topic select salient features unsupervised fashion modeling rest using universal shared model. unsupervised feature selection shared feature representations considered prior works. used minimum message length criterion salient features mixture model. features tied across components; i.e. feature either salient shared components. related bayesian framework presented following sections introduce objective function computationally efﬁcient procedure jointly performing structure parameter learning minimize objective. objective function derived model posterior probability criterion bayesian information criterion widely accepted criterion model comparison negative logarithm bayes marginal likelihood composed main terms data log-likelihood model complexity cost. thus achieves balance goodness model complexity. however deﬁciencies limit applicability. parameter penalty parameters model expect different parameter types general contribute unequally model complexity goodness moreover laplace’s approximation used deriving valid ratio sample size feature space dimensionality large however document modeling ratio small feature dimensionality dictionary size. paper derive approximation model posterior improves na¨ıve form aspects proposed form differentiated cost terms based different effective sample sizes different parameter types model. making shared feature representation essentially increases sample size feature dimension ratio thus giving better approximation model posterior. framework also gives wholly unsupervised fashion direct estimate number topics present corpus. number topics hyper-parameter topic models usually determined based validation performance secondary task classiﬁcation. here solely using document corpus select model order highest approximate model posterior thus model entirety structure parameter values number topics jointly chosen minimize bic. common speciﬁc words sparsity topic proportions word probabilities well estimation number topics subject previous studies. introduced asymmetric dirichlet ment. exact inference intractable. thus approximate algorithms variational inference markov chain monte carlo used. brieﬂy review mean-ﬁeld variational inference. approximate inference method achieved obtaining lower bound log-likelihood. family variational distributions deﬁned changing statistical dependencies original model here hidden variables zldd} dirichlet distribution parameter also multinomial distribution topics variational parameters values document determined minimizing kullback-leibler divergence posterior distribution hidden variables gives lower bound single-document log-likelihood. update equations variational parameters speciﬁc feature space; i.e. feature salient components represented shared model others. used minimum description length standard mixture unigrams modeling documents. performed unsupervised feature selection minimizing message length data considering mixtures generalized dirichlet distributions. model optimized bayesian framework variational inference extend concept shared feature space standard mixtures general topic models allowing presence multiple topics documents. achieve sparsity topic proportions topic-speciﬁc words. prior works best achieve sparsity senses. unlike works model allows subset salient words topic-speciﬁc. follows premise words common frequency occurrence subset topics. example word component different meanings statistics machine learning topics could higher frequencies occurrence specialized topics. derive novel objective function used learning model. unlike na¨ıve form satisfyingly derived objective distinct penalty terms different parameter types model interpretable effective sample size parameter types. rest paper organized follows section introduces notation throughout. section brieﬂy review introduce criterion measure sparsity topics lda. next section present parsimonious model. section derives objective function. then section develop joint structure parameter learning algorithm model locally minimizes objective. experimental results three text corpora image dataset reported section concluding remarks section bayesian setting natural alternative approach approximates bayesian model posterior. moreover unlike fully bayesian approach approach gives computationally tractable learning parsimonious models thus avoiding overﬁtting. stochastic data generation document corpus generated model follows. note {vjd} indicate topics present document topic probability generating word document non-zero likewise topic possesses topic-speciﬁc probability generating word otherwise probability aforementioned switches together number topics specify model structure given ﬁxed structure full complement model parameters given {{βn}{βjn}{αjd}}. together structure parameters constitute model. here double product appears documents words generated i.i.d. also emphasize clarity ujwid topic binary switch variable word i-th word document next step optimizing variational parameters lower bound optimized respect parameters model word probabilities minimization achieved closed form updates sparsity topic proportions controlled corpuslevel parameter optimized along parameters model. values smaller lead sparser topic proportions given document. order compare sparsity model estimate actual number topics present document. variational parameter essentially probability word document generated topic estimate number topics present hard-assign word document topic maximum doing determine topics used model least word given document. measure topic sparsity lda. next section develop topic model fundamentally differs possessing types sparsities; treating parameters deterministic unknowns rather random variables i.e. take maximum likelihood rather bayesian learning approach thus method require variational inference; requiring hyperparameters. sole hyperparameter. however approach automatically estimated along rest model. parsimonious topic model section develop parsimonious model associated parameter estimation. clarity’s sake section focus parameter estimation given model structure assumed known i.e. subset topics present document moreover seen penalty term fact dependence model parameters thus minimizing respect equivalent maximizing log-likelihood respect accordingly framework parameters chosen maximize log-likelihood parameters. alternatively treat shared model universal model estimated once initialization held ﬁxed performing experiments show latter approach quite effective. thus work shared model estimated once global frequency counts model ﬁxed minimize respect alternately given ﬁxed structure minimizing respect achieved described section using alternating minimization locally bic-optimal models learned bic-minimizing model order chosen. posterior mode using taylor series expansion. note ﬁrst taylor term constant respect variables integration term corresponding ﬁrst derivatives zero. thus exponentiating taylor’s series expansion second order plugging obtain approximation integral m-dimensional binary random vector single element equal elements equal zero. here non-zero element topic origin word wid. treat hidden data within framework iteration maximizes lower bound data log-likelihood guaranteed monotonic increase incomplete data log-likelihood locally optimal convergence iteration consists e-step computing expected value hidden data given observed data current parameter estimates used determine expected complete data loglikelihood; m-step update model parameters maximizing expected complete data log-likelihood. likelihood model choice hidden data steps speciﬁed follows e-step given model structure current estimate parameters compute adding normalization constraints expected value complete data log-likelihood measured respect construct lagrangian current parameter estimate expression reminiscent penalty term na¨ıve form where parameter pays log. here identify different penalties different parameter types model argument effective sample size consistent derivation. effective size parameter parameter. however also sanity-check interpretation parameter re-estimation equations. note particular parameters indeed re-estimated based word samples parameters reestimated based total word samples documents topic present. thus indeed derivation leads generalization standard cost wherein penalty parameter given type general principle deﬁning prior distributions invoke uninformativeness parameterize prior distribution switches function topic-speciﬁc words across topics assume conﬁgurations switches integrand exponentiated second order taylor series term scaled normal distribution mean covariance which integrated evaluates number parameters equal number active topics document number topic-speciﬁc words topic also since shared model parameters estimated universal fashion once ﬁxed descriptive complexity yielding complete data bic; taking expected value complete data bic. term complete data function hidden variables complete data log-likelihood. thus expected value complete data formed simply replacing incomplete data loglikelihood term expected complete data log-likelihood term e-step following section expected values hidden variables given based discussion above expected complete data formed replacing incomplete data loglikelihood term expected complete data log-likelihood term estimation given ﬁxed structure since model complexity terms dependence minimization expected complete data respect given ﬁxed structure equivalent maximizing expected complete data log-likelihood closed form updates described section updating switches given ﬁxed parameters done iterative loop switches cyclically visited parameters switches ﬁxed. change current switch reduces change accepted. process repeated switches decrease occurs predeﬁned maximum number cycles reached. update switches separately ﬁrst performing cycles switches convergence switches convergence. back e-step. computational efﬁciency optimization respect expected complete data bic. however directly minimize since computational advantage minimizing respect expected case. since minimizing expected complete data ensures descent update steps descend objective determining minimize objective supposing moment ﬁxed md+n different possible switch conﬁgurations global minimization search space formidable task. however using generalized expectation maximization algorithm break problem series simple update steps decreasing overall algorithm thus guaranteed converge local minimum. important initialize model sensibly avoid poor local minima. here simple pragmatic initialization process initialize topic randomly select dinit documents. words occur documents initially chosen topic-speciﬁc probabilities initialized frequency counts; based initial model maximum likelihood decision rule hard-assign document single topic; using reﬁned documents topic re-estimate topic-speciﬁc word probabilities frequency counts. ﬁxed computational complexity parameter learning number topics present document length document respectively. since topics potentially present document computational complexity higher model. however structure learning model imposes complexity. updates word switches involve iterative loop words topics. trial-ﬂip single switch requires scalar computation. thus complexity part structure learning experimentally heaviest part algorithm updating switches step trial-estimate topic proportions evaluate incomplete data log-likelihood current document consideration. need computations order trial-update switch. thus total complexity part including loop switches overall computational complexity model higher lda. complexity issue corpus large number active topics across documents. however also investigated complexity experimentally recorded execution times found method require comparable execution. compare estimated models different orders choose minimum bic. different strategies conceivable learning models range orders. sensible approach learn model order initializing models orders. sweep range model orders either bottom-up top-down fashion. top-down approach initialize model speciﬁed ceiling number topics mmax reduce number topics predeﬁned step. remove least plausible topics existing model apply learning algorithm minimize reduced model order using current computing change log-likelihood need corresponding word probability values switch. introducing variables allows compute efﬁciently effect changing expected value complete data log-likelihood update switches sequentially visiting topic every document obviously document constrained least active topic. moreover every topic constrained used least document. note since topic proportions document change ﬂipping switch unlike computing computational beneﬁt using expected value complete data log-likelihood. evaluating incomplete data loglikelihood updates. probable topic-speciﬁc words topic number documents corpus containing words similarly represents number documents word here percent topic-speciﬁc words topic model compute coherence. number words compute coherence topics. experiments report average coherence topics. results reported averaged based different initializations. determined hyperparameters validation approach working model highest number topics kept ﬁxed model orders. makes complexity manageable also reasonable observe best accuracies anyway achieved highest model order. validation created randomly choosing documents training set. inference test documents model allow topics active document optimize topic proportions using given ﬁxed number topics. taken approach rather using transductive inference approach. ohsumed consists documents assigned multiple labels mesh diseases categories. documents average labels. dataset randomly divided training test documents. unique words corpus applying standard stopword removal. four methods models initialized topics step least massive topics removed. fig. shows curve training held-out log-likelihood model compared ldabackground. minimum curve average ﬁgure shows achieves higher log-likelihood training models topics; controlling likelihood-complexity trade-off model achieves higher log-likelihood held-out orders. also used class labels provided dataset evaluate class label consistency measure. ﬁrst associated topic multinomial distribution class labels. learned label distributions model parameters initialization. here least plausible simply remove topics smallest aggregate mass across document corpus. applied repeatedly minimum number topics reached. experimentally method found superior alternative bottom-up approach. thus applied top-down method experiments. section report results model ohsumed reuters- -newsgroup corpora well subset labelme image dataset. dataset compared respect model sparsity class label consistency measure. also compared extension includes additional topic whose probabilities ﬁxed global frequency estimates. furthermore compared respect class label consistency. implementation model available https//github.com/hsoleimani/ptm. suggests comparing log-likelihoods computed different topic models unfair since compute different approximation loglikelihood. here method described compare model ﬁtness held-out test set. approach divide document test sets observed held-out parts keeping unique words parts disjoint. observed part test compute expected topic proportions compute log-likelihood words held-out note variational parameters normalized model directly topic proportions ujnβjn model. however simply equal corresponding estimated word probability. topic models also compared respect quality extracted topics. computational linguistics standpoint topics expected exhibit coherent semantic theme rather covering loosely related concepts. paper criterion proposed evaluate coherence learned topics. measure shown agreement experts’ coherence judgments topic coherence table sparsity measure topic-speciﬁc words model; average number topic-speciﬁc words topic; nunique average number unique topicspeciﬁc words topics; average number occurring topics documents considered topics dominantly contributed generating least word document occurring topics. also report average number topic-speciﬁc words topic total number topic-speciﬁc words topics model. average number occurring topics model compared much closer average number labels document corpus suggests topics better resemble ground-truth classes stc. also shared model widely used model relatively small number words speciﬁc topic. average number unique topic-speciﬁc words topics much larger indicates great overlap topic-speciﬁc words across topics. fact topicspeciﬁc words average salient topics modeled shared representation topics. section report results comparison -newsgroups. dataset consists respectively documents training test sets designated newsgroups. document labeled single newsgroup. standard stopword removal stemming unique words corpus. label consistency criteria measure precision recall test set. precision number true discovered labels divided total number groundtruth labels. recall number correctly classiﬁed labels divided total number labels assigned documents classiﬁer. measure criteria different threshold values report area precision/recall curve ﬁnal measure performance. ldabackground used normalized dirichlet variational parameters topic proportion document similarly used stc’s normalized document codes unsupervised fig. shows model ldabackground stc. adding background model improves class label consistency. however best performance model better methods’ entire range model orders. also highest model occurs near minimum curve. performance model relative ldabackground. minimum curve average although achieves higher likelihood training data parsimonious model better performance heldset models less topics. learning label distributions topics done similar procedure explained ohsumed corpus. since documents corpus single labels compute probability class labels label highest probability. class consistency test reported fig. bic-chosen model order achieves good class consistency. also model achieves better consistency compared methods models less topics. coherence discovered topics ldabackground model plotted fig. curves similar. sparsity measures model dataset compared ldabackground tables small fraction unique words used topic-speciﬁc words model. although adding background model reduces average number occurring topics model still smallest among methods. note documents corpus single-labeled. average unique topic-speciﬁc words dataset context-speciﬁc topics shared others. words highest probabilities topics discovered model reported table model separately report topic-speciﬁc shared words. note high probability words topics shared words model. compared model ldabackground subset reuters dataset- consisting documents classes. respectively documents training test sets labeled single class. documents include unique words applying standard stopword removal stemming. initialized models topics removed topics elimination step. average model well training held-out data loglikelihood model ldabackground shown fig. minimum curve average fig. shows model achieves higher log-likelihood held-out compared models orders. moreover spite fact held-out likelihood model increases modestly orders table words sample topics extracted -newsgroup reuters corpora method lda. speciﬁc shared denote respectively topic-speciﬁc shared words topics model. -newsgroup topics medic diseas articl write patient pitt bank health gordon doctor write articl time work year good even problem thing doctor patient diseas medic pain peopl pitt treatment gordon bank christian jesu israel church peopl bibl christ come write articl time good even come sinc gener christian exist peopl atheist question thing mean reason drive problem disk window system work hard driver printer write work good system want question post program printer font print window problem driver deskjet laser articl reuters topics gold mine ounc dixon miner dome south year feet compani oper corp unit includ expect report plan gold mine ounc year feet reserv opec price energi barrel petroleum crude stock last march industri expect quarter month report opec price saudi barrel crude product dividend april record split stock march declar payabl share corp offer unit quarter plan exchang invest acquir propos dividend stock share split april record compani common declar sharehold model ldabackground dataset shown fig. model best performance across model orders. also minimum curve consistent high class label consistency. tables report sparsity measures model ldabackground stc. topicspeciﬁc words model small subset dictionary size. also model achieves sparser topic presence compared ldabackground stc. table shows words three sample topics extracted model lda. model separately report topic-speciﬁc shared words. section report results comparison subset labelme image dataset. dataset downloaded consists images classes. unique codewords extracted images using method described first -dimensional sift vector descriptors genimage assigned nearest cluster clusters dataset performing k-means clustering merging close clusters pruning clusters small number members. cluster index since number codewords dataset small relative text corpora approximation used valid anymore. accordingly considered conﬁguration switches equally likely used exact form thus cost topic-speciﬁc words case. initialized models topics reduced number topics step. fig. shows performance model compared ldabackground. minimum average held-out log-likelihood higher model compared ldabackground across model orders. also performed single-label classiﬁcation similar procedure described single-labeled text corpora. fig. shows class label consistency model compared ldabackground stc. model achieves better label consistency ldabackground stc. seems implausible model provide reasonable sparsity topic proportions. however unlike model exhibits parsimony word probabilities average words modeled topic-speciﬁc fashion. discussion four data sets models achieve higher held-out likelihoods nearly model orders except high orders -newsgroups. even stronger model evaluated single order achieves higher held-out likelihood models evaluated nearly orders. respect class label consistency bic-minimum model gives accuracy better evaluated nearly orders. however seen data sets lda’s class consistency improves increasing order even exceeding model’s -newsgroups general model much greater topic sparsity modest increases sparsity achieved using background model. moreover model’s average number topics document good agreement average number class labels document. sense learned topics better correspond individual classes lda’s. curves figs. possible achieve greater class label consistency model data sets using large number topics. however must recognize topic models perform unsupervised clustering aiming capture salient content provide human-interpretable data groupings. choosing huge number topics improve lda’s class label consistency solutions would defy human interpretation. bic-minimizing solutions interpretable topic sparsity word sparsity noted topics better corresponding ground-truth classes. moreover parsimony interpretability achieved without giving performance execution time report execution times complete model learning process mmax mmin model four datasets table models machines quad-core processors. datasets small modest document length execution time model smaller comparable lda. ohsumed corpus larger typical document lengths running time model three times lda. also smallest overall running time across datasets. performed separately document. improve scalability larger data sets. nevertheless standard implementation model learning single model topics research awards corpus consists abstracts awards training time model respectively hours data set. proposed parsimonious model estimating topics salient words given database unstructured documents. unlike model gives sparse representation topic presence documents word probabilities different topics. derived objective function speciﬁc model complexity penalization consistent effective sample size parameter type. minimize objective jointly determine model including total number topics. experiments show model outperforms sparsitybased topic model respect several clustering performance measures including test log-likelihood agreement ground-truth class labels. graham miller unsupervised learning parsimonious mixtures large spaces integrated feature component selection ieee trans. signal process. vol. boutemedjet bouguila ziou hybrid feature extraction selection approach high-dimensional non-gaussian data clustering. ieee trans. pattern anal. mach. intell. vol. aug. bouguila ziou unsupervised hybrid feature extraction selection high-dimensional non-gaussian data clustering variational inference ieee trans. knowl. data eng. vol. ghosh delampady samanta introduction bayesian analysis theory methods. springer york graham knuth patashnik concrete mathematics foundation computer science boston addison-wesley longman publishing jan. fei-fei perona bayesian hierarchical model learning natural scene categories ieee computer society conf. computer vision pattern recognition vol. wang blei f.-f. simultaneous image classiﬁcation annotation ieee conf. computer vision pattern recognition jun.", "year": 2014}