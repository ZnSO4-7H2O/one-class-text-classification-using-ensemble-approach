{"title": "meProp: Sparsified Back Propagation for Accelerated Deep Learning with  Reduced Overfitting", "tag": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "abstract": "We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-$k$ elements (in terms of magnitude) are kept. As a result, only $k$ rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction ($k$ divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1--4\\% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given. The code is available at https://github.com/jklj077/meProp", "text": "propose simple effective technique neural network learning. forward propagation computed usual. back propagation small subset full gradient computed update model parameters. gradient vectors sparsiﬁed top-k elements kept. result rows columns weight matrix modiﬁed leading linear reduction computational cost. surprisingly experimental results demonstrate update weights back propagation pass. result larger number training iterations. interestingly accuracy resulting models actually improved rather degraded detailed analysis given. code available https//github.com/jklj/meprop. neural network learning typically slow back propagation usually dominates computational cost during learning process. back propagation entails high computational cost needs compute full gradients update model parameters learning step. uncommon neural network massive number model parameters. study propose minimal effort back propagation method call meprop neural network learning. idea compute small critical portion gradient information update school electronics engineering computer science peking university china laboratory computational linguistics peking university china. correspondence <xusunpku.edu.cn>. corresponding minimal portion parameters learning step. leads sparsiﬁed gradients highly relevant parameters updated parameters stay untouched. sparsiﬁed back propagation leads linear reduction computational cost. realize approach need answer questions. ﬁrst question highly relevant subset parameters current sample stochastic learning. propose top-k search method important parameters. interestingly experimental results demonstrate update weights back propagation pass. result larger number training iterations. proposed method general-purpose independent speciﬁc models speciﬁc optimizers second question whether minimal effort back propagation strategy hurt accuracy trained models. show strategy degrade accuracy trained model even small portion parameters updated. interestingly experimental results reveal strategy actually improves model accuracy cases. based experiments probably minimal effort update modify weakly relevant parameters update makes overﬁtting less likely similar dropout effect. propose sparsiﬁed back propagation technique neural network learning small subset full gradient computed update model parameters. experimental results demonstrate update weights back propagation pass. result larger number training iterations. surprisingly experimental results reveal accuracy resulting models actually improved rather degraded. demonstrate effect conducting experiments different deep learning models various optimization methods diverse tasks top-k values denote indices vector approximate gradient parameter matrix input vector figure illustration meprop single computation unit neural models. original back propagation uses full gradient output vectors compute gradient parameters. proposed method selects top-k values gradient output vector backpropagates loss corresponding subset total model parameters. propose simple effective technique neural network learning. forward propagation computed usual. back propagation small subset full gradient computed update model parameters. gradient vectors quantized top-k components terms magnitude kept. ﬁrst present proposed method describe implementation details. forward propagation neural network models including feedforward neural networks lstm consists linear transformations non-linear transformations. simplicity take computation unit linear transformation non-linear transformation example figure shows illustration computational meprop. forward propagation traditional forward propagation computes output vector matrix multiplication operation input tensors. original back propagation computes full gradient input vector weight matrix. meprop back propagation computes approximate gradient keeping top-k values backward ﬂowed gradient masking remaining values feedforward model transition-based dependency parsing mnist image recognition. optimizers automatically adaptive learning including adam adarates grad implementation make modiﬁcation optimizers although many zero elements gradients. experiments conducted framework coded own. framework builds dynamic computation graph model sample making suitable data variable lengths. typical training procedure contains four parts building computation graph forward propagation back propagation parameter update. also implementation based pytorch framework based experiments. proposed method aims reduce complexity back propagation reducing elements computationally intensive operations. preliminary observations matrix-matrix matrix-vector multiplication consumed time back propagation. implementation apply meprop back propagation output multiplication inputs. element-wise operations original back propagation procedure kept operations already fast enough compared matrix-matrix matrix-vector multiplication operations. multiple hidden layers top-k sparsiﬁcation needs applied every hidden layer sparsiﬁed gradient dense layer another. meprop gradients sparsiﬁed top-k operation output every hidden layer. apply meprop hidden layers using top-k usually output layer could different hidden layers output layer typically different dimension compared hidden layers. example tags mnist task dimension output layer hidden dimension thus best output layer could different hidden layers. riedmiller braun proposed direct adaptive method fast learning performs local adaptation weight update according behavior error function. tollenaere also proposed adaptive acceleration strategy back propagation. dropout proposed improve training speed reduce risk overﬁtting. sparse coding class unsupervised methods learning sets over-complete bases represent data efﬁciently ranzato proposed sparse autoencoder model learning sparse over-complete features. proposed method quite different compared prior studies back propagation dropout sparse coding. sampled-output-loss methods limited softmax layer based random sampling method limitations. sparsely-gated mixture-ofexperts sparsiﬁes mixtureof-experts gated layer limited speciﬁc settable results based lstm/mlp models adagrad/adam optimizers. time means averaged time iteration. iter means number iterations reach optimal score development data. model iteration used obtain test score. table overall forward propagation time overall back propagation time. time means averaged time iteration. means forward propagation. means back propagation. time means overall training time ting mixture-of-experts method limitations. also prior studies focusing reducing communication cost distributed systems quantizing value gradient -bit ﬂoat bit. settings also different ours. part-of-speech tagging standard benchmark dataset prior work derived penn treebank corpus sections wall street journal training sections testing evaluation metric per-word accuracy. popular model task lstm model transition-based dependency parsing following prior work english penn treebank evaluation. follow standard split corpus sections training section development section ﬁnal test evaluation metric unlabeled attachment score implement parser using following chen manning used baseline. mnist image recognition mnist handwritten digit dataset evaluation. mnist consists pixel training images additional test examples. image contains single numerical digit select ﬁrst images training images development rest training set. evaluation metric per-image accuracy. model baseline. dimension output dimension mnist input dimension output dimension discussed section optimal top-k output layer could different hidden layers because dimensions could different. parsing mnist using output hidden layers works well simply another task pos-tag output layer different hidden layers. simplicity apply meprop output layer pos-tag task computational cost output layer almost negligible compared layers. hyper-parameters tuned based development data. adam optimization method default hyper-parameters work well development sets learning rate follows adagrad learner learning rate pos-tag parsing mnist respectively experiments conducted computer intel xeon .ghz cpu. experiments conducted nvidia geforce experiment lstm based hidden layer based hidden layers conduct experiments different optimization methods including adagrad adam. since meprop applied linear transformations report linear transformation related backprop time backprop time. include nonlinear activations usually less computational cost. total time back propagation including non-linear activations reported overall backprop time. based development prior work mini-batch size pos-tag parsing mnist respectively. using transition examples parsing follows chen manning table shows results based different models different optimization methods. table meprop means applying meprop corresponding baseline model means hidden layer dimension means meprop uses top- elements back propagation. note that fair comparisons experiments ﬁrst conducted development data test data observable. then optimal number iterations decided based optimal score development data model iteration used upon test data obtain test scores. applying meprop substantially speed back propagation. provides linear reduction computational cost. surprisingly results demonstrate update weights back propagation pass. result larger number training iterations. surprisingly accuracy resulting models actually improved rather decreased. main reason could minimal effort update modify weakly relevant parameters makes overﬁtting less likely similar dropout effect. table shows overall forward propagation time overall back propagation time training time summing forward backward propagation time. back propagation major computational cost training lstm/mlp. results consistent among adagrad adam. results demonstrate meprop independent speciﬁc optimization methods. simplicity following experiments optimizer based adam. figure vary top-k meprop compare test accuracy different ratios meprop backprop. example means backprop ratio /=%. optimizer adam. meprop achieves consistently better accuracy baseline. best test accuracy meprop actually better reported table interesting check role top-k elements. figure shows results top-k meprop random meprop. random meprop means random elements selected back propagation. top-k version works better random version. suggests top-k elements contain important information gradients. still question top-k meprop work well simply original model require dimension hidden layers? example meprop works simply lstm works well hidden dimension need hidden dimension examine this perform experiments using hidden dimension results shown table however results small hidden dimensions much worse meprop. addition figure shows detailed curves varying value ﬁgure different gives different backprop ratio meprop different hidden dimension ratio lstm/mlp. answer question negative meprop rely redundant hidden layer elements. table shows results. meprop achieve improvement dropout. particular meprop improvement parsing. results suggest type overﬁtting meprop reduces probably different dropout. thus model able take advantage meprop dropout reduce overﬁtting. another question whether meprop relies shallow models hidden layers. answer question also perform experiments hidden layers hidden layers hidden layers. setting dropout rate works well cases different numbers layers. simplicity comparison dropout rate experiment. table shows adding number hidden layers hurt performance meprop. implementing meprop simplest solution treat entire mini-batch training example top-k operation based averaged values examples mini-batch. sparse matrix mini-batch consistent sparse patterns among examples consistent sparse matrix transformed small dense matrix removdespite simplicity table shows good performance implementation based mini-batch size also speedup less signiﬁcant hidden dimension low. reason gpu’s computational power fully consumed baseline normal back propagation already fast enough making hard meprop achieve substantial speedup. example supposing ﬁnish operations cycle could speed difference method method operations. indeed almost speed even forward propagation theoretically difference. forward propagation time respectively. provides evidence hypothesis fully consumed small hidden dimensions. thus speedup test meaningful heavy models baseline least fully consume gpu’s computational power. check this test speedup synthetic data matrix multiplication larger hidden dimension. indeed table shows meprop achieves much higher speed traditional backprop large hidden dimension. furthermore test speedup large hidden dimension table shows meprop also substantial speedup mnist large hidden dimension. experiment speedup based overall backprop time results demonstrate meprop achieve finally potentially implementation choices meprop gpu. example another natural solution sparse matrix represent sparsiﬁed gradient output mini-batch. then sparse matrix multiplication library used accelerate computation. could interesting direction future work. transition-based dependency parsing task existing approaches typically achieve score popular transition-based parsers maltparser uas. chen manning achieves using neural networks. method achieves mnist based approaches achieve accuracy often around method achieves help convolutional layers techniques accuracy improved method also improved additional techniques which however focus paper. back propagation deep learning tries modify parameters stochastic update inefﬁcient even lead overﬁtting unnecessary modiﬁcation many weakly relevant parameters. propose minimal effort back propagation method compute small critical portion gradient modify corresponding small portion parameters update. leads sparsiﬁed gradients modify highly relevant parameters given training sample. proposed meprop independent optimization method. experiments show meprop reduce computational cost back propagation orders magnitude updating parameters improve model accuracy cases. authors would like thank anonymous reviewers insightful comments suggestions paper. work supported part national natural science foundation china national high technology research development program china okawa research grant", "year": 2017}