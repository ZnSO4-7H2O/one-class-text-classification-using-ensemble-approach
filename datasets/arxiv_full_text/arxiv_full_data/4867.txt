{"title": "Possibilistic Networks: Parameters Learning from Imprecise Data and  Evaluation strategy", "tag": ["cs.AI", "cs.LG"], "abstract": "There has been an ever-increasing interest in multidisciplinary research on representing and reasoning with imperfect data. Possibilistic networks present one of the powerful frameworks of interest for representing uncertain and imprecise information. This paper covers the problem of their parameters learning from imprecise datasets, i.e., containing multi-valued data. We propose in the rst part of this paper a possibilistic networks sampling process. In the second part, we propose a likelihood function which explores the link between random sets theory and possibility theory. This function is then deployed to parametrize possibilistic networks.", "text": "ever-increasing interest multi-disciplinary research representing reasoning imperfect data. possibilistic networks present powerful frameworks interest representing uncertain imprecise information. paper covers problem parameters learning imprecise datasets i.e. containing multi-valued data. propose ﬁrst part paper possibilistic networks sampling process. second part propose likelihood function explores link random sets theory possibility theory. function deployed parametrize possibilistic networks. possibilistic networks graphical representations independence relationships variables described uncertain imprecise information. despite multitude research endeavors devoted applying possibilistic networks real domains propagating information learning data remains real challenge. works address problem existing ones direct adaptations bayesian networks learning methods without awareness speciﬁcities possibilistic framework made theoretically unsound. main limitation existing works learn separately parameters i.e. possibility distributions coding variables uncertainty structure i.e. graph possibilistic network. moreover existing methods suﬀer lack accurate standard validation procedure. working parameters possibilistic framework highlights several diﬃculties dealing learning task particular handle uncertain imprecise data. fact learning leads commonly additive assessment possibility theory deﬁnition maxitive i.e. possibility disjunction events maximum possibilities event disjunction. thereby want learn parameters data possibilistic framework steps primordial ﬁrst focuses counting occurrence observations dataset estimate non-normalized distributions. second aims approximate latter possibility distributions. paper rigorously addresses problem ﬁrst proposition possibilistic networks sampling method used evaluate learning algorithms control imprecision degree generated datasets. ﬁnal part paper propose likelihood function exploring link random sets theory possibility theory deployed learn possibilistic networks parameters. paper organized follows section gives brief introduction possibility theory presents possibilistic networks learning data. section proposes possibilistic networks sampling algorithms. section deﬁnes possibilistic likelihood function proposes possibilistic networks parameters learning approach. possibilistic networks represent possibilistic counterpart bayesian networks possibilistic framework coined zadeh developed dubois prade section ﬁrst presents basic notations used throughout paper introduces possibility theory. then deﬁnes possibilistic networks discusses existing learning methods. variables denotes domain denotes instance i.e. corresponds state agents knowledge encoded possibility distribution corresponding mapping universe discourse unit interval state means realization totally possible means impossible state. generally assumed least state totally possible said normalized. extreme cases knowledge presented complete knowledge i.e. ∃xik s.t. ∀xij s.t. total ignorance i.e. ∀xik deﬁnition possibility distribution could generalized variables deﬁned universe discourse encoded corresponds mapping unit interval called interpretation event denoted tuple given possibility distribution deﬁne subset dual measures possibility measure necessity xik∈a measure assesses level consistent knowledge represented whereas evaluates level impossible. particularity possibilistic scale interpreted ways ordinal manner means possibility degrees reﬂect speciﬁc order possible values. numerical meaning possibility degrees make sense ranking scale. interpretations induce deﬁnitions possibilistic conditioning consists reviewing possibility distribution certain information interpretation product-based conditioning deﬁned follows view possibility theory consider possibility distribution counter function random pertaining random random variable takes values subsets formally ﬁnite domain. basic probability random said consistent least element contained focal sets possibility distribution induced consistent random thereby normalized. exploring link possibility theory random sets theory extensively studied particular learning tasks cite instance variable sampling corresponds generation dataset representative possibility distribution. numerical interpretation approaches proposed sample variable. methods based α-cut notion α-cut {xik s.t. randomly generated method proposed guyonnet focuses generation imprecise data returning values α-cut variable chanas nowakowski proposed another method dedicated generation precise data returning single value uniformly chosen α-cut. possibilistic networks possibilistic counterpart bayesian networks sharing graphical component i.e. directed acyclic graph encodes independence relations variable conditionally independent non-descendent given parents. numerical component substitutes probabilistic framework possibilistic assigning conditional possibility distribution node context parents i.e. deﬁnitions possibilistic conditioning lead naturally diﬀerent ways deﬁne possibilistic networks product-based possibilistic networks based product-based conditioning expressed equation models theoretically algorithmically close bayesian networks. fact models share graphical component i.e. product operator computational process. case min-based possibilistic networks based min-based conditioning deﬁned equation represents diﬀerent semantic. cases possibilistic networks compact representation possibility distributions. precisely joint possibility distribution could computed possibilistic chain rule expressed follows attempts proposed learn possibilistic networks data. fact sangüesa proposed hybrid methods handling precise data ﬁrst learns trees second learns general structure dags. borgelt adapted methods initially proposed learn bayesian networks maximum weight spanning tree learn possibilistic networks imprecise data. attempts concern mainly structure learning ignore parameters learning problem. indeed sangüesa learn probability distributions transform possibility ones. borgelt methods estimate possibility distribution using possibilistic histograms i.e. based number occurrence diﬀerent values dataset. imprecise). number occurrences data precise sub-normalized estimation expressed probabilistic case evaluating bayesian networks learning algorithms ensured using following process select arbitrary bayesian network either synthetic gold standard generate dataset using forward sampling algorithm then recover initial network using learning algorithm compare initial network learned one. proposed transpose evaluation strategy proposed probabilistic case possibilistic one. follows mainly concentrate sampling possibilistic networks consists generating dataset representative joint distributions. sampling process constructs database observations instantiating variables w.r.t. possibility distributions. obviously variables easily processed w.r.t. topological order since ensures parents instantiated. instantiating parentless variable corresponds computing α-cut. instantiating conditioned variable corresponds computing also α-cut given sampled parents values. could directly applied conditional possibility distribution composed distribution depending number values sampled parents. instantiate conditioned variable s.t. compute α-cut computed follows main limitation sampling process generates particular case imprecise datasets i.e. obtained data relative variable conditionally consonant respect sampled values parents. fact sampling process based α-cut notion returns generally possible values observed ones. follows propose parametrize sampling process order generate generic imprecise data controlling imprecision degree generated datasets. fact propose extension sampling process proposed control imprecision degree generated data. controlling imprecision degree generated datasets create diﬀerent forms imprecision around possible value i.e. varying values dataset conserve possible combination given imprecision degree θimp variable α-cut presents values returned sampling process generate subsets pertaining α-cut including possible value assign probability equal θimp α-cut probability equal subset card)−card remaining subsets. finally sample formulation likelihood function made steps ﬁrst propose likelihood function deﬁned random sets. then propose approximation likelihood function leads deﬁnition possibilistic likelihood. deﬁnition parameters relative estimated dataset relative variable parents dij. number occurrences denoted nijk number times aijk appears nijk card express likelihood function follows note mass functions associated random sets probability distribution partial derivative follows principle partial derivative probabilistic likelihood function reaches maximum ˆmijk nijk note mass functions deﬁned singletons available data precise likelihood function deﬁned equation recovers probabilistic one. however opposite case computing likelihood functions computationally expensive. fact random relative variable deﬁned cardinality grows exponentially number values consequently propose investigate link possibility distributions mass functions presented equation deﬁne approximation random sets likelihood function i.e. possibilistic likelihood expressed possibility distributions deﬁned singletons. formally express possibilistic likelihood function follows deﬁnition parameters relative estimated dataset relative variable parents dij. number occurrences denoted nijk number times xijk appears nijk card express possibilistic likelihood follows probabilistic case learning bayesian networks parameters performed satisfying maximum likelihood principle evaluates level learned parameters dataset. know measure proposed possibilistic framework. absence learning possibilistic networks parameters method could justiﬁed fact learning usually viewed objective task i.e. based computing frequency observations possibility theory almost based subjective opinions. extent true especially deal measurement devices leading precise observations case probability theory remains adequate alternative. however measurement devices provide imprecise data want model data collected i.e. including imprecision physical measurement itself non-classical uncertainty theories stand best alternatives. case choose possibility theory since able oﬀer natural simple formal framework representing imprecise uncertain information. latter refers study maxitive minitive set-functions interpreted approximation upper lower frequentist probabilities presence imprecise observations link explored following. fact possibilistic likelihood deﬁnition learn possibilistic networks parameters. proposition given ﬁxed parameter πijk imprecision degree relative variable maximum possibilistic likelihood estimates paramk= πijk constant equal note corresponds imprecision degree relative variable could ﬁxed expert inferred dataset learn based variables description. obtain normalized possibility distributions divide every obtained distribution maximum. operation eliminate eﬀect imprecision degree objective learning task. however remains possible imprecision degree value variables studied domain. note obtaining possibility distributions equal zeros initial count instances nijk whose number added total number instances. paper propose evaluation strategy possibilistic networks parameters learning algorithms. sampling method proposed generate imprecise dataset possibilistic network. second part paper propose product-based possibilistic networks parameters learning algorithm based possibilistic likelihood function exploring link random sets theory possibility theory.", "year": 2016}