{"title": "A New Automatic Method to Adjust Parameters for Object Recognition", "tag": ["cs.CV", "cs.AI"], "abstract": "To recognize an object in an image, the user must apply a combination of operators, where each operator has a set of parameters. These parameters must be well adjusted in order to reach good results. Usually, this adjustment is made manually by the user. In this paper we propose a new method to automate the process of parameter adjustment for an object recognition task. Our method is based on reinforcement learning, we use two types of agents: User Agent that gives the necessary information and Parameter Agent that adjusts the parameters of each operator. Due to the nature of reinforcement learning the results do not depend only on the system characteristics but also on the user favorite choices.", "text": "abstract— recognize object image user must apply combination operators operator parameters. parameters must well adjusted order reach good results. usually adjustment made manually user. paper propose method automate process parameter adjustment object recognition task. method based reinforcement learning types agents user agent gives necessary information parameter agent adjusts parameters operator. nature reinforcement learning results depend system characteristics also user’s favorite choices. tools algorithms vision applications cause system parameters must properly adjusted. adjustment requires specific knowledge takes long time sometimes even done experimental process. accomplish segmentation task user must apply operators parameter adjust. lack general rule guides user choices fixation parameter values usually made intuitively. user proceeds trying manually possible cases finding desired result. usually majority vision tasks need apply combination several operators multitude parameters adjust. manual adjustment becomes tedious trustworthy. therefore automatic method adjust values parameter needed. quality results depends essentially operator chosen values assigned parameters. example ariane help users accomplish vision task proposing interactive interface values assigned parameters selected manually user. systems succeeded automate process parameter adjustment. b.nickolay proposed method automatically optimize parameters machine vision system surface inspection using specific evolutionary algorithms years later taylor proposed reinforcement learning framework uses connectionist systems function approximators handle problem determining optimal parameters computer vision application even case highly dimensional continuous parameter space recently farhang introduced method segmentation prostate reinforcement learning scheme. divided initial image sub-images works order reach good result. paper propose method adjust automatically parameters vision operators. method based reinforcement learning. agents user agent parameter agent gives necessary information system. gives combination applicable operators adjustable parameters operator values’ ranges parameter. uses reinforcement learning assign optimal values parameter order extract object interest image. nature terms interaction state action reward approach takes account system opportunities also user preferences learning mechanism suggest trustworthy solutions. overview reinforcement learning given section section outlines proposed approach introduces general framework parameter adjustment. section presents experimental results section concludes paper. reinforcement learning learning situations actions maximize numerical reward signal. learner told actions take forms machine learning instead must discover actions yield best reward trying them. challenges arise reinforcement learning kinds learning tradeoff exploration exploitation. obtain reward reinforcement learning agent must prefer actions tried past found effective producing reward. discover actions selected before. find trade-off immediate long-term returns. must explore unseen states well states maximize return choosing agent already knows. therefore balance exploration unseen states exploitation familiar states crucial. watkins developed q-learning wellestablished on-line learning algorithm practical method algorithm agent maintains numerical value state-action representing prediction worthiness taking action state. generally accomplish object recognition task user must apply sequentially operators operator parameter adjust. general rule guides user choices based usually intuition select values parameter. majority vision tasks apply multitude operators several parameters adjust. adjusting manually parameters basing experience intuition evident. it’s tedious work huge wasted time. paper propose automatic method find best values parameter recognition task. image ground truth introduced system. combination operators extract object interest proposed. operator parameters well adjusted. value given parameter gives different result. agent must find optimal values give best result. proceeds trial error finding uses reinforcement learning. actions states reward function must defined. table represents iterative policy evaluation updating state-action values reward value received taking action states next state learning rate discount factor policies taking action given states. boltzman policy estimates probability taking action state. policies qlearning εgreedy greedy. greedy policy actions explored whereas ε-greedy selects action highest q-value given state probability ones probability work ε-greedy policy used make balance exploration exploitation. reward defined according state-action pair goal find policy maximize discounted rewards received time. principal concerns cases optimal solutions cannot found approximated. online nature distinguishes techniques approximately solve markov decision processes dataset textured images containing object extract. object textured disc injected images. used images textured proposes combination operators glcm segment textures k-means classify them. operators parameters adjusted order executable. glcm texture always defined relation local window. size window affects result segmentation propose size window parameter adjust glcm. proposes range values seven values values glcm extracts fourteen texture features paper limit self four popular features angular second moment contrast correlation entropy. extracting textures classify using algorithm k-means. parameter adjust operator number possible clusters. take five values {…}. actions states reward defined according experience given below. proposes operators glcm k-means. glcm size sliding window parameter adjust. take seven values elementary action glcm values. k-means number possible cluster possible values elementary action k-means values. return reward agent chooses right action else punishment. reward defined according quality processing result. quality assessed using ground-truth models. define return calculate similarity resulting image ground truth. similarity calculated according features extracted images. type features depends task hand. example want detect object image extract number objects areas sizes etc. express difference scalars international journal advanced computer science applications vol. fig. shows curve learning agent first much knowledge experience behave uses several steps episode. time curve learning becomes almost constant proves really learning processing done number steps decreases episodes. means agent accumulates experience help take decision future. determining values parameters vision operators challenging task. paper proposed reinforcement learning approach handle problem even case vision task needing many operators sequence. texture segmentation application presented test approach. goal isn’t comparing method others goal present another manner thinking uses learning concepts show really gives good results. method applied decision process using parametric methods. nature reinforcement learning proposed approach takes account system opportunities also user preferences learning mechanism suggests trustworthy solutions. perspectives approach used large different images results compared methods. rewards punishments defined according quality criterion represents well image segmented. straightforward method comparing resulting image ground truth. comparison made scalar features obtained regions desired one. paper define reward according difference components image. define difference weighted four following differences images agent proceeds reinforcement learning finds optimal action gives best result w.taylor reinforcement learning framework parameter control computer vision applications proceedings first canadian conference computer robot vision ieee international journal advanced computer science applications vol. haralick shanmugan dinstein. textural features image classification. ieee transactions systems cybernetics vol.", "year": 2012}