{"title": "Ground Metric Learning", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Transportation distances have been used for more than a decade now in machine learning to compare histograms of features. They have one parameter: the ground metric, which can be any metric between the features themselves. As is the case for all parameterized distances, transportation distances can only prove useful in practice when this parameter is carefully chosen. To date, the only option available to practitioners to set the ground metric parameter was to rely on a priori knowledge of the features, which limited considerably the scope of application of transportation distances. We propose to lift this limitation and consider instead algorithms that can learn the ground metric using only a training set of labeled histograms. We call this approach ground metric learning. We formulate the problem of learning the ground metric as the minimization of the difference of two polyhedral convex functions over a convex set of distance matrices. We follow the presentation of our algorithms with promising experimental results on binary classification tasks using GIST descriptors of images taken in the Caltech-256 set.", "text": "transportation distances used decade machine learning compare histograms features. parameter ground metric metric features themselves. case parameterized distances transportation distances prove useful practice parameter carefully chosen. date option available practitioners ground metric parameter rely priori knowledge features limited considerably scope application transportation distances. propose lift limitation consider instead algorithms learn ground metric using training labeled histograms. call approach ground metric learning. formulate problem learning ground metric minimization diﬀerence polyhedral convex functions convex distance matrices. follow presentation algorithms promising experimental results binary classiﬁcation tasks using gist descriptors images taken caltech- set. consider paper problem supervised metric learning normalized histograms. normalized histograms arise frequently natural language processing computer vision bioinformatics generally areas involving complex datatypes. objects interest areas usually simpliﬁed represented smaller features. occurrence frequencies features considered object represented histogram. instance representation images histograms pixel colors sift gist features argue that general ground distance distance chosen according problem hand. knowledge ground metric always considered priori applications machine learning. precise applied datasets metric available motivated prior knowledge. argue problematic senses ﬁrst restriction limits application transportation distances problems knowledge exists. second even priori knowledge available argue cannot universal ground metric suitable learning problems involving histograms features. parameters machine learning algorithms ground metric selected adaptively. goal paper propose ground metric learning algorithms paper organized follows providing background transportation distances section propose section criterion diﬀerence convex function select ground metric given training histograms similarity measure histograms. show obtain local minima criterion using subgradient descent algorithm section propose diﬀerent starting points initialize descent section provide review relevant since terms metric distance interchangeable mathematically speaking always term metric metric features term distance resulting transportation distance histograms generally distance histograms. distances metric learning techniques section particular mahalanobis metric learning techniques inspired much work. provide empirical evidence section distances proposed paper compare favorably competing techniques. conclude paper section providing research avenues. upper case letters matrices. bold upper case letters used larger matrices; lower case letters used scalar numbers vectors upper case letter bold lower case stand matrix written matrix form vector form stacking successively column vectors left-most right-most bottom. notations stand respectively strict upper lower triangular parts enumerated must coherent sense upper triangular part expressed vector must equal finally frobenius dot-product matrix vector representations written scalar histograms dimension represented following column vectors canonical simplex dimension polytope transportation plans matrices coeﬃcients columns marginals equal respectively writing column positive coordinates. also known operations research statistical literatures transportation plans contingency tables two-way tables ﬁxed margins given histograms deﬁne following function function parameterized folsince null diagonal always zero; nonnegativity symmetry symmetric function arguments. generally distance histograms whenever metric namely whenever many variations -wasserstein monge-kantorovich mallow’s earth mover’s vision applications. rubner recently pele werman also proposed extend transportation distance compare un-normalized histograms. simply extensions compute distance unnormalized histograms combining diﬀerence total mass optimal transportation plan matrix means last line removed. modiﬁcation carried make sure constraints described independent equivalently rank deﬁcient. solved using network simplex feasible bounded polytope objective linear problem optimal solution ﬁnite extreme points thus minimum ﬁnite linear functions extension piecewise linear concave gradient equal ∇grc whenever optimal solution problem unique generally regardless uniqueness optimal solution problem sub-diﬀerential ∂grc property later section optimize criteria considered section below. quantiﬁes similar large positive whenever describe similar objects small negative dissimilar objects. assume similarity symmetric ωji. similarity object considered following simply give intuition weights practice. simple case weights reﬂect class taxonomy whenever come class diﬀerent classes. setting consider experiments later paper. weights also inferred hierarchical taxonomy weight corresponding histograms could instance reﬂect close respective classes histograms tree classes. metric agrees weights precisely criterion favor metrics which given pair similar histograms resulting distance small. conversely given pair dissimilar histograms resulting distance large. criterion balance requirements particular favor ground metrics ={j| e−}. adopt convention whenever larger cardinality follow convention n−ik. convention makes notation consistent deﬁnition since indeed check large enough notably since convex virtue convexity terms s−ik deﬁned equation follows concavity function fact functions weighted negative factors -ωij propose section algorithm obtain regardless unicity nearest neighbors histogram details computation subgradients given algorithm computations follow route; abbreviation s{+−} consider either cases algorithm outline. describe algorithm simple approach minimize locally based projected subgradient descent local linearization concave part algorithm runs subgradient descent using nested loops. ﬁrst loop parameterized variable minimize diﬀerence convex functions case paper either convex polyhedral function. functions convex polyhedral; overall quality local minima directly linked quality initial point choosing good thus crucial factor approach. provide options deﬁne section distance histograms provide educated guess deﬁne initial point optimize indeed distance interpreted kantorovich-rubinstein distance seeded uniform ground metric deﬁned i=j. distance popular distance compare histograms consider experiments note ﬁnite case nearest neighbors histogram need selected ﬁrst metric; purpose. although trick satisfactory observe similar approaches used weinberger saul seed algorithms near neighbors initial phase either equation depending value problem linear objective convex feasible set. norm deﬁning unit ball equation norm absolute values coeﬃcients matrix problem linear program constraints. large formulation might intractable. propose consider instead norm unit ball yields alternative form problem constraint replaced pseudo-distance symmetric zero diagonal nonnegative matrix. however straightforward check three conditions although intuitive considering metric valid non-symmetric matrices pointed authors themselves also applicable matrices negative entries non-zero diagonal entries. problem thus solved replacing however particular table easy compute considered central point previous work independence table table trivially also maximal entropy table table maximizes note however approximation tends overestimate substantially distance similar histograms. indeed easy check positive whenever deﬁnite distance positive entropy. case coordinates equal simply kmk/d. barvinok argues recently transportation tables close so-called typical table transportation polytope hinted good independence table. brieﬂy explain gradient hessian objective problem simple form. hessian expressed block form diagonal blocks diagonal matrices oﬀ-diagonal blocks rank thus easily computed using second-order methods. resulting matrices thus transportation plan sampled uniformly typically high average mismatched histograms pairs. contrary small pair features value typically high transportations plans similar histograms. recapitulate results section propose approximate linear function compute minimum intersection unit ball cone matrices. linear objective eﬃciently minimized using tools proposed adapted problem. order unit ball considered deﬁne order propose approximation used independence typical tables representative points polytopes successive steps computations yield initial point spelled algorithm bility measures apply probability measures narrowed distances probabilities unordered discrete sets dominant case machine learning applications rubner bin-to-bin distances easy compute accurate enough compare histograms features suﬃciently distinct. when contrary features known similar either because statistical co-occurrence form prior knowledge simple bin-to-bin comparison accurate enough cross-bin distances handle issue considering possible pairs cross-bin counts form distance. simple crosscoordinate distance general vectors arguably mahalanobis family distances interpreted euclidean distance cholesky factor square root learning linear maps directly using labeled information subject substantial amount research recent years. brieﬂy review literature following section. lanobis distance either positive semi-deﬁnite matrix linear techniques deﬁne ﬁrst criterion feasible candidate matrices obtain optimization algorithms relevant matrix criteria propose section modeled along ideas. weinberger ﬁrst consider criteria nearest neighbors inspired work proposal ﬁnite values section opposed considering average possible distances instance. semideﬁnite matrix linear operator ground metric learning produces instead metric matrix sets techniques operate diﬀerent mathematical objects. also worth mentioning although mahalanobis distances designed general vectors consequence applied histograms however knowledge statistical theory motivates probability simplex. aitchison argues perturbation operation naturally interpreted addition operation simplex. using notation distance becomes simple fisher metric applied perturbed histograms provide section details practical implementation algorithms follow presenting empirical evidence ground metric learning improves upon state-of-the-art metric learning techniques considered normalized histograms. algorithms implemented using several optimization toolboxes. algorithm requires computation several transportation problems. cplex matlab implementation network ﬂows warm starts eﬀect. computational gains obtain using instead using function call cplex matlab toolbox mosek solver approximately fold. beneﬁts come fact constraint vector problem needs updated iteration ﬁrst loop algorithm metricnearness toolbox carry projections inner loop iteration algorithm well last minimization algorithm compute typical tables using fminunc matlab function gradient hessian objective problem provided auxiliary functions. since fminunc deﬁnition unconstrained solver solution kept satisfy positivity constraints case large majority pairs histograms. whenever constraints violated optimize problem using slower constrained newton method. sample randomly classes taken caltech- family images consider images class. image represented normalized histogram gist features obtained using lear implementation gist features features describe edge direcn binary classiﬁcation task split points classes points form training points form test set. amounts training points following notations introduced section public implementations lmnn itml learn diﬀerent mahalanobis distances task. algorithms default settings lmnn itml. algorithms hellinger repreconsidered representation euclidean distance histograms using hellinger corresponds exactly hellinger distance since mahalanobis distance builds upon euclidean distance argue representation adequate learn mahalanobis metrics probability simplex. signiﬁcant gain performance observed figure obtained simple transformation conﬁrms intuition. directly comparable parameter used itml lmnn. subgradient stepsize algorithm guided preliminary experiments fact that normalization weights introduced above current iteration algorithm gradient steps comparable norms matrices. perform minimum gradient steps inner loop pmax inner loop terminated progress small objective progress every steps reaches qmax compute initial points using diﬀerent representative tables described algorithm figure illustrates variation performance diﬀerent choices natural distance gist features could consider. tried taking instance distances based directions features could come competitive methods considered above. situation illustrates claim introduction select agnostically metric features without using prior knowledge features. folds repeats cross validation procedure training select parameter pair lowest average error. transportation distances negative deﬁnite general case suﬃcient amount diagonal regularization κ-nn classiﬁcation error using considered metrics points training fold compute metrics needed compare test points. results also averaged test points classiﬁcation tasks. important results experimental section summarized figure displays considered distances average recall accuracy test average classiﬁcation error using κ-nearest neighbor classiﬁer. quantities averaged binary classiﬁcations. ﬁgure used shown provide average best possible performance left hand ﬁgure considers test points shows that point considered gml-emd selects average class points closest neighbors distance. performance gml-emd competing distances increases signiﬁcantly number retrieved neighbors increased. right hand ﬁgure displays average error tasks κ-nearest neighbor classiﬁcation algorithm considered distances varying values case combined fares much better competing distances. average error using distances represented legend right-hand side ﬁgure. results agree general observation metric learning support vector machines perform usually better κ-nearest neighbor classiﬁers learned metrics note however κ-nearest-neighbor classiﬁer seeded gml-emd average performance directly comparable support vector machines seeded kernels. also worth mentioning side remark distance perform well hellinger distances datasets validates earlier statement euclidean geometry usually poor choice compare histograms directly. intuition validated figure mahalanobis learning algorithms show perform signiﬁcantly better hellinger representation histograms. figure shows performance vary signiﬁcantly depending neighborhood parameter also considered ground metric initial point typ∞ obtained using typical tables algorithm corresponding results appear typ∞ curves ﬁgure. curves corresponding gml-emd illustrates fact subgradient descent performed algorithm real impact performance choosing good initial point enough. finally figure reports additional performance curves diﬀerent initial points experiments tend show that despite computational overhead barvinok’s typical tables seem provide better initial proposed paper approach tune adaptively unique parameter transportation distances ground metric given training dataset histograms. approach applied type features long histograms along side-information typically labels provided algorithm learn good candidate ground metric. algorithms performs projected subgradient descent diﬀerence convex functions local minimizers. propose initial points compensate arbitrariness show approaches provide compared competing distances superior average performance large image binary classiﬁcation tasks using gist features histograms. argued introduction paper ground metric never considered parameter could learned data. ground metric however attracted attention recently diﬀerent reason. ling okada gudmundsson pele werman recently argued computation dramatically sped ground metric matrix certain structure. instance pele werman shown computation earth mover’s distance significantly reduced whenever larger values arbitrary ground metric thresholded certain level. ground metrics follow constraints attractive result transportation problems provably faster compute. work paper suggests hand content ground metric learned improve classiﬁcation accuracy. believe combination viewpoints could result transportation distances adapted task hand fast compute. strategy achieve goals would enforce structural constraints candidate metrics looking minimizers criteria figure experimental setting ﬁgure identical figure except diﬀerent versions lmnn itml compared hellinger distance. ﬁgure supports claim section mahalanobis learning methods work better using ization setting typ∞ explained caption figure last curve typ∞ displays results corresponding ground metric directly output algorithm using typical algorithm performance curves also agree intuition metric given neighborhood parameter comparative advantage metrics parameter nearest neighbor classiﬁers close experimental setting identical figure distance accuracy κ-nearest neighbor performance gmlemd using diﬀerent initial points described section particularly section ﬁgure shows that despite computational overhead initializing algorithm using typical tables performs better average using independence tables. experimental setting identical figure computationally intensive part algorithm lies repeated calls algorithm computes number optimal transportations pairs histograms size training set. used network simplex algorithm warm starts carry computations faster alternatives along lines implementations provided pele werman could provide computational improvements. also known accept lower bounds particular cases. eﬃcient lower bound would reduce considerably complexity algorithm narrowing computation optimal transportations smaller subset neighbor candidates. proposed paper techniques learn ground metrics using histograms arbitrary features. believe techniques prove useful study histograms latent features topics blei laﬀerty dictionaries natural metrics always available.", "year": 2011}