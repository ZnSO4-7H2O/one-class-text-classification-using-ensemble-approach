{"title": "Mean Actor Critic", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We propose a new algorithm, Mean Actor-Critic (MAC), for discrete-action continuous-state reinforcement learning. MAC is a policy gradient algorithm that uses the agent's explicit representation of all action values to estimate the gradient of the policy, rather than using only the actions that were actually executed. This significantly reduces variance in the gradient updates and removes the need for a variance reduction baseline. We show empirical results on two control domains where MAC performs as well as or better than other policy gradient approaches, and on five Atari games, where MAC is competitive with state-of-the-art policy search algorithms.", "text": "propose algorithm mean actor-critic discrete-action continuous-state reinforcement learning. policy gradient algorithm uses agent’s explicit representation action values estimate gradient policy rather using actions actually executed. signiﬁcantly reduces variance gradient updates removes need variance reduction baseline. show empirical results control domains performs well better policy gradient approaches atari games competitive state-of-the-art policy search algorithms. reinforcement learning algorithms generally fall categories value-functionbased methods policy search methods. value-function-based methods maintain estimate value performing action state choose actions associated value current state contrast policy search algorithms maintain explicit policy agents draw actions directly policy interact environment subset policy search algorithms policy gradient methods represent policy using differentiable parameterized function approximator stochastic gradient ascent update parameters achieve reward. facilitate gradient ascent agent interacts environment according current policy keeps track outcomes actions. sampled outcomes agent estimates gradient objective function. critical question compute accurate gradient using samples costly acquire using sample interactions possible. actor-critic algorithms compute policy gradient using learned value function estimate expected future reward since expected reward function environment’s dynamics agent know typically estimated executing policy environment. existing algorithms compute policy gradient using value states agent visits critically methods take account actions agent actually executes environmental interaction. propose policy gradient algorithm mean actor-critic discreteaction continuous-state case. uses agent’s policy distribution average value function actions rather sampling action-value actions actually executed. approach signiﬁcantly reduces variance gradient updates removes need additional variance reduction term called baseline often used policy gradient methods implement using deep neural networks show empirical results control domains performs well better policy gradient approaches atari games competitive state-of-the-art policy gradient evolutionary policy search methods. note core idea behind also independently concurrently explored ciosek whiteson train agent select actions environment maximizes notion longterm reward. formalize problem markov decision process specify tuple sart states ﬁxed initial state discrete actions functions respectively describe reward transition dynamics environment discount factor representing relative importance immediate versus long-term rewards. concretely denote expected reward performing action state context policy search methods agent maintains explicit policy denoting probability taking action state policy parameterized note state policy outputs probability distribution discrete actions timestep agent takes action drawn policy environment provides reward signal transitions next state st+. agent’s goal every timestep maximize discounted future rewards simply return deﬁne slight abuse notation also denote total return trajectory equal trajectory. agent’s policy induces value function state space. expression return allows deﬁne state value function state-action value function here represents expected return starting state following policy thereafter represents expected return starting executing action following policy thereafter denotes trajectory. note probability speciﬁc trajectory depends policy parameters well dynamics environment. goal able compute gradient respect policy parameters γtpr discounted state distribution. second third lines rewrite summation using log-derivative trick. fourth line convert summation expectation notation place next make fact given williams intuitively makes sense since policy given state depend rewards achieved state. finally invoke deﬁnition nice property expectation that given access expectation estimated implementing policy environment. alternatively estimate using return unbiased sample essentially idea behind reinforce algorithm uses following gradient estimator note value function approximation general bias gradient estimation reducing variance reinforce actor-critic algorithms additive control variate baseline baseline function ﬁxed actions subtracting either sampled returns estimated q-values bias gradient estimation. refer techniques baseline advantage variations basic algorithms since approximate advantage choosing action baseline representing typical performance policy state update performed advantage reinforce scalar baseline measuring performance policy running average observed return past episodes interaction. advantage actor-critic uses approximation expected value state baseline however td-error sample advantage function approach higher variance methods explicitly compute moreover given easily computed practice still necessary estimate parameters overwhelming majority recent actor-critic papers computed policy gradient using estimate similar equation mnih wang estimate samples states actions trajectories executed according current policy order compute gradient objective function respect policy weights. instead using sampled actions mean actor-critic explicitly computes probability-weighted average q-values state sampled trajectories. eliminates need log-derivative trick remove preference actions higher probability result estimate gradient variance action sampling reduced zero. exactly difference computing sample mean calculating mean directly based observation expectation repeat here simpliﬁed follows implementation inner summation computed combining neural networks represent policy state-action value function. value function learned using variety methods temporal-difference learning monte carlo sampling. performing updates value function update parameters policy following update rule improve stability repeated updates value policy networks interleaved generalized policy iteration traditional actor-critic approaches refer sampled-action actor-critic actions involved computation policy gradient estimate actually executed environment. computing policy gradient estimate frequently involve actions actually executed environment. results trade-off bias variance. domains expect accurate q-value predictions function approximator despite actually executing relevant state-action pairs results lower variance gradient updates increased sample-efﬁciency. domains assumption valid perform worse sampled-action actor-critic though domains also tend pathological function-approximation general. ways similar expected sarsa expected sarsa considers next-actions computes expected td-error uses resulting error signal update function. contrast considers current-actions uses corresponding values update policy directly. natural consider whether could improved subtracting action-independent baseline sampled-action actor-critic reinforce gradient operator moved outside summation leaving action probabilities always hence gradient baseline term always zero. true regardless choice baseline since baseline cannot function actions else bias expectation. thus subtracting baseline unnecessary since effect policy gradient estimate. section presents empirical evaluation across three different problem domains. ﬁrst evaluate performance versus popular policy gradient benchmarks classic control problems. evaluate subset atari games investigate performance compared state-of-the-art policy search methods. order determine whether mac’s lower variance policy gradient estimate translates faster learning chose classic control problems namely cart pole lunar lander compared mac’s performance four standard sampled-action policy gradient algorithms. used open-source implementations cart pole lunar lander provided openai domains continuous state spaces discrete action spaces. screenshots domains provided figure problem domain implemented using independent neural networks representing policy function. performed hyperparameter search determine best network architectures optimization method learning rates. speciﬁcally hyperparameter search considered hidden layers; neurons layer; relu leaky relu tanh activation; rmsprop adam adadelta optimization method; learning rate chosen best setting independent trials combination hyperparameters chose setting best asymptotic performance trials. terminated episode timesteps regardless state agent. compared four standard benchmarks reinforce advantage reinforce actor-critic advantage actor-critic. implemented reinforce benchmarks using single neural network represent policy implemented actor-critic benchmarks using networks represent policy function. benchmark algorithm performed hyperparameter search used mac. order keep variance possible advantage actor-critic benchmark explicrather sampling using td-error determined best hyperparameter settings benchmark algorithms algorithm independent trials. figure shows learning curves different algorithms table summarizes results using mean performance trials episodes. cart pole learns substantially faster benchmarks lunar lander performs competitively best benchmark algorithm advantage actor-critic. test whether scale larger problem domains evaluated several atari games using arcade learning environment compared mac’s performance state-of-the-art policy search methods namely trust region policy optimization evolutionary strategies asyncronous advantage actor critic computational load inherent training deep networks play atari games limited experiments subset atari games beamrider breakout pong seaquest space invaders. games chosen common atari games tuning hyperparameters value network architecture exactly deepmind’s deep q-network policy network contained additional fully-connected layer neurons relu activation followed linear layer softmax output layer. policy value networks used shared weights ﬁrst four feature layers. trained value network feature layers using rmsprop learning rate keeping hyperparameters using experience replay buffer target network. place choosing actions using dqn’s \u0001-greedy strategy respect q-values given state instead sampled action policy network’s output distribution state compute value targets needed q-learning multiplied policy distribution next state corresponding q-values target network. every episode froze value network feature layers trained policy network using rmsprop learning rate following update rule given policy update sampled independent batches experience tuples drawn uniformly recent experiences replay buffer. game trained million frames pausing every frames evaluation episodes. compared mean score evaluation kept best-performing network effectively implementing early stopping. evaluated best-performing network types starting conditions random starts human starts order compare results previous methods. random starts game initialized randomly no-ops actions evaluated network episodes human starts game initialized random game state sampled minutes human expert play evaluated network episodes agent’s performance evaluated agent’s total score human start note used different human expert game initializations runs publicly available. evaluating network breakout noticed issue training scheme resulted strange behavior. used standard training scheme atari consists evaluation periods ﬁxed number timesteps regular intervals. consequence possible lucky ﬁrst game evaluation period achieve high score subsequently unable ﬁnish second game happens training process record average completed game score even much higher agent’s true average performance. occurred training could evaluate resulting network could ﬁnish enough games. wanted know whether behavior getting stuck simply learning anything added small amount noise policy evaluation order avoid getting stuck. results tables obtained using noisy policy denoted asterisk. random-start condition compared trpo well found performed signiﬁcantly better three games competitively fourth average three times better best performing benchmark algorithm. human-start condition compared state-of-the-art algorithm experiment better experiments worse three signiﬁcantly outperformed seaquest leading average performance across games three half times note performance beam rider relative games. algorithm falling local optimum always chose left shooting. strategy locally optimal measurably better random play prevented agent ﬁnding better policy. behavior typical policy gradient algorithms note mac’s performance still competitive trpo able overcome problem exploring different parts search space parallel approach always feasible practice. core offers computing policy gradient substantially reduce variance increase learning speed. number orthogonal improvements policy gradient methods using natural gradients off-policy learning second-order methods asynchronous exploration investigated performs extensions; however improvements added basic actor-critic methods could added well expect would improve performance similar way. typical use-case actor-critic algorithms problem domains continuous actions awkward value-function-based methods approach dealing continuous actions deterministic policy perform off-policy policy gradient updates. however settings on-policy learning necessary using deterministic policy leads sub-optimal behavior hence stochastic policy typically used instead. uses stochastic policy designed discrete-action domains. extending continuous actions would require changing mac’s actions integral operation complex. modiﬁcation subject future work. basic formulation policy gradient estimators presented here—where gradient estimated averaging state-action value function across actions—leads family actor-critic algorithms. family advantage requiring additional variance-reduction baseline substantially reducing design effort required apply them. also natural deep neural network function approximators resulting network architecture end-to-end differentiable. results show algorithm combined neural network implementation either outperforms competitive with stateof-the-art policy search algorithms make similar assumptions. future work develop family algorithms further ﬁrst including typical elaborations basic actor-critic architecture like natural second-order gradients second adding parallel searches would help avoid local optima inherent policy search cases cause perform badly. results suggest approach highly promising extensions provide even improvement performance. brockman greg cheung vicki pettersson ludwig schneider jonas schulman john tang zaremba wojciech. openai gym. corr abs/. http//arxiv. org/abs/.. greensmith evan bartlett peter baxter jonathan. variance reduction techniques gradient estimates reinforcement learning. journal machine learning research shixiang lillicrap timothy ghahramani zoubin turner richard levine sergey. qprop sample-efﬁcient policy gradient off-policy critic. arxiv preprint arxiv. lillicrap timothy hunt jonathan pritzel alexander heess nicolas erez tassa yuval silver david wierstra daan. continuous control deep reinforcement learning. arxiv preprint arxiv. mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg humanlevel control deep reinforcement learning. nature mnih volodymyr badia adria puigdomenech mirza mehdi graves alex lillicrap timothy harley silver david kavukcuoglu koray. asynchronous methods deep reinforcement learning. international conference machine learning schulman john levine sergey abbeel pieter jordan michael moritz philipp. trust region policy optimization. proceedings international conference machine learning silver david lever heess nicolas degris thomas wierstra daan riedmiller martin. deterministic policy gradient algorithms. proceedings international conference machine learning sutton richard mcallester david singh satinder mansour yishay. policy gradient methods reinforcement learning function approximation. advances neural information processing systems wang ziyu bapst victor heess nicolas mnih volodymyr munos remi kavukcuoglu koray freitas nando. sample efﬁcient actor-critic experience replay. arxiv preprint arxiv.", "year": 2017}