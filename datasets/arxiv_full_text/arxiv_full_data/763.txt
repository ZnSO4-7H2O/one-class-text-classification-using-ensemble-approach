{"title": "Probabilistic Neural Programs", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "We present probabilistic neural programs, a framework for program induction that permits flexible specification of both a computational model and inference algorithm while simultaneously enabling the use of deep neural networks. Probabilistic neural programs combine a computation graph for specifying a neural network with an operator for weighted nondeterministic choice. Thus, a program describes both a collection of decisions as well as the neural network architecture used to make each one. We evaluate our approach on a challenging diagram question answering task where probabilistic neural programs correctly execute nearly twice as many programs as a baseline model.", "text": "present probabilistic neural programs framework program induction permits ﬂexible speciﬁcation computational model inference algorithm simultaneously enabling deep neural networks. probabilistic neural programs combine computation graph specifying neural network operator weighted nondeterministic choice. thus program describes collection decisions well neural network architecture used make one. evaluate approach challenging diagram question answering task probabilistic neural programs correctly execute nearly twice many programs baseline model. recent years deep learning produced tremendous accuracy improvements variety tasks computer vision natural language processing. natural next step deep learning consider program induction problem learning computer programs input/output examples. compared traditional problems object recognition require making single decision program induction difﬁcult requires making sequence decisions possibly learning control concepts loops statements. prior work program induction described general classes approaches. first noise-free setting program synthesis approaches pose program induction completing program sketch program containing nondeterministic choices ﬁlled learning algorithm probabilistic programming languages generalize approach noisy setting permitting sketch specify distribution choices function prior parameters condition distribution data thereby training bayesian generative model execute sketch correctly second neural abstract machines deﬁne continuous analogues turing machines general-purpose computational models lifting discrete state computation rules continuous representation approaches demonstrated success inducing simple programs synthetic data applied practical problems. observe three dimensions along characterize program induction approaches figure probabilistic neural programs deﬁning multilayer perceptron computation graph applying create probability distribution program executions neural abstract machines conﬂate dimensions naturally support deep learning commit particular computational model approximate inference algorithm. choices suboptimal bias/variance trade-off suggests training expressive computational model require data less expressive suited task hand recent work suggested discrete inference algorithms outperform continuous approximations contrast probabilistic programming supports speciﬁcation different computational models inference algorithms including discrete search continuous approximations. however languages restricted generative models cannot leverage power deep neural networks. present probabilistic neural programs framework program induction permits ﬂexible speciﬁcation computational model inference algorithm simultaneously enabling deep neural networks. approach builds computation graph frameworks specifying neural networks adding operator weighted nondeterministic choice used specify computational model. thus program sketch describes decisions made architecture neural network used score decisions. importantly computation graph interacts nondeterminism scores produced neural network determine weights nondeterministic choices choices determine network’s architecture. probabilistic programs various inference algorithms applied sketch. furthermore sketch’s neural network parameters estimated using stochastic gradient descent either input/output examples full execution traces. evaluate approach challenging diagram question answering task recent work demonstrated formulated learning execute certain class probabilistic programs. task enhanced modeling power neural networks improves accuracy. probabilistic neural programs build computation graph frameworks specifying neural networks adding operator nondeterministic choice. developed scala library probabilistic neural programming illustrate concepts. figure deﬁnes multilayer perceptron probabilistic neural program. deﬁnition resembles computation graph frameworks. network parameters intermediate values represented computation graph nodes tensor values. manipulated standard operations matrix-vector multiplication hyperbolic tangent. evaluating function tensor yields program sketch object evaluated network parameters produce network’s output. figure shows choose function create nondeterministic choice. function nondeterministically selects value list options. score option given value computation graph node number elements list. evaluating function tensor yields program sketch object represents function neural network parameters probability distribution values. probability value proportional scores choices made execution produced performing inference object case using beam search produces explicit representation distribution. multiple nondeterministic choices combined produce complex sketches; capability used deﬁne complex computational models including general-purpose models turing machines. library also functions conditioning observations. although various inference algorithms applied program sketch work simple beam search executions. approach accords recent trend structured prediction combine greedy inference beam search powerful non-factoring models beam search maintains queue partial program executions associated score. step search continues execution encounters call choose adds zero executions queue next search step. lowest scoring executions discarded maintain ﬁxed beam width. execution proceeds generate computation graph nodes; search maintains single computation graph shared executions nodes added. search simultaneously performs forward pass nodes necessary compute scores future choices. neural network parameters trained maximize loglikelihood correct program executions using stochastic gradient descent. training example consists pair program sketches representing unconditional conditional distribution. gradient computation similar loglinear model neural network factors. ﬁrst performs inference conditional unconditional distributions estimate expected counts associated nondeterministic choice. counts backpropagated computation graph update network parameters. consider problem learning execute program sketches food computational model using visual information diagram. problem motivated recent work demonstrated diagram question answering formulated translating natural language questions program sketches model learning execute sketches. figure shows example questions work along accompanying diagram must interpreted determine answers. diagram food depicts collection organisms ecosystem arrows indicate organism eats. right side ﬁgure shows questions pertaining diagram associated program sketches. possible executions program sketch determined domain-speciﬁc computational model designed reason food webs. nondeterministic choices model correspond information must extracted diagram. speciﬁcally functions call choose nondeterministically return boolean value. ﬁrst function organism return true text label organism second function return true organism eats organism functions inﬂuence program control ﬂow. food model also includes various table results appling probabilistic neural programs execution sketches foodwebs dataset. choose accuracy indicates average accuracy decision whereas execution accuracy indicates entire program executed correctly. functions e.g. reasoning population changes call organism extract information diagram. thorough description theory; goal learn make choices theory. consider three models learning make choices organism non-neural model well probabilistic neural models three learn models organism using outputs computer vision system trained detect organism text arrow relations them. deﬁnes hand-engineered features heuristically created outputs vision system. loglinear -layer features difference simply greater expressivity two-layer neural network. however major strengths neural models ability learn latent feature representations automatically third model also uses direct outputs vision system made features. architecture maxpool reﬂects contains additional input layers maxpool detected relationships objects conﬁdence scores. expectation neural network modeling nondeterminism learn better latent representations manually deﬁned features. evaluate probabilistic neural programs foodwebs dataset introduced data contains training programs test programs. programs human annotated gold standard interpretations questions data corresponds assuming translation questions programs perfect. train probabilistic neural programs using correct execution traces program also provided data set. evaluate models using metrics. first execution accuracy measures fraction programs test executed completely correctly model. metric challenging correctly executing program requires correctly making number choose decisions. test programs decisions implying completely execute program correctly means getting average choose decisions correct without making mistakes. second choose accuracy measures accuracy decision independently assuming previous decisions made correctly. table compares accuracies three models foodwebs dataset. improvement accuracy baseline probabilistic neural program neural network’s enhanced modeling power. though choose accuracy improve large margin improvements translate large gains entire program correctness. finally expected inclusion lower level features possible loglinear signiﬁcantly improved performance. note task requires performing computer vision thus expected model achieve accuracy. presented probabilistic neural programs framework program induction permits ﬂexible speciﬁcation computational models inference algorithms simultaneously enabling deep learning. program sketch describes collection nondeterministic decisions made execution along neural architecture used scoring decisions. network parameters sketch trained data using stochastic gradient descent. demonstrate probabilistic neural programs improve accuracy diagram question answering task formulated learning execute program sketches domain-speciﬁc computational model.", "year": 2016}