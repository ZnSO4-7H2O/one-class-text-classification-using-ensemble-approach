{"title": "Ideological Sublations: Resolution of Dialectic in Population-based  Optimization", "tag": ["cs.LG", "cs.AI", "cs.CC", "cs.NE"], "abstract": "A population-based optimization algorithm was designed, inspired by two main thinking modes in philosophy, both based on dialectic concept and thesis-antithesis paradigm. They impose two different kinds of dialectics. Idealistic and materialistic antitheses are formulated as optimization models. Based on the models, the population is coordinated for dialectical interactions. At the population-based context, the formulated optimization models are reduced to a simple detection problem for each thinker (particle). According to the assigned thinking mode to each thinker and her/his measurements of corresponding dialectic with other candidate particles, they deterministically decide to interact with a thinker in maximum dialectic with their theses. The position of a thinker at maximum dialectic is known as an available antithesis among the existing solutions. The dialectical interactions at each ideological community are distinguished by meaningful distributions of step-sizes for each thinking mode. In fact, the thinking modes are regarded as exploration and exploitation elements of the proposed algorithm. The result is a delicate balance without any requirement for adjustment of step-size coefficients. Main parameter of the proposed algorithm is the number of particles appointed to each thinking modes, or equivalently for each kind of motions. An additional integer parameter is defined to boost the stability of the final algorithm in some particular problems. The proposed algorithm is evaluated by a testbed of 12 single-objective continuous benchmark functions. Moreover, its performance and speed were highlighted in sparse reconstruction and antenna selection problems, at the context of compressed sensing and massive MIMO, respectively. The results indicate fast and efficient performance in comparison with well-known evolutionary algorithms and dedicated state-of-the-art algorithms.", "text": "population-based optimization algorithm designed inspired main thinking modes philosophy based dialectic concept thesis-antithesis paradigm. impose diﬀerent kinds dialectics. idealistic materialistic antitheses formulated optimization models. based models population coordinated dialectical interactions. population-based context formulated optimization models reduced simple detection problem thinker according assigned thinking mode thinker her/his measurements corresponding dialectic candidate particles deterministically decide interact thinker maximum dialectic theses. position thinker maximum dialectic known available antithesis among existing solutions. dialectical interactions ideological community distinguished meaningful distributions step-sizes thinking mode. fact thinking modes regarded exploration exploitation elements proposed algorithm. result delicate balance without requirement adjustment step-size coeﬃcients. main parameter proposed algorithm number particles appointed thinking modes equivalently kind motions. additional integer parameter deﬁned boost stability ﬁnal algorithm particular problems. proposed algorithm evaluated testbed single-objective continuous benchmark functions. moreover performance speed highlighted sparse reconstruction antenna selection problems context compressed sensing massive mimo respectively. results indicate fast eﬃcient performance comparison well-known evolutionary algorithms dedicated state-of-the-art algorithms. optimization necessary tool ﬁelds science engineering. main approaches optimization based mathematical methods metaheuritic ways. mathematical methods gradient-based approaches reliable proof convergence global optimum solution predetermined conditions optimization model however conditions satisﬁed speciﬁc models still real-world problems tractable mathematical optimization. heuristics/evolutionary algorithms appropriate approach situations. potential discover global optimum solution regardless properties cost/ﬁtness function. operate minimum information function. consequence easy program adjust diﬀerent problems. metaheuristic algorithms borrow kind intelligence almost nature. able discover global optimum solution wide range problems. crucial requirement intelligence existence exploration exploitation features source inspiration. intelligent system able exploit solution conﬁrmed promising able explore enough number candidate solutions eﬃciently. obvious instance think natural thinking abilities human being. focused diﬀused thinking modes regarded exploitation exploration abilities mankind respectively thinking modes phycological point view. proposed algorithm inspired thinking modes developed context philosophy. history philosophers developed thinking modes equip mankind powerful tools path discovering truth terminology work truth desired global optimum solution population equipped opposite kinds thinking modes i.e. speculative thinking exploration practical thinking exploitation. borrow idea dialectics modern philosophy deﬁne thinking modes results. types dialectics modeled based distances objective subjective spaces. thinking refer simple procedure solution selects dialectical solution interaction. solving diﬀerent problems providing balance exploration exploitation. approached controlling parameters algorithm. perfect balance leads eﬃcient search reasonable time. hence main point designing algorithm consistency operators developed exploitation exploration. would make balance easily captured minimum number parameters. review main operators well-known algorithms gains insight evolution exploitation exploration operators. basic operator exploitation used particle swarm optimization algorithm motion particles toward best solution high exploitation power however expense risk trapped local optimum. side genetic algorithm random mutations driven speciﬁc probabilistic distribution signiﬁcant exploration power expense runtime. point view operators introduced metaheuristic algorithms relax determinism motion toward leader constrict randomness mutations. order avoid local optimums determinism motions toward leader relaxed variants. example ﬁtness-distance-ratio based near better solution selected particle follow local leader instead global leader. algorithms imperialistic competition natural aggregation utilize k-best solutions instead -best leader pso. however still randomness selection k-best solutions imperialist/shelter. side diﬀerential evolution constricts completely random mutations using diﬀerence vectors among solutions mutation vectors. however still randomness selection solutions computation mutation vector also determinism following best solution moreover random selections exist interactions among solutions algorithms brainstorm optimization learning phase teaching-learning-based optimization overall except variants particles follow criteria mentioned algorithms contain randomness selection phase interactions. main motivation development proposed algorithm discovering systematic interaction among particles without randomness selection phase. explained next section idea inspired modern philosophy based systematic dialectic instead arbitrary dialectic utilized among ancient philosophers. work arbitrary dialectic interpreted result random selection particles interaction. would develop eﬃcient deterministic selection scheme leveraging deﬁnitions kinds antitheses. literature opposite solutions utilized acceleration evolutionary algorithms further research direction high-level language programming inspired dialectical philosophy related work optimization algorithms called dialectic search however fundamental diﬀerences proposed algorithm context source inspiration modeling ways models dialectic searched among population population solutions improve positions based dialectical interactions dialectic search algorithm dialectic imposed local random changes single solution. proposed algorithm solutions generated meaningful steps toward dialectical solutions known antithesis dialectic search solution searched path toward dialectical solution. worth mentioning idea proposed algorithm formed developed without aware dialectic search algorithm. proposed algorithm named ideological sublations tendency thinkers canceling theses simultaneously preserve essential ideas behind algorithm deﬁnition euclidian distance solution metric idealistic contradiction diﬀerence objective functions metric materialistic contradiction. management contradictions separation solutions groups according qualities. rest paper organized follows. next section concept dialectics evolution reviewed philosophical point view. also connections proposed algorithm discussed section. section proposed algorithm explained modeling considered thinking modes. section experimental results test benchmark functions sparse reconstruction problem antenna selection challenge included. finally discussion provided section paper concluded section word dialect literally composed preﬁx diameans across greek root legein means speak context philosophy dialectic process contradiction opposite sides everything leads truth. first utilization dialectic belongs ancient greek philosophers innovated back-and-forth form dialectic arguments later dialectical thinking modes developed created diﬀerent philosophers philosophical expression universal thinking mode eliminates opposition thinking existence situation. among developed modes complementary modes dialectical thinking attentions; speculative thinking practical thinking. speculative mode dialectical thinking radically evolved hegel reformed classic version dialectic. systematic model dialectic included figure speculative moment moment resolution arises stages understanding moment dialectical moment. thesis seems stable understanding moment challenges pass opposite side dialectical moment. contradiction thesis antithesis unstable moment sublation leads emerging sophisticated thesis speculative moment. next repeat synthesis challenges itself interacts antithesis reforms another synthesis. process continues reaching truth. terminology proposed swarm-based optimization algorithm candidate solutions regarded existing theses truth optimum solution discovered speculative thinking mode modeled explore search space. introduce speculative operation guess particular randomness. main diﬀerence hegelian dialectic classical process self-sublation dialectical moment. process thesis cancels preserves simultaneously transforms antithesis. hence despite classical dialectic waits arbitrary opposition outside progress hegel’s process deterministic unity thesis-antithesis model. according hegel’s ﬁndings procedure leads exact truth despite ancient method leads approximate truth extreme reﬁnement deﬁnition dialectic expressing speculative thinking process mentioned three logical stages hegel introduced systematic idealism systematic deterministic change subjective idea leads improvement objective material. context metaheuristics regard random mutations genetic algorithm arbitrary dialectics diﬀerential mutations algorithm follow systematic intelligent production dialectic. however still randomness comes arbitrary choice generating pairs mutation vectors kind randomness exists teaching-learning-based optimization algorithm arbitrary interactions among students. proposed algorithm based deﬁnition self-sublation mutation vectors generated deterministically selected candidate solutions i.e. idealistic thesis available antithesis. fact speculative thinking mode used exploration operator eliminated randomness choosing pair solution vector. materialistic dialectic complementary part idealistic dialectic. well-known marx opposite direction hegel’s philosophy marx refused speculate details realized opposition thinking existence root human’s activities materialistic ideology social existence determines consciousness. contrary idealistic thoughts determination existence consciousness. nevertheless idea materialistic dialectic also expressed three-logical stages understanding dialectic resolution moments lead thesis-antithesissynthesis paradigm. practical thinking mode developed kind dialectic. according materialism change objective material leads improvement subjective idea. symbolic example procedure reformation mention important phycological progress conﬁdence ability resources master nature industrial revolution practical thinking mode translated population-based optimization context utilized exploitation operator relaxed determinism selection movement toward leader. block diagram figure illustrates main idea behind proposed algorithm. illustrated loop algorithm consists three understanding sublation speculative/practical moments. would clariﬁed understanding speculative/practical moments modeled simple operators regularly utilized context swarm-based optimization algorithms meaningful nuances. hence operators introduced sublation moment main idea behind proposed algorithm. section ﬁrst kinds diﬀerence among solution vectors population highlighted consequently models dialectics formulated. then proposed dialectic models lead unique antithesis translated population-based optimization context three logical stages. solution vector regarded individual thesis diﬀerent subjects. goal optimizing cost/ﬁtness function decision variables i.e. swarm/population based approaches candidate solutions follow rules discover global optimum solution. diﬀerence among theses population leads challenge motion. however philosophy promises extreme diﬀerence called dialectic lead high-resolution optimum solution higher speed arbitrary diﬀerence. order organize dialectical interactions among solutions idealistic materialistic antithesis modeled. simply deﬁne euclidian distance solutions idealistic diﬀerence distance objective space materialistic diﬀerence. regardless limitation acquired number samples function idealistic antithesis xanti speciﬁc thesis xthes modeled solution following optimization problem according proposed model thesis belongs speculative thinking community sublate leads antithesis largest distance level quality model idealistic deﬁnition speculative antithesis. according deﬁnition exact antithesis identiﬁable whole inﬁnite number solutions domain quality thesis xthes evaluated. actually procedure eﬃcient practical. however mimicked translation idea possible community ﬁnite number population. hand practical antithesis searchable among number best solutions; nearest distance practical thesis dialectical position. since approachable solution promises signiﬁcantly higher qualities. mathematical expression amount scalar guarantees dialectical materialistic thesis corresponding antithesis. canceling practical thesis materialistic self-sublation. side looking closest solution minimizing euclidian distance preserving side sublation practical thinking mode. following arrange thinkers/particles manner desired dialectics included interactions among thinkers. indicated although ﬁnding solution perfect dialectic candidate solution possible still approximating antithesis among existing solutions delivers taste dialectical philosophy promises. stage initial/new solutions evaluated. except ﬁrst iteration initial solutions considered theses iterations thesis accepted position current iteration better quality position previous iteration. expression best thesis thinker preserved optimization process. consequently iteration accepted solutions theses sorted according cost/ﬁtness values. sorted theses divided groups high-quality low-quality solutions. simply high-quality solutions appointed speculative thinking thinkers/particles. elaborated next subsection sort assignment simpliﬁes task thinker ﬁnding corresponding antithesis among available solutions. integer value main parameter algorithm easily adjusted trying capability increasing number practical thinkers. would seen simulation results problems appropriate value integer number larger summary repeat understanding moment theses/positions checked acceptance rejection sorted assignment speculative practical thinking mode thinker/particle. moment thinker challenges thesis according assigned thinking mode understanding moment. process called self-sublation thinker looks antithesis among available theses thinkers. discovered antithesis would reference point change. mentioned antithesis solution maximum contradiction dialectic thesis. course since kind dialectics hence along maximization particular dialectic opposite dialectic minimized thinking mode. fact implicitly formulated constraint part model objective sentence model following subsections reduce proposed models simple hypotheses ﬁnding approximate antithesis typical thesis collection solutions. depending assigned thinking mode thinker hypotheses would deployed. figure demonstrates proposed self-sublation scheme among sorted theses according qualities speciﬁc iteration. ﬁgure indicated solution arrow candidate considered antithesis corresponding thesis. figure illustration sublation moment thinker looks another candidate thesis located largest distance similar quality located nearest neighbor among best solutions smaller superscript index means higher-quality thesis. speculative thinking mode thinking among high-quality solutions. except ﬁrst best best solutions deterministically choose nearest speculative thinker objective space antithesis solutions face simple detection problem ﬁnding antithesis label speculative solutions quality order best solution best solution antithesis ﬁrst last theses would thesis theses higher-quality lower-quality thinker selects thesis longest distance respect thesis. fact looking objective neighborhood preserving choosing solution largest distance canceling speculative thesis self-sublation moment. another expression antithesis thesis best existing thesis idealistic antithesis. closer speciﬁc practical thesis chosen practical antithesis. indicated equation antithesis best solution always ﬁxed second best solution second best solution often reasonable candidate antithesis practical thinkers. however problems leads stability issues. deﬁne axillary parameter increase stability situations. iteration distance k-best solutions best solution measured solution largest distance chosen solution iteration i.e. xanti− used practical thinking low-quality solutions second best solution always ﬁxed antithesis speculation best solution words antithesis view point practical thinkers best thinker same. value recommended number initial setting indicated simulations rarely lead better performance. value best thesis compulsorily regarded antithesis practical thinkers. moreover increasing amount larger values lead stability algorithm optimization dialectic always held. ﬁnal remark although antitheses selected similar candidates aggregation nuanced decisions thinkers leads signiﬁcant impact ﬁnal result large number iterations. moment practical speculative thinkers update theses based corresponding antithesis. detected antitheses used reference point speculative/practical motions. update rule simply modeled following equation thinkers motions distribution random variables used step-size vector distributions diﬀerent speciﬁc speculative practical thinking modes. checking basic distributions realized step-sizes speculative motions driven uniform distribution negligible bias simultaneously normal biased distribution stepsizes practical movements algorithm always converges i.e. following parameters empirically found appropriate values dealing diﬀerent problems. ﬁxed parameters proposed algorithm. variable parameters adjustment inﬂuence performance number speculative thinkers number elites ﬁnding opposite directions exploitation. ﬁxed parameters step-sizes ﬁxed inferred parameters mean standard deviation uniform distribution speculative step-sizes always ﬁxed given values independent parameters cases. however amount mean normal distribution practical step-sizes depends materialistic antithesis chosen speciﬁc practical thesis. best solution detected antithesis bias imposed normal distribution. otherwise variance practical step-sizes antithesis practical theses best solution equivalently reason priority exploitation case. hand xanti− also engaged practical sublations diversity center attention antithesis increasing variance step-sizes. figure demonstrates distribution random motions speculative practical modes -dimensional space. start point motion thesis reference point interaction detected antithesis depicted figure practical mode signiﬁcantly better solution aimed thinker scans area around antithesis details. variance step-sizes sensing area shrinkages becomes concentrated around antithesis. exploitation capability algorithm gained practical motions. comparison swarm-based global optimization algorithms fdr-pso cost value lowquality solutions following antitheses. also decision practical thinkers antithesis deterministically taken best solution antithesis. despite randomness choosing empire shelter target elite solution. opposite side thinkers/particles speculative mode explore search space. illustrated figure ideal case bias toward idealistic antithesis thinker idealistic ideology freely move directions. kind motion realized uniform distribution step-sizes around zero mean. however practice realized little bias remarkable impact convergence performance algorithm. comparison population-based algorithms genetic diﬀerential evolution speculative motion regarded structured mutation intensity controlled idealistic antithesis. proposed systematic interactions deterministic selection antitheses contradiction randomness selection generating pairs mutation vectors algorithm. main stages proposed algorithm summarized algorithm initialization accomplished steps understanding moment implemented steps steps assigned sublation resolution moments respectively. obviously constraint optimization model imposed function evaluation i.e. step instance case single objective continuous problems decision variables candidate solutions preserved valid domain passing operators max. another example combinatorial problems continuous variables transformed valid discrete variables appropriate mapping. case algorithm usually operates continuous domain evaluations done discrete domain. remark combinatorial problems according observations antenna selection problem discrete models generate random vectors initial theses. determine number speculative thinkers elites evaluate theses. accept thesis leads improvement cost value sort theses according cost values. norm appropriate metric measuring distances. replacement metric instead norm equations gains reasonable decisions right antitheses consequently leads better performance. three main operations determine order computational complexity proposed algorithm. thinkers computational burden iteration comes sorting cost values computation approximately distances sublation moment computation thesis according update rule computational complexity ﬁrst operation second third operations similar complexity order hence worst case complexity algorithm max) indicates number function evaluations. result asymptotic order complexity remains since although computational complexity algorithm order magnitude roughly order evolutionary algorithms comparison shown simulation results runtime operators proposed algorithm least half test algorithms. section evaluate eﬃciency speed proposed algorithm using number benchmark single objective cost functions continuous optimization model sparse reconstruction binary optimization problem antenna selection large scale. benchmark functions comparisons obtained de/rand//bin cooperative comprehensive learning grey wolf optimization teaching-learning-based optimization sparse reconstruction problem additional comparisons provided constriction coeﬃcients also state-of-the-art dedicated algorithms sparse reconstruction. finally antenna selection problem comparisons provided competitive algorithm among considered algorithms ga-based discrete algorithm recently proposed antenna selection. mention previously developed algorithm inspired tornado’s currents despite eﬃciency low-dimension problems quickly failed competitive large scales. error indicates averaging number trials. trial optimization procedure regarded successful optimization approached cost value less threshold value number thinkers/particles population size ﬁxed algorithms otherwise mentioned. simulations computer intel core i-.ghz operating windows matlab subsection benchmark cost functions used evaluation. selected among challenging problems competition considered benchmark functions consists unimodal/multimodal diﬀerentiable functions separable decision variables. test functions variables depicted figure shown ﬁgure functions clustered group speciﬁc test algorithm outperforms ones. functions details summarized table parameters proposed algorithm adjusted problem fast smooth convergence. adjusted parameter values ﬁxed small large dimensions problem. also parameters algorithm carefully tuned fair competition proposed algorithm. algorithms implemented original parameter-free version recommended relations parameters variants algorithm developed getting ride parameter tuning task facing diﬀerent problems. generally speciﬁc application tuning original variant popularly de/rand//bin preferred. overview literature applications algorithm proofs statement. however literatures lack comparison tuned-de algorithm variants. here popular variants i.e. code included simulations justify reason behind popularity original variant speciﬁc application/problem. hand variants avoid trapping local optimums. clpso well-known variant used comparisons table summarizes parameter values algorithms. inferred appropriate value parameter usually value also eﬀective choice sparse reconstruction problem. larger integer numbers used functions order increase stability reduce sensitivity initial solutions. moreover smaller integer i.e. applied order special exploitation property. addition problems eﬀective integer number larger half population size shown table integers assigned number speculative thinkers almost multiplications hence sensitivity parameter adjustment required. appropriate integer easily found trying limited number possibilities ﬁxed adjustment parameter increased satiability issue diﬀerent runs decreased speciﬁc exploitation property desired. algorithms initiated initial solutions stopped predetermined number function evaluations initial solutions produced randomly values distributed uniformly within predeﬁned domains. domains scale problems listed table table respectively. figure figure show convergence curves functions dimensions respectively. also convergence curve three last benchmark functions included figure curves obtained averaging independent runs problem. mean standard deviation best cost value ﬁnal iteration problem reported functions based number competitive solutions global optimum solution equivalently number global minimums also based regularity allocation local minimums. division conduces rough conclusion possible functions proposed algorithm hopefully better performance. cluster consists prototype examples unimodal function without competitive solutions multimodal function regular allocation non-competitive local minimums multimodal function negligible irregularity allocation local minimums still without serious competitive solution finally challenging problem cluster similar structure existence competitive solutions located near distance global optimum. general algorithm successfully optimizes functions cluster. however failures solving small-scale version also discovering global optimal solution figure according table large standard deviation algorithm solving -dimension function indicates unstable optimization algorithm. indicated table number successful optimization algorithm problem total number trials. highest success rate exact approximations algorithm. worth mentioning similar structure function exists rastrigin ackley functions according observations algorithm also unstable dimensions problems poor performance large dimensions. despite performance large dimension stably optimized algorithm. hand small standard deviation proposed algorithm solving problem implicitly indicates algorithm always discovers competitive local optimal solution function. however according observations shrinking domain decision variables global optimum solution approachable roughly speaking algorithm along tlbo best functions cluster competitive optimal solutions exist relatively large distance respect together local minimums arranged irregular positions. instance function shifted-variable version local minimums distributed diﬀerent positions domain. functions small large scales proposed algorithm competes algorithm appropriate algorithm cluster. mentioned proposed algorithm successfully stabilized increasing parameter value however according observations increase lead completely stable optimization function small large scales failures successful optimization hence increase population size necessary stable optimization similar results optimization function algorithm success ﬁnding global minimum however indicated table despite tlbo algorithm shows stability ﬁnding highly competitive solutions problem i.e. successes zero success tlbo. finally unique feature functions cluster respect previous ones existence numerous competitive solutions. example two-variable state function number competitive solutions among functions cluster further number larger function becomes inﬁnity global optimum solution origin. rings around origin approximately equal optimal cost values original point include inﬁnite number competitive solutions. also region contains existing competitive solutions global optimum also inﬁnite number. results figure figure indicate proposed algorithm best performance functions cluster small large scales. cluster competitive algorithm proposed algorithm although small scale problem algorithm defeated failure dimensions remarkable diﬀerence number successful optimization i.e. algorithm i.e. similar instability optimization large scale. unsuccessful optimization according table algorithm completely successful. xin-she yang function similar structure function function algorithm also outperforms test algorithms according observations. results omitted brevity. side algorithm discovers exact optimum solution function minimum number function evaluations last least proposed algorithm successful algorithm approaching exact optimum solution although comparative algorithms touch optimal cost value close zero solutions large distance global optimum solution located origin. figure compares estimated solution minimization function figure compares average runtime proposed algorithm test algorithms. ﬁgure benchmark problems sorted according required runtime algorithm. shown algorithm smallest runtime optimization benchmark functions except order fair comparison complexity operations utilized algorithm without taking complexity function evaluations account concentrate function simplest evaluation. results indicate complexity operators tlbo times algorithm. sparse reconstruction generally referred solving underdetermined system linear equations prior knowledge sparsity solution. applications signal compression channel estimation adaptive identiﬁcation spectrum sensing specially developed compressed sensing theory. according compressed sensing sparse vector recovered linear measurements number measurements less original dimensions sparse vector i.e. vector called k-sparse number non-zero elements error usually modeled i.i.d. zero-mean gaussian distribution speciﬁed variance. condition reliable recovery holding degree randomness measurement matrix satisﬁed gaussian binary random measurements predetermined number measures reconstruction sparse vector optimization problem. various optimization models diﬀerent algorithms developed past decade. recently metaheuristic approaches proposed optimization sparse reconstruction models. main advantage metaheuristic approaches independency properties functions used optimization model. example preferred model sparse reconstruction minimization number non-zero elements solution vector metaheuristic approaches easily optimize non-diﬀerentiable discontinuous functions. approaches function approximated e.g. algorithm genetic algorithm combined clonal selection simulated annealing respectively solve nonconvex minimization problem. furthermore another evolutionary algorithm based soft-threshold measurement errors that minimization leads ﬁdelity discovered solution measurements finally constant regularization coeﬃcient making balance sparsity ﬁdelity. indeed ﬁnding appropriate value model tedious task. advanced approach separation model objective functions utilization multiobjective method ﬁnding good balance sparsity-inducing ﬁdelity functions paper sake simplicity target model realized valid amount easily approachable unit power ﬁdelity term. addition value following ﬁrst demonstrate eﬃciency proposed algorithm respect conventional evolutionary algorithms highlight possible advantage respect state-of-the-art sparse reconstruction algorithms. first experiment conducted scenarios case noiseless measurements decision variables measurements. ﬁrst scenario nonzero elements sparse vector selected random valued i.i.d. gaussian distribution zero mean unit variance. also case measurement matrix zero-mean gaussian random matrix i.i.d. elements normalized columns. second scenario desired sparse vector binary nonzero unit elements distributed random among variables case measurement matrix also binary matrix equal probability values. regularization coeﬃcient adjusted ﬁrst second scenarios respectively. gaussian binary scenarios parameters ﬁxed pso-cc respectively. number particles ﬁxed algorithms. figure shows averaged convergence curve test algorithms trials diﬀerent sparse vector diﬀerent measurement matrix trial. shown binary scenario proposed algorithm converges lowest cost value function evaluations except algorithm algorithms trapped local optimum solution slow convergence inferred figure gaussian scenario mentioned number function evaluations enough algorithm capture cost value algorithm. convergence curve algorithms omitted scenario poor performance similar binary case. table summarizes distortion exact optimal solution states. furthermore runtime included comparison. expected gaussian scenario distortion algorithms approximately same algorithm signiﬁcantly lower distortion binary case. moreover scenarios algorithm less runtime competitive algorithm. interior-point-based optimization method bayesian method laplace priors smoothly approximates norm algorithm minimizes approximated version nonconvex function norm settings previous experiment; dimension problem algorithms implemented parameters algorithm leaved unchanged. despite previous experiment measurements contaminated noise. variance noise gaussian binary scenarios respectively. nmse curves diﬀerent number nonzero elements plotted figure depicted figure case gaussian sparse vector gaussian measurements proposed algorithm outperforms algorithms except greedy ones range sparsity level less nonzero elements. better performance greedy approaches expense prior knowledge number nonzero elements. fact available information applications. side depicted figure binary scenario algorithm better performance algorithms optimal sparse solution less nonzero elements. despite gaussian scenario binary signals algorithms unstable performance since identiﬁcation nonzero elements values generally hard greedy approaches. price outstanding performance evolutionary algorithm large runtime even several order magnitudes sparse reconstruction algorithms. worst runtime among dedicated algorithms approximately around second algorithm. parallel computing main technique reduction runtime open problems order approach real-time implementation population-based approaches multiuser multiple-input-multiple-output communication system fundamental technology wireless networks. system modeled linear equations where elements diagonal matrix rm×m small-scale fading coeﬃcients modeled matrix cm×d i.i.d. complex random variables driven gaussian distribution mean zero unit variance. elements vector transmitted samples antenna consists received samples noise modeled receiver side recent developments mu-mimo based large number antennas support several mss. number antennas scales interference among vanishes simple linear precoding/combinig methods provide near-capacity performance however deployment large number radio frequency chains antenna leads high cost energy consumption. antenna selection tackle issues number rf-chains reduced selecting optimal subset antennas deactivating rest them. well-known selection criterions based maximization achievable throughput maximization minimum singular value selected channel matrix. according case utilization zero-forcing precoder minimum received signal-to-noise ratio among lower bounded scale squared minimum singular value selected channels transmission. maximization parameter leads maximization minimum received among equivalently minimization error rate. hence network terminology minimum quality service guaranteed criteria. fitness function maximization formulated following combinatorial model selected antennas deactivated antennas. operator calculates singular values selected channels cardinality vector constrained predeﬁned number active maximization. addition algorithm implemented continuous domain. initial solutions produced randomly range uniform distribution. modiﬁcation mapping continuous decision variables binary digits function evaluation step algorithm mapping simply accomplished replacement k-largest decision variables integer value context wireless communication speed optimization main challenge specially large-scale antennas. branch bound method developed maximization bab-based algorithms approach exact optimum solution reduced complexity respect exhaustive search algorithm considered large-scale antenna selection according results algorithm provides order magnitude reduction required number function evaluations comparison however reduction suﬃciently enough large scales. addition algorithm developed selection square matrix. words number selected antennas always equal number mss. metaheuristic algorithms eﬃcient alternative approximation optimal solution signiﬁcantly less complexity well ﬂexility number selected antennas. performance test algorithms competitive proposed algorithm expect algorithm. focus compare results algorithm recently developed binary optimizer large-scale antenna selection based genetic algorithm simulations parameters algorithm algorithm population size according observations utilized parameter values ga-based algorithm maximizing throughput also eﬃcient maximization. hence population size mutation vectors best solution. number antennas number single-antenna ﬁxed respectively. results averaged channel realizations normalised channel vectors trial uniformly distributed hexagonal cell radius around path loss exponent mean shadowing attenuation modeled lognormal distribution ﬁxed figure shows convergence curves cases i.i.d. correlated channels. correlation among antennas changes i.i.d. small-scale fading channel correlated where rd×d exponential correlation matrix deﬁned α|i−j| channel element. experiment correlation coeﬃcient number selected antennas shown ﬁgure algorithm performs slightly better outperform ga-based algorithm i.i.d. correlated channels. worth mentioning that parameters system exhaustive search requires while proposed algorithm converges function evaluations. means order csi. similar results here case maximization performance antenna selection algorithms also degrades signiﬁcantly quality decreases small values instance minimum singular value selected channel ga-based algorithms reduced respectively. results indicate similar sensitivity algorithms deﬁciency. lected antennas. figure shows achieved msvs four levels active antennas i.e. algorithms stopped function evaluations. channels uncorrelated available perfect csi. approximately performance algorithms indicates similar sensitivity parameters. results also compared case full array without antenna selection. obtained msvs algorithms optimal subset antennas approximately full antennas without selection. main advantage proposed algorithm computational speed. figure illustrates average runtime diﬀerent number active antennas increases runtime ga-based algorithms increases approximately rate rate algorithm roughly half them. attention runtime reduced decreasing without signiﬁcant loss performance. moreover number antennas running times signiﬁcantly less similar respective performances illustrated dimensional antenna array. example antennas active antennas required convergence takes average seconds running time algorithms respectively. grouping method update rule pivotal inﬂuence functioning metaheuristic algorithms. shown numerical results benchmark problems proposed dialectical grouping signiﬁcant performance optimization speciﬁc functions large number competitive solutions. however ﬁnding common feature possible problems metaheuristc algorithm superior performance really hard. proposed interactions lead simple delicate step-size mechanism. numerous experiments conﬁrm convergence algorithm suggested mechanism. mathematical proof optimality step-sizes challenging task. general analysis metaheuristic algorithms challenging problem existence various sources random operations. however hopeful proposed deterministic interactions speculation would simplify analysis algorithm. parameters algorithm easily tuned integer identities number possible pairs inﬂuence performance. nevertheless adaptive scheme adjustment parameters optimization process optimality remains open problem. future research directions extension algorithm multiobjective scenarios applications engineering scientiﬁc problems interest. philosophical paradigm thesis-antithesis-synthesis dialectical thinking modes promises efﬁcient search approach. inspired speculative practical thinking modes developed population-based optimization approach. speculative thinking assigned high quality solutions modeled boosts exploration capability proposed algorithm. thinking mode particle/thinker looks another community solution largest distance similar quality contradiction practical thinking assigned quality solutions exploits eﬃciency best solution idealistic antithesis selecting smaller distance detected antitheses used reference point reformation solutions/theses. uniformly distributed step-sizes negligible bias toward antithesis utilized explorative speculations biased gaussian distribution used step-sizes exploitive practices. results indicate eﬃciency proposed optimization scheme low-complexity operators.", "year": 2017}