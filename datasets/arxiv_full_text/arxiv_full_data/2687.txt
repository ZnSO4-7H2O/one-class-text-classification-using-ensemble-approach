{"title": "Symmetry Learning for Function Approximation in Reinforcement Learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "In this paper we explore methods to exploit symmetries for ensuring sample efficiency in reinforcement learning (RL), this problem deserves ever increasing attention with the recent advances in the use of deep networks for complex RL tasks which require large amount of training data. We introduce a novel method to detect symmetries using reward trails observed during episodic experience and prove its completeness. We also provide a framework to incorporate the discovered symmetries for functional approximation. Finally we show that the use of potential based reward shaping is especially effective for our symmetry exploitation mechanism. Experiments on various classical problems show that our method improves the learning performance significantly by utilizing symmetry information.", "text": "paper explore methods exploit symmetries ensuring sample eﬃciency reinforcement learning problem deserves ever increasing attention recent advances deep networks complex tasks require large amount training data. introduce novel method detect symmetries using reward trails observed episodic experience prove completeness. also provide framework incorporate discovered symmetries functional approximation. finally show potential based reward shaping especially eﬀective symmetry exploitation mechanism. experiments various classical problems show method improves learning performance signiﬁcantly utilizing symmetry information. reinforcement learning task training agent perform optimally environment using reward observation signals perceived upon taking actions change environment dynamics. learning optimal behavior inherently diﬃcult challenges like credit assignment exploration-exploitation trade need made converging solution. many scenarios like training rover move martian surface cost obtaining samples learning high sample eﬃciency important subproblem deserves special attention. often case environment intrinsic symmetries leveraged agent improve performance learn eﬃciently. example cart-pole domain state action space symmetric respect reﬂection plane perpendicular direction motion cart fact many environments number symmetry relations tend increase dimensionality state space. instance simple case grid world dimension exist fold symmetries. provide substantial gains sample eﬃciency learning would ideally need consider equivalence classes formed induced symmetry relations. however discovering symmetries challenging problem owing noise observations complicated dynamics environment. recent advances deep reinforcement learning demonstrated seemingly complex tasks like game play atari control simulated physics environments etc. pose challenges form large state action spaces diﬃculty learning good representations handled well deep networks functional approximators. training networks however requires large amounts data learning parameters necessitates coming careful update procedures deﬁning right objectives) topic study right. points fact essential come methods ensure sample eﬃciency function approximation based builds premise work. best knowledge ﬁrst motivate symmetry aforementioned context. paper investigate methods discovering state space symmetries inclusion prior information suitable cost objective. also show method dovetails seamlessly framework using potential based reward shaping help reduce size data structures required symmetry detection provides additional information establishing robust similarity estimates. experiments various classical control settings validate approach agents showing signiﬁcant gains sample eﬃciency performance using symmetry information. model minimization literature closely related ﬁeld symmetries deﬁned using equivalence relations state state action spaces generalized notion symmetries presented allows greater state space reduction. however approaches require symmetries environment explicitly stated handle discovery. methods like conditionally terminating sequences formed observed reward action sequences directly estimate state equivalence closely related methods symmetry detection however fail even relatively simple environments like cart-pole action mappings symmetry transformation invariant. methods prohibitively high overheads moreover none address aforementioned issues context using function approximation deep many realistic scenarios tasks needed learned agent composed several subtasks form hierarchy across state space. methods intrinsic structure fall framework temporally abstract actions although methods require extensive domain knowledge speciﬁcation agent’s designer. tried automate process learning agent observations approach suboptimal fail discover abstractions recent approaches incorporating linear function approximation shown eﬀective. value function generalization goal setting also gives better generalization unseen goals function approximation setting. method potential augmented works ﬁnding symmetries sub-goal spaces. surjection deﬁned tuple surjections f{gs particular satisﬁes requirements firstly preserves reward function secondly commutes transition dynamics ]bh|s notation bh|s denote projection equivalence classes partition relation symmetries formally deﬁned setting know underlying system dynamics consequently know advance thus cannot perform model reduction. workaround would estimate using agent’s experience estimated equivalent classes drive identical updates equivalent state-action pairs process learning optimal policy. takes actions. notion reward shaping roots behavioral psychology shown shaping helpful learning optimal policies shaping rewards carefully designed ensure policy invariance property holds shown reward shaping function potential based i.e. section present approach simultaneously discovering exploiting symmetries framework. goal incorporate state action space symmetry information early learning regret minimized optimal policy learned sample eﬃcient manner. main components involved method symmetry detection learning symmetry based priors. although work applied form functional approximation focus function approximation using deep feed forward networks study illustrate merits solutions. elaborate components next. eﬃciently compute similarities state action pairs auxiliary structure called reward history tree stores preﬁxes reward sequences length state action pairs observed policy execution. reward history tree labeled rooted tree here nodes node labeled reward observation. directed edges deﬁned following sequence reward labels obtained traversing starting root denoted directed edge appended reward label form preﬁx observed reward sequence. additionally node maintains list state action occurrence frequency tuples thus node store tuple reward sequence observed times starting breadth ﬁrst traversal maintaining arrays asa) similar given length threshold parameters thus propose symmetry environment deduced reward structure. assumption backed fact many real life applications provide rich information reward dynamics amenable method discovering symmetries. simple example would robot trying approach well deﬁned object placed distance fraction pixels covered object robot’s visual sensors would provide approximate notion distance robot object thus good reward signal. moreover allow discern locations equally rewarding environment thus uncovering symmetry. also notice reward shaping convert otherwise bland signal informational value. since method dependent estimating similarities state action space measuring fraction common observed reward trails shaping helps distinguish pairs making rewards suﬃciently distinct consequently preventing spurious similarity estimates. note deﬁnition enables ﬁnding state action symmetries even actions invariant symmetry transform right previous work unable this. finally present theorem establishes completeness similarity measure theorem equivalent pairs symmetry induces informally completeness asserts state action pair equivalent given symmetry identiﬁable using similarity measure proof theorem given appendix interested induces coarsest partition leads highest sample eﬃciency ideally wish update pairs partition parallel given observation. finally note method theorem strictly require symmetries level. method general homomorphic reductions associated equivalence classes symmetry inclusion priors learning function approximator used. found symmetric state action pairs χsym next task would information training function approximator network. speciﬁcally want network identical outputs symmetric state-action pairs. achieved constraining network learn identical representations symmetric pairs layers. intuitive moving towards goal would directly χsym inducing hard constraints minimizing appropriate loss based one-step targets however becomes diﬃcult optimize problem many constraints methods like might required. moreover since estimated similarity pairs guaranteed true better solve softer version problem introducing symmetry constraints additional loss term practice stochastic gradient descent loss minimization using mini batches represents familiar q-learning gradient function approximation. deﬁned prevent network destroying knowledge gained current episode. present proposed symmetric version algorithm. probability select action otherwise select argmaxaq execute action observe reward state store transition sample random minibatch find batch symmetric pairs targets perform gradient descent step order validate approach compare performance diﬀerent agents using fully connected deep feed forward neural networks hidden layers function approximation. relu non-linearity used activation hidden node.the parameters involved symmetry learning framework found using grid search. agent move. upon taking desired step chance landing expected state chance landing randomly chosen adjacent state. episode starts randomly chosen state discount factor exploration goal state chosen randomly start iteration. average reward time step potential settings plotted fig. comparison previous work girgin naive agent uses symmetry information also included. table gives number episodes required convergence optimal policy setup figure agent running algorithm learns optimal policy much faster baseline previous work. performed sided welsh’s t-test check performance agents using approach agents using methods. scenarios extremely signiﬁcant alg. indeed capable using symmetry information eﬃciently. next reward shaping potentials informative also convey nearness goal state thus agents learning former tend converge faster finally also evident eﬀects adding symmetry priors learning become signiﬁcant size grid increases. next analyze method performs dimensionality grid increased grid world similar vein. notice domain fold symmetry. type potentials used γ|x−xg|+|y−yg|+|z−zg|. maximum input average reward time step plotted fig. evident form plot table increase dimensionality impacts performance method relatively mildly comparison methods fail converge within episodes makes intuitive sense grid size dimensions total values learn whereas symmetries model reduction would required learn thus bounding position discrete dimension intuitively shaping motivates agent keep coordinates near stable cart-pole conﬁgurations. modify ‘cartpole-v’ environment available openai platform experiments. algorithms experimented agent proposed symmetric variant symmetric agent. discretization level experiments maximum episode length replay memory size mini batch size used. agents completely random policy ﬁrst episodes ensure proper initialization figure shows variation total reward obtained number episodes averaged iterations three diﬀerent parameter settings. table gives mean maximum values total rewards obtained episode algorithms symdqn clearly performs much better metrics. perform welsh’s test measure statistical signiﬁcance observations metrics comfortably reject hypothesis symmetry inclusion method change performance. although main objective ensure sample eﬃciency would like point method compares favorably baseline agents terms training times. grid-world at-most times slower whereas cart-pole at-most times slower. paper proposed novel framework discovering environment symmetries exploiting paradigm function approximation framework consists main components similarity detecting procedure calculates similarity estimates state action pairs reward observations similarity incorporating component promotes learning symmetric policies imposing symmetry cost. approach scalable requires minimal additional time space overheads proved completeness similarity measure. shown eﬃcacy method extensive experimentation using deep nets grid-world cart-pole domains. shown beneﬁts using symmetry information learning profound dimensionality environment increases scope multiple symmetry occurrences. finally also noticed important role reward shaping played method. random variables observed denoting states actions rewards respectively following symmetric policy episode length ˆnsa number times ˆnσsa denote frequency observing reward transits though state action pair ˆnσsa) πsat sequence transit number episode runs starting t+).clearly sequences obey transition dynamics former driven latter conforms given symmetry t=). since started symmetric state action pairs take symmetrical transition ..re reward sequence observed must exactly identical hence ˆnσsa ˆnσsa reward based observations symmetric pairs must identical particular finally deﬁnition χils asa) coupled execution times. since uncoupled case converge distribution behavior lim|episodes|→∞ theorem proved.", "year": 2017}