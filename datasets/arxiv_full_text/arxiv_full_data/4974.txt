{"title": "Gradient Episodic Memory for Continual Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.", "text": "major obstacle towards poor ability models solve problems quicker without forgetting previously acquired knowledge. better understand issue study problem continual learning model observes examples concerning sequence tasks. first propose metrics evaluate models learning continuum data. metrics characterize models test accuracy also terms ability transfer knowledge across tasks. second propose model continual learning called gradient episodic memory alleviates forgetting allowing beneﬁcial transfer knowledge previous tasks. experiments variants mnist cifar- datasets demonstrate strong performance compared state-of-the-art. starting point supervised learning collect training example composed feature vector target vector supervised learning methods assume example identically independently distributed sample ﬁxed probability distribution describes single learning task. goal supervised learning construct model used predict target vectors associated unseen feature vectors accomplish this supervised learning methods often employ empirical risk minimization principle loss function found minimizing |dtr| penalizing prediction errors. practice often requires multiple passes training set. major simpliﬁcation deem human learning. stark contrast learning machines learning humans observe data ordered sequence seldom observe example twice memorize pieces data sequence examples concerns different learning tasks. therefore assumption along hope employing principle fall apart. fact straightforward applications lead catastrophic forgetting learner forgets solve past tasks exposed tasks. paper narrows human-like learning description above. particular learning machine observe example example continuum data besides input target vectors learner observes task descriptor identifying task associated pair pti. importantly examples drawn ﬁxed probability distribution triplets since whole sequence examples current task observed switching next task. goal continual learning construct model able predict target associated test pair setting face challenges unknown transfer learning tasks continuum related exists opportunity transfer learning. would translate faster learning tasks well performance improvements tasks. rest paper organized follows. section formalize problem continual learning introduce metrics evaluate learners scenario. section propose model learn continuums data alleviates forgetting transferring beneﬁcial knowledge past tasks. section compare performance state-of-the-art. finally conclude reviewing related literature section offer directions future research section source code available https//github.com/ facebookresearch/gradientepisodicmemory. focus continuum data triplet formed feature vector task descriptor target vector yti. simplicity assume continuum locally every triplet satisﬁes iid∼ pti. observing data example example goal learn predictor queried time predict target vector associated test pair test pair belong task observed past current task task experience future. task descriptors important component framework collection task descriptors simplest case task descriptors integers enumerating different tasks appearing continuum data. generally task descriptors could structured objects paragraph natural language explaining solve i-th task. rich task descriptors offer opportunity zero-shot learning since relation tasks could inferred using task descriptors alone. furthermore task descriptors disambiguate similar learning tasks. particular input could appear different tasks require different targets. task descriptors reference existence multiple learning environments provide additional contextual information examples. however paper focus alleviating catastrophic forgetting learning continuum data leave zero-shot learning future research. next discuss training protocol evaluation metrics continual learning. literature learning sequence tasks describes setting number tasks small number examples task large iii) learner performs several passes examples concerning task metric reported average performance across tasks. contrast interested more human-like setting number tasks large number training examples task small iii) learner observes examples concerning task once report metrics measure transfer forgetting. therefore training time provide learner example time form triplet learner never experiences example twice tasks streamed sequence. need impose order tasks since future task coincide past task. besides monitoring performance across tasks also important assess ability learner transfer knowledge. speciﬁcally would like measure backward transfer inﬂuence learning task performance previous task hand exists positive backward transfer learning task increases performance preceding task hand exists negative backward transfer learning task decreases performance preceding task large negative backward transfer also known forgetting. forward transfer inﬂuence learning task performance future task particular positive forward transfer possible model able perform zero-shot learning perhaps exploiting structure available task descriptors. principled evaluation consider access test tasks. model ﬁnishes learning task evaluate test performance tasks. construct matrix rt×t test classiﬁcation accuracy model task observing last sample task letting vector test accuracies task random initialization deﬁne three metrics larger metrics better model. models similar preferable larger fwt. note meaningless discuss backward transfer ﬁrst task forward transfer last task. ﬁne-grained evaluation accounts learning speed build matrix rows tasks evaluating often. extreme case number rows could equal number continuum samples then number test accuracy task observing i-th example continuum. plotting column results learning curve. section propose gradient episodic memory model continual learning introduced section main feature episodic memory stores subset observed examples task simplicity assume integer task descriptors index episodic memory. using integer task descriptors cannot expect signiﬁcant positive forward transfer instead focus minimizing negative backward transfer efﬁcient episodic memory. practice learner total budget memory locations. number total tasks known allocate memories task. conversely number total tasks unknown gradually reduce value observe tasks simplicity assume memory populated last examples task although better memory update strategies could employed following consider predictors parameterized deﬁne loss memories k-th task obviously minimizing loss current example together results overﬁtting examples stored alternative could keep predictions past tasks invariant means distillation however would deem positive backward transfer impossible. instead losses inequality constraints avoiding increase allowing decrease. contrast state-of-the-art model therefore allows positive backward transfer. speciﬁcally observing triplet solve following problem following make observations solve efﬁciently. first unnecessary store predictors long guarantee loss previous tasks increase parameter update second assuming function locally linear memory representative examples past tasks diagnose increases loss previous tasks computing angle loss gradient vector proposed update. mathematically rephrase constraints inequality constraints satisﬁed proposed parameter update unlikely increase loss previous tasks. hand inequality constraints violated least previous task would experience increase loss parameter update. violations occur propose project proposed gradient closest gradient norm) satisfying constraints therefore interested since term constant. variables number observed tasks far. solve dual problem recover projected gradient update practice found adding small constant biased gradient projection updates favoured beneﬁtial backwards transfer. algorithm summarizes training evaluation protocol continuum data. pseudo-code includes computation matrix containing sufﬁcient statistics compute metrics described section causal compression view interpret model learns subset correlations common distributions furthermore used predict target vectors associated previous tasks without making task descriptors. desired feature causal inference problems since causal predictions invariant across different environments therefore provide compressed representation distributions mnist permutations variant mnist dataset handwritten digits task transformed ﬁxed permutation pixels. dataset input distribution task unrelated. incremental cifar variant cifar object recognition dataset classes task introduces classes. total number tasks task concerns examples disjoint subset classes. here input distribution similar tasks different tasks require different output distributions. datasets considered tasks. mnist datasets task examples different classes. cifar dataset task examples different classes. model observes tasks sequence example once. evaluation task performed test partition dataset. mnist tasks fully-connected neural networks hidden layers relu units. cifar tasks smaller version resnet three times less feature maps across layers. also cifar network ﬁnal linear classiﬁer task. simple leverage task descriptor order adapt output distribution subset classes task. train networks baselines using plain mini-batches samples. hyper-parameters optimized using grid-search best results model reported. independent predictor task. independent predictor architecture single times less hidden units single. independent predictor initialized random clone last trained predictor icarl class-incremental learner classiﬁes using nearestexemplar algorithm prevents catastrophic forgetting using episodic memory. icarl requires input representation across tasks method applies experiment cifar. figure summarizes average accuracy backward transfer forward transfer datasets methods. provide full evaluation matrices appendix overall performs similarly better multimodal model minimizes backward transfer exhibiting negligible positive forward transfer. figure shows evolution test accuracy ﬁrst task throughout continuum data. exhibits minimal forgetting positive backward transfer cifar. overall performs signiﬁcantly better continual learning methods like spending less computation gem’s efﬁciency comes optimizing number variables equal number tasks instead optimizing number variables equal number parameters gem’s bottleneck necessity computing previous task gradients learning iteration. table shows ﬁnal cifar- experiment icarl function episodic memory size. also seen table ﬁnal increasing function size episodic memory eliminating need carefully tune hyper-parameter. outperforms icarl wide range memory sizes. table illustrates importance memory pass data mnist rotations experiment. multiple training passe exacerbate catastrophic forgetting problem. instance last column table model shown examples task times switching next task. table shows memory-less methods exhibit higher negative leading lower acc. hand memory-based methods lead higher number passes data increases. however suffers less negative leading higher acc. finally relate performance best possible performance proposed datasets ﬁrst table reports single trained data tasks. mimics usual multi-task learning mini-batch contains examples taken random selection tasks. comparing ﬁrst last table matches oracle performance upper-bound provided learning minimizes negative bwt. continual learning also called lifelong learning considers learning sequence tasks learner retain knowledge past tasks leverage knowledge quickly acquire skills. learning setting implementations theoretical investigations although latter ones restricted linear models. work revisited continual learning proposed focus realistic setting examples seen once memory ﬁnite learner also provided task descriptors. within framework introduced metrics training testing protocol algorithm outperforms current state-of-the-art terms limiting forgetting. task descriptors similar spirit recent work reinforcement learning task goal descriptors also input system. commai project shares motivations focuses highly structured task descriptors strings text. contrast focus problem catastrophic forgetting several approaches proposed avoid catastrophic forgetting. simplest approach neural networks freeze early layers cloning ﬁne-tuning later layers task relates methods leverage modular structure network primitives shared across tasks unfortunately hard scale methods lots modules tasks given combinatorial number compositions modules. approach similar regularization approaches consider single model modify learning objective prevent catastrophic forgetting. within class methods approaches leverage synaptic memory learning rates adjusted minimize changes parameters important previous tasks. approaches instead based episodic memory examples previous tasks stored replayed maintain predictions invariant means distillation related latter approaches unlike them allows positive backward transfer. generally variety setups machine learning literature related continual learning. multitask learning considers problem maximizing performance learning machine across variety tasks setup assumes simultaneous access tasks once. similarly transfer learning domain adaptation assume simultaneous availability multiple learning tasks focus improving performance particular. zero-shot learning one-shot learning performing well unseen tasks ignore catastrophic forgetting previously learned tasks. curriculum learning considers learning sequence data sequence tasks sorted increasing difﬁculty. formalized scenario continual learning. first deﬁned training evaluation protocols assess quality models terms accuracy well ability transfer knowledge forward backward tasks. second introduced simple model leverages episodic memory avoid forgetting favor positive backward transfer. experiments demonstrate competitive performance state-of-the-art. three points improvement. first leverage structured task descriptors exploited obtain positive forward transfer second investigate advanced memory management third iteration requires backward pass task increasing computation time. exciting research directions extend learning machines beyond continuums data. mcclelland mcnaughton o’reilly. complementary learning systems hippocampus neocortex insights successes failures connectionist models learning memory. psychological review sutton modayil delp degris pilarski white precup. horde scalable real-time architecture learning knowledge unsupervised sensorimotor interaction. international conference autonomous agents multiagent systems report hyper-parameter grids considered experiments. best values mnist rotations mnist permutations cifar- incremental experiments noted accordingly parenthesis. details please refer implementation linked main text. single section report evaluation matrices model dataset. ﬁrst matrix baseline test accuracy training starts. rest entries matrix report test accuracy j-th task ﬁnishing training i-th task.", "year": 2017}