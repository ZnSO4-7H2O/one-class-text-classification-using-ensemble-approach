{"title": "Parallelizing Linear Recurrent Neural Nets Over Sequence Length", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "Recurrent neural networks (RNNs) are widely used to model sequential data but their non-linear dependencies between sequence elements prevent parallelizing training over sequence length. We show the training of RNNs with only linear sequential dependencies can be parallelized over the sequence length using the parallel scan algorithm, leading to rapid training on long sequences even with small minibatch size. We develop a parallel linear recurrence CUDA kernel and show that it can be applied to immediately speed up training and inference of several state of the art RNN architectures by up to 9x. We abstract recent work on linear RNNs into a new framework of linear surrogate RNNs and develop a linear surrogate model for the long short-term memory unit, the GILR-LSTM, that utilizes parallel linear recurrence. We extend sequence learning to new extremely long sequence regimes that were previously out of reach by successfully training a GILR-LSTM on a synthetic sequence classification task with a one million timestep dependency.", "text": "recurrent neural networks widely used model sequential data non-linear dependencies sequence elements prevent parallelizing training sequence length. show training rnns linear sequential dependencies parallelized sequence length using parallel scan algorithm leading rapid training long sequences even small minibatch size. develop parallel linear recurrence cuda kernel show applied immediately speed training inference several state architectures abstract recent work linear rnns framework linear surrogate rnns develop linear surrogate model long short-term memory unit gilr-lstm utilizes parallel linear recurrence. extend sequence learning extremely long sequence regimes previously reach successfully training gilr-lstm synthetic sequence classiﬁcation task million timestep dependency. recurrent neural networks widely used sequence modelling tasks domains natural language processing speech recognition reinforcement learning rnns including popular variants long short-term memories introduced hochreiter schmidhuber gated recurrent units introduced contain non-linear dependency sequential inputs. non-linear dependencies create ﬂexible class models limit feasibility training rnns long sequences sequence element must processed sequentially. modelling sequences thousands millions elements important domains robotics remote sensing control systems speech recognition medicine ﬁnance. serial evaluation inefﬁciency problem usually mitigated parallelizing forward backward pass minibatch inputs. without minibatches evaluation sequence matrix-vector multiplications. minibatches transform computation sequence efﬁcient matrix-matrix multiplications speed-up brings several disadvantages. model size often limited memory size running forward backward pass minibatch requires memory linear minibatch size. grouping data minibatches increases latency pass reduces rate optimization steps. finally training larger minibatches damages generalization ability given effects desirable obtain high training throughput small minibatches. persistent rnns novel implementation achieve high utilization small minibatch sizes recurrent state larger elements even persistent rnns become limited serial evaluation inefﬁciency smaller hidden sizes. numerous prior works shown strong performance neural sequential models linear dependence earlier sequence elements. balduzzi ghifary investigated rnns elementwise linear recurrence relations developed linear variants lstm perform similarly standard non-linear rnns text generation tasks. bradbury kalchbrenner gehring oord successfully applied networks convolutions sequences tasks machine translation language modelling audio generation. works observed order magnitude increase training throughput compared alternatives. convolutional sequence models typically rely either attention mechanism recurrent layer integrate information scales larger ﬁlter width. introduction recurrent layer prevents full parallelization sequence length attention mechanisms expensive apply long sequences online inference cases. linear recurrence speciﬁc instance general form computation known scan. scans reductions computations involving repeated application binary operator array data. computing maximum array example reduction cumulative common example scan operation. throughout work scan initial value deﬁned reduction array initial value denoted reduce ﬁnal element scan. despite dependent computation graph algorithms exist parallelize scans reductions associative blelloch shows ﬁrst order recurrences form parallelized parallel scan algorithm three conditions considering familiar operations linear algebra associative operation vector addition semiassociative operation matrix-vector multiplication associative operation matrix-matrix multiplication satisfy blelloch’s three conditions allowing λtht− evaluated parallel time steps vectors square matrices investigate idea deliver following contributions provide implementation parallel linear recurrence algorithm cuda kernel show speeds training qrnn zhang simple recurrent unit architectures factors describe several recent linear rnns described linear surrogates non-linear architectures. introduce linear surrogate lstm show able train speedup compared cudnn lstm parallel linear recurrence algorithm. cpscan csscan cost matrix multiplication parallel algorithm counter-act parallel speedups sufﬁciently large hidden states lead slower algorithm overall. avoid problem consider diagonal matrices case matrix-matrix matrix-vector multiplication cost proportional cpscan csscan gives parallel speedup factor assuming cpscan csscan considering diagonal matrices write linear recurrence indicates elementwise multiplication. limiting diagonal seem like severe constraint several reasons beyond favorable parallelization performance. relatively neural network models separate recurrent matrices sequence element using separate matrices would require potentially prohibitive memory. applying matrix sequence element also unappealing considering matrix multiplication thought rotation scaling. rotation every element seems unlikely useful scaling exactly what’s captured diagonal vectors recurrent coefﬁcient vectors provide enough ﬂexibility implement schemes exponential moving averages gating mechanism. backpropagation equations center around linear recurrence reverse order original sequence. allows parallelizing forwards backwards pass linear sequence length. work implemented parallel linear recurrence cuda kernel bindings tensorflow framework. warp acts processor means algorithmic theoretical parallelization speedup factor several hundred. lanes warp work different elements recurrence vector parallel. implementation details mean peak performance obtained sequences least several thousand steps least element vector. parallel linear recurrence used construct wide variety differentiable modules evaluated parallel. common applications linear recurrence include gating schemes exponential moving averages. although linear recurrence values depend linearly previous elements stacking linear recurrent layers separated non-linearities allows non-linear dependence past. sense non-linear depth linear recurrent network number layers sequence length. gilr layer applies non-linear transform sequence element accumulates sequence elements non-linear gating mechanism. gate uses sigmoid activation function give values reasonable gating semantics impulse activation function stacking gilr layers allows rich non-linear dependence previous events still taking advantage fast parallel sequence evaluation. consider evaluating recurrence inputs hidden units sequence length minibatch size using serial evaluation strategy. iterations naive approach performs matrix multiplications. larger matrix multiplications achieve higher throughput less overhead better approach computes ahead time single matrix multiply. non-linear recurrence forces even better approach perform potentially small matrix multiplications serial. makes serial performance heavily dependent minibatch size. consider gilr noting matrix-vector multiplications iteration rnn. intermediate variables evaluated single matrix multiplication each. given computed using parallel linear recurrence vectors elements. rather small operations gilr evaluated sequence elements large matrix multiplications parallel linear recurrence. gilr performance much less dependent batch size matrix multiplication kernel sees \"effective batch size\" typically large. rnns learn transition function combines previous state input compute current state non-linear prevents application parallel linear recurrence algorithm forces slow serial evaluation. work around inefﬁciency note serves dual purposes. serves input summarizing previous inputs serves output passed layers network. decouple uses introduce independent variables purpose passed onto layers network introduce linear surrogate passed onto next state still able choose non-linear limitation must linearly computable. refer class model linear surrogate qrnns ls-rnns using ˜ht− wkxt−k ...wxt− strongly typed rnns ls-rnns xt−. although rule ls-rnns often parallelized sequence length either convolution linear recurrence. lstm state since depends linearly surrogate needed non-linear dependence needs linear surrogate. introducing gilr layer surrogate obtain gilr-lstm inputs hidden size gilr-lstm contains parameters equivalently sized lstm handle mapping generally ls-rnn contains parameters underlying well additional parameters compute linear surrogate. perform several experiments. first parallel linear recurrence kernel able achieve higher throughput serial implementation applied long sequences. secondly conﬁrm kernel speedup translates speedup ls-rnns qrnns. order illustrate linearization necessarily come cost expressibility show gilr-lstm architecture computed parallel linear recurrence algorithm able train signiﬁcantly faster optimized lstm implementation pathological long-term dependency problem original lstm paper ﬁrst illustrate throughput advantage parallel scan algorithm evaluating linear recurrence. minibatch comprised sequences length deﬁne number events throughput number events processed second. implement cuda table parallel kernel speedup variety ls-rnns implemented stacked layers hidden units. keep memory usage constant ﬁxing minibatch size sequence length kernels evaluates parallel linear recurrence described algorithm evaluates linear recurrence serial sequence length parallel features minibatch. performance kernel depends factors sequence length product number features minibatch size. performance measurements experiment made directly kernel level avoiding overhead tensorflow. parallel kernel distinct advantage long sequence lengths speedup factor shown table parallel kernel perform well short sequence lengths overhead multiple passes data communication processors. several recently introduced ls-rnns accelerated parallel linear recurrence algorithm. implemented srus qrnns gilr-lstms computed either standard serial linear recurrence algorithm parallel linear recurrence. methods compute identical recurrence switching serial parallel implementation cause numerical changes takes single line code changes. notably srus qrnns claim order magnitude speedup compared cudnn lstm implemented serial linear recurrence. speedup parallel linear recurrence applies existing speedup. timed train throughput linear time pass also makes results applicable forwards performance. however parallel linear recurrence accelerate inference scenarios entire input sequence known start inference phase. controlled memory usage within experiments ﬁxing minibatch size sequence length chose popular architecture consisting stacked layers hidden units input size table shows throughput advantage using parallel linear recurrence compared serial linear recurrence reaches simpler architectures affected switch parallel kernel. particularly clear case qrnn including wider convolutional ﬁlters results time spent outside linear recurrence therefore reduces speedup linear recurrence parallelization. strengths lstm capable dealing long-term dependencies. order demonstrate gilr-lstm also able handle long-term dependencies tackle canonical example inference many time steps hochreiter schmidhuber show fact gilr-lstm able outperform cudnn lstm extend sequence figure structure synthetic example gilr-lstm architecture used tackle feed one-hot unit vectors chosen uniformly random class determined ﬁrst vector ﬁxed direction. sign determines class. diagram rounded block indicates cell whilst square indicates linear unit. lengths orders magnitude longer dealt previously. input consists sequences length element randomly chosen one-hot vector p-dimensional space. ﬁrst vector sequence always either sequential model must read entire sequence output sign ﬁrst sequence element. sequence classiﬁcation problem requires remembering ﬁrst element length sequence early rnns struggled small dozen. original formulation problem dimensionality input equal since would make size input data grow impractically large long sequences vary generated sequences equal compared layer gilr-lstm hidden units layer lstm hidden units layer implemented cudnn. experiments nvidia runs conﬁguration allowing average standard deviation time number iterations convergence. continually generated random sequences serve input data. brief search learning rate batch size carried parameters allow network converge rapidly runs. criterion convergence consecutive minibatches giving accuracy. learning curves ﬁgure give support reasonable convergence criteria. longest sequence length observe cudnn lstm converging even several days’ training. results show table illustrate gilr-lstm able converge times faster cudnn lstm. somewhat surprising given lstm speciﬁcally constructed problems sort cudnn lstm implementation highly optimized gilr-lstm implemented entirely standard tensorflow exception using linear recurrence instead tensorflow symbolic loop. convergence gilr-lstm models leads conclusion non-linearities present lstm necessary solving instance figure learning curves gilr-lstm cudnn lstm architectures various sequence lengths. plot shows moving mean standard deviation classiﬁcation accuracy training runs exception single cudnn lstm million sequence length. long-term dependency problem. time convergence leads conclusion inclusion non-linearity every step incurs signiﬁcant training time slowdown. furthermore gilr-lstm able learn carry dependencies million element sequence. know million step sequence experiment longest sequential learning problem handled neural networks date. signiﬁcant portion success deep learning attributed access massive amounts computation. computation accessed highly efﬁcient parallelizable building blocks matrix multiplication convolution. recent research demonstrated linear rnns achieve similar prediction accuracy non-linear rnns wide variety tasks fraction training time. propose framework ls-rnns tame growing sequential neural nets. identify linear recurrence another parallelizable building block current future sequential models obtain signiﬁcant speedups already fast models. power parallel linear recurrence able solve sequential dependency problem multiple orders magnitude larger anything done prior. future applications parallel linear recurrence within neural nets could include parallel training memory augmented models providing sort image ﬁlter high resolution images. hope parallel linear recurrence large scale sequence modelling fast convolution algorithms image recognition. would like acknowledge kevin bowers alex meiburg co-reyes carson mcneil andy palan sören mindermann several others fruitful conversations guidance. references abadi agarwal barham brevdo chen citro corrado davis dean devin tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. amodei anubhai battenberg case casper catanzaro chen chrzanowski coates diamos deep speech end-to-end speech recognition english mandarin. arxiv preprint arxiv. merriënboer gulcehre bahdanau bougares schwenk bengio. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. diamos sengupta catanzaro chrzanowski coates elsen engel hannun satheesh. persistent rnns stashing recurrent weights on-chip. international conference machine learning pages", "year": 2017}