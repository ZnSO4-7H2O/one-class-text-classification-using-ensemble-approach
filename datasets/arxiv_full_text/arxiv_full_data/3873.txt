{"title": "Scalable Semidefinite Relaxation for Maximum A Posterior Estimation", "tag": ["cs.LG", "cs.CV", "cs.IT", "math.IT", "math.OC", "stat.ML"], "abstract": "Maximum a posteriori (MAP) inference over discrete Markov random fields is a fundamental task spanning a wide spectrum of real-world applications, which is known to be NP-hard for general graphs. In this paper, we propose a novel semidefinite relaxation formulation (referred to as SDR) to estimate the MAP assignment. Algorithmically, we develop an accelerated variant of the alternating direction method of multipliers (referred to as SDPAD-LR) that can effectively exploit the special structure of the new relaxation. Encouragingly, the proposed procedure allows solving SDR for large-scale problems, e.g., problems on a grid graph comprising hundreds of thousands of variables with multiple states per node. Compared with prior SDP solvers, SDPAD-LR is capable of attaining comparable accuracy while exhibiting remarkably improved scalability, in contrast to the commonly held belief that semidefinite relaxation can only been applied on small-scale MRF problems. We have evaluated the performance of SDR on various benchmark datasets including OPENGM2 and PIC in terms of both the quality of the solutions and computation time. Experimental results demonstrate that for a broad class of problems, SDPAD-LR outperforms state-of-the-art algorithms in producing better MAP assignment in an efficient manner.", "text": "maximum posteriori inference discrete markov random ﬁelds fundamental task spanning wide spectrum real-world applications known np-hard general graphs. paper propose novel semideﬁnite relaxation formulation estimate assignment. algorithmically develop accelerated variant alternating direction method multipliers effectively exploit special structure relaxation. encouragingly proposed procedure allows solving large-scale problems e.g. problems grid graph comprising hundreds thousands variables multiple states node. compared prior solvers sdpad-lr capable attaining comparable accuracy exhibiting remarkably improved scalability contrast commonly held belief semideﬁnite relaxation applied small-scale problems. evaluated performance various benchmark datasets including opengm terms boththe quality solutions computation time. experimental results demonstrate broad class problems sdpad-lr outperforms state-of-the-art algorithms producing better assignments efﬁcient manner. ning wide scope scenarios ranging traditional applications graph matching stereo reconstruction object detection errorcorrecting codes gene mapping etc. recent application estimating consistent object orientations noisy pairwise measurements general graphs problem well-known nphard however part importance practice large body algorithms proposed approximate estimates solving various convex relaxation formulations. among methods based convex surrogates semideﬁnite relaxation usually strictly dominates formulations based linear programming quadratic programming terms solution quality. despite superiority obtaining accurate estimates however signiﬁcant challenge limits applicability semideﬁnite relaxation paradigm real problems efﬁciency. existing general-purpose solvers handle problems small dimensionality. paper propose novel semideﬁnite relaxation approach second-order inference pairwise undirected graphical models. observation marginalization constraints typical linear programming relaxation subsumed combing semideﬁnite conic constraint small linear constraints. result admits concise nicely decoupled constraints allows develop accelerated variant alternating direction method multipliers method scalable large-scale problems. proceeding introduce notations used throughout paper. linear operator represent conjugate operator. denote matrices nonnegative entries projection operator onto symmetric matrix represent projection onto positive semideﬁnite cone. finally denote kxkf frobenius norm matrix start state conﬁgurations discrete random variables xn}. without loss generality assume takes values discrete state consider pairwise markov random ﬁeld parameterized potentials vertices edges energy associated given estimation discrete sets np-hard combinatorial problem cast integer quadratic program denote binary vector estimation equivalent following integer program. hardness arises aspects binary-valued objective function quadratic function binary variables. motivate relax constraints appropriate manner leading semideﬁnite relaxation. sequel present proposed relaxation step-by-step fashion. sdpad-lr collections benchmark datasets opengm probabilistic inference challenge benchmark consists multiple categories problems derived various estimation tasks. experimental results demonstrate sdpad-lr outperforms state-of-the-art algorithms computational speed often obtaining better estimates. vast literature concerning estimation discrete undirected graphical models beyond scope paper discuss existing algorithms. interested readers referred in-depth introduction topic. following focus methods involve convex relaxation relevant approach. many prior convex relaxation techniques derived original graph structure underlying estimation problem among linear programming relaxation methods popular. addition researchers considered alternative convex relaxations e.g. quadratic relaxation second-order cone relaxation seminal work authors evaluate various convex relaxation approaches assert dominates qp-rl socp-ms. however shown later dominated standard relaxation main foci paper. recent line approaches aimed obtaining tighter convex relaxations incrementally adding higher-order interactions enforce proper marginalization groups variables despite practical success approaches remains open problem analyze behavior example decide whether polynomial number clusters sufﬁcient. several attempts applying semideﬁnite relaxation obtain assignment however methods primarily designed binary estimation problems. recent work considered general estimation problem variable multiple states. difference proposed formulation utilize semideﬁnite cone constraint prune redundant linear marginalization constraints. leads concise loosely decoupled constraints important developing effective optimization paradigms. careful readers remark might exist convex constraints enforce tighten proposed semideﬁnite relaxation. alternative following marginalization constraints widely invoked relaxation estimation relaxation proposed pioneered beautiful max-cut problem many approaches developed combinatorial problems employ integer indicator parameterize binary variables applies matrix lifting follows similar relaxation procedure resulting semideﬁnite relaxation derived follows limited scalability interior point methods inspired ﬂurry activity developing ﬁrst-order methods among alternating direction method multipliers proves well suited large-scale problems. section propose efﬁcient variant admm referred sdpad-lr tailored special structure enables solve problems large dimensionality. despite theoretical equivalence numerical perspective solving much harder solving sdr. difﬁculty arises complicated form linear constraints enforced note advantage diagonal entries equal follows encodes collects equality constraints gathers element-wise non-negative constraints. variables represent corresponding dual variables respective constraints. sequel start reviewing sdpad i.e. original alternating direction method introduced present modiﬁcation underlying proposed efﬁcient variant sdpad-lr. curse dimensionality poses inevitable numerical challenges applying general-purpose solvers solve sdr. despite superior accuracy primal-dual like sdpt limited small-scale problems scalable solvers csdp dsdp propose solve dual problem. however since non-negativity constraints produce numerous dual variables solvers still restrictive program none solve standard exceeds based assumption idea sdpad-lr invoke low-rank matrix small encode throughout iterative process. allows keep variables memory even large-scale problems. although dense matrix eigenvectors efﬁciently computed using lanczos process whose efﬁciency dictated complexity matrix multiplication operator rnm+ rnm+. involves constrains matrix turns share sparsity pattern thus complexity computing theoretically extremely challenging derive upper bound ensure exactness modiﬁed algorithm. address issue thus design sdpad-lr iteratively doubles value reapplies modiﬁed algorithm returns optimal solution. experiments found sufﬁcient. convergence property. general convergence properties sdpad known equality constraints present however inequality constraints special following aspects property arises equality constraints concerned diagonal blocks linear inequality constraints enforced off-diagonal blocks. special structure leads theoretical convergence guarantees sdpad stated following theorem. theorem sdpad method presented converges optimizer sdr. apparently computationally expensive step sdpad update involves eigendecomposition matrix. limits applicability sdpad large-scale problems bypass numerical bottleneck modify sdpad present efﬁcient heuristic called sdpad-lr exploits low-rank structure straightforward bottleneck algorithm lies compute store primary variable derive efﬁcient solver make low-rank. sumption optimal solution motivated empirical evidence variety problems similar admm methods sdpad-lr converges rapidly moderate accuracy within ﬁrst iterations signiﬁcantly slows afterwards. thus rather continuing sdpad-lr converges would efﬁcient shrink problem size ﬁxing variables whose optimal states likely revealed. speciﬁcally round sdpad-lr optimal state variable tmax max≤i≤n≤j≤m xij. reapply iterative procedures reduced problem. practice tightness size reduced problems signiﬁcantly smaller original problem iterative rounding procedure usually sufﬁcient. perform experimental evaluation estimation problems three popular benchmark data sets i.e. opengm data orient task estimating consistent camera orientations opengm comprises categories mostly sparse table.statistics datasets evaluated paper. graph structure problem category; number variables; number states; probs number instances; average running time sdpad-lr. problems. choose four representative categories evaluation geometric surface labeling chinese characters photomontage matching ﬁrst three categories gm-label gm-character gm-montage sparse estimation problems increasing scales. gm-matching special category convex relaxation tight. comprises categories inference problems various structure. already include sparse inference problems opengm pick representative dense categories object detection image alignment folding mosek cutting-edge interior point method. apply large-scale sdrs nonnegativity constraints incremental fashion i.e. iteration detect smallest negative entries constraint set. problem sets. evaluation consider four categories baseline algorithms applicable picobj pic-align pic-folding gm-label. simplicity pick representative problem category. dimensions problem sets range contain dense sparse problems analysis results. algorithm duality maximum number iterations reached. table shows running time duality maximum primal/dual infeasibility algorithm problem. sdpad-lr generates results comparable sdpad sdpnal. however sdpad-lr turns remarkably efﬁcient sdpad sdpnal large-scale sparse datasets. fact sdpad-lr requires computing eigenvalues memory computationally efﬁcient. interior point methods provable guarantees generate accurate results methods. however mosek scalable large data sets reported table ipm-nc scalable large-scale problems number variables involved small. however ipm-nc solves non-convex optimization problem easily trapped local minimals experimental algoopengm rithms include braobb based combinatorial αexpansion move making mcbc method based highly optimized max-cut solver trws-lf tree-reweighted message passing ogm-trbp— tree-reweighted belief propagation ﬁcolofo performing method dense problems pic. measures assess performance method. ﬁrst measure evaluates method mean objective values resulting assignments category. consistency report meaning smaller value better algorithm. second measure reports percentage method achieves best solution among existing methods higher percentage better algorithm. performance. table summarizes performance sdpad-lr v.s. state-of-the-art inference algorithms type problems. block element describes method category bottom block describes percentage obtaining best solution. overall performance sdpad-lr superior individual algorithm. except gm-matching sdpad-lr performing dataset. contrast existing method either apply generates poor results several datasets. shows advantage solving strong convex relaxation inference problem. break performance benchmark. ent. problems orient exhibit speciﬁc structures pair-wise potentials consist approximately shifted permutation matrices. experimentally found usually tight problems. explains superior performance sdpad-lr. contrast linear programming relaxations tight orient thus trbp trws deliver moderate performance. moreover structural pattern leads huge search spaces combinatorial algorithms easily stuck local optimums. dense problems. sdpad-lr also outperforms methods three dense categories pic. achieves best mean energy value well highest percentage obtaining best solution. arises since tight problems. sparse problems. yields comparable results state-of-the-art algorithms three sparse categories opengm. gm-label consists problems standard relaxation tight. gm-char consists large-scale binary problems comparable mcbc sense achieves better mean energy value mcbc attains higher percentage best solution. arises mcbc highly optimized solver designed binary quadratic problems. hand sdpad-lr approximate solver which cases converge global optimum numerical issues. gm-matching. yields moderate results gm-matching. occurs tight gm-matching. contrast gm-matching small-scale problem combinatorial optimization techniques braobb a-star capable ﬁnding globally optimal solutions. rounding procedure) scale convex relation techniques. shown table preliminary matlab implementation takes less mins small-scale problems medium size problems i.e. pic-folding pic-align gm-char orient running time sdpad-lr ranges minutes hour. large-scale problems gm-montage sdpad-lr takes around hours problem. however still huge room improvement. alternative eigenvalues computed previous iteration accelerate eigen-decomposition current iteration left future work. paper presented novel semideﬁnite relaxation second-order estimation proposed efﬁcient admm solver. extensively compared proposed solver various state-of-the-art solvers. experimental results conﬁrm solver much scalable prior approaches applied various estimation problem enables apply large-scale datasets. owing power semideﬁnite relaxation proves superior top-performing inference algorithms variety benchmark datasets. plenty opportunities future research. first would like extend higher-order problems. moreover would interesting integrate combinatorial optimization techniques potential boost power both. theoretical side theoretical support exact estimation would exciting direction investigation. would offer justiﬁcation presented low-rank heuristic. hand many combinatorial optimization problems formulated inference problems", "year": 2014}