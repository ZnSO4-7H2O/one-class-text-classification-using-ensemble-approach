{"title": "Teaching Machines to Code: Neural Markup Generation with Visual  Attention", "tag": ["cs.LG", "cs.CL", "cs.CV", "cs.NE"], "abstract": "We present a deep recurrent neural network model with soft visual attention that learns to generate LaTeX markup of real-world math formulas given their images. Applying neural sequence generation techniques that have been very successful in the fields of machine translation and image/handwriting/speech captioning, recognition, transcription and synthesis, we construct an image-to-markup model that learns to produce syntactically and semantically correct LaTeX markup code of over 150 words long and achieves a BLEU score of 89%; the best reported so far for the Im2Latex problem. We also visually demonstrate that the model learns to scan the image left-right / up-down much as a human would read it.", "text": "recognition etc. class sequence models employ so-called encoderdecoder sequence-to-sequence architecture wherein encoder encodes source sequence feature vectors decoder employs produce target sequence. source target sequences either belong modality different modalities encoder decoder sub-models constructed accordingly. entire model trained end-to-end using supervised-learning techniques. recent years architecture augmented attention alignment model selects subset feature vectors decoding. shown help longer sequences among things architecture used imagecaptioning.we employ variant architecture images math formulas latex markup code. contributions paper solves imlatex problem achieves best reported bleu score. pushes limits neural encoder-decoder architecture visual attention. analyses variations model cost function. speciﬁcally note changes base model impact performance. imlatex problem request research proposed openai. challenge build neural markup generation model trained end-to-end generate latex markup math formula given image. data problem produced rendering single-line real-world latex formulas obtained dataset. resulting grayscale images used input samples original markup used label/target sequence. training/test sample comprised image input corresponding target markup-sequence output present deep recurrent neural network model soft visual attention learns generate atex markup real-world math formulas images. applying neural sequence generation techniques successful ﬁelds machine translation image/handwriting/speech captioning recognition transcription synthesis; construct image-tomarkup model learns produce syntactically semantically correct latex markup code words long achieves bleu score best reported imlatex problem. also visually demonstrate model learns scan image left-right up-down much human would read past decade deep neural network models based rnns and/or cnns shown powerful sequence modelers. ability model joint distributions real-world data demonstrated remarkable achievements broad spectrum generative tasks image synthesis image description video description speech audio synthesis handwriting recognition handwriting synthesis machine translation speech figure model outline showing major parts model. beam search decoder used inferencing training. lstm-stack attention model jointly form conditioned attentive lstm stack stacked. shows details decoder. vectors size visual feature map’s dimensions along height width axes image respectively. table shows conﬁguration encoder cnn. convolution kernels shape stride tanh non-linearity whereas maxpooling windows shape stride section comments around architecture. next visual feature vectors pooled together rectangular blocks shape resulting pooled feature vectors size strides pooling operation moves along height width therefore shape overfeature reduces ´h/sh length word sequence belongs vocabulary dataset plus special tokens begin-of-sequence ‘bos’ end-of-sequence ‘eos’ denoting image dimensions vocabulary words dropping superscript brevity represent task generate markup latex εcompiler render back original image. therefore model needs generate syntactically semantically correct markup simply ‘looking’ image i.e. jointly model vision language. decoder lstm based model internal state holds relevant information regarding output sequence unfolded thus image regions attended initial state generated addition receives previous word encoded image inputs. architecture enables condition output entire previous sequence image. representing function have pooling allows varying receptive ﬁelds feature vectors changing stride. share results models stride pooled feature vector window distinct spatial region image; i.e. rectangle projected receptive ﬁeld. idea behind partition image spatially localized regional encodings setup decoder architecture selects/emphasizes relevant regions given time-step ignoring/de-emphasizing rest. bahdanau showed piecewise encoding enables modeling longer sequences opposed models encode entire input single feature vector represent ﬂattened sequence pooled feature vectors decoder recurrent neural network models discrete probably distribution output word conditioned sequence previous words encoded image denoting decoder’s output neighboring regions overlap region distinct overall. that said bahdanau employ bidirectional-lstm encoder whose receptive ﬁeld encompass entire input anyway likewise deng also solve imlatex problem also employ bi-directional lstm stacked cnn-encoder order full view image. contrast visual feature vectors hold spatially local information found sufﬁcient achieve good accuracy. probably owing nature problem; i.e. transcribing one-line math formula atexsequence requires local information step. figure schematic decoder showing sub-models. three nested cells decoder level nesting calstm nests lstm-stack. init model participate recurrence therefore shown outside box. figure focal-regions learnt attention model il-strips bottom il-nopool. image darkness proportional focal-regions blocks surround symbol whose markup produced step. also model able utilize ﬁner granularity ilnopool produces tighter focal region il-strips. samples website. potential distribution practice mass gets concentrated region neighborhood model learns focus attention small focal-regions image tending towards ‘hard attention’ formulation described except case focal-regions learnt therefore adaptive dataset problem. also notice given sample attention model able utilize extra granularity feature-map available il-nopool case consequently generates much smaller focal-region il-strips. would interesting experiment even granular image-map attention model reﬁnes focal-region even further. model trained output sequence generated starting word ‘bos’ repeatedly sampling ‘eos’ produced. sequence words thus sampled predicted sequence previously alluded decoder selects/emphasizes relevant image regions step. implemented via. ‘soft attention’ mechanism computes weighted mean pooled feature vectors visual attention model fatt computes weight distribution logistic sigmoid function respectively input gate forget gate output gate cell hidden activation vectors size lstm cells stacked multi-layer conﬁguration below lstm cell position input hidden activation cell state respectively. receives stack’s input soft attention context previous output word eyt−. produces stack’s output sent deep output layer. accordingly stack’s activation state deﬁned ﬁrst term equation average perplexity predicted sequence main objective. l-regularization term equal l-norm model’s parameters optional penalty term intended bias distribution amount attention placed constraint attention distributed across locations image. term serves steer variance penalizing deviation desired value. squared-difference mean asen normalized value therefore asen aset desired value asen hyperparameter needs discovered experimentation. also hyperparameters need tuned. section discussion around topic. experimentation split dataset ﬁxed parts training dataset data test dataset beginning training dataset randomly held validation-set remainder used training. therefore different training/validation data-split thus naturally cross-validating learnings duration project. trained model minibatches using adam optimizer updating paramfinit modeled common hidden layers distinct output layers element connected figure speciﬁed table said since provides small improvement performance exchange million parameters presence could questioned eters; periodically evaluating validation set. efﬁciency batched data minibatch similar length samples. ﬁnal evaluation however ﬁxed training validation dataset split retrained models epochs picked model-snapshots best validation bleu score evaluated model test-dataset publication. executed nvidia geforce graphics cards parallel towers conﬁguration. implementation uses tensorﬂow toolkit distributed agpl license. given multiple possible latexsequences render math image ideally perform visual evaluation. however since widely accepted visual evaluation metric report corpus bleu per-word levenstein edit distance scores also report exact visual match score reports percentage exact visual matches discarding partial matches. predicted targeted images match least cases model generates different correct sequences cases cases images exactly match differences cases minor overall models produce syntactically correct sequences least test samples datasets created single-line latex math formulas extracted scientiﬁc papers subsequently processed follows normalize formulas minimize spurious ambiguity. render normalized formulas using pdﬂatex discard ones didn’t compile render successfully. remove duplicates. remove formulas low-frequencey words remove images i.e. successfully rendered latex ’match without whitespace’ algorithm provided deng wherein images count matched match pixel-wise discarding white columns allowing upto pixel image translation outputs binary match/no-match verdict sample i.e. partial matches however close considered non-match. bigger formulas longer processing imlatex-k dataset resulted imlatex-k dataset samples. these aside test dataset remaining split training validation sets found imlatex-k dataset small good generalization therefore augmented additional samples resulted il-k dataset samples. since normalized formulas already space separated token sequences additional tokenization step necessary. vocabulary therefore produced simply identifying unique space-separated words dataset. datasets available website data processing code available source-code repository. bluche th´eodore hermann kermorvant christopher. comparison sequence-trained deep neural networks recurrent neural networks optical modeling handwriting recognition. slsp kyunghyun merrienboer bart aglar g¨ulehre bahdanau dzmitry bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder-decoder statistical machine translation. emnlp donahue jeff hendricks lisa anne guadarrama sergio rohrbach marcus venugopalan subhashini saenko kate darrell trevor. long-term recurrent convolutional networks visual recognition description. corr abs/. figure sample correct predictions il-strips. we’ve shown long predictions hence lengths touching note times target length greater predicted length times reverse true cases would evaluate less perfect bleu score edit-distance. happens cases. examples visit website. figure random sample mistakes made il-strips. observe usually model gets formula right mistake small portion overall formula cases mistake font cases images identical incorrectly ﬂagged image-match evaluation software cases predicted formula appears correct original sample clearly probably owing training samples underbraces. examples visit website. table test results. imtex results taken deng imlatex-k dataset samples imlatex-k dataset remained formulas normalized rendered ﬁltered duplicates non-compiling large formulas. last column percentage successfully rendering predictions. graves alex fern´andez santiago gomez faustino schmidhuber j¨urgen. connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks. icml luong minh-thang pham hieu manning christopher effective approaches attention-based neural machine translation. corr abs/. http//arxiv.org/abs/.. oord a¨aron dieleman sander heiga simonyan karen vinyals oriol graves alex kalchbrenner senior andrew kavukcuoglu koray. wavenet generative model audio. corr abs/. oord a¨aron kalchbrenner vinyals oriol espeholt lasse graves alex kavukcuoglu koray. conditional image generation pixelcnn decoders. corr abs/. vinyals oriol toshev alexander bengio samy erhan dumitru. show tell neural image caption generator. ieee conference computer vision pattern recognition jamie ryan kyunghyun courville aaron salakhutdinov ruslan zemel richard bengio yoshua. show attend tell neural image caption generation visual attention. icml initially experimented output model however bleu score didn’t improve beyond started training along model end-to-end model didn’t even start learning possibly large overall depth end-to-end model. reducing number convolution layers changing non-linearity tanh good results. reducing number layers yielded performance therefore stuck conﬁguration additon experimented il-strips reduces rectangular imagemap linear thereby presumably making alignment model’s task easier would need scan one-dimension. however performed around il-nopool therefore hypothesis debunked. formulation attention model receives inputs single image location. comparison formulation receives full encoded image input. change needed previous formulation progress beyond point presumably problem warranted wider receptive ﬁeld. formulation works equally well different pooling strides formulation includes scalar informs lstm much emphasis place image language model. experimentally found impact end-to-end performance therefore dropped model. also simpler formula call ‘doubly stochastic optimization’. formulation uses true mean instead normalizes ﬁxed range compared across models importantly includes target-ase term aset without term i.e. aset would bias attention model towards uniformly scanning image locations. undesirable since many empty regions images makes sense attention model spend much time. conversely densely populated regions model would reasonably spend time would produce longer output sequence. words optimal scanning pattern would non-uniform aset also scanning pattern would vary sample sample aset single value samples. therefore preferred remove attention-model bias altogether objective function setting situations except attention model needed ’nudge’ order ‘get ground’. cases aset based observed values asen figure random sample predictions il-strips containing good predictions. note though random sample prediction mistakes obvious takes effort point examples visit website. experimentation penultimate lstm-stack lstm layers units each gave validation score point experimental observations suggested lstm stack accuracy ’bottleneck’ sub-models performing well. increasing number lstm units better validation score worse overﬁt. reducing number layers best overall validation score. comparison used single lstm layer cells. note output layer receives skip connections lstm-stack input observed impact bleu score addition input-tooutput skip-connections. leads believe adding skip-connections within lstm-stack help improve model accuracy. overall accuracy also improved increasing number layers lastly observe sub-model different wherein three inputs afﬁne-transformed dimensions summed passed fully-connected layer. experimenting model ultimately chose instead feed inputs fully-connected layer thereby allowing naturally learn inputto-output function. also increased number layers changed activation function hidden units relu tanh ensured layer least many units softmax layer questioned need init model experimented using zero values initial state. caused slight consistent decline validation score indicating initial state learnt initial state model contribute towards learning generalization. note however init model different version uses feature vectors takes average. also added hidden layer used tanh activation function instead relu. start version provide appreciable impact bottom line made hypothesize perhaps taking average feature vectors causing loss information; mitigated taking feature vectors without summing them. making changes init model yields consistent albiet small performance improvement given consumes default values βandβ adam optimizer yielded choppy validation score curves frequent down-spikes validation score would fall levels ultimately resulting lower peak scores. reducing ﬁrst second moments ﬁxed problem suggesting default momentum high ‘terrain’. dropout regularization. however increasing data-set size raising minimum-word-frequency threshold yield better generalization overall test scores finally normalizing data yielded accuracy without.", "year": 2018}