{"title": "Spectral Learning for Supervised Topic Models", "tag": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "abstract": "Supervised topic models simultaneously model the latent topic structure of large collections of documents and a response variable associated with each document. Existing inference methods are based on variational approximation or Monte Carlo sampling, which often suffers from the local minimum defect. Spectral methods have been applied to learn unsupervised topic models, such as latent Dirichlet allocation (LDA), with provable guarantees. This paper investigates the possibility of applying spectral methods to recover the parameters of supervised LDA (sLDA). We first present a two-stage spectral method, which recovers the parameters of LDA followed by a power update method to recover the regression model parameters. Then, we further present a single-phase spectral algorithm to jointly recover the topic distribution matrix as well as the regression weights. Our spectral algorithms are provably correct and computationally efficient. We prove a sample complexity bound for each algorithm and subsequently derive a sufficient condition for the identifiability of sLDA. Thorough experiments on synthetic and real-world datasets verify the theory and demonstrate the practical effectiveness of the spectral algorithms. In fact, our results on a large-scale review rating dataset demonstrate that our single-phase spectral algorithm alone gets comparable or even better performance than state-of-the-art methods, while previous work on spectral methods has rarely reported such promising performance.", "text": "abstract—supervised topic models simultaneously model latent topic structure large collections documents response variable associated document. existing inference methods based variational approximation monte carlo sampling often suffers local minimum defect. spectral methods applied learn unsupervised topic models latent dirichlet allocation provable guarantees. paper investigates possibility applying spectral methods recover parameters supervised ﬁrst present two-stage spectral method recovers parameters followed power update method recover regression model parameters. then present single-phase spectral algorithm jointly recover topic distribution matrix well regression weights. spectral algorithms provably correct computationally efﬁcient. prove sample complexity bound algorithm subsequently derive sufﬁcient condition identiﬁability slda. thorough experiments synthetic real-world datasets verify theory demonstrate practical effectiveness spectral algorithms. fact results large-scale review rating dataset demonstrate single-phase spectral algorithm alone gets comparable even better performance state-of-the-art methods previous work spectral methods rarely reported promising performance. automatically learn latent semantic structure large collection documents images latent dirichlet allocation popular examples. vanilla unsupervised model built input contents documents images. many applications side information often available apart contents e.g. userprovided rating scores online review text user-generated tags image. side signal usually provides additional information reveal underlying structures data study. extensive studies developing topic models incorporate various side information e.g. treating supervision. representative models supervised captures realvalued regression response document multiclass slda learns discrete classiﬁcation responses discriminative incorporates classiﬁcation response discriminative linear transformations topic mixing vectors medlda employs max-margin ‡y.r. y.w. contributed equally. department computer science technology; tnlist lab; state laboratory intelligent technology systems; center bio-inspired computing research tsinghua university beijing china. email renymails.tsinghua.edu.cn; dcszjtsinghua.edu.cn. wang machine learning department carnegie mellon topic models typically learned ﬁnding maximum likelihood estimates local search sampling methods trapped local optima. much recent progress made developing spectral decomposition nonnegative matrix factorization methods estimate topic-word distributions. instead ﬁnding estimates known np-hard problem methods assume documents i.i.d. sampled topic model attempt recover underlying model parameters. compared local search sampling algorithms methods enjoy advantage provably effective. fact sample complexity bounds proved show given sufﬁciently large collection documents algorithms recover model parameters accurately high probability. recently attention paid supervised topic models methods. example nguyen present extension anchorword methods capture categorical information supervised sentiment classiﬁcation. however spectral methods previous work mainly focused unsupervised latent variable models leaving broad family supervised models largely unexplored. exception presents spectral method mixtures regression models quite different slda. ignorance coincidence supervised models impose technical challenges. instance direct application previous techniques slda cannot handle regression models duplicate entries. addition sample complexity bound gets much worse match entries regression models corresponding topic vectors. paper extend applicability spectral learning methods presenting novel spectral decomposition algorithms recover parameters slda models low-order empirical moments estimated data. present variants spectral methods. ﬁrst algorithm extension spectral methods extra power update step recovering regression model slda including variance parameter. powerupdate step uses newly designed empirical moment recover regression model entries directly data reconstructed topic distributions. free making constraints underlying regression model. provide sample complexity bound analyze identiﬁability conditions. fact two-stage method increase sample complexity much compared vanilla lda. however two-stage algorithm could disadvantages separation topic distribution matrix recovered unsupervised manner without considering supervision regression parameters recovered assuming ﬁxed topic matrix. unwarranted separation often leads inferior performance compared gibbs sampling methods practice address problem present novel singlephase spectral method supervised topic models jointly recovers model parameters single-step robust tensor decomposition newly designed empirical moment takes input data supervision signal consideration. therefore joint method supervision information recovering topic distribution matrix regression parameters. joint method also provably correct provide sample complexity bound achieve error rate high probability. finally provide thorough experiments synthetic real datasets demonstrate practical effectiveness spectral methods. twostage method combining gibbs sampling procedure show superior performance terms language modeling prediction accuracy running time compared traditional inference algorithms. furthermore demonstrate large-scale review rating dataset single-phase method alone achieve comparable even better results state-of-the-art methods promising results signiﬁcant literature spectral methods rest paper organized follows. section reviews basics supervised topic models. section introduces background knowledge notations slda high-order tensor decomposition. section presents two-stage spectral method rigorous theoretical analysis section presents joint spectral method together sample complexity bound. section presents implementation details scale computation. section presents experimental results synthetic real datasets. finally conclude section related work based different principles various methods learn supervised topic models. natural maximum-likelihood estimation however learning objective makes optimization problem hard. original paper used authors choose variational approximation handle intractable posterior expectations. method tries maximize lower bound built variational distributions meanﬁeld assumption usually imposed tractability. although method works well practice guarantee distribution learned close true one. bayesian framework gibbs sampling attractive method enjoys property stationary distribution chain target posterior distribution. however mean really accurate samples posterior distribution practice. slow mixing rate often makes sampler trapped local minimum true distribution ﬁnite number iterations. max-margin learning another principle learning supervised topic models maximum entropy discrimination popular example. medlda explores max-margin principle learn sparse discriminative topic representations. learning problem deﬁned regularized bayesian inference framework max-margin posterior regularization introduced ensure topic representations good predicting response variables. though carefully designed variational inference gibbs sampling methods given still canguarantee quality learnt model general. recently increasing efforts made recover parameters directly provable correctness topic models main focus unsupervised models lda. methods adopt topic mixing vectors. bag-of-words model adopted ﬁrst generates topic indicator word multi generates word multi. supervised latent dirichlet allocation incorporates extra response variable document. response variable modeled linear regression model either topic mixing vector averaging topic assignment vector number words document indicator function noise assumed gaussian zero mean variance. fig. shows graph structure slda model using regression. although previous work mainly focused model using averaging topic assignment vector convenient collapsed gibbs sampling variational inference integrated conjugacy dirichlet prior multinomial likelihood consider using topic mixing vector features regression considerably simplify spectral algorithm analysis. assume whenever document short empirical distribution word topic assignments close document’s topic mixing vector. scheme adopted learn sparse topic coding models demonstrated promising results practice. results also prove effective strategy. high-order tensor product orthogonal tensor decomposition brieﬂy introduce something tensors mostly follow real prni belongs tensor product euclidean spaces rni. without loss generality assume identify coordinate p-tuple i··· instance p-th order tensor vector matrix also consider p-th order tensor multilinear mapping p-th orrn×m j··· aj··· jiji jpip. consider concrete examples multilinear either spectral decomposition approaches. basic idea even nphard problem general topic distribution matrix recovered separable condition e.g. topic least anchor word. precisely topic method ﬁrst ﬁnds anchor word non-zero probability topic. recovery step reconstructs topic distribution given anchor words second-order moment matrix word-word co-occurrence original reconstruction step needs part matrix robust practice. thus author recovers topic distribution based probabilistic framework. methods produce good empirical results real-world data. recently work extends anchor word methods handle supervised topic models. method augments word co-occurrence matrix additional dimensions metadata sentiment shows better performance sentiment classiﬁcation. spectral methods start computing loworder moments based samples relate model parameters. tensors three order sufﬁcient recover parameters centralization moments expressed mixture parameters interested whitening robust tensor decomposition steps adopted recover model parameters. whitening step makes third-order tensor decomposed orthogonal eigenvectors corresponding eigenvalues operations based output robust tensor decomposition step ﬁnds them. previous work focuses unsupervised models extend ability spectral methods handle response variables. finally preliminary results two-stage recovery algorithm reported paper presents systematical analysis novel onestage spectral method yields promising results large-scale dataset. supervised latent dirichlet allocation hierarchical generative model topic modeling text documents images represented bag-of-visual-words format assumes different topics topicword distributions µ··· vocabulary size denotes probability simplex -dimensional random vector. document models topic mixing vector probability distribution topics. conjugate dirichlet prior parameter imposed moments observable variables spectral decomposition methods recover topic distribution matrix linear regression model manipulating moments observable variables. deﬁnition deﬁne list moments random variables underlying slda model. deﬁnition deﬁne following moments observable variables note moments also deﬁned recovering parameters models. slda need deﬁne moment order recover linear regression model moments based observable variables sense estimated i.i.d. sampled documents. instance estimated computing empirical distribution words estimated using word co-occurrence frequencies. though moments forms look complicated apply elementary calculations based conditional independence structure slda signiﬁcantly simplify importantly connected model parameters recovered summarized proposition whose proof elementary deferred appendix clarity. proposition moments expressed using model parameters simultaneous diagonalization proposition shows moments deﬁnition weighted sums tensor products {µi}k underlying slda model. idea reconstruct {µi}k perform simultaneous diagonalization tensors different orders. idea used number recent developments spectral methods latent variable models speciﬁcally ﬁrst whiten second-order tensor ﬁnding matrix {vi}k ⊗···⊗ denote p-th order tensor generated vector without loss generality assume nonnegative since change sign otherwise. although orthogonal tensor decomposition matrix case done efﬁciently singular value decomposition several delicate issues higher order tensor spaces instance tensors unique decompositions orthogonal decomposition exist every symmetric tensor issues complicated noisy estimates desired tensors available. reasons need advanced techniques handle high-order tensors. paper apply robust tensor power methods recover robust eigenvalues eigenvectors thirdorder tensor. algorithm recovers eigenvalues eigenvectors absolute error running polynomial time w.r.t tensor dimension log. details analysis robust tensor power method appendix denote euclidean norm vector denote spectral norm matrix denote operator norm highorder tensor denote frobenious norm one-hot vector represent word document two-stage spectral method denote two-stage spectral method ﬁrst present two-stage spectral method recover parameters slda. algorithm consists components—an orthogonal tensor decomposition observable moments recover topic distribution matrix power update method recover regression model present techniques rigorous theoretical analysis below. whitening procedure possible whenever topic distribuction vectors {µi}k linearly independent always correct since overcomplete case possible topic number larger vocabulary size however linear independent assumption gives compact representation topic model works well practice. hence simply assume whitening procedure possible. whitening procedure linear independence assumption also imply µi}k orthogonal vectors subsequently recovered performing orthogonal tensor decomposition simultaneously whitened third-order tensor finally multiplying pseudoinverse whitening matrix obtain topic distribution vectors {µi}k noted jennrich’s algorithm could recover {µi}k directly order tensor alone {µi}k linearly independent. however still adopt simultaneous diagonalization framework intermediate vectors µi}k play vital role recovery procedure linear regression model power update method although linear regression model recovered similar manner performing simultaneous diagonalization method several disadvantages thereby calling novel solutions. first obtaining entry values {ηi}k need match topic distributions {µi}k previously recovered. easily done access true moments becomes difﬁcult estimates observable tensors available estimated moments share singular vectors sampling noise. serious problem duplicate entries orthogonal decomposition longer unique. though randomized strategy similar used might solve problem could substantially increase sample complexity render algorithm impractical. authors provide method matching problem reusing eigenvectors. develop power update method resolve difﬁculties similar spirit. speciﬁcally obtaining whitened vectors {vi} recover entry linear regression model directly computing power update myvi. matching problem automatically solved know topic distribution vector used recovering furthermore singular values need distinct using unique properties result proposed algorithm works linear model alg. outlines parameter recovery algorithm slda first empirical estimations observable moments deﬁnition computed given documents. simultaneous diagonalization method used reconstruct topic distribution matrix prior parameter obtaining power update method introduced previous section recover linear regression model also recover noise level parameter parameters hand estimating since term computed analytical form using model parameters detailed appendix alg. admits three hyper-parameters deﬁned entries prior parameter following conventions assume known priori value perform parameter estimation. noted mild assumption practice usually homogeneous vector assumed entire vector known parameters used control number iterations robust tensor power method. general robust tensor power method runs time. ensure sufﬁcient recovery accuracy least linear function log) λmax error tolerance parameter. appendix provide deeper analysis choice parameters. sample complexity analysis analyze sample complexity alg. order achieve ε-error high probability. clarity focus presenting main results deferring proof details appendix including proofs important lemmas needed main theorem. brevity proof based matrix perturbation lemmas analysis orthogonal tensor decomposition methods performed inaccurate tensor estimations sample complexity lower bound consists three terms term comes sample complexity bound robust tensor power method η−σφ−) term characterizes recovery accuracy linear regression model recover topic distribution vectors ﬁnally term required technical conditions met. term depend either remark important implication theorem provides sufﬁcient condition supervised model identiﬁable shown remark extent remark best identiﬁability result possible inference framework makes take close look sample complexity bound theorem evident neglected number topics gets large practice norm linear regression model usually assumed small order avoid overﬁtting. moreover mentioned before prior parameter often assumed homogeneous observations sample complexity bound theorem greatly simpliﬁed. remark assume small number topics gets large sample complexity bound theorem simpliﬁed two-stage procedure possible disadvantages recovery topic distribution matrix supervision signal thus recovered topics often good enough prediction tasks shown experiments. disadvantage motivates develop joint spectral method theoretical guarantees. describe single-phase algorithm. ﬁrst deﬁne moments based observable variables including information need recover model parameters. since recover joint topic distribution matrix combine word vector response variable form joint vector deﬁne following moments robust tensor decomposition proposition shows centerized tensors weighted sums tensor products parameters {vi}k recovered. similar procedure two-stage method followed order develop joint spectral method consists whitening robust tensor decomposition steps. first whiten nd-order tensor ﬁnding matrix whitening procedure possible whenever joint topic distribution vectors {vi}k linearly independent matrix rank whitening procedure linear independence assumption also imply vi}k orthogonal vectors subsequently recovered performing orthogonal tensor decomposition simultaneously whitened third-order tensor summarized following proposition. intuition deﬁnitions derived important observation latent variable given mean value weighted combination regression parameters form µihi). therefore natural regard additional dimension word vector gives vector combination leads terms involving high-order moments introduce variance parameter centerize moments. although recover two-stage method recovering jointly parameters seems hard. thus treat hyper-parameter. determine cross-validation procedure. illustrated fig. rd-order moment viewed centerized version combination high-order statistics response variables. note combination already aligned regression parameters corresponding topics. hence need extra matching step. practice cannot exact values moments. instead estimate i.i.d. sampled documents. note need moments third order means document consisting least three words used estimation. furthermore although moments seem complex expressed model parameters graceful manner summarized proposition proved expanding terms deﬁnition similar proof proposition sample complexity comparison mentioned remark remark joint spectral method shares sample complexity two-stage algorithm order achieve accuracy except minor differences. joint method joint topic distribution matrix consists original topic distribution matrix extra regression parameters. thus weyl’s inequality smallest singular value joint topic distribution matrix larger original topic distribution matrix sample complexity joint method lower two-stage method empirically justiﬁed experiments. second different two-stage method errors topic distribution regression parameters estimated together joint method potentially give accurate estimation regression parameters considering number regression parameters much less topic distribution. storage corpus size number words document vocabulary size. time space complexities clearly prohibitive real applications vocabulary usually contains tens thousands terms. however employ trick similar speed moment computation. ﬁrst note need parameter information variance prediction error. although disadvantage need tune introduction sometimes increases ﬂexibility methods incorporating prior knowledge additionally need three hyper-parameters similar two-stage method. parameter deﬁned summation entries prior parameter used control number iterations robust tensor decomposition. ensure sufﬁciently high recovery accuracy least linear function ω+log log) λmax error rate. αmin smallest element λmin αmax largest element error-tolerance parameter algorithm runs least ω+log log) iterations i.i.d. sampled documents where polynomial inverse normal distribution norm regression parameters universal constants. probability least exist permutation following holds every similar theorem sample complexity bound consists three terms. ﬁrst second terms depend error rate required technical conditions met. thus could largely neglected practice. third term comes sample complexity bound robust tensor power method ciui proportional contains non-zero entries. allows ciui) ui). appendix provides details speed-up trick. overall time complexity space complexity directly. trick mentioned this. idea decompose third-order tensor different parts based occurrence words compute respectively. time comlexity space complexity needed single-phase method. sometimes balanced situation happens either vocabulary size large range large. image vocabulary consisting million words energy matrix concentrates consequence performs badly matrix ill-conditioned. practical solution problem scale word vector constant i-th word dictionary constant. main effect make matrix stable manipulation. note makes effect recovery accuracy. trick primarily computational stability. experiments dealing large vocabulary size step whitening procedure methods perform second order moment straightforward implementation complexity unbearable vocabulary size large. follow method perform random projection reduce dimensionality. experiments present experimental results synthetic real-world datasets. spectral methods hyper-parameters sufﬁciently large experiment settings. since spectral methods recover underlying parameters ﬁrst recover parameters training gibbs sampling infer topic mixing vectors topic assignments word testing. main competitor slda gibbs sampler asymptotically accurate often outperforms variational methods. implement uncollapsed gibbs sampler alternately draws samples local conditionals rest variables given. monitor behavior gibbs sampler observing relative change training data log-likelihood terminate average change less given threshold last iterations. hyper-parameters gibbs sampler methods including topic numbers evaluate hybrid method uses parameters recovered joint spectral method initialization gibbs sampler. strategy similar estimation spectral method used initialize method reﬁning. hybrid method gibbs sampler plays similar role reﬁning. also compare medlda state-of-theart topic model classiﬁcation regression real datasets. gibbs sampler data augmentation accurate original variational methods adopts stopping condition above. likelihood synthetic real-world datasets. quality prediction synthetic dataset measured mean squared error quality real-word dataset assessed deﬁned iyi) predictive normalized version mean testing data andyi estimation where logk per-word log-likelihood deﬁned generate synthetic dataset following generative process slda vocabulary size topic number generate topic distribution matrix ﬁrst sampling entry uniform distribution normalizing every column linear regression model sampled standard gaussian distribution. prior parameter assumed homogeneous i.e. documents response variables generated slda model speciﬁed section consider cases length document repectively. hyper-parameters ones used generate dataset fig. fig. show l-norm reconstruction errors document contains different number words. note unidentiﬁability topic models permutated estimation underlying parameters. thus bipartite graph matching permutation minimizes reconstruction error. sample size increases reconstruction errors parameters decrease consistently zero methods veriﬁes correctness theory. taking closer look ﬁgures empirical convergence rates almost spectral methods. however convergence rate regression parameters joint method much higher twostage method mentioned comparison sample complexity section fact joint method bound estimation error together. furthermore though theorem theorem involve number words document simulation results demonstrate signiﬁcant improvement words observed document nice complement theoretical analysis. fig. mean square errors negative per-word loglikelihood alg. gibbs slda. document contains words. axis denotes training size ref. model denotes underlying true parameters. fig. shows spectral methods consistently outperform gibbs-slda. methods also enjoy advantage less variable indicated curve error bars. moreover number training documents sufﬁciently large performance reconstructed model close true model implies spectral methods correctly identify slda model observations therefore supporting theory. performances two-stage spectral method joint comparable time largely fact giving enough training data recovered model accurate enough. gibbs method easily caught local minimum sample size increases prediction errors decrease monotonously. fig. scores negative per-word loglikelihood hotel review dataset. axis indicates number topics. error bars indicate standard deviation -fold cross-validation. fig. reconstruction errors spectral methods document contains words. axis denotes training size domain base error bars denote standard deviations measured independent trials setting. fig. reconstruction errors spectral methods document contains words. axis denotes training size domain base error bars denote standard deviations measured independent trials setting. hotel reviews dataset real-world datasets ﬁrst test relatively small hotel review dataset consists documents training documents testing randomly sampled tripadvisor website. document associated rating score task predict preprocess dataset shifting review scores zero mean unit variance fig. shows prediction accuracy perword likelihood vocabulary size mean level medlda adopts quite different objective slda compare prediction accuracy. comparing traditional gibbs-slda medlda twostage spectral method much worse joint spectral method comparable optimal value. result surprising since convergence rate regression parameters joint method faster two-stage one. hybrid method performs well state-ofthe-art medlda. results show spectral methods good ways avoid stuck relatively local optimal solution. amazon movie reviews dataset finally report results large-scale real dataset built amazon movie reviews demonstrate effectiveness spectral methods improving prediction accuracy well ﬁnding discriminative topics. dataset consists movie reviews written users review accompanied rating score indicating user likes particular movie. median number words review consider cases vocabulary terms built selecting high frequency words deleting common words names characters movies. vocabulary size small exact whitening step; large randomized approximate result. before also preprocess dataset shifting review scores zero mean unit variance. prediction performance fig. shows prediction accuracy per-word log-likelihood takes different values vocabulary size denotes mean level comparing classical gibbs sampling method spectral method sensitive hyper-parameter cases joint method alone outperforms gibbs sampler two-stage spectral method. medlda also sensitive hyper-parameter properly medlda achieves best result comparing methods however joint method medlda small. result signiﬁcant spectral methods whose practical performance often much inferior likelihood-based estimators. also note properly hybrid method initializes gibbs sampler results spectral methods lead high accuracy outperforming gibbs sampler medlda random initialization. results joint method initialization gives better performance compared two-stage method. fig. scores negative per-word log-likelihood amazon dataset. axis indicates number topics. error bars indicate standard deviation -fold cross-validation. vocabulary size fig. scores negative per-word log-likelihood amazon dataset. axis indicates number topics. error bars indicate standard deviation -fold cross-validation. vocabulary size time joint spectral method gets best result two-stage method comparable gibbs sampling worse medlda. hybrid method comparable joint method demonstrating strategy works well practice again. interesting phenomenon spectral method gets good results topic number means spectral method data using fewer topics. although rising trend prediction accuracy hybrid method cannot verify canget results spectral methods large. reason spectral method fails robust tensor decomposition step negative eigenvalues. phenomenon explained nature methods review dataset impossible whiten topics. fact used model selection avoid using many extra topics. also rising trend gibbs sampling measure indicator reaches peak finally note gibbs sampling hybrid gibbs sampling methods better log-likelihood values. result surprising gibbs sampling based spectral methods not. fig. shows hybrid gibbs sampling achieves best per-word likelihood. thus one’s main goal maximize likelihood hybrid technique desirable. parameter recovery take closer investigation recovered parameters spectral methods. table shows estimated regression parameters methods methods different ranges possible predictions normalization also examine estimated topics methods. topics large value positive words dominate topics spectral methods frequencies much higher negative ones thus mainly focus negative topics difference found expressly. table shows topics correspond smallest value method. save space topic method show non-neutral words highest probabilities. word show probability well rank topic distribution vector. negative words higher rank topic joint spectral method topic two-stage method positive words lower rank topic joint method topic two-stage method. result suggests topic joint method strongly associated negative reviews therefore yielding better negative review scores combined estimated therefore considering supervision information lead improved topics. finally also observe topics positive words rather high rank. occurrences positive words much frequent negative ones. methods time efﬁcient avoid time-consuming iterative steps traditional variational inference gibbs sampling methods. furthermore empirical moment computation time-consuming part alg. alg. dealing large-scale datasets consists elementary operations easily optimized. table shows running time synthetic dataset various sizes setting topic number vocabulary size document length spectral methods much faster gibbs sampling especially data size large. another advantage spectral methods easily parallelize computation loworder moments multiple compute nodes followed single step synchronizing local moments. therefore communication cost compared distributed algorithms topic models often involve intensive communications order synchronize messages accurate inference. small sufﬁcient amazon review dataset report results different values synthetic dataset vocabulary size document length document size shown fig. distributed implementation spectral methods almost ideal speedup respect number threads moments computing. computational complexity tensor decomposition step thirdorder tensor rk×k×k small topic number large follow recent developed stochastic tensor gradient descent method compute eigenvalues eigenvectors signiﬁcantly reduce running time tensor decomposition stage. conclusions discussions propose novel spectral decomposition methods recover parameters supervised models labeled documents. proposed methods enjoy provable guarantee model reconstruction accuracy highly efﬁcient effective. experimental results real datasets demonstrate kruskal. three-way arrays rank uniqueness trilinear decompositions applications arithmetic complexity statistics. linear algebra applications lacoste-julien jordan. disclda discriminative learning dimensionality reduction classiﬁcation. advances neural information processing systems leurgans ross abel. decomposition threeway arrays. siam journal matrix analysis applications moitra. algorithmic aspects machine learning. nguyen boyd-graber lund seppi ringger. anchor going down? fast accurate supervised topic models. north american chapter association computational linguistics zhang chen zhou jordan. spectral methods meet provably optimal algorithm crowdsourcing. advances neural information processing systems ahmed xing. medlda maximum margin supervised topic models. journal machine learning research proposed methods especially joint superior existing methods. result signiﬁcant spectral methods often inferior mle-based methods practice. work interesting recover parameters regression model non-linear. anandkumar kakade. method moments mixture models hidden markov models. conference learning theory arora halpern mimno moitra sontag zhu. practical algorithm topic modeling provable guarantees. international conference machine learning arora kannan moitra. computing nonnegative matrix factorization provably. symposium theory computing arora moitra. learning topic models-going beyond svd. blei jordan. latent dirichlet allocation. journal machine learning research chaganty liang. spectral experts estimating international conference appendix proof theorem section prove sample complexity bound given theorem proof consists three main parts. appendix prove perturbation lemmas bound estimation error whitened tensors terms estimation error tensors themselves. appendix cite results accuracy robust tensor power method performed estimated tensors prove effectiveness power update method used recovering linear regression model finally give tail bounds estimation error appendix ﬁrst deﬁne canonical topic distribution vectors estimation error observable tensors deﬁnition deﬁne canonical version topic distribution vector µiµi follows also deﬁne rn×k µ··· µk]. accuracy idea spectral recovery topic modeling simultaneous diagonalization trick asserts recover model parameters performing orthogonal tensor decomposition pair simultaneously whitened moments example following proposition details insight derive orthogonal tensor decompositions whitened tensor product imply expand σko) dominates second according deﬁnition note fact ﬁrst term one. missing third requirment discard both. bound estimation error linear classiﬁer need bound εyw. assume expanding according deﬁnition similar manner obtain term −απ| bound follows immediately lemma recovery rule requirment bound −ηπ| follows immediately lemma finally bound using lemma need assume αmaxσo)ep σko) maxσo) term. bound follows lemma lemma appeared theorem functions original model parameters remark indirect quantities σko) related following take close look sample complexity bound theorem evident neglected number topics gets large practice norm linear regression model usually assumed small order avoid overﬁtting. moreover mentioned before prior parameter often assumed homogeneous observations sample complexity bound theorem greatly simpliﬁed. remark assume small number topics gets large sample complexity bound theorem simpliﬁed sample complexity bound remark look formidable depends σko). however model parameters. furthermore dependence σko) introduced robust tensor power method recover parameters reconstruction accuracy depends σko) algorithms milder dependence singular value σko) might able algorithm appendix proof theorem section give proof theorem following similar line proof theorem bound estimation errors errors introduced tensor decomposition step respectively. estimation errors tail inequatlities last section gives following estimation lemma suppose obtain i.i.d. samples. denote mean true underlying distribution denote empirical mean. deﬁne appendix moments observable variables proof proposition equations already proved give proof equation fact three equations proved similar manner. computations. first note computation requires time applies terms except term requires time space using naive implementations. therefore section devoted speed-up computation ofe. rk×k×k. document words. empirical tensor demanded. deﬁnition consider remaining values least indices identical. ﬁrst consider values indices same example indices need subtract nink term shown need compute whitened tensor diag matrix deﬁned previously. result computational complexity depends compute since diagonal matrix non-zero entries computed operations. therefore time complexity computing document.", "year": 2016}