{"title": "Fast Dempster-Shafer clustering using a neural network structure", "tag": ["cs.AI", "cs.NE", "I.2.3; I.2.6; I.5.3"], "abstract": "In this article we study a problem within Dempster-Shafer theory where 2**n - 1 pieces of evidence are clustered by a neural structure into n clusters. The clustering is done by minimizing a metaconflict function. Previously we developed a method based on iterative optimization. However, for large scale problems we need a method with lower computational complexity. The neural structure was found to be effective and much faster than iterative optimization for larger problems. While the growth in metaconflict was faster for the neural structure compared with iterative optimization in medium sized problems, the metaconflict per cluster and evidence was moderate. The neural structure was able to find a global minimum over ten runs for problem sizes up to six clusters.", "text": "abstract article study problem within dempster-shafer theory pieces evidence clustered neural structure clusters. clustering done minimizing metaconﬂict function. previously developed method based iterative optimization. however large scale problems need method lower computational complexity. neural structure found effective much faster iterative optimization larger problems. growth metaconﬂict faster neural structure compared iterative optimization medium sized problems metaconﬂict cluster evidence moderate. neural structure able global minimum runs problem sizes clusters. article study neural structure clustering evidence large scale problems within dempster-shafer theory studied problem concerns situation reasoning multiple events handled independently. clustering process separate evidence subsets handled separately. earlier work developed method based iterative optimization clustering evidence medium sized problems. method developed part multiple-target tracking algorithm antisubmarine intelligence analysis system subsequent paper developed classiﬁcation method incoming pieces evidence. here used prototypes order obtain faster classiﬁcation. prototypes derived previous clustering process. method increased computation speed iterative optimization small medium sized problems little larger problems. large scale problems became clear need method much lower computational complexity. achieve prepared sacriﬁce clustering performance necessary. solution described article based clustering neural structure. neural network learning weights network. instead weights directly method conﬂict dempster’s rule input setting weights. recent paper method extended simultaneous clustering determination number clusters iteration neural structure. here output signals neurons represent degree pieces evidence belong corresponding cluster. derive probability distribution regarding number clusters gradually iteration transformed determination number clusters. gradual determination back neural structure iteration inﬂuence clustering process. many ideas article inspired solution traveling salesman problem hopﬁeld tank used neural network effective method good shortest path several cities. section describe problem hand section give overview iterative optimization solution developed describe neural structure achieve effective clustering presenting comparison neural structure iterative optimization receive several pieces evidence different separate events pieces evidence mixed want arrange according event referring thus partition pieces evidence subsets subset refers particular event. ﬁgure subsets denoted combined dempster’s rule denoted here thirteen pieces evidence partitioned four subsets. number subsets uncertain also domain conﬂict conﬂict current hypothesis number subsets prior belief. partition simply allocation pieces evidence different events. since events anything other analyze separately. uncertain event pieces evidence referring problem. could impossible know directly different pieces evidence referring event. know subset not. problem problem organization. evidence different events want analyze unfortunately mixed facing problem separating them. solve problem conﬂict dempster’s rule pieces evidence within subset combined indication whether pieces evidence belong together. higher conﬂict less credible belong together. create additional piece evidence subset proposition adequate partition. simple frame discernment metalevel {adp short adequate partition. proposition take value equal conﬂict combination within subset pieces evidence regarding subset reason partition original evidence. confuse original evidence call evidence metalevel evidence combination analysis combination take place metalevel ﬁgure establish criterion function overall conﬂict called metaconﬂict function reasoning multiple events. metaconflict derived plausibility viewed piece metalevel evidence partitioning evidence subsets definition. metaconﬂict function conﬂict partitioning evidences disjoint subsets here conﬂict subset conﬂict subsets propositions possible different number subsets. minimizing metaconﬂict function method partitioning evidence subsets representing events. method also handle situation number events uncertain. method ﬁnding best partitioning based iterative minimization metaconﬂict function. step consequence transferring piece evidence subset another investigated. this subset refers different event reasoning take place event treated separately. ﬁxed number subsets minimum metaconﬂict function found iterative optimization among partitionings evidences different subsets. step optimization consequence transferring evidence several different pieces evidence favorably transferred favorable transfer evidence minimizes mcf*. remembered analysis concerns situation piece evidence transferred subset another. favorable simultaneously transfer pieces evidence deemed favorable individual transfer. algorithm like hill-climbing–like algorithms guarantees ﬁnding local global optimum. study series problems pieces evidence simple clustered clusters port functions elements thus always global minimum metaconﬂict function equal zero since take pieces evidence includes –element cluster remaining evidence take includes –element subset forth. since evidence cluster includes –element intersection nonempty evidence cluster includes –element intersection also nonempty etc. thus conﬂicts zero always global minimum makes easy standard efﬁciency clustering process. reason choose problem minimum metaconﬂict zero makes good test example evaluating performance. another problem used would knowledge global minimum evaluation would difﬁcult. reason believe choice test examples atypical respect network performance. choose architecture minimizes sum. thus make change function want minimize. take logarithm minus metaconﬂict function change minimizing minimizing sum. change minimization follows since minimum obtained ﬁnal minimal minimization ﬁnal yields result minimization would have. thus neural network weights directly dependent conﬂicts different pieces evidence rather −log conﬂict piece evidence; functions identical ﬁrst order terms thus actual minimization slightly overestimates conﬂict within subset. price achieve fast clustering. study calculations taking place neural network iteration. terminology hopﬁeld tank input voltages weighted input signals neuron output voltages output signal neuron inhibition terms negative weights. neuron calculate input voltage weighted signals column ﬁgure previous input voltage previous iteration plus gain factor times weighted output voltages neurons column plus excitation bias minus previous input voltage nmn. column output voltages weighted data-term inhibition times weight conﬂict plus global inhibition; gain factor. iniexperiments used following parameter settings tially lowered problem size grew. assure inhibitory signals column overwhelm signals column length grows like length grows like problem size grows. excitation bias number columns i.e. clusters. task parameter tuning grows size neural network. larger problem might necessary thing automatically although done here. finally input voltage calculate output voltage atanh hyperbolic tangent. initial input voltage chosen uniformly interval iteration voltages calculated results previous iteration. continues convergence reached. long weights neural network symmetric convergence always guaranteed. always case since factor varies conﬂict pieces evidence. thus weights equal. iteration need make special checks. first assure output voltages neurons decrease iteration. could possibly lead piece evidence corresponding clustered all. happens output voltages decreased least unchanged. control plus fact logical conditions dataterms column neuron makes problem easier hopﬁeld tank’s model traveling salesman problem. logical conditions column plus data-terms previous next columns neuron. allows avoid problems convergence performance described wilson pawley hopﬁeld tank model. secondly check highest output voltages row. highest output voltage greater equal output voltages second highest output voltage regardless value highest output voltage highest output voltage done merely speed convergence. ﬁgure convergence -neural network rows columns clustering pieces evidence subset shown. leads figure different states neural network neurons. left right convergence clustering pieces evidence clusters ﬁrst eleventh iteration. snap-shot iteration columns represent cluster rows represent piece evidence. linear dimension square proportional output voltage neuron represent degree pieces evidence belong cluster. ﬁnal state output voltage four output voltages piece evidence represented clustered cluster output voltage global optimum found iterations. convergence achieved conﬂict within cluster i.e. column calculated combining pieces evidence output voltage column conﬂict subset calculate overall metaconﬂict previous formula. section investigate clustering performance computation time clustering processes neural structure iterative optimization. make comparison problem size grows. problem sizes clustering pieces evidence subsets. reported evidence support different subsets frame thus know metaconﬂict function global minimum metaconﬂict equal zero. ﬁgure notice iterative optimization exponential computation time number items evidence. neural structure much lower complexity although higher computation time small problems. problems subsets pieces evidence iterative optimization fastest subsets pieces evidence neural structure vastly superior. study clustering performance global optimum? best different runs methods manages global optimum problem sizes three subsets. however also notice median mean minimum metaconﬂict much higher neural structure iterative optimization. neural structure mean conﬂicts problem sizes three seven iterative optimization mean conﬂicts problem sizes three six. serious metaconﬂict? make assumption local minimum different conﬂicts clusters equal certainly global minimum calculate average conﬂict cluster. ﬁgure grows much slower total metaconﬂict. thus large part growth metaconﬂict depends increased number clusters whose conﬂicts becomes additional terms metaconﬂict function. notice cluster problem roughly pieces evidence cluster. pieces evidence average basic probability number still median conﬂict cluster investigate performance closer still ﬁrst study probability conﬂict different pieces evidence. pieces evidence simple support functions elements subsets study best clustering seven cluster problem. conﬂicts respectively seven different clusters. median conﬂict cluster. cluster contains pieces evidence. contains –element. remaining support respectively. least elements also present piece evidence. pieces evidence conﬂict thus pairs evidence cluster pairs conﬂict. small price obtain effective clustering. chosen random selected items could expected pairs conﬂict demonstrated neural structure effective clustering evidence large scale problems. trials pieces evidence clustered clusters neural structure faster iterative optimization problems clustering pieces evidence clusters larger. best runs found global optimum methods problem sizes clusters median metaconﬂict higher neural structure. however since good best found median conﬂict cluster evidence moderate deemed acceptable.", "year": 2003}