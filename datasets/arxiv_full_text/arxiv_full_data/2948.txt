{"title": "Learning Hierarchical Sparse Representations using Iterative Dictionary  Learning and Dimension Reduction", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "This paper introduces an elemental building block which combines Dictionary Learning and Dimension Reduction (DRDL). We show how this foundational element can be used to iteratively construct a Hierarchical Sparse Representation (HSR) of a sensory stream. We compare our approach to existing models showing the generality of our simple prescription. We then perform preliminary experiments using this framework, illustrating with the example of an object recognition task using standard datasets. This work introduces the very first steps towards an integrated framework for designing and analyzing various computational tasks from learning to attention to action. The ultimate goal is building a mathematically rigorous, integrated theory of intelligence.", "text": "paper introduces elemental building block combines dictionary learning dimension reduction show foundational element used iteratively construct hierarchical sparse representation sensory stream. compare approach existing models showing generality simple prescription. perform preliminary experiments using framework illustrating example object recognition task using standard datasets. work introduces ﬁrst steps towards integrated framework designing analyzing various computational tasks learning attention action. ultimate goal building mathematically rigorous integrated theory intelligence. working towards computational theory intelligence develop computational framework inspired ideas neuroscience. speciﬁcally integrate notions columnar organization hierarchical structure sparse distributed representations sparse coding. integrated view intelligence proptosed karl friston based free-energy framework intelligence viewed surrogate minimization entropy sensorium. work intuitively inspired view aiming provide computational foundation theory intelligence perspective theoretical computer science thereby connecting ideas mathematics. building foundations principled approach computational essence problems isolated formalized relationship fundamental problems mathematics theoretical computer science illuminated full power available mathematical techniques brought bear. computational approach focused developing tractable algorithms. exploring complexity limits intelligence. thereby improving quality available guarantees evaluating performance models improving comparisons among models moving towards provable guarantees sample size time complexity generalization error assumptions prior. furnishes solid theoretical foundation used among things basis building artiﬁcial intelligence. speculation cortical micro-circuit element dates back mountcastle’s observation cortical column serve algorithmic building block neocortex later work mumford hawkins george attempted investigation process. bottom-up organization cortex generally assumed hetrarchical topology columns. modeled directed acyclic graph usually simpliﬁed hierarchical tree. work poggio serre dean discuss hierarchical topology. smale attempts develop theory accounting importance hierarchical structure work modeling early stages sensory processing olshausen using sparse coding produced results account observed receptive ﬁelds early visual processing. usually done learning overcomplete dictionary. however remained unclear extend higher layers. work partially viewed progress direction. computational learning theory formal study learning algorithms. deﬁnes natural setting analyzing algorithms. however notable exceptions produced guarantees divorced practice. without tight guarantees machine learning studied using experimental results standard benchmarks problematic. closing theory practice providing stronger assumptions structures forms considered theory constraints inspired biology complex systems. several hierarchical models introduced literature. h-max based simple-complex cell hierarchy hubel wiesel. basically hierarchical succession template matching max-operations corresponding simple complex cells respectively hierarchical temporal memory learning model composed hierarchy spatial coincidence detection temporal pooling. coincidence detection involves ﬁnding spatial clustering input temporal pooling ﬁnding variable order markov chains describing temporal sequences data. h-max mapped straightforward manner. transformations data remains invariant learned temporal pooling step. h-max explicitly hard codes translational transformations operation. gives h-max better sample complexity speciﬁc problems translational invariance present. bouvrie introduced generalization hierarchical architectures centered around foundational element involving steps filtering pooling. filtering described reproducing kernel standard inner product gaussian kernel e−γx−y pooling remaps result single value. examples pooling functions include mean norm h-max convolutional neural nets deep feedforward neural networks belong category hierarchical architectures corresponding diﬀerent choices kernel pooling functions. model fall within bouvrie’s present framework viewed generalization hierarchical models bouvrie’s framework special case. friston proposed hierarchical dynamic models similar mentioned architectures framed control theoretic framework operating continuous time computational formalism approach thus prohibitively diﬃcult. approach circuit element attempt abstract computationally fundamental processes. conjecture class possible circuit elements bottom-up processing sensory stream. feedback processes mediating action attention incorporated model similar work chikkerur generically theory friston choose leave feedback future work allowing focus basic aspects model. following section introduces novel formulation elemental building block could serve bottom-up piece common cortical algorithm. circuit element combines dictionary learning dimension reduction formally introducing drdl show used iteratively construct hierarchical sparse representations sensory stream. comparisons relevant known models presented. gain insight model applied standard vision datasets. immediately leads classiﬁcation algorithm uses feature extraction. appendix discuss assumptions prior naturally expressed framework. circuit element simple concatenation dictionary learning step followed dimension reduction step. using overcomplete dictionary increase dimension data since data sparse compressed sensing obtain dimension reduction. extended maximum a-posteriori probability setting diﬀerent priors take account preferences recovered dictionary. similarly k-svd uses step iterative process truncated singular value decomposition update done taking every atom applying restricted columns contribution atom. step learns representation input lives high dimension obtain lower dimensional representation since readily seen sparse standard orthonormal basis dimension obtain dimension reduction using applying linear operator satisfying restricted isometry property compressed sensing theory. given s-sparse vector dimension reduces dimension since using matrix eﬃcient decompression guaranteed using approximation. data recovered exactly using minimization algorithms basis pursuit. matrices obtained probabilistically matrices random gaussian entries. alternatively matrices obtained using sparse random matrices paper follow latter approach. question deterministically constructing matrices similar bounds still open. columns dictionary learned correspond drawn vectors data expressed simply vector coeﬃcient corresponding inner product closest vector zero otherwise. produces sparse vector dimension apply dimension reduction preserves distances representations. representations correspond standard basis best dimension reduction simply projection representations onto plane perpendicular whereby points unit basis project vertices triangle illustrated figure forms output current node. symmetry drdl fact steps intimately related. show relationship clearly rewrite problems variable names. variables relevant section. problems stated drdl thought memory system dimension reduction technique data expressed sparsely dictionary. parameter trade-oﬀ number columns sparsity representation. hand note step puts data dimension. therefore desire maximize reduction dimension increasing raising constant power comparable multiplying means much rather increase number columns dictionary sparsity. hand increasing number columns forces columns highly correlated. become problematic basis pursuit vector selection. trade-oﬀ highlights importance investigating approaches dictionary learning vector selection beyond current results highly coherent dictionaries. discuss topic future papers. assume hierarchical architecture modeling topographic organization visual cortex singular drdl element factorized expressed tree simpler drdl elements. architecture learn hierarchical sparse representation iterating drdl elements. models assumes data generated hierarchy spatiotemporal invariants. given level node generative model assumed composed small number features generation proceeds recursively decompressing pattern parent nodes producing patterns child nodes. input learning algorithm bellow. paper assume topology generative model spatial temporal extent node known. discussion algorithms learning topology internal dimensions left future work. consider simple data stream consisting spatiotemporal sequences generative model deﬁned above. figure shows potential learning hierarchy. simple vision problems consider dictionaries within layer same. paper processing proceeds bottom-up hierarchy only. recursively divide spatiotemporal signal obtain tree representing known topographic hierarchy spatiotemporal blocks. block level then starting bottom tree note domains computer vision reasonable assume dictionaries level algorithm attempts mirror generative model. outputs inference algorithm induces hierarchy sparse representations given data point. used abstract invariant features data. supervised learning algorithm invariant features solve classiﬁcation problems. data points representation obtained analogy learning algorithm recursively dividing spatiotemporal signal obtain tree representing known topographic hierarchy spatiotemporal blocks. representation inferred naturally iteratively applying vector selection compressed sensing. vector selection employ common variational technique called basis pursuit de-noising minimizes limitation practice since desirable highly coherent dictionaries. representation inference proceeded iteratively applying basis pursuit compressing resulting sparse representation using matrix. simple powerful model used basis investigation tool future work. abstracted model need conceptual simplicity generality. several models inspired neocortex proposed many sharing similar characteristics. present compare model. h-max mapped replacing basis pursuit sparse approximation template matching. h-max uses random templates data whereas uses dictionary learning. ’max’ operation h-max understood terms operation sparse coding produces local competition among features representing slight variations data. alternatively ’max’ operation viewed limited form dimension reduction. mapped considering time another dimension space. bounded variable order markov chains written sparse approximation spatiotemporal vectors representing sequences spacial vectors time. constructs spatial coincidences automatically shared across time. moving window spatial sequences. alternatively simulate alternating time-only space-only spatiotemporally extended blocks. view single node mapped layer nodes representing time representing space. unlike treating time space equal footing added advantage algorithms used used both. winner-takes-all policy mapped model assuming sparsity distribution belief states mapped model template matching instead sparse approximation inference step. finally leverage dimension reduction step. uses feedback connections prediction restricted predictions forward time. extending feedback connections accounts dependency nodes connected directly enables feedback aﬀect space-time. advantage approach providing invertibility without dimensionality blow-up hierarchically sparse data. models falling bouvrie al’s framework loose information proceed hierarchy pooling operation. becomes problematic extending models incorporate feedback. moreover information loss forces algorithm designer hardcode invariances particular model must select hand invertible models suﬀer dimensionality blow number features learned given level greater input dimension level usually case. dimensionality reduction achieves savings computational resources well better noise resilience avoiding overﬁtting. dictionary learning represents data sparse combinations dictionary columns. viewed regularization provides noise tolerance better generalization. type regularization intuitive well motivated neuroscience organization complex systems. approach departs current models simple template matching leverages expressive power sparse approximation. provides disciplined prescription learning features every level. finally conceptually simple model. elegance lends well analysis. involve learning dictionary columns sparsity using samples dimension using layers ﬁrst layer learning dictionary size sparsity using samples dimension second layer learns dictionary size columns samples dimension eﬀect adding layer divided problem section elaborate preliminary numerical experiments performed drdl basic standard machine learning datasets. applied model mnist coil datasets subsequently used representation feature extraction step classiﬁcation algorithm support vector machines k-nearest neighbors practice additional prior assumptions included model discussed appendix. applied model pairs mnist data set. step tried random matrices sparse-random matrices tested eﬃcacy approach reconstructing training data basis pursuit. used layers. features learned applied k-nn refrained tweaking initial parameters since expect model work oﬀ-shelf. layer drdl obtained error rate standard deviation using layers obtained error standard deviation applied model pairs coil- data consists images every class. used labeled images class training. taken equally spaced angles used procedure obtaining checking matrix. applied single layer drdrl trained k-nn also refrained tweaking initial parameters. obtained mean error introduced novel formulation elemental building block could serve bottom-up piece common cortical algorithm. model leads several interesting theoretical questions. appendix illustrate additional prior assumptions generative model expressed within integrated framework. furthermore framework also extended address feedback attention action complimentary learning role time. future work closely probe issues propose", "year": 2011}