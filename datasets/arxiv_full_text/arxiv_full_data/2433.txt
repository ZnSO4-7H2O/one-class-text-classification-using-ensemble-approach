{"title": "Learning Gaussian Networks", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We describe algorithms for learning Bayesian networks from a combination of user knowledge and statistical data. The algorithms have two components: a scoring metric and a search procedure. The scoring metric takes a network structure, statistical data, and a user's prior knowledge, and returns a score proportional to the posterior probability of the network structure given the data. The search procedure generates networks for evaluation by the scoring metric. Previous work has concentrated on metrics for domains containing only discrete variables, under the assumption that data represents a multinomial sample. In this paper, we extend this work, developing scoring metrics for domains containing all continuous variables or a mixture of discrete and continuous variables, under the assumption that continuous data is sampled from a multivariate normal distribution. Our work extends traditional statistical approaches for identifying vanishing regression coefficients in that we identify two important assumptions, called event equivalence and parameter modularity, that when combined allow the construction of prior distributions for multivariate normal parameters from a single prior Bayesian network specified by a user.", "text": "describe metrics bayesian networks statistical data. previ­ user knowledge metrics work concentrated mains containing discrete assumption multinomial sample. paper tend work developing domains assumption sampled several bayesian herskovits halter beckerman distribution. previously tivariate normal working solution discrete tually eliminate tion metrics polynomial mulitivariate discrete exponential work viewed extension statistical vanishing sion coefficients particular assumptions containing modularity continuous variables. modularity addresses distributions network says bayesian-network sent independence correspond score. show that combined sumptions distributions single identification event equivalence arises subtle distinction works. first type called belief sents assertions dependence. represents sertions argue metrics satisfy score-equivalent similar metrics described ritzen except metrics score directed networks works. paper concentrate rather undirected lieve users find former easier build interpret. belief-network conditional pdfs corresponding structure. ular directed variable parepts node corresponding nodes corresponding remainder paper refer variable sociated union determined properties imal gaussian given multivariate normal density generate gaussian belief network vice versa. uncondi­ tional means representations. shachter kenley mul­ gaussian-belief-network representation better suited model tivariate standard rep­ elicitation assess resentation gaussian belief network user needs specify unconditional relative importance parent termining values child variance given parents conditional trast assessing directly needs guarantee assessed covariance matrix positive-definite-a manner correlations user. /i;b normalization however many network structures therefore order determine constant. score. term also problematic gument probability. network structure event. thus need def­ inition assuming struc­ ture complete gaussian belief network. com­ plete gaussian edges. applying property event equivalence know event associated lief network same; b'sc denote event. thought equation equivalent sample size m-that equivalent number cases user seen since ignorant cases seen posterior mean updated weighted average prior mean computed based cases mean based cases. furthermore x-;. random sample multivariate vector precision equation thus interpret user's equivalent sample matrix note must least number variables section researchers first researchers make assumption importance generating eter modularity current development. particular conjunction previous assumptions term form given equa­ tion multiplying equation tain metric arbitrary gaussian belief net­ work call metric stands bayesian metric gaussian networks score equivalence. score equivalence making assumptions dence parameter modularity specified prior densities eters terms ofthe structure sequently possibility violates theorem however demonstrates cation implies score equivalence. proof heckerman show belief network structure trans­ structure series formed isomorphic reversals that whenever prove case differ single reversal isomorphic network struc­ tures differ direction previous discussion three components user's prior knowledge relevant learning gaussian networks prior equivalent sample sizes probabilities parameters assess­ ment prior probabilities straightfor­ ward. buntine example describe meth­ facilitate user assess equivalent section concentrate whereas using gaussian belief network assessing multivariate therefore assum­ obtain structure multiplying structure find network highest posterior database table generated net­ work structure (with parameters shown properties allow statistician parameters tribution given single network user. provided legitimate multivariate model inappropriate turn general mixture geiger domains subject ables decomposed conditioned crete variables. bined approximation data provides variate mixtures. discrete rameter data; met­ overfit structure rics developed domains provide avoid overfitting. uous case complete parameters. duced methods exponential tures high scores decreasing events associated concern structure assertion ents. thus contrast appropriate properties equivalence score equivalence. con­ example sider domain containing causal network points causal network points represent assertion however causes whereas sertion equal. indeed events-and different clusive.", "year": 2013}