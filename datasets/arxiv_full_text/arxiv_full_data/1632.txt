{"title": "Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep  Reinforcement Learning", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Building a dialogue agent to fulfill complex tasks, such as travel planning, is challenging because the agent has to learn to collectively complete multiple subtasks. For example, the agent needs to reserve a hotel and book a flight so that there leaves enough time for commute between arrival and hotel check-in. This paper addresses this challenge by formulating the task in the mathematical framework of options over Markov Decision Processes (MDPs), and proposing a hierarchical deep reinforcement learning approach to learning a dialogue manager that operates at different temporal scales. The dialogue manager consists of: (1) a top-level dialogue policy that selects among subtasks or options, (2) a low-level dialogue policy that selects primitive actions to complete the subtask given by the top-level policy, and (3) a global state tracker that helps ensure all cross-subtask constraints be satisfied. Experiments on a travel planning task with simulated and real users show that our approach leads to significant improvements over three baselines, two based on handcrafted rules and the other based on flat deep reinforcement learning.", "text": "building dialogue agent fulﬁll complex tasks travel planning challenging agent learn collectively complete multiple subtasks. example agent needs reserve hotel book ﬂight leaves enough time commute arrival hotel check-in. paper addresses challenge formulating task mathematical framework options markov decision processes proposing hierarchical deep reinforcement learning approach learning dialogue manager operates different temporal scales. dialogue manager consists top-level dialogue policy selects among subtasks options low-level dialogue policy selects primitive actions complete subtask given top-level policy global state tracker helps ensure cross-subtask constraints satisﬁed. experiments travel planning task simulated real users show approach leads signiﬁcant improvements three baselines based handcrafted rules based deep reinforcement learning. growing demand intelligent personal assistants mainly form dialogue agents help users accomplish tasks ranging meeting scheduling vacation planning. however popular agents today’s market amazon echo apple siri google home microsoft cortana handle simple tasks reporting weather requesting songs. building dialogue agent fulﬁll complex tasks remains fundamental challenges community general. paper consider important type complex tasks termed composite task consists subtasks need fulﬁlled collectively. example order make travel plan need book tickets reserve hotel rent etc. collective satisfy cross-subtask constraints call slot constraints. examples slot constraints travel planning hotel check-in time later ﬂight’s arrival time hotel check-out time earlier return ﬂight depart time number ﬂight tickets equals hotel check-in people common learn task-completion dialogue agent using reinforcement learning cuay´ahuitl williams dhingra recent examples. compared dialogue agents developed individual domains composite task presents additional challenges commonly used approaches ﬁrst challenge reward sparsity. dialogue policy learning composite tasks requires exploration much larger state-action space often takes many conversation turns user agent fulﬁll task leading much longer trajectory. thus reward signals delayed sparse. show paper typical methods naive \u0001-greedy exploration rather inefﬁcient. second challenge satisfy slot constraints across subtasks. requirement makes existing methods learning multidomain dialogue agents inapplicable methods train collection policies domain cross-domain constraints required successfully complete dialogue. third challenge improved user experience experiments agent tends switch different subtasks frequently conversing users. incoherent conversations lead poor user experience main reasons cause dialogue session fail. paper address mentioned challenges formulating task using mathematical framework options mdps proposing method combines deep reinforcement learning hierarchical task decomposition train composite taskcompletion dialogue agent. heart agent dialogue manager consists top-level dialogue policy selects subtasks low-level dialogue policy selects primitive actions complete given subtask global state tracker helps ensure cross-subtask constraints satisﬁed. conceptually approach exploits structural information composite tasks efﬁcient exploration. speciﬁcally order mitigate reward sparsity issue equip agent evaluation module gives intrinsic reward signals indicating likely particular subtask completed based current state generated global state tracker. intrinsic rewards viewed heuristics encourage agent focus solving subtask moving another subtask. experiments show intrinsic rewards used inside hierarchical agent make exploration efﬁcient yielding signiﬁcantly reduced state-action space decision making. furthermore leads better user experience resulting conversations switch subtasks less frequently. task-completion dialogue systems attracted numerous research efforts. reinforcement learning algorithms hold promise dialogue policy optimization time experience recent advances deep learning inspired many deep reinforcement learning based dialogue systems eliminate need feature engineering work focuses single-domain problems. extensions composite-domain dialogue problems non-trivial several reasons state action spaces much larger trajectories much longer turn reward signals much sparse. challenges addressed hierarchical reinforcement learning decomposes complicated task simpler subtasks possibly recursive way. different frameworks proposed hierarchies machines maxq decomposition paper choose options framework conceptual simplicity generality details found next section. work also motivated hierarchicaldqn integrates hierarchical value functions operate different temporal scales. model achieved superior performance complicated atari game montezuma’s revenge hierarchical structure. related different extension singledomain dialogues multi-domain dialogues domain handled separate agent contrast compositedomain dialogues studied paper conversation multi-domain dialogue normally involves domain completion task require solving sub-tasks different domains. consequently work multi-domain dialogues focuses different technical challenges transfer learning across different domains lstmbased language understanding module identifying user intents extracting associated slots; state tracker tracking dialogue state; dialogue policy selects next action based current state; model-based natural language generator converting agent actions natural language responses. typically dialogue manager contains state tracker dialogue policy. implementation global state tracker maintain dialogue state accumulating information across subtasks thus helping ensure inter-subtask constraints satisﬁed. rest section describe dialogue policy details. subtasks completed collectively. process natural hierarchy top-level process selects subtasks complete lowlevel process chooses primitive actions complete selected subtask. hierarchical decision making processes formulated options framework options generalize primitive actions higher-level actions. different traditional setting agent choose primitive action time step options agent choose multi-step action example could sequence primitive actions completing subtask. pointed sutton options closely related actions family decision problems known semimarkov decision processes. following sutton option consists three components states option initiated intra-option policy selects primitive actions option control termination condition speciﬁes option completed. composite task travel planning subtasks like bookﬂight-ticket reserve-hotel modeled options. consider example option bookﬂight-ticket initiation state contains states tickets issued destination trip long away enough ﬂight needed; intra-option policy requesting conﬁrming information regarding departure date number seats etc.; also termination condition conﬁrming information gathered correct ready issue tickets. options mdps consider following process completing composite task agent ﬁrst selects subtask takes sequence actions gather related information users’ requirements subtask completed ﬁnally chooses next subtask complete. composite task fulﬁlled policy sequences options much consider intra-option policy sequences actions. propose method combines deep reinforcement learning hierarchical value functions learn composite taskcompletion dialogue agent shown figure two-level hierarchical reinforcement learning agent consists top-level dialogue policy low-level dialogue policy shown figure top-level policy perceives state environment selects subtask possible subtasks. low-level policy shared options. takes input state subtask outputs primitive action primitive actions subtasks. subtask remains constant input terminal state reached terminate internal critic dialogue manager provides intrinsic reward indicating whether subtask hand solved; signal used optimize πag. note state contains global information keeps track information subtasks. top-level low-level policies learned deep q-learning methods like dqn. speciﬁcally top-level dialogue policy estimates optimal q-function satisﬁes following following previous studies apply commonly used performance boosting methods target networks experience replay. experience replay tuples sampled experience replay buffers respectively. detailed summary learning algorithm hierarchical dialogue policy provided appendix dataset study made human-human conversation data derived publicly available multi-domain dialogue corpus collected using wizard-ofoz approach. made changes schema data composite taskcompletion dialogue setting. speciﬁcally added inter-subtask constraints well user preferences data mainly used create simulated users explained shortly. user simulator training reinforcement learners challenging because need environment interact with. dialogue research community common simulated users shown figure purpose work adapted publiclyavailable user simulator developed composite task-completion dialogue setting using human-human conversation data described section training simulator provides agent reward signal dialogue. dialogue considered successful travel plan made successfully information provided agent satisﬁes user’s constraints. dialogue agent receives positive reward ∗max turn success negative reward −max turn failure. furthermore turn agent receives reward shorter dialogue sessions encouraged. user goal user goal represented slots indicating user’s request requirement preference. example inform slot city=honolulu indicates user requirement request slot price=? indicates user asking agent information. experiment compiled list user goals using slots collected humanhuman conversation data described section follows. ﬁrst extracted slots appear dialogue sessions. slot multiple values like city= consider user preference user later revise value explore different options course dialogue. slot value treat user requirement unlikely negotiable. slot value treat user request. removed slots user goals values exist database. compiled user goals contains entries containing slots least subtasks book-ﬂight-ticket reserve-hotel. user type compare different agents’ ability adapt user preferences also constructed three additional user goal sets representing three different types users respectively type informed slots user goal single value. users hard constraints ﬂight hotel preference subtask accomplish ﬁrst. type least informed slots book-ﬂight-ticket subtask multiple values user prefers start book-ﬂight-ticket subtask. user receives ticket available agent conversation willing explore alternative slot values. type similar type least informed slots reserve-hotel subtask user goal multiple values. user prefers start reserve-hotel subtask. user receives room available response agent willing explore alternative slot values. agent size hidden layer agent top-level lowlevel dialogue policies hidden layer size rmsprop applied optimize parameters. batch size training used \u0001-greedy strategy exploration. simulation epoch simulated dialogues stored state transition tuples experience replay buffer. simulation epoch model updated transition tuples buffer batch manner. experience replay strategy critical success deep reinforcement learning. experiments beginning used rule-based agent dialogues populate experience replay buffer implicit imitation learning initialize agent. then agent accumulated state transition tuples ﬂushes replay buffer current agent reached success rate threshold worse rule agent. strategy motivated following observation. initial performance agent often strong enough result dialogue sessions reasonable success rate. data easy agent learn locally optimal policy failed fast; policy would ﬁnish dialogue immediately agent could suffer least amount per-turn penalty. therefore provided rule-based examples succeeded reasonably often ﬂush buffer performance agent reached acceptable level. generally threshold success rate rule agent. make fair comparison type users used rule agent initialize agent agent. simulated user evaluation composite task-completion dialogue task compared agent baseline agents terms three metrics success rate average rewards average number turns dialogue session. figure shows learning curves four agents trained different types users. learning curve averaged runs. table shows performance test data. types users hrl-based agent yielded robust dialogue policies outperforming hand-crafted rule-based agents rl-based agent measured success rate. also needed fewer turns dialogue session accomplish task rule-based agents agent. results table performance three agents different user types. tested dialogues using best model training. succ. success rate turn average turns reward average reward. figure performance agent versus agent tested real users success rate number tested dialogues p-values indicated bar; rightmost green ones total first agent signiﬁcantly outperformed agent. this large degree attributed hierarchical structure proposed agent. speciﬁcally top-level dialogue policy selected subtask agent focus time thus dividing complex task sequence simpler subtasks. selected subtasks combined intrinsic rewards alleviated sparse reward longhorizon issues helped agent explore efﬁciently state-action space. result shown figure table performance agent types users drop much compared type users despite increased search space former. additionally observed large drop performance agent increased complexity task required dialogue turns posed challenge temporal credit assignment. table sample dialogue agents real user left column shows dialogue agent; right column shows dialogue agent; bolded slots joint constraints subtasks. agent welcome travel agent. know original city destination city departure date? user depart campinas cancun prefer leave agent return ﬂight hotel checkout date too. user wife travel three people total. user depart time? agent hotel called hotel tropic available you. user depart time available? agent welcome travel agent. know original city destination city departure date? user depart campinas cancun prefer leave agent business class seat available. agent departure time user departure time return ﬂight? agent user seat available? agent business class available. user what’s price ﬂight? agent total dollars ﬂight. user what’s price hotel? agent dollars hotel. user could please help book tickets? agent departure ﬂight hotel checkin date available. user could please help book tickets? agent okay. tickets issued. user thanks agent thanks success agent behave naturally inter-subtask transition book-ﬂight-ticket reserve-hotel. agent departure time user wife travel three people total. agent return ﬂight departed user date return ﬂight? agent hotel checkout date also user what’s price ﬂight? agent total dollars ﬂight. user hotel available? agent hotel called hotel tropic available you. user what’s price hotel? agent dollars hotel. user could please help book tickets? agent okay. tickets issued. finally also found rule+ agents comparable success rates shown figure however closer look correlation success rate average number turns table suggests rule+ agent required turns adversely affects success whilst agent achieves similar success much less number turns user types. sufﬁces hierarchical agent outperforms terms success rate depicted figure evaluated agents trained simulated users real human users recruited authors’ afﬁliation. conducted study using agents tested types users type users preference subtask type users preferred complete book-ﬂightticket subtask ﬁrst. note type users symmetric type ones included study. compared pairs words four agents trained speciﬁc user types. dialogue session agents randomly picked converse user. user presented user goal sampled corpus instructed converse agent complete task. slots goal multiple values user multiple choices slot might revise slot value agent replied message like ticket available conversation. session user asked give rating scale based naturalness coherence dialogue. collected total dialogue sessions human users. figure presents performance agents real users terms success rate. figure shows comparison user rating. cases agent consistently better agent terms success rate user rating. table shows sample dialogue session. agent produced coherent conversation switched among subtasks much less frequently agent. paper considers composite task-completion dialogues subtasks need fulﬁlled collectively entire dialogue successful. formulate policy learning problem using options framework take hierarchical deep approach optimizing policy. experiments simulated real users show hierarchical agent signiﬁcantly outperforms agent rule-based agents. hierarchical structure agent also improves coherence dialogue ﬂow. promising results suggest several directions future research. first hierarchical approach demonstrates strong adaptation ability tailor dialogue policy different types users. motivates systematically investigate dialogue personalization. second hierarchical agent implemented using two-level dialogue policy. complex tasks might require multiple levels hierarchy. thus valuable extend approach handle deep hierarchies subtask invoke another subtask taking full advantage options framework. finally designing task hierarchies requires substantial domain knowledge time-consuming. challenge calls future work automatic learning hierarchies complex dialogue tasks. bhuwan dhingra lihong xiujun jianfeng yun-nung chen faisal ahmed deng. end-to-end reinforcement learning dialogue agents information access. proceedings annual meeting association computational linguistics layla asri hannes schulz shikhar sharma jeremie zumer justin harris emery fine rahul mehrotra kaheer suleman. frames corpus adding memory goal-oriented dialogue systems. arxiv preprint arxiv.. milica gasic dongho pirros tsiakoulis steve young. distributed dialogue policies multi-domain statistical dialogue manage ieee international conference ment. acoustics speech signal processing icassp south brisbane queensland australia april pages ieee. milica gasic nikola mrksic pei-hao david vandyke tsung-hsien steve young. policy committee adaptation multi ieee domain spoken dialogue systems. workshop automatic speech recognition understanding asru scottsdale december pages ieee. dilek hakkani-t¨ur gokhan asli celikyilmaz yun-nung chen jianfeng deng yeyi wang. multi-domain joint semantic frame proparsing using bi-directional rnn-lstm. ceedings annual meeting international speech communication association. tejas kulkarni karthik narasimhan ardavan saeedi josh tenenbaum. hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. advances neural information processing systems annual conference neural information processing systems december barcelona spain pages esther levin roberto pieraccini wieland eckert. stochastic model human-machine interieee trans. action learning dialog strategies. speech audio processing xiujun yun-nung chen lihong jianfeng asli celikyilmaz. investigation language understanding impact reinforcement arxiv preprint learning based dialogue systems. arxiv.. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature jost schatzmann blaise thomson karl weilhammer steve young. agenda-based user simulation bootstrapping pomdp diahuman language technology logue system. conference north american chapter association computational linguistics proceedings april rochester york pages association computational linguistics. konrad schefﬂer steve young. probabilistic simulation human-machine dialogues. ieee international conference acoustics speech signal processing. icassp june hilton hotel convention center istanbul turkey pages ieee. satinder singh. reinforcement learning hierarchy abstract models. proceedings national conference artiﬁcial intelligence. jose july pages aaai press press. pei-hao milica gasic nikola mrksic lina rojasbarahona stefan ultes david vandyke tsunghsien steve young. continuously learning neural dialogue management. arxiv preprint arxiv.. richard sutton doina precup satinder singh. intra-option learning temporally abstract actions. proceedings fifteenth international conference machine learning madison wisconsin july pages tsung-hsien milica gasic nikola mrksic peihao david vandyke steve young. semantically conditioned lstm-based natural language generation spoken dialogue systems. proceedings conference empirical methods natural language processing emnlp lisbon portugal september pages jason williams kavosh asadi geoffrey zweig. hybrid code networks practical efﬁcient end-to-end dialog control supervised reinforcement learning. proceedings annual meeting association computational linguistics kaisheng baolin peng zhang dong geoffrey zweig yangyang shi. spoken language understanding using long short-term memory ieee spoken language neural networks. technology workshop south lake tahoe december pages user simulator user goal task-completion dialogue setting ﬁrst step user simulator generate feasible user goal. generally user goal deﬁned types slots request slots user know value expects agent provide conversation; inform slots slot-value pairs user know mind serving soft/hard constraints dialog; slots multiple values termed soft constraints means user preference user might change value result returned agent based current values; otherwise slots value serve hard constraint. table shows example user goal composite task-completion dialogue. first user work focuses userinitiated dialogues randomly generate user action ﬁrst turn make ﬁrst user-act reasonable constraints generation process. example ﬁrst user turn inform request turn; least informable slots user knows original destination cities city city appear ﬁrst user turn etc.; intent ﬁrst turn request contain requestable slot. course dialogue user simulator maintains compact stack-like representation named user agenda user state factored agenda goal consists constraints request timestep user simulator generate next user action based current status last agent action amt− update current status here training testing policy without natural language understanding module error model exploration probability execute action obtain next state description perceive extrinsic reward environment obtain intrinsic reward internal critic sample random minibatch transitions", "year": 2017}