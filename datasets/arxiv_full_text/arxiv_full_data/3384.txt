{"title": "Online Feature Selection with Group Structure Analysis", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Online selection of dynamic features has attracted intensive interest in recent years. However, existing online feature selection methods evaluate features individually and ignore the underlying structure of feature stream. For instance, in image analysis, features are generated in groups which represent color, texture and other visual information. Simply breaking the group structure in feature selection may degrade performance. Motivated by this fact, we formulate the problem as an online group feature selection. The problem assumes that features are generated individually but there are group structure in the feature stream. To the best of our knowledge, this is the first time that the correlation among feature stream has been considered in the online feature selection process. To solve this problem, we develop a novel online group feature selection method named OGFS. Our proposed approach consists of two stages: online intra-group selection and online inter-group selection. In the intra-group selection, we design a criterion based on spectral analysis to select discriminative features in each group. In the inter-group selection, we utilize a linear regression model to select an optimal subset. This two-stage procedure continues until there are no more features arriving or some predefined stopping conditions are met. %Our method has been applied Finally, we apply our method to multiple tasks including image classification %, face verification and face verification. Extensive empirical studies performed on real-world and benchmark data sets demonstrate that our method outperforms other state-of-the-art online feature selection %method methods.", "text": "online selection dynamic features attracted intensive interest recent years. however existing online feature selection methods evaluate features individually ignore underlying structure feature stream. instance image analysis features generated groups represent color texture visual information. simply breaking group structure feature selection degrade performance. motivated fact formulate problem online group feature selection. problem assumes features generated individually group structure feature stream. best knowledge ﬁrst time correlation among feature stream considered online feature selection process. solve problem develop novel online group feature selection method named ogfs. proposed approach consists stages online intra-group selection online inter-group selection. intra-group selection design criterion based spectral analysis select discriminative features group. inter-group selection utilize linear regression model select optimal subset. two-stage procedure continues features arriving predeﬁned stopping conditions met. finally apply method multiple tasks including image classiﬁcation face veriﬁcation. extensive empirical studies performed real-world benchmark data sets demonstrate method outperforms state-of-the-art online feature selection methods. keywords online feature selection streaming feature group structure classiﬁcation face veriﬁcation. high dimensional data pose challenges data mining pattern recognition usually feature selection utilized order reduce dimensionality eliminating irrelevant redundant features contexts feature selection models oriented ofﬂine situation. global feature space obtained advance however real-world applications features actually generated dynamically. example image analysis multiple descriptors exacted capture various visual information images histogram oriented gradients color histogram scale-invariant feature transform shown figure time-consuming wait calculation features. thus necessary perform feature selection arrival referred online feature selection. main advantage online feature selection time efﬁciency suitable online applications therefore emerged important topic. online feature selection assumes features model dynamically. feature selection performed arrival features. different classical online learning feature space remains consistent samples sequentially papers focus direction perkins proposed gradient descent model grafting selects features minimizing predeﬁned binomial negative log-likelihood loss function. zhou introduced streamwise regression model evaluates dynamic feature performed online selection relevance analysis approaches evaluate features dynamically arrival feature suffer common limitation overlook relationship features important real-world applications image processing kind cues image describes certain information consists high bioinformatics microarray data consist groups gene sets dimensional feature spaces. terms biological meanings. group information considered type prior knowledge connection features difﬁcult discovered merely data labels. therefore performing selection feature groups perform better perform selection features individually. hence works focus feature selection group structure information group lasso sparse group lasso however methods performed batch manner. although yang proposed online group lasso method designed instance stream. global feature space data sets still desired advance feature selection. therefore ﬁrst formulate problem online group feature selection. challenges problem features generated dynamically; group structure. best knowledge none existing feature selection methods well handle issues. therefore paper propose novel feature selection method problem namely online group feature selection speciﬁcally time step group feature generated. develop novel criterion based spectral analysis aims select discriminative features process called online intra-group selection. feature evaluated individually stage. intra-group selection ﬁnished reevaluate selected features remove redundancy. process accomplished sparse linear regression model lasso. refer stage online inter-group selection. major contributions summarized follows best knowledge ﬁrst effort considers group structure online fashion. although online feature selection methods proposed utilize group structure information feature stream. based observation spectral analysis widely used discriminative variable analysis propose novel criterion based spectral analysis. criterion proven efﬁcient online intra-group feature selection. beneﬁt correlation among features groups sparse regression model lasso online inter-group feature selection. ﬁrst time sparse model lasso employed dynamic feature selection. demonstrate superiority method state-of-the-art online feature selection methods. experimental results real-world applications show effectiveness method tasks large scale data image classiﬁcation face analysis. online group feature selection ﬁrst introduced previous work comparison preliminary version improvements following aspects performed comprehensive survey existing related works; solve regression sparse model inter-group selection adopted efﬁcient solution; conducted empirical evaluations; discussions analysis provided. rest paper organized follows. review related work section introduce framework give algorithm section report empirical study real-world benchmark data sets section section concludes paper discusses possible future work. section ﬁrst give brief review traditional ofﬂine feature selection including ﬁlter wrapper embedded models. speciﬁcally review existing literature focus utilizing underlying group structure feature space group lasso extensions. then introduce state-of-the-art online feature selection methods. traditional feature selection oriented off-line situation. problem statement deﬁned below. given data rn×d consisting samples d-dimensional feature space pre-process features centered around zero unit norm ||fi|| object feature selection choose subset features global feature space desired number features general generally feature selection methods fall three classes based label information used. existing methods supervised evaluate correlation among features label variable. difﬁculty obtaining labeled data unsupervised feature selection attracted increasing attention recent years unsupervised feature selection methods usually select features preserve data similarity manifold structure semi-supervised feature selection called smalllabeled sample problem makes label information manifold structure corresponding labeled data unlabeled data existing feature selection methods categorized embedded ﬁlter wrapper approaches based methodologies ﬁlter methods evaluate features certain criterion select features ranking evaluation values. correlation criteria proposed feature selection include mutual information maximum margin kernel alignment hilbert schmidt independence criterion development ﬁltering methods involves taking consideration multiple criteria overcome redundancy. representative algorithm mrmr principle max-dependency max-relevance min-redundancy. aims subset features large dependency target class redundancy among other. wrapper methods employ speciﬁc classiﬁer evaluate subset directly. example weston used wrapper purpose optimizing accuracy subset features. wrapper methods usually better performance ﬁlter methods. however typically computationally expensive time complexity exponential respect number features. meanwhile performance selected subset relies speciﬁc training classiﬁer. embedded methods usually seek subset jointly minimizing empirical error penalty. tend efﬁcient wrapper model relatively small size ultimate subset. lars successful example falls category objective function minimize reconstruction error sparsity constraint coefﬁcients features. sparsity constraint take group lasso example. considers correlation structure feature space. underlying structure feature space important feature selection. take application bioinformatics example certain factors contribute predicting cancer consist group variables. then problem amounts selection groups variables. group lasso extended works mainly solve following optimization problem smooth convex loss function least squares loss. feature space partitioned groups parameter corresponding group regularization parameters modulate sparsity selected features groups respectively. parameters different values model falls different models seen table yang proposed online algorithm group lasso. weight vector updated arrival sample. important features corresponding large values selected group manner. thus algorithm suitable sequential samples especially applications large scale data. aforementioned feature selection methods ofﬂine designed classical online scenario instances arrive dynamically instead features. works focus aspect. brief review summarized next subsection. online feature selection assumes features arrive streams. different classical online learning lets samples dynamically. thus time step feature descriptor samples available. goal online feature selection justify whether feature accepted arrival. related works proposed including grafting alpha-investing osfs grafting integrates feature selection learning predictor within regularized framework. grafting oriented binomial classiﬁcation objective function binomial negative log-likelihood loss function deﬁned number samples number selected features predictor constrained regularization. note feature included penalized. guarantee decrease objective function reduction mean loss outweigh regularizer penalty λwj. therefore justify whether inclusion feature improve existing model grafting uses gradient-based heuristic. feature selected following condition satisﬁed regularization coefﬁcient. otherwise weight dropped feature rejected. time feature selected model goes back reapplies gradient test features selected far. framework adaptive linear non-linear models. grafting successfully employed applications edge detection limitations below. first though grafting obtain global optimum respect features included model optimal features dropped online selection. besides gradient retesting selected features greatly increases total time cost. last tuning good value important regularization parameter requires information global feature space. alpha-investing belongs penalized likelihood ratio methods require global model. speciﬁcally feature arriving time step alpha-investing evaluates p-statistic leads p-value. p-value probability feature could accepted actually discarded. comparing p-value threshold feature added model p-value greater threshold corresponds probability including spurious feature time step time feature added wealth increase shown represents current acceptable number future false positives. parameter controlling false discovery rate time step summary alpha-investing adaptively adjusts threshold feature selection. also handle inﬁnite feature stream. however alpha-investing reevaluate included features greatly inﬂuence following selection. osfs features characterized strongly relevant weakly relevant irrelevant label attribute. incoming feature time step osfs ﬁrst analyzes correlation label feature weakly strongly relevant label selected. feature added osfs performs redundancy analysis. condition selecting feature previously selected features become irrelevant also removed. speciﬁcally feature redundant class feature weakly relevant denote markov blanket subset containing weakly strongly relevant non-redundant features. thus redundancy analysis component optimal feature selection process. osfs need parameter tuning shows outstanding performance many applications impact crater detection. methods state-of-the-art online feature selection methods. although existing methods greatly relieve burden processing high dimensional data sets consider correlation among features. hence address online group feature selection problem work. make prior knowledge group information propose efﬁcient online feature selection framework including intra-group feature selection inter-group feature selection. based framework develop novel algorithm called online group feature selection online group feature selection ﬁrst formalize problem online group feature selection. assume data matrix rd×n number features arrived number data points class label vector {··· number classes. feature space number features group individual feature. terms feature stream class label vector select optimal feature subset space feature dimension solve problem propose framework online group feature selection consists components intra-group selection inter-group selection. intra-group selection process feature dynamically arrival. group features generated process feature individually select subset terms features obtained intra-group selection consider correlation among groups optimal subset namely inter-group selection. overview procedure illustrated figure based framework propose novel online group feature selection following subsections give details algorithm. spectral based feature selection methods demonstrated effectiveness given data matrix rd×n construct weighted undirected graphs given data. graph reﬂects within-class local afﬁnity relationship reﬂects between-class global afﬁnity relationship. graphs characterized weight matrices respectively. weight matrices constructed represent relationships among instances kernel denotes number data points class given adjacency matrix introduce deﬁnitions degree matrix laplacian matrix frequently used spectral graph theory. deﬁnition given adjacency matrix graph degree matrix deﬁned diag otherwise. similarly given adjacency matrix graph degree matrix deﬁned diag otherwise. identity vector. applying spectral graph theory feature selection ﬁnding smooth feature selector matrix consistent graph structure. rd×m denote feature selector matrix number features selected dimension global feature space. entry equal procedure feature selection data matrix transformed rm×n feature space projection feature space indicated smooth selection matrix instances class close time instances different classes distant reﬂects within-class local afﬁnity relationship. speciﬁcally data belong class close other relatively larger value. otherwise relatively ||zi zj||sw small possible. similarly reﬂects between-class global afﬁnity relationship. instances belong different classes relatively larger value. therefore select feature ||zi zj||sb large possible. best selection matrix obtaining feature scores feature-level approach select leading features corresponding ranking scores. traditional spectral feature selection approaches rely global information efﬁcient online fashion. hence beneﬁt spectral analysis evaluate arrival feature criterion deﬁned streaming feature scenario denotes online feature selector matrix denotes arrived features denotes selected features. given selected feature space arrival feature selected inclusion improves discriminative ability feature space small positive parameter. however performance easily inﬂuenced sequence arriving features. speciﬁcally previous arrived features high level discriminative capacity difﬁcult following features satisfy thus allow discriminative ability feature disturb within range then criterion based spectral analysis streaming feature scenario deﬁned follows. deﬁnition given previously selected subset newly arrived feature assume inclusion good feature between-class distances larger within-class distance smaller. feature selected following criterion satisﬁed intra-group selection obtain subset original feature space however criterion include discriminative features also cause redundancy. meanwhile intra-group selection evaluates streaming features individually consider group information. thus apply inter-group selection. inter-group selection based classical sparse model lasso could reduce redundancy among selected features efﬁciently. section introduce online inter-group selection aims obtain optimal subset based global group information. propose solve problem linear regression model lasso. given subset selected ﬁrst phase previously selected subset features combined feature space dimension data matrix rm×n class label vector projection vector constructs predictive variable stands norm stands norm vector parameter controls amount regularization applied estimators general smaller lead sparser model. solve problem deﬁned reformulate function solved efﬁciently many optimization methods feature-sign search optimization methods value usually determined cross validation. sparse regression model selects features setting several component zero corresponding feature deemed irrelevant class label discarded. finally features corresponding non-zero coefﬁcients selected. algorithm shows pseudo-code online group feature selection algorithm. ogfs divided parts intra-group selection inter-group selection details follows. intra-group selection feature group evaluate features criterion deﬁned section steps evaluate signiﬁcance features based criterion inclusion feature within-class distance minimized between-class distance maximized feature considered good feature added inclusion feature causes discriminative ability feature space disturb arrange helpful also selected.after intra-group selection subset features implement global information groups build sparse regression model based selected subset newly selected subset algorithm selected features re-evaluated intra-group selection iteration. time complexity intra-group selection time complexity inter-group selection therefore time complexity ogfs linear respect number features number groups. iterations continue performance reaches predeﬁned threshold below. number features need select; accu prediction accuracy model based reaches predeﬁned accuracy section empirically show superiority method. experimental settings present comparative methods evaluation metrics simulation online situation. encouraging results real-world applications image classiﬁcation face veriﬁcation reported. verify inﬂuence group orders ogfs method. also conduct experiments benchmark data sets verify effectiveness method. conduct comparative experiments online ofﬂine feature selection methods. state-ofthe-art online feature selection methods include alpha-investing osfs grafting. choose three representative ofﬂine feature selection ﬁlter embedded wrapper models speciﬁcally lars gbfs employed evaluation metrics accuracy compactness. compactness number selected features. accuracy denotes classiﬁcation veriﬁcation accuracy based selected feature space. also report results based global feature space baseline. according authors maximum number selected features parameters alpha-investing according tune parameters grafting cross validation. inter-group selection method implemented efﬁcient sparse coding method parameter simulate online group feature selection allow features groups. features group processed individually. data sets natural feature groups pre-existing group structure used. data sets natural feature groups divide feature space randomly. speciﬁcally denotes global feature stream split several groups randomly. feature group f∗k+··· fi∗k] dimension case less dimension less half global dimension. otherwise chosen cifar- caltech- image classiﬁcation. ﬁrst introduce data sets experiments present experimental results. cifar- dataset consists images classes images class. randomly select images class training rest used testing. caltech- dataset contains images categories including animals vehicles ﬂowers etc. images category. take images class training take images class testing. caltech- extract sift feature three-layer pyramid. then image represented normalized -dimensional sparse-coding feature vector. thus feature stream consists denotes sift descriptor whole image denotes sift descriptor local region image. cifar- dataset contains tiny images size extract sift feature two-layer pyramid. feature stream consists adopt linear test classiﬁcation performance selected feature space. involved parameter model tuned -fold cross-validation. details experimental results follows. ﬁrst explore individual performance process ogfs denoted ogfs-intra ogfsinter respectively. table reports compactness accuracy time cost algorithm dataset. considering classiﬁcation accuracy ogfs-intra obtains best overall accuracy shown table grafting performs ogfs-intra ogfs-inter ogfs reach comparative accuracy respectively. alpha-investing inferior ogfs-inter ogfs still performs better osfs. possibly constraint maximal number selected features osfs. demonstrated ogfs-intra select discriminative features leads redundancy. ogfs-inter reduce redundancy. thus ogfs achieves better accuracy ogfs-inter little inferior ogfs-intra. three ofﬂine feature selection methods obtain comparative accuracy around accuracy baseline best observe accuracy method baseline least. terms compactness shown table osfs selects features. ogfs-intra selects largest number features similar grafting ogfs-inter uses sparse model leads relatively small size feature space. gbfs obtains least number features among ofﬂine feature selection methods ogfs comparative features. guarantee classiﬁcation performance selects size features lars terms time complexity ogfs-intra obtains highest efﬁciency seconds others require hundreds thousands seconds. ogfs-intra linear number features discuss section inter-group selection needs less seconds much faster alpha-investing grafting osfs. time cost osfs exponential number desired features. order simulate online situation online feature selection methods tend spend time feature transformation. time complexity ﬁlter method seconds much faster ofﬂine methods lars gbfs however ogfs-intra even efﬁcient seconds. beneﬁts criterion deﬁned intra-group selection. since studied online feature stream groups examine performance online feature selection methods response increasing groups figure generally arrival groups compactness increases classiﬁcation accuracy improves. improvement obvious alpha-investing osfs. grafting method obtains best accuracy. method obtains better compactness. actually number groups increases compactness method remains stable. complementary effects stages ogfs. ogfs-intra selects discriminative features ogfs-inter achieves optimal subset. beneﬁt group information ogfs favors good trade-off accuracy compactness. time complexity show combination stages reasonable applicable real-world applications. thus following experiments compare ogfs algorithm comparative algorithms. report average accuracy classes. detailed results shown table seen ogfs gives leading classiﬁcation accuracy cases. speciﬁcally ogfs gains alphainvesting. performance grafting improves increase training samples still inferior method. accuracy ogfs higher grafting. example case training images grafting reaches accuracy method case training images accuracy methods ogfs reaches terms compactness alpha-investing achieves best performance. case training images class alpha-investing obtains compactness features much better comparative methods grafting ogfs however alpha-investing achieves accuracy much lower grafting ogfs implies reevaluation features necessary. also conﬁrms correlation among features important. terms time complexity time complexity methods increases increase training samples. alpha-investing efﬁcient cases. however case training images ogfs seconds faster. osfs grafting achieve similar computational efﬁciency varies seconds. thus summary ogfs could obtain discriminative feature space within acceptable time cost. table reports ofﬂine feature selection methods baseline. baseline obtains best accuracy huge feature space. case less training samples ofﬂine feature selection methods obtain comparative accuracy less variation. increase training samples enjoys great improvement accuracy. case training samples obtains best accuracy better lars gbfs ogfs comparative results demonstrate ogfs superior ofﬂine feature selection methods real-world image classiﬁcation task. investigate inﬂuence increasing feature groups. classiﬁcation results based group plot figure also observe increase feature groups ogfs enjoys improvement accuracy. instance shown figure ogfs obtains much better accuracy grafting groups. feature group reaches performance methods keep steady. compactness method changes increase groups others remain stable. demonstrates efﬁcacy online group feature selection. features extraction expensive time consuming. model based existing feature space reaches predeﬁned performance feature extraction necessary. dataset collected unconstrained face recognition contains images identities. dataset identities images identities single image. images captured daily conditions variations pose expression lighting figure lists samples dataset. extract image patches landmarks scales. patch size ﬁxed scales. patch divided non-overlapping cells. cell extract -dimensional descriptor. image represented feature vector dimension ××××. feature space landmark group. then feature stream consists denotes descriptor landmark. dataset divided folds. test performance selected feature space leave-one-out cross validation scheme. experiment nine folds combined form training tenth subset used testing. verify whether pair belongs subject euclidean distance. table lists details compactness veriﬁcation accuracy selected feature spaces fold. shown table ogfs higher alpha-investing cases. alpha-investing selects features. indices selected features among previously selected features never reevaluated. conﬁrms importance reevaluating collected features. general osfs achieves accuracy features much higher alpha-investing still inferior ogfs ogfs also outperforms grafting terms time complexity alpha-investing still obtains highest efﬁciency seconds average. time complexity alpha-investing linear. osfs inferior alpha-investing seconds. grafting slowest seconds much slower method seconds. time complexity osfs grafting ogfs related selected number features alpha-investing correlated procession dimension feature. time complexity method acceptable. table variance splits data small. therefore fold data test ofﬂine feature selection methods. complementary results shown table table ogfs obtains best accuracy even better baseline demonstrates necessary feature selection face veriﬁcation. lars reach similar accuracy grafting also obtains better accuracy ofﬂine methods. encouraging results show superiority performance online feature selection methods. figure illustrates performance online feature selection methods response increasing groups. alpha-investing remains stable terms accuracy compactness. number groups increases osfs grafting gain stable compactness. sometimes accuracy also decreases. implies features include redundant irrelevant information. results demonstrate framework online feature selection suitable large-scale real-world application. part show performance method regarding order feature groups table experiment conducted cifar- dataset. original feature space randomly generated order groups features times shown second column table algorithm obtains average accuracy standard variation accuracy order feature groups inﬂuence towards method variation within certain range. thus demonstrates method stable real-world applications. table lists eight benchmark data sets repository microarray domains note that eight data sets natural group information group structure generated randomly dividing feature space. experiment help test robustness ogfs approach. feature selection test performance feature space based three classiﬁers randomforest spider toolbox. adopt -fold cross-validation three classiﬁers choose average accuracy ﬁnal result. table shows experimental results classiﬁcation accuracy versus compactness data sets. though grafting uses information global feature space algorithm outperforms grafting data sets terms accuracy. data sets method obtains gain accuracy. speciﬁcally dataset colon accuracy grafting ogfs achieves datasets leukemia lungcancer algorithm achieves fairly high accuracy data sets wdbc ionosphere ogfs also obtains comparative accuracy lower. ionosphere dataset ogfs achieves better compactness. results show ogfs able obtain features discriminative capability. alpha-investing obtains better compactness ogfs algorithm data sets performs worse terms accuracy. method outperforms alpha-investing data sets. speciﬁcally dataset colon accuracy alpha-investing ogfs reaches wdbc ionosphere data sets methods achieve comparable accurac. instance ionosphere dataset algorithm achieves accuracy alphainvesting achieves accuracy previously selected subset never reevaluated alpha-investing affects selection later arrived features. however algorithm selected features reevaluated inter-group selection iteration. thus algorithm able select sufﬁcient features discriminative power. osfs obtains better compactness data sets algorithm better osfs accuracy data sets small compactness loss. speciﬁcally ionosphere spambase data sets accuracy algorithm slightly lower osfs data sets algorithm signiﬁcantly outperforms osfs. example dataset colon algorithm achieves accuracy osfs reaches prostate dataset method performs much better osfs reason osfs evaluates features individually rather groups. meanwhile different osfs algorithm facilitates relationship features within groups correlation groups lead better feature subset. terms time complexity alpha-investing fastest except seconds slower algorithm dataset spambase. ﬁrst data sets grafting costs seconds wdbc dataset algorithms accomplish feature selection less second. feature space reaches thousands ogfs alpha-investing grafting methods take less seconds. osfs takes seconds colon dataset. time relevant feature added redundancy analysis triggered selected features. lungcancer dataset alpha-investing takes less seconds. osfs inferior alpha-investing seconds. ogfs costs minute still faster grafting seconds. demonstrates simple consideration dimension coming feature efﬁcient like alpha-investing. time time complexity algorithms correlated global feature space also selected features previous stage. although reevaluation selected features costs time robust achieve relatively better classiﬁcation performance. ogfs ofﬂine feature selection methods table reports results ofﬂine feature selection methods baseline. lars obtains best accuracy. instance leukemia dataset lars reaches accuracy better gbfs. cases gbfs comparative lars. ofﬂine methods obtain compactness less features. observe ogfs comparative best result ofﬂine feature selection methods. results demonstrate efﬁcacy ogfs general feature selection applications. summary term classiﬁcation accuracy experimental results data sets show algorithm superior comparative online feature selection methods. ogfs achieves comparative results best ofﬂine performance. implies method enjoys signiﬁcant improvement compared state-of-the-art online feature selection models. paper investigate online group feature selection problem present novel algorithm namely ogfs. comparison traditional online feature selection proposed approach considers situation features arrive groups real-world applications. divide online group feature selection stages i.e. online intra-group inter-group selection. then design novel criterion based spectral analysis intra-group selection introduce sparse regression model reduce redundancy inter-group selection. extensive experimental results image classiﬁcation face veriﬁcation demonstrate method suitable real-world applications. also validate efﬁcacy method several microarray benchmark data sets.", "year": 2016}