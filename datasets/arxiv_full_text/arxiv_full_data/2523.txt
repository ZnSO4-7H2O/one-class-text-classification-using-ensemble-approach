{"title": "Variational Information Maximisation for Intrinsically Motivated  Reinforcement Learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm --- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.", "text": "mutual information core statistical quantity applications areas machine learning whether training density models multiple data modalities maximising efﬁciency noisy transmission channels learning behaviour policies exploration artiﬁcial agents. learning algorithms involve optimisation mutual information rely blahut-arimoto algorithm enumerative algorithm exponential complexity suitable modern machine learning applications. paper provides approach scalable optimisation mutual information merging techniques variational inference deep learning. develop approach focusing problem intrinsically-motivated learning mutual information forms deﬁnition well-known internal drive known empowerment. using variational lower bound mutual information combined convolutional networks handling visual input streams develop stochastic optimisation algorithm allows scalable information maximisation empowerment-based reasoning directly pixels actions. problem measuring harnessing dependence random variables inescapable statistical problem forms basis large number applications machine learning including rate distortion theory information bottleneck methods population coding curiositydriven exploration model selection intrinsically-motivated reinforcement learning problems core quantity must reasoned mutual information. general mutual information intractable compute existing algorithms useful realistic applications. received algorithm estimating mutual information blahut-arimoto algorithm effectively solves enumeration approach exponential complexity suitable modern machine learning applications. combining best current practice variational inference deep learning bring generality scalability seen problem domains information maximisation problems. provide approach maximisation mutual information signiﬁcantly lower complexity allows computation high-dimensional sensory inputs allows exploit modern computational resources. technique derive generally applicable shall describe develop approach focussing popular increasingly topical application mutual information measure ‘empowerment’ intrinsically-motivated reinforcement learning. reinforcement learning seen number successes recent years established practical scalable solution realistic agent-based planning decision making limitation standard approach agent able learn using external rewards obtained environment; truly autonomous agents often exist environments lack external rewards environments rewards sparsely distributed. intrinsically-motivated reinforcement learning attempts address shortcoming equipping agent number internal drives intrinsic reward signals hunger boredom curiosity allows agent continue explore learn meaningfully reward-sparse world. many ways formally deﬁne internal drives deﬁnitions common they unsupervised fashion allow agent reason value information action-observation sequences experiences. mutual information allows exactly type reasoning forms basis popular intrinsic reward measure known empowerment. paper begins describing framework online self-motivated learning describes general problem associated mutual information estimation empowerment make following contributions develop stochastic variational information maximisation algorithm scalable estimation mutual information channel capacity applicable discrete continuous settings. combine variational information optimisation tools deep learning develop scalable algorithm intrinsically-motivated reinforcement learning demonstrating application variational theory problems reinforcement learning decision making. demonstrate empowerment-based behaviours obtained using variational information maximisation match using exact computation. apply algorithms broad range high-dimensional problems possible compute exact solution able according empowerment learning directly pixel information. intrinsicallyself-motivated learning attempts address question rewards come used autonomous agent. consider online learning system must model reason incoming data streams interact environment. perception-action loop common many areas active learning process control black-box optimisation reinforcement learning. extended view framework presented singh describe environment factored external internal components agent receives observations takes actions external environment. importantly source nature reward signals assumed provided oracle external environment moved internal environment part agent’s decisionmaking system; internal environment handles efﬁcient processing input data choice computation appropriate internal reward signal. important components framework state representation critic. principally interested vision-based self-motivated systems solutions currently developed. achieve this state representation system convolutional neural network critic ﬁgure responsible providing intrinsic rewards allow agent different types internal motivations information maximisation enters intrinsically-motivated learning problem. nature critic particular reward signal provides main focus paper. wide variety reward functions proposed include missing information bayesian surprise uses divergence measure change agents internal belief observation data measures based prediction errors future states predicted change predicted mode change probability gain salient event prediction measures based information-theoretic quantities predicted information gain causal entropic forces empowerment paper oudeyer kaplan currently provides widest singular discussion breadth intrinsic motivation measures. although wide choice intrinsic reward measures none available informationtheoretic approaches efﬁcient compute scalable high-dimensional problems require either knowledge true transition probability summation conﬁgurations state space tractable complex environments states large images. mutual information empowerment mutual information core information-theoretic quantity acts general measure dependence random variables deﬁned joint distribution random variables corresponding marginal distributions. many quantities interest computational neuroscience sensory inputs spiking population code; telecommunications input signal channel received transmission; learning exploration policies current state action time future respectively. intrinsic motivation internal reward measure referred empowerment obtained searching maximal mutual information conditioned starting state sequence actions ﬁnal state reached sequence primitive actions leading ﬁnal state k-step transition probability environment. p|s) joint distribution action sequences ﬁnal state distribution k-step action sequences p|s) joint probability marginalised action sequence. equation deﬁnition channel capacity information theory measure amount information contained action sequences future state measure compelling since provides well-grounded task-independent measure intrinsic motivation naturally within framework intrinsically motivated learning described ﬁgure furthermore empowerment like stateaction-value function reinforcement learning assigns value state environment. agent seeks maximise value move towards states reach largest number future states within planning horizon intuition authors describe empowerment measure agent ‘preparedness’ means agent quantify extent reliably inﬂuence environment motivating agent move states maximum inﬂuence empowerment-based agent generates open-loop sequence actions steps future used agent internal planning using optimised using distribution becomes efﬁcient exploration policy allows uniform exploration state space reachable horizon another compelling aspect empowerment policy used agent acting agent must world follows closed-loop policy obtained planning algorithm using empowerment value expand sect. consequence acting agent ‘curious’ parts environment reached within internal planning horizon shall explore effect horizon work widely-explored defer insights salge scalable information maximisation mutual information described thus whether problems empowerment channel capacity rate distortion hides difﬁcult statistical problems. firstly computing involves expectations unknown state transition probability. seen rewriting terms difference conditional entropies p|as)ω s)]. computation requires marginalisation k-step transition dynamics environment unknown general. could estimate distribution building generative model environment model compute since learning accurate generative models remains challenging task solution avoids preferred secondly currently lack efﬁcient algorithm computation. exists scalable algorithm computing mutual information allows apply empowerment highdimensional problems allow easily exploit modern computing systems. current solution blahut-arimoto algorithm essentially enumerates states thus limited small-scale problems applicable continuous domain. scalable non-parametric estimators developed high memory footprint require large number observations approximation bound making reasoning correctness harder cannot easily composed existing systems allow design uniﬁed system. continuous domain monte carlo integration proposed applications monte carlo estimators require large number draws obtain accurate solutions manageable variance. also explored monte carlo estimators empowerment describe alternative importance sampling-based estimator channel capacity appendix variational information lower bound made tractable deriving lower bound maximising instead present bound derived barber agakov using entropy formulation reveals bounding conditional entropy component sufﬁcient bound entire mutual information. using non-negativity property divergence obtain bound introduced variational distribution parameters distribution parameters bound becomes exact equal true action posterior distribution lower bounds mutual information also possible jaakkola jordan present lower bound using convexity bound logarithm; brunel nadal gaussian assumption appeal cramer-rao lower bound. bound highly convenient since transition probability appears linearly expectation never need evaluate probability thus evaluate expectation directly monte carlo using data obtained interaction environment. bound also intuitive since operate using marginal distribution action sequences acts source transition distribution acts encoder variational distribution conveniently acts decoder taking variational information maximisation straightforward optimisation procedure based alternating optimisation parameters distributions barber agakov made connection approach generalised algorithm refer algorithm follow optimisation principle. optimisation perspective maximisation bound w.r.t. ill-posed avoid divergent solutions adding constraint value entropy results constrained optimisation problem action sequence performed agent moving inverse temperature times general source decoder distributions formed complex non-linear functions using deep networks stochastic gradient ascent optimisation. refer approach stochastic variational information maximisation highlight computation mini-batch recent experience agent. optimisation decoder becomes maximum likelihood problem optimisation source requires computation unnormalised energy-based model describe next. summmarise overall procedure algorithm maximum likelihood decoder ﬁrst step alternating optimisation optimisation equation w.r.t. decoder supervised maximum likelihood problem. given data past interactions environment learn distribution start termination states respectively action sequences taken. parameterise decoder auto-regressive distribution k-step action sequence free choose distributions action sequence choose categorical distributions whose mean parameters result function parameters non-linear function specify using two-layer neural network rectiﬁed-linear activation functions. maximising log-likelihood able make stochastic updates variational parameters distribution. neural network models used expanded upon appendix estimating source distribution given current estimate distribution computed solving functional derivative ω/δω constraint normalisation term. substituting optimal distribution original objective expressed terms normalisation function only distribution implicitly deﬁned unnormalised distribution direct mechanisms sampling actions computing normalising function distributions. could gibbs importance sampling solutions satisfactory would require several evaluations unknown function decision state. obtain convenient problem approximating unnormalised distribution normalised distribution equivalent approximating energy term function log-likelihood directed model introduced scalar function approximation since dependent action sequence change approximation veriﬁed substituting since normalised distribution leaves account normalisation term veriﬁed substituting therefore obtain cheap estimator empowerment optimise parameters directed model scalar function minimise measure discrepancy sides approximation minimise squared error giving loss function optimisation convergence optimisation obtain compact function compute empowerment requires forward evaluation function parameterised using auto-regressive distribution similar conditional distributions speciﬁed deep networks. scalar function also parameterised using deep network. details networks provided appendix empowerment-based behaviour policies using empowerment intrinsic reward measure agent seek states maximal empowerment. treat empowerment value state-dependent reward utilise standard planning algorithm e.g. q-learning policy gradients monte carlo search. simplest planning strategy using one-step greedy empowerment maximisation. p|sa) policy account effect actions beyond planning horizon natural enhancement value iteration allow agent take actions maximising long term {compute state repr.} {draw action sequence.} obtain data {acting env. convnetλ) {compute state repr.} {empowerment} discounted) empowerment. third approach would empowerment potential function difference current previous state’s empowerment shaping function planning fourth approach agent uses source distribution behaviour policy. source distribution similar properties greedy behaviour policy also used since effectively acts empowered agents internal exploration mechanism large variance understanding choice behaviour policy important line ongoing research. algorithm summary complexity system described scalable general purpose algorithm mutual information maximisation summarise core components using computational graph ﬁgure algorithm state representation mechanism used throughout obtained transforming observations produce start ﬁnal states respectively. observations pixels vision state representation convolutional neural network observations fully-connected neural network indicate parameters models using since uniﬁed loss function apply gradient descent backpropagate stochastic gradients entire model allowing joint optimisation information representation parameters. optimisation preconditioned optimisation algorithm adagrad computational complexity empowerment estimators involves planning horizon number actions number states exact computation must enumerate number states grid-worlds binary images complexity using blahut-arimoto algorithm grid worlds binary images. algorithm even environments small number interacting objects becomes quickly intractable since state space grows exponentially number possible interactions also exponential planning horizon. contrast approach deals directly image dimensions. using visual inputs convolutional network produces vector size upon subsequent computation based consisting llayer neural network. gives complexity state representation autoregressive distributions complexity size hidden layer. thus approach quadratic complexity size hidden layers used linear quantities matches complexity currently employed large-scale vision-based models. addition since gradient descent throughout able leverage power gpus distributed gradient computations. demonstrate empowerment effectiveness variational information maximisation types environments. static environments consists rooms mazes different conﬁgurations objects agent interact moving objects. number states settings equal number locations environment still manageable approaches rely state enumeration. dynamic environments aspects environment change ﬂowing lava causes agent reset predator chases agent. part consider discrete action settings agent actions agent actions picking laying brick. external rewards available agent must reason purely using visual information. experiments used horizon effectiveness bound ﬁrst establish variational information lower bound results behaviour obtained using exact mutual information static environments. consider environments discrete states compute true mutual information using blahut-arimoto algorithm. compute variational information bound environment using pixel information compare approaches look empowerment landscape obtained computing empowerment every location environment show heatmaps. action selection matters location maximum empowerment comparing heatmaps ﬁgure empowerment landscape matches exact variational solution hence lead agent-behaviour. image ﬁgure show heat-map empowerment location environment. analyze point highest empowerment large room centre room; cross-shaped room centre cross two-rooms environment located near doors. addition show empowerment values obtained method constitute close approximation true empowerment two-rooms environment results match authors klyubin wissner-gross freer advantage variational approach clear discussion able obtain solutions quality exact computation favourable computational scaling able plan directly pixel information. dynamic environments established usefulness bound understanding empowerment examine empowerment behaviour environments dynamic characteristics. even small environments number states becomes extremely large objects moved added removed environment making enumerative algorithms quickly infeasible since exponential explosion number states. ﬁrst reproduce experiment salge considers empowered behaviour agent room-environment room that empty ﬁxed moveable moveable boxes. salge explore setup discuss choice state representation including existence severely limits planning ability agent. approach face problem choosing state representation since agent reason objects appear within visual observations obviating need hand-designed state representations. figure shows empty room empowerment uniform almost everywhere except close walls; room ﬁxed ﬁxed limits future reachable states expected empowerment around box; room moved seen tool high empowerment near box; similarly four boxes empowerment highest around figure predator agent scenario. panels show simulation. panels show trace path predator prey take points trajectory. blue/red shows path history; cyan shows direction maximum empowerment. boxes. results match salge show effectiveness reasoning pixel information directly. figure shows planning empowerment works dynamic maze environment lava ﬂows source bottom eventually engulfs maze. agent able safeguard itself stem lava building wall entrance corridors. every point time agent decides next action computing expected empowerment taking action. environment show planning available actions graph empowerment values resulting state. action leads highest empowerment taken indicated black panels. figure shows two-rooms separated door. agent able collect allows open door. collecting maximum empowerment region around agent collected region maximum empowerment close door. figure shows agent corridor must protect building wall bricks able successfully using empowerment planning approach described maze setting. predator-prey scenario demonstrate applicability approach continuous settings studying simple physics simulation shown ﬁgure here agent followed predator randomly reset location environment caught predator. agent predator represented spheres environment roll surface friction. state position velocity angular momentum agent predator action force vector. expected maximum empowerment lies regions away predator results agent learning escape predator. conclusion developed approach scalable estimation mutual information exploiting recent advances deep learning variational inference. focussed speciﬁcally intrinsic motivation reward measure known empowerment requires core efﬁcient computation mutual information. using variational lower bound mutual information developed scalable model efﬁcient algorithm expands applicability empowerment high-dimensional problems complexity approach extremely favourable compared complexity blahut-arimoto algorithm currently standard. overall system require generative model environment built learns using interactions environment allows agent learn directly visual information continuous state-action spaces. chose develop algorithm terms intrinsic motivation mutual information wide applications domains stand beneﬁt scalable algorithm allows exploit abundance data applied large-scale problems. acknowledgements thank daniel polani invaluable guidance feedback. obtain intuitive understanding empowerment examining analytical properties equation simplicity focus deterministic discrete environments. setting transition probability delta distribution transition function starts state executes action sequence provides resulting state. solving equation optimal source distribution using blahut-arimoto yields ﬁxed-point iteration normalising constant. starting recursion uniform distribution solution recursion next iteration number alternative action-paths terminate state action-path starting state also relate quantity intuitive number number different action-paths brings agent state number different states reached state horizon demonstrate reasoning using tree-structured environment ﬁgure agent selecting actions uniformly general visit terminal states uniformly unless every action-path terminates different state. however agent selects actions according distribution visit terminal states uniformly. seen computing therefore distribution seen efﬁcient exploration policy allows agent explore states uniformly. adds similar analysis presented salge approach described main text ’model-free’ sense model transition dynamics environment. building accurate transition models hard much success achieved model-free methods. ideally would like model-based method since allow reasoning task-independent aspects world allow transfer learning across domains potentially faster learning. describe model-based approaches developed empowerment. methods efﬁcient reasons describe below hence part main text. most-generic model-based empowerment method approximate empowerment using generic importance-sampling estimator. assume model environment p|ai available point specify model obtained. generate samples source distribution importance weights samples kept constant optimization importance weights adapted maximize additionally action-sequence sample generate futurestate samples transition model p|ai could approximate quantities required compute samples shall instead directly approximate blahut-arimoto iteration. compute distortion t-th iteration using p|ai s)αti normalized weights αt+i best approximates blahut-arimoto update. yields simple update rule importance weights normalizing constant. algorithmic complexity update cost computing distortion scales approach applicable continuous domain typically requires large number samples accurate estimation empowerment. case smooth transition models p|ai policy efﬁcient algorithm derived using stochastic backpropagation rewrite model policy differentiable functions. using representation rewrite variational objective function expectation show blahut-arimoto algorithm derived variational bound variational distribution maximises bound posterior distribution actions given present future states observations images make convolutional network obtain state representation. convolutional network experiments. convolution apply rectiﬁed non-linearity. experiments make ﬁlters layer convolution. ﬁrst convolution consists kernels stride second convolution consists kernels stride output convolution passed fully connected layer hidden units followed rectiﬁed non-linearity. -dimensional representation forms state representation used variational information maximisation components follow processing stage. two-layer neural network forms shared component distribution. element-wise non-linearity rectiﬁed non-linearity case rect max. scalar function also speciﬁed two-layer neural network. rezende danilo jimenez mohamed shakir wierstra daan. stochastic back-propagation variational inference deep latent gaussian models. international conference machine learning", "year": 2015}