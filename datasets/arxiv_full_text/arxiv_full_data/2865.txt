{"title": "Learning the Joint Representation of Heterogeneous Temporal Events for  Clinical Endpoint Prediction", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "The availability of a large amount of electronic health records (EHR) provides huge opportunities to improve health care service by mining these data. One important application is clinical endpoint prediction, which aims to predict whether a disease, a symptom or an abnormal lab test will happen in the future according to patients' history records. This paper develops deep learning techniques for clinical endpoint prediction, which are effective in many practical applications. However, the problem is very challenging since patients' history records contain multiple heterogeneous temporal events such as lab tests, diagnosis, and drug administrations. The visiting patterns of different types of events vary significantly, and there exist complex nonlinear relationships between different events. In this paper, we propose a novel model for learning the joint representation of heterogeneous temporal events. The model adds a new gate to control the visiting rates of different events which effectively models the irregular patterns of different events and their nonlinear correlations. Experiment results with real-world clinical data on the tasks of predicting death and abnormal lab tests prove the effectiveness of our proposed approach over competitive baselines.", "text": "however problem challenging since patients’ historical records contain variety heterogeneous temporal events different tests routine vital signals diagnosis drug administrations visiting rates different events vary signiﬁcantly. example patient take blood test every morning take temperature test every hours. besides high level dependency among different kinds events. instance diagnosis made according results tests. result heterogeneous temporal events yield heterogeneous event sequences consisting thousands correlated event types visiting rate varies signiﬁcantly. literature learning representations sequences widely studied especially domain speech recognition natural language understanding. state-of-theart approaches sequence modeling recurrent neural networks long shortterm memory units rnns commonly used modeling homogeneous sequences nontrivial apply modeling heterogeneous event sequences. recent works based multi-task gauss process modeling correlations multiple sequences. however computational cost mtgp expensive data since thousands types events. therefore seeking approach able effectively model irregular visiting patterns different events; model complex nonlinear relationships different events; scale large availability large amount electronic health records provides huge opportunities improve health care service mining data. important application clinical endpoint prediction aims predict whether disease symptom abnormal test happen future according patients’ history records. paper develops deep learning techniques clinical endpoint prediction effective many practical applications. however problem challenging since patients’ history records contain multiple heterogeneous temporal events tests diagnosis drug administrations. visiting patterns different types events vary signiﬁcantly exist complex nonlinear relationships different events. paper propose novel model learning joint representation heterogeneous temporal events. model adds gate control visiting rates different events effectively models irregular patterns different events nonlinear correlations. experiment results real-world clinical data tasks predicting death abnormal tests prove effectiveness proposed approach competitive baselines. volume electronic health records expanding staggering rate providing great opportunity machine learning data mining researchers analyze data provide better health care service. important application machine learning health care predicting clinical endpoints disease symptom laboratory abnormality based patients’ historical records. paper develops effective deep learning techniques clinical endpoint prediction since deep learning techniques proved effective predictive analysis variety applications image recognition speech recognition natural language understanding goal deep learning learn effective semantic representations high-dimensional data images speeches natural language. therefore goal effectively represent patients’ historical records. paper propose approach called heterogeneous event lstm learning joint representation heterogeneous event sequences. approach extension phased lstm recently proposed used model irregular event-based sequential data. compared vanilla lstm model phased lstm adds time gate able naturally integrate inputs several sensors arbitrary sampling rates. phased lstm suitable modeling heterogeneous event sequence thousands event types longitudinal data. proposed model extends modeling correlated heterogeneous events multi-scale sampling rates. event type attributes embedded he-lstm. he-lstm equipped event gate controlled event type embeddings timestamps. help event gates helstm perfectly trace temporal information different event types long heterogeneous event sequence asynchronously sample important related events heterogeneous event sequence. therefore representation heterogeneous temporal events updated base dependency current input event events maintained he-lstm. conduct extensive experiments real-world clinical data. experiment results tasks death prediction abnormal test prediction prove proposed approach outperforms competitive baselines. proposed approach widely used modeling data collected sensors arbitrary sampling rates data collected mobile sensors. main contributions formulate clinical endpoint prediction task based data representation learning problem heterogeneous temporal events consists asynchronous clinical records multiple sources. propose novel model called he-lstm learning representations heterogeneous event sequence. model effectively models multi-scale sampling rates different kinds events temporal dependency. conducted experiment real-world clinical data tasks death abnormal tests. promising results prove effectiveness proposed approach competitive baselines. clinical endpoint prediction plenty works trying solve clinical endpoint prediction problem. however many works small subset whole sequences order avoid dealing high-dimensional event types. works select subset clinical events data according expertise physicians instance alaa uses physiological streams comprising vital signs test scores predict admission techniques select time series whole data transformed ﬁxed-size subset latent space using hyper-parameters multi-task models. calculate similarity patient’s records hyper-parameter space notable manually selecting fraction clinical sequences original data input brings expert bias thus works seldom make full important information original data. works ignore content value clinical events type information clinical events predict endpoints speciﬁcally approaches train semantic embeddings different categories clinical events endpoint predictions .retain uses reversed recursive neural networks generating attention variables sequential icd- code groups prediction tasks works using convolution neural network model irregular medical codes future risk predictions works exploit type information historical clinical events make predictions ignoring ﬁne-grained varying attributes events. work address issue utilizing rich type information clinical events well content values events. deep learning models sequential data standard rnns trained stochastic gradient descent difﬁculty learning long-term dependencies encoded input sequences owing vanishing gradient problem addressed example using specialized neuron structure long short-term memory networks maintains constant backward error signal. clockwork hidden layer partitioned separate modules processing inputs temporal granularity making computations prescribed clock rate. ﬁxed clock periods help contain long-term dependencies. phased lstm state-ofthe-art architecture modeling event-based sequential data. extends lstm adding time gate. gate three phases rises ﬁrst phase drops second phase active states. third phase model inactive state. updates permitted active state. phased lstm network achieve fast convergence experiments owing fact autosampling long sequential data conducted time gate maintains derivative error longer back propagation. however models focus learning long-term dependencies homogeneous sequences lacking ability capture various complex temporal dependencies heterogeneous temporal events usually exist data. event attribute encoding vector represents combining information event type type attribute value main input following he-lstm. event kinds attribute values. categorical representation valuec categorical values event types. numerical representation valuen number numerical value types. notice value categorical value assigned vector rn×c embedding dimension. numerical values associated value encoding vector rn×u embedding dimension. representing vector record mainly decided event type however attribute value event also carries lots information modeling patients. different values event type abnormal label test event lead distinct estimates patient’s future health status. important part disturbance numerical attribute values. instance high numerical value lactate blood test event indicates potential health problem patient value offer much information. finally combine three parts information attribute encoding vector given heterogeneous event lstm long short-term memory units important ingredient modern deep architectures. ﬁrst deﬁne update equations commonly-used version following main difference classical rnns gating functions represent input forget output gate time respectively. cell activation vector whereas represent input feature vector hidden output vector respectively. gates typical sigmoid function tanh nonlinear function tanh weight parameters connect different inputs gates memory cells outputs well biases cell state updated fraction previous cell state controlled input state created element-wise product denoted output cell state nonlinearity tanh. optional peephole connection notations deﬁnition task. heterogeneous events sequence heterogeneous event deﬁned triple type category event value attribute event type value logged time. noteworthy attribute values different event types either numerical categorical variable. example value test e.g.lactate blood test numerical value clinical status e.g. ectopia type categorical variable. heterogeneous events merged ascending order record time triple sequence {ei}. denote heterogeneous event sequence period time {ei}tstart≤ei.t≤tend. clinical endpoint prediction task clinical endpoint prediction task formulated follow given clinical heterogeneous event sequence {ei}tstart≤ei.t≤tend binary label target endpoint occurring tend hours objective predict target endpoint hours using {ei}tstart≤ei.t≤tend. paper dynamically predict endpoint outcomes base heterogeneous event sequence patient data ehr. ﬁrst death prediction dataset endpoint outcome death either hospital discharge home. second test result prediction dataset endpoint outcome either abnormal result potassium test clinical stability. event type embedding attribute encoding help he-lstm trace temporal information various kinds events event type embedding attribute encoding embed type attributes high dimensional events compact continuous vectors trained end-to-end following helstm. event sequence embedded three parts feed he-lstm endpoint prediction. three input including embedding vector event type event attribute encoding vector scale variable time event type vector carries information event category constructed representation type sequence type. similar word embedding provide lowdimension vector sequence type semantic meaning clinical ﬁeld. embedding lookup matrix ctype rn×m embedding dimension number sequence types established training. sequence type vector given ctype type figure model architecture. standard lstm model. he-lstm model event gate consist event ﬁlter phase gate separately controlled event type timestamp he-lstm formulation neural cell value hidden output updated open phase certain types events; otherwise previous values maintained. he-lstm extends lstm model adding event gate jst. event gate factors event ﬁlter phase gate. event ﬁlter allows information certain cluster events fuse corresponding memory cell cell trace particular group events. collaborated phase gate event ﬁlter help network maintain temporal information different events multi-scaled sampling rates. dependency heterogeneous events easier capture diverse long memory correlated events. opening closing event gate controlled event type embedding independent rhythmic oscillation speciﬁed phase gate three parameters. updates cell state permitted gate opened. factor event gate event ﬁlter neuron feed forward network hidden layer size tanh activation function following. considering multi-scale sampling rates events extend event ﬁlter time factor proposed phased lstm three parameters represents real-time period gate represents phase shift ratio open phase full period. learned training. therefore formally deﬁned different traditional rnns single sequential data even sparser variants rnns updates he-lstm optionally performed irregularly sampled time points different event types. allows rnns learn multi-scale rhythm related events work asynchronously sampled heterogeneous temporal event data. shorthand notation cell states time denote state previous update time tl−. rewrite regular lstm cell update equations using proposed cell updates mediated event gate he-lstm formulation ensures ﬂexible allocation retain information event clusters. neuron memory cell hidden layer he-lstm states updated open periods event gate. words information certain cluster events’ records certain neuron independent lstm lstm model homogeneous event independently average resulting representations logistic regression layer. computational cost thousands independent lstm exceed tolerance select important events done many works independent lstm model previous except weights single lstm shared events used input model. retain retain mimics physician practice modeling data reverse time order two-level generating attention variables sequential data provides interpretation prediction. lstm event embedding ﬁrst part proposed method section input traditional lstm. logistic regression applied hidden layer. clock-work clockwork tl−δ. thus neurons tracing events directly information cluster events even away term sequence index. allocation mechanism he-lstm much diverse longer memory modeling dependency multiple events. data description experimental settings data sets evaluation models real clinical data source. mimic-iii large freely-available database comprising de-identiﬁed health-related data relating forty thousand patients stayed critical care units beth israel deaconess medical center extract kinds events mimic-iii database initial event type set.the statistics event types frequency listed table.. merging heterogeneous events triple sequence clinical event sequences. drop sparse event types whose frequency total less extract episodes patients hours occurrence time endpoint event sequences samples. upper bound record number samples resulting sample events labeled according target endpoint outcome task. dataset split parts ﬁxed proportions namely training validation evaluation set. data validation used select hyper-parameters proposed comparing models conduct early stop training samples different cross-validation. evaluation details non-transparent imbalanced labeled. metrics binary labels accuracy suitable measuring performance. similar work adopt area curves area evaluation. reﬂect overall quality predicted scores decision time according true labels. area curve comparing true label ˆyi. robust imbalanced positive/negative prediction labels making appropriate evaluating classiﬁcation accuracy endpoint prediction prediction tasks. average precision average precision emphasizes ranking positive samples higher. average precisions computed point positive samples ranked sequence ascending order predict score firstly models considering dependency correlated events types outperform independent sequential models proposed he-lstm achieves best performance. example death prediction task retain lstm he-lstm improve test prediction around respectively compared best independent lstm model without weight share parameters independent lstms. similar results shown experiments metrics. furthermore model achieves highest performance among heterogeneous sequential models. example test prediction task he-lstm improves compared retain lstm. besides improvements respectively. draw conclusion dependency information correlated clinical temporal events useful endpoint prediction tasks learning joint representations effective model temporal dependency different events data compared simple independent sequential models. secondly compared densely updating recurrent neural networks rnns adaptive sampling rate pattern events make improvement prediction performance. example clock-work improve death prediction compared kinds independent lstms improvements phased lstm compared best independent lstms test prediction task. draw conclusion multi-scaled sampling rate pattern events effective endpoint prediction makes model concentrate important events different phases treating clinical events equally long sequence. thirdly he-lstm achieves best performance datasets evaluation metrics. he-lstm outperforms sparsely updating recurrent neural networks heterogeneous sequential models metrics datasets. models solely utilizing multi-scale sampling pattern event sequence models straight-forwardly merging different type events best choice clinical endpoint prediction data. take result death prediction example he-lstm improves respectively compared best sparsely updating methods without event type embedding attribute encoding modules. improvements he-lstm compared heterogeneous sequential models without event gates average term draw conclusion proposed he-lstm effectively improve performance joint effects tracing temporal dependency heterogeneous events adaptively ﬁtting multi-scaled sampling patterns. experiment variations event gate evaluate effect components event gate replace factors namely phase gate event ﬁlter remaining parts model identical. results datasets list table including cross entropy test data event ﬁlter mainly helps improve performance clinical endpoint prediction tasks modeling dependency heterogeneous events. event gate event ﬁlter achieve good performance metrics datasets training ﬁnished. example event gate event ﬁlter improve death prediction compared phase gate improvements improvements entropy phase gate helps achieve fast convergence early stage training ﬁtting multi-scaled sampling rates different events. he-lstm model phase gate much higher performance metrics datasets ﬁrst epoch training. take results test task example phase gate event gate improve ﬁrst epoch compared event ﬁlter improvements improvements entropy comparisons draw conclusion event ﬁlter phase gate collaborates jointly modeling dependency heterogeneous temporal events multi-scale sampling rates leads accurate efﬁcient performance clinical endpoint prediction task. experiment varying length multiple sequential data evaluate ability model temporal dependency heterogeneous temporal events proposed architect baselines feed trained models multiple events test various length range input. ﬁgure draw following conclusions firstly temporal information effective endpoint prediction tasks. performances models improve increase input sequence length. especially performance increases sharply length input sequence less secondly he-lstm better handling dependency heterogeneous temporal events models. input sequence short performances different models similar. reason lies fact that short sequence input combination independent representations single event makes less difference joint representation heterogeneous events he-lstm. input sequence longer longer performance model steadily increase term term auc. performance models remained almost unchanged almost different initial period explore effect event ﬁlter event gate modeling heterogeneous sequential data compare performance proposed he-lstm reduced he-lstm event ﬁlter factor event gate removed. different initial periods training death prediction task. period drawn uniformly exponential domain comparing four sampling intervals exp) exp) exp) exp) model. results figure. show initialization affects performance models. he-lstm robust initialization. example improvements he-lstm compared without event ﬁlter .%and average. draw conclusion that help event ﬁlter event gate adaptive multi-scale sampling rates events heterogeneous temporal sequence. ghassemi pimentel naumann brennan clifton szolovits feng multivariate timeseries modeling approach severity illness assessment forecasting sparse heterogeneous clinical data. proceedings aaai conference artiﬁcial intelligence. aaai conference artiﬁcial intelligence volume public access. henriksson zhao bostrom dalianis modeling electronic health records ensembles semantic spaces adverse drug event detection. ieee international conference bioinformatics biomedicine hinton deng dahl mohamed a.-r.; jaitly senior vanhoucke nguyen sainath deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine wang xiong temporal phenotyping longitudinal electronic health records graph based framework. sigkdd international conference knowledge discovery data mining mikolov sutskever chen corrado dean distributed representations words phrases compositionality. advances neural information processing systems multi-scaled sampling rates events heterogeneous event sequence. tracing temporal information different kinds events long sequence temporal dependency different types events captured learned representations. experimental results real-world clinical data tasks predicting death abnormal tests prove effectiveness proposed approach competitive baselines.", "year": 2018}