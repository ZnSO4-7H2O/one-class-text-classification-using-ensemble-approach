{"title": "Clustering with feature selection using alternating minimization,  Application to computational biology", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "This paper deals with unsupervised clustering with feature selection. The problem is to estimate both labels and a sparse projection matrix of weights. To address this combinatorial non-convex problem maintaining a strict control on the sparsity of the matrix of weights, we propose an alternating minimization of the Frobenius norm criterion. We provide a new efficient algorithm named K-sparse which alternates k-means with projection-gradient minimization. The projection-gradient step is a method of splitting type, with exact projection on the $\\ell^1$ ball to promote sparsity. The convergence of the gradient-projection step is addressed, and a preliminary analysis of the alternating minimization is made. The Frobenius norm criterion converges as the number of iterates in Algorithm K-sparse goes to infinity. Experiments on Single Cell RNA sequencing datasets show that our method significantly improves the results of PCA k-means, spectral clustering, SIMLR, and Sparcl methods, and achieves a relevant selection of genes. The complexity of K-sparse is linear in the number of samples (cells), so that the method scales up to large datasets.", "text": "terms suitable semi-deﬁnite program another efﬁcient approach spectral clustering main tools graph laplacian matrices however methods recently simlr provide sparsity. popular approach selecting sparse features supervised classiﬁcation regression least absolute shrinkage selection operator formulation lasso formulation uses norm instead added penalty term. hyperparameter unfortunaltely simple interpretation used tune sparsity. authors lasso-type penalty select features propose sparse k-means method. main issue optimizing values lagrangian parameter computationally expensive methods require k-means heuristic retrieve labels. alternating scheme propose combines k-means step dimension reduction feature selection using sparsity constraint. general framework matrix made line samples belonging d-dimensional space features. }m×k label matrix number clusters. line exactly nonzero element equal indicating sample belongs j-th cluster. projection matrix matrix centroids projected space abstract—this paper deals unsupervised clustering feature selection. problem estimate labels sparse projection matrix weights. address combinatorial non-convex problem maintaining strict control sparsity matrix weights propose alternating minimization frobenius norm criterion. provide efﬁcient algorithm named k-sparse alternates k-means projection-gradient minimization. projection-gradient step method splitting type exact projection ball promote sparsity. convergence gradientprojection step addressed preliminary analysis alternating minimization made. frobenius norm criterion converges number iterates algorithm k-sparse goes inﬁnity. experiments single cell sequencing datasets show method signiﬁcantly improves results k-means spectral clustering simlr sparcl methods. complexity k-sparse linear number samples method scales large datasets. finally extend k-sparse supervised classiﬁcation. paper deals unsupervised clustering feature selection high dimensional space. application choose single-cell rna-seq technology able measure expression thousands genes single cells. characterization diverse cell types distinguishing features require robust accurate clustering methods. however clustering high dimension suffers curse dimensionality dimensions increase vectors become indiscernible predictive power aforementioned methods drastically reduced order overcome issue popular approach highdimensional data perform principal component analysis prior clustering. approach however difﬁcult justify general alternative approach proposed combine clustering dimension reduction means linear discriminant analysis heuristic used based alternating minimization consists iteratively computing projection subspace using labels current iteration running k-means projection data onto subspace. departing work authors propose convex relaxation barlaud gilet univ. côte d’azur cnrs sophia antipolis. e-mail barlaudis.unice.fr giletis.unice.fr. j.-b. caillau ljad univ. côte d’azur cnrs/inria nice. e-mail caillauunice.fr deprez ipmc univ. côte d’azur cnrs sophia antipolis. e-mail deprezipmc.cnrs.fr recall estimation best lipschitz constant. lemma real matrix acting linearly real matrices left multiplication then norm linear operator endowed frobenius norm equal largest singular value σmax. denote column vectors matrix accordingly operator norm equal largest singular value block-diagonal matrix whose diagonal made matrix blocks. matrix readily largest singular value byproduct theorem corollary ﬁxed step forward-backward scheme applied problem exact projection balls converges linear rate towards solution estimate holds. proof. ball compact existence holds. convergence provided condition step lengths according previous lemma best lipschitz constant gradient σmax note implies sample belongs exactly cluster ensures cluster empty prevents trivial solutions consisting empty clusters contrast lagrangian lasso formulation want direct control value bound constrain according attack difﬁcult nonconvex problem propose alternating scheme another option would design global convex relaxation address joint minimization e.g. ﬁrst convex subproblem ﬁnds best projection dimension dimension given clustering. exact gradient-projection splitting method solve problem gradient-projection method. belongs class splitting methods designed solve minimization problems form exact projection. algorithm denote projection vectorized matrix important asset method takes advantage availability efﬁcient methods compute projection. denote closed ball radius space centered origin simplex denote projection well known projection fista implementation. constant step suitable size used accordance corollary setting useful normalization design matrix obtained replacing x/σmax. sets lipschitz constant theorem one. convergence rate algorithm speeded using fista step practice modiﬁed version ensures convergence iterates algorithm note ﬁxed step fista algorithm applied problem exact projection balls converges quadratic rate towards solution estimate holds. convergence algorithm. similarly approaches advocated method involves nonconvex k-means optimization convergence towards local minimizers proved practice k-means++ several replicates improve clustering step. assume initial guess labels matrix weights associated centroids different. note research recent attempts convexify k-means step alternating minimization scheme decreases norm nonnegative following readily holds. proposition frobenius norm converges number iterates algorithm goes inﬁnity. property illustrated next section biological data. analysis convergence build recent results proximal regularizations gauss-seidel alternating scheme convex problems gene selection. issue feature selection thanks sparsity inducing constraint also addressed speciﬁc context. projection aims sparsify matrix gene selected given constraint practical stopping criterion alternating minimization algorithm involves evolution number selected genes higher level loop bound itself evolution criterion accuracy versus analyzed. also note extension multi-label obvious sufﬁces allow several ones line matrix relaxing constraint supervised learning ﬁnal remark note straightforward modiﬁcation algorithm allows address supervised classiﬁcation. labels available simpler goal compute matrix weights well resulting centroids projected space. sake completeness include corresponding update algorithm experimentations supervised case scope paper reported somewhere else. klein scrna-seq dataset. klein characterized culture conditions using indrop sequencing. gene expression quantiﬁed unique molecular identiﬁer counts identify individual molecules allowing removal ampliﬁcation bias). counts cells label downloaded https//hemberg-lab.github.io/scrna.seq.datasets/. lowly expressed genes count million normalization reduce cell-to-cell variation sequencing report clustering four cell sub-populations corresponding four culture conditions. zeisel scrna-seq dataset. zeisel collected mouse cells primary somatosensory cortex hippocampal region using fluidigm microﬂuidics cell capture platform followed. gene expression quantiﬁed counts. counts metadata downloaded http//linnarssonlab.org/cortex. applied expressed gene ﬁltering normalization. report clustering nine major classes identiﬁed study. usoskin scrna-seq dataset. uzoskin collected cells mouse dorsal root ganglion using robotic cell-picking setup sequenced singlecell tagged reverse transcription method. filtered normalized data downloaded full sample annotations http//linnarssonlab.org/drg. report clustering four neuronal cell types. iii. application single cell rna-seq clustering experimental settings normalize features fista implementation constant step accordance corollary problem estimating number clusters scope paper refer popular method compare labels obtained clustering true labels compute clustering accuracy. also report popular adjusted rank index normalized mutual information criteria. processing times obtained macbook processor. give tsne results visual evaluation different methods k-means spectral clustering simlr sparcl method. single cell datasets single-cell sequencing technology elected \"method year\" nature methods. widespread methods enabled publication many datasets ground truth cell type annotations compare algorithms four public single-cell rna-seq datasets patel dataset klein dataset zeisel dataset usoskin dataset scrna-seq dataset. characterize intra-tumoral patel heterogeneity redundant transcriptional pattern glioblastoma tumors patel efﬁciently proﬁled expressed genes cells dissociated human glioblastomas using smart-seq protocol. ﬁltered centered-normalized data along corresponding cell labels downloaded https//hemberg-lab.github.io/scrna.seq.datasets/. described study report clustering clusters corresponding different dissociated tumors cells extracted. perform normalization gene selection dataset. experimental conclusions comparison advanced clustering methods accuracy nmi. k-sparse signiﬁcantly improves results sparcl simlr terms accuracy fig. report detailed evolution frobenius norm splitting loop k-means++ versus number loops. shows projection-gradient k-means++ steps contribute minimize iteratively frobenius norm. table comparison methods clusters cells genes ¯dopt ηopt k-sparse selects genes outperforms k-means spectral respectively. k-sparse similar accuracy better simlr. k-sparse times faster sparcl. fig. evolution accuracy function dimension projection order allow comparisons several databases accuracy plotted minus number clusters usoskin database e.g. optimal table comparison methods clusters cells genes preprocessing ¯dopt ηopt k-sparse selects genes. k-sparse simlr similar accuracy performances. k-sparse times faster simlr times faster sparcl. main issue sparcl optimizing values lagrangian parameter using permutations computationally expensive. computing kernel simlr also computationally expensive. complexity k-sparse linear number samples thus scales large databases. however main advantage k-sparse spectral simlr provides selected genes. table comparison methods clusters cells genes preprocessing ¯dopt ηopt k-sparse selected genes. k-means poor clustering performances. spectral sparcl similar performances. k-sparse outperforms methods. k-sparse times faster simlr times faster sparcl. note algorithms fail discover small clusters over-segment large cluster reﬂect main challenge biology identify rare events cell types discriminative characteristics fig. selection optimal bound ηopt. typical behaviour biological applications existence plateau-like zone many genes selected presence technical biological noise expression reduces accuracy. conversely small enough information available accuracy clustering also reduced. fig. accuracy versus number genes. results show minimum number genes required best possible clustering accuracy. genes involved relevant biological processes necessary distinguish cell types. hand patel klein datasets increasing number genes used clustering conditions repetitive signal minimum number genes necessary neither increase decrease clustering accuracy. hand zeisel usoskin datasets adding many genes would result decrease clustering accuracy. implying additional genes noisy technical biological variations little relevance distinguish cell types. k-sparse gradient part plus projection part average number nonzero entries sparse matrix number depends sharpness constraint deﬁned iteration must cost k-means expected average. allows k-sparse scale large large databases. contrast optimizing values lagrangian parameter using permutations sparcl computationally expensive complexity naive implementation kernel methods simlr results complexity. although computational cost reduced rank approximation computational cost expensive large data sets whence aggarwal. k-anonymity curse dimensionality. proceedings vldb conference trondheim norway arthur vassilvitski. k-means++ advantages careful seeding. proceedings eighteenth annual acm-siam symposium discrete algorithms attouch bolte redont soubeyran. proximal alternating minimization projection methods nonconvex problems approach based kurdyka-lojasiewicz inequality. mathematics operations research bach harchaoui. diffrac discriminative ﬂexible framework clustering. platt koller singer roweis editors advances neural information processing systems pages curran associates inc. bottou bengio. convergence properties k-means algorithms. tesauro touretzky leen editors advances neural information processing systems pages press combettes j.-c. pesquet. proximal splitting methods signal processing. fixed-point algorithms inverse problems science engineering pages springer ding adaptive dimension reduction using discriminant analysis k-means clustering. proceedings international conference machine learning icml pages york acm. duchi shalev-shwartz singer chandra. efﬁcient projections onto -ball learning high dimensions. proceedings international conference machine learning pages paper focus unsupervised clustering. provide efﬁcient algorithm based alternating minimization achieves feature selection introducing constraint gradient-projection step. step splitting type uses exact projection ball promote sparsity alternated k-means. convergence projectiongradient method established. iterative step algorithm necessarily lowers cost monotonically decreasing. experiments single-cell rna-seq dataset section demonstrate method promising compared algorithms ﬁeld. note algorithm straightforwardly applied clustering mosci rosasco santoro verri villa. solving structured sparsity regularization proximal methods. machine learning knowledge discovery databases pages springer jordan weiss. spectral clustering analysis algorithm. dietterich becker ghahramani editors advances neural information processing systems pages press fig. comparison visualization using tsne point represents cell. misclassiﬁed cells black reported datasets patel klein usoskin. k-sparse signiﬁcantly improves visually results sparcl simlr ﬁgure shows nice small ball-shaped clusters k-sparse simlr methods.", "year": 2017}