{"title": "Deep Learning using Rectified Linear Units (ReLU)", "tag": ["cs.NE", "cs.CV", "cs.LG", "stat.ML"], "abstract": "We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer $h_{n - 1}$ in a neural network, then multiply it by weight parameters $\\theta$ to get the raw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$, i.e. $f(o) = \\max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide class predictions $\\hat{y}$ through argmax function, i.e. argmax $f(x)$.", "text": "abstract introduce rectified linear units classification function deep neural network conventionally relu used activation function dnns softmax function classification function. however several studies using classification function softmax study addition those. accomplish taking activation penultimate layer neural network multiply weight parameters scores afterwards threshold scores i.e. relu function. provide class predictions function i.e. keywords artificial intelligence; artificial neural networks; classification; convolutional neural network; deep learning; deep neural networks; feed-forward neural network; machine learning; rectified linear units; softmax; supervised learning introduction number studies deep learning approaches claimed state-of-the-art performances considerable number tasks image classification natural language processing speech recognition text classification. deep learning models employ conventional softmax function classification layer. paper introduce rectified linear units classification layer deep learning model. approach novelty presented study i.e. relu conventionally used activation function hidden layers deep neural network. accomplish taking activation penultimate layer neural network learn weight parameters relu classification layer backpropagation. demonstrate compare predictive performance dl-relu models dl-softmax models mnist fashionmnist wisconsin diagnostic breast cancer classification. adam optimization algorithm learning network weight parameters. methodology machine intelligence library keras google tensorflow backend used implement deep learning algorithms study scientific computing libraries matplotlib numpy scikit-learn. mnist. mnist established standard datasets benchmarking deep learning models. -class classification problem training examples test cases grayscale image resolution fashion-mnist. xiao presented fashion-mnist dataset alternative conventional mnist. dataset consists grayscale images fashion products classes images class. wisconsin diagnostic breast cancer wdbc dataset consists features computed digitized image fine needle aspirate breast mass. data points dataset malignant benign. represents dataset features represents mean value dataset feature represents corresponding standard deviation. normalization technique implemented using standardscaler scikit-learn. case mnist fashion-mnist employed principal component analysis dimensionality reduction. select representative features image data. accomplished using scikit-learn. softmax. deep learning solutions classification problems usually employ softmax function classification function softmax function specifies discrete prob.. rectified linear units relu activation function introduced strong biological mathematical underpinning. demonstrated improve training deep neural networks. works thresholding values i.e. max. simply outputs conversely outputs linear function prediction units parameters learned backpropagating gradients relu classifier. accomplish this differentiate relu-based cross-entropy function w.r.t. activation penultimate layer algorithm mini-batch stochastic gradient descent training neural network rectified linear unit classification function. input rm}n output number training iterations deep learning using relu. relu conventionally used activation function neural networks softmax classification function. then networks softmax cross-entropy function learn weight parameters neural network. paper still implemented mentioned loss function distinction using relu table mnist classification. comparison ffnnsoftmax ffnn-relu models terms accuracy. training cross validation average cross validation accuracy splits. test accuracy unseen data. precision recall f-score unseen data. table shows architecture feed-forward neural network used experiments. last layer dense_ used softmax classifier relu classifier experiments. despite fact softmax-based ffnn slightly higher test accuracy relu-based ffnn models f-score. results imply ffnn-relu conventional ffnn-softmax. table mnist classification. comparison cnn-softmax cnn-relu models terms accuracy. training cross validation average cross validation accuracy splits. test accuracy unseen data. precision recall f-score unseen data. cnn-relu outperformed cnn-softmax since converged slower training accuracies cross validation inspected however despite slower convergence able achieve test accuracy higher granted lower test accuracy cnn-softmax optimization done cnn-relu achieve on-par performance cnn-softmax. figures show predictive performance models mnist classification classes. since cnn-softmax converged faster cnn-relu number correct predictions class. despite fact softmax-based ffnn slightly higher test accuracy relu-based ffnn models f-score. results imply ffnn-relu conventional ffnn-softmax. table fashion-mnist classification. comparison ffnn-softmax ffnn-relu models terms accuracy. training cross validation average cross validation accuracy splits. test accuracy unseen data. precision recall f-score unseen data. table fashion-mnist classification. comparison cnnsoftmax cnn-relu models terms accuracy. training cross validation average cross validation accuracy splits. test accuracy unseen data. precision recall f-score unseen data. figures show predictive performance models fashion-mnist classification classes. values correct prediction matrices seem balanced classes relu-based ffnn outperformed softmax-based ffnn vice-versa. similar findings mnist classification cnn-relu outperformed cnn-softmax since converged slower training accuracies cross validation inspected despite slightly lower test accuracy cnn-relu f-score cnn-softmax also similar findings mnist classification. figures show predictive performance models fashion-mnist classification classes. contrary findings mnist classification cnn-relu number correct predictions class. conversely faster convergence cnn-softmax higher cumulative correct predictions class. table wdbc classification. comparison cnn-softmax cnn-relu models terms accuracy. training cross validation average cross validation accuracy splits. test accuracy unseen data. precision recall f-score unseen data. similar findings classification using cnn-based models ffnn-relu outperformed ffnn-softmax wdbc classification. consistent cnn-based models ffnnrelu suffered slower convergence ffnn-softmax. however f-score difference them. stands reason ffnn-relu still comparable ffnn-softmax. wdbc implemented ffnn defined table hidden layers neurons followed neurons instead hidden layers neurons. wdbc classfication normalized dataset features. dimensionality reduction might prove prolific since wdbc features. figures show predictive performance models wdbc classification binary classification. confusion matrices show ffnn-softmax false negatives ffnn-relu. conversely ffnn-relu false positives ffnn-softmax. conclusion recommendation relatively unfavorable findings dl-relu models probably dying neurons problem relu. gradients flow backward neurons neurons become stuck eventually die. effect impedes richard hahnloser rahul sarpeshkar misha mahowald rodney douglas sebastian seung. digital selection analogue amplification coexist cortex-inspired silicon circuit. nature alex krizhevsky ilya sutskever geoffrey hinton. imagenet classification deep convolutional neural networks. advances neural information processing systems. pedregosa varoquaux gramfort michel thirion grisel blondel prettenhofer weiss dubourg vanderplas passos cournapeau brucher perrot duchesnay. scikit-learn machine learning python. journal machine learning research yichuan tang. deep learning using linear support vector machines. arxiv ludovic trottier philippe gigu brahim chaib-draa parametric exponential linear unit deep convolutional neural networks. machine learning applications ieee international conference ieee tsung-hsien milica gasic nikola mrksic pei-hao david vandyke steve young. semantically conditioned lstm-based natural language generation spoken dialogue systems. arxiv preprint arxiv. william wolberg nick street olvi mangasarian. breast cancer wisconsin data set. machine learning repository learning progress neural network. problem addressed subsequent improvements relu aside drawback stated dl-relu models still comparable better than conventional softmax-based models. supported findings dnn-relu image classification using mnist fashion-mnist. future work done thorough investigation dl-relu models numerical inspection gradients backpropagation i.e. compare gradients dl-relu models gradients dl-softmax models. furthermore relu variants brought table additional comparison. references martín abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg mané rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda viégas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems. http//tensorflow.org/ software available tensorflow.org. abien fred agarap. neural network architecture combining gated recurrent unit support vector machine intrusion detection network traffic data. arxiv preprint arxiv. abdulrahman alalshekmubarak leslie smith. novel approach combining recurrent neural network support vector machines time series classification. innovations information technology international conference ieee françois chollet keras. https//github.com/keras-team/keras. chorowski dzmitry bahdanau dmitriy serdyuk kyunghyun yoshua bengio. attention-based models speech recognition. advances", "year": 2018}