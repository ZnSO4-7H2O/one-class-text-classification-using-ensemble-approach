{"title": "Function space analysis of deep learning representation layers", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In this paper we propose a function space approach to Representation Learning and the analysis of the representation layers in deep learning architectures. We show how to compute a weak-type Besov smoothness index that quantifies the geometry of the clustering in the feature space. This approach was already applied successfully to improve the performance of machine learning algorithms such as the Random Forest and tree-based Gradient Boosting. Our experiments demonstrate that in well-known and well-performing trained networks, the Besov smoothness of the training set, measured in the corresponding hidden layer feature map representation, increases from layer to layer. We also contribute to the understanding of generalization by showing how the Besov smoothness of the representations, decreases as we add more mis-labeling to the training data. We hope this approach will contribute to the de-mystification of some aspects of deep learning.", "text": "abstract—in paper propose function space approach representation learning analysis representation layers deep learning architectures. show compute ‘weak-type’ besov smoothness index quantiﬁes geometry clustering feature space. approach already applied successfully improve performance machine learning algorithms random forest tree-based gradient boosting experiments demonstrate well-known well-performing trained networks besov smoothness training measured corresponding hidden layer feature representation increases layer layer. also contribute understanding generalization showing besov smoothness representations decreases mis-labeling training data. hope approach contribute de-mystiﬁcation aspects deep learning. representation learning main issues raised survey simple smoothness assumptions data hold. exists curse dimensionality ‘close’ feature representations ‘similar’ values. authors write advocate learning algorithms ﬂexible non-parametric rely exclusively smoothness assumption. work fact advocate smoothness analysis representation layers line notion smoothness indeed ﬂexible adaptive non-parametric. rely geometric multivariate function space theory machinery besov ‘weak-type’ smoothness robust enough support quantifying smoothness high dimensional discontinuous functions. although machine learning mostly associated ﬁeld statistics argue popular machine learning algorithms support vector machines tree-based gradient boosting random forest fact closely related ﬁeld multivariate adaptive approximation theory. essence algorithms work best exists geometric structure clusters feature space. geometry exists algorithms capture segmenting different clusters. claim absence geometry machine learning algorithms fail. however exactly deep learning comes play. absence geometrical structure given initial representation space goal layers create series transformations representation space next structure geometry clusters improves sequentially. quote whole process applying complex geometric transformation input data visualized imagining person trying uncrumple paper ball crumpled paper ball manifold input data model starts with. movement operated person paper ball similar simple geometric transformation operated layer. full uncrumpling gesture sequence complex transformation entire model. deep learning models mathematical machines uncrumpling complicated manifolds highdimensional data. network successfully trained classify images relatively high precision. allows extract representation image hidden layers. create representation layer concatenate rows pixel values image create vector dimension also normalize pixel values range since advocate function-theoretical approach transform class labels vector-values space assigning label vertex standard simplex thus images general case hope exists geometric clustering classes initial feature space sufﬁcient ‘weak-type’ smoothness thus transform different feature space needed. thus associate k-th samples vectors created normalizing concatenating feature maps computed images. interestingly enough although series functions embedded different dimensions simple normalizing features method able assign smoothness indices layer comparable. claim well performing networks related work architecture convolutional sparse coding analyzed. connection work emphasis ‘sparsity’ analysis hidden layers. however signiﬁcant differences since advocate function space theoretical analysis neural network architecture current use. also recent work authors take ‘information-theoretical’ approach analysis stochastic gradient descent optimization networks representation layers. safely approaches including presented here need evaluated larger datasets deeper architectures. paper organized follows section review smoothness analysis machinery wavelet decomposition random forest section present required geometric function space theoretical background. since comparing different representations different spaces different dimensions theory presented relevant ‘dimension-free’ results. section show apply theory practice. speciﬁcally wavelet decomposition used numerically compute besov ‘weak-type’ smoothness index given function representation space section provides experimental results demonstrate theory able explain empirical ﬁndings various scenarios. finally section presents conclusions well future work. measure smoothness dataset various representation layers apply construction wavelet decompositions random forests wavelets geometric wavelets powerful simple tool constructing sparse representations ‘complex’ functions. random forest introduced breiman effective machine learning method considered overcome ‘greedy’ nature high variance single decision tree. combined wavelet decomposition unravels sparsity underlying function establishes order nodes ‘important’ components ‘negligible’ noise. therefore method provides better understanding constructed furthermore method basis robust feature importance algorithm. note apply similar approach improve performance tree-based gradient boosting algorithms begin overview single trees. statistics machine learning construction called decision tree classiﬁcation regression tree given real-valued function initial domain subdomains e.g. intersecting hyper-plane. subdivision performed minimize given cost function. subdivision process continues recursively subdomains stopping criterion turn determines leaves tree. describe instance cost function related minimizing variance. stage subdivision process certain node observe given subdividing hyperplane approximating polynomials uniquely determined least square minimization. order approximating polynomials nothing mean function values subdomains many applications decision trees highdimensionality data allow search possible subdivisions. experimental results restrict subdivisions class hyperplanes aligned main axes. contrast cases would like consider advanced form subdivisions take certain hyper-surface form even non-linear forms kernel support vector machines. paradigm wavelet decompositions support principle forms. random forest popular machine learning tool collects decision trees ensemble model trees constructed independently diverse fashion prediction done voting mechanism among trees. element large diversity trees reduces ensemble’s variance. many variations differ randomness injected model bagging random feature subset selection partition criterion wavelet decomposition paradigm applicable versions known literature. geometric wavelet associated subdomain function given discrete dataset }i∈i wavelet ‘local difference’ component belongs detail space levels tree ‘low resolution’ level associated ‘high resolution’ level associated also wavelets ‘zero moments’ property i.e. response variable sampled polynomial recall approach convert classiﬁcation problems ‘functional’ setting assigning class labels vertices simplex rl−. cases multi-valued functions choosing wavelet bagging method produces partial replicates training data tree. typical approach randomly select tree certain percentage training randomly select samples repetitions randomness achieved node partitioning level. node restrict partition criteria small random subset parameter values typical selection search partition random subj creates decision tree based subset data provides weight tree based estimated performance supervised learning typically uses remaining data points evaluate performance simplicity mostly consider paper choice uniform weights point approximation associated tree denoted computed ﬁnding leaf contained evaluating corresponding polynomial associated decision node assigns classes. scenario input training point assigned class convert problem ‘functional’ setting described assigns class value node regular simplex consisting vertices thus assume input data form following classic paradigm nonlinear approximation using wavelets geometric function space theory presented introduced construction wavelet decomposition forest. important research area approximation theory pioneered pencho petrushev characterization adaptive geometric approximation algorithms generalizations classic ‘isotropic’ besov space ‘geometric’ besov-type spaces ﬁrst review deﬁnition results essence generalization theoretical framework successfully applied context dimensional structured signal processing however context machine learning need analyze unstructured possibly high dimensional datasets. notion smoothness allows handle functions even continuous. higher index ﬁnite smoother function generalizes sobolev smoothness differentiable functions partial derivatives integrable space. also deﬁnition generalizes classical function space theory besov spaces tree partitions non-adaptive. fact classical besov spaces special case tree constructed partitioning dyadic cubes time using levels tree. recall ‘well clustered’ function fact inﬁnitely smooth right adaptively chosen besov space. observe that contrary existing tree pruning techniques tree pruned separately approximation process applies ‘global’ pruning strategy signiﬁcant components come node trees level. simplicity could choose obtain fig. depicts m-term selected ensemble. colored nodes illustrate selection wavelets highest norm values entire forest. observe selected tree level connectivity restrictions. figure depicts parameter selected challenging wine quality dataset repository generation decision trees training creates approximately wavelets. parameter selected minimization approximation error validation set. contrast pruning methods using wavelet approximation method select signiﬁcant components tree level forest. method need predetermine maximal depth trees over-ﬁtting controlled selection signiﬁcant wavelet components. least dimension functions domain typically ﬁxed. speciﬁcally observe equivalence depends dimension feature space. theory ‘dimension-free’ analysis case remark known different geometric approximation schemes characterized different ﬂavors besovtype smoothness. work example experimental results compute smoothness representations using partitions along main axes. restriction lead general potentially lower besov smoothness underlying function lower sparsity wavelet representation. theoretical deﬁnitions results paper also apply generalized schemes where example tree partitions performed using arbitrary hyper-planes. case smoothness index given function increase. setting wish apply function theoretical approach comparing smoothness representation different layers networks. implies analyzing comparing smoothness functions different representation space different dimension sense non-standard function space theory space number features/neurons k-th layer. samples obtained applying network original images given k-th layer. example convolution layer capture representations cycle convolution non-linearity pooling. extract vectors created normalizing concatenating feature values corresponding images. recall next describe estimate smoothness function made several improvements simpliﬁcations method compute samples choice apply wavelet decomposition computes discrete error wavelet -term approximation case solves least squares finally estimate ‘critical’ besov smoothness index remarks observe signiﬁcant terms avoid ﬁtting ‘noisy’ tail exponential expression. cases allow select adaptively discarding tail wavelet components over-ﬁtting training data increasing error validation samples however cases goal demonstrate understanding generalization restrict analysis using training pre-select notice since representation space different dimension crucial method invariant different dimension embedding. note approach compute geometric besov smoothness labeled dataset signiﬁcant generalization method used compute besov smoothness single image. nevertheless distinct similar underlying function space approach. experiments used tensorflow networks models. extracted representation data sample layer network simply running tensorflow ‘session’ object given layer data sample parameters. computation besov smoothness index given feature space implemented explained section used updated version code available link hyper-parameter well known averaged forms modulus equivalent form constants depend dimension. however replacing allows produce ‘dimension-free’ analysis. deﬁne explain theory presented section used estimate ‘weak-type’ smoothness given function given representation layer. recall introduction create representation images layer color image). also normalize pixel values range transform class labels vectorvalues space assigning label vertex standard simplex thus images determines number -term errors used model α-smoothness used code executed amazon services cloud r.xlarge conﬁgurations virtual cpus memory. note computing smoothness representation certain dataset images layers requires signiﬁcant computation. needs create approximation representation layer sort wavelet components based norms compute errors -term errors numeric smoothness index computed. thus experiments computed used smoothness errors speed computation. present results estimates smoothness analysis layer representations datasets trained networks. begin audio dataset urban sound classiﬁcation applied smoothness analysis representations urbank audio data layers deeplisten model achieves accuracy network simple feed-forward fully connected layers relu non-linearities. described section created functional representation data layer estimated besov smoothness index layer. figure clustering ‘unfolded’ network besov α-index increases layer layer. network convolution layers fully connected layers additional soft-max layer training data size testing expected trained network achieves accuracy testing data. figure clear indication smoothness begins evolve training epochs ‘unfolding’ clustering improves layer layer. also smoothness improves epochs correlating improvement accuracy. version composed convolution layers fully connected layers. training model epochs produce model accuracy training data clear monotone increase besov smoothness across layers shown figure following applied random mis-labeling mnist cifar image sets various levels. randomly picked subsets size size dataset image subset picked random label. trained network misclassiﬁed mnist datasets network misclassiﬁed cifar set. emphasize goal experiment understand generalization automatically detect level corruption solely smoothness analysis training data. recall network converge relatively quickly over-ﬁt even highly mis-classiﬁed training sets. thus convergence good indication generalization capabilities speciﬁcally level mis-labeling training data. next created wavelet decomposition representation training last inner layer network. typically fully connected layer right softmax. figure decay precision error adaptive wavelet approximations wavelet terms. clear datasets less mis-labeling ‘sparsity’ i.e. better approximated less wavelet terms. also measured mis-labeled training dataset besov αsmoothness last inner layer. results presented table strong correlation amount mis-labeling smoothness. paper presented theoretical approach analysis performance hidden layers architectures. plan continue experimental analysis deeper architectures larger datasets hope demonstrate approach applicable wide variety machine learning deep learning architectures. advanced architectures millions features hidden layers need overcome problem estimating representation smoothness high dimensions. furthermore experiments noticed interesting phenomena within subcomponents layers hope reach understanding share insights regarding aspects too. authors would like thank vadym boikov kobi gurkan tel-aviv university help running experiments. research carried generous support amazon research program. wavelets ieee transactions image processing alpaydin introduction machine learning press bengio courville vincenty representation learning review perspectives ieee transactions pattern analysis machine intelligence criminisi shotton konukoglu forests classiﬁcation regression density estimation manifold learning semisupervised learning microsoft research technical report report genuer poggi christine variable selection using random forests pattern recognition letters geurts gilles learning rank extremely randomized trees jmlr workshop conference proceedings joly schnitzler f.geurts wehenkel l-based compression random forest models proceedings european symposium artiﬁcial neural networks computational intelligence machine learning martinez-muoz hern´andez-lobato suarez analysis ensemble pruning techniques based ordered aggregation ieee transactions pattern analysis machine intelligence papyan romano elad convolutional neural networks analyzed convolutional sparse coding submitted raileanu stoffel theoretical comparison gini index information gain criteria annals mathematics artiﬁcial intelligence salembier garrido binary partition tree efﬁcient representation image processing segmentation information retrieval ieee transactions image processing-", "year": 2017}