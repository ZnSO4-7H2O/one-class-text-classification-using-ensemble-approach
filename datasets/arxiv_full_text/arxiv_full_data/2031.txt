{"title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret  Online Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.", "text": "sequential prediction problems imitation learning future observations depend previous predictions violate common i.i.d. assumptions made statistical learning. leads poor performance theory often practice. recent approaches provide stronger guarantees setting remain somewhat unsatisfactory train either non-stationary stochastic policies require large number iterations. paper propose iterative algorithm trains stationary deterministic policy seen regret algorithm online learning setting. show regret algorithm combined additional reduction assumptions must policy good performance distribution observations induces sequential settings. demonstrate approach outperforms previous approaches challenging imitation learning problems benchmark sequence labeling problem. sequence prediction problems arise commonly practice. instance robotic systems must able predict/make sequence actions given sequence observations revealed time. complex robotic systems standard control methods fail must often resort learning controller make predictions. imitation learning techniques expert demonstrations good behavior used learn controller proven useful practice stateof-the performance variety applications typical approach imitation learning train classiﬁer regressor predict expert’s behavior given training data encountered observations actions performed expert. however since learner’s prediction affects future input observations/states execution learned policy violate crucial i.i.d. assumption made statistical learning approaches. ignoring issue leads poor performance theory practice particular classiﬁer makes mistake probability distribution states/observations encountered expert make many mistakes expectation -steps distribution states classiﬁer induces intuitively soon learner makes mistake encounter completely different observations expert demonstration leading compounding errors. recent approaches guarantee expected number mistakes linear task horizon error training several iterations allowing learner inﬂuence input states expert demonstration provided approach learns non-stationary policy training different policy time step sequence starting ﬁrst step. unfortunately impractical large ill-deﬁned. another approach called smile similar searn trains stationary stochastic policy adding policy mixture iteration training. however unsatisfactory practical applications policies mixture worse imitation learning necessarily know observe true costs particular task. instead observe expert demonstrations seek bound cost function based well mimics expert’s policy denote observed surrogate loss function minimize instead instance expected loss respect state squared/hinge loss respect importantly many instances function– instance interested optimizing learner’s ability predict actions chosen expert. system dynamics assumed unknown complex cannot compute sample executing system. hence non-i.i.d. supervised learning problem dependence input distribution policy itself. interaction policy resulting distribution makes optimization difﬁcult results non-convex objective even loss convex states brieﬂy review previous approaches guarantees. traditional approach imitation learning ignores change distribution simply trains policy performs well distribution states encountered expert dπ∗. achieved using standard supervised learning algorithm. ﬁnds policy ˆπsup example error rate trained predict next output sequence previous correct output input lead expected number mistakes sequences length test time. propose meta-algorithm imitation learning learns stationary deterministic policy guaranteed perform well induced distribution states take reduction-based approach enables reusing existing supervised learning algorithms. approach simple implement free parameters except supervised learning algorithm sub-routine requires number iterations scales nearly linearly effective horizon problem. naturally handles continuous well discrete predictions. approach closely related regret online learning algorithms better leverages expert setting. additionally show no-regret learner used particular fashion learn policy achieves similar guarantees. begin establishing notation setting discuss related work present dagger method. analyze approach using noregret reduction approach beyond reduction analysis consider sample complexity approach using online-to-batch techniques. demonstrate dagger scalable outperforms previous approaches practice challenging imitation learning problems learning steer racing game learning play super mario bros. given input image features corresponding actions human expert near-optimal planner respectively. following daumé treating structured prediction degenerate imitation learning problem apply dagger benchmark prediction problem achieving results competitive state-of-the-art using single-pass greedy prediction. begin introducing notation relevant setting. denote class policies learner considering task horizon. policy denote distribution states time learner executed policy time step furthermore average distribution states denote follow policy steps. given state denote expected immediate cost performing action state task considering denote ea∼π] expected immediate cost assume bounded total cost executing policy -steps es∼dπ ross bagnell provided imitation learning example hence traditional supervised learning approach poor performance guarantees quadratic growth instead would prefer approaches guarantee growth linear near-linear following approaches ross bagnell achieve classes imitation learning problems including surrogate loss upper bounds forward training algorithm introduced ross bagnell trains non-stationary policy iteratively iterations iteration trained mimic distribution states time induced previously trained policies πt−. trained actual distribution states encounter execution learned policy. hence forward algorithm guarantees expected loss distribution states induced learned policy matches average loss training hence improves performance. provide theorem slightly general provided ross bagnell applies policy guarantee surrogate loss distribution states. useful bound performance approach presented section denote t-step cost executing initial state following policy assume loss following performance guarantee respect task cost function bounded theorem es∼dπ t−t+ action ditional supervised learning approach. however many cases sub-linear forward algorithm leads improved performance. instance loss respect expert additionally able recover mistakes made sense within steps back distribution states close would executed initially instead drawback forward algorithm impractical large must train different policies sequentially cannot stop algorithm complete iterations. hence applied real-world applications. smile proposed ross bagnell alleviates problem applied practice large undeﬁned adopting approach similar searn stochastic stationary policy trained several iterations. initially smile starts policy always queries executes expert’s action choice. iteration policy trained mimic expert distribution trajectories induces updates αn−. update interpreted adding probability executing policy step removing probability executing queried expert’s action. iteration mixture policies probability using queried expert’s action stop algorithm iteration returning re-normalized policy πn−n doesn’t query expert anymore. ross bagnell showed choosing guarantees near-linear regret class problems. simplest form algorithm proceeds follows. ﬁrst iteration uses expert’s policy gather dataset trajectories train policy best mimics expert trajectories. iteration uses collect trajectories adds trajectories dataset next policy ˆπn+ policy best mimics expert whole dataset this case instance markov desision processes markov chain deﬁned system dynamics policy rapidly mixing. particular α-mixing exponential decay rate surrogate loss bound tells directly arbitrary task cost function upper bound loss respect combining result theorem yields that theorem dagger exists policy s.t. finite sample results ﬁnite sample case suppose sample trajectories iteration denote dataset es∼di] training loss minπ∈π best policy sampled trajectories using azumahoeffding’s inequality leads following guarantee theorem dagger probability least exists policy s.t. es∼dˆπ reﬁned analysis taking advantage strong convexity loss function lead tighter generalization bounds require order ˜o). similarly theorem dagger probability least exists policy s.t. j+ut theoretical analysis dagger relies noregret property underlying follow-the-leader algorithm strongly convex losses picks sequence policies hence presented results also hold regret online learning algorithm would apply imitation learning setting. particular consider results reduction imitation learning no-regret online learning treat mini-batches trajectories single policy single online-learning example. ﬁrst brieﬂy review concepts online learning regret used analysis. online learning algorithm must provide policy iteration incurs loss observing loss algorithm provide different policy next iteration incur loss necessary best policy sequence minimizes loss distribution; guarantee holds policy uniformly randomly picks policy sequence executes policy steps. words dagger proceeds collecting dataset iteration current policy trains next policy aggregate collected datasets. intuition behind algorithm iterations building inputs learned policy likely encounter execution based previous experience algorithm interpreted follow-the-leader algorithm iteration pick best policy ˆπn+ hindsight i.e. trajectories seen iterations. better leverage presence expert imitation learning setting optionally allow algorithm modiﬁed policy βiπ∗ iteration queries expert choose controls fraction time collecting next dataset. often desirable practice ﬁrst policies relatively datapoints make many mistakes visit states irrelevant policy improves. typically specify initial policy getting data expert’s behavior. could choose probability using expert decays exponentially smile searn. show requirement {βi} sequence simple parameter-free version algorithm described special case indicator function often performs best practice general dagger algorithm detailed algorithm main result analysis next section following guarantee dagger. denote sequence policies assume strongly convex bounded suppose constant independent minπ∈π true loss best policy hindsight. following holds inﬁnite sample case error reduction assumption input distribution policy achieves surrogate loss implies guaranteed policy achieves surrogate loss state distribution limit provided instance choose form extra penalty becomes negligible need least iterations make negligible number iterations required dagger similar required no-regret algorithm. note strong general error regret reductions considered require classiﬁcation require no-regret method strongly convex surrogate loss function stronger assumption. finite sample case previous results hold online learning algorithm observes inﬁnite sample loss i.e. loss true distribution trajectories induced current policy practice however algorithm would observe loss small sample trajectories iteration. wish bound true loss distribution best policy sequence function regret ﬁnite sample trajectories. iteration assume algorithm samples trajectories using observes loss es∼di) dataset trajectories. es∼di) online learner guarantees es∼di) minπ∈π es∼di] training loss minπ∈π best policy hindsight. following similar analysis cesa-bianchi obtain theorem dagger probability least exists policy s.t. es∼dˆπ average regret proof. difference expected step loss state distribution average step loss sample trajectory iteration random variables zero mean bounded max] form martingale azuma-hoeffding’s inequality loss functions vary unknown even adversarial fashion time. no-regret algorithm algorithm produces sequence policies average regret respect best policy hindsight goes goes show no-regret algorithms used policy good performance guarantees distribution states imitation learning setting. must choose loss functions loss distribution states current policy chosen online algorithm es∼dπi analysis dagger need bound total variation distance distribution states encountered continues call expert. following lemma useful lemma ||dπi dˆπi|| better trivial bound ||dπi dˆπi|| assume non-increasing deﬁne largest loss best polminπ∈π hindsight iterations upper bound loss i.e. policies state dˆπi following theorem dagger exists policy s.t. es∼dˆπ average regret compare performance race track called star track. track ﬂoats space kart fall track point measure performance terms average number falls lap. smile dagger used training iteration methods iterations. smile choose parameter ross bagnell dagger parameter indicator function. figure shows conﬁdence intervals average falls method iterations function total number training data collected. ﬁrst observe baseline supervised approach training always occurs expert’s trajectories performance improve data collected. training laps similar help learner learn recover mistakes makes. smile obtain improvements policy iterations still falls track twice average. part stochasticity policy sometimes makes choices actions. dagger able obtain policy never falls track iterations training. though even iterations policy obtain almost never falls track signiﬁcantly outperforming smile baseline supervised approach. furthermore policy obtained dagger smoother looks qualitatively better policy obtained smile. video available youtube shows qualitative comparison behavior obtained method. azuma-hoeffding’s inequality suggests need generalization error negligible steps. leveraging strong convexity lead tighter bound requiring trajectories. super kart racing game similar popular mario kart. goal train computer steer kart moving ﬁxed speed particular race track based current game image features input human expert used provide demonstrations correct steering observed game images. methods linear enemies falling gaps running time. used simulator recent mario bros. competition randomly generate stages varying difﬁculty goal train computer play game based current game image features input expert scenario near-optimal planning algorithm full access game’s internal state simulate exactly consequence future actions. action consists binary variables indicating subset buttons press {leftrightjumpspeed}. nell dagger obtain results different choice parameter indicator function values report best results obtained also report results shows slower convergence using expert frequently later iterations. similarly searn obtain results choice report best results obtained also report results shows unstability pure policy iteration approach. figure shows conﬁdence intervals average distance travelled stage iteration function total number training data collected. observe supervised compare performance terms average distance travelled mario stage dying running time completing stage randomly generated stages difﬁculty time limit seconds complete stage. total distance stage varies around average performance vary roughly stages difﬁculty fairly easy average human player contain types enemies gaps except fewer enemies gaps stages harder difﬁculties. compare performance dagger smile searn supervised approach approach collect data points iteration methods iterations. smile choose parameter ross bagfor input features image discretized grid cells centered around mario; binary features describe cell history features last images used addition features describing last actions state mario total binary features output binary variable optimizes objective regularizer using stochastic gradient descent approach performance stagnates collect data expert demonstrations help particular errors learned controller makes. particular reason supervised approach gets score learned controller mario often stuck location obstacle instead jumping since expert always jumps obstacles signiﬁcant distance away controller learn unstuck situations right next obstacle. hand iterative methods perform much better eventually learn unstuck situations encountering later iterations. experiment dagger outperforms smile also outperforms searn choice considered. using convergence signiﬁcantly slower could beneﬁted iterations performance still improving iterations. choosing yields slightly better performance indicator function potentially large number data generated mario stuck location early iterations using indicator; whereas using expert small fraction time still allows observe locations also unstucks mario makes collect wider variety useful data. video available youtube also shows qualitative comparison behavior obtained method. finally demonstrate efﬁcacy approach structured prediction problem involving recognizing handwritten words given sequence images character word. follow daumé adopting view structured prediction degenerate form imitation learning system dynamics deterministic trivial simply passing earlier predictions made inputs future predictions. dataset taskar used extensively literature compare several structured prediction approaches. dataset contains roughly words partitioned folds. consider large dataset experiment consists training folds testing fold repeating folds. performance measured terms character accuracy test folds. consider predicting word predicting character sequence left right order using previously predicted character help predict next linear following greedy searn approach daumé compare method smile well searn also compare approaches baseline non-structured approach simply predicts character independently supervised training approach training conducted previous character always correctly labeled. choice searn report results best approaches iterations. figure shows performance approach test folds iteration function training data. baseline result without structure achieves character accuracy using predicts character independently. adding previous character feature training always previous character correctly labeled performance increases using dagger increases performance surprisingly observe searn pure policy iteration approach performs well experiment similarly best dagger. small part input inﬂuenced current policy binary features used encode previously predicted letter word. train multiclass using all-pairs reduction binary classiﬁcation predicted character feature) makes approach unstable general reinforcement/imitation learning problems searn smile small performs similarly signiﬁcantly worse dagger. note chose simplest decoding illustrate beneﬁts dagger approach respect existing reductions. similar techniques applied multi-pass beam-search decoding leading results competitive state-of-the-art. show batching iterations interaction system no-regret methods including presented dagger approach provide learning reduction strong performance guarantees imitation learning structured prediction. future work consider sophisticated strategies simple greedy forward decoding structured prediction well using base classiﬁers rely inverse optimal control techniques learn cost function planner prediction imitation learning. believe techniques similar presented leveraging cost-to-go estimate provide understanding success online methods reinforcement learning suggest similar data-aggregation method guarantee performance settings.", "year": 2010}