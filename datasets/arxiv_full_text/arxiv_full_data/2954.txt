{"title": "Higher-Order Markov Tag-Topic Models for Tagged Documents and Images", "tag": ["cs.CV", "cs.AI", "cs.IR", "cs.LG"], "abstract": "This paper studies the topic modeling problem of tagged documents and images. Higher-order relations among tagged documents and images are major and ubiquitous characteristics, and play positive roles in extracting reliable and interpretable topics. In this paper, we propose the tag-topic models (TTM) to depict such higher-order topic structural dependencies within the Markov random field (MRF) framework. First, we use the novel factor graph representation of latent Dirichlet allocation (LDA)-based topic models from the MRF perspective, and present an efficient loopy belief propagation (BP) algorithm for approximate inference and parameter estimation. Second, we propose the factor hypergraph representation of TTM, and focus on both pairwise and higher-order relation modeling among tagged documents and images. Efficient loopy BP algorithm is developed to learn TTM, which encourages the topic labeling smoothness among tagged documents and images. Extensive experimental results confirm the incorporation of higher-order relations to be effective in enhancing the overall topic modeling performance, when compared with current state-of-the-art topic models, in many text and image mining tasks of broad interests such as word and link prediction, document classification, and tag recommendation.", "text": "abstract—this paper studies topic modeling problem tagged documents images. higher-order relations among tagged documents images major ubiquitous characteristics play positive roles extracting reliable interpretable topics. paper propose tag-topic models depict higher-order topic structural dependencies within markov random ﬁeld framework. first novel factor graph representation latent dirichlet allocation -based topic models perspective present efﬁcient loopy belief propagation algorithm approximate inference parameter estimation. second propose factor hypergraph representation focus pairwise higher-order relation modeling among tagged documents images. efﬁcient loopy algorithm developed learn encourages topic labeling smoothness among tagged documents images. extensive experimental results conﬁrm incorporation higherorder relations effective enhancing overall topic modeling performance compared current state-of-the-art topic models many text image mining tasks broad interests word link prediction document classiﬁcation recommendation. index terms—topic models latent dirichlet allocation markov random ﬁelds bayesian networks factor graph hypergraph higherorder relation tagged documents images belief propagation message passing hierarchical bayesian models. goal work model infer semantically meaningful word clusters referred topics large-scale tagged documents images. broad sense deﬁne label characterizes certain properties documents images. example author identiﬁes authorship document time stamp marks document published. hand treat images documents composed visual words. users often manually annotate images semantic tags building tree label local contents objects interests. generally document associated multiple tags attached multiple documents. fig. illustrates example tagged documents tags authors link denotes author writes document. fig. shows another example tagged images four images annotated three tags building people. conveniently represent tagged documents images bipartite graph fig. composed nodes document image nodes connected links. zeng school computer science technology soochow university suzhou china. also shanghai laboratory intelligent information processing china center bioinformatics computational biology university maryland college park usa. correspondence addressed. e-mail j.zengieee.org. besides pairwise relations higher-order relations among tagged documents images formed multiple tags major ubiquitous characteristics. example authors collaborate write document topic machine learning denoted intersection subset circles fig. similarly collaborate write topic computer vision collaborate write topic data mining. three authors also jointly collaborate write document denoted intersection subset three circles fig. simply decompose higher-order relation three pairwise relations come conclusion focuses combined topics machine learning computer vision data mining. nevertheless possibility fact modeling totally topic like computational biology intersection subset three circles excluded shown fig. obviously lies speciﬁc subset quite different union subsets γ}}. explicit modeling higher-order relation among documents constituted multiple tags needed distinguish speciﬁc topic combined topics furthermore modeling higher-order relations also reﬂects truth tags often attached document jointly rather separately explain document content. similar higher-order relations among images induced fig. examples tagged documents images tagged documents tags authors tagged images tags annotations bipartite graph representation higher-order relation distinguishes speciﬁc topic combined topics however prior efforts pairwise relation modeling topic models rarely consider higher-order relations encode speciﬁc topic structural dependencies among tagged documents images. therefore paper propose tag-topic models describe higher-order topic structural dependencies within markov random ﬁeld framework. approach extends previous work modeling higher-order relations coauthors generic tagged documents images allowing develop efﬁcient inference parameter estimation algorithms within theoretically well-founded framework. first reformulate topic modeling task labeling problem novel perspective. represent latent dirichlet allocation -based topic models factor graphs develop classic loopy belief propagation algorithm make approximate inference parameter estimation. second represent using factor hypergraph according bipartite graph fig. focus pairwise higher-order relation modeling within higher-order framework. indeed higherorder recently found important applications modeling high-level image structural priors many computer vision problems including image restoration disparity estimation object segmentation generally inferring higher-order intrinsically computationally expensive problem since even encoding -order topic structural dependencies topics requires labeling conﬁgurations. however similar image structural priors higher-order relations used topic modeling also certain properties smoothness sparsity makes easy handle. intuitively co-tagged documents images tend higher likelihood share similar topic labeling conﬁguration. based smoothness sparsity prior many higher-order topic labeling conﬁgurations equally unlikely thus need encouraged. therefore encourage total smooth topic labeling conﬁgurations avoids encoding arbitrary topic structural dependencies. design higherorder functions encode major representative smoothness relations develop loopy algorithm make efﬁcient inference parameter estimation ttm. rest paper organized follows. section introduces related work. sections presents topic modeling develops loopy algorithms approximate inference parameter estimation. section proposes focuses pairwise higher-order relation modeling among tagged documents images. section shows extensive experimental results several challenging text image mining tasks broad interests. finally section draws conclusions envisions future work. related work probabilistic topic models text mining state-of-the-art approach learning terminological ontologies basic topic model fig. allocates topic label word document based document-speciﬁc topic proportion topic-speciﬁc multinomial distribution vocabulary words smoothed conjugate dirichlet hyperparameters respectively. plates indicate replication. example document repeats times corpus word repeats times document total topics. builds implicit links documents sharing topic distribution encourages similar topic labeling conﬁgurations documents contain similar words. however uses exchange topic information among documents ignores rich link information like citations hyperlinks documents. motivates recent variants regularize topic distribution pairwise relations documents. pairwise topic models focus link generation process turn inﬂuences topic allocation words. link uses document-speciﬁc topic proportion topic-speciﬁc distribution documents generate cited document citing docfig. l-lda shaded circles observed variables others latent variables parameters. plates indicate replication. hyperparameter subscripts omitted simplicity. ument. documents cite document tend similar topic labeling conﬁguration words. sense link indirectly depicts co-citation link documents citing documents scales badly large-scale corpus parameters increases total number documents. overcome weaknesses pairwise directly generates binary citation link variable documents using topic-dependent bernoulli distribution. randomly uses topic labels rather entire topic labels document generate links signiﬁcantly limits inﬂuence link information topic regularization. relax limitation relational topic model represents entire document topics mean value document topic proportions. uses hadamard product mean values linking documents link features learned generalized linear model generate observed citation link variable citation link variables replaced observed tags adapted account tagged documents images. similar basic idea latent topic hypertext model assumes links originate words uses partial word topic labels generate links. furthermore topic-link multirelational topic models markov random topic ﬁelds simultaneously generate multiple types links citations coauthor relations social community authors improve accuracy topic modeling. citation inﬂuence model allows topic citing document dependent either topic proportions cited documents’ topic proportions. topic modeling network regularization adopts graph-based regularizer encourage minimum euclidean distance document-layer topic labeling conﬁgurations. markov topic models gaussian markov random ﬁeld describe topic interactions among documents different conferences. nevertheless recent pairwise topic models limauthor-topic models labeled able associate observed tags words directly. uses documentspeciﬁc uniform distribution generate uses tag-speciﬁc topic proportions generate topic label word. plate indicates unique tags. documents share tag-speciﬁc topic proportions implicitly encodes pairwise relation documents associated l-lda constrains latent topics observed tags generated document-speciﬁc topic proportions tags associated multinomial distribution generate words. sense l-lda supervised topic model replaces latent topic labels observed tags. documents share multinomial distribution also encodes statistical information documents associated however higher-order relations among documents images multiple connected tags largely neglected l-lda motivates explore speciﬁc higher-order study. table summarizes important notations paper. perspective subsection formulates topic modeling labeling problem within framework. objective topic modeling assign semantic topic labels {zwd} explain observed words word index vocabulary document index corpus. generally topic label takes topic index partitions words topic groups topic modeling technique often viewed document index vocabulary word index index topic index words topic labels words labels excluding labels excluding factor document factor word factor factor hyperedge topic messages word clustering paradigms. theory solves labeling problem assigning best topic labels according maximum posteriori estimation mrf-map framework found many important applications image analysis computer vision speciﬁcally attempts best topic labeling conﬁguration words maximizing posterior probability nature prohibited combinatorial optimization problem discrete latent topic space. avoid high computational cost often uses smoothness sparsity property labeling problem reduce total number possible labeling conﬁgurations topic modeling concerned encourage smoothness neighboring topic labels i.e. neighboring topic labels tend same. deﬁne neighborhood system topic label z−wd zw−d z−wd denotes topic labels associated word indices document excluding word index zw−d denotes topic labels associated word index documents excluding furthermore factor graph represent treat parameters factors parameterized functions designing proper factor functions equivalent clique potentials encourage penalize different local labeling conﬁgurations neighborhood system. speciﬁcally encourage topic labeling smoothness among {zwd z−wd zw−d}. paper consider type ﬁxed symmetric dirichlet hyperparameters order avoid complex full bayesian inference respectively. transform generative graphical representation fig. factor graph fig. perspective. illustrate factors squares denote connected variables circles. obviously factors connects neighboring topic labeling conﬁgurations {zwd z−wd zw−d}. hierarchically directed graphical model fig. becomes generic undirected graphical model fig. absorb observed word index index factor similar absorbing observed document index index factor fig. factors parameterized functions multinomial functions smoothed dirichlet priors deﬁned also hyperparameters viewed pseudo-counts estimating corresponding multinomial distributions. resembles collapsed integrates parameter variables treats hyperparameters pseudo topic counts order perform inference collapsed hidden variable space recently reformulated bayesian network constrained undirected graphical models causal dependencies between hidden variables. indeed fig. fig. reﬂect facets former focuses generative process observed words hierarchically latter emphasizes topic labeling smoothness within framework. original factor graph representation naturally extended describe generative process probabilistic model. example extension directed factor graph enhances visual language represent lda. topic modeling task formulated labeling problem perspective original undirected factor graph enough expressive power represent directly. although undirected graph explicitly emphasize generative process directed counterpart still captures underlying structural dependencies hidden variables without loss information. sense factor graph generic visual representation directed undirected graphical models various real-world applications. although factor graph fig. slightly different directed graphical model fig. fulﬁll topic modeling task using speciﬁc neighborhood systems factor functions. first figs. neighborhood system connection hidden variables remains same. second next subsection shall design speciﬁc factor functions realize topic modeling goal fig. without loss information. loopy algorithms sumproduct max-sum algorithms provide efﬁcient approximate solutions inference problems graphs loops fig. rather directly calculating posterior probability turn calculating posterior marginal probability referred message normalized efﬁciently using local computation. message passing proceeds fig. factor graph lda. passing messages factors variable zwd. passing messages neighboring variables z−wd zw−d factors respectively. arrows show directions message passing. variables factors turn factors variables convergence several iterations. subsection adopt sum-product algorithm infer marginal posterior probability message passing scheme instantiation e-step expectation-maximization algorithm widely used infer marginal probabilities hidden variables various graphical models according maximum-likelihood estimation. example e-step inference gaussian mixture models forward-backward algorithm hidden markov models probabilistic relaxation labeling algorithm formulated within message passing framework e-step estimate parameters based inferred marginal probabilities m-step algorithm almost algorithms learning ﬁnite mixture models like gmm-based hmm. details learning ﬁnite mixture models using algorithm found book arrows denote message passing directions. normalized message turn passed back factors. fig. messages factors variables calculated based input messages neighboring variables follows z−wd zw−d represent possible neighboring labeling conﬁgurations factor function evaluates topic structural dependencies input topic messages. topic labeling smoothness prior implies topic conﬁgurations encourpractice eqs. often cause product multiple input messages close zero avoid arithmetic underﬂow approximate product messages messages product value increases value increases subsequent formulas. factor functions correspond clique potentials designed arbitrarily encode prior knowledge encouraging penalizing topic labeling conﬁgurations. indeed higher value encourages passing neighboring messages. here design normalizes input messages total number topics associated document order make output messages comparable across different documents. normalizes input messages total number messages word indices vocabulary order make output messages comparable across different vocabulary words. notations denote word indices except document indices except notations represent possible neighboring messages excluding current message normalize updated message practice ﬁnite iterations message converge factor graph shown fig. usually converges fast note need multiply number word counts relative word frequencies corresponding word topic message message passing parameter estimation. given inferred marginal posterior probability parameter estimation performed simply using adding input messages including evaluated corresponding factor functions alternatively also derive parameter estimation equations using algorithm e-step calculate marginal posterior probability employing multinomial-dirichlet conjugacy bayes’ rule following marginal dirichlet distributions resembles proposed except randomly samples topic label marginal posterior probability word token immediately updates parameters based currently sampled topic label. therefore needs sample topic label word token document calculates message word index vocabulary within document. word sparsity document signiﬁcantly lowers computational cost addition randomly sampled topic label always loses information compared marginal posterior probability result accurate parameter estimation keeps uses complete messages learning iteration without loss information. uses jensen’s inequality adjustable lower bound objective function maximizes objective maximizing lower bound tuning variational parameters. also resembles proposed except calculates topic messages minimizing kullback-leibler divergence variational distribution true posterior distribution. thus variational message update equations differ signiﬁcantly involving complicated digamma functions. learning iteration computational cost requires average vocabulary size average number word tokens document. document number word indices usually much smaller total number word tokens word sparsity i.e. generally scale much better large-scale corpus. detailed comparisons among found hierarchical baysian model maximizes objective generate topic labels words undirected model maximizes objective assign best topic labels words. objectives identical according bayes’ rule since constant terms collapsed gibbs sampling variational bayes commonlyused approximate inference algorithms lda-based topic models. paper provide alternative inference method lda-based topic models using algorithm novel perspective. fig. shows factor hypergraph representation directly combines factor graph fig. bipartite graph fig. note undirected hypergraph equivalent bipartite graph fig. factor hyperedge connects factors attached document fig. factor hypergraph representation absorb observed index factor connects variable neighbors z·d′ using solid black line shown fig. assume document pair share document pair d′′} share document associated three tags t′′}. although fig. follow standard deﬁnition factor graphs factor hyperedge variant factor graph represent pairwise higher-order relations among tagged documents images shown fig. factor hypergraph tag-topic models three tags illustrated simplicity. pairwise relation modeling passing messages factor higher-order relation modeling passing messages factor hyperedge denoted yellow block. arrows show directions message passing. deﬁned hadamard product captures similarity connected documents latent topic representations. result average hadamard product pairs documents connected encodes dominant pairwise topic structural dependencies. weighted word messages document respect viewed normalized message passed words document fig. factor function depicts higherorder topic dependencies among documents images connected tags t′′} generally modeling -order -order relations sufﬁcient practice documents images often contain less four tags table without loss generality present -order relation modeling higher -order relation modeled fig. fig. example topic labeling conﬁgurations z·d′ z·d′′ inﬂuence neighboring label separately factor based pairwise relation fig. meanwhile {z·d′ z·d′′} also inﬂuence neighbor jointly factor hyperedge based higherorder relation resulted connected factors γt′} fig. result factor function encodes pairwise relation {z·d′ zwd} factor function depicts higher-order relation among {z·d′ z·d′′ zwd}. attached semantic usually accounts parts words documents local contents images. credit attribution task associate individual words document appropriate tags probabilistic framework assume tags document associate word different likelihoods calculated based pairwise relation formed shown fig. speciﬁcally label associated word document calculate likelihood based following similarity terms topic messages message µγt→zwd factor introduced next subsections. intuitively measures similarity word content latent topic space. practice randomly initialize tags word iteratively update normalize using notation denotes total number -order document image triples constituted connected tags respectively. obviously average hadmard product triples documents images capturing major representative -order topic structural dependencies among tagged documents images. fig. contains loops develop loopy algorithm approximate inference parameter estimation. subsection calculated messages µθd→zwd µφw→zwd subsection focus computing message µγt→zwd µδd→zwd based sumproduct algorithm involves pairwise relations also higher-order relations among documents figs. respectively. topic smoothness constraint message factor variable ne\\d contains document pairs except current document connected tags respectively. passes messages factor neighboring documents individual passes joint messages factor hyperedge connects multiple factors therefore inﬂuences word message pairwise relation across individual plays similar role higher-order relation across multiple tags t′}. similar replace product operation operation neighboring input messages order avoid arithmetic underﬂow. standard sum-product algorithm calculate marginal posterior probability µzwd product input messages according fig. however direct product ﬂexible balance messages factors fig. conceivably message µθd→zwd measures topic labeling inﬂuence within document message µγt→zwd captures inﬂuence neighboring documents pairwise relations message µδd→zwd plays similar role weights balance three messages factors messages µγt→zwd terms individual attached document accumulates inﬂuence attached tags. obviously shows current word message regularized pairwise higher-order relations tagged documents. reduces becomes without information. depict pairwise relations tagged documents. automatic estimating best weights requires studies future work. paper manually tune weights based training data sets. inference parameter estimation equations almost except update equation message replaced fig. summarizes loopy algorithm learning ttm. learning iteration need estimate pairwise higher-order topic structural dependencies using computational cost learning total number pairwise higher-order relations among tagged documents images. four data sets tagged documents images cora medline former contains abstracts cora research paper search engine machine learning area latter contains abstracts medline biomedical paper search engine. author names tags paper. cora documents classiﬁed major categories documents fall broadly categories. photographs. image associated manually labeled tags depict main objects appearing picture. colored pattern appearance model represent image visual words. sliding window decomposes image visual words tile mapped word vocabulary indexes built cpam lots image patches using vector quantization. table summarizes statistics four data sets total number documents total number tags vocabulary size average number words document average vocabulary size document average number tags document. following experiments randomly divide entire cora documents training test sets. training test partition images constitute training remaining images constitute test set. randomly partition entire images training test sets. manually tune weights based perplexity training data. refer ttm-p pairwise relation modeling. refer ttm-h pairwise higherorder relation modeling. comparative study ttm-p ttm-h explore effectiveness modeling higher-order topic interactions among tagged documents images. compare three current state-of-the-art topic models exponential link probability function l-lda using experimental settings. discussed section benchmark http//cran.r-project.org/web/packages/lda/ http//psiexp.ss.uci.edu/research/programs data/toolbox.htm http//nlp.stanford.edu/software/tmt/tmt-./ topic models able handle pairwise relations between tagged documents. contrast additionally considers higher-order relations induced connected tags among documents. l-lda supervised topic model compare l-lda recommendation task. experiments assume tags unobserved test data estimated topic distributions training data predict words links tags well class labels documents test set. word prediction task evaluate likelihood learned topic distributions generate unseen test data. fig. compares test perplexity ttm-p ttm-h. lower perplexity corresponds higher likelihood learned topics generate unseen test set. data sets ttm-h consistently achieves lowest perplexity different topics implies best generalization ability predict words unseen test sets. unlike explicitly model pairwise topic representations tagged documents images insufﬁciently beneﬁt rich relational information regularizing topic distributions. hand estimates link probability function document pairs connected different tags ttmp estimates tag-speciﬁc pairwise relations using result ttm-p potential capture subtle topic structural dependencies documents images speciﬁc tags. fig. shows ttm-p achieves almost reduction average perplexity compared rtm. furthermore ttm-h gains average reduction perplexity compared ttm-p indicates joint inﬂuence higher order relations paly positive roles topic distribution regularization. although ttm-h higher computational complexity ttm-p worth gaining better word prediction performance realworld applications. generally predictive perplexity decreases number topics increases knowledge system paper design reasoning problem theory approach case systems system knowledge learning paper reasoning case approach planning design cases design system reasoning case knowledge theory cases systems approach planning measure interpretability topic model word intrusion topic intrusion proposed involve subjective judgements basic idea volunteer subjects identify number word intruders topic well topic intruders document intruders deﬁned inconsistent words topics based prior knowledge subjects. lack volunteer subjects fig. shows three consistent topics words cora training qualitative evaluation. topics share similar words different ranking orders. nevertheless extract ﬁrst topics contain word intruder paper even extracts three word intruders design research university third topic. obviously ttm-p ttm-h show much better interpretability least words contain irrelevant common words paper. moreover ttm-h slightly better ttm-p natural word ranking order topic. link prediction suggest tags document linking document. tags author names link prediction reviewers collaborators linking document. also link prediction help retrieve related documents similar tags. effectiveness applications depend highly link prediction accuracy. deﬁne link prediction binary classiﬁcation problem. hadmard product pair document topic proportions link feature train decide link them. evaluate link prediction performance using number linking/nonlinking training test samples. fig. compares f-measure link prediction. encode hadmard link features pairs documents prediction results almost random guess f-measure close data sets. contrast shows signiﬁcantly better link prediction performance using generalized linear models estimated link features efﬁciently differentiate links non-links. data sets ttm-p deviates slightly ttm-p encode pairwise relations tagged documents. however ttm-h outperforms around f-measure link prediction. possible reason ttm-h incorporates much richer higherorder topic structural dependencies makes topic proportions documents sharing tags differentiable documents without sharing tags. interestingly f-measure always increase number topics increases. although latent topics predict unseen words better shown fig. cannot consistently enhance link prediction performance test shown fig. phenomenon suggests content similarity documents alone cannot completely account link information. additional information partially observed links documents help better link prediction performance. document classiﬁcation partitions documents several mutually exclusive categories. topic models used dimensionality reduction method reduce high-dimensional word vector space classiﬁcation document topic proportions reduced feature vectors study discriminative ability document classiﬁcation. train svms document topic proportions given class labels compare document classiﬁcation accuracy test set. cora randomly select training samples seven categories. randomly select training samples categories. choose four tags class labels water trees people. images associated four tags training purposes. randomly select training samples class. randomly select training samples class. remaining documents images test samples. fig. shows classiﬁcation accuracy based lowdimensional document topic proportions. generally outperforms inconsistent word prediction performance fig. reason treats sharing tags equal links reality different tags encode different topic structural dependencies documents. thus erroneously encourage topic smoothness documents different tags often close correspondence class labels documents especially tags used class labels contrast ttm-p relaxes limitation encouraging smoothness document topic proportions using tag-speciﬁc pairwise relation modeling. furthermore ttm-h still outperforms ttmp higher classiﬁcation accuracy average forcing tag-speciﬁc smoothness constraint pairwise higher-order relations. image classiﬁcation performance generally worse cora partly tags tend describe individual image components exactly equivalent class labels describe global image contents. similar link prediction task latent topics enhance overall document classiﬁcation performance. recommendation multi-label classiﬁcation problem suggests tags query documents images found many real-world applications credit attribution expert ﬁnding image annotation lack benchmark data evaluate expert ﬁnding performance focus recommendation image annotation section. propose ttm-based recommendation system including svms class label. train multiclass called classify image topic proportions tags training samples images associated tag. images used training samples multiple tags. training sample predicts vector likelihoods table. compares ttm-h ttm-p stateof-the-art recommendation methods l-lda similar coverage rate ttm-h provides competitive image annotation performance sml. although l-lda shows comparable better recommendation performance tagged pages show clear advantages image annotation problem especially data set. indeed l-lda connected information training data play major roles rule many false positives enhance average precision. ttm-h still outperforms ttm-p consistent superior document classiﬁcation performance shown fig. furthermore latent topics improve recommendation performance show best results ttm-h ttm-p number latent topics conclusions paper presented discussed effectiveness encoding smoothness pairwise higherorder topic interactions among tagged documents images. within framework allows efﬁcient loopy algorithm inference parameter estimation. four large-scale data sets consistently outperforms current state-of-the-art topic models also train total binary svms called tags. positive sample tagged image feature vector predicted connected tags feature encodes information connected tags robust prediction. balance training data choose number positive/negative samples. training sample predicts vector likelihoods likelihood recommended test image predict tags. then likelihoods tags. balance predict prediction results linearly combine likelihoods best mixture weight estimated training set. follow standard image annotation evaluation protocol suggest tags query image highest system uses image content information suggest tags uses connected tags reﬁne recommendation result. basic idea suggested image connected tags also high likelihood suggested. performance measures image recommendation include recall precision rates speciﬁcally given number images test labeled human number images test labeled recommendation system number images system gives correct recommendation. recall precision rates deﬁned recall nc/nh precision nc/ns. also evaluate coverage rate rate+ recommended tags calculated number tags positive recall divided total number tags test set. higher rate+ implies better generalization ability achieve relative szeliski zabih scharstein veksler kolmogorov agarwala tappen rother comparative study energy minimization methods markov random ﬁelds smoothness-based priors ieee trans. pattern anal. mach. intell. vol. ramage hall nallapati manning labeled supervised topic model credit attribution multilabeled corpora empirical methods natural language processing zeng z.-q. type- fuzzy gaussian mixture models pattern recognition vol. zeng z.-q. type- fuzzy hidden markov models application speech recognition ieee trans. fuzzy syst. vol. june zeng z.-q. markov random ﬁeld-based statistical character structure modeling handwritten chinese character recognition ieee trans. pattern anal. mach. intell. vol. carneiro chan moreno vasconcelos supervised learning semantic classes image annotation retrieval ieee trans. pattern anal. mach. intell. vol. furthermore observe higher-order relations also exist many important computer vision text mining applications. example unsupervised activity perception crowded complicated scenes involves lots higher-order interactions multiple agents encoded topic models discovering speciﬁc motion patterns. another example tracking historical topics time-stamped documents. speculate higher-order temporal topic interactions characterize speciﬁc long-range topic evolution patterns also studied future work. nallapati ahmed xing cohen joint latent topic models text citations chang blei hierarchical relational models document networks annals applied statistics vol.", "year": 2011}