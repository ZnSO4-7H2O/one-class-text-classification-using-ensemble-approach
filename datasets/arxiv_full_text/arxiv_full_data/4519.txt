{"title": "Learning Low-Density Separators", "tag": ["cs.LG", "cs.AI"], "abstract": "We define a novel, basic, unsupervised learning problem - learning the lowest density homogeneous hyperplane separator of an unknown probability distribution. This task is relevant to several problems in machine learning, such as semi-supervised learning and clustering stability. We investigate the question of existence of a universally consistent algorithm for this problem. We propose two natural learning paradigms and prove that, on input unlabeled random samples generated by any member of a rich family of distributions, they are guaranteed to converge to the optimal separator for that distribution. We complement this result by showing that no learning algorithm for our task can achieve uniform learning rates (that are independent of the data generating distribution).", "text": "abstract. deﬁne novel basic unsupervised learning problem learning lowest density homogeneous hyperplane separator unknown probability distribution. task relevant several problems machine learning semi-supervised learning clustering stability. investigate question existence universally consistent algorithm problem. propose natural learning paradigms prove that input unlabeled random samples generated member rich family distributions guaranteed converge optimal separator distribution. complement result showing learning algorithm task achieve uniform learning rates theory machine learning achieved extensive understanding many aspects supervised learning theoretical understanding unsupervised learning leaves desired. spite obvious practical importance various unsupervised learning tasks state current knowledge provide anything comes close rigorous mathematical performance guarantees classiﬁcation prediction theory enjoys. consider following task unknown data distribution homogeneous hyperplane lowest density cuts distribution. assume underlying data distribution continuous density function data available learner ﬁnite i.i.d. samples distribution. model viewed restricted instance fundamental issue inferring information probability distribution random samples generates. tasks nature range ambitious problem density estimation estimation level sets densest region detection course clustering. tasks notoriously diﬃcult respect sample complexity computational complexity aspects task seems modest these. although aware previous work problem believe rather basic problem relevant various practical learning scenarios. important domain detection low-density linear data separators relevant semi-supervised learning semi-supervised learning motivated fact many real world classiﬁcation problems unlabeled samples much cheaper easier obtain labeled examples. consequently great incentive develop tools unlabeled samples utilized improve quality sample based classiﬁers. naturally utility unlabeled data classiﬁcation depends assuming relationship unlabeled data distribution class membership data points rigorous discussion point). common postulate type boundary data classes passes low-density regions data distribution. transductive support vector machines paradigm example algorithm implicitly uses density boundary assumption. roughly speaking tsvm searches hyperplane small error labeled data time wide margin respect unlabeled data sample. another area low-density boundaries play signiﬁcant role analysis clustering stability. recent work analysis clustering stability found close relationship stability clustering data density along cluster boundaries roughly speaking lower densities stable clustering low-density-cut algorithm family probability distributions takes input ﬁnite sample generated distribution output hyperplane origin density w.r.t. particular consider family distributions continuous density functions. investigate notions success low-density-cut algorithms uniform convergence consistency. uniform convergence prove general negative result showing algorithm guarantee ﬁxed convergence rates negative result holds even simplest case data domain one-dimensional unit interval. consistency prove success natural algorithmic paradigms; soft-margin algorithms choose margin parameter output separator lowest empirical weight margins around hard-margin algorithms choose separator widest sample-free margins. paper organized follows section provides formal deﬁnition learning task well success criteria investigate. section present natural learning paradigms problem real line prove universal consistency rich class probability distributions. section extends results show learnability lowest-density homogeneous linear cuts probability distributions arbitrary dimension section show previous universal consistency results cannot improved obtain uniform learning rates conclude paper discussion directions research. shall mostly consider distance measure theses cases omit explicit reference results hold well taken probability mass symmetric diﬀerence taken deﬁnition denote family probability distributions assume members density functions identify distribution density function. denote distance density function. consider natural algorithms lowest density family. ﬁrst simple bucketing algorithm. explain detail show consistency section second algorithm hard-margin algorithm outputs mid-point largest consecutive points sample. section show hard-margin algorithm consistent section bucketing algorithm consistent. section show algorithms interval equal length subintervals given input sample counts number sample points lying bucket outputs mid-point bucket fewest sample points. case ties picks rightmost bucket. denote algorithm turns exists choice makes algorithm consistent theorem number buckets bucketing algorithm consistent proof. assume unique minimizer neighbourhood unique minimizer compact hence exists since unique minimizer hence positive. thus pick neighbourhood since algorithm must output mid-point bucket henceforth algorithm’s output mid-point bucket intersects thus estimate diﬀers radius neighbourhood radius bucket. since length bucket combining above exists probability least draw i.i.d. sample size |bk− note proof cannot replace condition since vapnik-chervonenkis bounds allow detect o-diﬀerence probability show large enough probability least least buckets receives sample point. since probability masses same think buckets coupon types collecting sample points coupons. lemma suﬃces verify number trials prove theorem need following property distribution largest adjacent elements points forming i.i.d. sample uniform distribution statement present proof originally proven l´evy proof consider uniform distribution unit circle. suppose draw i.i.d. sample size distribution. denote size largest adjacent samples. hard distribution lm−. furthermore since first show suﬃciently large probability lower bound follows lemma constant i.i.d. sample points least bucket empty probability show expression sample points misses least bucket probability therefore largest probability least division unit circle position left end-point ﬁrst bucket bucketing called oﬀset bucketing. next show suﬃciently large probability upper bound consider bucketings b/ǫ. bucketing equal length buckets; bucket length bucketing left end-point ﬁrst bucket position bucket least sample point. consider sample bucketing bucket least point then largest bigger bucket size plus diﬀerence oﬀsets adjacent bucketings since otherwise largest would demonstrate empty bucket least bucketings. other words largest section consider problem learning minimum density homogeneous linear distributions namely assuming unknown probability distribution generates i.i.d. ﬁnite sample points wish process samples -dimensional hyperplane origin lowest probability density respect sample-generating distribution. words wish space origin sparsest direction. since algorithm must output weight vector lying words algorithm’s output lies i.e. |hγt proven exists sample drawn i.i.d. |hγt words consistent algorithm using random sample size drawn distributions chosen uniformly random identify distribution probability error less probability least random choices sample. reﬂection w.r.t. centre unit interval i.e. functions simply described constant functions anywhere except thin -shape around resp. bottom them. lower-bound probability sample size drawn misses furthermore constant containing entire probability mass therefore denoting probability point drawn distribution density hits hence i.i.d. sample size misses probability least e−m/n constant suﬃciently large. proper suﬃciently large e−m/n symmetry random sample size drawn misses probability. shown suﬃciently large regardless whether sample drawn either distributions intersect probability since density functions equal probability error discrimination conditioned sample intersect cannot less paper presented novel unsupervised learning problem modest enough allow learning algorithm asymptotic learning guarantees relevant several central challenging learning tasks. analysis viewed providing justiﬁcation common semi-supervised learning paradigms maximization margins unlabeled sample search empirically-sparse separating hyperplanes. know results provide ﬁrst performance guarantees paradigms. general perspective paper demonstrates type meaningful information data generating probability distribution reliably learned ﬁnite random samples distribution fully non-parametric model without postulating prior assumptions structure data distribution. such search low-density data separating hyperplane viewed basic tool initial analysis unknown data. analysis carried situations learner prior knowledge data question access unsupervised random sampling. analysis raises intriguing open questions. first note prove universal consistency ‘hard-margin’ algorithm real data distributions similar result higher dimensional data. since searching empirical maximal margins common heuristic interesting resolve question consistency algorithms. another natural research direction work calls extension results complex separators. clustering example common search clusters separated sparse data regions. however between-cluster boundaries often linear. provide reliable algorithm detection sparse boundaries ﬁnite random samples boundaries belong richer family functions? research focused information complexity task. however evaluate practical usefulness proposed algorithms also carry computational complexity analysis low-density separation task. conjecture problem ﬁnding homogeneous hyperplane largest margins lowest density around np-hard however even conjecture true interesting eﬃcient approximation algorithms problems.", "year": 2008}