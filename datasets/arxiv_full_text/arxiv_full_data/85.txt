{"title": "Learning Intrinsic Sparse Structures within Long Short-Term Memory", "tag": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "abstract": "Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will simultaneously decrease the sizes of all basic structures by one and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Based on group Lasso regularization, our method achieves 10.59x speedup without losing any perplexity of a language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset. Our approach is successfully extended to non- LSTM RNNs, like Recurrent Highway Networks (RHNs). Our source code is publicly available at https://github.com/wenwei202/iss-rnns", "text": "yuxiong samyam rajbhandari† minjia zhang† wenhan wang† fang liu§ business bing§ microsoft {yuxhesamyamrminjiazwenhanwfangliubinhu}microsoft.com model compression signiﬁcant wide adoption recurrent neural networks user devices possessing limited resources business clusters requiring quick responses large-scale service requests. work aims learn structurally-sparse long short-term memory reducing sizes basic structures within lstm units including input updates gates hidden states cell states outputs. independently reducing sizes basic structures result inconsistent dimensions among them consequently invalid lstm units. overcome problem propose intrinsic sparse structures lstms. removing component simultaneously decrease sizes basic structures thereby always maintain dimension consistency. learning within lstm units obtained lstms remain regular much smaller basic structures. based group lasso regularization method achieves speedup without losing perplexity language modeling penn treebank dataset. also successfully evaluated compact model weights machine question answering squad dataset. approach successfully extended nonlstm rnns like recurrent highway networks source code available. model compression louizos class approaches reducing size deep neural networks accelerate inference. structure learning philipp carbonell cortes emerges active research area structure exploration potentially replacing human labor machine automation design space exploration. intersection techniques important area learn compact structures dnns efﬁcient inference computation using minimal memory execution time without losing accuracy. learning compact structures convolutional neural networks widely explored past years. proposed connection pruning sparse cnns. pruning method also works successfully coarse-grain levels pruning ﬁlters cnns reducing neuron numbers presented general framework learn versatile compact structures dnns. learning compact structures recurrent neural networks challenging. recurrent unit shared across time steps sequence compressing unit aggressively affect steps. recent work narang proposes pruning approach deletes connections rnns. connection pruning methods sparsify weights recurrent units cannot explicitly change basic structures e.g. number input updates gates figure speedups matrix multiplication using non-structured structured sparsity. speeds measured intel implementations intel xeon .ghz. general matrix-matrix multiplication implemented cblas sgemm. matrix sizes selected reﬂect commonly used gemms lstms. example represents gemm lstms hidden size input size batch size accelerate gemm sparsity sparsiﬁed. non-structured sparsity approach randomly sparsiﬁed encoded compressed sparse format sparse computation structured sparsity approach columns rows removed match level sparsity faster gemm smaller sizes. hidden states cell states outputs. moreover obtained sparse matrices irregular/nonstructured pattern non-zero weights unfriendly efﬁcient computation modern hardware systems previous study sparse matrix multiplication gpus showed speedup either counterproductive ignorable. speciﬁc sparsity weight matrices alexnet speedup respectively. problem also exists cpus. fig. shows non-structured pattern sparsity limits speedup. starts observe speed gain sparsity beyond speedup even sparsity theoretical work focus learning structurally sparse lstms computation efﬁciency. speciﬁc reduce number basic structures simultaneously learning obtained lstms original schematic dense connections smaller sizes basic structures. compact models structured sparsity columns rows weight matrices removed whose computation efﬁciency shown fig. moreover off-the-shelf libraries deep learning frameworks directly utilized deploy reduced lstms. details explained. vital challenge originated recurrent units basic structures interweave other independently removing structures result mismatch dimensions inducing invalid recurrent units. problem exist cnns neurons independently removed without violating usability ﬁnal network structure. contributions identify structure inside rnns shall considered group effectively explore sparsity basic structures. speciﬁc propose intrinsic sparse structures groups achieve goal. removing weights associated component sizes/dimensions simultaneously reduced one. evaluated method lstms rhns language modeling penn treebank dataset machine question answering squad dataset approach works ﬁne-tuning training scratch. stacked lstm layers hidden sizes language modeling method learns sizes ﬁrst second lstms respectively sufﬁcient perplexity. achieves speedup inference time. result obtained training scratch number epochs. directly training lstms sizes cannot achieve perplexity proves advantage learning model compression. encouraging results also obtained major approach compression reduce complexity structures within dnns. studies categorized three classes removing redundant structures original dnns approximating original function dnns jaderberg hinton prabhavalkar molchanov designing dnns inherently compact structures bradbury method belongs ﬁrst category. research removing redundant structures feed-forward neural networks typically cnns extensively studied. based regularization park connection pruning number connections/parameters dramatically reduced. group lasso based methods proved effective reducing coarse-grain structures cnns alvarez salzmann lebedev lempitsky yoon hwang instance reduced number layers resnet without accuracy loss cifar- dataset. recent work narang advances connection pruning techniques rnns. compresses size deep speech around however best knowledge little work carried reduce coarse-grain structures beyond ﬁne-grain connections rnns. work targets develop method learn reduce number basic structures within lstm units. learning structures ﬁnal lstms still regular lstms connectivity sizes reduced. another line related research structure learning fnns cnns. zoph uses reinforcement learning search good neural architectures. philipp carbonell dynamically adds eliminates neurons fnns using group lasso regularization. cortes gradually adds sub-networks current networks incrementally reduce objective function. works focused ﬁnding optimal structures fnns cnns classiﬁcation accuracy. contrast work aims learning compact structures lstms model compression. element-wise multiplication sigmoid function tanh hyperbolic tangent function. vectors vectors. weight matrices transform concatenation input updates gates fig. schematic lstms layout olah transformations corresponding nonlinear functions illustrated rectangle blocks. goal reduce size sophisticated structure within lstms meanwhile maintaining original schematic. element-wise operators vectors along blue band fig. must dimension. call constraint dimension consistency. vectors required obey dimension consistency include input updates gates hidden states cell states outputs. note hidden states usually outputs connected classiﬁer layer stacked lstm layers. seen fig. vectors interweave removing individual component vectors independently result violation dimension consistency. overcome this propose intrinsic sparse structures within lstms shown blue band fig. component highlighted white strip. decreasing size able simultaneously reduce dimensions basic structures. learn sparse turn weight sparsifying. totally eight weight matrices organize form fig. basic lstm cells tensorflow. remove component zeroing associated weights white rows white columns fig. why? suppose k-th hidden state removable k-th lower four weight matrices zeros weights connections receiving k-th useless hidden state. likewise connections receiving k-th hidden state next layer removed shown right white horizontal line. note next layer output layer lstm layers fully-connected layers them. overlay layers without explicit explanation refer ﬁrst lstm layer ownership iss. k-th hidden state turns useless k-th output gate k-th cell state generating hidden state removable. k-th output gate generated k-th column weights zeroed tracing back computation fig. reach similar conclusions forget gates input gates input updates respectively shown ﬁrst second third vertical line fig. convenience call weights white rows columns weight group. although propose lstms variants vanilla rnns gated recurrent unit recurrent highway networks also realized based philosophy. even medium-scale lstm number weights weight group large. seems aggressive simultaneously slaughter many weights maintain original recognition performance. however proposed intrinsically exists within lstms even unveiled independently sparsifying weight using -norm regularization. experimental result covered appendix unveils sparse intrinsically exist lstms learning process easily converge status high ratio removed. section propose learning method explicitly remove much implicit -norm regularization. suppose vector weights k-th component n-th lstm layer number lstm layers number components n-th lstm layer. optimization goal remove many weight groups possible without losing accuracy. methods remove weight groups successfully studied cnns summarized section however methods perform rnns unknown. here extend group lasso based methods rnns sparsity learning. speciﬁc group lasso regularization added minimization function order encourage sparsity iss. formally regularization data loss learning rate coefﬁcient group lasso regularization trade recognition accuracy sparsity. regularization gradient i.e. last term unit vector. constantly squeezes euclidean length zero that high portion components enforced fully-zeros learning. avoid division zero computation regularization gradient tiny number learning method effectively squeeze many groups near zeros hard exactly stabilize zeros always-present ﬂuctuating weight updates. fortunately ﬂuctuation within tiny ball centered zero. stabilize sparsity training zero weights whose absolute values smaller pre-deﬁned threshold process thresholding applied mini-batch. experiments published models baselines. application domains include language modeling penn treebank machine question answering squad dataset. comprehensive evaluation sparsify lstm models large hidden size small hidden size also extended approach state-of-the-art recurrent highway networks reduce number units layer. maximize threshold fully exploit beneﬁt. speciﬁc application preset cross validation. maximum sparsiﬁes dense model without deteriorating performance selected. validation performed training effort needed. stacked lstms penn treebank bidaf model. used hyperdrive rasley explore hyperparameter details found source code. measure inference speed experiments dual socket intel xeon .ghz processor total cores memory. intel library update used matrix-multiplication operations. openmp runtime utilized parallelism. used intel compiler generate executables windows server experiments iterations execution time averaged execution latency. figure intrinsic sparse structures learned group lasso regularization original weight matrices plotted blue dots nonzero weights white ones refer zeros. better visualization original matrices evenly down-sampled stacked lstm layers language modeling selected baseline. hidden sizes lstm units. output layer vocabulary words. dimension word embedding input layer word embedding layer sparsiﬁed computation selecting vector matrix efﬁcient. training scheme baseline adopted learn sparsity except larger dropout keep ratio versus baseline group lasso regularization also avoid over-ﬁtting. models trained scratch epochs. results shown table note that trained using dropout keep ratio without adopting group lasso regularization baseline over-ﬁts lowest validation perplexity trade-off perplexity sparsity controlled second tiny perplexity difference baseline approach reduce number ﬁrst second lstm unit respectively. reduces model size achieves speedup. remarkably practical speedup even goes beyond theoretical mult-add reduction shown table —which comes increased computational efﬁciency. applying structured sparsity underlying weight matrices become smaller cache good locality improves flops advantage approach non-structurally sparse rnns generated connection pruning suffers irregular memory access pattern inferior-theoretical speedup. last learning compact structure method perform structure regularization avoid overﬁtting. shown third table lower perplexity achieved even smaller faster model. learned weight matrices visualized fig. components shown white strips removed ﬁrst second lstm respectively. straightforward reduce model complexity directly design smaller hidden size train scratch. compare direct design approach method automatically learn optimal structures within lstms. importantly compact models learned method lower perplexity comparing direct design method. evaluate directly design exactly structure second table train scratch instead learning larger rnn. result included last table tuned dropout keep ratio best perplexity directly-designed rnn. ﬁnal test perplexity higher method. recurrent highway networks class state-of-the-art recurrent models enable step-to-step transition depths larger one. deﬁne number units layer width. speciﬁcally select variational model table zilly baseline. depth width totally parameters. nutshell approach reduce width without losing perplexity. following idea identifying weight groups reduce size basic structures lstms identify groups rhns reduce width. brief group include corresponding columns/rows weight matrices nonlinear transform gates embedding output layers. group size groups indicated json ﬁles source code. learning rhns simultaneously reduce dimension word embedding number units layer. table summarizes results. experiments trained scratch hyperparameters baseline except smaller dropout ratios used learning. larger smaller width higher perplexity. importantly without losing perplexity approach learn smaller model width initial model width reduces model size reduction. moreover learning smaller model width meanwhile improve state-of-the-art perplexity shown second entry table evaluate method state-of-the-art dataset model squad recently released reading comprehension dataset crowdsourced question-answer pairs wikipedia articles. exactmatch scores major metrics task. higher scores better model adopt bidaf evaluate method works small lstm units. bidaf compact machine question answering model totally weights. sizes lstm units. implementation bidaf made available authors bidaf character word contextual embedding layers extract representations input sentences following bi-directional attention layer modeling layer ﬁnal output layer. lstm units used contextual embedding layer modeling layer output layer. lstms bidirectional bidirectional lstm forward plus backward lstm branch. branches share inputs outputs concatenated next stacked layers. found hard remove components contextual embedding layer representations relatively dense close inputs original hidden size relatively small. experiments exclude lstms contextual embedding layer sparsify lstm layers. lstm layers computation bottleneck bidaf. proﬁled computation time cpus lstm layers consume total inference time. three bi-directional lstm layers sparsify belong modeling layer belongs output layer. details bidaf covered brevity mark forward path bi-directional lstm modeling layer modfwd similarly modfwd modbwd bi-directional lstm. forward lstm path output layer marked outfwd outbwd. discussed section multiple parallel layers receive hidden states lstm layer connections receive hidden states belong iss. instance modfwd modbwd receive hidden states modfwd inputs therefore k-th weight group includes k-th rows weights modfwd modbwd plus weights k-th component within modfwd. simplicity modfwd refer whole group weights. structures included table appendix learn sparsity bidaf ﬁne-tuning baseline training scratch. training schemes keep baseline except applying higher dropout keep ratio. training zero weights whose absolute values smaller impact scores increase sparsity. table shows number remaining components model size inference speed. ﬁrst baseline bidaf. rows obtained ﬁne-tuning baseline using regularization. second learning small loss reduce lstms except modfwd. example almost half components removed outbwd. increasing strength group lasso regularization increase sparsity losing em/f scores. trade-off listed table score loss sizes outfwd outbwd reduced original respectively. last hard reduce sizes without losing em/f score. implies bidaf compact enough scale suitable computation accuracy. however method still signiﬁcantly compress compact model acceptable performance loss. last instead ﬁne-tuning baseline train bidaf scratch learning. results summarized table approach also works well training scratch. overall training scratch balances sparsity across layers better ﬁne-tuning results even better compression model size speedup inference time. histogram vector lengths weight groups plotted appendix proposed intrinsic sparse structures within lstms learning method simultaneously reduce sizes input updates gates hidden states cell states outputs within sophisticated lstm structure. learning structurally sparse lstm obtained essentially regular lstm reduced hidden dimension. thus software hardware speciﬁc customization required storage saving computation acceleration. though proposed lstms easily extended vanilla rnns gated recurrent unit recurrent highway networks thank researchers engineers microsoft giving valuable feedback work acknowledgments freddie zhang jacob devlin chen zhou. also thank jeff rasley helping hyperdrive hyper-parameter exploration. work supported part ccf- opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect views contractors. dario amodei sundaram ananthanarayanan rishita anubhai jingliang eric battenberg carl case jared casper bryan catanzaro qiang cheng guoliang chen deep speech endinternational conference machine to-end speech recognition english mandarin. learning kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. arxiv. kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition baoyuan wang hassan foroosh marshall tappen marianna pensky. sparse convolutional neural networks. proceedings ieee conference computer vision pattern recognition zhiyun vikas sindhwani tara sainath. learning compact recurrent neural networks. acoustics speech signal processing ieee international conference ieee pavlo molchanov stephen tyree tero karras timo aila kautz. pruning convolutional neural networks resource efﬁcient inference. international conference learning representations jongsoo park sheng ping peter tang yiran chen pradeep dubey. faster cnns direct sparse convolutions guided pruning. international conference learning representations rohit prabhavalkar ouais alsharif antoine bruguier mcgraw. compression recurrent neural networks application lvcsr acoustic modeling embedded speech acoustics speech signal processing ieee international recognition. conference ieee minjoon aniruddha kembhavi farhadi hannaneh hajishirzi. bidirectional attention machine comprehension. international conference learning representations christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition chunpeng tariq afzal yongmei zhang yiran chen compact approaching googlenet-level accuracy classiﬁcation domain adaptation. proceedings ieee conference computer vision pattern recognition julian georg zilly rupesh kumar srivastava koutn´ık j¨urgen schmidhuber. recurrent highway networks. proceedings international conference machine learning figure intrinsic sparse structures unveiled regularization shows original weight matrices blue dots nonzero weights white ones refer zeros; bottom weight matrices format fig. white strips components whose weights zeros. better visualization original matrices evenly down-sampled take large stacked lstms zaremba language modeling example. network stacked lstm layers whose dimensions inputs states output layer vocabulary words. sizes weight groups lstm layers perplexities validation test respectively ﬁne-tune baseline lstms -norm regularization. training hyper-parameters baseline adopted except bigger dropout keep ratio weaker dropout used -norm also regularization avoid overﬁtting. strong dropout plus -norm regularization result underﬁtting. weight decay norm regularization sparsiﬁed network validation perplexity test perplexity respectively approximately baseline. sparsity weights ﬁrst lstm layer second lstm layer last output layer respectively. fig. plots learned sparse weight matrices. sparse matrices reveal interesting patterns lots all-zero columns rows positions highly correlated. patterns proﬁled bottom row. much surprise sparsifying individual weight independently converge sparse lstms many removed— components ﬁrst second lstm layer all-zeros. figure histogram vector lengths weight groups bidaf. iss-learned bidaf third table using approach lengths regularized closer zeros peak zero resulting high sparsity.", "year": 2017}