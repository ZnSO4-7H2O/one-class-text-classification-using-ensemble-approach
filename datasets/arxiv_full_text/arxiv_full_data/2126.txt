{"title": "Investigating practical linear temporal difference learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Off-policy reinforcement learning has many applications including: learning from demonstration, learning multiple goal seeking policies in parallel, and representing predictive knowledge. Recently there has been an proliferation of new policy-evaluation algorithms that fill a longstanding algorithmic void in reinforcement learning: combining robustness to off-policy sampling, function approximation, linear complexity, and temporal difference (TD) updates. This paper contains two main contributions. First, we derive two new hybrid TD policy-evaluation algorithms, which fill a gap in this collection of algorithms. Second, we perform an empirical comparison to elicit which of these new linear TD methods should be preferred in different situations, and make concrete suggestions about practical use.", "text": "oﬀ-policy reinforcement learning many applications including learning demonstration learning multiple goal seeking policies parallel representing predictive knowledge. recently proliferation policyevaluation algorithms longstanding algorithmic void reinforcement learning combining robustness oﬀpolicy sampling function approximation linear complexity temporal diﬀerence updates. paper contains main contributions. first derive hybrid policy-evaluation algorithms collection algorithms. second perform empirical comparison elicit linear methods preferred diﬀerent situations make concrete suggestions practical use. recently using temporal diﬀerence methods approximate value function oﬀ-policy samples potentially unstable without resorting quadratic computation storage even case linear approximations. oﬀ-policy learning involves learning estimate total future reward would expect observe agent followed target policy learning samples generated diﬀerent behavior policy. oﬀ-policy policy-evaluation problem combined policy improvement step used model many diﬀerent learning scenarios learning many policies parallel learning demonstrations learning batch data simply learning optimal policy following exploratory policy case q-learning paper focus exclusively oﬀ-policy policy evaluation problem commonly referred value function approximation simply prediction problem. past decade proliferation linear-complexity policy-evaluation methods designed convergent oﬀ-policy case. novel algorithmic contributions focused different ways achieving stable oﬀ-policy prediction learning. ﬁrst methods gradient family algorithms perform approximate stochastic gradient descent mean squared projected bellman error primary drawback methods requirement second learned weights second step size parameter potentially high variance updates importance sampling. empirically results mixed results indicating superior on-policy settings others concluding exact opposite later provisional introduced rectify issue bootstrap parameter used gradient methods correspond well parameter used conventional learning speciﬁcally gradient methods correspond known variant oﬀ-policy monte carlo. algorithm ﬁxes issue on-policy prediction exactly equivalent conventional algorithm. gradient corrections guaranteed converge tabular oﬀ-policy prediction setting. empirical performance relative gradient however completely unknown. recently sutton observed conventional correct update based notion followdistribution. distributional mis-match provides anway understand oﬀ-policy divergence conventional oﬀ-policy derive emphatic algorithm surprisingly achieves convergence withneed second weights like used gradient methods. like gradient methods however seems algorithm also suﬀers high variance importance sampling. hallak introduced variant utilizes scaling parameter meant reduce magnitude follow-on trace. comparative empirical studies however limited. recent contribution line work explores mirror-prox approach minimizing mspbe main beneﬁt work enabled ﬁrst ﬁnite sample analysis oﬀ-policy td-based method function approximation application advanced stochastic gradient optimizations. introduced mirror-prox algorithms based algorithm based showed paper investigates problem estimating discounted future rewards online function approximation. context reinforcement learning take online mean agent makes decisions environment produces outcomes agent updates parameters continual real-time interaction stream. model agent’s interaction markov decision process deﬁned countably inﬁnite states ﬁnite actions scalar discount function feature vector current state |s|. time step agent selects action according it’s behavior policy environment transitions state emits scalar reward agent’s objective evaluate ﬁxed target policy estimate expected return methods evaluated study perform temporal diﬀerence updates utilize eligibility traces. algorithm prototypical example concepts useful understanding algorithms discussed remainder paper. estimates linear function weight vector estimate formed inner product between weight vector features current state algorithm maintains memory trace allowing updates assign credit previously vispolicy data generated oﬀ-policy setting must estimate oﬀ-policy. samples generated selecting actions according setting cause algorithm diverge. algorithm solves divergence issue minimizing mspbe resulting stochastic gradient descent algorithm looks similar important difµ ferences. uses importance weights eligibility trace reweight data obtain unbiased estimate note policy iteration case— studied here—it still reasonable assume knowledge example less widely known approach oﬀ-policy prediction problem based algorithms perform precisely updates data sampled on-policy corrected gradient-td style updates data generated oﬀpolicy. idea exploit supposed superior eﬃciency on-policy learning maintaining robustness oﬀ-policy case. hybrid methods introduced state value-function based prediction state-action value-function based prediction extended utilize eligibility traces compared recent developments linear oﬀ-policy learning meanwhile separate related thread algorithmic development sought improve operation eligibility traces used onoﬀ-policy algorithms. direction based another nonequivalence observation update performed forward view variant conventional equivalent backward view update sampling trajectories. proposed trueonline prediction algorithm trueonline prediction algorithm remedy issue shown outperform conventional gradient methods respectively chain domains. to-td algorithm requires modest increase computational complexity however to-gtd algorithm signiﬁcantly complex implement requires three eligibility traces compared gtd. nevertheless to-td to-gtd achieve linear complexity implemented completely incremental way. although asymptotic convergence properties many methods rigorously characterized empirically still much understand large collection methods. frequent criticism gradient methods example hard tune well-understood empirically. somewhat disappointing perhaps famous application reinforcement learning—learning play atari games uses potentially divergent oﬀ-policy q-learning. addition little understanding methods compare terms learning speed robustness parameter sensitivity. clarifying empirical properties algorithms hope promote wide-spread adoption theoretically sound computationally eﬃcient algorithms. paper primary contributions. first introduce novel extension hybrid methods eligibility traces resulting algorithms trueonline htd. second provide empirical study td-based prediction learning linear function approximation. conclusions experiments surprisingly clear rule case yield stable convergence positive deﬁnite positive deﬁnite satisfy conditions ordinary diﬀerential equation proof convergence holds positive deﬁnite therefore positive deﬁnite full rank sutton nice discussion matrix must positive deﬁnite ensure stable nondivergent iterations. matrix equation replaced positive deﬁnite matrix ﬁxed point unaﬀected rate convergence almost surely change. space constraints describe td-based linear learning algorithms found literature investigated study. provide algorithm’s pseudo code appendix next section describe oﬀ-policy gradient methods turning empirical questions. conventional temporal diﬀerence updating data eﬃcient gradient temporal diﬀerence updating correction term used gradient-td methods helps prevent divergence. previous empirical studies demonstrated situations linear outperform gradient methods others demonstrated expected sarsa outperform multiple variants algorithm even oﬀ-policy sampling. hand diverge small though somewhat contrived counterexamples. idea hybrid-td methods achieve sample eﬃciency closer on-policy sampling ensuring non-divergence oﬀ-policy sampling. achieve this hybrid algorithm could conventional uncorrected updates data sampled on-policy gradient corrections data sampled oﬀ-policy. approach pioneered maei leading derivation hybrid temporal diﬀerence learning algorithm htd. later hackman produced hybrid version algorithm estimating state-action value functions rather state-value functions here. paper derive ﬁrst hybrid temporal diﬀerence method make eligibility traces called htd. idea behind derivation learning methods modify gradient mspbe produce learning algorithm. represent expectation according samples generated behavior policy mspbe written wt−. forward-view algorithm deﬁned computes online assuming access future samples exactly equivalent incremental backward-view algorithm derived require access future samples. framework used derive to-td algorithm on-policy setting to-gtd general oﬀ-policy setting. true-online equivalence interesting theoretically also translates improved prediction control performance used on-policy behaves similarly to-td. goal section combine beneﬁts hybrid learning true-online traces single algorithm. proceed similar derivation to-gtd main diﬀerence appearing update auxiliary weights. notice primary weights auxiliary weights similar structure. recall modiﬁed gradient mspbe expected primary-weight update written step-size parameters hybrid-td algorithm converge oﬀ-policy sampling using proof technique similar used leave future work. algorithm completely speciﬁed following equations recently forward-backward view equivalence proposed online methods resulting trueonline true-online algorithms. original forward-backward equivalence oﬄine derive forward-backward equivalence online updating truncated return proposed uses idea deﬁning forward view objective converting computationally impractical forward-view eﬃciently implementable algorithm using traces extensively treated sutton barto’s introductory text real-world domains binary representation viewed approximation poor relatively low-dimensional representations common real applications. feature encoding normalized. experiments conducted binary representation rest generate policies purposeful behavior forced agent favor single action state. target policy generated randomly selecting action assigning probability state assigning remaining actions remaining probability evenly. oﬀ-policy experiments behavior policy modiﬁed slightly diﬀerent target policy selecting base action instead assigning probability choice ensures policies related guarantees never greater thus avoiding inappropriately large variance importance sampling. experiment compared diﬀerent linear complexity value function learning algorithms including true-online true-online trueonline true-online linear oﬀ-policy trueonline later applicable onpolicy domains mirror-prox methods straightforward extensions gtd-mp tdc-mp methods handle traces drop designation method ﬁgure labels reduce clutter. results generated performing large parameter sweep averaged many independent runs random instance averaging results entire mdps. tested diﬀerent values step-size parameter seven values {j|j values intentionally precluded smaller values parameter sweep many gradient methods simply become on-policy variants approaches zero whereas oﬀ-policy domains values required avoid divergence believe range fairly reﬂects algorithms would used practice avoiding divergence priority. parameter toetd equal .γt. algorithm instance deﬁned combination evaluated using mean absolute value error time step graphs figures include learning curves selected minimize mean absolute value error three diﬀerent feature representations parameter sensitivity graphs mean absolute value error plotted parameter value remaining parameters to-htd algorithm equivalent to-htd equivalent to-td on-policy def= sampling. achieve later equivalence replace αtρtxt wt−)xt. opted ﬁrst equivalence reasons. preliminary experiments to-htd described equation already exhibited similar performance compared to-td designing second equivalence unnecessary. further to-gtd derived ensure equivalence to-gtd gtd; choice therefore better parallels equivalence. empirical study focused three main aspects early learning performance diﬀerent feature representations parameter sensitivity eﬃcacy oﬀ-policy learning. majority experiments conducted random mdps random contains states three actions state. state action agent transition four next states assigned randomly entire without replacement. transition probabilities instance randomly sampled transitions normalized one. expected reward transition also generated randomly reward transition sampled without noise. transitions randomly selected terminate problem instance held ﬁxed learning. experimented three diﬀerent feature representations. ﬁrst tabular representation state represented binary vector single corresponding current state index. encoding allows perfect representation value function generalization states. second representation computed taking tabular representation aliasing states feature vector agent cannot diﬀerentiate states. states selected randomly without replacement instance. third representation dense binary encoding feature vector state binary encoding state index thus feature vector state requires components. although binary representation appears exhibit inappropriate amount generalization believe realistic tabular representation access state rare achieved poor performance settings except baird’s counterexample. gtd-mp tdc-mp achieved best performance baird’s counterexample. hypothesize two-step gradient computation eﬀectively uses transition state ideally suited structure domain. however gtd-mp method performed worse oﬀ-policy oﬀ-policy random domains learning curves tdc-mp exhibited higher variance methods on-policy binary case high parameter sensitivity across settings except baird’s. seem consequence extension eligibility traces cases except baird’s tdc-mp gtd-mp performed best like mirror prox methods would likely performed better values however undesirable larger required ensure good performance policy domains baird’s behavior baird’s counterexample. methods exhibited reliable error reduce baird’s near zero suggesting eligibility traces limited extreme oﬀ-policy domains. case non-convergent behavior surprising since implementation algorithm include gradient correction—a possible extension suggested authors —and thus guaranteed converge oﬀ-policy sampling tabular case. emphatic methods performance baird’s remains concern especially considering well to-etd performed experiments. addition parameter appears signiﬁcantly improve to-etd oﬀ-policy domain binary features could mitigate large variance produced counterexample. clear behavior inherent emphatic methods could solved careful speciﬁcation state-based interest function. implementation followed original author’s recommendation setting interest state domains discounted continuing. additionally to-htd diverge baird’s performance less satisfactory least. overall conclusions implied empirical study surprisingly clear. guarding large variance oﬀ-policy sampling chief concern to-gtd preferred. preferred computation premium. poor performance problems like baird’s concern to-etd clearly best across experiments exhibited nearly best runtime results. to-etd hand exhibited high variance oﬀ-policy domains sharp parameter sensitivity indicating parameter turnng emphatic methods issue practice. baird’s counterexample uses speciﬁc initialization primary weights true solutions variance to-etd examined state domains thought higher variance algorithms emphasis weighting. selected minimize mean absolute value error. graphs included across feature representations oﬀ-policy learning. across results parameters selected optimize performance last half experiment ensure stable learning throughout run. analyze large variance importance sampling oﬀ-policy learning also investigated baird’s counterexample simple causes learning diverge. seven state uses target policy diﬀerent behavior policy feature representation allows perfect representation value function also causes inappropriate generalization. used variant problem described maei white present results root mean squared error addition performance results figures table summarizes runtime comparison algorithms. though algorithms linear storage computation diﬀer implementation runtime particularly true-online traces. appendix contains several plots runtime verses value error illustrating trade-oﬀ computation sample complexity algorithm. space constraints included aliased tabular representation results onpolicy learning appendix since similar tabular representation results on-policy learning. three broad conclusions suggested results. first could clearly demonstrate supposed superiority gradient methods on-policy setting. tabular aliased feature settings achieved faster learning superior parameter sensitivity compared htd. notably η-sensitivity algorithm reasonable domains however large required achieve good performance baird’s to-gtd. on-policy experiments binary features indicate slight advantage exhibit zero sensitivity choice expected. oﬀ-policy learning little diﬀerence htd. results combined prior work dann suggest advantage conventional gradient methods on-policy learning limited speciﬁc domains. counterexample mean absolute value error appropriate optimal values task zero. mspbe often used performance measure mspbe changes completeness include results mspbe appendix. figure oﬀ-policy performance random mdps three diﬀerent representations. plots report mean absolute value error averaged runs mdps. plots organized columns left right corresponding tabular aliased binary features. plots organized rows bottom corresponding learning curves figure on-policy performance random mdps diﬀerent representations oﬀ-policy performance baird’s counterexample. plots report mean absolute value error averaged runs random mdps runs baird’s. plots organized columns left right corresponding results random mdps tabular binary features results baird’s counterexample. plots also organized rows bottom corresponding learning curves sensitivity. mahmood sutton. oﬀ-policy learning based weighted importance sampling linear computational complexity. conference uncertainty artiﬁcial intelligence sutton modayil delp degris pilarski white precup. horde scalable real-time architecture learning knowledge unsupervised sensorimotor interaction. international conference autonomous agents multiagent systems original algorithm proposed sutton entirely obvious manipulation true online described used experiments. diﬀerence deﬁnition eligibility trace primary weight update. achieve original algorithm modify true-online algorithm section includes additional results analyzing relative performance linear td-based policy evaluation algorithms. runtime results follows figures figure graphs indicate sample eﬃciency versus time trade-oﬀ. increasing algorithms given time sample ﬁnish computing. computation done within allotted time agent continues ﬁnish computation essentially paused interaction. several possible iterations required algorithm slow done computing sample point given sample. simulates real-time decision making tasks mobile robot control. samples cannot processed buﬀered oﬀ-line computation previous sample process. however multiple samples processed typically computationally frugal learning methods perform well smaller samples processed iteration. example on-policy processes samples processes samples to-td processes samples to-htd processes samples. even though algorithm allowed process samples iteration achieve best performance trade-oﬀ to-td sample eﬃcient computationally eﬃcient. larger time available algorithm iteration. case algorithms better performance trade-oﬀs. example oﬀ-policy learning eﬀectively ties best performance. figure on-policy performance random mdps aliased tabular features. plots report mean absolute value error averaged runs mdps. left plot depicts learning curves best parameters found averaged random instances. remaining graphs depict algorithms parameter sensitivity mean absolute value error figure oﬀ-policy performance variant baird’s -state counterexample. plots report root mean square projected bellman error white’s thesis detailed explanation compute mspbe parameters mdp. left graph reports rmspbe averaged plotted time best parameter setting found extensive sweep. remaining plots depict parameter sensitivity rmspbe method respect algorithm parametersα figure runtime analysis on-policy random mdps tabular features. time iteration increased milliseconds obtain original learning curve graphs runtime restrictions algorithms point since fast enough much time second. line style colors correspond exactly labels main paper. detailed discussion ﬁgure appendix text. figure runtime analysis oﬀ-policy random mdps tabular features. time iteration increased milliseconds obtain original learning curve graphs runtime restrictions algorithms point since fast enough time allotted line style colors correspond exactly labels main paper. detailed discussion ﬁgure appendix text.", "year": 2016}