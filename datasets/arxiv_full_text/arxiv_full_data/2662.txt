{"title": "Semi-supervised Bayesian Deep Multi-modal Emotion Recognition", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In emotion recognition, it is difficult to recognize human's emotional states using just a single modality. Besides, the annotation of physiological emotional data is particularly expensive. These two aspects make the building of effective emotion recognition model challenging. In this paper, we first build a multi-view deep generative model to simulate the generative process of multi-modality emotional data. By imposing a mixture of Gaussians assumption on the posterior approximation of the latent variables, our model can learn the shared deep representation from multiple modalities. To solve the labeled-data-scarcity problem, we further extend our multi-view model to semi-supervised learning scenario by casting the semi-supervised classification problem as a specialized missing data imputation task. Our semi-supervised multi-view deep generative framework can leverage both labeled and unlabeled data from multiple modalities, where the weight factor for each modality can be learned automatically. Compared with previous emotion recognition methods, our method is more robust and flexible. The experiments conducted on two real multi-modal emotion datasets have demonstrated the superiority of our framework over a number of competitors.", "text": "emotion recognition difﬁcult recognize human’s emotional states using single modality. besides annotation physiological emotional data particularly expensive. aspects make building effective emotion recognition model challenging. paper ﬁrst build multi-view deep generative model simulate generative process multi-modality emotional data. imposing mixture gaussians assumption posterior approximation latent variables model learn shared deep representation multiple modalities. solve labeled-data-scarcity problem extend multi-view model semi-supervised learning scenario casting semi-supervised classiﬁcation problem specialized missing data imputation task. semi-supervised multi-view deep generative framework leverage labeled unlabeled data multiple modalities weight factor modality learned automatically. compared previous emotion recognition methods method robust ﬂexible. experiments conducted real multi-modal emotion datasets demonstrated superiority framework number competitors. development human-computer interaction emotion recognition become increasingly important. since human’s emotion contains many nonverbal cues various modalities ranging facial expressions voice electroencephalogram movements physiological signals used indicators emotional states real-world applications difﬁcult recognize emotional states using single modality signals different modalities represent different aspects emotion provide complementary information. recent studies show integrating multiple modalities signiﬁcantly boost emotion recognition accuracy example proposed learn joint density model emotion analysis multi-modal deep boltzmann machine multi-modal exploited model joint distribution visual proposed ditory textual features. multi-modal emotion recognition method using multimodal deep autoencoders joint representations movement signals extracted. nevertheless still limitations deep multi-modal emotion recognition methods e.g. performances depend amount labeled data. using modern sensor equipments easily collect massive physiological signals closely related people’s emotional states. despite convenience data acquisition data labeling procedure requires lots manual efforts. therefore cases small labeled samples available majority whole dataset left unlabeled. traditional emotion recognition approaches utilized limited amount labeled data result severe overﬁtting. attractive deal issue based semi-supervised learning builds robust model exploiting labeled unlabeled data though multi-modal approaches widely implemented emotion recognition explored simultaneously. best knowledge proposed enhanced multi-modal co-training algorithm semi-supervised emotion recognition shallow structure hard capture high-level correlation different modalities. amongst existing approaches competitive based deep generative models employs deep neural networks learn discriminative features casts semi-supervised classiﬁcation problem suitable likelihood function formed non-linear transformation latent variable non-linear transformation essential allow higher moments data captured density model choose non-linear functions dnns referred generative networks parameters {θ}v note that likelihoods different data views assumed independent other different nonlinear transformations. bayesian canonical correlation analysis model seen special case model linear shallow transformations used generate data view different views used similar deep nonconsidered. linear generative process construct deep bayesian model inference construct variational posterior approximation view ignore rest one. choice convenient inference computation seeks suboptimal solutions doesn’t fully exploit data. shown following assume variational approximation posterior latent variables mixture gaussians utilizing information multiple views. gaussian prior mixture gaussians posterior typically prior approximate posterior assumed gaussian distributions order maintain mathematical computational tractability. although assumption leaded favorable results several tasks clearly restrictive often unrealistic assumption. speciﬁcally choice gaussian distribution imposes strong uni-modal structure assumption latent space. however data distributions strongly multi-modal uni-modal gaussian assumption inhibits model’s ability extract represent important structure data. improve ﬂexibility model impose mixture gaussians assumption however risk creating separate islands discontinuous manifolds break meaningfulness representation latent space. learn powerful expressive models particular models multi-modal latent variable structures multi-modal emotion recognition applications seek mixture gaussians preserving standard gaussian. thus mean covariance nonlinear functions observation variational parameter generative model choose nonlinear functions dnns referred inference networks. non-negative normalized weight factor conditioning posterior approximation data point avoid shown deep generative models approximate bayesian inference exploiting recent advances scalable variational methods provide state-of-theart performance semi-supervised classiﬁcation. though variational autoencoder framework shown great advantages potential merits remain under-explored. example recently successful multi-view extension main difﬁculty lies inherent assumption posterior approximation conditioned data point natural single-view data becomes problematic multi-view case. paper propose novel semi-supervised multiview deep generative framework multi-modal emotion recognition. framework combines advantages deep multi-view representation learning bayesian modeling thus sufﬁcient ﬂexibility robustness learning joint features classiﬁer. main contributions summarized follows. propose multi-view extension imposing mixture gaussians assumption posterior approximation latent variables. multi-view learning critical fully exploiting information multiple views. introduce semi-supervised multi-modal emotion recognition framework based multi-view vae. framework leverage labeled unlabeled samples multiple modalities weight factor modality learned automatically critical building robust emotion recognition system. framework recently introduced robust model latent feature learning however single-view architecture can’t effectively deal multi-view data. section ﬁrst build multi-view learn shared deep representation multi-view data. then extend semi-supervised scenario. assume faced multi-view data appears pairs observation v-th view corresponding class label dnn-parameterized likelihoods assume latent variable generate multi-view features {x}v speciﬁcally assume generates following generative model variational parameters data point instead requiring global variational parameters. note that mixed gaussian assumption variational approximation distinguishes method existing ones using autoencoding variational framework multi-view learning critical fully exploiting information multiple views. semi-supervised classiﬁcation subset samples corresponding class labels focus using multi-view build model learns classiﬁer labeled unlabeled multi-view data. since emotional data continuous choose gaussian likelihoods. generative model deﬁned denotes categorical distribution treated latent variable unlabeled data points mean variance nonlinear functions parameter inference model deﬁned qϕqφ brevity omit explicit dependencies moment variables mentioned hereafter. principle implemented various models e.g. multiple layer perceptrons convolutional neural networks evaluated practice sufﬁces small estimate gradient using minibatches data points. random numbers estimators lower variance. gradient w.r.t. omitted here since derived straightforwardly using traditional reparameterization trick gradients loss semimvae computed direct application chain rule estimators presented above. optimization estimated gradients conjunction standard stochastic gradient based optimization methods rmsprop adam overall model trained reparameterization trick backpropagation mixed gaussian latent variables. labeled unlabeled dataset respectively. classiﬁcation accuracy improved introducing explicit classiﬁcation loss labeled data. extended objective function hyper-parameter weight generative discriminative learning. scaling constant numbers labeled unlabeled data points minibatch respectively. note that classiﬁer also used test phase prediction unseen data. provides uniﬁed objective function optimizing parameters encoder decoder classiﬁer networks. optimization done jointly without resort variational algorithm using stochastic backpropagation technique reparameterization trick reparameterization trick vital component algorithm allows easily take derivative respect variational parameters however mixture gaussians variational distribution makes application reparameterization trick challenging. shown that rewritten using location-scale transformation gaussian distribution dataset used experiments. seed dataset contains movement signals subjects watching movie clips movie clip lasts minutes long. signals recorded channels movement signals contained information blink saccade ﬁxation experiment used data subjects across sessions totally data ﬁles. data data watching movie clips used training data watching movie clips used validation rest used testing set. deap dataset contains peripheral physiological signals participants. signals recorded watching one-minute duration music videos. signals recorded channels whereas peripheral physiological signals recorded channels. participants using values rated music video terms levels valence arousal experiment valence-arousal space divided four quadrants according ratings. threshold used leading four classes data. considering fuzzy boundary emotions variations participants’ ratings possibly associated individual difference rating scale discarded samples whose ratings arousal valence dataset randomly divided -folds folds training fold validation last fold testing. size testing relative small graph-based semi-supervised baselines hard deal large dataset. feature selection seed dataset extracted differential entropy features movement features movement signals also used features experiments. deap dataset extracted features peripheral physiological signals. features calculated four frequency bands theta alpha beta gamma used band’s features. details data used experiments summarized table compared methods compared semimvae broad range solutions including supervised learning transductive inductive semi-supervised learning. brieﬂy summarize various baselines following. deep autoencoders. particular dccae consists autoencoders optimizes combination canonical correlation learned bottleneck representations reconstruction errors autoencoders semivae single-view semi-supervised deep generative model proposed evaluate semivae’s performance modality concatenation modalities respectively. dcca dccae used support vector machines transductive supervised learning transductive semi-supervised learning respectively. parameter setting semimvae considered multiple layer perceptrons type inference generative networks. datasets structures inference generative networks view ‘--’ respectively. used adam optimizer learning rate training. scaling constant selected throughout experiments. weight factor view initialized number views. dcca dccae considered setups semimvae. ammss tuned parameters suggested amgl semivae used default settings. simulate semi-supervised learning scenario datasets randomly labeled different proportions samples training remained rest samples training unlabeled. transductive semi-supervised learning trained models dataset consisting testing data labeled data belonging training set. inductive semi-supervised learning trained models entire training consisting labeled unlabeled data. supervised learning trained models labeled data belonging training test performance testing set. table presents classiﬁcation accuracies methods seed deap datasets. proportions labeled samples training vary several observations drawn follows. first average accuracy semimvae signiﬁcantly surpasses baselines cases. second examining semimvae supervised learning approaches trained limited labeled data semimvae always outperforms them. encouraging result shows models. figs. show changes semimvae’s average accuracy datasets different proportions labeled unlabeled samples training set. observe labeled unlabeled samples effectively boost classiﬁcation accuracy semimvae. instead treating modality equally semimvae weight modality perform classiﬁcation simultaneously. fig. shows learned weight factors inductive semimvae seed deap datasets observe modality highest weight datasets consistent single modality’s performance semivae shown table results previous work scaling constant controls weight discriminative learning semimvae. fig. shows performance inductive semimvae different values scaling constant chosen semimvae achieves good results. semimvae effectively leverage useful information unlabeled data. third multi-view semi-supervised algorithms ammss amgl perform worst cases. attribute fact graph-based shallow models ammss amgl can’t extract deep features original data. fourth performances three tsvm based semi-supervised methods moderate. although mae+tsvm dcca+tsvm dccae+tsvm also integrate multi-modality information unlabeled samples two-stage learning can’t obtain global optimal model parameters. finally compared single-view semi-supervised method semivae multi-view method effective integrating multiple modalities. paper proposes semi-supervised multi-view deep generative framework emotion recognition leverage labeled unlabeled data multiple modalities. framework parts multiview fully integrate information multiple modalities semi-supervised learning overcome labeled-data-scarcity problem. experimental results real multi-modal emotion datasets demonstrate effectiveness approach.", "year": 2017}