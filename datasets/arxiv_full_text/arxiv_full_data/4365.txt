{"title": "Human Attention in Visual Question Answering: Do Humans and Deep  Networks Look at the Same Regions?", "tag": ["stat.ML", "cs.CV"], "abstract": "We conduct large-scale studies on `human attention' in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Overall, our experiments show that current attention models in VQA do not seem to be looking at the same regions as humans.", "text": "conduct large-scale studies ‘human attention’ visual question answering understand humans choose look answer questions images. design test multiple game-inspired novel attentionannotation interfaces require subject sharpen regions blurred image answer question. thus introduce vqa-hat dataset. evaluate attention maps generated state-of-the-art models human attention qualitatively quantitatively experiments show current attention models seem looking regions humans. helps attention. humans ability quickly perceive scene selectively attending parts image instead processing whole scene entirety inspired human attention recent trend computer vision deep learning build computational models attention. given input signal models learn attend parts processing successfully applied machine translation object recognition image captioning work study attention task visual question answering unlike image captioning coarse understanding image often sufﬁcient producing generic descriptions visual questions selectively target different areas image including background details underlying context. suggests model beneﬁt explicit implicit attention mechanism answer question correctly. work interested following questions image regions humans choose look order answer questions images? deep models attention mechanisms attend regions humans? design conduct studies collect human attention maps. fig. shows human attention maps image different questions. asked ‘what type surface?’ humans choose look ﬂoor attention ‘which game played?’ concentrated around player racket. human attention maps used evaluating machine-generated attention maps explicitly training attention-based models. contributions. first design test multiple gameinspired novel interfaces collecting human attention maps humans choose look answer questions large-scale dataset vqa-hat dataset released publicly. second perform qualitative quantitative comparison maps generated state-of-theart attention-based models task-independent saliency baseline human attention maps visualizations rank-order correlation. machine-generated attention maps accurate model mean rank-correlation human attention maps worse task-independent saliency maps mean rank-correlation well understood task-independent saliency maps ‘center bias’ control center bias human attention maps correlation task-independent saliency poor trends machine-generated vqaattention maps remain work draws recent work attention-based human studies saliency prediction. work free-form open-ended dataset released models. attention-based models typically convolutional neural networks highlight relevant regions image given question. stacked attention networks proposed lstm encodings question words produce spatial attention distribution convolutional layer features image. hierarchical co-attention network generates multiple levels image attention based words phrases complete questions entry challenge time submission. another interesting approach uses question parsing compose neural network modules attention sub-tasks addressed modules note works unsupervised attention models attention simply intermediate variable produced model optimize downstream loss fact spatial distributions interpretable simply fortuitous. contrast study humans choose look answer visual questions. human attention maps used evaluate unsupervised maps. human studies. there’s rich history work collecting tracking data human subjects gain understanding image saliency visual perception tracking data study natural visual exploration useful difﬁcult expensive collect large scale. established mouse tracking accurate approach collecting attention maps. collected large-scale attention annotations coco amazon mechanical turk studies natural exploration collects task-independent human annotations asking subjects freely move mouse cursor anywhere wanted look blurred image approach task-driven. design test multiple game-inspired novel interfaces conducting large-scale human studies amt. basic interface design consists deblurring exercise answering visual questions. speciﬁcally present subjects blurred image question image subjects sharpen regions image help answer question correctly smooth clickand-drag ‘coloring’ motion mouse. successively scrubbing region progressively sharpens fig. shows intermediate steps attention annotation interface completely blurry image deblurred attention map. dataset evaluation. pilot studies experiment multiple interfaces question show question answer question answer well original high-resolution image along blurred image. order quantitatively evaluate interfaces conducted second human study subjects shown attentionsharpened images generated attention interfaces ﬁrst experiment asked answer question. intuition behind experiment attention revealed little information second subjects would answer question incorrectly. table shows accuracies answers given human subjects interfaces. blurred image answer interface gives highest accuracy evaluation humans. since payments structure encourage completing tasks quickly possible implicitly incentivizes subjects deblur regions possible human study shows humans still answer questions. thus overall achieve balance highlighting little much. total) questionimage pairs dataset. overall conducted approximately human intelligence tasks among unique workers. fig. shows examples collected human attention maps. vqa-hat dataset released publicly. collected human attention maps question unsupervised attention models learn predict attention maps similar human attention maps? rephrase neural networks look regions humans answer visual question? comparison metric rank correlation. ﬁrst scale machine-generated human attention maps rank pixels according spatial attention compute correlation ranked lists. choose order-based metric make evaluation invariant absolute spatial probability values made peaky diffuse tweaking ‘temperature’ parameter. table shows rank-order correlation averaged image-question pairs validation set. compare random attention maps task-independent saliency maps generated model trained predict human ﬁxation locations subjects asked freely view image seconds san- hiecoatt attention maps positively correlated human attention maps strongly task-independent judd saliency maps. ﬁndings lead take-away messages signiﬁcant potential impact future research active ﬁeld. first current attention models seem ‘looking’ regions humans produce answer. second attentionbased models become accurate seem better correlated humans terms look. dataset allow thorough validation observation future attention-based models proposed. numbers perspective computed interhuman agreement validation collecting human attention maps image-question pair computing mean rank-correlation center bias. judd saliency maps predict human ﬁxations natural visual exploration. tend strong center bias although human attention maps dataset tracking study center bias still exists albeit severe. potential source center bias fact dataset human-generated subjects looking images. thus salient objects center image likely potential subjects questions. compute rank-correlation synthetically generated central attention judd saliency human attention maps. judd saliency maps mean rank-correlation human attention maps mean rank-correlation validation set. eliminate effect center bias evaluation removed human attention maps positive rankcorrelation center attention map. compute rank-correlation machine-generated attention human attention reduced set. table mean correlation goes signiﬁcantly judd saliency maps since strong center bias. relative trends among hiecoatt similar whole validation hiecoatt-q higher correlation human attention maps judd saliency. demonstrates discounting center bias vqaspeciﬁc machine attention maps correlate better vqaspeciﬁc human attention maps task independent machine saliency maps. introduce release vqa-hat dataset. dataset used evaluate attention maps generated unsupervised manner attention-based models explicitly train models attention supervision vqa. quantify whether current attention-based models ‘looking’ regions image humans produce answer. necessary sufﬁcient maps. human attention maps ‘necessary’ and/or ‘sufﬁcient’? regions highlighted human attention maps sufﬁcient answer question accurately region superset. example attention mass concentrated ‘cat’ ‘what animal present picture?’ attention assigns weights arbitrary-sized region includes ‘cat’ sufﬁcient well. contrary necessary sufﬁcient attention would smallest visual region sufﬁcient answering question accurately. ill-posed problem deﬁne necessary attention space pixels; random pixels blacked chances humans would still able answer question given resulting subset attention map. work thus poses interesting question future work right semantic space meaningful talk necessary sufﬁcient attention maps humans? acknowledgements. thank jiasen ramakrishna vedantam helpful suggestions discussions. work supported part following national science foundation career awards army research ofﬁce awards ictas junior faculty awards army research grant wnf--- ofﬁce naval research grant n--- paul allen family foundation allen distinguished investigator award google faculty research award education research grant nvidia donation references andreas jacob rohrbach marcus darrell trevor klein dan. learning compose neural networks question answering. corr abs/. http// arxiv.org/abs/.. kyunghyun courville aaron bengio yoshua. describing multimedia content using attention-based encoderdecoder networks. corr abs/. http //arxiv.org/abs/.. devlin jacob gupta saurabh girshick ross mitchell margaret zitnick lawrence. exploring nearest neighbor approaches image captioning. arxiv preprint firat orhan kyunghyun bengio yoshua. multi-way multilingual neural machine translation shared attention mechanism. corr abs/. http// arxiv.org/abs/.. tsung-yi maire michael belongie serge hays james perona pietro ramanan deva dollr piotr zitnick lawrence. microsoft coco common objects context kelvin jimmy kiros ryan kyunghyun courville aaron salakhutdinov ruslan zemel richard bengio yoshua. show attend tell neural image caption generation visual attention. corr abs/. http//arxiv.org/abs/..", "year": 2016}