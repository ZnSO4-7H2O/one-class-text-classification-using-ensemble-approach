{"title": "Towards Bayesian Deep Learning: A Survey", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.LG", "cs.NE"], "abstract": "While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, the subsequent tasks that involve inference, reasoning and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a general introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this survey, we also discuss the relationship and differences between Bayesian deep learning and other related topics like Bayesian treatment of neural networks.", "text": "abstract—while perception tasks visual object recognition text understanding play important role human intelligence subsequent tasks involve inference reasoning planning require even higher level intelligence. past years seen major advances many perception tasks using deep learning models. higher-level inference however probabilistic graphical models bayesian nature still powerful ﬂexible. achieve integrated intelligence involves perception inference naturally desirable tightly integrate deep learning bayesian models within principled probabilistic framework call bayesian deep learning. uniﬁed framework perception text images using deep learning boost performance higher-level inference return feedback inference process able enhance perception text images. survey provides general introduction bayesian deep learning reviews recent applications recommender systems topic models control. survey also discuss relationship differences bayesian deep learning related topics like bayesian treatment neural networks. many perception tasks including seeing hearing recognition) undoubtedly fundamental tasks functioning comprehensive artiﬁcial intelligence system. however order build real system simply able read hear enough. should possess ability thinking. take medical diagnosis example. besides seeing visible symptoms hearing descriptions patients doctor look relations among symptoms preferably infer etiology them. doctor provide medical advice patients. example although abilities seeing hearing allow doctor acquire information patients thinking part deﬁnes doctor. speciﬁcally ability thinking could involve causal inference logic deduction dealing uncertainty apparently beyond capability conventional deep learning methods. fortunately another type models probabilistic graphical models excels causal inference dealing uncertainty. problem good deep learning models perception tasks. address problem therefore natural choice tightly integrate deep learning within principled probabilistic framework call bayesian deep learning paper. tight principled integration bayesian deep learning perception task inference task regarded whole beneﬁt other. example above able medical image could help doctor’s diagnosis inference. hand diagnosis inference return help understanding medical image. suppose doctor achieve high accuracy need fully recommender items analyze proﬁle preference users evaluate similarity among users. deep learning good ﬁrst subtask excels two. besides fact better understanding item content would help analysis user proﬁles estimated similarity among users could provide valuable information understanding item content return. order fully utilize bidirectional effect boost recommendation accuracy might wish unify deep learning single principled probabilistic framework done besides recommender systems need bayesian deep learning also arise dealing control non-linear dynamical systems images input. consider controlling complex dynamical system according live video stream received camera. problem transformed iteratively performing tasks perception images control based dynamic models. perception task taken care using multiple layers simple nonlinear transformation control task usually needs sophisticated models like hidden markov models kalman ﬁlters feedback loop completed fact actions chosen control model affect received video stream return. enable effective iterative process perception task control task need two-way information task-speciﬁc component. ideally ﬁrst-order second-order information able back forth components. natural represent perception component seamlessly connect task-speciﬁc done survey give comprehensive overview models recommender systems topic models control. rest survey organized follows section provide review basic deep learning models. section covers main concepts techniques pgm. sections serve background next section section would survey models applied areas like recommender systems control. section discusses future research issues concludes paper. deep learning normally refers neural networks layers. better understand deep learning start simplest type neural networks multilayer perceptrons example show conventional deep learning works. that review several types deep learning models based mlp. essentially multilayer perceptron sequence parametric nonlinear transformations. suppose want train multilayer perceptron perform regression task maps vector dimensions vector dimensions. denote input matrix j-th denoted m-dimensional vector representing data point. target denoted similarly denotes d-dimensional vector. problem learning l-layer multilayer perceptron formulated following optimization problem element-wise sigmoid function +exp purpose imposing matrix allow nonlinear transformation. normally transformations like tanh used alternatives sigmoid function. hidden units. easily computed given. since given data need learn here. usually done using backpropagation stochastic gradient descent compute gradients objective function respect denote value exchange them. perception component would basis control component estimates states control component dynamic model built would able predict future trajectory cases bayesian deep learning suitable choice apart major advantage provides principled unifying deep learning another beneﬁt comes implicit regularization built bdl. imposing prior hidden units parameters deﬁning neural network model parameters specifying causal inference degree avoid overﬁtting especially sufﬁcient data. usually model consists components perception component bayesian formulation certain type neural networks task-speciﬁc component describes relationship among different hidden observed variables using pgm. regularization crucial both. neural networks usually large numbers free parameters need regularized properly. regularization techniques like weight decay dropout shown effective improving performance neural networks bayesian interpretations terms task-speciﬁc component expert knowledge prior information kind regularization incorporated model prior imposed guide model data scarce. another advantage using complex tasks provides principled bayesian approach handling parameter uncertainty. applied complex tasks three kinds parameter uncertainty need taken account uncertainty neural network parameters. uncertainty task-speciﬁc parameters. uncertainty exchanging information perception component task-speciﬁc component. using distributions instead point estimates offers promising framework handle three kinds uncertainty uniﬁed way. worth noting third uncertainty could handled uniﬁed framework like bdl. train perception component task-speciﬁc component separately equivalent assuming uncertainty exchanging information components. course challenges applying real-world tasks. first nontrivial design efﬁcient bayesian formulation neural networks reasonable time complexity. line work pioneered widely adopted lack scalability. fortunately recent advances direction seem shed light practical adoption bayesian neural network. second challenge ensure efﬁcient effective information exchange perception component refer bayesian treatment neural networks bayesian neural network. term bayesian deep learning retained refer complex bayesian models perception component task-speciﬁc component. regularization parameter denotes frobenius norm. sdae regarded multilayer perceptron regression tasks described previous section. input corrupted version data target clean version data example data matrix randomly entries nutshell sdae learns neural network takes noisy data input recovers clean data last layer. ‘denoising’ name means. normally output middle layer i.e. figure would used compactly represent data. convolutional neural networks convolutional neural networks viewed another variant mlp. different initially designed perform dimensionality reduction biologically inspired. according types cells identiﬁed cat’s visual cortex. simple cells respond maximally speciﬁc patterns within receptive ﬁeld complex cells larger receptive ﬁeld considered locally invariant positions patterns. inspired ﬁndings concepts developed convolution max-pooling. convolution feature result convolution input linear ﬁlter followed element-wise nonlinear transformation. input image feature previous layer. speciﬁcally input weights bias k-th feature obtained follows tanhij bk). note equation assume single input feature multiple output feature maps. practice often multiple input feature maps well deep structure. convolutional layer input feature maps output feature maps shown figure max-pooling usually convolutional layer followed max-pooling layer seen type nonlinear downsampling. operation max-pooling simple. example feature size result max-pooling region would downsampled feature size regularization terms omitted. denotes element-wise product mean matlab operation matrices. practice small part data compute gradients update. called stochastic gradient descent. autoencoder feedforward neural network encode input compact representation reconstruct input learned representation. simplest form autoencoder multilayer perceptron bottleneck layer middle. idea autoencoders around decades abundant variants autoencoders proposed enhance representation learning including sparse contrastive denoising details please refer nice recent book deep learning introduce kind multilayer denoising known stacked denoising autoencoders example variants background applications bdl-based recommender systems section sdae feedforward neural network learning representations input data learning predict clean input output shown figure hidden layer middle i.e. ﬁgure constrained bottleneck learn compact representations. difference traditional sdae input layer corrupted version clean input data. essentially sdae solves denote weight matrices input-to-hidden hidden-to-hidden hidden-to-output connections respectively corresponding biases. task classify input data time step compute classiﬁcation probability softmax similar feedforward networks train algorithm called back-propagation time used. essentially gradients computed unrolled network shown figure shared weights biases time steps. gated recurrent neural network problem vanilla introduced gradients propagated many time steps prone vanish explode makes optimization notoriously difﬁcult. addition signal passing decays exponentially making impossible model long-term dependencies long sequences. imagine want predict last word paragraph many books like reading’. order answer need ‘long-term memory’ retrieve information start text. address problem long short-term memory model designed type gated model accumulate information relatively long duration. intuition behind lstm processing sequence consisting several subsequences sometimes useful neural network summarize forget states moving process next subsequence using index words sequence formulation lstm follows word embedding t-th word -by-s word embedding matrix -of-s representation stands element-wise product operation vectors denotes sigmoid function cell state t-th word denote biases input weights recurrent weights respectively. forget gate units input gate units equation computed using corresponding weights biases fig. left conventional feedforward neural network hidden layer input hidden layer output corresponding weights right recurrent neural network input {xt}t hidden states {ht}t entry downsampled feature maximum value corresponding region feature map. max-pooling layers reduce computational cost ignoring non-maximal entries also provide local translation invariance. putting together usually form complete working input would alternate convolutional layers max-pooling layers going tasks like classiﬁcation regression. famous example lenet- alternates convolutional layers max-pooling layers going fully connected target tasks. read article would normally take word time understand current word based previous words. recurrent process needs short-term memory. unfortunately conventional feedforward neural networks like shown figure fail example imagine want constantly predict next word read article. since feedforward network computes output function denotes element-wise nonlinear transformation unclear network could naturally model sequence words predict next word. solve problem need recurrent neural network instead feedforward one. shown figure computation current hidden states depends current input previous hidden states ht−. loop rnn. loop enables short-term memory rnns. represents network knows t-th time step. computation clearly unroll loop represent fig. encoder-decoder architecture involving lstms. encoder lstm encodes sequence ‘abc’ representation decoder lstm recovers sequence representation. marks sentence. similar output state cell state last time step ﬁrst lstm initial output state cell state second lstm. lstms concatenated form encoder-decoder architecture shown figure note vast literature deep learning neural networks. introduction section intends serve background bayesian deep learning. readers referred comprehensive survey details. probabilistic graphical models probabilistic graphical models diagrammatic representations relationships among them. similar graph contains nodes links nodes represent random variables links express probabilistic relationships among them. classic example would latent dirichlet allocation used topic model analyze generation words topics documents. usually comes graphical representation model generative process depict story random variables generated step step. figure shows graphical model corresponding generative process follows generative process gives story random variables generated. graphical model figure shaded node denotes observed variables others latent variables parameters model deﬁned learning algorithms applied automatically learn latent variables parameters. bayesian nature like easy extend incorporate information perform tasks. example different variants topic models based proposed. proposed incorporate temporal information extends assuming correlations among topics. extends batch mode online setting making possible process large datasets. recommender systems extends incorporate rating information make recommendations. model extended incorporate social information inference learning strictly speaking process ﬁnding parameters called learning process ﬁnding latent variables given parameters called inference. however given observed variables learning inference often intertwined. usually learning inference would alternate updates latent variables updates parameters learning inference completed would parameters document comes learned perform inference alone topic proportions document. like various learning inference algorithms available pgm. among them cost-effective probably maximum posteriori amounts maximizing posterior probability latent variable. using learning process equivalent minimizing objective function regularization. famous example probabilistic matrix factorization learning variance hyper-variance hyper-variance learnable variance learnable variance hyper-variance hyper-variance zero-variance zero-variance learnable variance efﬁcient gives point estimates latent variables order take uncertainty account harness full power bayesian models would resort bayesian treatments like variational inference markov chain monte carlo example original uses variational inference approximate true posterior factorized variational distributions learning latent variables parameters boils minimizing kl-divergence variational distributions true posterior distributions. besides variational choice bayesian treatment mcmc. example mcmc algorithms like proposed learn posterior distributions lda. bayesian deep learning background deep learning ready introduce general framework concrete examples bdl. speciﬁcally section list recent models applications recommender systems topic models control. summary models shown table general framework mentioned section principled probabilistic framework seamlessly integrated components perception component task-speciﬁc component. figure shows simple model example. part inside rectangle left represents perception component part inside blue rectangle right task-speciﬁc component. typically perception component would probabilistic formulation deep learning model multiple nonlinear processing layers represented chain structure pgm. nodes edges perception component relatively simple task-speciﬁc component often describe complex distributions relationships among variables three sets variables three sets variables model perception variables hinge variables task variables. paper denote perception variables variables perception component. usually would include weights neurons probabilistic formulation deep learning model. used denote hinge variables variables directly interact perception component task-speciﬁc component. table shows hinge variables listed models. task variables i.e. variables task-speciﬁc component without direct relation perception component denoted i.i.d. requirement note hinge variables always task-speciﬁc component. normally connections hinge variables perception component i.i.d. convenience parallel computation perception component. example related corresponding although mandatory models meeting requirement would signiﬁcantly increase efﬁciency parallel computation model training. variance related mentioned section motivations model uncertainty exchanging information perception component task-speciﬁc component boils modeling uncertainty related example kind uncertainty reﬂected variance conditional density equation according degree ﬂexibility three types variance shown above terms model ﬂexibility normally models properly regularized model would outperform model superior model. table show types variance different models. note although model table speciﬁc type always adjust models devise counterparts types. example table model easily adjust devise counterparts. compare performance ﬁnds former performs signiﬁcantly better meaning sophisticatedly modeling uncertainty components essential performance. criterion implies conventional variational inference mcmc methods applicable. usually online version needed sgd-based methods work either unless inference needed typically large number free parameters perception component. means methods based laplace approximation realistic since involve computation hessian matrix scales quadratically number free parameters. bayesian deep learning recommender systems despite successful applications deep learning natural language processing computer vision attempts made develop deep learning models uses restricted boltzmann machines instead conventional matrix factorization formulation perform extends work incorporating user-user item-item correlations. although methods involve deep learning actually belong cf-based methods information like crucial accurate recommendation. uses low-rank matrix factorization last weight layer deep network signiﬁcantly reduce number model parameters speed training classiﬁcation instead recommendation tasks. music recommendation directly conventional deep belief networks assist representation learning content information deep learning components models deterministic without modeling noise hence less robust. models achieve performance boost mainly loosely coupled methods without exploiting interaction content information ratings. besides linked directly rating matrix means models perform poorly serious overﬁtting ratings sparse. collaborative deep learning address challenges above hierarchical bayesian model called collaborative deep learning novel tightly coupled method introduced based bayesian formulation sdae tightly couples deep representation learning content information collaborative ﬁltering rating matrix allowing two-way interaction two. experiments show signiﬁcantly outperforms state art. following text start introduction notation used presentation cdl. review design learning cdl. notation problem formulation similar work recommendation task considered takes implicit feedback training test data. entire collection items represented j-by-b matrix bag-of-words vector xcj∗ item based vocabulary size users deﬁne i-by-j binary rating matrix i×j. example dataset citeulike-a user article personal library otherwise. given part ratings content information problem predict ratings note although current focuses movie recommendation article recommendation like section general enough handle recommendation tasks matrix plays role clean input sdae noise-corrupted matrix also j-by-b matrix denoted output layer sdae denoted j-by-kl matrix. similar denoted xlj∗. weight matrix bias vector respectively layer wl∗n denotes column number layers. convenience denote collection layers weight matrices biases. note l/-layer sdae corresponds l-layer network. fig. left graphical model cdl. part inside dashed rectangle represents sdae. example sdae shown. right graphical model degenerated cdl. part inside dashed rectangle represents encoder sdae. example sdae shown right. note although still decoder sdae vanishes. prevent clutter omit variables except graphical models. hyperparameters conﬁdence parameter similar note middle layer serves bridge ratings content information. middle layer along latent offset enables simultaneously learn effective feature representation capture similarity relationship items similar generalized sdae computational efﬁciency also take inﬁnity. learning based model above parameters could treated random variables fully bayesian methods markov chain monte carlo variational approximation methods applied. however treatment typically incurs high computational cost. consequently uses em-style algorithm obtaining estimates note goes inﬁnity gaussian distribution equation become dirac delta distribution centered sigmoid function. model degenerate bayesian formulation sdae. call generalized sdae. note ﬁrst layers network encoder last layers decoder. maximization posterior probability equivalent minimization reconstruction error weight decay taken consideration. note generation clean input part generative process bayesian sdae generation noise-corrupted input artiﬁcial noise injection process help sdae learn robust feature representation. encoder function takes corrupted content vector item input computes encoding item function also takes input computes encoding reconstructs content vector item example number layers output third layer output sixth layer. perspective optimization third term objective function equivalent multi-layer perceptron using latent item vectors target fourth term equivalent sdae minimizing reconstruction error. seeing view neural networks approaches positive inﬁnity training probabilistic graphical model figure would degenerate simultaneously training neural networks overlaid together common input layer different output layers shown figure note second network much complex typical neural networks involvement rating matrix. ratio λn/λv approaches positive inﬁnity degenerate two-step model latent representation learned using sdae directly ctr. another extreme happens λn/λv goes zero decoder sdae essentially vanishes. right figure graphical model degenerated λn/λv goes zero. demonstrated experiments predictive performance suffer greatly extreme cases prediction observed test data. similar uses point estimates calculate predicted rating t|d] denotes expectation operation. words approximate predicted rating algorithm turns bayesian generalized version well-known back-propagation learning algorithm. list conditional densities follows solver expectation equation provided note linear one-layer case generalized nonlinear multi-layer case using techniques marginalized perception variables hinge variables task variables collaborative deep ranking assumes collaborative ﬁltering setting model ratings directly. however output recommender systems often ranked list means would natural ranking rather ratings objective. motivation collaborative deep ranking proposed jointly perform representation learning collaborative ranking. corresponding generative process follows interestingly goes inﬁnity adaptive rejection metropolis sampling used sampling turns bayesian generalized version speciﬁcally figure shows getting gradient loss function point next sample would drawn region line equivalent probabilistic version sample curve loss function tangent line would added better approximate distribution corresponding loss function. that samples would drawn region lines. sampling besides searching local optima using gradients algorithm also takes variance consideration. called bayesian generalized back-propagation. marginalized collaborative deep learning sdae corrupted input goes encoding decoding recover clean input. usually different epochs training different corrupted versions input. hence generally sdae needs enough epochs training sufﬁcient corrupted versions input. marginalized sdae seeks avoid marginalizing corrupted input obtaining variants simultaneously extract effective deep feature representation content capture similarity implicit relationship items learned representation also used tasks recommendation. unlike previous deep learning models simple target like classiﬁcation reconstruction cdl-based models complex target probabilistic framework. mentioned section information exchange components crucial performance bdl. cdl-based models above exchange achieved assuming gaussian distributions connect hinge variables variables perception component generative process perception variable) simple effective efﬁcient computation. among cdl-based models table three models others models according deﬁnition section since veriﬁed signiﬁcantly outperforms counterpart expect extra performance boost counterparts three models. besides efﬁcient information exchange designs models also meet i.i.d. requirement distribution concerning hinge variables discussed hence easily parallelizable. models introduced later alternative designs enable efﬁcient i.i.d. information exchange components bdl. relational stacked denoising autoencoders topic models problem statement notation assume denoting items content item besides denote k-dimensional identity matrix denote relational latent matrix representing relational properties item j-by-b matrix represents clean input sdae noise-corrupted matrix size denoted besides denote output layer sdae j-by-kl matrix denoted xlj∗ weight matrix bias vector layer wl∗n denotes column number layers. shorthand refer collection weight matrices biases layers note l/-layer sdae corresponds l-layer network. similar algorithms used learn parameters cdr. reported using ranking objective leads signiﬁcant improvement recommendation performance. following deﬁnition section cdr’s perception variables {{wl}{bl}{xl} hinge variables task variables symmetric collaborative deep learning models like focus deep learning component modeling item content. besides content information items attributes users sometimes contain much important information. therefore desirable extend model user attributes well call variant symmetric cdl. example using extra msdae user attributes gives following joint log-likelihood collection different corrupted versions yci∗ k-time repeated version yci∗ transformation matrix user latent factors number user attributes. similar marginalized solution given parameters laplacian matrix incorporating relational information. diagonal matrix adjacency matrix representing relational information binary entries indicating links items. indicates link item item otherwise. denotes product gaussian gaussian also gaussian xlj∗ note ﬁrst term corresponds matrix variate distribution equation besides simple k∗lask∗ manipulation denotes k-th maximizing equivalent making closer item item linked {{xl} xc{wl}{bl}} hinge variables task variables {a}. representation learning relational information available. model simultaneously learn feature representation content information relation items. graphical model rsdae shown figure generative process listed follows learned representation vector item denotes relational latent matrix column relational latent vector item note equation matrix variate normal distribution deﬁned number layers corresponds equation xpnk count word comes topic document perception variables {{h}{wl}{bl}} hinge variables task variables {{φk}{rk} weight matrix containing columns bias vector containing entries learning using bayesian conditional density filtering efﬁcient learning algorithms needed bayesian treatments deep pfa. proposed online version mcmc called bayesian conditional density ﬁltering learn global parameters local variables conditional densities used gibbs updates follows xpnk|− multi φk|− θkn|− gamma kn|− δber learning using stochastic gradient thermostats alternative learning deep stochastic gradient n´ose-hoover thermostats accurate scalable. sgnht generalization stochastic gradient langevin dynamics stochastic gradient hamiltonian monte carlo compared previous introduces momentum variables system helping system jump local optima. speciﬁcally following stochastic differential equations used given learn layer using back-propagation algorithm. alternating update local optimum found. also techniques including momentum term help avoid trapped local optimum. poisson distribution support nonnegative integers known natural choice model counts. therefore desirable building block topic models motivation proposed model dubbed poisson factor analysis latent nonnegative matrix factorization poisson distributions. poisson factor analysis assumes discrete -by-n matrix containing word counts documents vocabulary size nutshell described using following equation denotes factor loading matrix factor analysis k-th column encoding importance word topic k-by-n matrix factor score matrix n-th column containing topic proportions document k-by-n matrix latent binary matrix n-th column deﬁning topics associated document different priors correspond different models. example dirichlet priors all-one matrix would recover beta-bernoulli prior leads nb-ftm model deep-structured prior based sigmoid belief networks imposed form deep model topic modeling. log-posterior model. indexes time denotes standard wiener process. thermostats variable make sure system constant temperature. injected variance constant. speed convergence generalized deep poisson factor analysis restricted boltzmann machine similar deep above restricted boltzmann machine used place used equation would deﬁned using energy learning similar algorithms deep used. speciﬁcally sampling process would alternate {{φk}{γk} {{w}{c}}. former similar conditional density sbn-based contrastive dpfa used. latter divergence algorithm. discussion perception component bdl-based topic models responsible inferring topic hierarchy documents task-speciﬁc component charge modeling word generation topic generation word-topic relation inter-document relation. synergy components comes bidirectional interaction them. hand knowledge topic hierarchy would facilitate accurate modeling words topics providing valuable information learning inter-document relation. hand accurately modeling words topics inter-document relation could help discovery topic hierarchy learning compact document representations. information exchange mechanism bdl-based topic models different section example sbn-based dpfa model exchange natural since bottom layer relationship inherently probabilistic shown equation means additional assumptions distribution necessary. sbn-based dpfa model equivalent assuming generated dirac delta distribution centered bottom layer hence dpfa models table models according deﬁnition section worth noting rsdae model hinge variable others perception variables) bayesian deep learning control mentioned section bayesian deep learning also applied control nonlinear dynamical systems images. consider controlling complex dynamical system according live video stream received camera. solving control problem iteration tasks perception images control based dynamic models. perception task taken care using multiple layers simple nonlinear transformation control task usually needs sophisticated models like hidden markov models kalman ﬁlters enable effective iterative process perception task control task need two-way information exchange them. perception component would basis control component estimates states hand control component dynamic model built would able predict future trajectory reversing perception process latent states. applied control time denotes system noise. equivalently equation written σξ). hence need mapping function corresponding image latent space corresponding system noise. similarly equation rewritten σω). function given ﬁnding optimal control trajectory length dynamical system amounts minimizing following cost bdl-based control minimize function equation need three components encoding model encode transition model infer given reconstruction model reconstruct inferred zt+. encoding model encoding model mean rnz×nz encodes diagonal covariance diag posterior distribution reconstructs images latent states equation parameters bernoulli distribution wphdec hdec output third network called decoding network reconstruction network. putting together equation show generative process full model. discussion mentioned section bdl-based control models consist components perception component live video control component infer states dynamical system. inference system based mapped states conﬁdence mapping perception component return control signals sent control component would affect live video received perception component. components work interactively within uniﬁed probabilistic framework model reach full potential achieve best control performance. information exchange components bdl-based control model discussed uses different mechanism section section uses neural networks separately parameterize mean covariance hinge variables perception variables parameterized equation ﬂexible models like section gaussian distributions ﬁxed variance also used. note bdl-based control model model shown table since covariance assumed diagonal model still meets i.i.d. requirement section conclusions future research survey identiﬁed current trend merging probabilistic graphical models neural networks reviewed recent work bayesian deep learning strives combine merits organically integrating single principled probabilistic framework. learn parameters several algorithms proposed ranging block coordinate descent bayesian conditional density ﬁltering stochastic gradient thermostats stochastic gradient variational bayes. bayesian deep learning gains popularity success recent promising advances deep learning. since many real-world tasks involve perception inference natural choice harness perception ability inference ability pgm. although current applications focus recommender systems topic models stochastic optimal control future expect increasing number applications like link prediction community detection active learning bayesian reinforcement learning many complex tasks need interaction perception causal inference. besides advances efﬁcient bayesian neural networks important component expected scalable. kyunghyun bart merrienboer ccaglar ¨ulccehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using emnlp encoder-decoder statistical machine translation. pages geoffrey hinton. training products experts minimizing contrastive divergence. neural computation geoffrey hinton drew camp. keeping neural networks simple minimizing description length weights. colt pages takamitsu matsubara vicencc g´omez hilbert kappen. latent kullback leibler control continuous-state systems using probabilistic graphical models. pages porteous david newman alexander ihler arthur asuncion padhraic smyth welling. fast collapsed gibbs sampling latent dirichlet allocation. pages salah rifai pascal vincent xavier muller xavier glorot yoshua bengio. contractive auto-encoders explicit invariance feature extraction. icml pages tara sainath brian kingsbury vikas sindhwani ebru arisoy bhuvana ramabhadran. low-rank matrix factorization deep neural network training high-dimensional output targets. icassp pages pascal vincent hugo larochelle isabelle lajoie yoshua bengio pierre-antoine manzagol. stacked denoising autoencoders learning useful representations deep network local denoising criterion. jmlr manuel watter jost springenberg joschka boedecker martin riedmiller. embed control locally linear latent dynamics nips pages model control images.", "year": 2016}