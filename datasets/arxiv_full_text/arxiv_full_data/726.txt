{"title": "Bitwise Neural Networks", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Based on the assumption that there exists a neural network that efficiently represents a set of Boolean functions between all binary inputs and outputs, we propose a process for developing and deploying neural networks whose weight parameters, bias terms, input, and intermediate hidden layer output signals, are all binary-valued, and require only basic bit logic for the feedforward pass. The proposed Bitwise Neural Network (BNN) is especially suitable for resource-constrained environments, since it replaces either floating or fixed-point arithmetic with significantly more efficient bitwise operations. Hence, the BNN requires for less spatial complexity, less memory bandwidth, and less power consumption in hardware. In order to design such networks, we propose to add a few training schemes, such as weight compression and noisy backpropagation, which result in a bitwise network that performs almost as well as its corresponding real-valued network. We test the proposed network on the MNIST dataset, represented using binary features, and show that BNNs result in competitive performance while offering dramatic computational savings.", "text": "although dnns extending state results various tasks image classiﬁcation speech recognition speech enhancement also case relatively bigger networks parameters call resources sometimes critically constrained applications running embedded devices. examples applications span context-aware computing collecting analysing variety sensor signals device always-on computer vision applications speechdriven personal assistant services siri. primary concern hinders applications being successful assume always-on pattern recognition engine device drain battery fast unless carefully implemented minimize resources. additionally even environment necessary resources available speeding greatly improve user experience comes tasks like searching databases either case compact still well-performing welcome improvement. efﬁcient computational structures deploying artiﬁcial neural networks long studied literature. effort focused training networks whose weights transformed quantized representations minimal loss performance typically quantized weights feedforward step every training iteration trained weights robust known quantization noise caused limited precision. also shown bits bits enough represent gradients storing weights implementing stateof-the-art maxout networks even training network however quantized based assumption exists neural network efﬁciently represents boolean functions binary inputs outputs propose process developing deploying neural networks whose weight parameters bias terms input intermediate hidden layer output signals binary-valued require basic logic feedforward pass. proposed bitwise neural network especially suitable resourceconstrained environments since replaces either ﬂoating ﬁxed-point arithmetic significantly efﬁcient bitwise operations. hence requires less spatial complexity less memory bandwidth less power consumption hardware. order design networks propose training schemes weight compression noisy backpropagation result bitwise network performs almost well corresponding realvalued network. test proposed network mnist dataset represented using binary features show bnns result competitive performance offering dramatic computational savings. according universal approximation theorem single hidden layer ﬁnite number units approximate continuous function mild assumptions theorem implies shallow network potentially intractable number hidden units comes modeling complinetworks still needs employ arithmetic operations multiplication addition ﬁxed-point values. even though faster ﬂoating point still require relatively complex logic consume power. proposed bitwise neural networks take extreme view every input node output node weight represented single bit. example weight matrix hidden layers units matrix binary values rather quantized real values although learning bitwise weights boolean concept npcomplete problem bitwise networks studied limited setting µperceptron networks input node allowed connected hidden node ﬁnal layer union hidden nodes practical network proposed recently posterior probabilities binary weights sought using expectation back propagation scheme similar backpropagation form advantages parameterfree learning straightforward discretization weights. promising results binary text classiﬁcation tasks however rely real-valued bias terms averaging predictions differently sampled parameters. paper presents completely bitwise network participating variables bipolar binaries. therefore feedforward xnor counting operations used instead multiplication addition nonlinear activation ﬂoating ﬁxed-point variables. training propose two-stage approach whose ﬁrst part typical network training weight compression technique helps real-valued model easily converted bnn. train actual compressed weights initialize parameters noisy backpropagation based tentative bitwise parameters. binarize input signals adapt binarization techniques e.g. ﬁxed-point representations hash codes. regardless binarization scheme input node given single time opposed packet representing ﬁxed-point number. signiﬁcantly different networks quantized inputs real-valued signal quantized bits bits input node place corresponding single real value. lastly apply sign function activation function instead sigmoid make sure input next layer bipolar binary well. compare performance proposed corresponding ordinary real-valued networks hand-written digit recognition tasks show bitwise operations small performance loss providing large margin improvement terms necessary computational resources. long known boolean function takes binary values input produces binary outputs well represented bitwise network hidden layer example merely memorizing possible mappings input output patterns. deﬁne forward propagation procedure follows based assumption trained network bipolar binary parameters bipolar binaries i.e. stands bitwise xnor operation indicate layer input output units layer respectively. bold characters vector number input units l-th layer. therefore equals input vector omit sample index notational convenience. sign activation function generate bipolar outputs. check prediction error measuring bitwise agreement target vector output units l-th layer using xnor multiplication operator xnor operation faster substitute binary multiplication. therefore seen special version ordinary feedforward step works inputs weights bias bipolar binaries. note bipolar bits practice implemented using binary values activation equivalent counting number checking accumulation bigger half number input units plus loss generality paper bipolar representation since ﬂexible deﬁning hyperplanes examining network behavior. sometimes solve problem realvalued network without size modiﬁcations general expect could require larger network structures real-valued one. example problem figure inﬁnite number solutions real-valued parameters pair tions real-valued system power consumption multipliers adders ﬂoating-point operations various dynamic ranges ﬁxed-point representations erroneous ﬂips signiﬁcant bits etc. note bitwise parameters sparse reduce number hyperplanes. example inactive element weight matrix sparsity simply ignore computation similarly operations sparse representations. conceptually inactive weights serve zero weights solve problem figure using hyperplane extended version inactive weights cases needs hyperplanes real-valued network even sparsity. figure xnor table. problem needs hyperplanes. multi-layer perceptron solves problem. linearly separable problem bitwise networks need hyperplanes solve bitwise network zero weights solves problem. hyperplanes successfully discriminate among possible solutions binary weights bias enough deﬁne hyperplanes likewise separation performance particular deﬁned classiﬁcation power inputs binary well. figure shows another example requires hyperplanes real-valued network. linearly separable problem solvable hyperplane impossible describe hyperplane binary coefﬁcients. instead come solution combining multiple binary hyperplanes eventually increase perceived complexity model. however even larger number nodes necessarily complex smaller real-valued network. parameter node requires represent real-valued node generally requires that bits. moreover simple xnor counting operations bypass computational complicafirst train real-valued network takes either bitwise inputs real-valued inputs ranged special part network constrain weights values well wrapping tanh. similarly choose tanh activation network relaxed version corresponding bipolar bnn. weight compression technique relaxed forward pass training deﬁned follows network suitable initializing following bipolar bitwise network. number iterations enough build baseline. ﬁrst table shows performance baseline real-valued network bits ﬂoating-point. input real-valued networks rescale pixel intensities bipolar range i.e. bipolar case second column original input third column encode four equally spaced regions bits feed input node. hence baseline network third input type binary input nodes rather cases. learn real-valued parameters train binarized inputs. instance instead real values bipolar case take sign bipolar binary features. binaries simply round pixel intensity. fixed-point inputs already binarized. train noisy backpropagation technique described second table shows results. bitwise networks perform well small additional errors. note performance original real-valued dropout network similar network topology since trained real-valued network proper range weights next train actual bitwise network. training procedure similar ones quantized weights except values deal bits operations bitwise. ﬁrst initialize real-valued parameters ones learned previous section. then setup sparsity parameter says proportion zeros binarization. then divide parameters three groups therefore decides note boundaries e.g. equals number zero weights main idea second training phase feedforward using binarized weights operations then noisy backpropagation errors gradients calculated using binarized weights signals well gradients errors properly take binarization weights signals account. since gradients small update binary parameters instead update corresponding realvalued parameters section details results hand-written digit recognition task mnist data using proposed system. throughout training adopt softmax output layer multiclass classiﬁcation cases. networks three hidden layers units layer. work propose bitwise version artiﬁcial neural networks inputs weights biases hidden units outputs represented single bits operated using simple bitwise logic. network computationally efﬁcient valuable resource-constrained situations particularly cases ﬂoating-point ﬁxed-point variables operations prohibitively expensive. future plan investigate bitwise version convolutive neural networks efﬁcient computing desirable. soudry hubara meir expectation backpropagation parameter-free training multilayer neural networks continuous discrete weights. advances neural information processing systems srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting. journal machine learning research january hinton deng dahl abdelrahman jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine", "year": 2016}