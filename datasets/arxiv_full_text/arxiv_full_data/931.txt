{"title": "On the Number of Linear Regions of Deep Neural Networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.", "text": "study complexity functions computable deep feedforward neural networks piecewise linear activations terms symmetries number linear regions have. deep networks able sequentially portions layer’s input-space output. deep models compute functions react equally complicated patterns different inputs. compositional structure functions enables re-use pieces computation exponentially often terms network’s depth. paper investigates complexity compositional maps contributes theoretical results regarding advantage depth neural networks piecewise linear activation functions. particular analysis speciﬁc single family models example employ rectiﬁer maxout networks. improve complexity bounds pre-existing work investigate behavior units higher layers. keywords deep learning neural network input space partition rectiﬁer maxout artiﬁcial neural networks several hidden layers called deep neural networks become popular unprecedented success variety machine learning tasks view empirical evidence deep neural networks becoming increasingly favoured shallow networks often implemented layers. time being however limited amount publications investigated deep networks theoretical perspective. recently delalleau bengio showed shallow network requires exponentially many sum-product hidden units deep sum-product network order compute certain families polynomials. interested extending kind analysis popular neural networks. wealth literature discussing approximation estimation complexity artiﬁcial neural networks well-known result states feedforward neural network single huge hidden layer universal approximator borel measurable functions works investigated universal approximation probability distributions deep belief networks well approximation properties figure binary classiﬁcation using shallow model hidden units deep model layers units right panel shows close-up left panel. filled markers indicate errors made shallow model. previous theoretical results however trivially apply types deep neural networks seen success recent years. conventional neural networks often employ either hidden units bounded smooth activation function boolean hidden units. hand recently become common piecewise linear functions rectiﬁer activation max{ maxout activation max{a practical success deep neural networks piecewise linear units calls theoretical analysis speciﬁc type neural networks. respect pascanu reported theoretical result complexity functions computable deep feedforward networks rectiﬁer units. showed that asymptotic limit many hidden layers deep networks able separate input space exponentially linear response regions shallow counterparts despite using number computational units. building ideas develop general framework analyzing deep models piecewise linear activations. intermediary layers models able several pieces inputs output. layer-wise composition functions computed re-uses low-level computations exponentially often number layers increases. property enables deep networks compute highly complex structured functions. underpin idea estimating number linear regions functions computable important types piecewise linear networks rectiﬁer units maxout units. results complexity deep rectiﬁer networks yield signiﬁcant improvement previous results rectiﬁer networks mentioned above showing favourable behavior deep shallow networks even moderate number hidden layers. analysis deep rectiﬁer maxout networks serves plattform study broad variety related networks convolutional networks. number linear regions functions computed given model measure model’s ﬂexibility. example given fig. compares learnt decision boundary single-layer two-layer model number hidden units illustrates advantage depth; deep model captures desired boundary accurately approximating larger number linear pieces. noted earlier deep networks able identify exponential number input neighborhoods mapping common output intermediary hidden layer. computations carried activations intermediary layer replicated many times identiﬁed neighborhoods. allows networks compute complex looking functions even deﬁned relatively parameters. number parameters upper bound dimension functions computable network small number parameters means class computable functions dimension. functions computable deep feedforward piecewise linear network although dimensional achieves exponential complexity re-using composing features layer layer. section discuss ability deep feedforward networks re-map input-space create complex symmetries using relatively computational units. observation analysis layer deep model able different regions input common output. leads compositional structure computations higher layers effectively replicated input regions produced output given layer. capacity replicate computations input-space grows exponentially number network layers. expanding ideas introduce basic deﬁnitions needed rest paper. section give intuitive perspective reasoning replicative capacity deep models. composed input weight matrices rk·nl×nl− bias vectors rk·nl layer activations units output l-th layer vector layer. computed activations preceding layer gl). given activations units layer pre-activation layer given structure network refers units arranged. speciﬁed number input dimensions number layers number units width layer. classify functions computed different network structures different choices parameters terms number linear regions. linear region piecewise linear function maximal connected subset input-space linear. functions consider linear region full dimension rectiﬁer units types behavior; either constant linear depending inputs. boundary behaviors given hyperplane collection hyperplanes coming units rectiﬁer layer forms hyperplane arrangement. general activation function distinguished behavior zero function distinguished behavior inputs hyperplanes hyperplanes capturing distinguished behavior also form hyperplane arrangement. hyperplanes arrangement split input-space several regions. formally region hyperplane arrangement connected component complement i.e. points delimited hyperplanes number regions arrangement given terms characteristic function arrangement shown well-known result zaslavsky arrangement hyperplanes start deﬁning identiﬁcation input neighborhoods mentioned introduction formally deﬁnition identiﬁes neighborhoods input domain maps common subset output domain. case also identiﬁed computation carried l-th layer feedforward network activations layer effectively carried regions input space lead activations layer. choose input weights biases given layer computed function behaves interestingly activation values preceding layer largest number preimages input space thus replicating interesting computation many times input space generating overall complicatedlooking function. given choice network parameters hidden layer computes function gl◦fl output activations preceding layer. consider function rnl; computes activations l-th hidden layer. denote image i.e. activations reachable l-th layer possible inputs. subsets mapped given subset denote onto subsets satisfy fig. illustration. number separate input-space neighborhoods mapped common neighborhood given recursively example computed ﬁrst layer equals recursive formula counts number identiﬁed sets moving along branches tree rooted j-th layer’s output-space based observations estimate maximal number linear regions follows. lemma maximal number linear regions functions computed l-layer neural deﬁned neighbordhoods distinct linear regions function computed last hidden layer. here idea construct function many linear regions ﬁrst hidden layers identify many input-space neighborhoods mapping activation neighborhoods hidden layer belongs distinct linear region last hidden layer. figure space folding euclidean space along axes. illustration top-level partitioning replicated original input space identiﬁcation regions across layers deep model. section discuss intuition behind lemma terms space folding. considered operator folds domain identiﬁes subsets coincide mapped output. instance absolute subsets value function folds domain twice illustrated fig. folding identiﬁes four quadrants euclidean space. composing operations kind applied output order re-fold ﬁrst folding. hidden layer deep neural network associated folding operator. hidden layer folds space activations previous layer. turn deep neural network effectively folds input-space recursively starting ﬁrst layer. consequence recursive folding function computed ﬁnal folded space apply collapsed subsets identiﬁed corresponding succession foldings. means deep model partitioning last layer’s image-space replicated input-space regions identiﬁed succession foldings. fig. offers illustration replication property. space foldings restricted foldings along coordinate axes preserve lengths. instead space folded depending orientations shifts encoded input weights biases nonlinear activation function used hidden layer. particular means sizes orientations identiﬁed input-space regions differ other. fig. bounds complexity attainable deep models based suitable choices network weights. however mean indicated complexity attainable singular cases. parametrization functions computed neural network continuous. precisely maps input weights biases bi}l continuous functions computed network continuous. analysis considers number linear regions functions deﬁnition linear region contains open neighborhood input-space given function ﬁnite number linear regions \u0001-perturbation parameter resulting function fθ+\u0001 least many linear regions linear regions preserved small perturbations parameters ﬁnite volume. deﬁne probability density space parameters probability event function represented network given number linear regions? discussion probability getting number regions least large number resulting particular choice parameters nonzero even though small. exists epsilon-ball non-zero volume around particular choice parameters least number linear regions attained. future work would interesting study partitions parameter space pieces resulting functions partition input-spaces isomorphic linear regions investigate many pieces parameter space correspond functions given number linear regions. empirically examined behavior trained folds input-space described above. first make observation tracing activation hidden unit model gives piecewise linear hence analyze behavior unit visualizing different weight matrices corresponding different linear pieces map. weight matrix piece found tracking linear piece used intermediary layer starting input example. visualization technique byproduct theoretical analysis similar proposed zeiler fergus motivated different perspective. computing activations intermediary hidden unit training example instance inspect examples result similar levels activation hidden unit. linear maps hidden unit corresponding examples perturb examples results exactly activation. inputs safely considered points regions identiﬁed hidden unit. appendix provide details examples visualization technique. also show inputs identiﬁed deep mlp. section analyze deep neural networks rectiﬁer units based general observations sec. improve upon results pascanu tighter lower-bound maximal number linear regions functions computable deep rectiﬁer networks. intervals subset mapped onto interval illustrated fig. function identiﬁes input-space strips j-th coordinate restricted intervals consider subsets rectiﬁers function function locally symmetric hyperplane ﬁxed j-th coordinate equal note periodic pattern emerges. fact function identiﬁes total hypercubes delimited hyperplanes. note arises composition linear function linear function effectively absorbed pre-activation function next layer. hence treat function computed current layer. computations deeper layers functions unit hypercube output rectiﬁer layer replicated identiﬁed input-space hypercubes. generalize construction described case deep rectiﬁer network inputs hidden layers widths obtain following lower bound maximal number linear regions deep rectiﬁer networks theorem maximal number linear regions functions computed neural network input units hidden layers rectiﬁers i-th layer lower bounded next corollary gives expression asymptotic behavior bounds. assuming number regions single layer model hidden units behaves deep model theorem implies thus number linear regions deep models grows exponentially polynomially much faster shallow models hidden units. result obtained pascanu signiﬁcant improvement bound particular result demonstrates even small values deep rectiﬁer models able produce substantially linear regions shallow rectiﬁer models. additionally using strategy pascanu result reformulated terms number linear regions parameter. results similar behaviour deep models exponentially efﬁcient shallow models since maximum convex functions convex maxout units maxout layers compute convex functions. maximum collection functions called upper envelope. view graph linear function supporting hyperplane convex -dimensional space. particular unique maximizer max{f input neighborhood number linear regions upper envelope max{fi exactly shows maximal number linear regions maxout unit equal rank. linear regions maxout layer intersections linear regions individual maxout units. order obtain number linear regions layer need describe structure linear regions maxout unit study possible intersections. voronoi diagrams lifted upper envelopes linear functions hence describe inputspace partitions generated maxout units. many regions obtain intersecting regions voronoi diagrams regions each? computing intersections voronoi diagrams easy general. trivial upper bound number linear regions corresponds case intersections regions different units different other. give better bound proposition purpose computing lower bounds sufﬁcient consider certain wellbehaved special cases. simple example division input-space parallel hyperplanes. consider arrangement hyperplanes maxout unit case number regions arguments yield regions. proposition maximal number regions single layer maxout network inputs network hidden layers width identify input-space regions turn compute functions linear regions. rank-k case note rank-k maxout unit identify cones input-domain whereby cone neighborhood positive half-ray {rwi corresponding gradient linear function elaborating observation obtain theorem proposition show deep maxout networks compute functions number linear regions grows exponentially number layers exponentially faster maximal number regions shallow models number units. similarly rectiﬁer model exponential behavior also established respect number network parameters. note although certain functions computed maxout layers also computed rectiﬁer layers rectiﬁer construction last section leads functions computable maxout networks proof theorem based general arguments sec. uses different construction theorem studied complexity functions computable deep feedforward neural networks terms number linear regions. speciﬁcally focused deep neural networks piecewise linear hidden units found provide superior performance many machine learning applications recently. discussed idea layer deep model able identify pieces input composition layers identiﬁes exponential number input regions. results exponentially replicating complexity functions computed higher layers model. functions computed deep models complicated still intrinsic rigidity caused replications help deep models generalize unseen samples better shallow models. framework applicable neural network piecewise linear activation function. example consider convolutional network rectiﬁer units used convolution followed pooling layer identiﬁes patches input within pooling region. deep convolutional neural network recursively identify patches images lower layers resulting exponentially many linear regions input space. parameter space given network partitioned regions resulting functions corresponding linear regions. correspondence linear regions computed functions described terms adjacency structure poset intersections regions. combinatorial structures general hard compute even simple hyperplane arrangements. interesting question future analysis whether many regions parameter space given network correspond functions given number linear regions. delalleau bengio. shallow deep sum-product networks. nips glorot bordes bengio. deep sparse rectiﬁer neural networks. aistats goodfellow warde-farley mirza courville bengio. maxout networks. hinton deng dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury. deep neural networks acoustic modeling speech recognition. ieee signal processing magazine nov. krause fischer glasmachers igel. approximation properties dbns binary hidden units real-valued visible units. proceedings international conference machine learning restricted boltzmann machines. neural computation nair hinton. rectiﬁed linear units improve restricted boltzmann machines. bottou littman editors proceedings twenty-seventh international conference machine learning pages zaslavsky. facing arrangements face-count formulas partitions space hyperplanes. number memoirs american mathematical society. american mathematical society proof lemma output-space neighborhood preimages input-space neighborhoods r-identiﬁed number inputspace preimages denoted image distinct linear region function computed last layer then continuity preimages different belong different linear regions therefore number linear regions functions computed entire network least equal number preimages proof theorem proof done counting number regions suitable choice network parameters. idea construction divide ﬁrst layers network independent parts; part input neuron. parameters part chosen folds one-dimensional input-space many times itself. part number foldings layer equal number units layer. fig. outlined above organize units layer non-empty groups units sizes simple choice sizes example nl/n layer width dropping remainder units. deﬁne input weights group units group sensitive coordinate n-dimensional input-space. discussion sec. choosing input bias weights right alternating activations units within group folds input-space coordinate times itself. since alternating activations afﬁne absorbed preactivation function next layer. order make arguments transparent view alternating activations units group activation ﬁctitious intermediary unit. compound output intermediary units partitions identiﬁed regions. input-space regions mapped n-dimensional unit cube output-space intermediary layer. view unit cube effective inputs next hidden layer repeat construction. according consider network parts consider weights used sec. function computes alternating responses sufﬁcient show folds input-coordinate times interval inspecting values need explore intervals consider intervals then hence values always zero since function linear obtain maps interval interval total number input-space neighborhoods mapped ﬁrst layers onto unit hypercube output space layer given number units i-th group units l-th layer. inputs bias last hidden layer chosen function partitions input neighborhood arrangement hyperplanes general figure illustration proof theorem ﬁgure shows rectiﬁer network divided independent parts. part sensitive coordinate input-space. layer part ﬁctitious intermediary afﬁne unit computes activation value passed next layer. illustration function computed depicted rectiﬁer network intermediary layer. function composed foldings; ﬁrst pair hidden units fold input-space along line parallel x-axis second pair along line parallel y-axis. computed bounds maximal number linear regions functions computable different networks terms number hidden units. difﬁcult express results terms number parameters networks derive expressions asymptotic rate growth number linear regions added parameter. kind expansions computed pascanu number parameters deep model layers width behaves i.e. bounded asymptotically. number parameters shallow model hidden units behaves theorem discussion shallow networks given sec. imply following asymptotic rates shows that deep models maximal number linear regions grows exponentially fast number parameters whereas shallow models grows polynomially fast number parameters. figure illustration rank- maxout layer inputs outputs. preactivation function maps input mk-dimensional space rank layer. activation function maximizes groups preactivation values. illustration -dimensional arrangement arrangement corresponds input-space partition rank- maxout layer inputs outputs pair parallel hyperplanes delimits linear regions maxout unit. maximal number regions hyperplane arrangement. case maxout units hyperplane arrangements. however upper bound number linear regions maxout layer number regions hyperplane arrangement. arguments follows. mentioned sec. maxout unit divides input linear regions upper envelope real valued linear functions. words input space divided pieces hyperplanes deﬁning boundaries inputs entry pre-activation vector larger another. boundaries since corresponds solution equation form extend boundary hyperplane number regions linear regions layer given intersections regions individual units. hence number linear regions layer upper bounded number regions arrangement hyperplanes n-dimensional space. zaslavsky proof theorem consider network maxout units rank layer. fig. deﬁne seeds maxout unit {wi}i unit vectors pointing positive negative direction coordinate vectors. larger forget case symmetric coordinate hyperplanes normals linear region gradient remaining consider similar functions whereby change coordinate system slight rotation independent direction. implies output interval linear regions composition divide input space regions rjk. since change coordinates used slight rotation independent directions ∩jrji cone dimension furthermore gradients basis hence image maxout layer contains open cone identical image shifted bias terms effective input next layer contains open neighbourhood origin arguments show maxout layer width rank identify least regions sec. mentioned maxout layers compute functions whose linear regions correspond intersections voronoi diagrams. describing intersections voronoi diagrams difﬁcult general. superpositions voronoi diagrams correspond hyperplane arrangements well understood. particularly nice examples example consider layer inputs rank- maxout units labeled pairs input bias weights chosen regions unit delimited hyperplanes xi−xj intersections regions units given regions hyperplane arrangement {hs}≤i<j≤ns= known arrangement regions. right panel fig. illustrates arrangement related arrangement corresponding rank- maxout units catalan arrangement triplets parallel hyperplanes total regions number. details arrangements introduction mention analysis rectiﬁer maxout networks serves platform study types feedforward neural networks. without going many details exemplify particular case convolutional networks. convolutional network network whose units take values space features whose edges pass features convolution ﬁlters since convolution linear preactivation function convolutional network form preactivation functions considered paper. output feature written vector like fli’s considered. hence convolutional networks piecewise linear activations fall class networks considered here. difference lies corresponding input weight matrices convolutional networks belong restricted classes matrices. experiment considered mlps single hidden layer hidden units hidden layers hidden unit each. mlps trained synthetic dataset using conjugate natural gradient used minimize effect optimization. plot best several runs. shallow model misclassiﬁed examples whereas deep model examples. two-layer model better capturing sinusoidal decision boundary deﬁne linear regions. piecewise linear function fully deﬁned different linear pieces composed. piece given domain–a region input space rn–and linear describes behaviour afﬁne interpreted hidden units shallow model are. namely write vector measures cosine distance image also image shows pattern unit responds whenever given input example arbitrary region input space construct corresponding linear generated j-th unit l-th layer. speciﬁcally weight linear computed bias linear similarly computed. linear speciﬁc hidden unit layer found keeping track linear piece used layer layer j-th weight matrix multiplied. although present formula speciﬁc rectiﬁer straightforward adapt piecewise linear activation convolutional neural network maxout activation. fact linear computed depends sample/point need traverse points identify different linear responses hidden unit. give possible responses points large enough sufﬁciently many provide better understanding behaviour. trained rectiﬁer three hidden layer toronto faces dataset ﬁrst hidden layers hidden units last units. trained model using stochastic gradient descent. used regularization penalty coefﬁcient dropout ﬁrst hidden layers enforced weights unit norm column-wise projecting weights step. used learning rate output layer composed sigmoid units. purpose regularization schemes sigmoid output layer obtain cleaner sharper ﬁlters. model trained fold dataset achieves error reasonable dataset non-convolutional model. since ﬁrst layer hidden units responds single linear region directly visualize learned weight vectors randomly selected units ﬁrst hidden layer. shown fig. hand hidden layer randomly pick units layer visualize interesting four units based maximal euclidean distance different linear responses unit. linear responses unit computed clustering responses obtained training four clusters using k-means algorithm. show representative linear response clusters looking differences among distinct linear regions hidden unit responds investigate type invariance unit learned. fig. show differences among four linear maps learned last visualized hidden unit third hidden layer visualizations hidden unit learns invariant abstract interesting translations higher layers. also types invariance hidden unit higher layer clearly. zeiler fergus attempt visualize behaviour units upper layer speciﬁcally deep convolutional network rectiﬁers. approach extent similar approach proposed here except make assumption beside hidden unit networks uses piece-wise linear activation function. perspective visualization considered also different. zeiler fergus approaches problem visualization perspective inverting feedforward computation neural network whereas approach derived identifying linear maps hidden unit. difference leads number minor differences actual implementation. instance zeiler fergus approximates inverse rectiﬁer simply using another rectiﬁer. hand need approximate inverse rectiﬁer. rather identify regions input space maps activation. figure visualizations linear maps learned hidden layer rectiﬁer trained dataset. corresponds hidden layer. ﬁrst column shows unnormalized linear maps last column shows normalized linear maps showing direction map. colors used improve distinction among different ﬁlters. randomly chosen hidden unit third hidden layer exactly found points ﬁrst ﬁnding three training samples activation close hidden unit found sample search along linear point exactly results activation obviously found point training samples. three points chosen hidden unit responds face wide-open mouth open eyes invariant features face pertubation analysis assume open around points identiﬁed hidden unit. figure visualization three distinct points input space activation randomly chosen hidden unit third hidden layer. shows three points input space point plot linear below.", "year": 2014}