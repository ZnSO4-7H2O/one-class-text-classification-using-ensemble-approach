{"title": "Beyond Monte Carlo Tree Search: Playing Go with Deep Alternative Neural  Network and Long-Term Evaluation", "tag": ["cs.AI", "cs.LG", "cs.NE"], "abstract": "Monte Carlo tree search (MCTS) is extremely popular in computer Go which determines each action by enormous simulations in a broad and deep search tree. However, human experts select most actions by pattern analysis and careful evaluation rather than brute search of millions of future nteractions. In this paper, we propose a computer Go system that follows experts way of thinking and playing. Our system consists of two parts. The first part is a novel deep alternative neural network (DANN) used to generate candidates of next move. Compared with existing deep convolutional neural network (DCNN), DANN inserts recurrent layer after each convolutional layer and stacks them in an alternative manner. We show such setting can preserve more contexts of local features and its evolutions which are beneficial for move prediction. The second part is a long-term evaluation (LTE) module used to provide a reliable evaluation of candidates rather than a single probability from move predictor. This is consistent with human experts nature of playing since they can foresee tens of steps to give an accurate estimation of candidates. In our system, for each candidate, LTE calculates a cumulative reward after several future interactions when local variations are settled. Combining criteria from the two parts, our system determines the optimal choice of next move. For more comprehensive experiments, we introduce a new professional Go dataset (PGD), consisting of 253233 professional records. Experiments on GoGoD and PGD datasets show the DANN can substantially improve performance of move prediction over pure DCNN. When combining LTE, our system outperforms most relevant approaches and open engines based on MCTS.", "text": "tree simulate evaluate action. however playing strength mcts-based programs still human-level major limitation uneven performance. well known weaknesses include capturing race semeai positions multiple cumulative evaluation errors ﬁghts close endgames attribute following reasons. first effectivity truncating search tree based prior knowledge away perfect play second board spacious especially opening simulation expensive useless. besides outputs leaves monte carlo tree difﬁcult precisely evaluated last important mcts follow professionals’ playing since professionals hardly make brute simulation every possible future positions. instead situations ﬁrst obtain candidates using pattern analysis determine optimal evaluating candidates. recently deep learning revolutionizes gradually dominate many tasks computer vision community researcher start borrow deep learning techniques move prediction develop computer systems however compared visual signals board much smaller size poses importance relative position. consistent playing situation dramatically alter minor change position. hand existing dcnns often mine contexts stacking convolutional layers exploit high-order encodings low-level features. simply increasing layers suffers parameter burden also embed contexts local features evolutions. based discussions paper introduces computer system consisting major parts. ﬁrst part novel deep architecture used provide probability distribution legal candidates learned professionals’ records. candidates sent long-term evaluation part considering local future impact instead immediate reward. expect model focus several suggested important regions rather blind simulation every corner board. primary contributions work summarized follows. monte carlo tree search extremely popular computer determines action enormous simulations broad deep search tree. however human experts select actions pattern analysis careful evaluation rather brute search millions future interactions. paper propose computer system follows experts thinking playing. system consists parts. ﬁrst part novel deep alternative neural network used generate candidates next move. compared existing deep convolutional neural network dann inserts recurrent layer convolutional layer stacks alternative manner. show setting preserve contexts local features evolutions beneﬁcial move prediction. second part long-term evaluation module used provide reliable evaluation candidates rather single probability move predictor. consistent human experts nature playing since foresee tens steps give accurate estimation candidates. system candidate calculates cumulative reward several future interactions local variations settled. combining criteria parts system determines optimal choice next move. comprehensive experiments introduce professional dataset consisting professional records. experiments gogod datasets show dann substantially improve performance move prediction pure dcnn. combining system outperforms relevant approaches open engines based mcts. game profound complexity draws attention. although rules simple difﬁcult construct suitable value function actions situations mainly high branching factors subtle board situations sensitive small changes. previous solutions focus simulating future possible interactions evaluate candidates. methods monte carlo tree search popular constructs broad deep search copyright association advancement artiﬁcial intelligence rights reserved. propose novel deep alternative neural network learn pattern recognizer move prediction. proposed dann enjoys advantages recurrent neural network preserving contexts local features evolutions show essential playing compared existing dcnnbased models dann substantially improve move prediction performance using less layers parameters. enhance candidates generated dann present novel recurrent model make long-term evaluation around candidate ﬁnal choice. formulate process partially observe markov decision process problem propose reinforcement learning solution careful control variance. introduce professional dataset comprehensive evaluation. consists modern professional records considered useful computer community. thorough experiments gogod demonstrates advantages system dcnns mcts move prediction rate open source engines. monte carlo tree search best-ﬁrst search method based randomized explorations search space require positional evaluation function using results previous explorations algorithm gradually grows game tree successively becomes better accurately estimating values optimal moves programs strong amateur level performance considerable still remains professionals strongest computer programs. majority recent progress increased quantity quality prior knowledge used bias search towards promising states widely believed knowledge major bottleneck towards progress. ﬁrst successful current program based mcts. basic algorithm augmented mogo leverage prior knowledge bootstrap value estimates search tree. training professionals’ moves enhanced fuego pachi achieved strong amateur level. supervised pattern-matching policy learning. professionals rely heavily pattern analysis rather brute simulation cases gain strong intuitions best moves consider glance. contrast mcts simulates enormous possible future positions. prediction functions expected non-smooth highly complex since fair assume professionals think complex non-linear ways choose moves. neural networks especially cnns widely used recent works image recognition demonstrated considerable advantages dcnns showed substantial improvement shallow networks based manually designed features simple patterns extracted previous games dcnns yielded several state-of-the-art playing system convolutional layers. besides works also indicated combining dcnns mcts improve overall playing strength. similar conclusions validated state-of-the-art programs major difference between paper existing combinations comes perspectives. ﬁrst novel architecture dann generate candidates consideration local contexts evolutions. proposed architecture shows substantial improvement pure dcnn fewer layers parameters. second long-term evaluation analyze previous candidates instead mcts assist ﬁnal choice. strategy faster mcts former needs consider large search space. proposed system illustrated figure given situation ﬁrst obtain probability distribution legal points learned pattern-aware prediction instrument based supervised policy learning existing professional records. analyze candidates high-conﬁdence long-term evaluation pursue expectation reward several future steps local situation settled. action highest score criteria combination criteria ﬁnal choice. deep alternative neural network train novel deep alternative neural network generate probability distribution legal points given current board situation input. treat board image multiple channels. channel encodes different aspect board information following describe structure dann including component overall architecture. afterwards discuss relations advantages popular dcnns. alternative layer. component dann alternative layer consists standard convolutional layer followed designed recurrent layer. specifically convolution operation ﬁrst performed extract features local neighborhoods feature maps previous layers. recurrent layer applied output iteratively proceeds times. procedure makes unit evolve discrete time steps aggregate larger receptive ﬁelds formally input unit position feature time denoted uxyz figure proposed computer system deep alternative neural network long-term evaluation. given situation system generate several candidates dann learned professional records. candidates analyzed using long-term evaluation consideration future rewards determine ﬁnal action. connections shortest path goes feedforward connection only. effective unit feature maps previous layer expands iteration number increases. input recurrent kernels equation square shapes feature size lfeed lrec effective unit also square whose side length lfeed lrec advantages dcnns. recurrent connections dann provide three major advantages compared popular dcnns used move prediction first enable every unit incorporate contexts arbitrarily large region current layer particular suitable game input signal small contexts essential. time steps increase state every unit inﬂuenced units larger larger neighborhood current layer. consequence size regions unit watch input space also increases. standard convolutional layers size effective units current layer ﬁxed watching larger region possible units higher layers. unfortunately context seen higher-level units cannot inﬂuence states units current layer without top-down connections. second recurrent connections increase network depth keeping number adjustable parameters constant weight sharing. specially stacking higher layers consume parameters uses additional constant parameters compared standard convolutional layer. consistent trend modern deep architectures i.e. going deeper relatively small number parameters note simply increasing depth sharing weights layers result depth number parameters dann. tried model leads lower performance. third advantage time-unfolded manner figure actually multiple paths input layer output layer facilitate learning procedure. hand existence longer paths makes possible model learn highly complex features. hand existence shorter paths help gradient backpropagation training. multi-path also used extra objective functions used uxyz denotes feed-forward output convoluij recurrent input previous tional layer uxyz time vectorized feed-forward kernels recurrent kernels bias feature layer uxyz output convolutional output induced reprevious layer function followed local response normalization number feature maps constants controlling amplitude normalization. forces units location compete high activities mimics lateral inhibition cortex. experiments found consistently improve accuracy though slightly. following respectively. equation describes dynamic behavior contexts involved local features extracted. unfolding layer time steps results feed-forward subnetwork depth shown right figure recurrent input evolves iterations feed-forward input remains iterations. feed-forward input present. subnetwork several paths input layer output layer. longest path goes unfolded recurrent training. policy agent possibly combination dynamics interactions induces distribution possible interaction sequences maximize reward distribution maximizing exactly difﬁcult involves expectation interaction sequences turn involve unknown environment dynamics. viewing problem pomdp problem however allows bring techniques literature bear. shown sample approximation gradient given interaction sequences obtained running current agent episodes. learning rule equation also known reinforce rule involves running agent current policy obtain samples interaction sequences adjusting parameters log-probability chosen actions high cumulative reward increased actions produced reward decreased. equation gradient computed standard backpropagation overall architecture. overall architecture dann kernels followed fully connected layers size each. kernel convolutional layer recurrent layers als. network includes relu activation. pooling kernels size. convolutional layers recurrent layers applied appropriate padding stride. layers followed relu softmax outputs probabilities board illegal points long-term evaluation candidates dann provides probability distribution next move candidates give situation. enhance model evaluating candidates long-term consideration since predicting immediate next move liminformation received lower layers besides many situations intensive battle capture chase beyond fair evaluation need accurately judged local variation settled. avoiding shortsighted moves. works consider playing games sequential decision process goal-directed agent interacting visual environment. extend idea evaluate candidates similar manner. calculate cumulative rewards candidate several future interactions. combining previous probabilities criterion obtain ﬁnal score determine optimal action. recurrent model internal state. figure shows model structure build agent around rnn. avoiding blind search space like mcts agent observes environment bandwidth-limited sensor i.e. never senses full board. extract information local region around candidates. goal model provide reliable evaluation candidate assist ﬁnal choice. agent maintains internal state summarizes information extracted past observations. encodes agent’s knowledge environment instrumental deciding deploy next action. internal state formed hidden units recurrent neural network updated time core network action reward. step agent performs actions. decides deploy sensor sensor control action might affect state environment. location chosen stochastically distribution parameterized location network action similarly drawn distribution conditioned second network output finally model also augmented additional action decides stop local ﬁghts settled. executing action agent receives visual observation reward signal goal agent maximize setup special instance partially observable markov decision profeature ladder capture ladder escape sensibleness legality player color zeros stone color liberties liberties∗ turn since capture size self-atari size description whether point successful ladder capture whether point successful ladder escape whether point legal eyes whether point legal current player whether current player black constant plane ﬁlled number liberties number liberties number liberties many opponent stones would captured many stones would captured cumulative reward following baseline depends execution action itself. estimate equal equation expectation lower variance. select value function baseline following form type baseline learn reducing squared error final score. deﬁne ﬁnal score candidate using criteria dann long-term evaluation. select action highest score ﬁnal choice system probability produced softmax layer dann. setup datasets. ﬁrst dataset used gogod dataset consists historical modern games. limited experiments subset games satisﬁed following criteria board modern standard komi handicap stones. distinguish rulesets criteria produced training around games. popular dataset consists games lower players. average level approximately dan. besides collected professional dataset consisting professional records exceeds gogod quantity playing strength. parsed non-proﬁt sites. records saved widely used smart named re.sgf dtevpwwrpbbr represent date tournament type black player name black playing strength white player name white playing strength result. feature channels. features used come directly representation game rules table many features split multiple planes binary values example case liberties separate binary implementation details. major implementations dann including convolutional layers recurrent layers optimizations derived torch toolbox applied mini-batches negative likelihood criterion. size mini-batch training performed minimizing cross-entropy loss function using backpropagation time algorithm equivalent using standard algorithm timeunfolded network. ﬁnal gradient shared weight gradients time steps. initial learning rate networks learned scratch networks ﬁne-tuned pre-trained models. momentum weight decay initialized reduced factor every decrease learning rate. move prediction ﬁrst evaluate different conﬁgurations dann. compare best dann model dcnn-based methods. finally study impact long-term evaluation report overall performance move prediction. model investigation dann. crucial conﬁgurations dann model. ﬁrst setting including order number. unfolding time recurrent layers. comparison details reported table baseline composed similar conﬁguration dann using standard convolutional layers instead als. ﬁrst column left table table layer accuracy comparison demonstrates beneﬁts inserting advance. attribute context mining lower features. fourth column left table table shows performance increases number increases veriﬁes effectiveness inserting recurrent layer. speciﬁcally order contribute performance gain indicates mining contexts lower layer beneﬁcial playing right table table uses study impact results prove larger leads better performance cases. given results best dann model following experiments. comparison dcnn-based methods. figure reports performance comparison best dann model related approaches using pure dcnn. following evaluate accuracy correct move within networks conﬁdent predictions. figure shows model consistently outperform recent approaches using pure dcnn datasets. also note architecture consume less layers parameters also saved combining long-term evaluation. next examine inﬂuence long-term evaluation focus mainly future step used achieve expectation reward episode number solving equation combine best dann model used gnugo level mogo pachi pattern ﬁles fuego throughout experiments. setting groups games played. report average rate standard deviation computed group averages. game experiments mentioned paper used komi chinese rules. pondering pachi fuego table shows system outperform mcts-based programs. also rate approach higher previous works except work proposed computer system based novel deep alternative neural networks long-term evaluation also public dataset consisting around professional records. datasets showed dann predict next move made professionals accuracy substantially exceeds previous deep convolutional neural network methods. strategy enhance quality candidates selection combining inﬂuence future interaction instead immediate reward. without brute simulation possible interaction large deep search space system able outperform mcts-based open source programs. future work mainly includes improvement dann structure move prediction reliable implementation. advance techniques computer vision community residual networks help dann obtain further improvement. domain knowledge attempted provide reliable estimation next move candidates. top- move prediction accuracy. table demonstrates details. seen best performance achieved around steps. episode often converges around episodes. using optimal setting part overall top- accuracy obtained gogod datasets respectively. playing strength finally evaluate overall playing strength system playing several publicly available benchmark programs. programs played strongest available settings ﬁxed number rollouts move.", "year": 2017}