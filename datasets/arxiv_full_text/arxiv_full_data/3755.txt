{"title": "Sim-to-real Transfer of Visuo-motor Policies for Reaching in Clutter:  Domain Randomization and Adaptation with Modular Networks", "tag": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY"], "abstract": "A modular method is proposed to learn and transfer visuo-motor policies from simulation to the real world in an efficient manner by combining domain randomization and adaptation. The feasibility of the approach is demonstrated in a table-top object reaching task where a 7 DoF arm is controlled in velocity mode to reach a blue cuboid in clutter through visual observations. The learned visuo-motor policies are robust to novel (not seen in training) objects in clutter and even a moving target, achieving a 93.3% success rate and 2.2 cm control accuracy.", "text": "fig. robot learns visuo-motor policies control left reach target blue cuboid clutter table. baxter visually observes table-top environments monocular camera right hand. modular network used efﬁciently learn transfer visuo-motor policies simulation real world modular network consists perception control modules connected bottleneck layer environments benchmark visually-guided tabletop object reaching task robotic vision kinematics data gathered simulation decrease amount real world collection necessary signiﬁcantly introducing modular approach perception skill controller transferred individually robotic platform retaining ability ﬁne-tune end-to-end fashion improve hand-eye coordination real robot simulated real data makes approach able transfer perception real world simulated real images. beneﬁting modular structure weighted end-to-end ﬁnetuning learned visuo-motor policy achieve reaching accuracy trajectories learned visuo-motor policy able reach target object clutter seen distractor objects also cases novel distractor objects even target object moving. abstract— modular method proposed learn transfer visuo-motor policies simulation real world efﬁcient manner combining domain randomization adaptation. feasibility approach demonstrated table-top object reaching task controlled velocity mode reach blue cuboid clutter visual observations. learned visuo-motor policies robust novel objects clutter even moving target achieving success rate control accuracy. advent large datasets sophisticated machine learning models commonly referred deep learning recent years created trend away hand-crafted solutions towards data-driven ones. learning techniques shown signiﬁcant improvements robustness performance particularly computer vision ﬁeld. traditionally robotic reaching approaches based crafted controllers combine motion planners hand-crafted features localize target visually. recently learning approaches tackle problem presented however consistent issue faced approaches reliance large amounts data train models. example google researchers addressed problem developing \"arm farm\" robots collecting data parallel generalization forms another challenge many current systems brittle learned models applied robotic conﬁgurations differ used training. leads question better learn transfer visuo-motor policies robots tasks reaching? various approaches proposed address problems robot learning context simulators synthetic data methods transfer learned models real-world scenarios directly learning real-world tasks collecting large amounts data contribution paper method connects three usually separately considered approaches. propose modular deep learning approach efﬁciently learn transfer visuo-motor policies simulated real *this research conducted australian research council centre excellence robotic vision computational resources services used work partially provided research support group queensland university technology. authors australian centre robotic vision queensland university technology brisbane australia fangyi.zhanghdr.qut.edu.au real-world robotic applications still scarce require manually designed mapping information e.g. similaritybased approach skill transfer robots reduce number real-world images required method adapting visual representations simulated real environments proposed achieving success rate hook loop task times less real-world images domain randomization also successfully used transfer deep neural networks simulation real world domain randomization successfully used transfer deep neural networks simulation real world object position recognition also visuomotor control auxiliary tasks cube pose estimation used training improve performance end-to-end manner. paper propose modular approach take object position estimation task efﬁcient learning transfer visuo-motor policies. modular deep networks studies deep visuo-motor policies indicate convolutional layers focus perception i.e. extracting useful information visual inputs fully connected layers perform control make learning transfer visuo-motor policies efﬁcient propose separate deep neural network perception control modules connected bottleneck layer bottleneck forces network learn low-dimensional representation unlike auto-encoders difference explicitly equate bottleneck layer object position bottleneck perception module learns estimate object position raw-pixel image control module learns determine appropriate joint velocities given object position joint angles values normalized interval training method perception perception module trained using supervised learning ﬁrst using simulated images ﬁnetuned small number real samples skill transfer quadratic loss function lutions also robotic applications. especially robotic vision tasks robotic tasks based directly real image data navigation object grasping manipulation seen increased interest. lack largescale real-world datasets expensive slow acquire limit general applicability approach limited broader application. collecting datasets required deep learning sped using many robots operating parallel grasp attempts recorded deep network trained predict success probability sequence motions aiming grasping using robotic manipulator -ﬁnger gripper. combined simple derivative-free optimization algorithm grasping system achieved success rate another example dataset collection grasping approach self-supervised grasp learning real world force sensors used autonomously label samples training real-world trials using staged leaning method deep convolutional neural network achieved grasping success rate around impressive results achieved high cost terms dollars space time. learning system able synthesize control actions computer games directly vision data result important exciting breakthrough transfer directly real robots real cameras observing real scenes fact modest image distortions simulation environment caused performance system fall dramatically. introducing real camera observing game screen even worse increasing interest create robust visuomotor policies robotic applications especially reaching grasping. levine introduced cnn-based policy representation architecture added guided policy search learn visuo-motor policies allows reduction number real world training examples providing oracle impressive results achieved complex tasks hanging coat hanger inserting block tightening bottle cap. recently proposed simulated depth images learn transfer grasping skills real-world robotic arms adaptation real-world performed. transfer learning attempts develop methods transfer knowledge different tasks reduce amount data collected real world transferring skills simulation real world attractive alternative. progressive neural networks leveraged improve transfer avoid catastrophic forgetting learning complex sequences tasks effectiveness validated reinforcement learning tasks atari maze game playing. modular reinforcement learning approaches shown skill transfer capabilities simulation however methods fig. modular network consists perception control modules connected bottleneck layer representing target object position perception module architecture customized ﬁrst convolutional layer initialized weights pre-trained vgg. control module consists fully connected layers determines joint velocities according target position joint angles. perception control modules ﬁrst trained separately ﬁne-tuned end-to-end fashion using weighted losses end-to-end ﬁne-tuning using weighted losses further improve hand-eye coordination end-to-end ﬁnetuning conducted combined network separate training using weighted control perception losses. note equation end-to-end ﬁne-tuning rather control module updated using perception module updated using weighted loss pseudo-loss reﬂects loss bottleneck; balancing weight. backpropagation algorithm infer βδlp δlbn gradients resulting gradients resulting respectively δlbn canonical target reaching task benchmark evaluate feasibility proposed approach. task deﬁned controlling robot end-effector position operational space moves position target robot’s joint conﬁguration represented joint angles spaces related forward kinematics i.e. reaching controller adjusts robot conﬁguration velocity mode minimize error robot’s current target position i.e. consider robotic i.e. steering end-effector position i.e. ignoring orientation. real-world task employs baxter robot’s left reach blue cuboid clutter. objects arbitrarily placed operation area shown fig. blue cuboid side length robot observes environments monocular camera right hand providing images resolution left controlled velocity mode. reach deemed successful euclidean fig. baxter robot controls left velocity mode reach blue cuboid clutter arbitrarily placed operation area. centre bottom centre centre target cuboid bottom center suction gripper. fig. shows left reference initial conﬁguration. distance centre target cuboid bottom center suction gripper smaller task left randomly initialized conﬁguration normal distribution around reference conﬁguration shown fig. work used network architecture shown fig. perception module architecture customized lower computational cost without losing much performance task. consists twelve convolutional layers ﬁlters seven pooling layers followed three fully connected layers. simulated real images cropped down-sampled inputs perception module. control module consists fully connected layers units hidden layers. input control module scene conﬁguration outputs estimates joint velocities networks ﬁrst convolutional layer initialized weights pretrained observed converge faster randomization uniformly distributed. reference colors table pose tuned manually approximate real scene. reference joint angles tuned real world making sure in-hand camera entire operation area. parameters aspects randomized based references manually tuned simulate possible variations real scene. real images shown fig. collected real baxter robot random objects left conﬁgurations. ground-truth position target blue cuboid collected putting end-effector bottom centre cuboid centre recording left conﬁguration forward kinematics i.e. ground-truth position collected accurate enough benchmark task although errors might caused manually matching end-effector cuboid. ground-truth position collection method also used control performance evaluation section training increase training data diversity data augmentation done on-the-ﬂy simulated real images varying image brightness white balance post-processing manner. augmentation parameters empirically determined. sceneconﬁguration-velocity pairs well image-velocity image-position pairs. pairs training control modules separately i-x∗ pairs end-to-end ﬁne-tuning obtain collected control datasets simulation using v-rep. trajectories generated control left random initial conﬁguration reach target arbitrarily placed operation area regardless obstacle avoidance. introduced section iv-a random initial conﬁguration normal distribution around reference conﬁguration shown fig. random targets uniformly distributed operation area. generating trajectories pseudo inverse method used calculate desired conﬁguration reach target i.e. simple proportional controller used control left reach desired conﬁguration initial conﬁguration control frequency process target cuboid position joint angles velocity commands recorded accompanying images camera right hand. experiments show simulated control training data sufﬁcient achieve good performance alone need collect real control datasets. fig. images training perception modules. simulated images collected v-rep simulator using domain randomization real images collected domain adaptation real baxter perception datasets contain number image-position pairs. work label position target cuboid centre target position rather mass-centre. aiming good balance domain randomization adaptation collected simulated real data shown fig. investigation. simulated data collected using v-rep domain randomization following aspects euclidean distance between estimated ground-truth object positions; control error euclidean distance target cuboid centre end-effector bottom centre success rate percentage successful reaching among trials reach deemed successful ﬁnal euclidean distance target end-effector smaller deﬁned section iv-a; investigate inﬂuence numbers simulated real images perception accuracy evaluated different perception modules. trained different combinations images introduced section iii-b. perception modules ﬁrst trained using simulated images ﬁnetuned real images. training scratch except ﬁrst convolutional layer initialized weights pre-trained training used minibatch size learning rate rmsprop adopted training control modules end-to-end ﬁne-tuning pixel values images normalized image. mean standard deviation perception errors test shown fig. test real images target uniformly distributed operation area random distractor objects appeared training. fig. perception modules trained simulated real images large errors. ones trained simulated real images increasing number either simulated real images helped reduce error. fine-tuning real images make perception module work real world average error trained simulated real images achieved smallest average error however trading accuracy used number real images pick trained simulated real images experiments labelled average error slightly larger best needs real images. study much on-the-ﬂy data augmentation method help improve perception accuracy. trained perception module using simulated real images without data augmentation. achieved average error larger fig. object position estimation error map. numbers show mean standard deviation euclidean distances predicted ground-truth positions. means result case. fig. control performance curve shows mean standard deviation euclidean distances target end-effector. three control modules evaluated trials. trained different numbers trajectories shows data augmentation method help improve perception accuracy. investigate many trajectories sufﬁcient training control module evaluated three control modules trained different control datasets varying numbers trajectories introduced section iv-c trajectories dataset collected simulation targets uniformly distributed operation area. training used mini-batch size learning rate decreasing metrics control error success rate used evaluation. performance real-world reaching trials shown fig. trials targets uniformly distributed operation area random initial left conﬁgurations fig. control module trained trajectories able achieve better control performance terms control error success rate. trained trajectories success rate others also much larger control error two. indicates trajectories fig. test cases end-to-end performance. reaching blue cuboid without distractor objects; reaching seen novel objects distractors; reaching occlusion; reaching target moving. good control module. trained trajectories achieved slightly smaller control error trained much fewer trajectories shows trajectories sufﬁcient reasonably good control module. trading performance number trajectories pick trained trajectories compose network end-to-end reaching section labelled improve hand-eye coordination proposed end-to-end ﬁne-tuning approach using weighted losses. evaluate feasibility approach compare three combined networks weighted end-to-end ﬁne-tuning used learning rate mini-batch size control perception losses respectively. image-velocity pairs control dataset used ﬁne-tuning obtain δlc; image-position pairs used obtain δlp. make sure perception module remembers adaptation real scene real images also used obtain δlp. mini-batch samples real ones i.e. weight updating step real simulated images used. naive ﬁne-tuning used learning rate mini-batch size similarly samples mini-batch real ones labelled velocities obtained using method section iv-c. evaluation ﬁne-tuning ﬁrst done real world without distractor objects table shown fig. using metrics control error success rate time cost. results real-world reaching trials listed table trials targets initial left conﬁgurations used section v-b. results single object case that weighted end-to-end ﬁne-tuning achieved better performance compared smaller control error; higher success rate; fewer average steps. comparison much worse performance shows proposed approach end-to-end ﬁne-tuning weighted losses able significantly improve performance combined network naive approach could make performance even worse. addition even smaller control error perception error individually evaluate perception module perception error increased indicates endto-end ﬁne-tuning improve coordination perception control modules rather improving individually. evaluate performance challenging cases novel distractor objects clutter target cuboid partially occluded conducted experiments settings shown fig. b-c. results table achieved comparable performance case even multiple novel distractor objects clutter case target cuboid partially occluded observed performance drop still able reach half targets. also tested case target cuboid moving able adapt target position changes real time performed well cases shown attached video. value modular structure end-to-end ﬁnetuning signiﬁcant performance improvement end-to-end ﬁne-tuning weighted losses shows feasibility modular approach. beneﬁting modular structure well domain randomization adaptation visuo-motor policies table-top reaching task learned transferred simulation real world simulated real samples achieving comparable performance fewer training data. modular approach used general ways. although explicitly equated bottleneck layer target object position work bottleneck general could explicit latent low-dimensional features like auto-encoders. perception control modules also trained methods unsupervised learning reinforcement learning. feasibility modular approach reinforcement learning validated planar reaching task domain randomization adaptation section perception module trained simulated images large error much higher expected apart experiments section also trained number perception modules using simulated images random values table ﬂoor robot body rather changes around reference colors. however bring obvious accuracy improvement. possible reasons include simple textures simple randomization light conditions; simulated shadows; sensitivity domain randomization parameters tuning. nevertheless adaptation real images able transfer network simulation real world needs fewer simulated images reach comparable accuracy combination domain randomization adaptation promising efﬁcient deep neural network transfer. paper proposed modular approach learn transfer visuo-motor policies simulation real world domain randomization adaptation. feasibility demonstrated task reaching table-top object amongst clutter robotic velocity mode. using weighted losses ﬁne-tune combined network end-to-end fashion performance signiﬁcantly improved achieving success rate average control error learned policies robust novel distractor objects clutter even moving target. modular approach promising efﬁcient transfer visuo-motor policies simulation real world. zhang leitner milford upcroft corke towards vision-based deep reinforcement learning robotic motion control australasian conference robotics automation december available https//arxiv.org/abs/. levine sampedro krizhevsky quillen learning hand-eye coordination robotic grasping deep learning large-scale data collection international symposium experimental robotics bateux marchand leitner chaumette corke visual servoing deep neural networks frontiers deep learning robotics workshop robotics science systems conference available https //arxiv.org/abs/. katyal i.-j. wang burli leveraging deep reinforcement learning reaching robotic tasks ieee conference computer vision pattern recognition workshops d’innocente carlucci colosi caputo bridging computer robot vision data augmentation case study object recognition international conference computer vision systems tobin fong schneider zaremba abbeel domain randomization transferring deep neural networks simulation real world ieee/rsj international conference intelligent robots systems tzeng devin hoffman finn abbeel levine saenko darrell adapting deep visuomotor representations weak pairwise constraints workshop algorithmic foundations robotics mnih kavukcuoglu silver rusu veness bellemare graves riedmiller fidjeland ostrovski human-level control deep reinforcement learning nature vol. viereck saenko platt learning visuomotor controller real world robotic grasping using simulated depth images annual conference robot learning yang survey transfer learning ieee transactions knowledge data engineering vol. devin gupta darrell abbeel levine learning modular neural network policies multi-task multi-robot transfer ieee international conference robotics automation zhang leitner milford corke modular deep networks sim-to-real transfer visuo-motor policies queensland university technology tech. rep. available https//arxiv.org/abs/. lecun theoretical framework back-propagation proceedings connectionist models summer school touretzky hinton sejnowski eds. pittsburgh morgan kaufmann corke tuning modular networks weighted losses hand-eye coordination ieee conference computer vision pattern recognition workshops july available https//arxiv.org/abs/.", "year": 2017}