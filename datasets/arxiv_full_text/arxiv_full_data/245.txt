{"title": "Evolutionary Synthesis of Deep Neural Networks via Synaptic  Cluster-driven Genetic Encoding", "tag": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "abstract": "There has been significant recent interest towards achieving highly efficient deep neural network architectures. A promising paradigm for achieving this is the concept of evolutionary deep intelligence, which attempts to mimic biological evolution processes to synthesize highly-efficient deep neural networks over successive generations. An important aspect of evolutionary deep intelligence is the genetic encoding scheme used to mimic heredity, which can have a significant impact on the quality of offspring deep neural networks. Motivated by the neurobiological phenomenon of synaptic clustering, we introduce a new genetic encoding scheme where synaptic probability is driven towards the formation of a highly sparse set of synaptic clusters. Experimental results for the task of image classification demonstrated that the synthesized offspring networks using this synaptic cluster-driven genetic encoding scheme can achieve state-of-the-art performance while having network architectures that are not only significantly more efficient (with a ~125-fold decrease in synapses for MNIST) compared to the original ancestor network, but also tailored for GPU-accelerated machine learning applications.", "text": "signiﬁcant recent interest towards achieving highly efﬁcient deep neural network architectures. promising paradigm achieving concept evolutionary deep intelligence attempts mimic biological evolution processes synthesize highly-efﬁcient deep neural networks successive generations. important aspect evolutionary deep intelligence genetic encoding scheme used mimic heredity signiﬁcant impact quality offspring deep neural networks. motivated neurobiological phenomenon synaptic clustering introduce genetic encoding scheme synaptic probability driven towards formation highly sparse synaptic clusters. experimental results task image classiﬁcation demonstrated synthesized offspring networks using synaptic cluster-driven genetic encoding scheme achieve state-of-the-art performance network architectures signiﬁcantly efﬁcient compared original ancestor network also tailored gpu-accelerated machine learning applications. introduction strong interest toward obtaining highly efﬁcient deep neural network architectures maintain strong modeling power different applications self-driving cars smartphone applications available computing resources practically limited combination low-power embedded gpus cpus limited memory computing power. optimal brain damage method ﬁrst approaches area synapses pruned based strengths. gong proposed network compression framework vector quantization leveraged shrink storage requirements deep neural networks. utilized pruning quantization huffman coding reduce storage requirements deep neural networks. hashing another trick utilized chen compress network smaller amount storage space. rank approximation sparsity learning strategies used sparsify deep neural networks. recently shaﬁee tackled problem different manner proposing novel framework synthesizing highly efﬁcient deep neural networks idea evolutionary synthesis. differing signiﬁcantly past attempts leveraging evolutionary computing methods genetic algorithms creating neural networks attempted create neural networks high modeling capabilities direct highly computationally expensive manner proposed novel evolutionary deep intelligence approach mimics biological evolution mechanisms random mutation natural selection heredity synthesize successive generations deep neural networks progressively efﬁcient network architectures. architectural traits ancestor deep neural networks encoded probabilistic ‘dna’ sequences offspring networks possessing diverse network architectures synthesized stochastically based ‘dna’ ancestor networks computational environmental factor models thus mimicking random mutation heredity natural selection. offspring networks trained much like would train newborn efﬁcient diverse network architectures achieving powerful modeling capabilities. important aspect evolutionary deep intelligence particular interesting worth deeper investigation genetic encoding scheme used mimic heredity signiﬁcant impact architectural traits passed generation generation thus impact quality descendant deep neural networks. effective genetic encoding scheme facilitate better transfer important genetic information ancestor networks allow synthesis even efﬁcient powerful deep neural networks next generation. such deeper investigation exploration incorporation synaptic clustering genetic encoding scheme potentially fruitful synthesizing highly efﬁcient deep neural networks geared improving memory storage requirements also tailored devices designed highly parallel computations embedded gpus. study introduce synaptic cluster-driven genetic encoding scheme synthesizing highly efﬁcient deep neural networks successive generations. achieved introduction multi-factor synapse probability model synaptic probability product probability synthesis particular cluster synapses probability synthesis particular synapse within synapse cluster. genetic encoding scheme effectively promotes formation synaptic clusters successive generations also promoting formation highly efﬁcient deep neural networks. methodology proposed genetic encoding scheme decomposes synaptic probability multi-factor probability model architectural traits deep neural network encoded probabilistically product probability synthesis particular cluster synapses probability synthesis particular synapse within synapse cluster. cluster-driven genetic encoding. network architecture deep neural network expressed denoting possible neurons denoting possible synapses network. neuron connected synapses neuron synaptic connectivity associated denoting strength. architectural traits deep neural network generation encoded conditional probability given architecture previous generation denoted treated probabilistic ‘dna’ sequence deep neural network. without loss generality based assumption synaptic connectivity characteristics ancestor network desirable traits inherited descendant networks instead encode genetic information deep neural network synaptic probability wkg− encodes synaptic strength synapse proposed genetic encoding scheme wish take consideration incorporate neurobiological phenomenon synaptic clustering probability synaptic co-activation increases correlated synapses encoding similar information close together dendrite. explore idea promoting formation synaptic clusters successive generations also promoting formation highly efﬁcient deep neural networks following multifactor synaptic probability model introduced ﬁrst factor models probability synthesis particular cluster synapses ¯sgc second factor models probability particular synapse within synaptic cluster speciﬁcally probability represents likelihood particular synaptic cluster ¯sgc synthesized part network architecture generation given synaptic strength generation example deep convolutional neural network synaptic cluster subset synapses kernel kernels within deep neural network. probability represents likelihood existence synapse within cluster generation given synaptic strength generation such proposed synaptic probability model promotes persistence strong synaptic connectivity offspring deep neural networks successive generations also procluster-driven evolutionary synthesis. seminal paper evolutionary deep intelligence shaﬁee synthesis probability composed synaptic probability mimic heredity environmental factor model mimic natural selection introducing quantitative environmental conditions offspring networks must adapt realization cluster-driven genetic encoding. study simple realization proposed cluster-driven genetic encoding scheme presented demonstrate beneﬁts proposed scheme. here since wish promote persistence strong synaptic clusters offspring deep neural networks successive generations probability synthesis particular cluster synapses ¯sgc modeled encodes truncation synaptic weight normalization factor make probability distribution p¯sgc|wg− truncation synaptic weights model reduces inﬂuence weak synapses within synaptic cluster genetic encoding process. probability particular synapse within synaptic cluster denoted expressed layer-wise normalization constant. incorporating aforementioned probabilities proposed scheme relationships amongst synapses well individual synaptic strengths taken consideration genetic encoding process. experimental results evolutionary synthesis deep neural networks across several generations performed using proposed genetic encoding scheme network architectures accuracies investigated using three benchmark datasets mnist stl- cifar lenet- architecture selected network architecture original ﬁrst generation ancestor network mnist stl- alexnet architecture utilized ancestor network cifar ﬁrst layer modiﬁed utilize kernels instead kernels given smaller image size cifar. environmental factor model imposed different generations study designed form deep neural networks progressively efﬁcient network architectures ancestor networks maintaining modeling accuracy. speciﬁcally formulated study offspring deep neural network total number synapses direct ancestor network. furthermore study kernel deep neural network considered synaptic cluster synapse probability model. words probability synthesis particular synaptic cluster modeled truncated summation weights within kernel. results discussion. study offspring deep neural networks synthesized successive generations accuracy offspring network exceeded better study changes architectural efﬁciency descendant networks multiple generations. table shows architectural efﬁciency versus modeling accuracy several generations three datasets. observed table descendant network generation mnist staggering ∼-fold efﬁcient original ﬁrst-generation ancestor network without exhibiting signiﬁcant drop test table architectural efﬁciency test accuracy different generations synthesized networks. gen. acc. denote generation architectural efﬁciency accuracy respectively. table cluster efﬁciency convolutional layers fully connected layer ﬁrst last reported generations deep neural networks mnist stl-. columns show overall cluster efﬁciency synthesized deep neural networks. accuracy trend consistent observed stl- results descendant network generation ∼-fold efﬁcient original ﬁrstgeneration ancestor network without signiﬁcant drop test accuracy also worth noting since training dataset stl- dataset relatively small descendant networks generations actually achieved higher test accuracies compared original ﬁrst-generation ancestor network illustrates generalizability descendant networks compared original ancestor network descendant networks fewer parameters train. finally case cifar different network architecture used descendant network generation network ∼.-fold efﬁcient original ancestor network drop test accuracy thus demonstrating applicability proposed scheme different network architectures. embedded ramiﬁcations. table shows cluster efﬁciency layer synthesized deep neural networks last generations cluster efﬁciency deﬁned study total number kernels layer original ﬁrst-generation ancestor network divided current synthesized network. observed mnist cluster efﬁciency last-generation descendant network result near .-fold potential speed-up running time embedded gpus reducing number arithmetic operations ∼.-fold compared ﬁrst-generation ancestor network though computational overhead layers relu lead reduction actual speed-up. potential speed-up lastgeneration descendant network stl- lower compared mnist dataset reported cluster efﬁciency last-generation descendant network finally cluster efﬁciency last generation descendant network cifar shown table results demonstrate proposed genetic encoding scheme promotes synthesis deep neural networks highly efﬁcient maintains modeling accuracy also promotes formation highly sparse synaptic clusters make highly tailored devices designed highly parallel computations embedded gpus. acknowledgments research supported canada research chairs programs natural sciences engineering research council canada ministry research innovation ontario. authors also thank nvidia hardware used study nvidia hardware grant program.", "year": 2016}