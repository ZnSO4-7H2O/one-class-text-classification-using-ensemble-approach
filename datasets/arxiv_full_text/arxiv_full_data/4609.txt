{"title": "On the Performance Bounds of some Policy Search Dynamic Programming  Algorithms", "tag": ["cs.AI", "cs.LG"], "abstract": "We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on Policy Search algorithms, that compute an approximately optimal policy by following the standard Policy Iteration (PI) scheme via an -approximate greedy operator (Kakade and Langford, 2002; Lazaric et al., 2010). We describe existing and a few new performance bounds for Direct Policy Iteration (DPI) (Lagoudakis and Parr, 2003; Fern et al., 2006; Lazaric et al., 2010) and Conservative Policy Iteration (CPI) (Kakade and Langford, 2002). By paying a particular attention to the concentrability constants involved in such guarantees, we notably argue that the guarantee of CPI is much better than that of DPI, but this comes at the cost of a relative--exponential in $\\frac{1}{\\epsilon}$-- increase of time complexity. We then describe an algorithm, Non-Stationary Direct Policy Iteration (NSDPI), that can either be seen as 1) a variation of Policy Search by Dynamic Programming by Bagnell et al. (2003) to the infinite horizon situation or 2) a simplified version of the Non-Stationary PI with growing period of Scherrer and Lesner (2012). We provide an analysis of this algorithm, that shows in particular that it enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a time complexity similar to that of DPI.", "text": "consider inﬁnite-horizon discounted optimal control problem formalized markov decision processes. focus policy search algorithms compute approximately optimal policy following standard policy iteration scheme \u0001-approximate greedy operator describe existing performance bounds direct policy iteration conservative policy iteration paying particular attention concentrability constants involved guarantees notably argue guarantee much better comes cost relative—exponential increase time complexity. describe algorithm non-stationary direct policy iteration either seen variation policy search dynamic programming bagnell inﬁnite horizon situation simpliﬁed version non-stationary growing period scherrer lesner provide analysis algorithm shows particular enjoys best worlds performance guarantee similar within time complexity similar dpi. study approximation dynamic programming algorithms inﬁnite-horizon discounted markov decision processes rich history ﬁrst important results gathered bertsekas tsitsiklis provide bounds closeness optimality computed policy function max-norm errors iterations. value policy iteration error known value policies generated algorithm close optimal policy errors small enough since distributions. possibility express right hand side respect ν-weighted norm comes price constant called concentrability coeﬃcient measures stochastic smoothness dynamics concentrate parts interestingly work —anterior munos —proposed approximate dynamic programming algorithm conservative policy iteration performance bounds similar equation constant is—as argue precisely paper —better others involves mismatch input measure algorithm baseline distribution corresponding roughly frequency visitation optimal policy natural piece information expert domain provide. main motivation paper emphasize importance concentrability constants regarding signiﬁcance performance bounds. section describe direct policy iteration simple dynamic programming algorithm proposed lagoudakis parr fern lazaric similar cpi; algorithm provide extend analysis developed lazaric consider section describe theoretical properties originally given kakade langford well bounds ease comparison dpi. particular corollary analysis obtain original bound practical variation uses ﬁxed stepsize. argue concentrability constant involved analysis better dpi. improvement quality unfortunately comes price number iterations required exponentially bigger dpi. motivate introduction another algorithm describe section non-stationary direct policy iteration either seen variation policy search dynamic programming bagnell inﬁnite horizon situation simpliﬁcation non-stationary growing period algorithm scherrer lesner analyze algorithm prove particular enjoys best worlds guarantee similar fast rate like dpi. next section begins providing background precise setting considered. tsitsiklis possibly inﬁnite state space ﬁnite action space probability kernel reward function bounded rmax discount factor. stationary deterministic policy maps states actions. write pπ|s) stochastic kernel associated policy value policy function mapping states expected discounted rewards received following state value clearly bounded vmax rmax/. well-known characterized unique ﬁxed point linear bellman operator associated policy γpπv. similarly bellman optimality operator maxπ unique ﬁxed point optimal value maxπ policy greedy w.r.t. value function greedy policies written finally policy optimal value equivalently paper focus algorithms approximate greedy operator takes input distribution function returns policy -approximately practice achieved l-regression so-called advantage function kakade langford kakade sensitive classiﬁcation problem lazaric case generating learning problems rollout trajectories induced policy considered algorithms provide bounds expected loss es∼µ using generated policy instead optimal policy distribution interest function errors iteration. though second bound involves errors instead value noted remark coeﬃcient better stated following corollary theorem implies asymptotic performance bound approched small number iterations. proximate greedy operator deciding whether stop not. furthermore uses adaptive stepsize generate stochastic mixture policies returned successive calls approximate greedy operator explains adjective conservative. remark motivation revisiting related fact constant appear soon analysis better algorithms like sense always moreover distribution always exists parameter might exist smallest coeﬃcient satisfying inequality. consider positive part claim trivial suﬃcient take dπ∗µ negative part shown considering deﬁned equal dirac measure state inﬁnite number actions result deterministic transition deﬁnition actions consequence though develop here seen concentrability coeﬃcients introduced approximate dynamic programming algorithms like value iteration farahmand modiﬁed policy iteration equal practice choice learning step algorithm conservative makes slow algorithm. natural solutions problem instance considered variation search-based structure prediction problems ones give here stepsize factor bigger description thus number iterations slightly better result stated terms error known advance input parameter kakade langford assume uniform bound errors known equal parameter iterations. latter solution call works indeed well practice signiﬁcantly simpler implement since relieved necessity estimate rollouts description process); indeed becomes almost simple except uses distribution dπkν conservative steps. since proof based generalization analysis thus speciﬁc properties turns results given straightforwardly specialized case algorithm. parameter directly controls rate cpi. furthermore sets suﬃciently small value recover nice properties theorem -corollary monotonicity expected value performance bound respect best constant though formalize here. summary corollary remark tell performance guarantee arbitrarily better though opposite true. this however comes cost signiﬁcant exponential increase time complexity since corollary states might number iterations scales coeﬃcient able slightly improve rate—by factor though still exponentially slower dpi. algorithm present next section best worlds enjoy performance guarantee involving best constant time complexity similar dpi. going describe algorithm ﬂavour similar sense step full step towards policy also conservative ﬂavour like sense policies evolve slowly. algorithm based ﬁnite-horizon non-stationary policies. write k-horizon policy makes ﬁrst action according second action according etc. value tπtπ write empty non-stationary policy. note inﬁnite-horizon policy begins denote value vσ... γkvmax. last algorithm consider named non-stationary direct policy iteration behaves builds non-stationary policy iteratively concatenating policies returned approximate greedy operator described algorithm devised nsdpi dpi-like simpliﬁed variation non-stationary dynamic programming algorithm recently introduced scherrer lesner non-stationary algorithm growing period. main diﬀerence nsdpi considers value policy loops inﬁnitely instead value ﬁrst steps here. following intuition values close other ended focusing nsdpi simpler. remarkably nsdpi turns almost identical older algorithm policy search dynamic programming algorithm bagnell however noted psdp introduced slightly diﬀerent control problem horizon ﬁnite considering inﬁnite-horizon problem. given problem horizon psdp comes guarantee essentially identical ﬁrst bound corollary requiring many input distributions time steps. main contribution respect psdp considering inﬁnite-horizon case managed require input parameter much milder assumption—and provide second performance bound respect better article described algorithms literature introduced nsdpi algorithm borrows ideas also close algorithmic connections psdp bagnell non-stationary algorithm growing period scherrer lesner figure synthesizes theoretical guarantees discussed algorithms. guarantee provide dependency performance bound concentrability coeﬃcients highlight bounds knowledge new. important message work usually hidden constants performance bounds matter. constants discussed sorted worst best follows knowledge ﬁrst time in-depth comparison bounds done hierarchy constants interesting implications beyond policy search algorithms focusing paper. matter fact several dynamic programming algorithms—avi scherrer ampi —come guarantees involving worst constant easily made inﬁnite. positive side argued guarantee arbitrarily stronger state-of-the-art algorithms. identiﬁed nsdpi algorithm similar nice property. furthermore observe nsdpi best time complexity guarantee. thus nsdpi turns overall best guarantees. technical level several bounds come pair fact introduced proof technique order derive bounds various constants. bound better since involves constant instead also enabled derive bounds worse terms assumed bagnell baseline distributions provide estimates optimal policy leading system distribution time steps then authors measure kind assumption essentially identical mismatch coeﬃcients concentrability assumption underlying constant measure main limitation analysis lies assumption considered along paper algorithms disposal \u0001-approximate greedy operator. general realistic since hard control directly quality level furthermore unreasonable compare algorithms basis since underlying optimization problems slightly diﬀerent complexities instance methods like look space stochastic policies moves space deterministic policies. digging understanding depth potentially hidden term \u0001—as done concentrability constants—constitutes natural research direction. last least practical side preliminary numerical experiments somesupport theoretical argument algorithms better concentrability constant preferred. simulations relatively small problems cpi+ nsdpi shown always perform signiﬁcantly better nsdpi always displayed least variability performed best average. refer reader appendix details. running analyzing similar experiments bigger domains constitutes interesting future work.", "year": 2013}