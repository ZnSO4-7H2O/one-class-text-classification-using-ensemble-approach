{"title": "Constrained Extreme Learning Machines: A Study on Classification Cases", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Extreme learning machine (ELM) is an extremely fast learning method and has a powerful performance for pattern recognition tasks proven by enormous researches and engineers. However, its good generalization ability is built on large numbers of hidden neurons, which is not beneficial to real time response in the test process. In this paper, we proposed new ways, named \"constrained extreme learning machines\" (CELMs), to randomly select hidden neurons based on sample distribution. Compared to completely random selection of hidden nodes in ELM, the CELMs randomly select hidden nodes from the constrained vector space containing some basic combinations of original sample vectors. The experimental results show that the CELMs have better generalization ability than traditional ELM, SVM and some other related methods. Additionally, the CELMs have a similar fast learning speed as ELM.", "text": "abstract—extreme learning machine extremely fast learning method powerful performance pattern recognition tasks proven enormous researches engineers. however good generalization ability built large numbers hidden neurons beneficial real time response test process. paper proposed ways named constrained extreme learning machines randomly select hidden neurons based sample distribution. compared completely random selection hidden nodes celms randomly select hidden nodes constrained vector space containing basic combinations original sample vectors. experimental results show celms better generalization ability traditional related methods. additionally celms similar fast learning speed elm. neural network architectures developed past several decades. feedforward neural networks especially popular ones studied times. proven learning capacity multilayer feedforward neural network non-polynomial activation functions approximate continuous function single hidden layer feedforward neural networks studied applied extensively researches model simplicity relatively high learning responding speed. learning capacity slfns inferior multilayer feedforward neural networks proved mainly three different ways train slfns gradient based learning methods. famous gradient based learning method back-propagation algorithm however methods face quite slow learning speed stack local minimal. although many assistant methods proposed solve problems levenberg-marquardt method dynamically network construction evolutionary algorithms generic optimization enhanced methods require heavy computation cannot obtain global optimal solution. optimization based learning methods. popular optimization based slfns support vector machine objective function optimize maximum margin corresponding structural risk minimization. solution obtained convex optimization methods dual problem space global optimal solution. popular method attracting many researchers least mean square based methods radial basis function network no-prop network based algorithm methods quite fast learning speed essence no-prop fast learning speed lms. recent years huang proposed novel extremely fast learning model slfns called extreme learning machine salient essences weights input layer hidden layer randomly generated firstly shown tamura huang completely proved random feature mapping theory rigorously. random nonlinear feature mapping hidden layer rest considered linear system therefore closed form solution simple network structure random hidden layer weights. essence linear system used minimize training error norm connection weights hidden layer output layer time hence good generalization performance according feedforward neural network theory consequence desirable features hidden layer parameters need tuned fast learning speed good generalization performance. additionally unified framework classification regression semi-supervised supervised unsupervised tasks advantages lead popularity researchers engineers however random selection hidden layer parameters makes quite inefficient hidden nodes usually randomly generate great number hidden nodes achieve desirable performance. leads time consuming test process helpful real applications. large numbers hidden nodes also easily make trained model sample extreme learning machine random extreme learning machine constrained mixed extreme learning machine based data-driven hidden layer features mapping ways. experimental results show that celms better generalization ability related methods related methods neural network whilst retaining fast learning characteristics elm. also compared celm algorithms related algorithms cifar- data results show celm algorithms outperform methods. remaining part paper organized follows section review traditional algorithm propose celms section experiments presented section conclusion discussion given section type slfns. hidden layer parameters i.e. connection weights input layer hidden nodes randomly generated elm. output layer linear system connection weights hidden layer output layer learned computing moore-penrose generalized inverse network extreme high learning speed simple network structure closed form solution. additionally randomness makes necessarily tune hidden layer parameters iteratively. online incremental learning methods hidden layer nodes dynamically methods randomly generate parts hidden layer nodes select candidate hidden nodes chunk chunk fixed varying chunk size. whether hidden layer node added usually depending objective function output layer. pruning methods select candidate hidden layer nodes methods start large neural network using traditional apply metrics statistical criteria multi-response sparse regression rank hidden nodes. finally eliminate relevance hidden nodes form compact neural network structure. gradient based methods update weights input layer hidden layer randomly initialize weights input layer hidden layer close-form least square solution calculate weights hidden layer output layer methods gradient descending method update weights input layer hidden layer iteratively. essence hidden layer functions data feature space output layer linear classifier separate data perfectly. therefore hidden layer extract discriminative features data representations classification tasks. probably commonly used method extract discriminative features. however traditional drawbacks number feature-mapped dimensions less number classes small sample size problem gaussian distribution assumption equal covariance different means. proposed projection pursuit based method overcome problems. method showed difference vectors between-class samples strong discriminative property classification tasks method rather complex many embedded trivial tricks. work balance high discriminative feature learning fast training speed propose novel model called constrained difference extreme learning machine utilizes random subset difference vectors between-class samples replace completely random connection weights input layer hidden linear combination sample vectors vectors within-class samples sample vectors classes vectors randomly selected sample vectors mixed vectors including difference vectors between-class samples vectors within-class samples connection weights input layer hidden layer validated. proposed constrained extreme learning machine generate great number hidden nodes meet desirable generalization performances. hidden nodes mean processing time easily fitting. problems solved. although method rather complex many embedded trivial tricks shows difference vectors between-class samples effective classification tasks. considering simplicity extreme high learning speed extend model constrained difference extreme learning machine constraining weight vector parameters randomly drawn closed difference vectors between-class samples instead open arbitrary vectors tackle problem generation discriminative hidden nodes. simple case illustrate idea difference vectors between-class samples. fig. illustration difference vectors between-class samples. generate difference vectors illustrated completely random connection weight vectors input layer hidden layer illustrated constrained random weight vectors celm illustrated essence weight vectors input layer neurons hidden layer original samples discriminative feature space spanned vectors samples classified. weight vectors helpful classification directions weight vectors number hidden nodes activation function associated connection weight input vector vector bias algorithm network summarized following three steps hidden nodes randomly generated values. however condition number random projected matrix large traditional model encounter ill-posed problems practice regularized term hidden layer’s output connection weights added optimization objective avoid problem solution regularized obtained analyzed theory verified simulation results classification tends achieve better generalization performance traditional svm. also overcome local minimal problem neural nets faced convex model structure. learning speed extremely fast time. section introduce celms idea using simple linear combination sample vectors generate hidden nodes traditional network structure. constrained difference extreme learning machine constrained difference extreme learning machine attempts extract discriminative features hidden layer. completely random parameters hidden layer always represent discriminative features. unconstrained random parameters make replace line paper identification number class class reversely illustrated fig. blocks figure represent samples class circles represent samples class comparison weight vectors input layer hidden nodes completely random without constraints illustrated fig. inferred weight vectors follow direction class class less discriminative classification tasks. reason hidden nodes efficient discriminative classification tasks. discriminative feature space elm. weight vectors input layer hidden layer cdelm illustrated fig. directions weight vectors close direction class class discriminative classification tasks intuitively. normalize difference vectors weights input layer hidden layer. reason normalize weights normalize introduced following discussion. originally constrained difference extreme learning machine deleted difference vectors small norms relevant difference vectors. somewhat time-consuming especially number hidden nodes large. although processes improve performance improvement little experiments. cdelm prior information samples’ class distribution utilized generate weights input layer hidden layer. split different classes’ samples different areas feature space. ideal case that example class mapped negative semi axis class mapped positive semi axis feature space. hence bias must middle point selected samples geometric sense intuitively. result biases hidden neurons determined assuming samples class mapped samples another class mapped respectively. samples drawn classes. denote weight vector input layer hidden neuron generated normalized factor. original data transformed feature mapping bias respect weight vector model. mapped assumption written discussion training algorithm cdelm concluded algorithm essence cdelm constrain hidden neuron’s input connection weights consistent directions class another class. random weights constrained chosen composed difference vectors between-class samples. randomly generate weight vectors input layer hidden layer differences between-class samples illustrated fig. difference vectors between-class samples samples higher constrained extreme learning machine constrained extreme learning machine utilizes vectors random chosen within-class sample vectors construct weights input layer hidden layer. cselm firstly randomly selects calculate within-class sample vectors assigned weights input layer hidden layer. biases used cselm also randomly generated uniform distribution elm. constrained vectors used firstly inspired difference vectors between-class samples. constrained vectors also considered derivative samples somewhat weaken affection noise samples cselm. discussion design selm algorithm algorithm essence cselm constrain hidden neuron’s input connection weights consistent directions derivative robust sample vectors. random weights constrained chosen composed vectors within-class sample vectors. algorithm training constrained extreme learning machine sample extreme learning machine utilizes sample vectors randomly drawn training construct weights input layer hidden layer. selm firstly randomly selects group sample vectors chosen sample vector. normalization sample transformed norm serve linear classification output layer. normalized sample vectors assigned weights input layer hidden layer. biases used selm randomly generated uniform distribution elm. hidden node weight input vector selected randomly sample vectors. actually polynomial kernel function tackle linearly inseparable cases data sets quadratic curve. difference kernel used kernel that samples kernel function support vectors. difference kernel selm kernel uses training samples. sigmoidal activation function used stretch kernel mapped data helps linear classification output layer. discussion write selm algorithm algorithm essence selm constrain hidden neuron’s input connection weights consistent directions sample vectors. original random weights constrained chosen training set. constrained mixed extreme learning machine utilizes mixed vectors containing class-constrained difference vectors class-constrained vectors construct weights input layer hidden layer. cmelm firstly generates half numbers hidden nodes whose weights biases constructed constrained vectors generates others whose weights biases constructed constrained difference vectors. constrained vectors normalized cselm constrained difference vectors normalized cdelm. normalized sample vectors assigned weights input layer hidden layer. constrained mixed vectors considered random extreme learning machine utilizes vectors random sample vectors regardless classes construct weights input layer hidden layer. rselm firstly randomly selects calculate vectors sample vectors normalized vectors assigned weights input layer hidden layer. biases used rselm also randomly generated uniform distribution elm. vectors random samples used accelerate speed hidden layer weights generation. discussion design rselm algorithm algorithm essence rselm constrain hidden neuron’s input connection weights consistent vectors random samples. section evaluate proposed celms compare classifiers related deep learning methods synthetic real-world datasets. rounds experiments conducted data set. experiment training test randomly generated using samples synthetic datasets database samples database normalized zero mean unit variance. performances recorded means standard deviations classification accuracies. experiments also compare celms orthogonal makes weight vectors orthogonal biases orthogonal other. comparison compare celms related methods appeared literature sufficiently. code used experiments downloaded following figures solid performance curve stands performance green solid curve stands orthogonal performance blue solid curve stands cdelm performance blue dashed curve triangle markers stands selm performance brilliant blue solid curve stands cselm performance brilliant blue dashed curve triangle markers stands rselm performance black solid curve stands cmelm performance. software used experiments matlab microsoft windows serve operation system. configuration hardware intel xeon .ghz. total server experiments cannot take much users’ usage. experiments synthetic dataset first evaluate celm algorithms synthetic dataset spiral data. illustrated fig. retain symmetrical shape spiral normalize samples range total number generated spiral data thirds data samples used training rest used test set. data samples randomly drawn original spiral data samples. sets randomly generated total rounds experiments. shown fig. celms perfect performance number hidden nodes reaches slight larger orthogonal drops number hidden nodes larger drop orthogonal probably orthogonalization vectors dimensions help even degrades information random weights. test accuracy reaches even number hidden nodes test accuracies celms orthogonal time. result shows celms better generalization abilities orthogonal elm. besides variances celms becoming quite small increase hidden nodes number little difference among performances celms. therefore difference vectors sample vectors really work performance comparison celms orthogonal elm. data used test set. compare methods regularized regularized orthogonal network linear methods. mentioned optimization objectives orthogonal celms used benchmark evaluation added regularized term. three-fold cross validation used select best regularization factor space step number hidden nodes used orthogonal celms network step five data sets. wdbc data number hidden nodes used step selection hidden nodes number based fig. although unfair methods converged completely proposed methods improve efficiency hidden neurons. kernels used used linear svm. cost factor used selected regularization factor elm. performances rounds experiments recorded. mean test accuracies training times recorded table best test accuracies represented bold face. seen table performances celms outperform methods. celms improve performance significantly whilst retaining extremely high learning speed elm. experiments large scale datasets also evaluate celms large size datasets i.e. mnist cifar- mnist database handwritten digits contains training samples test samples. consists binary images classes size digits pixels. samples normalized input celms elm. cifar- dataset contains color images classes image class. training test consist images images respectively. cifar- dataset standard evaluation pipeline defined adopted. first extract dense local patches whitening stride second encode. codebook threshold coding trained omp- codebook size experiment. third average-pool features grid form global image representation. performances celms orthogonal illustrated fig. experiment methods implemented without regularized terms sake evaluating efficiency hidden neurons sufficiently. experiments datasets datasets database including wisconsin diagnostic breast cancer dataset digit datasets five different features used evaluating proposed celms. celms compared orthogonal elm. first experiment number training samples size total samples rest used test samples. training test sets randomly generated. comparison celms orthogonal first experiment illustrated fig. performances models displayed sufficiently trends curves figure. seen test accuracy curves celms methods experiment generalization ability proposed celms better models real world datasets. samples’ distribution prior introduced makes efficient hidden nodes celms really helps classification tasks. fig. learned performances celms percentages higher averagely datasets. efficiency sample vectors based weighting adopted selm evaluated follows. numbers vectors used method higher test accuracy means efficient. fig. performances celms higher orthogonal converged. gaps performances celms orthogonal show sample vector based weighting effective. curve celms’ performance always orthogonal datasets shows sample vectors based weighting really helps efficient hidden nodes. experiments conducted mnist data compare celms orthogonal multi-layer extreme learning machine ml-elm stacked denoising auto encoder model based elm. random feature mapping used encoder linear system used decoder. original ml-elm three layers hidden nodes outperform deep learning methods deep belief network deep boltzmann machine stacked auto encoder sdae experiment network structure ml-elm limit machine capacity. number hidden nodes used orthogonal celms evaluation. experiments regularization term used orthogonal celms. parameter selection before. average performance recorded table table learned number celms’ hidden nodes test accuracies celms comparable ml-elm celms much high learning speed. number celms’ hidden nodes ml-elm implemented version. learning speed celms slower computation complexity matrix inversion. significant performance celms similar learning speed celms conclude sample based features used celms really help improvement whilst retaining fast learning property test accuracy training time test accuracy training time test accuracy training time test accuracy training time test accuracy training time test accuracy training time test accuracy training time test accuracy training time test accuracy training time table celms found better performances linear-svm rsvm. celms test accuracies least percentages higher linear svm. rsvm layers layer linear random projection sigmoid transformation. although rsvm many layers celms hidden layer test accuracy least percentages higher discriminative deep learning methods. besides celms train model case training data well suggests proposed celms tackle large scale data effectively. understand feature mapping orthogonal celms last experiment conducted visualization hidden layer’s feature mapping methods mnist data set. visualization method t-sne used. t-sne ideal visualization tool preserving local structure overcoming crowding problem mapped data. s-sne much better visualization effect methods auto encoder mnist data set. experiment whole training data used training data randomly selected test data used test t-sne. number hidden nodes orthogonal celms visualization illustrated fig. seen figure celms retain data structure well pre-assigned iteration number. hidden layer’s feature mapping celms separate data different classes well. however transformation hidden neurons orthogonal input data converge many t-sne visualization. phenomenon probably reveals essence constrained random mapping hidden neurons outperform traditional elms. also provides insights related research fast generation meaningful hidden neurons could boost elm’s performance greatly. celms also compared related methods e.g. linear rsvm deep related method e.g. cifar- dataset. rsvm deep learning model building block linear model. outputs previous layers transformed random matrix transformed outputs added original features. modified features input next layer transformed sigmoid function. also deep learning model building block based model parts hidden nodes built random projection part hidden nodes built weights instead adds output previous layer bias next layer outputs previous layers concatenated original features sequentially input next layer model. experiment training samples used train model test samples used evaluate performance. table shows performances orthogonal celms linear-svm rsvm methods. note performances linear rsvm cited experimental conditions orthogonal celms features number used training test sets research include study invariant feature generating improvement celms experimental verification celms’ application regression problems. analyses kinds problems celms work related theories also expected studied future. authors would like thank kasun zhou prof. g.-b. huang nanyang technological university singapore prof. vong university macau macau kindly help multi-layer extreme learning machine g.-b. huang h.a. babri upper bounds number hidden neurons feedforward networks arbitrary bounded nonlinear activation functions ieee trans. neural networks vol. werbos beyond regression tools prediction analysis behavioral sciences ph.d. thesis harvard university cambridge moré levenberg-marquardt algorithm implementation theory numerical analysis springer berlin heidelberg lowe adaptive radial basis function nonlinearities problem generalization first international conference artificial neural networks october widrow greenblatt park no-prop algorithm learning algorithm multilayer neural networks neural networks vol. g.-b. huang q.-y. c.-k. siew extreme learning machine theory applications neurocomputing vol. dec. http//www.ntu.edu.sg/home/egbhuang/elm_ random_hidden_nodes.html]. g.-b. huang q.-y. c.-k. siew extreme learning machine learning scheme feedforward neural networks proceedings international joint conference neural networks vol. jul. compared celms several based methods normalized orthogonal elm. main contributions celms study introduce constrained hidden weights based sample distributions normalized hidden weights square norms norm several work validated effectiveness celms experiments celms observe number hidden nodes small celms outperform normalized orthogonal greatly. however number hidden nodes large margins normalized big. observation constrained weights normalized strategy work success celms. hand number hidden nodes small hidden layer equivalent dimension reduction. thus directions hidden weights quite import since represent directions retained reduction. weights based sample distribution work well dimension reduction hand normalization hidden weights important property sigmoid activation function. effective response area sigmoid function near zero. weights normalized dimension inputs high absolute values many elements wx+b big. thus normalization quite import. strongly recommend operator added research future. address inefficient hidden nodes paper proposed novel learning models celms. celms constrain random weights’ generation smaller space compared i.e. replacing completely random weight vectors ones randomly drawn simple linear combination sample vectors. main contribution celms introduces sample distribution prior construction hidden layer make better feature mapping benefit next layer’s linear classification. effective feature mapping greatly contributes efficient hidden nodes elm. extensive comparisons celms related methods synthetic real-world datasets showed celms better performances almost cases. however celms still problems typical owned. celms face fitting problem number hidden nodes large although celms improve effective discriminative hidden nodes. relief methods tackle problem effectively. another problem solving weights hidden layer output layer time-consuming number hidden nodes large. case much common large scale applications. lecun bottou bengio haffner gradient-based learning applied document recognition proceedings ieee vol. available http//yann.lecun.com/exdb/mnist. hinton osindero fast learning algorithm deep belief nets neural computation vol. vincent larochelle lajoie bengio manzagol stacked denoising autoencoders learning useful representations deep network local denoising criterion journal machine learning research vol. roweis saul nonlinear dimensionality reduction locally linear embedding science vol. miao qing extreme support vector regression proceedings international conference extreme learning machines beijing china spring-verlag oct. mcdonnell tissera schaik \"fast simple accurate handwritten digit classification using extreme learning machines shaped input-weights\" arxiv preprint arxiv. h.-j. rong g.-b. huang sundararajan saratchandran online sequential fuzzy extreme learning machine function approximation classification problems ieee transactions systems cybernetics part cybernetics vol. z.-l. k.-f. t.-m. choi neuro-fuzzy inference system integration fuzzy logic extreme learning machines ieee transactions systems cybernetics part cybernetics vol. n.-y. liang g.-b. huang saratchandran sundararajan fast accurate online sequential learning algorithm feedforward networks ieee transactions neural networks vol. miao qing constrained extreme learning machine novel highly discriminative random feedforward neural network proceedings international joint conference neural networks", "year": 2015}