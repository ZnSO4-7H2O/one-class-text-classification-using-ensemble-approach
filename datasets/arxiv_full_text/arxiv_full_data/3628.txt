{"title": "Exponential Discriminative Metric Embedding in Deep Learning", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "With the remarkable success achieved by the Convolutional Neural Networks (CNNs) in object recognition recently, deep learning is being widely used in the computer vision community. Deep Metric Learning (DML), integrating deep learning with conventional metric learning, has set new records in many fields, especially in classification task. In this paper, we propose a replicable DML method, called Include and Exclude (IE) loss, to force the distance between a sample and its designated class center away from the mean distance of this sample to other class centers with a large margin in the exponential feature projection space. With the supervision of IE loss, we can train CNNs to enhance the intra-class compactness and inter-class separability, leading to great improvements on several public datasets ranging from object recognition to face verification. We conduct a comparative study of our algorithm with several typical DML methods on three kinds of networks with different capacity. Extensive experiments on three object recognition datasets and two face recognition datasets demonstrate that IE loss is always superior to other mainstream DML methods and approach the state-of-the-art results.", "text": "remarkable success achieved convolutional neural networks object recognition recently deep learning widely used computer vision community. deep metric learning integrating deep learning conventional metric learning records many ﬁelds especially classiﬁcation task. paper propose replicable method called include exclude loss force distance sample designated class center away mean distance sample class centers large margin exponential feature projection space. supervision loss train cnns enhance intra-class compactness inter-class separability leading great improvements several public datasets ranging object recognition face veriﬁcation. conduct comparative study algorithm several typical methods three kinds networks diﬀerent capacity. extensive experiments three object recognition datasets face recognition datasets demonstrate loss always superior mainstream methods approach state-of-the-art results. recently convolutional neural networks continuously setting records classiﬁcation aspect object recognition scene recognition face recognition estimation facing complex data deeper wider cnns tend obtain better accuracies. meanwhile many troubles show gradient saturating model overﬁtting parameter augmentation etc. solve ﬁrst problem non-linear activations proposed. considerable eﬀorts made reduce model overﬁtting data augmentation dropout regularization besides model compressing methods largely reduced computing complexity original models performance improved simultaneously. general object recognition scene recognition estimation identities possible testing samples within training set. training testing sets object classes images. case softmax classiﬁer often used designate label input. face recognition deeply learned features need separable also discriminative. roughly divided aspects namely face identiﬁcation face veriﬁcation. former object recognition training testing sets face identities aims classifying input image large number identity classes. face veriﬁcation classify pair images belonging identity since impractical precollect enough number possible testing identities training face veriﬁcation becoming mainstream ﬁeld. clariﬁed deepid series classifying identities simultaneously instead binary classiﬁers training make learned features discriminative diﬀerent classes. decide joint supervision softmax classiﬁer metric loss function train veriﬁcation signal feature similarity discriminant test shown section fig. illustrates general face recognition pipeline maps input images discriminative deep features progressively predicted labels. recent trend towards deep learning discriminative features reinforce cnns better metric loss functions namely deep metric learning intra-class compactness inter-class separability simultaneously maximized. inspired idea many metric learning methods proposed. traced back early subspace face recognition methods linear discriminant analysis bayesian face uniﬁed subspace example aims maximizing ratio inter-class intra-class variations ﬁnding optimal projection direction. metric learning methods proposed project original feature space another metric space features identity close different identities stay apart. subsequent contrastive loss triplet loss witnessed success face recognition. interestingly closely related learning hash major solutions nearest neighbor search problem. given high dimensionality high complexity multimedia data cost ﬁnding exact nearest neighbor prohibitively high. learning hash datadependent hashing approach aims learn hash functions speciﬁc dataset nearest neighbor search result hash coding space close possible search result original space signiﬁcantly improving search eﬃciency space cost. main methodology learning hash similarity preserving i.e. minimizing similarities computed original space similarities hash coding space various forms. utilizes linear trace ratio criterion learn hash functions pseudo labels hash codes jointly learned. proposes semi-supervised deep learning hashing method fast multimedia retrieval simultaneously learn good multimedia representation hash function. comprehensive survey dimension reduction using diﬀerent similarity preserving algorithms hashing found surprisingly similarity metric loss functions could used learning hash. large scale training unreasonable address iteration. mini-batch based stochastic gradient descent algorithm doesn’t reﬂect real distribution total training superior sampling strategy becomes important training process. besides selecting appropriate pairs triplets like previous dramatically increase number training samples. result inevitably hard converge optimum steadily. paper propose novel well-generalized metric loss function named include exclude loss make deeply learned features discriminative diﬀerent classes closer images class. idea veriﬁed fig. section obviously inter-class distance away intra-class distance large margin. training learn center class like center loss does. subsequently show center loss variant special case method. another parameter regularize distance features corresponding class centers. furthermore hyperparameter control number valuable inter-class distances accelerate convergence model. simultaneously supervision signals softmax loss loss train network. extensive experiments object recognition face veriﬁcation validate eﬀectiveness loss. method signiﬁcantly improves performance compared original softmax method competitive nowadays mainstream algorithms. main contributions summarized follows enforcing mean inter-class distance larger intra-class distance margin exponential feature projection space opposed distance sample nearest cluster centers magnet loss avoiding large intra-class distances. recent years deep learning successfully applied computer vision domains object recognition face recognition image retrieval speech recognition natural language processing time deep learning models prone deeper wider. complicated deep networks accompanied larger training model overﬁtting costly computational overhead. considering these produce methods concatenate conventional metric learning losses deeply learned features. classiﬁcation aspect generally aims mapping originally learned features discriminative feature space maximizing inter-class variations minimizing intra-class variations. degree properly chosen metric loss function would make training easy converge optimal model without much training data. brieﬂy discuss typical methods below. encourage faces identity projected onto single point embedding space. ensemble networks diﬀerent face patches ﬁnal concatenated features. joint bayesian classiﬁer used achieve ﬁnal performance lfw. loss function mainly based idea contrastive loss minimizes intra-class distance enforces inter-class distance larger ﬁxed margin. schroﬀ employ triplet loss stems lmnn encourage distance constraint similar contrastive loss. diﬀerently triplet loss requires triple training samples input time pair. triplet loss minimizes distance anchor sample positive sample maximizes distance anchor sample negative sample order make inter-class distance larger intra-class distance margin relatively. also largest training database face images insurmountable record rippel propose novel magnet loss explicitly designed maintain distribution diﬀerent classes feature space. terms computational performance alleviates training ineﬃciency traditional triplet loss veriﬁed classiﬁcation task attribute concentration. complicated oﬀ-line sampling strategy makes diﬃcult reproduce. addition intra-class distribution maintaining local clusters would impair inter-class separability general classiﬁcation tasks especially face recognition. assume training consists input-label pairs yn}m belonging classes. consider parameterized model parameters. work transformation selected complex architectures. deﬁne class label feature corresponding class center. section existing superior methods ﬁrst presented. triplet loss schroﬀ veriﬁed eﬀectiveness triplet loss large training set. exponentially increased computational complexity training examples diﬃculty convergence impede general application. formula follows achieve ﬂexible learning objective adjustable diﬃculty altering classiﬁcation angle margin classes. although relatively rigorous learning objective adjustable angle margin avoid overﬁtting diﬃcult convergence hinders generalization many deep networks. crucial continuously adjust component weight softmax l-softmax guarantee progressing training. weight matrix fully connected layer softmax layer yi-th column angle corresponding weight vector integer control learning objective. meanwhile must monotonically decreased satisfy center loss propose loss function regards distance sample away corresponding class center objective penalization. joint supervision center loss softmax loss makes approach outperform existing best results face recognition benchmark databases. clariﬁed magnet loss liberates unreasonable prior target neighbourhood assignments divides class several clusters aims maintaining distributions diﬀerent classes representation space. result similar samples diﬀerent classes closer classes. speciﬁcally intra-class variations larger inter-class variations object recognition face recognition. thus local distribution maintaining loss functions like magnet loss bring many beneﬁts practical classiﬁcation tasks. despite great performance triplet loss googlenet training ineﬀectiveness exponentially increased training samples hinder widespread application generic classiﬁcation tasks. considering diﬃculty magnet loss reproduce disadvantages mentioned above propose replicable method called loss learn discriminative features. calculate distances sample class centers mini-batch take advantage batch information compared pair/triplet samples like previous. objective initially deﬁned follows {·}+ hinge loss function predeﬁned margin hyperparameter variance examples away respective class centers feature space. training class center variance update together deep feature means entire training iteration. obviously impractical. decide employ mini-batch based algorithm update parameters. denominator part computed summing inter-class distances sample class centers appear mini-batch. approach seems natural choice probability interpretation softmax loss. existing similar methods express sample quite away corresponding class center vanish term objective approximating denominator equation small number nearest classes. variance standardization also renders objective invariant characteristic length scale problem. whereas beneﬁts based superb neighborhood sampling strategy class keep local distribution. diﬀerent strategy exploited sampling nearest clusters class decide nearest class centers obtain objective. improved objective loss function formulated follows eﬀectively selected number diﬀerent inter-class distances sample class centers mini-batch distances sorted ascending order. choose proper according diﬀerent training datasets acquire best performance. notice sophisticated oﬀ-line nearest clusters sampling strategy avoided mini-batch based works well training. besides large inter-class distances removed accelerate convergence especially valid datasets many classes. subsequent results show proposed method greatly improve training eﬃciency without sacriﬁcing speed since auxiliary loss layers removed classiﬁcation step. clear formula variant eﬃcient center loss triplet loss. loss function seems appropriate reﬂect characteristics proposed method. apparently forces minimum inter-class distance larger intra-class distance margin figure visualization deeply learned features training testing sets mnist regarding softmax loss l-softmax loss center loss loss respectively. points diﬀerent colors correspond features diﬀerent classes. eﬀectiveness method shown fig.. visualization features training testing sets suﬃciently reﬂects relative intraclass compactness inter-class separability loss compared softmax loss. also l-softmax loss obviously ampliﬁes angle features diﬀerent classes center loss seriously shrinks intraclass distances deeply learned features discriminative small subspace. considering classical back-propagation algorithm entire parameter updating process loss summarized algorithm softmax loss incorporated accelerate converge training process. weighting parameter softmax loss loss ﬁnal objective keep balance supervision symbols. concrete implementation details given section section three kinds cnns diﬀerent capacity given validate eﬀectiveness algorithm object recognition databases cifar cifar experiments face recognition databases also performed section caﬀe library implement experiments speedparallel computing technique tesla gpus exploited. networks part based existing cnns. partition three classes lighter normal powerful. refer respective notations following experiments. normal networks shown table table inspired also powerful ones similar adopt relu default activation function except table prelu used. weight decay momentum note mean subtraction image preprocessing performed mentioned. normally used works well training. lighter networks known structures built caﬀe library comply original setings. cases entire inter-class distances mini-batch speciﬁed. joint supervision softmax loss loss necessary accelerate convergence training process. testing softmax classiﬁer used object recognition cosine similarity metric computed obtain face veriﬁcation accuracies. fair comparison train four kinds models experiment namely supervision softmax loss softmax loss l-softmax loss softmax loss center loss softmax loss loss. simplicity refer four original loss names corresponding methods. details every experiment training setups presented respective subsections subsequently. experiments single model used achieve ﬁnal performance. table normal architectures diﬀerent benchmark datasets. conv.x conv.x conv.x denote structures contain multiple successive convolutional layers. batch normalization used networks. generalization algorithm. lighter lenet included caﬀe library. train according default updating strategy learning rate parameter initialization eventually terminate normal depicted table model trained batch size learning rate started divided iterations eventually terminated iterations. experiments preprocess images dividing provide range inputs. existing best results compared methods shown table obvious loss outperforms methods setings also among performance compared state-of-the-art methods. cifar dataset classes objects training testing. experiments three cnns carried here. lighter cifar network built caﬀe library. updating strategy initialization parameters follow original settings. normal depicted table start learning rate divide iterations eventually terminate iterations. simple mean/std normalization horizontal ﬂips used preprocess dataset. powerful wrn-- illustrated diﬀerences. wrn-- network said achieve comparable accuracy layers resnet cifar. speed training process ﬁne-tune three compared methods softmax baseline model. experiment dataset preprocessed global contrast normalization mean/std normalization. follow standard data augmentation training batch size results listed table observe method always achieves best performance among four compared methods regardless size cnns. cifar ﬁnal part section verify eﬀectiveness loss cifar dataset. dataset like cifar except classes containing images class training testing. classes cifar grouped superclasses. image comes label coarse label former protocol here. convention normal network shown table powerful wrn--. also training strategy described cifar. powerful wrn-- ﬁne-tune three compared methods softmax baseline model. diﬀerently better inspect eﬀectiveness compared methods capacity networks growing preprocess dataset normal powerful networks simple mean/std normalization horizontal ﬂips augment data. table clearly method consistently performs better compared approaches. results presented above loss always achieves best results among four compared methods three object recognition datasets. speciﬁcally performance center loss l-softmax loss ﬂuctuates signiﬁcantly diﬀerent network structures. fig. training testing process cifar cifar normal cnns displayed. seen convergence rate loss comparable compared loss functions avoiding notoriously slow convergence triplet loss. considering performance training testing observe loss mitigate serious overﬁtting softmax loss diﬃcult convergence l-softmax loss. testing accuracies method diﬀerent best settings normal networks shown appendix diﬀerent object recognition face veriﬁcation compute feature similarity images threshold comparison exploited decide whether person not. speciﬁcally softmax classiﬁer metric loss functions jointly supervise training process cosine similarity features used obtain testing accuracy section evaluate approach face veriﬁcation figure general pipeline face veriﬁcation paper classiﬁer loss function used train similarity discriminant used obtain ﬁnal veriﬁcation accuracy. datasets. face datasets recognized benchmarks face image video respectively. publicly available casia-webface training originally labeled face images individuals. removing images failing detect mislabeled resulting dataset training images. cropped faces images detected facial landmarks labeled globally align face images similarity transformation normal network depicted table reduced version resnet convolutional layers. input faces cropped normalized subtracting mean image dividing start training learning rate divide iterations terminate iterations. face images using wider resnet fewer layers like wrn-- bring many beneﬁts accompanied rapidly growing memory space. decide widen network listed table obtain powerful one. speciﬁcally widen convolutional layers conv conv widening factor testing extract features frontal face mirror image merge features element-wise summation. evaluations based similarity scores image table normal resnet architecture used face veriﬁcation. resblock classical residual unit consists consecutive convolutional layers unit mapping. layer conv conv pool resblock conv pool resblock resblock conv pool resblock resblock resblock resblock resblock conv pool resblock resblock resblock convolution convolution pooling convolution convolution pooling convolution convolution convolution pooling convolution convolution convolution convolution convolution convolution pooling convolution convolution convolution considering diﬀerence previous experiments select ﬁrst inter-class distances every mini-batch calculate objective here. reason datasets like casia-webface many subjects inter-class distances tend large method thus leading diﬃcult convergence training process. fig. shows veriﬁcation accuracies ranging number inter-class distances. importance choosing proper displayed clearly. here regard case original softmax method. dataset contains face images diﬀerent identities internet large variations pose expression illumination. comparison purpose algorithms typically report mean face veriﬁcation accuracies curves given face pairs following standard protocol unrestricted labeled outside data according previous experience properly chosen balances figure veriﬁcation accuracies loss diﬀerent using normal network number inter-class distances regarding sample mini-batch. face veriﬁcation accuracies loss diﬀerent using normal network. weight softmax loss loss improve performance. experiment method across wide range select best setting. results shown fig. seen loss stable diﬀerent best setting fig. illustrates veriﬁcation accuracies loss functions diﬀerent similarity metrics testing. results show cosine similarity suitable similarity feature representations. obviously method robust cases always achieves best performance. dataset consists videos diﬀerent people average videos everyone. besides average length video clip frames clip duration varying frames frames. experiments report results video pairs table according unrestricted protocol labeled outside data also fig. shows accuracy loss regard diﬀerent ranging curves compared loss functions. loss always outstanding loss functions small training dataset casia-webface competitive state-of-the-art methods using larger training datasets model ensemble. noticeably results triplet loss l-softmax loss satisfactory exhibits large margin triplet loss compared results convincingly demonstrates diﬃcult convergence data dependence triplet loss. conjecture maybe performance method improved considerably larger training powerful network used. anyway excellent performance undoubtedly verify great generalization loss. visualization datasets shown fig. paper propose powerful replicable method enforces mean inter-class distance larger intra-class distance margin enhance discriminability deeply learned features object recognition face veriﬁcation. extensive experiments several public datasets convincingly demonstrated eﬀectiveness method. results also exhibit excellent generalization loss various size cnns. instead requiring superior neighborhood sampling strategy approach uses mini-batch based conduct experiments avoiding exponentially increased computational complexity image pairs triplets. maybe better hard sample mining strategy could improve performance further. inspired outstanding performance loss object recognition face recognition explore extension case swarm intelligent methods exploited figure examples datasets experiments. image pairs positive pairs method succeeds recognize softmax method fails. likewise green ones negative pairs. networks details. experiments part obey following steps. first vary according corresponding range diﬀerent databases. then best setting previous results vary ﬁnal optimal setting. optimal values displayed bold.", "year": 2018}