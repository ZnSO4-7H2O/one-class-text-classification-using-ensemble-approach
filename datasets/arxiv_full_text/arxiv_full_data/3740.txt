{"title": "Approximate Ranking from Pairwise Comparisons", "tag": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML"], "abstract": "A common problem in machine learning is to rank a set of n items based on pairwise comparisons. Here ranking refers to partitioning the items into sets of pre-specified sizes according to their scores, which includes identification of the top-k items as the most prominent special case. The score of a given item is defined as the probability that it beats a randomly chosen other item. Finding an exact ranking typically requires a prohibitively large number of comparisons, but in practice, approximate rankings are often adequate. Accordingly, we study the problem of finding approximate rankings from pairwise comparisons. We analyze an active ranking algorithm that counts the number of comparisons won, and decides whether to stop or which pair of items to compare next, based on confidence intervals computed from the data collected in previous steps. We show that this algorithm succeeds in recovering approximate rankings using a number of comparisons that is close to optimal up to logarithmic factors. We also present numerical results, showing that in practice, approximation can drastically reduce the number of comparisons required to estimate a ranking.", "text": "common problem machine learning rank items based pairwise comparisons. ranking refers partitioning items sets pre-speciﬁed sizes according scores includes identiﬁcation top-k items prominent special case. score given item deﬁned probability beats randomly chosen item. finding exact ranking typically requires prohibitively large number comparisons practice approximate rankings often adequate. accordingly study problem ﬁnding approximate rankings pairwise comparisons. analyze active ranking algorithm counts number comparisons decides whether stop pair items compare next based conﬁdence intervals computed data collected previous steps. show algorithm succeeds recovering approximate rankings using number comparisons close optimal logarithmic factors. also present numerical results showing practice approximation drastically reduce number comparisons required estimate ranking. problem ranking collection items noisy pairwise comparisons arises wide range applications including recommender systems rating movies books consumer items peer grading ranking students massive open online courses ranking players tournaments; search engines; quantifying people’s perception cities pairwise comparison street views cities online sequential survey sampling assessing popularity proposals population voters applications obtain statistically sound ranking comparisons possible. work investigate power adaptively selecting pairs compare based outcomes previous comparisons setting call active adaptive contrast passive non-adaptive ranking approaches comparisons make ranking. data collected. well understood typically learn ranking using fewer adaptively chosen comparisons would need passively choosing comparisons however moderately large large collections items–such ones appear applications mentioned above–or collections many items similar quality learning exact ground-truth ranking still require prohibitively many comparisons. motivated large-scale ranking problems work studies problem adaptivity obtaining approximate rankings. demonstrate learning approximate ranking still statistically tractable even recovering exact ranking not. formally consider collection items make comparison queries pairs items apart intuitive appeal borda scores generalize orderings considered several popular comparison models including classical parametric bradley-terry-luce thurstone models well non-parametric strong stochastic transitivity model models intrinsic model-deﬁned ordering coincides exactly ranking items according exact score paper considers problem approximately partitioning items sets pre-speciﬁed sizes according respective scores. includes ﬁnding total ordering approximately correct task ﬁnding items close top-k items. simplicity exclusively focus latter problem paper. contributions main contribution present analyze novel active ranking algorithm estimating approximate ranking items. algorithm based adaptively estimating scores within suﬃcient resolution deduce ranking. establish high probability algorithm returns ranking satisﬁes desired approximation guarantee attains distribution-dependent sample complexity parameterized terms prove distribution-dependent lower bounds match upper bound logarithmic factors many problem instances. analysis leverages fact related particular class multi-armed bandit problems connection observed context ﬁnding item since best knowledge approximate subset selection problem studied bandit literature version algorithm results also specialized multi-armed bandit problem. finally examine pathological distributions complexity approximate ranking seems diverge would expect. cases show careful randomized guessing strategies yield signiﬁcant improvements sample complexity. motivation approximate rankings order understand approximation drastically reduce number comparisons required consider motivating example. suppose interested identifying top-k items suppose simplicity items ordered i.e. paper shows active setting number comparisons necessary suﬃcient ﬁnding scores often obey scaling average identify top-k items exactly aforementioned optimal active scheme would require order comparisons minimax-optimal passive ranking scheme would even require order comparisons drastically speciﬁcally factor proportional particular want items elements among true items overall number comparisons required would order thus relaxing approximate ranking yield speedups linear quadratic number items compared optimal exact active exact passive schemes. moreover algorithm obtains factor-of-h speedup instead learns near-optimal related works vast literature ranking estimation pairwise comparison data; however work focuses ﬁnding exact rankings. number papers devoted settings pairs compared chosen priori whereas assume pairs chosen active manner. moreover several works impose restrictions pairwise comparison probabilities e.g. assuming bradley-terry-luce parametric model eriksson considers problem ﬁnding items using graph-based techniques whereas busa-fekete consider problem ﬁnding top-k items. ailon considers problem linearly ordering items disagree pairwise preference labels possible. work also related literature multi-armed bandits discussed later paper. given collection items denote probability item wins comparison item denote bernoulli random variable taking value beats otherwise moreover require comparison results winner +mji item recall score deﬁned j∈\\{i} corresponds probability item wins comparison item chosen uniformly random \\{i}. denote permutation words denotes item largest score. ranking corresponds partitioning items disjoint sets according scores. simplicity paper focus ranking problem splitting figure estimated scores three diﬀerent domains scores association tennis professionals world tour computed games played within week interval fraction games total number games played. comparisons proposals planyc survey reported paper rated least times depicted). scores comparisons gif’s according whether display certain emotion approximate partition items disjoint sets active comparisons. time instant algorithm compare arbitrary items algorithm select based outcomes previous comparisons. comparing algorithm obtains independent draw random variable response. algorithm terminates based parameter ranking algorithm -accurate pairwise comparison matrix ranking returned h-hamming accurate probability least moreover uniformly -accurate given pairwise comparison models δ-accurate exact version ranking problem considered paper related subset selection problem bandit literature speciﬁcally multi-armed bandit model consists arms random variable unknown distribution. subset selection problem concerned identifying arms taking independent draws random variables. various works observed that deﬁnition score comparing item item chosen uniformly random \\{i} modeled drawing bernoulli random variable mean subsequent analysis relies relation. ignoring fact means coupled must realized pairwise algorithm turns near-optimal even though take constraints account. seems corroborate observation many types constraints surprisingly improve sample complexity bandit problems. finally least best knowledge problem approximate subset selection studied bandit literature meaning algorithm results also specialized multi-armed bandit problem. however noted versions approximation considered literature; instance zhou studied problem selecting arms aggregate regret deﬁned average reward optimal solution solution given algorithm. section introduce family parametric models popular pairwise comparison literature focus parametric models section show that perhaps surprisingly pairwise comparison probabilities bounded away zero constellations scores assumptions provide little gains sample complexity. member family deﬁned strictly increasing continuous function obeying function assumed known. pairwise comparison matrix family associated unknown vector entry represents quality strength corresponding item. parametric model cpar popular examples models family bradley-terry-luce model obtained setting equal sigmoid function +e−t thurstone model obtained setting equal gaussian cdf. since equivalent involved along empirical estimate associated score notational convenience adopt shorthands within round also denote permutation thatτ deﬁne indices items thinks bottom items moreover algorithm keeps items consideration items algorithm below). crucial ensure algorithm stuck trying distinguish middle items number comparisons necessary suﬃcient ﬁnding top-k items hamming-lucb algorithm depends gaps ∆ik++h ∆k−hi instead gaps ∆ik+ appear sample complexity ﬁnding items gaps typically signiﬁcantly larger resulting lower sample complexity. moreover hamming lucb provides strict improvement optimal sample complexity passive setup shah wainwright establish upper bounds minimax lower bounds state comparisons necessary suﬃcient identify items hamming error high probability. increases upper bound depends gaps items increasingly disparate position ranking thus upper bound sample complexity decreases. following lower bound shows that logarithmic factors doubly logarithmic factors gaps multiplicative scaling hamming-lucb algorithm optimal. that rescaling hamming error tolerance upper lower bounds respectively) match logarithmic factors. many problem instances interest—such models class equation —the sample complexity bounds degrade gracefully hamming tolerance typically observe recover exact top-k recovery upper bound equation related similar results multi armed bandits believe modifying conﬁdence intervals hamming lucb lucb++ algorithm simchowitz sharpen upper bound sample complexity replacing k−hi corresponding items thereby matching known lower terms bounds top-k subset selection problem bandit literature interest simplicity defer reﬁning logarithmic factors later work. even though lower bound gives misleading impression h-approximate algorithm away without querying proof section techniques establish reﬁned technical lower bound showing items including ranks close must compared adequate number times. simplicity state consequence lower bound applied parametric models described section addition showing item compared certain number times bound also establishes even knowledge exact parametric form pairwise comparison probabilities cannot drastically improve performance active ranking algorithm. note popular thurstone models equation holds φmin/φmax close provided mmin small. algorithm symmetric distribution comparisons commutes permutations items. algorithm main lower bound follows equivalent upper bound achieved hamming-lucb algorithm logarithmic factors. lower bound theorem stronger lower bound theorem applies larger class algorithms -accurate smaller class parametric models. fact parametric subclass cpar ∩cmmin signiﬁcantly smaller full pairwise comparison models cmmin sense matrices cmmin cannot well-approximated parametric model therefore theorem shows that rescaling hamming error tolerance logarithmic factors hamming-lucb algorithm optimal even restrict ourself algorithms uniformly -accurate parametric subclass. thus regime pairwise comparison probabilities bounded away zero parametric assumptions cannot substantially reduce sample complexity ﬁnding approximate ranking; observation made previously paper exact rankings. second equally important consequence theorem item sampled certain number times intuition captured theorem conclusion continues hold general pairwise comparison matrices please theorem section formal statement. even though upper lower bounds essentially match whenever pathological instances algorithm make considerably comparisons careful random guessing strategy. section provide experimental evidence corroborates theoretical claims hamming-lucb algorithm allows signiﬁcantly reduce number comparisons content approximate ranking. show gains attained real-world data set. speciﬁcally generate pairwise comparison model choosing borda scores coincide found empirically planyc survey panel figure emphasize that since hamming lucb depends borda scores comparison probabilities simulations provide faithful representation hamming lucb performs real-world data. figure plot results running hamming-lucb algorithm plannyc-pairwise comparison model order determine items diﬀerent values observed results values similar. suggested theory number comparisons approximate ranking decays manner inversely proportional compare hamming-lucb algorithm another sensible active ranking strategy obtaining hamming-accurate ranking. speciﬁcally consider version successive elimination strategy proposed ﬁnding exact ranking. strategy adapted yield hamming-accurate ranking changing stopping criterium. instead stopping items eliminated stop strictly smaller hamming-lucb algorithm. figure shows strategy requires signiﬁcantly comparisons ﬁnding approximate ranking thereby validating beneﬁts approach. figure sample complexity hamming-lucb algorithm elimination strategy pairwise comparison model resembling planyc online sequential survey. algorithms proposals proposals hamming error error bars correspond standard deviation mean. results show sample complexity hamminglucb algorithm ﬁnding h-accurate ranking drops factor moreover hamming-lucb algorithm requires signiﬁcantly fewer samples elimination strategy. t-th iteration steps lucb algorithm items selected step algorithm. note iteration items compared items. lemma therefore bound total number comparisons inequality follows deﬁnition last inequality follows τk−h τk++h thus ebad occur. item ebad false argument equivalent. item middle event ebad false deﬁnition. concludes shown either condition holds true simultaneously; consequently conclude least conditions hold true. next show either inequality inequality hold true leads contradiction concludes proof. bandit literature. {νj}m real line consider algorithm that times selects index receives independent draw distribution response. algorithm select based past observations measurable σ-algebra generated xit. algorithm stopping rule determines termination assume stopping time measurable respect obeying denote total number times index selected algorithm pair distributions denote kullback-leibler divergence denote kullback-leiber corresponding success algorithm recalling stopping rule algorithm guaranteed given linear relations pairwise comparison matrix determined entries {mij section state second lower bound number comparisons shows obtain -hamming accurate ranking algorithm compare item certain number times. proof lower bound also forms foundation proof theorem theorem symmetric algorithm i.e. distribution comparisons commutes permutations items uniformly -hamming accurate choose integer then item applied given changes. speciﬁcally need show given pairwise comparison matrix cpar∩cmmin construct alternative matrix obeying equality lies cpar∩ cmmin well. consider parametric pairwise comparison matrix cpar cmmin. exists parameter vector items proof paper considered problem ﬁnding hamming-approximate ranking pairwise comparisons. provided algorithm allows signiﬁcantly reduce sample complexity content approximate ranking. moreover showed algorithm near optimal remains near optimal imposing common parametric assumptions. number open practically relevant questions suggested work. work shows non-trivial adapt approximate notions ranking. would interesting understand optimally adapt approximate notions ranking closing bounds pathological problem instances importantly studying notions approximate rankings. would also interesting study algorithms work limited budget queries quantify approximation accuracy.", "year": 2018}