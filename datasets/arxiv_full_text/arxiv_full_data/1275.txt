{"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd's minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be directly combined $-$ applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by $10.4\\times$, $6.8\\times$ and $10.8\\times$ respectively with loss of accuracy less than $0.1\\%$, outperforming previous baselines by $2.0\\times$-$3.0\\times$. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.", "text": "xingyu liu∗ jeff pool† song han‡§ william dally∗† ∗stanford university nvidia massachusetts institute technology google brain {xyl dally}stanford.edu convolutional neural networks computationally intensive limits application mobile devices. energy dominated number multiplies needed perform convolutions. winograd’s minimal ﬁltering algorithm network pruning reduce operation count methods cannot directly combined applying winograd transform ﬁlls sparsity weights activations. propose modiﬁcations winograd-based cnns enable methods exploit sparsity. first move relu operation winograd domain increase sparsity transformed activations. second prune weights winograd domain exploit static weight sparsity. models cifar- cifar- imagenet datasets method reduces number multiplications respectively loss accuracy less outperforming previous baselines .×-.×. also show moving relu winograd domain allows aggressive pruning. deep convolutional neural networks shown signiﬁcant improvement many machine learning applications. however cnns compute-limited. performance dominated number multiplies needed perform convolutions. moreover computational workload cnns continues grow time. lecun proposed model less multiplies handwritten digit classiﬁcation. later krizhevsky developed alexnet imagenet-winning multiplies. imagenetwinning runner cnns increased number multiplies respectively. despite powerful representational ability large scale cnns computational workload prohibits deployment mobile devices. research directions explored address problem. lavin proposed using winograd’s minimal ﬁltering algorithm reduce number multiplies needed perform kernel convolutions. pruning model exploiting dynamic sparsity activations relu also reduces required multiplies. unfortunately directions compatible winograd transformation ﬁlls zeros weights activations eliminating gain exploiting sparsity. thus pruned network winograd’s algorithm actually increases number multiplies; loss sparsity offsets reduced operation count. paper introduce modiﬁcations original winograd-based convolution algorithm eliminate problem. first move relu operation winograd transform also make activations sparse point multiplies performed. second prune weights transformed. thus weights sparse elementwise multiply performed reducing operation count. together modiﬁcations enable gains winograd’s algorithm exploiting sparsity combined. open-source code models https//github.com/xingyul/sparse-winograd-cnn. figure combining winograd convolution sparse weights activations. conventional winograd-based convolution ﬁlls zeros weights activations. pruning transformed kernel restores sparsity weights. proposed winograd-relu cnn. moving relu layer winograd transformation also restores sparsity activations. linear algebra property convolution previous research proposes using linear algebra property convolution reduce number multiplies trading additions multiplies. cong xiao convert convolution matrix multiplies utilize linear algebra property sub-matrix block level. approach achieves saving multiplies. lavin exploits element-level linear algebra property convolution i.e. winograd’s minimal ﬁltering algorithm approach reduces number multiplies depending image patch size used algorithm. winograd’s algorithm also used state-of-the-art deep learning library cudnn improve computation efﬁciency. model compression model compression reduces number multiplies cnns pruning network parameters exploiting weight sparsity. proposed learning sparsity pattern network weights eliminating weights whose absolute value less empirical threshold. approach prune convolutional layers model original size reduce number multiplies required. ﬁrst proposed pruning re-training weights winograd domain conventional winograd convolution. later showed promising results large datasets reported sparsity winograd parameters alexnet less accuracy loss. dynamic activation sparsity relu non-linearity sets activations whose values negative zero causing dynamic sparsity activations. model compression work tandem dynamic activation sparsity reduce multiplication workload. showed exploiting sparsity weights activations reduce number multiplies huan proposed manually small positive relu threshold test time exploit greater sparsity activation without losing testing accuracy. research novel architectures also optimizations deep learning accelerators exploit sparsity activations. proposed using leading non-zero detection unit fully-connected layer accelerator efﬁciently skip zeros input activations. albericio proposed similar mechanism convolution layer accelerator. ﬁrst introduce conventional winograd convolution show sparsity weights activations lost dataﬂow algorithm. present novel winograd-relu architecture. preserves sparsity weights activations multiplies performed signiﬁcantly reduces computational workload. sparsity conventional spatial winograd basic block conventional winograd convolution algorithm works patch extracted stride input feature map. valid padding patch convolved kernel produce output patch output patches assembled output feature map. input activation patch kernel transformed using matrices gggt respectively shape element-wise product winograd-domain output activation obtained using matrix matrices p-speciﬁc. consists multiplication requires addition. reduces number multiplies lavin gives details algorithm. spatial baseline network using vanilla pruned network introduced relu non-linear operation performed previous layer spatial-domain input spatial-domain weight pruned. output activation patch obtained equation illustrated figure though sparse pruning relu respectively element-wise multiply dense transformations ﬁlling spatial-domain zeros. sparsity reduce number multiplies winograd’s algorithm. winograd native pruned network using winograd-domain pruned network introduced spatial-domain input relu-ed previous layer winograd-domain weight gggt pruned. output activation patch obtained equation algorithm also illustrated figure though winograd-domain weights sparse pruning winograd-domain activations still dense transforms. sparsity spatial activations relu reduce number multiplies. address problems introduce winograd-relu network. instead applying relu activations spatial domain apply relu activations winograd domain equation figure relu operation zeros negative transformed activations reducing number multiplies winograd domain. winograd-relu eliminate spatial-domain kernel entirely. relu really associated previous layer perform transformed relu starting second layer. point proposed architecture mathematically equivalent vanilla conventional winograd cnn. change network architecture training pruning also changed. method operates three phases dense training pruning retraining. dense training train dense kernel directly transform domain. transformed kernel initialized trained directly back-propagation inverse transform eliminating need maintain kernel spatial domain transform spatial kernel. pruning prune transformed kernel computing threshold required achieve desired pruning rate setting weights whose absolute value less zero. experiments used winograd-relu layers. sensitivity varies layer layer expect better performance could achieved varying pruning rate layer re-training re-train model using sparsity mask force weights pruned remain zero. sparsity mask computed pruning step kept constant re-training. gradient network’s loss respect input activation winograd weights derived using chain rule. equation shows calculation input activation gradient winograd weight gradient ∇gggt using loss gradient passed upstream layers ∇sl. applied methodology described several different cnns different datasets. original network models chosen majority convolution layers kernels. ensures largest portion layers converted winograd convolution layers relu winograd domain. used image classiﬁcation datasets different scales cifar- cifar- imagenet network architectures chose vgg-nagadomi convpool-cnn-c model variation resnet- respectively three datasets. using tensorﬂow framework trained spatial baseline corresponding conventional winograd winograd-relu models scratch. three models iteratively pruned re-trained. speciﬁc dataset used data augmentation training models dataset. used vgg-nagadomi cifar- dataset. vgg-nagadomi lightweight version vggnet contains convolution layers kernels. best reported validation accuracy achieves cifar- trained three models scratch. corresponding conventional winograd model winograd-relu model achieve validation accuracy respectively. ﬁrst convolution layer sensitive pruning density constant iteratively pruned re-trained convolution layers density figure shows test accuracy function weight density three models. baseline models pruned density accuracy falls signiﬁcantly winograd-relu model pruned density falling accuracy. table shows input activation density compares workloads pruned convolution layer three models. pruning baseline models reduces convolution layer workload respectively. pruning winograd-relu model reduces convolution layer workload improvement respectively baselines. improvement overall network workload reduction respectively baselines. winograd model workload reduction results include intrinsic reduction. used convpool-cnn-c model cifar- dataset. convpool-cnn-c contains convolution layers kernels. trained three models scratch. spatial baseline model conventional winograd model achieve single model validation accuracy respectively. corresponding winograd-relu network model achieve validation accuracy pruned ﬁrst convolution layer constant density iteratively pruned re-trained layers densities figure shows accuracy function density spatial baseline winograd-relu models. spatial-baseline winograd-relu models pruned density without signiﬁcant loss accuracy. contrast conventional winograd model pruned density. given density winograd-relu model highest accuracy. table shows input activation density compares workloads pruned convolution layer three models. pruning baseline models reduces convolution layer workload respectively. pruning winograd-relu model reduces workload improvement respectively baselines. improvement overall network workload reduction respectively baselines. used variation full pre-activation version resnet- imagenet dataset. used version performs best among various resnet versions structure suits winograd-relu approach relu units located convolutions residual modules. variation different original resnet- replacing -stride convolution layers max-pooling layer followed -stride convolution layer. difference ensure convolution layers converted winograd convolution layer. another difference doesn’t last pooling layer last group residual modules spatial size order keep spatial size even instead odd. setting suits winograd convolution best even spatial size required even values. trained three models scratch. single model single central cropping spatial baseline model conventional winograd model achieve single model top/top- validation accuracy .%/.% .%/.%. corresponding winogradrelu model achieve validation top-/top- accuracy .%/.%. kept ﬁrst convolution layer intact. iteratively pruned convolution layers density rate figure shows accuracy function density three models. spatial baseline model conventional winograd model pruned respectively without signiﬁcant loss top- top- accuracy. winograd-relu model pruned much further density without signiﬁcant loss top-/top- accuracy. densities top- accuracies three models respectively dense spatial baseline top- accuracies three models respectively dense spatial baseline table shows input activation density compares workloads pruned convolution layer three models. pruning baseline models reduces convolution layer workload respectively. pruning winograd-relu model reduces workload improvement respectively baselines. improvement overall network workload reduction respectively baselines. section summarize experiment results compare three models terms weight activation dimensions dynamic density activations. visualize kernels illustrate pattern proposed winograd-relu model kernel. convolutional neural network convolution-relu pair acts classiﬁer spatial patch input feature. dimension space classiﬁed total number elements passing relu layer. decision boundaries classiﬁer determined weights. insufﬁcient non-zero weights insufﬁcient activations results simple decision boundary causes accuracy loss. experimental results shown winograd-relu reach accuracy vanilla spatial baseline conventional winograd without pruning winogradrelu robust aggressive pruning. subsection provide explanation latter observation aspect activation weight dimensions. provide summary dimensions table table comparison relu dimension weight dimension three types networks. assume convolution-relu pair operates input activation spatial size number input output channels respectively. weight dimension increase compared vanilla conventional winograd uses -dimension winograd kernels. training winograd scratch allows higher dimension winograd kernels winograd-relu shares characteristics. relu dimension increase major difference winograd-relu conventional winograd relu layers winograd-relu higher dimension. dimension increase comes winograd transformation extracting feature patches strides activations. total number extracted winograd-domain activations winograd-relu architecture advantage dimensions weights activations models. means winograd-relu cnns classify higher dimension complex decision boundaries forms stronger representational ability high dimensional image feature space. shown imagenet results previous section dynamic activation density spatial baseline model varies signiﬁcantly among layers. layers earlier stages typically higher density activation later stages. winograd-relu model dynamic activation densities vary little among layers close explanation nature image convolution ensures activations spatially smooth. thus structure matrix elements matrix winograd-domain activation patch mean close zero. beneﬁts classiﬁcation within patch since relu layer powerful half activations positive. visualize kernels proposed winograd-relu model. selected ﬁrst input output channels layer resa_a resnet- three different pruning densities. unlike spatial domain kernels winograd-relu kernels show clear physical meanings edge corner detectors. however observe values elements kernel typically distinct kernel likely kept aggressive pruning. possible reason elements winograd-domain activation patch special interested readers calculate symbolically realize elements elements transformed linear combination adding subtraction. spatially smooth activation patch means elements ones ones non-zero mean. shown combine computational savings sparse weights activations savings winograd transform making modifcations conventional cnns. make weights sparse point multiplication train prune weights transform domain. simple approach reduce workload respect spatial pruning though move relu non-linear operation winograd transform make activations sparse point multiplication. moving relu winograd domain also allows weights aggressively pruned without losing accuracy. output patch result reduction computation three datasets cifar- cifar- imagenet. plan extend work following directions. first expect even greater savings computation realized using larger patch sizes beneﬁt exploring different winograd transformation matrices second expect using different pruning rates network layer help maintain accuracy improve overall workload reduction. finally expect combining winograd-relu network network simpliﬁcation techniques e.g. quantization weights and/or activations reduce energy computation even further. jorge albericio patrick judd tayler hetherington aamodt natalie enright jerger andreas moshovos. cnvlutin ineffectual-neuron-free deep neural network computing. proceedings international symposium computer architecture sharan chetlur cliff woolley philippe vandermersch jonathan cohen john tran bryan catanzaro evan shelhamer. cudnn efﬁcient primitives deep learning. corr abs/. http//arxiv.org/abs/.. matthieu courbariaux yoshua bengio jean-pierre david. binaryconnect training deep neural networks binary weights propagations. advances neural information processing systems song xingyu huizi jing ardavan pedram mark horowitz william dally. efﬁcient inference engine compressed deep neural network. proceedings international symposium computer architecture song huizi william dally. deep compression compressing deep neural networks pruning trained quantization huffman coding. international conference learning representations yuxiang huan yifan yantian lirong zheng zhuo zou. multiplication reduction technique near-zero approximation embedded learning devices. system-on-chip conference ieee international ieee zhouhan matthieu courbariaux roland memisevic yoshua bengio. neural networks multiplications. proceedings international conference learning representations xingyu song huizi william dally. efﬁcient sparse-winograd convolutional neural networks. international conference learning representations workshop mohammad rastegari vicente ordonez joseph redmon farhadi. xnor-net imagenet classiﬁcation using binary convolutional neural networks. european conference computer vision olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei. imagenet large scale visual recognition challenge. international journal computer vision karen simonyan andrew zisserman. deep convolutional networks large-scale image recognition. proceedings international conference learning representations jost tobias springenberg alexey dosovitskiy thomas brox martin riedmiller. striving simplicity convolutional net. international conference learning representations workshop christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. ieee conference computer vision pattern recognition june", "year": 2018}