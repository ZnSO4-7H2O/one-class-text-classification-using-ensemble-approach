{"title": "Bayesian Model Averaging Using the k-best Bayesian Network Structures", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We study the problem of learning Bayesian network structures from data. We develop an algorithm for finding the k-best Bayesian network structures. We propose to compute the posterior probabilities of hypotheses of interest by Bayesian model averaging over the k-best Bayesian networks. We present empirical results on structural discovery over several real and synthetic data sets and show that the method outperforms the model selection method and the state of-the-art MCMC methods.", "text": "study problem learning bayesian network structures data. develop algorithm ﬁnding k-best bayesian network structures. propose compute posterior probabilities hypotheses interest bayesian model averaging k-best bayesian networks. present empirical results structural discovery several real synthetic data sets show method outperforms model selection method stateof-the-art mcmc methods. bayesian networks widely used various data mining tasks probabilistic inference causal modeling major challenge applications learn structures data. bayesian approach provide prior probability distribution space possible bayesian networks compute posterior distributions network structure given data compute posterior probability hypothesis interest averaging possible networks. applications interested structural features. example causal discovery interested causal relations among variables represented edges network structure applications interested predicting posterior probabilities observations example classiﬁcation tasks. number possible network structures superexponential number variables example directed acyclic graphs nodes dags nodes. result impractical possible structures unless tiny domains common solution model selection approach relative posterior probability scoring metric attempt single network best score network. model make future predictions. good approximation amount data large relative size model posterior sharply peaked around model. however domains amount data small relative size model often many high-scoring models non-negligible posterior. situation using single model could lead unwarranted conclusions structure features also poor predictions observations. example edges appear model necessarily appear approximately equally likely models. also model selection sensitive data samples given sense different data might well lead different model. cases using bayesian model averaging preferred. recently progress computing exact posterior probabilities structural features edges subnetworks using dynamic programming techniques techniques exponential time memory complexity capable handling data sets around variables. problem algorithms computer posteriors modular features directed edges compute non-modular features paths another problem expensive perform data prediction tasks. compute exact posterior observational data algorithms re-run data case computing exact posterior probabilities features feasible solution proposed approximate full bayesian model averaging ﬁnding high-scoring networks making prediction using models leaves open question construct representative models. possible approach bootstrap technique where appropriate parameter priors scorei closed form solution. paper focus discrete random variables assuming variable take values ﬁnite domain. popular score scorei refer detailed expression. often convenience omit mentioning explicitly scorei score. since number possible dags superexponential number variables impractical dags unless small networks solution approximate exhaustive enumeration using selected models theoretically well-founded approach markov chain monte carlo techniques. madigan york used mcmc algorithm space network structures friedman koller developed mcmc procedure space node orderings shown efﬁcient mcmc space dags outperform bootstrap approach well. eaton murphy developed hybrid mcmc method ﬁrst uses dynamic programming technique develop global proposal distribution runs mcmc space. experiments showed dp+mcmc algorithm converged faster previous methods resulted accurate structure learning. common problem mcmc bootstrap approach guarantee quality approximation ﬁnite runs. madigan raftery proposed discard models whose posterior probability much lower best ones paper study approach approximating bayesian model averaging using best bayesian networks. intuitive make predictions using best models believe computational difﬁculties actually ﬁnding best networks idea systematically studied. paper develop algorithm ﬁnding k-best network structures generalizing dynamic programming algorithm ﬁnding optimal bayesian network structures demonstrate algorithm several real data sets machine learning repository synthetic data sets gold-standard network. empirically study quality bayesian model averaging using k-best networks structure discovery show method outperforms model selection method state-ofthe-art mcmc methods. bayesian network encodes joint probability distribution random variables node graph representing variable convenience typically work index represent variable index represent parents represent corresponding index set. algorithm finding k-best parent sets variable candidate input scorev local scores bestp arentsv priority queues k-best parent sets variable candidate output bestp arentsv priority queue k-best parents candidate initialize bestp arentsv k-best sets among whole candidate itself k-best parent sets smaller candidate sets \\{c}|c therefore compute k-best parent sets every candidate \\{v} start sets size consider sets {v}. skeleton algorithm ﬁnding k-best parent sets candidate given algorithm bestp arentsv denote best parent sets variable candidate stored priority queue operation erge outputs priority queue best parents given input priority queues elements. assuming merge operation takes time ﬁnding k-best parent sets candidate takes time |c|) computing takes time on−). calculated k-best parent sets variable ﬁnding k-best network structures variable done recursively. exploit fact every sink node outgoing edges. first variable kbest networks sink. k-best networks k-best networks among {the kbest networks sink k-best networks sink identiﬁed looking k-best parent sets k-best networks {s}. formally bestp arentss denote best parent variable candidate parent interested computing posteriors structural features edges paths markov blankets etc.. structural feature represented indicator function feature present otherwise. k-best structures using dynamic programming techniques extending algorithm ﬁnding optimal bayesian network structures algorithm consists three steps synthetic data sets various sample sizes goldstandard -variable bayesian network known structure parameters. data sets contain discrete variables missing values. data learned k-best networks certain estimate posterior probabilities hypotheses using eqs. idea close estimation true posteriors used algorithm compute exact evaluate quality posterior estimation follows. deﬁne following quantity practice cases large amount data much smaller bounds proposition could loose useful. therefore introduce another measure quality posterior estimation relative ratio posterior probability network gmap posterior worst network gmin best networks argued make predictions using best models discarding models predict data less well even though many models small posterior probabilities contribute substantially cutoff value suggested analogy cutoff p-values. nput bestp arentsi priority queues k-best parent sets variable candidate bestn priority queues k-best network structures output bestn priority queue k-best networks skeleton algorithm ﬁnding k-best network structures given algorithm time spent best-ﬁrst search worst case nodes need visited. complexity ﬁnding k-best network structures used score scorei uniform structure prior equivalent sample size implemented algorithm language experiments method linux ordinary desktop .ghz intel pentium processor memory. tested algorithm several data sets experimental results reported table lists data number variables number instances value time computing local scores total running time ﬁnding k-best networks quality measure expected values increases increases. value often small proposition useful however proposition indeed provide guarantee quality approximation nontrivial cases like nursey tictac-toe data sets still large exhaustive enumeration possible networks. based value best networks enough reliable estimation letter data sets. synthetic data increases sample size increases. based value best networks give reliable estimation exact posterior probabilities k-best networks data shown figures could multiple networks posterior probability. example number best networks sharing largest posterior probability follows nursery case tic-tac-toe case letter case synthetic data synthetic data mainly scoring criterion likelihood-equivalence property i.e. assigns score bayesian networks within independence equivalence class. however found also possible networks across different equivalence classes posterior probability. take tic-tac-toe case example found best networks actually belong multiple equivalence classes different skeletons. exceptional case shows always assert networks posterior probability must within equivalence class. table show difference best equivalence class second best equivalence class case. second column lists number different undirected edges best equivalence classes third column shows result shows difference typically small case. order evaluate ability k-best networks method structural discovery tested synthetic data gold-standard -variable bayesian network. computed edge feature pair variables different values comparison also computed exact posterior probability edge method full model averaging since true goldstandard network could compute corresponding curves. results shown figures ﬁgures indicate usefulness k-best method structural discovery. observe area non-decreasing function even small increase lead non-negligible improvement corresponding even though tiny small data performance almost full model averaging method regardless fact still tiny comparison mcmc approach also demonstrates usefulness method. paper compared method hybrid method directly best equivalence classes. however proposed algorithm search equivalence class space. algorithm individual networks regardless existence equivalent networks. seems dynamic programming idea cannot naturally generalized equivalence class space directly best equivalence classes research direction worth pursue.", "year": 2012}