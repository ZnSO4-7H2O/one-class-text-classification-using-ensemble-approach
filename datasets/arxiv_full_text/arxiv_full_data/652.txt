{"title": "Inferencing Based on Unsupervised Learning of Disentangled  Representations", "tag": ["cs.CV", "cs.AI", "cs.NE"], "abstract": "Combining Generative Adversarial Networks (GANs) with encoders that learn to encode data points has shown promising results in learning data representations in an unsupervised way. We propose a framework that combines an encoder and a generator to learn disentangled representations which encode meaningful information about the data distribution without the need for any labels. While current approaches focus mostly on the generative aspects of GANs, our framework can be used to perform inference on both real and generated data points. Experiments on several data sets show that the encoder learns interpretable, disentangled representations which encode descriptive properties and can be used to sample images that exhibit specific characteristics.", "text": "abstract. combining generative adversarial networks encoders learn encode data points shown promising results learning data representations unsupervised way. propose framework combines encoder generator learn disentangled representations encode meaningful information data distribution without need labels. current approaches focus mostly generative aspects gans framework used perform inference real generated data points. experiments several data sets show encoder learns interpretable disentangled representations encode descriptive properties used sample images exhibit speciﬁc characteristics. learning meaningful representations data important step models understand world recently generative adversarial network proposed method learn characteristics data distributions without need labels. gans traditionally consist generator generates data randomly sampled vectors discriminator tries distinguish generated data real data training generator learns generate realistic data samples discriminator becomes better distinguishing generated real data result generator discriminator learn characteristics underlying data distribution without need labels desirable characteristic learned representations disentanglement means diﬀerent parts representation encode diﬀerent factors data-generating distribution. makes representations interpretable easier modify useful property many tasks classiﬁcation clustering image captioning. introduced variant generator’s input split parts here encodes unstructured noise encodes meaningful data-generating factors. enforcing high mutual information generated images generator trained using inputs meaningful encodings certain image characteristics. example ten-dimensional categorical code could represent diﬀerent digit classes mnist data set. since labels provided generator learn image characteristics represented drawback model perform inference i.e. real data samples representation discriminator. however guarantee discriminator learns good representations data general trained discriminate real generated data therefore focus features helpful discriminating necessarily descriptive data distribution general zhang tried enforce disentangled representations order improve controllability generator. latent representation split parts encoding meaningful information unknown factors variation. additional inference networks introduced enforce disentanglement parts latent representation. setup yields better controllability generative process depends labeled samples training objective discover unknown data-generating factors encodes known factors variation disentangled representation. introduced extension includes encoder learns encodings real data samples. discriminator gets input data sample according representation classify either coming generator encoder. generator encoder fool discriminator misclassifying samples. result encoder learns approximate inverse generator used real data samples representations applications. however approaches representations follow simple prior e.g. gaussian uniform distribution exhibit disentangled properties. model bidirectional-infogan integrates approaches extending traditional gans encoder learns disentangled representations unsupervised setting. training encoder data points meaningful disentangled representations potentially used diﬀerent tasks classiﬁcation clustering image captioning. compared infogan introduce encoder mitigate problems using discriminator adversarial loss inference task. unlike structured training procedure completely unsupervised detect unknown data-generating factors introduces additional inference network contrast bidirectional replace simple prior latent representation distribution amenable disentangled representations introduce additional loss encoder generator achieve disentangled representations. mnist celeba svhn data sets show encoder learn interpretable representations encode meaningful properties data distribution. using sample images exhibit certain characteristics e.g. digit identity speciﬁc stroke widths mnist data diﬀerent hair colors clothing accessories celeba data set. fig. high-level overview bidirectional-infogan. generator generates images vector tries fool discriminator classifying real. encoder encodes images representation tries fool discriminator misclassifying fake input real image trying approximate input generated image. model shown fig. consists generator discriminator encoder implemented neural networks. input vector given generator made parts here sampled uniform distribution used represent unstructured noise images. hand part representation encodes meaningful information disentangled manner made categorical values ccat continuous values ccont. takes input transforms image i.e. convolutional network gets input either real fake images encodes latent representation gets input image corresponding representation concatenated along channel axis. tries classify pair coming either generator encoder i.e. fool discriminator misclassifying input. result original minimax game extended becomes order force generator information provided maximize mutual information maximizing mutual information directly hard requires posterior therefore follow approach chen deﬁne auxiliary distribution approximate maximize lower bound x∼g] mutual information depicted fig. simplicity reasons fig. represents value ten-dimensional code encodes diﬀerent digits despite never seeing labels training process. images maximum minimum values categorical value distribution therefore entropy term treated constant. case encoder network gets images generated input trained approximate unknown posterior categorical ccat softmax nonlinearity represent treat posterior continuous ccont factored gaussian. given structure minimax game bidirectional-infogan perform experiments mnist celeba svhn data set. ﬁnal performance model likely inﬂuenced choosing optimal characteristics usually possible since know data-generating factors beforehand. choosing characteristics dimensionality disentangled vector therefore mostly stick values previously chosen chen information network architectures examples learned characteristics diﬀerent data sets https//github.com/tohinz/bidirectional-infogan. mnist data model latent code categorical variable continuous variables fig. images sampled celeba svhn test sets. shows images sampled according speciﬁc categorical variable ccat represents learned characteristic. optimization process without labels encoder learns encode diﬀerent digit classes encode stroke width digit rotation. fig. shows images randomly sampled test according diﬀerent categorical values. encoder learned reliably assign diﬀerent categorical value diﬀerent digits. indeed manually matching diﬀerent categories digit type achieve test accuracy without ever using labels training compared chen accuracy zhang accuracy fig. shows images sampled test diﬀerent values encodings sample digits certain characteristics stroke width rotation even though information explicitly provided training. celeba data latent code modeled four categorical codes four continuous variables again encoder learns associate certain image characteristics speciﬁc codes includes characteristics presence glasses hair color background color visualized fig. svhn data network architecture latent code representations celeba data set. again encoder learns interpretable disentangled representations encoding characteristics image background contrast digit type. fig. examples sampled svhn test set. results indicate bidirectional-infogan indeed capable mapping data points disentangled representations encode meaningful characteristics completely unsupervised manner. showed encoder coupled generator generative adversarial network learn disentangled representations data without need explicit labels. using encoder network maximize mutual information certain parts generator’s input images generated generator learns associate certain image characteristics speciﬁc parts input. additionally adversarial cost discriminator forces generator generate realistic looking images encoder approximate inverse generator leading disentangled representations used inference. learned characteristics often meaningful humanly interpretable potentially help tasks classiﬁcation clustering. additionally method used pre-training step unlabeled data sets lead better representations ﬁnal task. however currently inﬂuence characteristics learned unsupervised setting means model also learn characteristics features meaningless interpretable humans. future mitigated combining approach semi-supervised approaches supply limited amount labels characteristics interested exert control data-generating factors learned still able discover generating factors known speciﬁed beforehand.", "year": 2018}