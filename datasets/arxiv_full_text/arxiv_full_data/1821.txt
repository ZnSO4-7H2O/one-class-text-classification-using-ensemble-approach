{"title": "Gated Recurrent Neural Tensor Network", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "Recurrent Neural Networks (RNNs), which are a powerful scheme for modeling temporal and sequential data need to capture long-term dependencies on datasets and represent them in hidden layers with a powerful model to capture more information from inputs. For modeling long-term dependencies in a dataset, the gating mechanism concept can help RNNs remember and forget previous information. Representing the hidden layers of an RNN with more expressive operations (i.e., tensor products) helps it learn a more complex relationship between the current input and the previous hidden layer information. These ideas can generally improve RNN performances. In this paper, we proposed a novel RNN architecture that combine the concepts of gating mechanism and the tensor product into a single model. By combining these two concepts into a single RNN, our proposed models learn long-term dependencies by modeling with gating units and obtain more expressive and direct interaction between input and hidden layers using a tensor product on 3-dimensional array (tensor) weight parameters. We use Long Short Term Memory (LSTM) RNN and Gated Recurrent Unit (GRU) RNN and combine them with a tensor product inside their formulations. Our proposed RNNs, which are called a Long-Short Term Memory Recurrent Neural Tensor Network (LSTMRNTN) and Gated Recurrent Unit Recurrent Neural Tensor Network (GRURNTN), are made by combining the LSTM and GRU RNN models with the tensor product. We conducted experiments with our proposed models on word-level and character-level language modeling tasks and revealed that our proposed models significantly improved their performance compared to our baseline models.", "text": "model dataset task short-term dependencies like slot ﬁlling spoken language understanding however diﬃcult tasks like language modeling machine translation predictions need longer information historical context sentence gating units needed achieve good performance. gating units blocking passing information previous future hidden layer learn long-term information recursively backpropagate error prediction without suﬀering vanishing exploding gradient problems spite situation concept gating mechanism provide powerful model relation current input previous hidden layer representations. interactions inside rnns current input previous hidden states represented using linear projection addition transformed nonlinear activation function. transition shallow intermediate hidden layers exist projecting hidden states powerful representation hidden layer pascanu modiﬁed rnns additional nonlinear layer input hidden layer transition hidden hidden layer transition also hidden output layer transition. socher proposed another approach using tensor product calculating output vectors given input vectors. modiﬁed recursive neural network overcome limitations using direct interaction input layers. architecture called recursive neural tensor network uses tensor product child input vectors represent parent vector representation. adding tensor product operation calculate parent vector recntn signiﬁcantly improves performance sentiment analysis reasoning entity relations tasks compared standard recnn architecture. however models struggle learn long-term dependencies utilize concept gating mechanism. paper proposed architecture combine gating mechanism tensor product concepts incorporate advantages single architecture. using concept gating mechanisms lstmrnn grurnn proposed architecture learn temporal sequential data longer dependencies input time-step simple rnns without gating units combine gating units tensor products represent hidden abstract—recurrent neural networks powerful scheme modeling temporal sequential data need capture long-term dependencies datasets represent hidden layers powerful model capture information inputs. modeling long-term dependencies dataset gating mechanism concept help rnns remember forget previous information. representing hidden layers expressive operations helps learn complex relationship current input previous hidden layer information. ideas generally improve performances. paper proposed novel architecture combine concepts gating mechanism tensor product single model. combining concepts single proposed models learn long-term dependencies modeling gating units obtain expressive direct interaction input hidden layers using tensor product -dimensional array weight parameters. long short term memory gated recurrent unit combine tensor product inside formulations. proposed rnns called long-short term memory recurrent neural tensor network gated recurrent unit recurrent neural tensor network made combining lstm models tensor product. conducted experiments proposed models word-level character-level language modeling tasks revealed proposed models signiﬁcantly improved performance compared baseline models. modeling temporal sequential data crucial machine learning applied many areas speech natural language processing. deep neural networks garnered interest many researchers being successfully applied image classiﬁcation speech recognition another type neural network called recurrent neural network also widely used speech recognition machine translation language modeling rnns achieved many state-of-the-art results. compared dnns extra parameters modeling relationships previous future hidden states current input parameters shared input time-step. generally rnns separated simple without gating units elman jordan advanced rnns gating units long-short term memory gated recurrent unit simple usually adequate layer powerful operation direct interaction. hidden states generated interaction current input previous hidden states using tensor product non-linear activation function allows expressive model representation. describe diﬀerent models based lstmrnn grurnn. lstmrntn proposed model combination lstm unit tensor product inside cell equation grurntn name unit tensor product inside candidate hidden layer equation. section provide background information related research. section describe proposed architecture detail. evaluate proposed architecture word-level character-level language modeling tasks reported result section present related works section section summarizes paper provides possible future improvements. using second-order hessian free optimization another approach addressed vanishing exploding gradient problem modiﬁed architecture additional parameters control information previous hidden layers using gating mechanism concept gated special recurrent neural network architecture overcomes weakness simple introducing gating units. variants gating units long short term memory gated recurrent unit rnn. following sections explain lstmrnn grurnn detail. long short term memory long short term memory gated three gating layers memory cells. gating layers used lstm control existing memory retaining useful information forgetting unrelated information. memory cells used storing information across time. lstm hidden layer time deﬁned following equations recurrent neural network kind neural network architecture modeling sequential temporal dependencies typically input sequence calculate hidden vector sequence output vector sequence rnns. standard time t-th usually formulated represents input layer hidden layer weight matrix represents hidden hidden layer weight matrix represents hidden output weight matrix represent bias vectors hidden output layers. nonlinear activation functions sigmoid tanh. sigmoid activation function respectively input gates forget gates output gates memory cells time-step input gates keep candidate memory cell values useful memory cell computation forget gates keep previous memory cell values useful calculating current memory cell. output gates ﬁlter memory cell values useful output next hidden layer input. simple rnns hard train capture long-term dependencies long sequential datasets gradient easily explode vanish gradient vanishes several steps optimizing simple complicated standard neural networks. overcome disadvantages simple rnns several researches done. instead using ﬁrstorder optimization method approach optimized gated recurrent unit gated recurrent unit gated similar properties lstm. however several diﬀerences separated memory cells instead three gating layers gating layers reset gates update gates. sigmoid activation function reset update gates candidate hidden layer values hidden layer values time-t. reset gates determine previous hidden layer value useful generating current candidate hidden layer. update gates keeps previous hidden layer values replaced candidate hidden layer values. spite fewer gating layer match lstm’s performance convergence speed convergence sometimes outperformed lstm recursive neural tensor network variant recursive neural network modeling input data variable length properties tree structure dependencies input features compute input representation recnn input must parsed binary tree leaf node represents input data. then parent vectors computed bottom-up fashion following computed tree structure whose information built using external computation tools heuristic dataset observations. given fig. deﬁned nonlinear activation function sigmoid tanh depends task rd×d weight parameter projecting child input vectors parent vector weight parameter computing output vector biases. want train recnn classiﬁcation tasks deﬁned softmax function. however standard recnns several limitations vectors implicitly interact addition applying nonlinear activation function standard recnns able model long-term dependency tree structures. proposed gating mechanism standard recnn model solve latter problem. former limitation recnn performance improved adding interaction input vectors. therefore architecture called recursive neural tensor network tried overcome previous problem adding interaction vectors using tensor product connected tensor weight parameters. slice tensor weight used capture speciﬁc pattern left right child vectors. recntn value previously sections ii-b ii-c discussed gating mechanism concept helps rnns learn longterm dependencies sequential input data adding powerful interaction input hidden layers simultaneously tensor product operation bilinear form improves neural network performance expressiveness. using tensor product increase model expressiveness using second-degree polynomial interactions compared ﬁrst-degree polynomial interactions standard product followed addition common rnns architecture. therefore paper proposed gated recurrent neural tensor network combine advantages architecture. architecture tensor product operation applied current input previous hidden layer multiplied reset gates calculating current candidate hidden layer values. calculation parameterized tensor weight. construct grurntn deﬁned formulation grurntn also applied tensor product operation lstm unit improve performance. architecture tensor product operation applied current input previous hidden layers calculate current memory cell. calculation parameterized tensor weight. call architecture long short term memory recurrent neural tensor network construct lstmrntn deﬁned formulation ri×d×d tensor weight tensor product current input previous hidden layer candidate cell ˜ct. slice matrix ri×d. fig. visualizes calculation detail. matrix ri×d. advantage asymmetric version still maintain interaction input hidden layers bilinear form. reduce number parameters original neural tensor network formulation using asymmetric version. fig. visualizes calculation detail. proposed models partial derivative ∂ei/∂w eqs. derivative tensor product w.r.t tensor weight parameters depends values input hidden layers. slices tensor weight derivative multiplied error corresponding pre-activated hidden unit values. derivations able slice tensor weight learned directly input hidden layer values compared using standard addition operations. accumulated every parameter’s gradients previous time-steps stochastic gradient optimization method adagrad optimize model parameters. used penntreebank corpus standard benchmark corpus statistical language modeling. corpus subset corpus. experiment followed standard preprocessing step done previous research dataset divided follows training sections total words validation sections total words test sections total words. vocabulary limited common words words outside mapped <unk> token. used preprocessed corpus rnnlm-toolkit website. section explain train tensor weight proposed architecture. generally backpropagation train neural network models training researchers tend backpropagation time recurrent operation unfolded feedforward neural network along time-step backpropagate error sometimes face performance issue unfold long sequences. handle issue truncated bptt limit number time-steps unfold backpropagation. assume want segment classiﬁcation trained function input sequence output label sequence. case probability output label sequence given input sequence deﬁned applying backpropagation time need unfold grurntn backpropagate error candidate hidden layer gradient want truncated bptt ignore history past time-steps limit ..i]. deﬁne standard bptt grurntn calculate ∂ei/∂w diﬀerent language modeling tasks. first experimented word-level language model predicts next word probability given previous words current word. used perplexity measure performance word-level language modeling. formula calculating word sequence deﬁned second experimented character-level language model predicts next character probability given previous characters current character. used average number bits-per-character measure performance character-level language modeling. formula calculating character sequence deﬁned experiment compared performance baseline models grurnn lstmrnn proposed grurntn lstmrntn models. used dimensions embedding matrix represent words characters vectors real numbers. word-level language modeling task used hidden units grurntn lstmrntn grurnn lstmrnn. models dimensions word embedding. used dropout regularization dropout probability grurntn lstmrntn baseline model. total number free parameters grurnn grurntn million million lstmrnn lstmrntn. character-level language modeling task used hidden units grurntn lstmrntn grurnn lstmrntn. models used dimensions character embedding. used dropout regularization dropout probability. total number free parameters grurnn grurntn million million lstmrnn lstmrntn. constrained baseline grurnn similar number parameters grurntn model fair comparison. also applied constraints baseline lstmrnn lstmrntn model. experiment scenarios used adagrad stochastic gradient optimization method mini-batch training batch size sentences. multiplied learning rate decay factor cost development current epoch greater previous epoch. also used rescaling trick gradient norm larger avoid issue exploding gradients. initializing parameters used orthogonal weight initialization trick every model. section report experiment results character-level language modeling using baseline models grurnn lstmrnn well proposed models grurntn lstmrntn. fig. shows performance comparisons every model based validation set’s epoch. experiment grurnn made faster progress lstmrnn eventually lstmrnn converged better based development set. proposed model grurntn made faster quicker progress lstmrntn converged similar last epoch. proposed models produced lower baseline models ﬁrst epoch last epoch. table shows test among baseline models proposed models several published results. proposed model grurntn lstmrntn outperformed baseline models. grurntn reduced baseline grurnn lstmrntn reduced baseline lstmrnn. overall grurntn slightly outperformed lstmrntn proposed models outperformed baseline models character-level language modeling task. results word-level language modeling using baseline models grurnn lstmrnn proposed models grurntn lstmrntn. fig. compares performance every models based validation set’s epoch. experiment grurnn made faster progress lstmrnn. proposed grurntn’s progress also better lstmrntn. best model task grurntn consistently lower models. table shows test among baseline models proposed models several published results. proposed models outperformed baseline models. grurntn reduced perplexity baseline grurnn lstmrntn reduced perplexity baseline lstmrnn. overall lstmrntn improved lstmrnn model performance closely resembles baseline grurnn. however grurntn outperformed baseline models well models large margin. representing hidden states deeper operations introduced years works pascanu additional nonlinear layers representing transition input hidden layers hidden hidden layers hidden output layers. also improved architecture adding shortcut connection deep transition skipping intermediate layers. another work proposed design stacked model called gated feedback adds connections previous time-step stacked hidden layers current hidden layer computations. despite adding additional transition layers connection weight previous hidden layers models still represent input hidden layer relationships using linear projection addition nonlinearity transformation. tensor-based models irsoy proposed simple tensor product input hidden layers. architecture resembles recntn given parse tree completely unbalanced tree side. another work also tensor products representing hidden layers dnn. splitting weight matrix parallel weight matrices calculated parallel hidden layers combined pair hidden layers using tensor product. however since models gating mechanism tensor parameters tensor product operation fully utilized vanishing gradient problem. recurrent neural network-based model sutskever proposed multiplicative characterlevel language modeling using tensor weight parameters. proposed diﬀerent models. ﬁrst selected slice tensor weight based current character input second improved ﬁrst model factorization constructing hidden-to-hidden layer weight. however models fail fully utilize tensor weight tensor product. selected weight matrix based current input information continue linear projection addition nonlinearity interacting input hidden layers. best knowledge none works combined gating mechanism tensor product concepts single neural network architecture. paper built combining gating units tensor products single architecture. expect proposed grurntn lstmrntn architecture improve performance modeling temporal sequential datasets. presented architecture combining gating mechanism tensor product concepts. proposed architecture learn long-term dependencies temporal sequential data using gating units well powerful interaction current input previous hidden layers introducing tensor product operations. experiment penntreebank corpus proposed models outperformed baseline models similar number parameters character-level language modeling word-level language modeling tasks. character-level language modeling task grurntn obtained absolute reduction grurnn lstmrntn obtained absolute reduction lstmrnn. word-level language modeling task grurntn obtained absolute reduction grurnn lstmrntn obtained absolute reduction lstmrnn. future investigate possibility combining model stacked rnns architecture gated feedback would also like explore possible tensor operations integrate architecture. applying ideas together expect gain performance improvement. last investigation apply proposed models temporal sequential tasks speech recognition video recognition.", "year": 2017}