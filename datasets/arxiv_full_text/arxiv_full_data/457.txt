{"title": "Compression of Neural Machine Translation Models via Pruning", "tag": ["cs.AI", "cs.CL", "cs.NE"], "abstract": "Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT'14 English-German translation task. This sheds light on the distribution of redundancy in the NMT architecture. Our main result is that with retraining, we can recover and even surpass the original performance with an 80%-pruned model.", "text": "signiﬁcantly smaller memory footprint traditional phrase-based approaches model size still prohibitively large mobile devices. example recent state-of-the-art system requires million parameters resulting storage size hundreds megabytes though trend bigger deeper neural networks brought great progress also introduced over-parameterization resulting long running times overﬁtting storage size issue discussed above. solution overparameterization problem could potentially three issues though ﬁrst outside scope paper. neural machine translation like many deep learning domains typically suffers over-parameterization resulting large storage sizes. paper examines three simple magnitude-based pruning schemes compress models namely class-blind class-uniform class-distribution differ terms pruning thresholds computed different classes weights architecture. demonstrate efﬁcacy weight pruning compression technique state-of-the-art system. show model million parameters pruned little performance loss measured wmt’ englishgerman translation task. sheds light distribution redundancy architecture. main result retraining recover even surpass original performance %-pruned model. neural machine translation simple architecture translating texts language another single deep neural network trained end-to-end holding several advantages ability capture long-range dependencies sentences generalization unseen texts. despite relatively already achieved state-of-the-art translation results several language pairs including englishfrench english-german architecture lower layers parameters input crucial higher layers parameters gates also become important. pruning parameters neural network referred weight pruning network pruning well-established idea though implemented many ways. among popular optimal brain damage optimal brain surgeon techniques involve computing hessian matrix loss function respect parameters order assess saliency parameter. parameters saliency pruned network remaining sparse network retrained. shown perform better so-called ‘naive magnitude-based approach’ prunes parameters according magnitude however high computational complexity compare unfavorably computational simplicity magnitude-based approach especially large networks recent years deep learning renaissance prompted re-investigation network pruning modern models tasks. magnitudebased pruning iterative retraining yielded strong results convolutional neural networks performing visual tasks. collins kohli prune alexnet parameters small accuracy loss imagenet task approaches focus pruning neurons rather parameters sparsity-inducing regularizers ‘wiring together’ pairs neurons similar input weights approaches much constrained weight-pruning schemes; necessitate ﬁnding entire zero rows weight matrices near-identical pairs rows order prune single neuron. contrast weight-pruning approaches allow weights pruned freely independently other. neuron-pruning approach srinivas babu shown perform poorly compared weightpruning approach though murray chiang demonstrates neuronpruning language modeling part machine translation pipeline approach geared towards architecture selection compression. many compression techniques neural networks including approaches based low-rank approximations weight matrices weight sharing hash functions several methods involve reducing precision weights activations sometimes conjunction specialized hardware even using binary weights ‘knowledge distillation’ technique hinton involves training small ‘student’ network soft outputs large ‘teacher’ network. approaches sophisticated pipeline several techniques achieve impressive feats compression work focused compressing cnns vision tasks. extend magnitude-based pruning approach recurrent neural networks particular lstm architectures knowledge ﬁrst recent work compression rnns focuses other non-pruning compression techniques. nonetheless general observations distribution redundancy lstm detailed section corroborated ﬁrst give brief overview neural machine translation describing model architecture interest deep multi-layer recurrent model lstm. explain different types weights together approaches pruning retraining. neural machine translation neural machine translation aims directly model conditional probability translating source sentence target sentence accomplishes goal encoder-decoder framework encoder computes representation source sentence. based source representation decoder generates translation target word time hence decomposes conditional probability work speciﬁcally consider deep multi-layer recurrent architecture lstm hidden unit type. figure illustrates instance architecture training source target sentence pair input supervised learning. testing target sentence known advance; instead probable target words predicted model inputs next timestep. network stops emits end-of-sentence symbol special ‘word’ vocabulary represented dash figure understanding weights figure shows system detail highlighting different types parameters weights model. architecture bottom top. first vocabulary chosen language assuming frequent words selected. thus every word source target vocabulary represented one-hot vector length pruning schemes follow general magnitude-based approach consists pruning weights smallest absolute value. however question authors’ pruning scheme respect different weight classes experiment three pruning schemes. suppose wish prune total parameters model. distribute pruning different weight classes model? propose examine three different pruning schemes class-distribution class weights magnitude less pruned. here standard deviation class universal parameter chosen total parameters pruned. used schemes seeming advantages. class-blind pruning simplest adheres principle pruning weights least damaging weights small regardless locations architecture. class-uniform pruning classdistribution pruning seek prune proportionally within weight class either absolutely relative standard deviation class. class-blind pruning outperforms schemes retraining order prune models aggressively withperformance loss retrain pruned networks. continue train remaining weights maintain sparse structure introduced pruning. implementation pruned source input sentence target input sentence represented sequence one-hot vectors transformed sequence word embeddings embedding weights. embedding weights learned training different source words target words. word embeddings hidden layers vectors length word embeddings input main network consists multilayer rnns ‘stuck together’ encoder source language decoder target language weights. feedforward weights connect hidden unit layer upper block recurrent weights connect hidden unit previous time-step block current time-step block. hidden state layer decoder attention layer guides translation ‘paying attention’ relevant parts source sentence; information bahdanau section luong finally target word layer hidden unit transformed softmax weights score vector length target word highest score selected output translation. weight subgroups lstm aforementioned block choose lstm hidden unit type. facilitate later discussion different subgroups weights within lstm ﬁrst review details lstm formulated zaremba follows here lstm block time layer computes output pair hidden memory vectors input vector vectors length core lstm block weight matrix size matrix decomposed subgroups responsible comparing pruning schemes despite simplicity observe figure class-blind pruning outperforms schemes terms translation quality pruning percentages. order understand result three pruning schemes pruned class separately recorded effect performance figure shows class-uniform pruning overperformance loss caused disproportionately classes target layer attention softmax weights. looking figure damaging classes prune also tend weights greater magnitude classes much larger weights others percentile pruning classuniform pruning scheme damaging. situation similar class-distribution pruning. contrast figure shows classblind pruning damage caused pruning softmax attention target layer weights greatly decreased contribution class towards performance loss overall uniform. fact distribution begins reﬂect number parameters class example source target embedding classes larger contributions weights. class-blind pruning rest experiments. figure also reveals interesting information distribution redundancy architectures namely seems higher layers important lower layers attention softmax weights crucial. explore distribution redundancy section pruning retraining pruning immediate negative impact performance exponential pruning percentage; demonstrated blue line figure however pruning performance mostly unaffected indicating large amount redundancy over-parameterization nmt. weights represented zeros weight matrices binary ‘mask’ matrices represent sparse structure network ignore updates weights pruned locations. implementation advantage simplicity requires minimal changes training deployment code note complex implementation utilizing sparse matrices sparse matrix multiplication could potentially yield speed improvements. however implementation beyond scope paper. evaluate effectiveness pruning approaches state-of-the-art model. speciﬁcally attention-based english-german system luong considered. training data obtained wmt’ consisting sentence pairs details training hyperparameters refer readers section luong models tested newstest model achieves perplexity bleu score retraining pruned systems following settings start smaller learning rate train fewer epochs instead using plain simple learning rate schedule employed; epochs begin halve learning rate every half epoch hyperparameters figure ‘breakdown’ performance loss weight class pruning weights using three pruning schemes. ﬁrst eight classes million weights attention million last three million weights each. proved upon pruning small performance loss pruning seem surprising might expect sparse model signiﬁcantly out-perform model times many parameters. several possible explanations given below. firstly found less-pruned models perform better training validation whereas more-pruned models closer performance sets. indicates pruning regularizing effect retraining phase though clearly always better pruned retrained model better validation performance alternatively pruning serve means escape local optimum. figure shows loss function time training pruning retraining process. original training process loss curve ﬂattens seems converge pruning causes immediate increase loss function enables gradient descent allowing retraining process better local optimum. seems disruption caused figure graphical representation location small weights various parts model. black pixels represent weights absolute size bottom white pixels represent absolute size equivalently pictures illustrate parameters remain pruning using class-blind pruning scheme. visualize figure redundancy structore baseline model. black pixels represent weights near zero white pixels represent larger ones. first consider embedding weight matrices whose columns correspond words vocabulary. unsurprisingly figure parameters corresponding less common words dispensable. fact pruning rate uncommon source words uncommon target words delete parameters corresponding word. quite removing word vocabulary true out-of-vocabulary words mapped embedding ‘unknown word’ symbol whereas ‘pruned-out’ words mapped zero embedding. however original unpruned model uncommon words already near-zero embeddings indicating model unable learn sufﬁciently distinctive representations. returning figure look eight weight matrices source target connections four layers. matrix corresponds matrix equation eight matrices observe weights connecting input crucial followed input gate output gate forget gate particularly true lower layers focus primarily input however higher layers especially target side weights connecting gates important connecting input gates represent lstm’s ability delete retrieve information memory cell. figure therefore shows sophisticated memory cell abilities important pipeline reasonable expect higher-level features learned later deep learning pipeline. also observe lower layers feedforward input much important recurrent input whereas higher layers recurrent input becomes important. makes sense lower layers concentrate low-level information current word embedding whereas higher layers make figure validation loss training pruning retraining. vertical dotted line marks point parameters pruned. horizontal dotted line marks best performance unpruned baseline. starting sparse models favorable performance pruned retrained models raises question shortcut performance starting sparse models? rather train prune retrain simply prune train? test this took sparsity structure pruned models trained completely models sparsity structure. purple line figure shows ‘sparse beginning’ models perform well pruned retrained models come close baseline performance. shows sparsity structure alone contains useful information redundancy therefore produce competitive compressed model important interleave pruning training. though method involves pruning stage pruning methods interleave pruning training closely including several iterations expect implementing would likely result compression performance improvements. storage size original unpruned model size pruned retrained model reduction. work focus compression terms number parameters rather storage size beshown weight pruning retraining highly effective method compression regularization state-of-the-art system compressing model size loss performance. though ﬁrst apply compression techniques obtain similar degree compression current work compressing state-of-the-art deep neural networks approach simpler most. found absolute size parameters primary importance choosing prune leading approach extremely simple implement applied neural network. lastly gained insight distribution redundancy architecture. work partially supported award iis- partially supported gift bloomberg l.p. gratefully acknowledge support defense advanced research projects agency communicating computers program prime contract wnf---. lastly acknowledge nvidia corporation donation tesla gpus. lastly close inspection notice several white diagonals emerging within subsquares matrices figure indicating even without initializing weights identity matrices identity-like weight matrix learned. higher pruning percentages diagonals become pronounced. test generalizability results also test pruning approach smaller nonstate-of-the-art model trained vietnamese-english dataset consists sentence pairs. model effectively scaled-down version state-of-the-art model luong fewer layers smaller vocabulary size smaller hidden layer size attention mechanism many parameters total. achieves bleu score validation set. although model training different scale main model language pair different found similar results. model possible prune parameters immediate performance loss retraining possible prune regain original performance. main observations sections also replicated; particular class-blind pruning successful ‘sparse beginning’ models less successful pruned retrained models observe patterns seen figure noted section including several iterations pruning retraining would likely improve compression performance pruning method. possible would highly valuable exploit sparsity pruned models speed training runtime perhaps sparse matrix representations multiplications though found magnitude-based pruning perform well would instructive revisit original claim pruning methods principled perform comparative study. references gethsiyal augasta thangairulappan kathirvalavakumar. pruning algorithms neural networks comparative study. central european journal computer science kyunghyun bart merrienboer caglar gulcehre fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. emnlp. emily denton wojciech zaremba joan bruna yann lecun fergus. exploiting linear structure within convolutional networks efﬁcient evaluation. nips. forrest iandola matthew moskewicz khalid ashraf song william dally kurt keutzer. squeezenet alexnet-level accuracy fewer parameters model size. arxiv preprint arxiv.. rohit prabhavalkar ouais alsharif antoine bruguier mcgraw. compression recurrent neural networks application lvcsr acoustic modeling embedded speech recognition. icassp.", "year": 2016}