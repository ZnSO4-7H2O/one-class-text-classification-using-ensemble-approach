{"title": "Probabilistic Data Analysis with Probabilistic Programming", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Probabilistic techniques are central to data analysis, but different approaches can be difficult to apply, combine, and compare. This paper introduces composable generative population models (CGPMs), a computational abstraction that extends directed graphical models and can be used to describe and compose a broad class of probabilistic data analysis techniques. Examples include hierarchical Bayesian models, multivariate kernel methods, discriminative machine learning, clustering algorithms, dimensionality reduction, and arbitrary probabilistic programs. We also demonstrate the integration of CGPMs into BayesDB, a probabilistic programming platform that can express data analysis tasks using a modeling language and a structured query language. The practical value is illustrated in two ways. First, CGPMs are used in an analysis that identifies satellite data records which probably violate Kepler's Third Law, by composing causal probabilistic programs with non-parametric Bayes in under 50 lines of probabilistic code. Second, for several representative data analysis tasks, we report on lines of code and accuracy measurements of various CGPMs, plus comparisons with standard baseline solutions from Python and MATLAB libraries.", "text": "feras saad computer science artiﬁcial intelligence laboratory massachusetts institute technology cambridge vikash mansinghka department brain cognitive sciences massachusetts institute technology cambridge probabilistic techniques central data analysis diﬀerent approaches diﬃcult apply combine compare. paper introduces composable generative population models computational abstraction extends directed graphical models used describe compose broad class probabilistic data analysis techniques. examples include hierarchical bayesian models multivariate kernel methods discriminative machine learning clustering algorithms dimensionality reduction arbitrary probabilistic programs. also demonstrate integration cgpms bayesdb probabilistic programming platform express data analysis tasks using modeling language structured query language. practical value illustrated ways. first cgpms used analysis identiﬁes satellite data records probably violate kepler’s third composing causal probabilistic programs non-parametric bayes lines probabilistic code. second several representative data analysis tasks report lines code accuracy measurements various cgpms plus comparisons standard baseline solutions python matlab libraries. keywords probabilistic programming non-parametric bayesian inference probabilistic databases hybrid modeling multivariate statistics authors thank taylor campbell gregory marton alexey radul engineering support anthony richard tibbetts marco cusumano-towner helpful contributions feedback discussions. research supported darpa iarpa oﬃce naval research army research oﬃce gifts analog devices google. probabilistic techniques central data analysis diﬃcult apply combine compare. families approaches parametric statistical modeling machine learning probabilistic programming associated diﬀerent formalisms assumptions. paper shows address challenges deﬁning family probabilistic models integrating bayesdb probabilistic programming platform data analysis. also gives paper introduces composable generative population models computational formalism extends graphical models probabilistic programming. cgpms specify table observable random variables ﬁnite number columns countably inﬁnite number rows. support complex intra-row dependencies among observables well inter-row dependencies among ﬁeld latent variables. cgpms described computational interface generating samples evaluating densities random variables including entries table well broad class random variables derived conditioning. show implement cgpms several model families outputs standard discriminative learning methods kernel density estimators nearest neighbors non-parametric bayesian methods arbitrary probabilistic programs. also describe algorithms syntaxes probabilistic metamodeling language building compositions cgpms interoperate bayesdb. practical value illustrated ways. first paper outlines collection data analysis tasks cgpms high-dimensional real-world dataset heterogeneous types sparse observations. bayesdb script builds models combine non-parametric bayes principal component analysis random forest classiﬁcation ordinary least squares causal probabilistic program implements stochastic variant kepler’s third law. second illustrate coverage conciseness cgpm abstraction quantifying lines code accuracy achieved several representative data analysis tasks. estimates given models expressed cgpms bayesdb well baseline methods implemented python matlab. savings lines code cost improvement accuracy typical. remainder paper structured follows. section reviews related work cgpms graphical statistics probabilistic programming. section describes conceptual computational statistical formalism cgpms. section formulates wide range probabilistic models cgpms provides algorithmic implementations interface well examples invocations metamodeling language bayesian query language. section outlines architecture bayesdb cgpms. show cgpms composed form generalized directed acyclic graph constructing hybrid models simpler primitives. also present syntaxes building querying cgpms bayesdb. section applies cgpms several probabilistic data analysis tasks complex realworld dataset reports lines code accuracy measurements. section concludes discussion directions future work. directed graphical models statistics provide compact general-purpose modeling language describe factorization structure conditional distributions high-dimensional joint distribution node random variable conditionally independent non-descendants given parents conditional distribution given parents speciﬁed conditional probability table density cgpms extend mathematical description computational nodes random variables conditional densities also computational units interface allows composed directly software. cgpm node typically encapsulates complex statistical object single variable graphical model. node required input variables output variables variables associated statistical data types. nodes required simulate evaluate density subset outputs conditioning inputs well either conditioning marginalizing another subset outputs. internally joint distribution output variables single cgpm node speciﬁed general model either directed undirected. cgpms combine ideas vast literature modeling inference graphical models ideas probabilistic programming. paper illustrates cgpms integrating bayesdb probabilistic programming platform data analysis. bayesdb demonstrated bayesian query language express several tasks multivariate statistics probabilistic machine learning model-independent way. however idea illustrated emphasizing domain-general baseline model builder based crosscat limited support plug-in models called foreign predictors provides good enough performance common statistical tasks. limitations underlying formalism generative population models accept inputs learn joint distributions observable variables paper provide expressive modeling language constructing wide class models applicable diﬀerent data analysis tasks integrating domain-speciﬁc models built experts bayesdb. accepting input variables exposing latent variables queryable outputs cgpms provide concrete proposal mediating automated custom modeling using metamodeling language modelindependent querying using bayesian query language. cgpm abstraction thus exposes generality much broader model class originally presented includes hybrids models generative discriminative components. helpful contrast cgpms bayesdb probabilistic programming formalisms stan stan probabilistic programming language specifying hierarchical bayesian models built-in algorithms automated highly eﬃcient posterior inference. however straightforward integrate models diﬀerent formalisms discriminative machine learning sub-parts overall model directly query outputs model downstream data analysis tasks needs done per-program basis build composite programs smaller stan programs since program independent unit without interface. cgpms provide interface addressing limitations makes possible wrap stan programs cgpms interact bayesdb cgpms implemented systems. tabular schema-driven probabilistic programming language shares similarity composable generative population models. instance statistical representation cgpm probabilistic schema tabular characterize data generating process terms input variables output variables latent variables parameters hyper-parameters. however unlike tabular schemas cgpms explicitly provide computational interface general description internal structure facilitates composition tabular probabilistic programs centered around parametric statistical modeling factor graphs user manually constructs variable nodes factor nodes quantitative relationships them. hand cgpms express broad range model classes necessarily naturally admit natural representations factor graphs combine higher-level automatic model discovery user-speciﬁed overrides hybrid modeling. section describe composable generative population models computational abstraction provides uniform treatment broad class models methods probabilistic data analysis. section divided three parts. ﬁrst part formalizes notion statistical population terms random tabular data structure ﬁnite number columns countably inﬁnite number rows establishes notation used throughout paper. second part outlines computational interface deﬁnes cgpms. third part describes class statistical graphical models naturally expressed using cgpm framework. populations framework population deﬁned terms ﬁnite variables variable takes values general observation space variable qualitative interpretation particular property attribute members population. member population denoted t-dimensional vector element variable corresponding variable member entire population organized inﬁnite exchangeable sequence members. population conceptualized tabular data structure ﬁnite number columns inﬁnite number rows. column corresponds variable member cell element table associated observation spaces exchangeability assumption translates requirement unchanged permuting member ids. finally measurement deﬁned observed value cell data structure. general indicate element variable well measured value meaning typically clear context. collection measurements recorded inﬁnite table referred dataset helpful compare standard notion statistical population formalism described above. classical multivariate statistics data analysis tasks starts data matrix ﬁnite array containing measurements experiment additional modeling assumptions specify measurements random sample statistical population. members population generated distribution whose unknown parameters wish discover usage term statistical population thus combines domain knowledge observed data quantitative modeling assumptions umbrella idea. contrast framing characterizes population terms population variables observation spaces. framing commit probabilistic description data generating process intended invite questions populations without reference underlying statistical model. moreover every member deﬁnition population associated unique identiﬁer paper focuses modeling measurements conditioned member principle member could modeled process complex random sampling. moreover mathematical speciﬁcation population attempts granular standard formalism multivariate statistics. explicitly diﬀerentiate variable elements versions variable member. separating variable related element-level variables carefully accounting elements data structure discuss precisely mathematical algorithmic operations performed cgpms. level analysis would possible coarsely speciﬁed population single random vector viewed measurements collected data matrix independent realizations moreover specifying measurements cell level deals arbitrary/sparse patterns observations inﬁnite table contrast standard notion data matrices often treated objects linear algebra. similarly explicitly notating observation spaces allows capture heterogeneity population variables rather assume universe t-dimensional euclidean space. characteristics common real-world populations arise probabilistic data analysis. established populations introduce composable generative population models terms computational interface provide. composable generative population model characterizes data generating process population cgpm selects population variables output variables input member responsible modeling full joint distribution variables denote output variable similarly input variable member elements often collected vectors respectively incorporate measurements recorded cell-level allowing sparse subset observations member exist. measurement either output element input element simulate logpdf computed single member population. query parameter diﬀers methods simulate {qk} indices output variables simulated jointly; logpdf values output variables whose density assessed jointly. evidence parameter simulate logpdf contains additional information possibly including values output variables disjoint query variables. particular empty cgpm asked marginalize output variables query empty cgpm required condition output values. target distributions simulate logpdf also conditioned previously incorporated measurements dataset cgpms generally model populations inter-row dependencies measurements members relevant simulate logpdf query cgpm interface allows user override previous measurement per-query basis; occurs element evidence contradicts existing asking hypothetical queries addresses several tasks intermeasurement probabilistic data analysis simulating what-if scenarios detecting outliers high-dimensional populations. finally infer procedure evolves cgpm’s internal state response inﬂow measurements. inference program based learning strategy applicable cgpm markov chain monte carlo transitions variational inference maximum-likelihood least-squares estimation learning. previous section outlined external interface deﬁnes cgpm without specifying internal structure. practice many cgpms described using general graphical model directed undirected edges. data generating process characterized collection variables graph figure internal independence constraints broad class composable generative population models. nodes diagram multidimensional. internally hyperparameters ﬁxed known quantities. global latents shared members population. member-speciﬁc latents interact corresponding observations well member-latents indicated dashed loop around plate. nodes across diﬀerent members independent conditioned member-latents. however general dependencies permitted within elements node input variables ambient conditioning variables population always observed; general output another cgpm externally speciﬁed opaque binary e.g. probabilistic program describing data generating process outputs inputs specify variable names simulate logpdf. notion global local latent variables common motif hierarchical modeling literature useful specifying constraints governing dependence observable variables terms latent structure. lens cgpms satisfy following conditional independence constraint equation formalizes notion dependencies across members fully mediated global parameters member-speciﬁc variables {zr}. however elements within member free assume dependence structure allowing arbitrary inter-row dependencies. feature allows cgpms express undirected models output variables exchangeably-coupled gaussian markov random ﬁelds common specialization constraint requires member-speciﬁc latent variables {zr} conditionally independent given comprehensive list models machine learning statistics satisfying additional constraint given however cgpms permit general dependencies member latents coupled conditioned thus allowing complex intra-row dependencies. cgpms thus used models gaussian process regression noisy observations member-speciﬁc latent variables across diﬀerent members population jointly gaussian figure summarizes ideas showing cgpm graphical model. finally note also possible cgpm fully implement interface without admitting natural representation terms graphical structure figure shown several examples section providing computational interface cgpm interface provides layer abstraction separates internal implementation probabilistic model generative process represents. section explore computational description cgpm provides fundamentally diﬀerent understanding statistical description. example consider dirichlet process mixture model expressed cgpm. hyperparameters base measure concentration parameter parametric distribution observable variables {xr}. member latent variable cluster assignment consider diﬀerent representations underlying leading diﬀerent notion population parameters conditional independence constraints. stick breaking representation population parameters atoms parameterize likelihood weights conditioned member latents independent ∼iid categorical. chinese restaurant process representation population parameters atoms weights fully collapsed out. conditioned member latents exchangeably coupled crp. internal representation choices exposed cgpm interface interchanged without altering queries answer. follows computational description cgpms provides abstraction boundary particular implementation probabilistic model generative process population represents. implementations cgpm encapsulate process inducing identical marginal distribution observable variables maintaining diﬀerent auxiliary-variable representations internally. encapsulation cgpm’s internal state relaxed asking cgpm expose member-speciﬁc latent variables outputs. terms inﬁnite table metaphor section operation conceptualized cgpm fantasizing existence columns underlying population. providing gateway internal state cgpm trades-oﬀ model independence interface ability query hidden structure particular probabilistic process. section describes surface-level syntaxes exposing latent variables section illustrates utility inferring latent cluster assignments inﬁnite mixture model well simulating projections high-dimensional data onto low-dimensional latent subspaces. section illustrate computational abstraction cgpms applicable broad classes modeling approaches philosophies. table shows collection models whose internal structure develop perspective cgpms. section shows comparisons cgpms practical application data analysis tasks. section cross categorization section ensemble classiﬁers regressors section factor analysis probabilistic section parametric mixture experts section multivariate kernel density estimation section generative nearest neighbors section non-parametric bayesian generative modeling discriminative machine learning dimensionality reduction discriminative statistical modeling classical multivariate statistics clustering based generative modeling probabilistic programming marginalizing output variables query evidence. conditioning marginalizing joint distributions allow users cgpms pose nontrivial queries populations arise multivariate probabilistic data analysis. algorithms generally assume information known member simulate logpdf provided evidence parameter. extending implementations deal observed members mostly straightforward often implementation-speciﬁc. also note ﬁgures subsections contain excerpts probabilistic code bayesian query language metamodeling language venturescript; syntaxes outlined section finally leave many possible implementations infer cgpm learns latent state using observed data primarily external references. statistical data type population variable provides reﬁned taxonomy observation space described section table shows collection statistical data types available metamodeling language complex cgpms built. support statistical type deﬁnes samples simulate take values. statistical type also associated base measure ensures logpdf well-deﬁned. high-dimensional populations heterogeneous types logpdf taken product measure univariate base measures. statistical type also identiﬁes invariants variable maintains. instance values nominal variable permutation-invariant; distance values cyclic variable deﬁned circularly etc. ﬁnal column table shows primitive univariate cgpms compatible statistical type. simple cgpms logpdf implemented directly probability density functions algorithms simulate well-known infer cgpms ﬁxed parameters learn data using i.e. maximum likelihood bayesian priors cross-categorization bayesian non-parametric method learning joint distribution variables heterogeneous high-dimensional population generative model begins ﬁrst partitioning variables blocks. step crosscat’s outer clustering since partitions columns. denote variable partition |π|} denote blocks. global latent variable dictates structural dependencies variables; collection variables diﬀerent blocks mutually independent variables block mutually dependent. follows member joint distribution factorizes bundle global parameters includes well block-speciﬁc latent variables {θb}b∈π. within block dependent variables elements conditionally independent given member-speciﬁc latent variable variable inner clustering assignment crosscat since speciﬁes cluster identity respect variables block joint distribution elements factorizes global parameter parameterizes primitive univariate cgpm cluster parameter governing distribution latent variable description fully speciﬁes crosscat factorization joint distribution generative template encoded hierarchical bayesian model specifying priors partition mixture weights block distributional parameters contrast algorithm presents fully uncollapsed representation crosscat prior using distribution inner described generative process established notation outline algorithms logpdf simulate. since crosscat bayesian cgpm distribution interest pg|xd) requires marginalize latent variables sampling posterior covered focus implementing simulate logpdf assuming posterior samples latents available. implementations summarized algorithms routines access posterior sample latent variables algorithm algorithms based uncollapsed crosscat practice parameter-prior primitive cgpms lines algorithm form conjugate pair. density terms pg|φ) computed marginalizing using suﬃcient statistics cluster along column hyperparameters i.e. pg|{xc] λi). rao-blackwellization enhances inferential quality predictive performance crosscat sample approximation line algorithm instance algorithm becomes exact evaluating logpdf. section contains discussion implications diﬀerent internal representations generative process perspective cgpms. sample concentration outer sample partition variables block variable partition sample concentration inner sample stick-breaking weights clusters variable population sample hyperparams hyperprior sample component distribution params member population block variable partition sample cluster assignment variable block sample observable element initialize empty sample block variable partition retrieve posterior probabilities proposal clusters sample cluster query variable block sample observation element overall sample query variables black dots represent observed samples noisy ring decreasing noise level. colored dots represent samples crosscat’s posterior predictive minutes analysis. color point indicates latent cluster assignment crosscat’s inner dirichlet process mixture. panel illustrates phenomenon known bayes occam’s razor. higher noise levels less evidence patterns data posterior prefers less complex model small number large clusters. lower noise levels evidence functional relationship posterior prefers complex model large number small clusters required emulate ring. heatmaps show evolution crosscat’s posterior predictive density increasing number inference transitions given ring ﬁxed noise level brighter shades green indicate greater density mass region. surface plots right heatmap show density projected three dimensions. early stages inference density surface unimodal appears cloud plane. modalities patterns data captured increasing inference markov chain centers regions high posterior mass crosscat’s latent state. section describe construct cgpms class ensemblebased classiﬁers regressors common machine learning. cgpms typically described graphical model still able satisfy cgpm interface implementing simulate logpdf. member assume cgpm generates single output variable requires input feature vector ensemble method carries learners learner returns point prediction given denoted simple example represent random forest learner constituent decision tree. infer construct ensemble learners given measurements using meta-learning algorithm boosting bagging others. classification denote possible values output variable given input simplest strategy deﬁne probability event compute proportion learners ensemble predict baseline strategy guarantees discrete probabilities however suﬀers degeneracy simulate logpdf undeﬁned empty. address issue introduce smoothing parameter probability output uniform symbols probability aggregate outputs learners practice prior placed smoothing parameter uniform) transitioned gridded gibbs sampling prediction likelihood measurement set. distribution given hypothesis space learners expect limn→∞ simulate logpdf implemented directly regression regression setting predictions {lk} returned learner real-valued discrete aggregation strategy lead well-deﬁned implementation logpdf. instead input vector ensemble-based regression cgpm ﬁrst computes predictions incorporates primitive univariate cgpm compatible statistical type output variable normal numerical lognormal magnitude. strategy statistical type appropriate noise model based variability responses learners relates noisy regression implementations logpdf simulate directly inherited constructed primitive cgpm. development factor analysis closely follows extend exposition describe implementations simulate logpdf arbitrary patterns latent observable variables. factor analysis continuous latent variable model vector basis vector d-dimensional vector dimension latent space less member latents known factor scores represent low-dimensional projection global latents bases covariance matrix noise mean vector specify generative model memberspeciﬁc latent variables given prior normal. combining prior joint distribution latent observable variables deﬁned joint vector rd+l. cgpm implementing factor analysis exposes member-speciﬁc latent variables output variables. multivariate normal provides ingredients simulate logpdf pattern latent observable variables query evidence arrive target distribution bayes theorem gaussians invoked two-step process. implementation infer uses expectation maximization factor analysis alternative approach posterior inference bayesian setting finally probabilistic principal component analysis recovered covariance constrained satisfy %mml create table iris ‘iris.csv’; %mml create population iris %mml create metamodel regression model data exhibit highly non-linear characteristics heteroskedastic noise piecewise continuous patterns. cgpm generates output variables given input variables using mixtures local parametric mixtures. member latent variable takes values induces naive bayes factorization outputs regression parameters variable looks similar naive bayes factorization crosscat diﬀer important ways. crosscat variables sampled primitive univariate cgpms mixture experts sampled discriminative cgpm conditioned term pg|yr generalized linear model correct statistical data type second mixture experts gating function also conditioned general function softmax even dirichlet process mixture crosscat member latents necessarily given prior block. leave implementations simulate logpdf refer figure comparison posterior samples crosscat mixture experts given data piecewise continuous function. figure posterior samples crosscat mixture experts given piecewise continuous linear function. observed data points shown black posterior samples shown color represents latent cluster assignment internal cgpm. crosscat emulates curve using mixture axis-aligned gaussians requiring larger number small noisy clusters. mixture linear regression experts identiﬁes linear regimes able interpolate well orange datapoints appear outliers samples singleton cluster since gating function implemented using dirichlet process mixture. section present compositional generative population model implements simulate logpdf building ad-hoc statistical models per-query basis. method simple extension nearest neighbors generative modeling. generative nearest neighbor cgpm denote query evidence simulate logpdf query. method ﬁrst ﬁnds nearest neighbors dataset based values evidence variables denote neighbors whose generic member denoted within assume query variables independent learn cgpm product primitive univariate cgpms measurements used learn primitive cgpm neighborhood. procedure summarized algorithm implementations simulate logpdf follow directly product cgpm summarized algorithms figure illustrates behavior simulate synthetic x-cross varies neighborhood size parameter noted building independent models neighborhood result poor performance query variables remain highly correlated even conditioned evidence. baseline approach modiﬁed capture dependence query variables instead building independent cgpm around local neighborhood neighbor rather independent cgpm entire neighborhood. improvements left future work. marginalize exclusion neighbor search neighbors query variable initialize primitive cgpm neighbor incorporate primitive cgpm transition primitive cgpm collection primitive cgpms figure posterior samples generative nearest neighbors cgpm given x-cross varying values neighbors samples synthetic x-cross data generator. produces three variables real-valued scattered plane binary variable indicating functional regime. small neighborhoods members neighborhood satisfy reﬂected sharp posterior distribution neighborhood size increases become noisy include members smoothing posterior section show express multivariate kernel density estimation mixed data types developed using cgpms. similarly ensemble methods approach implements cgpm interface without admitting natural representation terms graphical model figure extend exposition include algorithms conditional sampling density assessment. given measurements joint distribution variables estimated non-parametrically product kernel global parameter containing bandwidths kernel note using product kernel imply independence elements within member. bandwidths typically learned cross-validation maximum-likelihood. nominal statistical type symbols kernel combining provides immediate algorithm logpdf. implement simulate begin ignoring normalizing constant denominator unnecessary sampling. express numerator suggestively particular simulate algorithm ﬁrst samples member categorical weight labeled next samples query elements independently corresponding kernels curried intuitively cgpm weights member population well local kernel explains evidence known section show construct composable generative population model directly terms computational statistical deﬁnitions section expressing venturescript probabilistic programming language. simplicity section assumes cgpm satisﬁes words every observation element exists latent variable mediates coupling variables population. member latent variables still exhibit arbitrary dependencies within among another. essential requirement simpliﬁes exposition inference algorithms. approach simulate logpdf based approximate inference tagged subparts venture trace. cgpm carries independent samples {θk}k global latent variables assigned weights per-query basis. since venturescript cgpms bayesian target distribution simulate logpdf marginalizes internal state weight pg|θkd) likelihood evidence weighting scheme computational trade-oﬀ circumventing requirement inference population parameters per-query basis i.e. given evidence suggests simulate implemented sampling joint local posterior zr|x θkd} returning elements shows logpdf implemented ﬁrst sampling member latents local posterior. invoking conditional independence constraint query factors product density terms element evaluated directly. description completes algorithm simulate logpdf trace repeated θk}. cgpm implements simulate drawing trace categorical returning sample similarly logpdf computed using weighted monte carlo estimator algorithms illustrate implementations general probabilistic programming environment. figure composing venturescript expressions compiling cgpms. expressions teal lambda expressions anonymous functions venturescript compiled cgpms inline_venturescript adapter. forward simulation inversion joint generative model achieved algorithm code instance polyglot probabilistic programming; includes expressions diﬀerent languages interacting single program. plot shows samples forward simulating middle plot shows samples given y=-. successfully capturing posterior modes; bottom plot shows overlay. trace retrieve weight obtain samples latents scope transition operator leaving target invariant compute density estimate aggregate density estimates simple monte carlo importance weight estimate weighted importance sampling estimator without probabilistic programming systems languages treat data analysis computationally diﬃcult utilize expressive power cgpms general-purpose inference machinery develop query them. section show cgpms integrated bayesdb probabilistic programming platform languages bayesian query language model-independent querying metamodeling language model discovery building. ﬁrst describe simple queries directly invocations cgpm interface. show compose cgpms networks outline expressions used construct populations networks cgpms. experiments section illustrate extending bayesdb cgpms used non-trivial data analysis tasks. figure system architecture modules comprise bayesdb. metamodeling language interpreter reads population schemas deﬁne variables statistical types metamodel deﬁnitions apply automatic custom modeling strategies groups variables population commands initialize instantiates ensemble cgpm networks analyze applies inference operators cgpms learn observed data. bayesian query language model-independent probabilistic query language allows users estimate properties cgpms strength existence dependence relationships between variables similarity members conditional density queries simulate missing hypothetical observations subject user-provided constraints. together components allow users build population models query probable implications data. interpreter allows users probabilistic questions populations using structured query language. figure shows queries simulate estimate probability translate invocations simulate logpdf illustrative population cgpm. deﬁnes large collection row-wise column-wise estimators cgpms mutual information dependence probability similiarity respect quantities admit default implementations terms monte carlo estimators formed simulate logpdf cgpm override interpreter’s generic implementations custom optimized implementation. full description implementing terms cgpm interface beyond scope work. mapping simulate query cgpm interface invocation simulate. sampled quantity also includes conditioning value extracted dataset cgpm must condition every observed value well additional per-query constraints speciﬁed user result table rows corresponding requested samples. development cgpms focused computational interface internal probabilistic structures. section outline mathematical formalism justiﬁes closure cgpms input/output composition. collection cgpms operating population show organized generalized directed graph cgpm provide monte carlo strategy performing joint inference outputs inputs internal cgpms. composition allows complex probabilistic models built simpler cgpms. communicate another using simulate logpdf interface answer queries overall network. next section describe surface syntaxes construct networks cgpms bayesdb. variables cgpm generates outputs vout subset inputs composition applies subset outputs vout vout resulting cgpm output input i.e. share output rules composition require correspond subset variables original population genvout eralizing idea further collection cgpms thus organized graph node represents internal cgpm labeled edge denotes composition ◦g). labeled edges diﬀerent cgpms network must form directed acyclic graph. however elements member within particular required satisfy constraint general follow directed and/or undirected dependencies. topology overall cgpm network summarized generalized adjacency matrix vout output elements upstream cgpms connected inputs illustrate class cgpms closed composition need show network implements interface. first note produces outputs union output variables constituent cgpms takes inputs collection variables population output cgpm network. latter collection variables exogenous network must provided queries require them. implementations simulate logpdf shown algorithms algorithms importance sampling scheme combines methods provided individual node shared forward-sampling subroutine algorithm estimator logpdf uses ratio likelihood weighting; estimators derived lines algorithm computed using unnormalized importance sampling ratio estimator line exact inﬁnite limit importance samples algorithms explicitly pass member cgpm agree member-speciﬁc latent variables relevant query preserving abstraction boundaries. importance sampling strategy used compositional simulate logpdf feasible networks shallow primitive cgpms fairly noisy; better monte carlo strategies perhaps even variational strategies needed deeper networks left future work. network’s infer method implemented invoking infer separately internal cgpm node. general several improvements baseline strategy possible also interesting areas research number importance samples identiﬁer population indices cgpm nodes network cgpm representing node parents node input variables exogenous network node query node evidence node query/evidence sets aggregated nodes network table parameters symbols used algorithms parameters provided functions appear. weighted-sample ignores query evidence global environment provided explicit constrained nodes simulate logpdf. initialize empty sample zero weight topologically sort adjacency matrix retrieve required inputs node update weight constraint likelihood simulate unconstrained nodes append sample overall sample weight shown figure interpreter bayesdb interacts data tables populations metamodels library cgpms. population schemas programs used declare list variables statistical types. every population backed base table bayesdb stores measurements. metamodel deﬁnitions programs used declare composite network cgpms given population. internal cgpms nodes network come cgpm library available bayesdb. declaring population metamodel commands used instantiate stochastic ensembles cgpm networks apply inference operators section describe surface level syntaxes metamodeling language population schemas metamodel deﬁnitions commands. also describe bayesian query language query ensembles cgpms varying levels granularity. formal semantics precisely describes relationship compositional surface syntax network cgpms left future work. uses existing measurements base table guess statistical data types columns table. argument target columns appear model ignore. argument subset columns guessed. every column base table must derivable policy schema. statistical data types available shown table guess command implemented using various heuristics measurements assigns variable either nominal numerical. using reﬁned statistical type variable achieved explicit model...as command. finally populations identical base tables variables diﬀerent statistical type assignments considered distinct populations. metamodel definitions creating population bayesdb metamodel deﬁnitions declare cgpms population. program speciﬁes topology internal cgpm nodes network starting baseline cgpm rootof graph nodes edges constructed sequence overrides extract variables root node place newly created cgpm nodes. syntax metamodel deﬁnition identiﬁes automatic model discovery engine learns full joint distribution variables population baselines include cross-categorization multivariate kernel density estimation generative k-nearest-neighbors overrides baseline-cgpm creating node cgpm network. node generates output-vars possibly requires speciﬁed input-vars. additionally cgpm expose latent variable queryable outputs. token cgpm-name refers name cgpm overriding baseline-cgpm speciﬁed subpart joint distribution. answer arbitrary queries population bayesdb requires cgpm carry full joint model population variables. thus metamodel declared baseline cgpm crosscat non-parametric bayesian structure learner high-dimensional heterogeneous data tables among others outlined section important note input-vars override model command outputs baseline collection upstream cgpms. also possible completely override baseline overriding variables population. homogeneous ensembles cgpm networks bayesdb metamodel formally deﬁned ensemble cgpm networks weight network cgpms homogeneous metamodel deﬁnition created population inputs outputs binary. ensemble populated instances cgpms using following command cgpm instances ensemble diﬀerent bayesdb provides unique seed create. means invoking infer causes network’s internal state evolve diﬀerently course inference surface syntax infer invoked using following command variables runs analysis cgpm nodes least output variable var-names. skip transitions cgpm nodes except output variable var-names. outlined section cgpm node learned independently present time. weighted ensembling homogeneous cgpms interpreted based modeling inference tactics internal cgpm. example bayesian cgpm network analyze invokes mcmc transitions represent diﬀerent posterior sample; variational inference converge diﬀerent latent parameters diﬀerent random initializations. extensive syntaxes inference plans left future work. heterogeneous ensembles cgpm networks section deﬁned metamodel ensemble homogeneous cgpm networks metamodel deﬁnition. also possible construct heterogeneous ensemble cgpm networks deﬁning metamodels population diﬀerent metamodel deﬁnitions. cgpm network metamodel bayesian query language able query cgpm networks three levels granularity starting coarse granular. monte carlo estimators obtained simulate logpdf remain well-deﬁned even ensemble contains heterogeneous cgpms. cgpms across diﬀerent metamodels deﬁned population determines statistical types variables. guarantees associated supports base measures simulate logpdf queries type-matched. informative compare conceptual technical diﬀerences generative population models bayesdb composable generative population models original presentation interface served purpose primary vehicle motivating model-independent query language moreover gpms based around crosscat baseline model-discovery engine provided good solutions several data analysis tasks. however accepting inputs gpms oﬀered means composition; non-crosscat objects known foreign predictors discriminative models embedded directly crosscat joint density contrast main purpose cgpm interface motivate expressive syntaxes building hybrid models comprised arbitrary generative discriminative components. since cgpms natively accept inputs admit natural form composition violate internal representation particular cgpm. computational interface probabilistic structure gpms cgpms diﬀerent several respects. gpms presented bayesian models markov chain monte carlo inference simulate logpdf explicitly conditioned particular latent variables extracted state posterior inference chain hand cgpms capture much broader model classes simulate logpdf impose conditioning constraints internal model besides conditioning input variables entire dataset internally gpms enforced much stronger assumptions regulating inter-row independences; elements conditionally independent give latent variable eﬀectively restricting internal structure directed graphical model. cgpms allow arbitrary coupling elements within uniformly expresses directed undirected probabilistic models well approaches naturally probabilistic implement interface. finally unlike gpms cgpms expose member-speciﬁc latent variables queryable outputs. features trades-oﬀ model independence ability learn query details internal probabilistic process encapsulated cgpm. ﬁrst part section outlines case study applying compositional generative population models bayesdb population satellites maintained union concerned scientists. dataset contains entries satellites numerical categorical features material functional physical orbital economic characteristics. construct hybrid cgpm using metamodel deﬁnition combines classical physics model written probabilistic program venturescript random forest classify nominal variable ordinary least squares regressor predict numerical variable principal component analysis real-valued features satellites. cgpms allow identify satellites probably violate orbital mechanics accurately infer missing values anticipated lifetime visualize dataset projecting satellite features dimensions. second part section explores eﬃcacy hybrid compositional generative population models collection common tasks probabilistic data analysis reporting lines code accuracy measurements standard baseline solutions. large savings lines code improved accuracy demonstrated several important regimes. analysis experimental results contained ﬁgure gallery section. left panel figure illustrates session declares population schema satellites data well metamodel deﬁnition building hybrid cgpm network models various relationships interest variables. create population block shows high-dimensional features satellite heterogeneous statistical types. simplicity several variables perigee_km launch_mass_kg anticipated_lifetime modeled numerical rather reﬁned type magnitude. remainder section explain cgpms declared metamodel deﬁnition create metamodel block refer ﬁgures results queries executed them. cgpm line metamodel deﬁnition generates output real-valued variables exposes ﬁrst principal component scores bayesdb. low-dimensional projection allows visualize clustering dataset latent space discover oddities distribution latent scores satellites whose class_of_orbit elliptical. also identiﬁes single satellite cyan grid point candidate investigation. figure shows result commentary experiment. four variables population relate orbital characteristics satellite apogee_km perigee_km period_minutes eccentricity variables constrained theoretical keplerian relationships physical constant. reality satellites deviate theoretical orbits variety reasons orbital measurement noise engines even data-entry errors. right panel figure shows cgpm pure venturescript accepts input generates output prior dirichlet process mixture model program executed iventure experimental interactive probabilistic programming environment supports running %bql %mml %venturescript code cells operate common underlying bayesdb instance venture interpreter. internal details external interface adapter compiles venturescript source cgpm beyond scope paper note declaration uses expose command line command makes inferred cluster identity noise latent variables available bql. figure shows posterior sample cluster assignments error distribution identiﬁes three distinct classes anomalous satellites based magnitude error. instance satellite orion right panel figure belongs cluster extreme deviation. investigation reveals orion period minutes data-entry error true period hours figure shows improvement prediction accuracy achieved hybrid cgpm purely generative crosscat baseline challenging multiclass classiﬁcation task. shown lines metamodel deﬁnition figure hybrid cgpm uses random forest cgpm target variable type_of_orbit given numerical categorical predictors. figures shows confusion matrices test composite baseline cgpms. methods systematically confuse sun-synchronous intermediate orbits random forest classiﬁer results less classiﬁcation errors improvement percentage points. using purely discriminative model task i.e. random forest without generative model features would require additional logic heuristic imputation feature vectors test general contained missing entries. ﬁnal experiment figure compares posterior distribution vanilla crosscat baseline multivariate two-dimensional density estimation task nominal data types. task jointly simulate country_of_operator purpose hypothetical satellite given type_of_orbit geosynchronous. empirical conditional distribution dataset shown red. crosscat multivariate capture posterior modes although distribution form fatter tail indicated high number samples classiﬁed other. ﬁgure caption contains additional discussion. dozens additional queries posed satellites population based analysis task interest answered using existing cgpms hybrid metamodel well customized cgpms. empirical studies section shown possible practical apply cgpms bayesdb challenging data analysis tasks realworld dataset queries compare performance characteristics. sparsely observed variables satellites dataset anticipated_lifetime roughly four missing entries. analysis task figure infer anticipated lifetime satellite given subset numerical nominal features shown codeblock plot. quantify performance predictions cgpm evaluated held-out satellites known lifetimes. many satellites training test contained missing entries covariates requiring cgpm additionally impute missing values predictors forward simulating regression. unlike purely generative purely discriminative baselines hybrid cgpm learns joint distribution predictors discriminative model response leading signiﬁcantly improved predictive performance. improvement lines code baseline methods figure using combinations data processing model building predictive querying bayesdb. baselines required custom logic manual data preprocessing reading ﬁles euclidean embedding large categorical values heuristic imputation missing features train test time left panel figure shows end-to-end session bayesdb preprocesses data builds hybrid cgpm runs analysis training computes predictions test set. right panel figure shows single ad-hoc routine used python baselines dummy codes data frame missing entries nominal data types. nominal variables taking values large dummy coding zeros cause solvers fail system under-determined. workaround code baselines drop problematic dimensions feature vector. regression hybrid cgpm suﬀer problem because default linear regressor cgpm library gives parameters bayesian prior smooths irregularities. figures extend lines code accuracy comparisons cgpms baseline methods several tasks using diverse statistical methodologies. ﬁgures illustrate coverage conciseness cgpms captions detail setup commentary experiment greater detail. country_of_operator operator_owner purpose class_of_orbit type_of_orbit users contractor launch_vehicle country_of_contractor launch_site source_used_for_orbital_data figure finding satellites whose orbits likely violations kepler’s third using causal cgpm venturescript learns dirichlet process mixture residuals. scatter plot satellite dataset color represents latent cluster assignment learned causal cgpm. cluster identity inferred noise exposed latent variables. histogram shows four distinct clusters roughly translates qualitative description magnitude satellite’s deviation theoretical period yellow magenta green blue clusters learned non-parametrically. figure dimensional projection satellites using cgpm reveals clusterings latent space suggests candidate outliers. principal component scores based numerical features satellite color class_of_orbit. satellites earth medium earth geosynchronous orbit form tight clusters latent space along exhibit within-cluster variance along distribution factor scores elliptical satellites much higher variability along dimensions indicating collection weak local modes depending regime satellite’s eccentricity and/or many statistical outliers. figure confusion matrices multiclass classiﬁcation task show improved prediction accuracy hybrid cgpm crosscat baseline. y-axis shows true label type orbit held-out satellites x-axis shows predicted label cgpm. feature vectors dimensional consist numerical categorical variables test training sets contained missing data. crosscat crosscat random forest systematically confuse sun-synchronousand intermediate orbits overall error rate reduced hybrid cgpm. figure simulating joint distribution country purpose hypothetical satellite given orbit type. y-axis shows simulated country-purpose pairs x-axis shows frequency simulations compared true frequency dataset. samples obtained crosscat multivariate estimate posterior probabilities. posteriors crosscat smooth versions empirical data smoothing crosscat induced inner dirichlet process mixture category models induced bandwidth parameters aitchison aitken kernels. plot shows crosscat’s samples provide tighter dataset. distribution fatter tail indicated high number samples classiﬁed other category. figure high-dimensional regression problem mixed data types missing data composite cgpm shows improvement prediction accuracy purely generative purely discriminative baselines. task infer anticipated lifetime held-out satellite given categorical numerical features type orbit launch mass orbital period. feature vectors test missing entries leading purely discriminative models either heuristically impute missing features ignore features predict mean lifetime marginal distribution training set. purely generative model able impute missing data full joint distribution indirectly mediates dependencies predictors response latent variables. composite cgpm combines advantages approaches; statistically rigorous imputation followed direct regression features leads improved predictive accuracy. figure dependence discovery. binary hypothesis tests independence synthetic twodimensional data drawn noisy zero-correlation datasets wave parabola x-cross diamond ring. datasets dimensions dependent. y-axis shows fraction correct hypotheses achieved method averaged datasets. decision rule kernel-based tests based frequentist signiﬁcance level decision rule crosscat based dependence probability threshold figure dependence strength estimating mutual information noisy wave. y-axis shows squared estimation error randomized observed datasets. ground truth mutual information derived analytically integral computed quadrature. baseline methods estimate mutual information using nearest neighbors kernel density estimation crosscat estimates mutual information ﬁrst learning dirichlet process mixture gaussians using monte carlo estimation generating samples posterior predictive distribution assessing density. figure bivariate categorical density estimation. simulating posterior joint distribution country purpose hypothetical satellite given orbit type. samples obtained method estimate posterior probabilities. y-axis shows hellinger distance posterior samples method empirical conditional distribution dataset used ground truth. standard discriminative baselines struggle learn distribution two-dimensional discrete outcome based discrete input predictor response variables take values large categorical sets. figure anomaly detection. detecting satellites anomalous orbital periods. satellites dataset demonstrated non-trivial deviation theoretical period used ground truth anomalies. method satellites ranked outlyingness score used predicted anomalies. hybrid cgpms learn multivariate multimodal distributions variables dataset leading higher detection rates baseline methods univariate and/or unimodal statistics. kepler cgpm identiﬁes anomalies expense highly complex program comparison baselines. paper shown possible computational formalism probabilistic programming apply combine compare broad class probabilistic data analysis techniques. cgpms extend core provided directed graphical models express elaborate probabilistic models terms smaller univariate pieces specifying computational interface allows pieces multivariate black-box deﬁned directly software. feature framework enables statistical modelers compose discriminative generative hybrid models diﬀerent philosophies machine learning statistics using probabilistic programming. moreover compositional abstraction neutral cgpm’s internal choices modeling assumptions i.e. hierarchical bayesian non-bayesian inference tactics i.e. optimizationsampling-based. several models statistics admit natural implementations terms current cgpm interface non-linear mixed eﬀect models member represents potentially repeated measurement latent variables grouping members observation units; gaussian processes input variables time indexes another cgpm outputs noisy observations function values computational representations models cgpms allows composable hybrid models reusable software queryable interesting ways using bayesian query language. simulate logpdf listing executed single member population i.e. variables within single row. queries target multiple members population currently supported explicit sequence incorporate infer simulate logpdf. interesting consider extending cgpm interface natively handle arbitrary multi-row cases idea originally presented interface although concrete algorithms implementing multi-row queries surface-level syntax bayesian query language invoking them left open questions. rather support multi-row queries directly cgpm interface instead possible extend interpreter probabilistic query planner. given given cross-row query interpreter automatically determines candidate invocation sequences cgpm interface answer selects among based time/accuracy requirements. worthy direction future work extending statistical data types possibly cgpm interface support analysis tasks beyond traditional multivariate statistics. possible data types associated cgpms composing cgpms data types leads interesting tasks induced joint distributions. consider image variable associated text annotation; generative cgpm image discriminative cgpm text leads image classiﬁcation; generative cgpm text discriminative cgpm image allows simulating unstructured text followed associated images. also interesting consider introducing additional structure current formalism populations section support richer notions population modeling. instance populations hierarchical variables population correspond outputs produced cgpm population simplest case summary statistics means medians inter-quartile ranges. hierarchical populations common census data contain measurements variables individual households well row-wise column-wise summaries based geography income level ethnicity educational background populations also extended support merge operations analogous join operations cgpm joined population allows transfer learning. presentation algorithm infer composite network cgpms left open improvements baseline strategy learning cgpm node separately. achieve joint learning without violating abstraction boundaries cgpm interface running infer individually cgpm reﬁne phase missing measurements population imputed using forward pass simulate throughout network cgpm updates parameters based imputed measurements. strategy repeated generate several imputed networks organized ensemble cgpms bayesdb metamodel cgpm metamodel corresponds diﬀerent imputations. weighted-averaging cgpms bayesdb would thus correspond integration diﬀerent imputations well induced parameters. extending developing probabilistic programming languages assess inference quality cgpms built important step toward broader application probabilistic programming tools real-world analysis tasks. instance possible develop command takes cgpms returns estimate divergence conditional predictive distributions based monte carlo estimator using simulate logpdf. model-independent estimators inference quality backed cgpm interface provide proposal unifying testing proﬁling infrastructure among range candidate solutions given data analysis task. paper shown possible unify formalize broad class probabilistic data analysis techniques integrating probabilistic programming platform integrated traditional database. focused class probabilistic models tightly integrated database tables. population schemas deﬁne variables interest along types unlike traditional database schemas additionally include variables whose values never directly observed. concrete probabilistic models populations built automated inference mechanisms according baseline meta-modeling strategy also customized. idea similar concrete indexes tables traditional databases built automated mechanisms according indexing strategy customized schema. encouraged early successes approach vast literature richer data modeling formalisms databases statistics. integrating ideas could yield conceptual insight practical beneﬁts. hope paper encourages others develop connections along generation intelligent tools machineassisted probabilistic data analysis.", "year": 2016}