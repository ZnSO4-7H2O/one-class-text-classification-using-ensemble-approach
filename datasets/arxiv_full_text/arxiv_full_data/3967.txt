{"title": "A nonclassical symbolic theory of working memory, mental computations,  and mental set", "tag": ["cs.AI", "cs.NE"], "abstract": "The paper tackles four basic questions associated with human brain as a learning system. How can the brain learn to (1) mentally simulate different external memory aids, (2) perform, in principle, any mental computations using imaginary memory aids, (3) recall the real sensory and motor events and synthesize a combinatorial number of imaginary events, (4) dynamically change its mental set to match a combinatorial number of contexts? We propose a uniform answer to (1)-(4) based on the general postulate that the human neocortex processes symbolic information in a \"nonclassical\" way. Instead of manipulating symbols in a read/write memory, as the classical symbolic systems do, it manipulates the states of dynamical memory representing different temporary attributes of immovable symbolic structures stored in a long-term memory. The approach is formalized as the concept of E-machine. Intuitively, an E-machine is a system that deals mainly with characteristic functions representing subsets of memory pointers rather than the pointers themselves. This nonclassical symbolic paradigm is Turing universal, and, unlike the classical one, is efficiently implementable in homogeneous neural networks with temporal modulation topologically resembling that of the neocortex.", "text": "paper tackles four basic questions associated human brain learning system. brain learn mentally simulate diﬀerent external memory aids perform principle mental computations using imaginary memory aids recall real sensory motor events synthesize combinatorial number imaginary events dynamically change mental match combinatorial number contexts? propose uniform answer based general postulate human neocortex processes symbolic information nonclassical way. instead manipulating symbols read/write memory classical symbolic systems manipulates states dynamical memory representing diﬀerent temporary attributes immovable symbolic structures stored long-term memory. approach formalized concept e-machine. intuitively e-machine system deals mainly characteristic functions representing subsets memory pointers rather pointers themselves. nonclassical symbolic paradigm turing universal unlike classical eﬃciently implementable homogeneous neural networks temporal modulation topologically resembling neocortex. conventional computers process symbolic information manipulating symbols read/write memory call buﬀer. classical symbolic computational paradigm encounters serious problems metaphor symbolic level information processing human brain. unlikely brain counterpart conventional buﬀer. even buﬀer existed would small slow allow brain eﬃciently process symbolic information traditional way. brain produce cognitive phenomena working memory mental computations language without buﬀer? tackle general question helpful start following observations working memory. people learn mentally simulate diﬀerent external memory aids properties read/write memory. example highly skilled abacus user learns compute imaginary abacus eﬃciently real device similarly experienced chess player learns play chess imaginary chess board. computer simulation chess board requires buﬀer less addresses symbols twelve symbols representing chess pieces colors symbol representing empty square. observation raises question show learning system uses gradient-descent-type statisticaloptimization-type learning algorithm learn simulate even small buﬀer though buﬀer deterministic system behavior statistically unpredictable traditional sense. means human brain cannot rely traditional statistical prediction techniques mentally simulate behavior external world. turing universality learning. person good visual memory taught perform principle mental computation imaginary memory aid. ignoring theoretically unimportant limitations size imaginary memory observation means human brain must treated system theorist turing universal learning system. interesting memorization recollection synthesis. people memorize recall long sequences real sensory motor events. time synthesize combinatorial number imaginary events. attractive think learning algorithm account outlined phenomena. combinatorial synthesis? argue learning algorithm attempts preprocessing learner’s experience putting experience learner’s cannot answer question. contrast algorithm simply memorizes learner’s experience call complete memory algorithm limitation mental context. people interpret inputs diﬀerently depending context. number possible contexts explodes exponentially. leads question system linearly growing size knowledge learn eﬃciently deal exponentially growing number possible contexts? problem attitude context calls question traditional approaches brain modeling cognitive modeling brain cannot diﬀerent mental agents possible contexts. accordingly consistent answer must explain brain dynamically synthesize agents depending context. paper oﬀers uniform answer questions based assumption human neocortex processes symbolic information nonclassical way. postulate that instead manipulating symbols read/write memory classical symbolic systems brain manipulates states dynamical memory representing diﬀerent temporary attributes immovable symbolic structures stored long-term memory. integrated symbolic-dynamical paradigm referred concept e-machine answering questions provides insight general question complex e-machine hierarchical associative learning system built several primitive associative learning systems called primitive e-machines main types states states encoded long-term memory representing symbolic knowledge states called g-states implying notion synaptic gain. states diﬀerent types dynamical short-term memory intermediate-term memory accounting context-dependent dynamic reconﬁguration knowledge stored ltm. states called e-states implying notion residual excitation. name e-machine emphasizes special importance e-states. demonstrate possibilities e-machine paradigm presenting example learning robot brain organized e-machine. robot interacts external world represented keyboard screen. model produces following educational eﬀects mental context. robot interprets visual input combinatorial number diﬀerent ways depending context created auditory input. rest paper consists following sections system cognitive model. external world generalized limitations traditional learning systems. concept primitive e-machine learn simulate gram. turing universality learning. eﬀect context-dependent mental set. promising directions research. methodological remarks. general architecture cognitive model used paper shown figure model consists external world robot consisting sensorimotor devices brain system-theoretical viewpoint convenient treat system composition subsystems external system brain representation systems viewed abstract machines outputs inputs vice versa. note brain know external world knows external system screen divided squares. simplicity squares shown. assume robot’s square time call scanned square idea borrowed turing also assume system tracking device robot depresses character appears scanned square. external system behaves essentially ram. motor inputs addr representing position typed character serve respectively address data input ram. sensory output dout serves data output. control input similar write-enable input ram. system write mode nonempty motor input din. otherwise system read mode. section formalize verbal description concept generalized systems so-called supervised learning mode. course training teacher produce desired output centers teacher also switch output centers output system output dout. ns.y dout system serves teacher system systems inputs denoted inputs deliver output signals needed learning. inputs often referred desired outputs. notation. follows matlab-like notation. array indices start symbol denotes range operator. symbol used delimiter control statements. rightmost parentheses include time index. example denotes value moment similarly dout denotes value dout moment gram always write mode input data present input data present gram read mode. approach need special control input e.g. write enable indicate write read mode. used notion gram helpful follow experiment gram shown figure three-row table upper part ﬁgure displays input/output sequence gram function discrete time entries shown blank squares. two-row instance memory empty mem) produces output dout writes location memory state input produces output dout writes location memory state input reads data location dout memory state change. rules type dout called ﬁxed rules. speciﬁc example ﬁxed rules rules. fixed rules easily extracted shown input/output sequence diﬀerent learning algorithms. rules type dout called variable rules. example variable rules dout dout. variable rules. output part dout variable rule depends recently executed ﬁxed rule address. example output rule dout dout recently executed ﬁxed rule addr rule variable rules cannot correctly executed learning system save information recently executed ﬁxed rules. useful view ﬁxed rules tool assigning right parts variable rules. approach meaning value address symbol variable rule depends recent assignment. discussed example address symbol assigned either meanings assume traditional learning system used system figure convenient redraw relevant part figure experimental setup shown figure external system replaced gram. think input desired output learning system. ns.sel gram serves target system system claim that experiment supervised learning traditional learning systems used system learn simulate target system properties gram. follows prove claim broad classes learning systems. theorem learning system statistical learning algorithm learns predict output target system samples input/output sequence. maximum length samples taken account exceed system cannot learn simulate system properties gram. proof. going show reaction gram input statistically unpredictable. suﬃcient speciﬁc example gram discussed section bigger gram always simulate smaller gram result theorem hold gram assume learned predict behavior gram. produce contradiction following test step send input sequence before except deﬁnition system predicts output input sequence longer take account reaction however reaction gram dout proves theorem. deﬁnition. learning system input alphabet ...x input sequence system loses information order input events exist satisfying condition reaction theorems show loss information time learning leads principal limitations time decision making. case theorem system attempted predict output gram using samples gram’s behavior limited length case theorem ignored order gram’s input events. states called states encoded long-term memory g-states. mentioned before letter implies notion synaptic gain. g-states represent symbolic knowledge e-machine. states called states dynamical short-term memory intermediate-term memory e-states. letter implies notion residual excitation. e-machine several types e-states representing diﬀerent temporary attributes data stored ltm. e-states serve mechanism context-dependent dynamic reconﬁguration knowledge represented g-states. next section present explicit example pem. model simple enough theoretically understandable time suﬃciently complex produce nontrivial cognitive phenomena. used system system figure figure illustrates architecture simple pem. interpretation procedure divided four elementary procedures decoding modulation choice encoding. model uses single e-state array next e-state procedure associations ltm. recorded association gets elevated level residual excitation produce eﬀect recency. described addition next e-state procedure. general nonnegative real number representing similarity example simple criterion similarity number matching non-empty symbols. accordingly ...m}. ..e) e-state array. variable nonnegative real number represents level residual excitation associated i-th location ltm. example single e-state array. complex models several e-state arrays e... diﬀerent dynamic properties used. general element real array represents level activation i-th location oltm. model random winner-take-all choice component array corresponding winner iwin equal zero. formally example need variable iwin. r-array introduced sake completeness. appear following equations. array needed complex models primitive e-machines employ complex encoding procedures. treated real-time cognitive models e-machines thought psychological time step order msec. complex models e-machines multi-step cycles several time variables ν... diﬀerent time steps. remark. expression tape-records input output vectors iltm oltm respectively recording enabled simplicity model assume weights input symbols aﬀect recording. always remark. truth function deﬁned expression expression adds residual excitation location iltm wptr location data recorded. happens recording enabled level added residual excitation equal would produced input vector already recorded location iltm trick allows avoid introducing intermediate steps cycle. helpful intuitive link e-machines neural networks. link provides source neurobiological heuristic considerations design models e-machines source psychological heuristic considerations design corresponding class neural models. figure shows general architecture homogeneous neural network corresponding pem. architecture discussed large circles incoming outgoing lines represent centers elements assigned certain coordinates network. center incoming outgoing lines interpreted neuron dendrites axons respectively network functionally equivalent large neuron. small circles represent couplings elements whose position network described pair centers communicating coupling. coupling interpreted synapse circuit functionally equivalent large synapse. white black small circles represent excitatory inhibitory synapses respectively. follows terms neuron synapse instead terms center coupling respectively i-th neuron j-th set. skja synapse neurons additional index describing type synapse substituted index indicates whole subset elements corresponding entire values index. ...s). etc. diagram depicts three types synapses e-states serving diﬀerent purposes. similar model describes eﬀect lateral modulation allows network decode temporal sequences; describes spreading residual excitation producing eﬀect broad temporal context. note. neural network corresponds complex model input output implemented respectively input output synaptic matrices network also intermediate synaptic memory called structural memory corresponds modiﬁable connections among neuron decoders neuron encoders everyone-and-excite-itself principle. choice could also implemented using inhibitory synapses gains synapses except so-called inhibit-everyone-but-itself principle. layer inhibitory feedback gain less unity provide contrasting. layers inhibitory inputs diagram also depicts outputs carrying information global levels activation corresponding layers. invent diﬀerent functional models primitive e-machines corresponding network figure however attempt discuss interesting topic detail would take goal paper. implement large iltm oltm needs intermediate neurons increase fan-out input neurons fan-in output neurons ideas inspired organization cerebellum discussed concept e-machine supports notion e-states associated properties individual neurons synapses. interesting possibility formally connect dynamics phenomenological e-states statistical conformational dynamics ensembles membrane proteins treated markov systems neural network researchers strongly oppose notion symbols brain. argue anti-symbolic bias counterproductive. interesting possibility represent symbols iltm oltm sparse quasi-orthogonal synaptic vectors challenging neurobiological mechanisms could provide implementation learning algorithms functionally close complete memory algorithm used model mechanisms exist taking account principle limitations traditional learning algorithms demonstrated section writing enabled as.wen beginning training write pointer initial position as.wptr empty as.gx as.gy residual excitation zero as.e assume training ﬁxed rule gram recorded least ltm. since number rules need examination starts lasts long needed. examination ns.sel as.xy as.y. simplicity assume writing disabled note. writing could enabled examination. would make diﬀerence theorem concerned. emax rule placed working memory. reason statement that send input output retrieved locations highest level residual excitation among locations locations expression accordingly expression locations independently level residual excitation. eﬀect executing recent rule placed working memory last decays certain level eloss. model eloss modulating coeﬃcient expression must less /emax expression time decay emax eloss location containing ﬁxed rules expression tmax expression guarantee emax decay eloss entire experiment training examination suﬃcient tmax ν−ν. condition satisﬁed suppose course examination test read mode. send inputs addr retrieved location addr highest level among locations addr. guarantied eloss retrieved location certain data recorded recently therefore din. exactly gram would react input. remark. model data loss level eloss reason residual excitation created wrong location accordingly residual excitation higher relied upon. result decay level emax=. latter level input addr input corresponds writing non-empty data address addr gram. already shown system figure organized learn simulate gram locations data symbols steps. make brain turing-universal learning system need system capable learning simulate ﬁnite-state part turing machine stage training nm.sel ns.sel input vector coming system am.x dout output vector coming teacher am.xy nm.y t.y. training starts ends beginning training empty write pointer initial position am.wptr writing enabled training am.wen ...ν stage examination writing disabled am.wen nm.sel output nm.y produced system nm.y am.y. teacher controls select signal ns.sel switch external system imaginary external system role played system since stage training teacher controls motor signals nm.y productions formed system easy verify productions correctly interpreted expressions expression reduced since consequently expressions eﬀect. completes proof. transform system figure working model needs take care synchronization units nsnmas technical problems signiﬁcance purpose paper. model actually implemented interactive program windows called erobot. model interesting psychological interpretation. suggests turing universality human thinking result mental interaction motor system forms sensory motor motor associations sensory system forms associations. symbols left parts associations suggest motor sensory feedbacks. using terminology borrowed system called central executive. speech feedback produces eﬀect internal states simulated turing machine. robot needs talk able perform principle arbitrarily complex computations. demonstrated importance e-states sensory system introduction e-states motor system leads interesting eﬀect context-dependent mental set. going show used acquire knowledge needed simulate m-input boolean functions steps training. eﬀect achieved context-dependent dynamic reconﬁguration limited knowledge time decision making. shown case auditory channel allow robot several squares time. discussed experiment robot need type keyboard. expresses reactions speech channel. removed system centers units needed discussed experiment. described figure table displays contents steps training. upper part table shows contents iltm lower part shows contents oltm association input symbols output symbol. understand structure association consider association upper symbol represents auditory name assigned association. name taken auditory names ...a}. symbols represent visual input read screen. symbols taken visual alphabets experiment robot visual symbols simultaneously inputs moment shown robot sees symbols output symbol represents motor speech symbol corresponding reaction robot simulates boolean function input read screen output represented motor speech symbol. motor speech symbols taken alphabet diagram depicts delayed proprioceptive speech feedback nm.y. corresponding proprioceptive symbol stored role proprioceptive speech feedback explained later. follows give verbal explanation work model. explanation transformed rigorous proof done section before experiment divided stages training examination. assume model reset button allows experimenter go/stop button start stop execution. also assume speech feedback turned off. start case feedback disregard beginning training empty write pointer initial position wptr training writing enabled teacher creates associations corresponding productions participating k-input boolean functions. number productions minimum length training shown case production given unique auditory name ..an}. stage examination writing disabled using auditory input experimenter pre-tunes associations corresponding given boolean function sending auditory names associations input case maximum level residual excitation created auditory channel bigger maximum level excitation created channels. since time decay residual excitation made arbitrarily number productions working memory residual excitation selected subset associations stays loss level eloss robot execute pre-tuned boolean function. discussed model eloss explain idea speech feedback. suppose input cycle divided steps. ﬁrst step robot executes association feedback. second step proprioceptive input arrives increases level residual excitation executed association. feedback interrupted next cycle starts. refresh mechanism produce eﬀect self-supporting mental set. many interesting problems possibilities associated development exploration understanding symbolic-dynamical computational paradigm represented e-machines include decoding temporal sequences. adding lateral pre-tuning next e-state procedure addresses problem. corresponding learn simulate principle output independent ﬁnite memory machine. introducing delayed feedback leads system capable learning simulate output dependent ﬁnite memory machine. waiting associations. e-state creating eﬀect waiting association. produce eﬀect ﬁnite stack without using buﬀer simulate context-free grammars limited memory. allows broad temporal context. eﬀect produced adding diﬀerent types spreading e-states. changing time constants radii spread states leads eﬀects mental signiﬁcantly complex discussed section active scanning associative memory. people answer questions happened certain event. eﬀect achieved ﬁrst creating e-state proﬁle activates information mentioned event actively shifting proﬁle time-wise counter-time-wise direction. done introducing additional control inputs pem. inhibiting data given features. think e-states excitations activations. imagine e-states producing inhibiting pre-inhibiting pre-activating post-activating etc. eﬀects. fact functions sets data stored assign dynamic labels subsets data thought diﬀerent kinds e-states. example imagine situation ﬁrst creates e-state activating data temporarily inhibits selected data sending inhibiting control input. brain many diﬀerent neurotransmitters receptors justify diﬀerent hypotheses possible types control inputs. viewpoint paper important thing diﬀerent hypotheses type eﬃciently expressed terms e-machine formalism. connecting dynamics e-states statistical conformational dynamics ensembles membrane proteins. interesting possibility connecting e-states statistical conformational dynamics ensembles membrane proteins treated probabilistic molecular machines markov system transitional probabilities depending macroscopic inputs membrane potentials concentrations neurotransmitters approach ensemble pmms becomes statistical mixed signal computer e-states interpreted occupation numbers diﬀerent microscopic states pmms. formalism natural system theoretical extension classical hodgkin huxley theory works well generation spikes validating approach representing dynamics short-term intermediateterm memories would justify postulating quite complex next e-state procedures. remark. many diﬀerent hypotheses e-states diﬀerent types could implemented cellular level translating neurobiological hypotheses next e-state procedures gives mathematical tool exploring psychological implications hypotheses. eﬀects drugs higher mental functions. shown section changing dynamical e-states changes symbolic personality e-machine. accordingly e-machine formalism gives tool expressing broad range ideas diﬀerent drugs could aﬀect higher mental functions aﬀecting e-states. higher level lower levels brain must able convert sensory signals sensory symbols motor symbols motor signals. possibilities signal-to-symbol conversion done diﬀerent feature detectors. detector would ﬁxed sparse serving unique pointer detector. sets would form primary sensory alphabets. similarly units generating primary motor features would sparse forming primary motor alphabets. approach higher association areas neocortex could deal mostly symbols. sparse recoding hierarchical structure associative memory. mentioned before complex e-machine hierarchical associative learning system built several pems. concept sparse extended allow pems higher levels store data terms sparse-recoded references data stored pems lower levels. would naturally produce diﬀerent eﬀects data compression chunking communication among pems association ﬁbers. sparse encoding symbols addresses question pems diﬀerent modalities levels could eﬃciently communicate association ﬁbers. numbers association ﬁbers enough provide crossbar connectivity. case sparse encoding symbols several symbolic messages could sent simultaneously level crosstalk. would allow small subset talkers broadcast sparse-encoded messages simultaneously large numbers listeners. limitation many talkers must talk time association ﬁbers. example estimate shows around neuron-talkers talk simultaneously neuron-listeners association ﬁbers. any-n-to-any-n communication physically impossible however any-m-of-n-to-anyn communication quite possible computing statistics depending context. symbolic level brain pre-compute statistics time learning statistics depends context. could neocortex compute statistics depending context? sparse encoding symbols oﬀers solution problem. change procedures choice encoding allow several sparse-encoded symbols read simultaneously locations oltm high enough level activation summing several sparse vectors produces statistical ﬁltering eﬀect problem natural language. challenging interesting problem that believe well suited integrated symbolic-dynamical approach discussed paper. learn problem.) eﬀect contextdependent mental illustrated section reveals iceberg. takes less information dynamically activate data structures already present create data structures. even less information needed data structures already pre-tuned inputs modalities. accordingly unlike statements formal language sentences natural language need carry complete information. hint suﬃcient remove ambiguity given context. sheds light people similar backgrounds mental sets eﬃciently communicate short messages whereas people diﬀerent backgrounds mental sets diﬃculties understanding other. emotions motivation. mention problem emotions motivation. e-machine paradigm shed light problem? postulate that higher level exists symbolic representation emotions otherwise language would names emotional states. postulate correct e-machine formalism applied higher level learning involving emotions. people remember pleasant unpleasant emotional states. means that higher level eﬀect positive/negative reinforcement cannot reduced eﬀect increasing/decreasing weights sensorimotor associations. postulate that higher level brain forms associations involving symbols representing emotions observable internal states. sets associations dynamically reconﬁgured depending context changing e-states. helps understand concepts good depend knowledge mental set. easy imagine situation retrieving emotional symbols aﬀects control inputs change e-states that turn aﬀect retrieval emotional symbols would shed light nature various self-reinforcing loops well known panic attack loop. cognitive system human-like robot basic cognitive characteristics similar person. formal representation robot’s brain time corresponds beginning learning sake concreteness years represent highly trained brain. speciﬁc number important. could perhaps even years. argue must relatively short formal representation whereas must long representation methodologically advantageous look ﬁrst representation understand changes course learning. remark. reasons representation cannot long representation must encoded form human genome. whole genome takes enough room argue size must include size genetic code also size procedure translates genetic code reasonable postulate size latter procedure signiﬁcantly smaller size cellular machinery creates biological systems vastly diﬀerent complexity diﬀerent genetic codes. theoretically consistent mathematical cognitive theory must able derive arbitrarily complex cognitive phenomena model interacting external world situation loosely compared traditional physical theory. speciﬁc metaphor consider problem simulating behavior electromagnetic ﬁeld linear accelerator mathematical model underlying latter simulation represented pair maxwell equation speciﬁc constraints describing design lac. case quite obvious would impossible simulate speciﬁc behavior system without adequate representation basic constraints argue similarly impossible simulate speciﬁc cognitive phenomena system without adequate representation basic constraints remark. unusual whole physical phenomenon simpler mathematical representation parts. example whole behavior electromagnetic ﬁeld eﬃcient formal representation however case nontrivial external constraints practically impossible separate formal representations either electric magnetic projections behavior. concept e-machine suggests holds symbolic-dynamical behavior human brain. whole behavior simpler representation symbolic and/or dynamical projections. said sections raises problem adequate mathematical formalism representing diﬀerent levels physical phenomenon information processing human brain. diﬀerence numerically correct mathematical representation adequate mathematical representation. adequate representation must allow brain eﬃciently think phenomenon question. understand problem consider extreme examples. extreme imagine model windows operating system represented system billions nonlinear diﬀerential equations complex initial conditions. note that dynamical model software would represented initial conditions. encoding symbols real vectors sampling vectors right moments model would produce correct simulation results. safe nobody would able understand dynamical model works. another extreme imagine model behavior electromagnetic ﬁeld represented turing machine simulating discussed behavior with accuracy. given enough time enough memory space symbolic model would produce numerically correct simulation results. before nobody would able understand symbolic model works. adequate mathematical formalism representing thinking physical phenomenon information processing human brain must take fact account. help know physically implementable computing system including brain represented principle either symbolic dynamical terms. need ﬁgure system need represent rather represent given system. finding adequate formal representation main challenge. much greater challenge develop adequate language would allow understand behavior system larger larger values even case simple maxwell equations behavior pair section extremely complex. complexity behavior comes complexity speciﬁc external constraints must true case adequate mathematical theory system trying many speciﬁc constraints inevitably reduces predictive power cognitive theory. argue many basic principles organization functioning already known. missing adequate formalization extrapolation integration basic principles single mathematical theory whole human brain integrated computing system. critical issues development system level theory system integration falsiﬁcation. following considerations explain issues must separated other. ..cm basic properties human brain integrated computing system possible systems property treats ...cm constraints single integrated model search area intersection sets ...cm. properties considers smaller becomes search area. systems outside area eliminated search falsiﬁcation principle. contrast ignores issues system integration falsiﬁcation treats property independently search area union ...cm. case properties considers bigger becomes search area. consequently becomes increasingly diﬃcult eliminate impossible truth. users similar brains unlimited source reliable system-level constraints whole human brain integrated computing system. similarly reliable constraints parts brain and/or parts brain’s performance. therefore methodologically important separate problem parts brain parts brain’s behavior problem whole brain. section shown losing information time learning leads principal limitations time decision making. theoretically system learn system complete memory algorithm e-machine formalism demonstrates powerful enough interpretation procedure make dumb universal learning algorithm. contrast interpretation procedure make smart learning algorithm loses information. popular notion smart learning algorithm contradicts requirement universality human learning creates methodological pitfall ﬁxed learning algorithm smart enough know advance information turn important future. terms architecture figure traditional learning systems characterized models diﬃcult deﬁne explicitly must accordingly diﬃcult rigorously demonstrate limitations traditional learning systems brain models. situation becomes transparent treats learning systems models system rather properties many interesting external systems formally deﬁned. section used approach show many traditional learning systems cannot learn simulate external systems properties read/write memory whereas human brain can. learning problem call ram-buﬀerproblem presents harder falsiﬁcation test neural theories learning famous xor-problem remark. emphasized postulating existence conventional buﬀer would explain brain learns simulate external systems properties read/write memory. would remain absolutely unclear could learn conventional buﬀer simulate diﬀerent memory aids. buﬀer behavior must learned. psychological terms means brain exposed external system properties read/write memory would learn working memory consequently would develop needed perform nontrivial mental computations. disclaimer. system level constraints discussed paper applicable higher level human learning. introduce limitations learning algorithms used lower levels item section postulated lower levels perform signal-to-symbol symbol-to-signal transformations. attempted discuss important question type transformations performed levels. evolution found large number eﬃcient signal processing solutions. parts solutions genetically determined aﬀected learning open question. theories learning equate problem learning problem deciphering structure target machine observed black another machine usually target machine treated grammar identiﬁed sentences deﬁnition learning learner cannot learn simulate behavior teacher type higher type even type behaviors learnable. general result seems contradict theorem section showing behavior gram learned gram system type fact contradiction. system figure treat gram black box. deﬁned built-in information external system accordingly black limitations apply. argue holds phenomenon human learning. human learner treat human teacher external systems black boxes. expects systems certain properties. shown section grey approach learning principle limitations learned. experimental question whether approach applicable problem human learning. remark. said suggests falsiﬁcation principle applied formulations biologically-inspired mathematical problems solutions problems. well known adequately deﬁne physical problem mathematical terms easier solve problem. includes biological problems well. catch biological problems already deﬁned nature. accordingly great challenge biologically-consistent mathematical theory decipher adequately formalize natural deﬁnitions replace artiﬁcial deﬁnitions.", "year": 2009}