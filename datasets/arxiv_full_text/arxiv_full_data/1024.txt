{"title": "Distribution-Specific Hardness of Learning Neural Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Although neural networks are routinely and successfully trained in practice using simple gradient-based methods, most existing theoretical results are negative, showing that learning such networks is difficult, in a worst-case sense over all data distributions. In this paper, we take a more nuanced view, and consider whether specific assumptions on the \"niceness\" of the input distribution, or \"niceness\" of the target function (e.g. in terms of smoothness, non-degeneracy, incoherence, random choice of parameters etc.), are sufficient to guarantee learnability using gradient-based methods. We provide evidence that neither class of assumptions alone is sufficient: On the one hand, for any member of a class of \"nice\" target functions, there are difficult input distributions. On the other hand, we identify a family of simple target functions, which are difficult to learn even if the input distribution is \"nice\". To prove our results, we develop some tools which may be of independent interest, such as extending Fourier-based hardness techniques developed in the context of statistical queries \\cite{blum1994weakly}, from the Boolean cube to Euclidean space and to more general classes of functions.", "text": "although neural networks routinely successfully trained practice using simple gradientbased methods existing theoretical results negative showing learning networks difﬁcult worst-case sense data distributions. paper take nuanced view consider whether speciﬁc assumptions niceness input distribution niceness target function sufﬁcient guarantee learnability using gradient-based methods. provide evidence neither class assumptions alone sufﬁcient hand member class nice target functions difﬁcult input distributions. hand identify family simple target functions difﬁcult learn even input distribution nice. prove results develop tools independent interest extending fourier-based hardness techniques developed context statistical queries boolean cube euclidean space general classes functions. artiﬁcial neural networks seen dramatic resurgence recent years proven highly effective machine learning method computer vision natural language processing challenging problems. moreover successfully training networks routinely performed using simple scalable gradient-based methods particular stochastic gradient descent. despite practical success theoretical understanding computational tractability methods quite limited results negative. example discussed learning even depth- networks formal learning framework computationally hard worst case even algorithm allowed return arbitrary predictors. common worst-case results proven using rather artiﬁcial constructions quite different real-world problems neural networks highly successful. particular since framework focuses distribution-free learning hardness results rely carefully crafted distributions allows relate learning problem np-hard problem breaking cryptographic system. however insist natural distributions? possible show neural networks learning becomes computationally tractable? show learned using standard heuristics employed practice stochastic gradient descent? target function learning assumed output equals unkown target function hypothesis class considering. studying neural networks common consider class networks share ﬁxed architecture however argue parameters real-world networks arbitrary exhibit various features non-degeneracy random like appearance. indeed networks random structure shown amenable analysis various situations references therein). empirical evidence clearly suggest many pairs input distributions target functions computationally tractable using standard methods. however characterize pairs? would appropriate assumptions sufﬁcient show learnability? paper investigate components provide evidence neither alone enough guarantee computationally tractable learning least methods resembling used practice. speciﬁcally focus simple shallow relu networks assume data perfectly predicted network even allow over-speciﬁcation sense allow learning algorithm output predictor possibly larger complex target function even favorable conditions show following hardness natural target functions. individual target function coming simple class small shallow relu networks show algorithm invariant linear transformations successfully learn w.r.t. input distributions polynomial time result based reduction learning intersections halfspaces. although problem known hard worstcase input distributions target functions essentially show invariant algorithms distinguish worst-case average-case learn particular target function algorithm algorithm learn nearly target functions class. hardness natural input distributions. show target functions form periodic generally difﬁcult learn using gradient-based methods even input distribution ﬁxed belongs broad class smooth input distributions note functions constructed simple shallow networks seen extension generalized linear models unlike previous result relies computational hardness assumption results geometric nature imply gradient objective function nearly everywhere contains virtually signal underlying target function. therefore algorithm relies gradient information cannot learn functions. interestingly difﬁculty plethora spurious local minima saddle points associated stochastic optimization problem actually critical points. instead objective function exhibit properties ﬂatness nearly everywhere unless already close global optimum. highlights potential pitfall non-convex learning occurs already slight extension generalized linear models even nice input distributions. together results indicate order explain practical success neural network learning gradient-based methods would need employ careful combination assumptions input prove results develop tools independent interest. particular techniques used prove hardness learning functions form based fourier analysis close connections hardness results learning parities well-known framework learning statistical queries cases essentially shows fourier transform target function small support hence correlate functions making difﬁcult learn using certain methods. however consider general arguably natural class input distributions euclidean space rather distributions boolean cube. sense show learning general periodic functions euclidean space difﬁcult reasons learning parities boolean cube difﬁcult statistical queries framework. elegant work janzamin shown certain method based tensor decompositions allows provably learn simple neural networks combination assumptions input distribution target function. however drawback method requires rather precise knowledge input distribution derivatives rarely available practice. contrast focus algorithms utilize knowledge. works show computationallyefﬁcient learnability certain neural networks sufﬁciently strong distributional assumptions include context learning functions boolean cube known even restrict ourself particular input distribution difﬁcult learn parity functions using statistical query algorithms moreover recently shown stochastic gradient descent methods approximately posed algorithms since parities implemented small real-valued networks implies most input distributions boolean cube neural networks unlikely learnable gradient-based methods. however data provided neural networks practice form boolean vectors rather vectors ﬂoating-point numbers. moreover assumptions input distribution smoothness gaussianity make sense consider support euclidean space rather boolean cube. perhaps enough guarantee computational tractability? contribution paper show case formally demonstrate phenomena similar boolean case also occurs euclidean space using appropriate target functions distributions. finally note provides improper-learning hardness results hold even standard gaussian distribution euclidean space algorithm. however unlike paper focus hardness agnostic learning results speciﬁc standard gaussian distribution proofs based reduction boolean case. paper structured follows sec. formally present notation concepts used throughout paper. sec. provide hardness results natural input distributions sec. provide hardness results natural target functions. proofs presented sec. generally bold-faced letters denote vectors. given complex-valued number denote complex conjugate denote modulus. given function denote gradient denote hessian neural networks. focus results learning predictors described simple shallow neural networks. standard feedforward neural network composed neurons computes mapping parameters scalar activation function example popular relu function max{ neurons arranged parallel layers output layer compactly represented σx+b) matrix vector applies activation function coordinates vanilla feedforward networks layers connected other given input output equals parameter i-th layer. number layers denoted depth network maximal number columns denoted width network. simplicity paper focus networks output real-valued number measure performance respect squared loss vector loss predictor example y)). gradient-based methods. gradient-based methods class optimization algorithms solving problems form minw∈w based computing approximations various points perhaps simplest algorithm gradient descent initializes deterministically randomly point iteratively performs updates form ηt∇f step size parameter. context statistical supervised learning problems usually interested solving problems form minw∈w ex∼d class predictors target function loss function. since distribution generally unknown cannot compute gradient function w.r.t. directly still compute approximations e.g. sampling random computing gradient approach used solve empirical approximations above i.e. minw∈w dataset {)}m generally known stochastic gradient methods popular scalable machine learning methods practice. learning. results sec. rely following standard deﬁnition learning respect boolean functions given hypothesis class functions learning algorithm pac-learns distribution algorithm given oracle access i.i.d. samples sampled according time poly algorithm returns function prx∼d high probability note deﬁnition above allow belong hypothesis class often denoted improper learning allows learning algorithm power proper learning must member i=wi x]+] max{ relu function min{ max{ clipping operation interval corresponds depth- networks bias ﬁrst layer outputs ﬁrst layer simply summed moved clipping non-linearity letting write predictors appropriate ﬁxed function goal would show target function virtually choice polynomial-time learning algorithm satisfying conditions exists input distribution must fail. careful reader noticed impossible provide target-function-speciﬁc result holds algorithm. indeed target function advance always learn returning target function regardless training data. thus imposing constraints algorithm necessary. speciﬁcally consider algorithms exhibit certain natural invariances coordinate system used. natural invariance respect orthogonal transformations example rotate input instances ﬁxed manner orthogonally-invariant algorithm return predictor still makes predictions instances. formally invariance deﬁned follows outputs deﬁnition algorithm inputs dataset predictor orthogonally-invariant orthogonal matrix rd×d feed algorithm yi}m algorithm returns predictor remark deﬁnition stated refers deterministic algorithms. stochastic algorithms understand orthogonal invariance mean orthogonal invariance conditioned realization algorithm’s random coin ﬂips. example standard gradient stochastic gradient descent methods easily shown orthogonally-invariant. however results need make somewhat stronger invariance assumption namely invariance general invertible linear transformations data formally deﬁned follows deﬁnition algorithm linearly-invariant satisﬁes deﬁnition invertible matrix rd×d well-known example algorithm newton method relevant purposes linear invariance occurs whenever orthogonallyinvariant algorithm preconditions whitens data covariance ﬁxed structure example even though gradient descent methods linearly invariant become precede preconditioning step. formalized following theorem theorem algorithm given yi}m computes whitening matrix thin decomposition feeds yi}m orthogonally-invariant algorithm given output predictor returns predictor )x). linearly-invariant. easily veriﬁed covariance matrix transformed instances identity matrix indeed whitening transform. note whitening common preprocessing heuristic even done explicitly scalable approximate whitening preconditioning methods batch normalization common widely recognized useful training neural networks. show result rely reduction pac-learning problem known computationally hard namely learning intersections halfspaces. boolean predictors parameterized compute mapping form klivans sherstov show certain well-studied cryptographic assumption algorithm pac-learn intersection halfspaces even coordinates integers maxi poly. daniely shalev-shwartz show assumption related hardness refuting random k-sat formulas algorithm pac-learn intersections halfspaces even coordinates integers maxi essentially gradient function w.r.t. proportional thus multiply orthogonal gradient also gets multiplied since inner products instances gradients remain same. therefore induction shown algorithm operates incrementally updating iterate linear combinations gradients rotationally invariant. theorem below result applies intersection smaller number halfspaces smaller norms. however similar results shown using cost worse polynomial dependencies maxi linearly independent smallest singular value smin) strictly positive. assumption stated linearly-invariant algorithm distribution vectors norm runs time poly returns high probability predictor note result holds even returned predictor different structure larger size thus applies even algorithm allowed train larger network complicated predictor proof sketched follows first hardness assumption learning intersection halfspaces shown imply hardness learning networks described however implies algorithm learn input distributions contrast want show learning would difﬁcult even ﬁxed show algorithm linearly invariant ability learn respect distributions means learn respect roughly speaking argue linearly-invariant algorithms average-case worst-case hardness here. intuitively given arbitrary create different input distribution look like linear transformation therefore linearly-invariant algorithm succeeds also succeed other. formally suppose linearly-invariant algorithm successfully learn respect input distribution. matrix distribution respect wish learn shown invertible matrix since algorithm successfully learns respect input distribution would also successfully learn input distribution deﬁned sampling returning means algorithm would succesfully learn data distributed much second issue apply linearly-invariant algorithm dataset transformed invariance respect data necessarily respect instances sampled distribution however shown dataset large enough invariance still occur high probability sampling sufﬁcient purposes. section consider difﬁculty gradient-based methods learn certain target functions even respect smooth well-behaved distributions speciﬁcally consider functions form vector bounded norm periodic function. note continuous piecewise linear implemented depth- neural relu network bounded subset domain. generally continuous periodic function approximated arbitrarily well networks. formal results rely fourier analysis technical. hence precede informal description outlining main ideas techniques presenting speciﬁc case study independent interest formal results presented subsection consider target function form input distribution whose density function written square function suppose attempt learn target function using hypothesis class parameterized bounded-norm vector subset euclidean space predictor class written ﬁxed mapping thus goal essentially solve stochastic optimization problem ﬁxed value objective function almost independent sense pick direction uniformly random value extremely concentrated around ﬁxed value independent gaussian mixture gaussians). therefore assuming reasonably large standard gradient-based method follow trajectory nearly independent fact practice even access exact gradients noisy biased versions case noise completely obliterate exponentially small signal gradients make trajectory essentially independent result assuming distribution function sensitive direction follows methods fail optimize successfully. finally note practice common solve directly rather empirical approximation respect ﬁxed ﬁnite training set. still concentration measure empirical objective would converge given enough data issues occur. important feature results make virtually structural assumptions predictors particular represent arbitrary classes neural networks thus results imply target functions form periodic would difﬁcult learn using gradient-based methods even allow improper learning consider predictor classes different structure. furthermore suppose input distribution standard gaussian dimensions objective function turns form illustrated figure objective function three critical points global maximum global minima nevertheless would difﬁcult optimize using gradient-based methods since extremely everywhere except close critical points. shortly phenomenon occurs higher dimensions. high dimensions direction chosen randomly overwhelmingly likely initialize global minima hence start plateau gradient-based methods stall. turn explain form shown figure also help illustrate proof techniques apply much generally. main idea analyze fourier transform letting cosw denote function cosw write standard norm space square integrable functions. standard properties fourier transform squared norm function equals squared norm function’s fourier transform equals turn expression inner parenthesis viewed mixture gaussian-like functions centers −w). thus mixtures nearly disjoint support nearly value regardless words ﬂat. since equation nothing re-formulation original objective function similar behavior well. behavior extends however much generally speciﬁc objective first replace standard gaussian distribution distribution localized support. would still imply refers difference functions nearly disjoint support ﬂatness phenomenon occur. second replace function periodic function properties fourier transform periodic functions still localized functions fourier domain finally instead considering hypothesis classes predictors similar target function consider quite arbitrary mappings even though function longer localized fourier domain enough target function localized implies regardless looks like random choice minuscule portion mass overlaps target function hence getting sufﬁcient signal difﬁcult. mentioned introduction techniques observations close resemblance hardness results learning parities boolean cube statistical queries learning model well considers fourier transform essentially show functions localized fourier transform difﬁcult detect using ﬁxed function. however results different general sense apply generic smooth distributions euclidean space general class periodic functions rather parities. side results constrained methods based gradients objective whereas statistical queries framework general considers algorithms based computing expectations arbitrary functions data. extending results generality interesting topic future research. turn provide formal statement results. distributions consider consist arbitrary mixtures densities whose square roots rapidly decaying tails fourier domain. precisely following deﬁnition deﬁnition function function fourierconcentrated square root belongs satisﬁes canonical example gaussian distributions given gaussian density function covariance matrix square root proportional gaussian covariance fourier transform well-known proportional gaussian covariance standard gaussian concentration results follows fourier-concentrated exp) λmin minimal eigenvalue similar bound shown gaussian arbitrary mean. generally well-known smooth functions fourier transforms rapidly decaying tails. example consider broad class schwartz functions fourier transform function also schwartz function implies super-polynomial decay chapter proposition formally state main result section. consider predictor form ﬁxed function parameter vector coming domain assume w.l.o.g. subset euclidean space various points however following theorem shows regardless type predictor network attempting train gradient virtually independent underlying target function hence provides little signal note bounded variation weaker than lipschitz continuity. assuming decays rapidly exponentially case gaussian mixture bound theorem order exp). overall theorem implies moderately large gradient point extremely concentrated around ﬁxed value independent implies gradient-based methods attempt optimize gradient information unlikely succeed. formalize consider iterative algorithm relies ε-approximate gradient oracle optimize every iteration algorithm chooses point receives vector |∇fw case interested order bound thm. since bound extremely small moderate realistic model gradient-based methods ﬁnite-precision machines even attempts compute gradients accurately. following theorem implies number iterations extremely large iterations gaussian mixtures) high probability gradient-based method return predictor independent however since objective function highly sensitive choice means gradient-based method train reasonable predictor. proof thm. denote whitening matrix employed transform instances invertible matrix whitening matrix employed original data. using notation theorem easily veriﬁed rank matrices rows consisting orthonormal vectors related orthogonal transformation therefore since data orthogonally-invariant algorithm output satisﬁes turn implies multiplying sides right taking transpose hence words orthogonal transformation depending therefore proof. suppose contradiction exists algorithm distribution described theorem returns function ex∼d bility. particular focus distributions supported distributions argue intersection halfspaces speciﬁed integer coordinates integer speciﬁed function described theorem statement. this note support integer hence particular consider boolean function rnd) argue −hx)) since arbitrary −hx) speciﬁes intersection halfspaces would contradict hardness result therefore prove theorem. argument follows following chain inequalities denotes indicator function proposition thm. holds even restrict linearly independent smin proof. suppose contradiction exists algorithm succeeds stated above. describe algorithm succeeds described thm. hence reaching contradiction. speciﬁcally suppose access samples supported matrix described thm. following every }d+nd transformed samples predictor }d+nd return predictor reduction works note mapping deﬁned distributed according induces distribution }d+nd. matrix minimal eigenvalue least hence smin satisﬁes conditions proposition. moreover norm column larger norm corresponding column norm constraint thm. still holds. finally therefore thus distribution valid distribution corresponding conditions proposition thm. returns high probability predictor deﬁnitions orthogonal invariance linear invariance required invariance hold respect instances dataset. stronger condition invariance satisﬁed however following lemma shows invariance w.r.t. dataset sampled i.i.d. distribution sufﬁcient imply invariance w.r.t. nearly lemma suppose dataset yi}m distribution following holds probability least invertible linearly-invariant algorithm conditioned algorithm’s internal randomness returned matrices satisfy show algorithm given poly samples successfully learn w.r.t. matrix distribution satisfying proposition thm. contradicting results. invertible matrix this note size conditions imply full column rank. thus simply augment invertible matrices columns orthonormal basis subspace orthogonal column space nd). spectral norm ˆw]− bounded /smin ˆw]) smin ˆw]) equals square root smallest eigenvalue easily veriﬁed min{ smin)}. consider following thought experiment suppose would algorithm samples sampled supported vectors norm supported vectors norm dnd)/ min{ smin)} outputs correspond network speciﬁed therefore assumption algorithm would return w.h.p. matrix used fact maps union bound recall refers output algorithm given samples {xi))}m poly. thus shown w.h.p. long algorithm samples algorithm returns satisﬁes thus enough prove bound theorem consists single element whose square root fourier-concentrated. individually using simplify notation stand function ψw·) stand function also several times fact functions strictly speaking function fourier transform sense since associated integrals converge. however function still well-deﬁned fourier transform general sense generalized function distribution survey). derivation below simply rely standard formulas fourier analysis literature refer formal justiﬁcations. coefﬁcient lemma universal positive constant. proof. symmetry given function expectation equivalently written ew∈weu euew∈w rotation matrix chosen uniformly random ew∈w refers uniform distribution ﬁnite vectors norm particular choose wexp} satisﬁes following words corresponds union open balls radius around ±w±w±w important property sets disjoint distinct note them would imply non-zero hence triangle inequality. squaring sides performing simple manipulations |z||z| would assume w.l.o.g. algorithm deterministic randomized simply prove statement possible realization random coin ﬂips. consider oracle given point returns |∇fw−ew]| otherwise. thus enough show probability least oracle return responses form clearly independent since algorithm’s output depend oracle responses prove theorem. algorithm’s ﬁrst point ﬁxed receiving information oracle therefore independent thm. varw) chebyshev’s inequality implies probability choice assuming event occur oracle returns depend actual choice means next point chosen algorithm ﬁxed independent thm. chebyshev’s inequality repeating argument applying union bound follows long number iterations satisﬁes oracle reveals information whatsoever choice point chosen algorithm independent required.", "year": 2016}