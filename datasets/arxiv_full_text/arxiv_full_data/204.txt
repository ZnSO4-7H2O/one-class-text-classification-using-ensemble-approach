{"title": "Theory and Tools for the Conversion of Analog to Spiking Convolutional  Neural Networks", "tag": ["stat.ML", "cs.CV", "cs.LG", "cs.NE"], "abstract": "Deep convolutional neural networks (CNNs) have shown great potential for numerous real-world machine learning applications, but performing inference in large CNNs in real-time remains a challenge. We have previously demonstrated that traditional CNNs can be converted into deep spiking neural networks (SNNs), which exhibit similar accuracy while reducing both latency and computational load as a consequence of their data-driven, event-based style of computing. Here we provide a novel theory that explains why this conversion is successful, and derive from it several new tools to convert a larger and more powerful class of deep networks into SNNs. We identify the main sources of approximation errors in previous conversion methods, and propose simple mechanisms to fix these issues. Furthermore, we develop spiking implementations of common CNN operations such as max-pooling, softmax, and batch-normalization, which allow almost loss-less conversion of arbitrary CNN architectures into the spiking domain. Empirical evaluation of different network architectures on the MNIST and CIFAR10 benchmarks leads to the best SNN results reported to date.", "text": "deep convolutional neural networks shown great potential numerous real-world machine learning applications performing inference large cnns real-time remains challenge. previously demonstrated traditional cnns converted deep spiking neural networks exhibit similar accuracy reducing latency computational load consequence data-driven event-based style computing. provide novel theory explains conversion successful derive several tools convert larger powerful class deep networks snns. identify main sources approximation errors previous conversion methods propose simple mechanisms issues. furthermore develop spiking implementations common operations max-pooling softmax batch-normalization allow almost loss-less conversion arbitrary architectures spiking domain. empirical evaluation different network architectures mnist cifar benchmarks leads best results reported date. computation spiking neural networks event-based data-driven thus neurons update whenever relevant information needs processed. contrasts framebased computations performed traditional analog neural networks process entirety input followed computing neuronal activations layer-by-layer producing ﬁnal output. recent results shown event-based mode operation snns particularly attractive reducing latency computational load deep neural networks represent state machine learning benchmarks deep snns queried results already ﬁrst output spike produced unlike anns result available layers completely processed snns also naturally suited process input event-based sensors even classical frame-based machine vision applications object recognition detection snns shown accurate fast efﬁcient particular neuromorphic hardware platforms snns could thus play important role supporting cases replacing deep anns tasks fast efﬁcient classiﬁcation real-time crucial detection objects larger moving scenes tracking tasks activity recognition training deep snns directly spiking activity notoriously difﬁcult recently methods backpropagation-like training snns developed however number studies shown snns successfully constructed converting conventionally trained anns relating activations units ﬁring rates spiking neurons. purpose study identify number challenges conversion networks novel theory proposing mechanisms signiﬁcantly improve performance deep snns. previous work ann-to-snn conversion starts units translated biologically inspired spiking units leaks refractory periods aiming processing inputs event-based sensors. suggested close link transfer function spiking neuron i.e. relation input current output ﬁring frequency activation rectiﬁed linear unit nowadays standard model units anns. achieved good performance conventional computer vision benchmarks converting class cnns restricted zero bias average-pooling layers. method improved achieved nearly loss-less conversion anns mnist task weight normalization. technique rescales weights avoid approximation errors snns either excessive little ﬁring. introduced conversion method noise injection training improves robustness approximation errors biologically realistic neuron models. demonstrated approach optimized cnns neuromorphic truenorth platform low-precision weights restricted connectivity. recently developed adapting snns encode information minimum number spikes thereby achieving good accuracy order magnitude less spikes approaches. approaches showed snns achieve near state-of-the-art accuracy time improving classiﬁcation speed aiming towards constructing deep snns low-power neuromorphic platforms. work investigate detail conversion algorithm proposed convert small models snns minimal accuracy loss however found running algorithms without modiﬁcations larger networks leads unacceptable drop accuracy order more. tests identiﬁed main reason drop performance reduction ﬁring rates higher layers arise overly pessimistic weight normalization. furthermore algorithms proposed severe restrictions kind cnns could converted. prevented using features commonly used successful cnns visual classiﬁcation max-pooling softmax batch normalization even basic features neuron models biases. following ﬁrst analytically investigate problem ann-to-snn conversion section introducing tools tricks expand class networks converted improve accuracy contribution proposed methods evaluated cifar benchmark section section investigate analytically ﬁring rates snns approximate relu activations anns. suggested ﬁrst basis ann-to-snn conversion theoretical basis principle lacking. basic approximation equations derive simple modiﬁcation reset mechanism spikes turns neuron unbiased approximator target function. assume one-to-one correspondence units neurons even though also possible represent unit population spiking neurons. network layers denote weight matrix connecting units layer layer biases number units layer relu activations neuron layer computed according every input pattern presented time steps time step highest ﬁring rate supported simulator given inverse time resolution rmax input rates proportional pixel intensity value. compute ﬁring rate neuron number spikes generated. principle ann-to-snn conversion introduced ﬁring rates irmax. relationship formalized introducing membrane equation spiking neuron estimating mean ﬁring rates membrane equation spiking neuron integrates inputs membrane potential exceeds threshold spike generated. time membrane potential reset compare types reset reset zero used always sets membrane potential back baseline typically zero whereas reset subtraction subtracts threshold membrane potential time threshold exceeded membrane equations derive slightly different approximation properties reset mechanisms. simplicity ﬁrst assume input currents remain analyze ﬁrst hidden layer. reset zero case constant time implies always constant number time steps spikes neuron threshold always exceeded constant amount therefore rmax/n easily derive exact ﬁring rate neuron non-time stepped simulation threshold would crossed earlier namely rmax/t∗ would correct estimate given ﬁring rate reset zero mechanism inevitably leads approximation error appears second term right error depends mainly away simply longer simulation time. shallow networks easy tasks mnist error seems minor problem found accumulation approximation errors deeper layers degrades accuracy. also larger smaller inputs improve approximation expense longer integration times. also increase smaller inputs. explanation weight normalization scheme proposed according network activations never exceed unity improves performance reset-to-zero case. another obvious possibility improve approximation reduce simulation time step comes cost increased computational effort. simple switch reset subtraction mechanism improves approximation makes conversion scheme suitable also deeper networks. case time spikes constant membrane potential time point given rmax apmeans ﬁring rate estimate converges target value proximation error discrete sampling. mechanism therefore leads accurate approximations underlying methods proposed show later section results improved accuracy larger networks. potential problem exists case case best neuron maximal ﬁring rate rmax never fully reach target frequency. makes case using weight normalization order prevent saturation ﬁring. firing rates higher layers previous results based assumption neuron receives constant input step simulation. using spiking neurons hidden neurons condition holds ﬁrst hidden layer input form analog currents instead irregular spike trains. reset-by-subtraction case analytically derive approximation error propagates deeper layers network. insert expression input simulation time solve ﬁring rate result surprising since ﬁring rate neuron layer given weighted ﬁring rates previous layer minus time-decaying approximation error also found ﬁrst layer. shows approximation errors earlier layers propagated network multiplied weights next higher layer. recursive expression solved iteratively inserting rates previous layer rates starting known rates ﬁrst layer thus neuron layer receives input spike train slightly lower spike rate reduced according sampling error previous layer neurons. errors accumulate higher layers explains takes longer achieve high correlations activations ﬁring rates deteriorate higher layers. following introduce methods heuristics improve classiﬁcation accuracy deep snns either allowing conversion wider ranger anns reducing approximation errors snn. biases standard anns explicitly excluded previous conversion methods snns. spiking network bias simply implemented constant input current cell’s membrane potential. theory section fully applies case neurons biases following section shows parameter normalization applied biases well. source approximation errors time-stepped simulation neurons restricted ﬁring rate range whereas anns constraints. introduced weight normalization means avoid approximation errors high ﬁring thereby signiﬁcantly improving performance converted snns. extend data-based weight normalization mechanism introduced case neurons biases suggest heuristic makes normalization process robust outliers. data-based normalization scheme based linearity relu unit used anns. simply extended biases linearly rescaling weights activation smaller training examples. order preserve information encoded within layer parameters layer need scaled jointly. denoting maximum relu activation layer weights biases normalized although weight normalization avoids saturating ﬁring rates might result ﬁring rates thereby increasing latency information reaches higher layers. algorithm sketched section refer \"max-norm\" normalization factor weights biases scaled maximum activation among samples training set. conservative approach ensures ﬁring rates never exceed maximum ﬁring rate. drawback procedure prone inﬂuenced singular outlier samples lead high activations majority remaining samples ﬁring rates remain considerably saturation. outliers uncommon shown figure plots distribution non-zero activations ﬁrst convolution layer cifar samples maximum observed activation three times higher percentile. figure shows distribution highest activations across samples units layer revealing large variance across dataset peak away absolute maximum. explains normalizing maximum potentially perform poorly vast majority samples even maximum activation units within layer chosen normalization scale thus insufﬁcient ﬁring drive higher layers obtain accurate results. robust alternative propose instead choosing maximum activation p-th percentile total activity distribution. discards extreme outliers increases ﬁring rates larger fraction samples. potential drawback small percentage neurons saturate choosing normalization scale involves trade-off saturation insufﬁcient ﬁring. following refer percentile \"normalization scale\" note \"max-norm\" method recovered special case typical values perform well range general saturation small fraction neurons seems degrade network performance less spike rates. method combined using batch-normalization training standardizes activations layer therefore produces fewer extreme outliers. figure distribution non-zero activations ﬁrst convolution layer cifar samples log-scale. dashed line plots indicates percentile relu activations across dataset corresponding normalization scale three times less overall maximum λmax distribution maximum relu activations cifar samples. samples maximum activation λmax. batch-normalization reduces internal covariate shift anns thereby speeds training process. introduces additional layers afﬁne transformations inputs performed order achieve zero-mean unit variance. input transformed obtained training described training transformations integrated weight vectors thereby preserving effect eliminating computations. speciﬁcally makes simple convert layers snns transforming weights preceding layer additional conversion layers necessary. empirically found loss-less conversion parameters integrated weights like this advantage lies purely obtaining better anns using training. truly event-based benchmark datasets rare conventional frame-based image databases mnist cifar used evaluate accuracy conversion. previous work usually converted analog input activations e.g. gray levels values poisson ﬁring rates. introduces variability ﬁring network impairs performance without notable beneﬁts. simple alternative analog input values ﬁrst hidden layer compute spikes empirically found particularly effective low-activation regime units usually undersampling spiking neurons poses challenge successful conversion. softmax commonly used output deep results normalized strictly positive class likelihoods. previous approaches ann-to-snn conversion could convert softmax layers simply predicted class corresponding neuron spiked presentation stimulus. however approach fails neurons ﬁnal layer receive negative inputs thus never spike. convert softmax layers using mechanism proposed output spikes triggered external poisson generator variable ﬁring rate. spiking neurons simply accumulate inputs. external generator determines spike produced softmax competition according accumulated membrane potentials performed. successful anns max-pooling spatially down-sample feature maps used snns computing maxima spiking neurons non-trivial. instead simple average pooling used results using weaker anns conversion. lateral inhibition suggested fulﬁll properly selects winner actual maximum ﬁring rate. another suggestion time-to-ﬁrst-spike encoding ﬁrst neuron considered maximally ﬁring one. propose simple mechanism spiking max-pooling output units contain gating functions spikes maximally ﬁring neuron pass discarding spikes neurons. gating function controlled computing estimates pre-synaptic ﬁring rates e.g. computing online exponentially weighted average. practice found several methods work well demonstrate results using exponentially weighted averages ﬁring rates control gating function. ways improve accuracy conversion training better conversion improving conversion eliminating approximation errors snn. following show inﬂuence approaches. methods introduced section allow conversion cnns biases softmax batchnormalization max-pooling layers improve accuracy ann. quantiﬁed cifar benchmark using convolution layers relu activations batch-normalization max-pooling layers convolutions followed fully connected layers softmax output. achieved accuracy. constraining biases zero reduced accuracy replacing max-pooling average-pooling decreased accuracy eliminating softmax using relus output drop methods therefore start conversion already much better anns previously possible. figure shows case cifar conversion best using default approach fails yielding accuracy barely chance level. adding weight normalization suggested raises accuracy still drop result changing reset-by-subtraction mechanism section leads another improvement switching analog inputs ﬁrst hidden layer instead poisson spike trains results accuracy finally using percentile activations robust weight normalization yields accuracy close performance best result cifar single snns. therefore conclude proposed mechanisms training ann-to-snn conversion contribute positively success method. conversion nearly loss-less results competitive classiﬁcation benchmarks using snns. results conﬁrmed also mnist -layer network max-pooling achieved accuracy thereby improving previous state-of-the-art results snns reported snns known exhibit so-called accuracy-latency-tradeoff means accuracy improves longer network simulated. figure show robust weight normalization factor tuned ideally exploit property. empirically best results obtained normalization factors corresponding percentiles activations. lead accurate classiﬁcations quickly also converge error rates similar underlying ann. allowing larger class cnns converted snns introducing number novel improved conversion techniques could signiﬁcantly improve accuracy networks cifar mnist. best result accuracy cifar compares favorably previous results achieved accuracy cifar albeit smaller network cropping images similarly small network cropped images achieve accuracy. conversion methods lose less conversion since approach weight normalization results shown figure suggest methods would problems larger networks. better accuracies date reported accuracy reported large network optimized truenorth chips making ternary weights multiple network-in-network layers. smaller network ﬁtting single chip reported achieve recent experiments similar low-precision training schemes snns converted binaryconnect model starting figure inﬂuence novel mechanisms ann-to-snn conversion accuracy cifar. best section converted snn. default mode poisson inputs reset-to-zero weight normalization. applying weight normalization next three bars apply novel techniques presented section shown accuracy time steps. accuracy-latency-tradeoff snns give approximate results even inputs incomplete improve accuracy time. tested cifar samples accuracy improves rapidly approaches level. robust weight normalization factor tuned achieve ideal tradeoff latency ﬁnal accuracy. accuracy achieved accuracy cifar best result reported date. know anns larger networks typically perform better smaller networks thus fully expect boost accuracy applying conversion techniques even larger networks. fact best anns date achieve less error cifar goal expand toolkit ann-to-snn conversion point large networks using typical mechanisms converted snns minimal loss accuracy. drop-off typically less encouraging. also shown typical accuracy-latency tradeoff still present although networks speciﬁcally trained converge fast. using techniques proposed yield accurate results even faster. deriving ﬁrst solid theory ann-to-snn conversion directly revealed mechanisms improve classiﬁcation accuracy resulting simple switch reset mechanisms. this together novel tools convert large class cnns covering standard features conventional cnns helped achieving state-of-the-art results almost loss-less ann-tosnn conversion. future research apply methods datasets require large networks imagenet. another promising line research investigate mechanisms reduce number spikes produced eliminating redundancies information static inputs sent. demonstrated accuracy anns snns almost completely closed.", "year": 2016}