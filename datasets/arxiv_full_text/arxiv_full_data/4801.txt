{"title": "Interpretable Two-level Boolean Rule Learning for Classification", "tag": ["cs.LG", "cs.AI"], "abstract": "This paper proposes algorithms for learning two-level Boolean rules in Conjunctive Normal Form (CNF, i.e. AND-of-ORs) or Disjunctive Normal Form (DNF, i.e. OR-of-ANDs) as a type of human-interpretable classification model, aiming for a favorable trade-off between the classification accuracy and the simplicity of the rule. Two formulations are proposed. The first is an integer program whose objective function is a combination of the total number of errors and the total number of features used in the rule. We generalize a previously proposed linear programming (LP) relaxation from one-level to two-level rules. The second formulation replaces the 0-1 classification error with the Hamming distance from the current two-level rule to the closest rule that correctly classifies a sample. Based on this second formulation, block coordinate descent and alternating minimization algorithms are developed. Experiments show that the two-level rules can yield noticeably better performance than one-level rules due to their dramatically larger modeling capacity, and the two algorithms based on the Hamming distance formulation are generally superior to the other two-level rule learning methods in our comparison. A proposed approach to binarize any fractional values in the optimal solutions of LP relaxations is also shown to be effective.", "text": "paper proposes algorithms learning two-level boolean rules conjunctive normal form disjunctive normal form type human-interpretable classiﬁcation model aiming favorable trade-oﬀ classiﬁcation accuracy simplicity rule. formulations proposed. ﬁrst integer program whose objective function combination total number errors total number features used rule. generalize previously proposed linear programming relaxation onelevel two-level rules. second formulation replaces classiﬁcation error hamming distance current two-level rule closest rule correctly classiﬁes sample. based second formulation block coordinate descent alternating minimization algorithms developed. experiments show two-level rules yield noticeably better performance one-level rules dramatically larger modeling capacity algorithms based hamming distance formulation generally superior two-level rule learning methods comparison. proposed approach binarize fractional values optimal solutions relaxations also shown eﬀective. boolean rules important classiﬁcation model machine learning data mining. typical boolean rule connects subset binary input features logical operators conjunction disjunction negation form prediction. example boolean rule prediction year coronary heart disease risk ∗research laboratory electronics massachusetts institute technology cambridge usa. †mobile solutions mathematical sciences department thomas watson research center yorktown heights usa. advantage boolean rules high human interpretability features included learned rule provide reasons behind prediction results; example above smoking reason prediction year risk. reasons easily understood users. human interpretability high importance wide range applications medicine business results prediction models generally presented human decision maker/agent makes ﬁnal decision. decision maker often needs understanding reasons prediction accepting result; thus high prediction accuracy without providing reasons suﬃcient model trusted. example medical diagnosis models predict high risk certain diseases patient; doctor needs know underlying factors compare his/her domain knowledge take correct action communicate patient. another application requiring interpretability fraud detection convincing reasons needed justify auditing. paper considers learning two-level boolean rules datasets joint criteria classiﬁcation accuracy human interpretability measured total number features used optimization-based formulations introduced. objective function ﬁrst formulation weighted combination total number classiﬁcation errors sparsity based extend previously proposed relaxation approach one-level two-level rules. second formulation replaces classiﬁcation error cost hamming distance current rule closest rule correctly classiﬁes sample; propose objective function combination accuracy sparsity balance controlled parameter accuracy related costs false negatives false positives formulated respectively. relaxation yields linear program eﬃciently solvable suﬃcient conditions relaxation exact discussed two-level rule learning two-level boolean rules signiﬁcantly larger modeling capacity one-level rules. fact include negations input features two-level rules represent boolean function input features hold one-level rules. algorithms proposed rule learning based one-level learning algorithm. ﬁrst algorithm uses covering approach obtains two-level rule. suppose want learn rule training ﬁrst clause entire training remove samples output ﬁrst clause; predictions samples determined regardless clauses. train second clause remaining samples repeat removetrain procedure rest clauses. since covering approach one-pass greedy-style algorithm space improvement. second algorithm rule sets applies boosting predictor weighted combination rules rather two-level rule thus hinders interpretability. another algorithm learning hamming clustering approach uses greedy methods iteratively cluster samples category features close hamming block coordinate descent alternating minimization approaches optimizing objective second formulation. tackle issue fractional optimal solutions relaxations introduce binarization method convert solutions binary values. experiments show compared one-level rules two-level rules noticeably lower error rate well ﬂexible accuracy-simplicity tradeoﬀs. algorithms based hamming distance formulation generally superior performance among approaches two-level rule learning compare binarization method shown eﬀective. remainder paper organized follows. section reviews related work ﬁelds. problem formulations section optimization approaches introduced section evaluated section section concludes work. two-level boolean rules work examples sparse decision rule lists major classes interpretable models decision trees constitute another class represent boolean functions converted decision rule lists although diﬀer representation complexity depending dataset section focus existing work learning one-level two-level boolean rules respectively. one-level rule learning method forms building block current work. one-level rule learning standard binary supervised classiﬁcation problem considered training dataset labeled samples; sample binary label total binary features goal learn classiﬁer generalize well unseen feature vectors sampled distribution training dataset. class classiﬁers considered consists one-level boolean rules take conjunction selected features. morgan’s laws show equivalence corresponding conjunctive disjunctive rules actual clauses assigned large. rule clause regarded disabled output always thus input feature matrix trivial always true feature samples also include corresponding decision variables clauses. sparsity cost lower variables even zero. clause output thus disabled rule. option might reduce accuracy tradeoﬀ improved sparsity. index features selected clause. learn rule learning algorithm ﬁrst negate features labels samples learn rule negated features labels ﬁnally decision variables original features construct rule. thus sections focus only. formulation error cost natural choice accuracy-related cost total number misclassiﬁcations sparsity cost number features used clause formulation challenges formulation. first one-level rule learning two-level rule learning problem combinatorial. second clauses symmetric however generally would like clauses distinct since duplication clauses ineﬃcient. formulation minimal hamming distance formulation motivations below. first potentially desirable ﬁner-grained accuracy cost error example consider rules clauses predicting sample ground truth label distance. seen bottom-up whereas algorithms top-down treat training dataset globally. experiments seem imply produces high number clauses hinders interpretability. number methods ﬁelds related two-level rule learning. first bayesian approaches typically utilize approximate inference algorithms obtain solution produce posterior distribution decision lists. however assignment prior likelihood bayesian framework always clear certain approximate inference algorithms high computational cost. second logical analysis data learns patterns positive negative samples techniques covering typically builds classiﬁer weighted combination patterns i.e. two-level rule. third learnability boolean formulae considered perspective probably approximately correct learning. diﬀerent problem setup related work typically assumes positive negative samples generated demand without noise. fourth two-level logic optimization circuit design considers simplifying two-level rules exactly match given truth table. however rule learning neither needed desirable exactly match noisy dataset. goal learn two-level boolean rule conjunctive normal form training dataset setup binary supervised classiﬁcation section lower level rule form clause disjunction selected subset input features; upper level ﬁnal predictor formed conjunction clauses. suppose total number clauses ﬁxed denoted binary decision variables represent whether include feature clause output clause sample simplify description algorithms later show formulation below equivalent involves wjr. taking minimization ﬁxed eliminates variables becomes identical suppose clauses ﬁrst rule predict clause second rule predicts predicts although rules misclassify sample taking clauses second rule closer correct ﬁrst one. iterative algorithm reﬁne learned rule might beneﬁcial accuracy cost term favor second rule example could push solution towards correct. second motivation formulation avoid identical clauses training clause diﬀerent subset samples done formulation accuracy cost single sample minimal hamming distance given rule ideal rule latter means rule correctly classiﬁes sample. hamming distance rules total number diﬀerent rules. intuitive explanation minimal hamming distance smallest number modiﬁcations current rule needed correct misclassiﬁcation sample i.e. rule correct. mathematical formulation introduce ideal clause outputs represent rule correctly classiﬁes sample. values always consistent ternary alphabet means don’t care value vir. setup least value implementation implies removal sample training updating clause generally leads diﬀerent training subset clause. denote minimal hamming distance current rule ideal rule sample. derive positive negative samples respectively. since implies clause output current rule least positive feature needs included match thus minimal hamming distance positive sample number clauses output unfortunately numerical results seem suggest relaxation likely fractional values optimal solution optimal possibly close undesirable since aims represent error cost term. possible reason convex concave interpolations loosen enable fractional results lower cost binary solutions. block coordinate descent algorithm algorithm considers decision variables single clause block coordinates performs block coordinate descent minimize hamming distance objective function iteration updates single clause clauses ﬁxed using one-level rule learning algorithm denote clause updated. optimization even clauses ﬁxed still involves joint minimization ideal clause outputs exact solution could still challenging. simplify values actual clause outputs ˆvir current assign exists ˆvir sample guaranteed correctly classiﬁed assign minimize objective contrast ˆvir holds constraint requires derivation leads updating process follows. update clause remove samples label already predicted least clauses update clause remaining samples using one-level rule learning algorithm mulation section generalize approach one-level rule learning two-level rules proper relaxation section based formulation section propose block coordinate descent algorithm section alternating minimization algorithm section objective since algorithms utilize relaxations section considers binarization problem result binary. two-level linear programming relaxation approach considers error formulation directly generalizes idea replacing binary operations linear-algebraic operations used one-level rule learning since deﬁned binary points various interpolations functions fractional points thus convex concave interpolations exist operators. function following interpolations since predictor two-level rule composition operators possible properly interpolate using convex function concave function composing individual interpolations operators. convex interpolation concave interpolation obtained similarly. denote error cost sample since errors thus need concave interpolation ˆyi; thus convex interpolation needed. finally formulation exactly converted mixed integer diﬀerent choices clause update iteration. example update clauses cyclically randomly update clause greedily choose minimum cost. greedy update used experiments. alternating minimization algorithm algorithm uses hamming distance formulation alternately minimizes respect decision variables ideal clause outputs vir. iteration steps update current update vir. latter step simpler ﬁrst discussed. ﬁxed values minimization relatively straight-forward objective becomes separated terms depends single clause ﬁxed thus clauses decoupled minimization problem becomes parallel learning onelevel clauses. explicitly update clause ﬁrst remove samples utilize one-level rule learning algorithm update ﬁxed follows discussion section positive samples negative samples deﬁned negative samples i.e. non-unique breaking achieved clustering approach similar spirit first clause compute cluster center feature space taking average samples minimal then sample assigned clause closest cluster center ℓ-norm among minimal redundancy aware binarization section discusses solution potential issue relaxation widely used algorithms proposed paper. although conditions under optimal solution relaxation one-level rule learning guaranteed binary aware similar guarantees two-level rule learning; addition conditions unlikely always hold real-world noisy dataset. thus optimal solution fractional values case need convert binary. already yields binary optimal solution binarization methods change straight-forward binarization method compare speciﬁed threshold done however empirical results seem suggest resulting binarized rule redundancy making rule unnecessarily complex possibly inﬂuencing accuracy. following improved binarization method considers three types redundancy sets binary features single disjunctive clause. among features redundancy sets feature appear single clause optimal rule. third type also applies option disable clause. type happen nested sets pairwise complementary especially binary features obtained thresholding continuous valued features. example suppose binary features thresholding continuous feature thresholds zigzag path forms redundancy since four features selected ﬁxed clause optimal rule otherwise either ﬁrst second redundancies happen thus rule optimal. typically multiple zigzag paths e.g. binarization approach takes types redundancies account. illustration suppose binary features obtained thresholding continuous valued features. clause ﬁxed continuous valued feature sweep nonredundant combinations binary features induced continuous feature obtain minimal cost. since total number non-redundant combinations nested zigzag features linear quadratic number thresholds respectively sweeping relatively eﬃcient single continuous feature. however joint minimization across continuous features seems combinatorial challenging. thus ﬁrst sort continuous features decreasing order corresponding decision variables optimal solution relaxation sequentially binarize decision variables induced continuous feature. setup section evaluates algorithms repository datasets including connectionist bench sonar bupa liver disorders pima indian diabetes parkinsons continuous valued features datasets converted binary using quantile thresholds. goal learn rule dataset. stratiﬁed -fold cross validation average test training error rates folds. solved cplex version sparsity parameter sweep total values. sweep total number clauses rule option disable clause used evaluation except section compare results with/without option. algorithms comparison abbreviations two-level relaxation block coordinate descent alternating minimization covering decision list spss decision trees redundancy-aware binarization. maximum number iterations minimal average test error rate minimal average test error rates achieved among values algorithms listed table denotes total number clauses. results cart cited refer reader results classiﬁers generally interpretable; accuracy algorithms generally quite competitive them. algorithm dataset number marked bold font lowest error rate among observations results. first bold-font numbers appear rows since corresponds one-level rules corresponds two-level rules two-level rules reduce error rate datasets especially signiﬁcant block coordinate descent alternating minimization algorithms. second block coordinate descent alternating minimization algorithms generally superior performance methods two-level rule learning comparison; however two-level relaxation seem good performance. thus focus block coordinate descent alternating minimization algorithms remainder section. third sonar dataset covering approach binarization noticeably lower error rates simple binarization shows eﬀectiveness redundancy-aware binarization. figure comparison pareto fronts diﬀerent numbers clauses pima test error rate pima training error rate liver test error rate liver training error rate. coordinate descent algorithm typically lower error rate; however total number features used increases alternating minimization algorithm start outperform. comparing covering approach simple binarization binarization generally obtains sparser rules improved similar accuracy. pareto fronts with/without option disable clause fig. shows comparison pareto fronts average test error rates without option disable clause always true feature. option generally improves sparsity error rate remain similar increase. paper provided optimization-based formulations two-level boolean rule learning ﬁrst based classiﬁcation error second hamming distance. three algorithms developed namely two-level relaxation block coordinate descent alternating minimization. redundancyaware binarization method introduced. preliminary comparison hamming clustering approach consider pima dataset shared work test error rate average features used rule reported block coordinate descent alternating minimization algorithms lower minimal error rates respectively. thus algorithms pima dataset produce rules higher accuracy signiﬁcantly fewer features used pareto fronts diﬀerent numbers clauses pareto fronts diﬀerent numbers clauses shown fig. vary fig. show average test training error rates alternating minimization algorithm pima dataset fig. show error rates block coordinate descent algorithm liver dataset. point ﬁgure corresponds pair average error rate average number features learned rule obtained values pareto fronts denoted lines ease visualization. following observations implied fig. first comparison pareto fronts suggests two-level rules ﬂexible tradeoﬀ accuracy simplicity. second shown fig. increase learned rule typically uses features lower training error rates. however exact tendency pareto front test error rate depend complexity datasets fig. pareto front becomes worse increase implying overﬁtting relatively simple dataset; contrast relatively complex liver dataset fig. minimum test error rate decrease features used seems suggest overﬁt yet. pareto fronts diﬀerent algorithms pareto fronts average test error rates diﬀerent algorithms sonar liver datasets shown fig. respectively. comparing block coordinate descent alternating minimization algorithms total number features used small block", "year": 2015}