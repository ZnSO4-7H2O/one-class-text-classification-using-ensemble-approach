{"title": "Block Neural Network Avoids Catastrophic Forgetting When Learning  Multiple Task", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "In the present work we propose a Deep Feed Forward network architecture which can be trained according to a sequential learning paradigm, where tasks of increasing difficulty are learned sequentially, yet avoiding catastrophic forgetting. The proposed architecture can re-use the features learned on previous tasks in a new task when the old tasks and the new one are related. The architecture needs fewer computational resources (neurons and connections) and less data for learning the new task than a network trained from scratch", "text": "present work propose deep feed forward network architecture trained according sequential learning paradigm tasks increasing difﬁculty learned sequentially avoiding catastrophic forgetting. proposed architecture re-use features learned previous tasks task tasks related. architecture needs fewer computational resources less data learning task network trained scratch recently suggested architectures block neural network progressive neural network tested respectively supervised learning paradigm reinforcement learning paradigm shown impressive results multi-task learning. block neural network created training several deep feed forward networks different tasks. networks connected using neurons connections forming bigger network trained task allowing added connections updated. block neural networks progressive neural networks shown beneﬁt advantages transfer learning. whereas past different forms pre-training multi-task learning also achieved this block neural networks progressive networks without suffering disadvantage catastrophic forgetting tasks case pre-training necessity persistent reservoir data multi-task learning. paper quickly revisiting block network architecture propose binary classiﬁcation tasks show block architecture learns simply quickly network trained scratch. deﬁned tasks trained task. ﬁrst training phase used trained networks build block architecture trained remaining tasks tm+. block architecture figure architecture built adding block neurons three hidden layers base model. adding block neurons hidden layers base model. adding block neurons hidden layer base model. adding block neurons base models. dashed boxes indicate layers base models block neurons added. arrow connecting boxes indicates neurons ﬁrst connected neurons second box. formed adding neurons previously trained networks block neurons connected base models follows ﬁrst hidden layer block neurons received input task input sent networks second hidden layer fully connected ﬁrst hidden layer block neurons ﬁrst hidden layer network pattern repeated layers. architecture tested variations. variations respectively ﬁrst second layer block neurons removed. training task none parameters base model networks allowed change. figure provides representation block neural network. used binary classiﬁcation tasks networks trained tasks involved concepts line angle. wished show networks trained tasks would develop features could reused block architecture solve another task involving concepts. task stimuli gray scale images pixels size. image contained four line segments least pixels long segments white dark random background black light random background. tasks ang_crs requires classifying images containing angle pair crossing line segments ang_crs_ln ang_crs additional line segment crossing neither line segments. ang_tri_ln distinguishes images containing angle triangle image also contains line segment crossing neither angle triangle. blt_srp requires classifying images blunt sharp angles them. blt_srp_ln blt_srp additional line segment crossing neither line segments forming angle. section ﬁrst report results obtained training previously described tasks. report results training different block neural networks tasks. number possible architectures built changing base models number block neurons task block network trained large exploring possibilities feasible. detailed analysis conﬁgurations tried found previous studies summarize results obtained kinds block network architectures particularly interesting obtained adding small number block neurons. moreover paper focus ability architectures learn using much smaller dataset. fact present performance obtained several block architectures architectures trained dataset almost half size dataset used training network scratch. performance networks evaluated computing percentage misclassiﬁed samples test dataset. architecture trained times randomly initializing weights. mean performance repetitions best worst performance reported tables. prior building block architectures trained task. networks used type nn--- nodes ﬁrst second third layers respectively. networks type used base models block networks. percentages misclassiﬁed test examples networks shown table together results another architecture namely nn---. networks approximately number parameters block networks making interesting performance comparisons possible. networks trained datasets examples. block architecture namely ba--- ba---. architecture ba--- obtained connecting base models units ﬁrst hidden layer units second hidden layer units third hidden layer. plots ﬁgure obtained follow. built several instantiations kinds block architectures using randomly selected base models. architecture trained tasks task used train base models. example block network built using base model trained blt_srp ang_crs trained tasks excepts two. performance obtained block network compared performance obtained network trained scratch task. percentage times block network obtained better score evaluated. plot ﬁgure clearly shows increase performance block network number base models grows. hand result expected simply bigger number base models parameters trained; hand important stress number parameters trained block network case much smaller network trained scratch. architecture ba--- base models example parameters compared network table table show performance block network network trained dataset examples almost half size dataset used train network nn---. percentages misclassiﬁed test examples block architectures four base models presented tables. architectures performed better network nn--- trained scratch shown bold. tables tasks block architectures trained listed together tasks base models trained block architecture proves effective solution approaching problem multi-task learning dnn. architecture ﬁrst step toward construction architectures which unsupervised fashion able proﬁt training prior tasks learning task.", "year": 2017}