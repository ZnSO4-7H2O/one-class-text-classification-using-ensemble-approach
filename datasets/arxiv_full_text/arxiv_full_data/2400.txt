{"title": "Multi-Relational Learning at Scale with ADMM", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Learning from multiple-relational data which contains noise, ambiguities, or duplicate entities is essential to a wide range of applications such as statistical inference based on Web Linked Data, recommender systems, computational biology, and natural language processing. These tasks usually require working with very large and complex datasets - e.g., the Web graph - however, current approaches to multi-relational learning are not practical for such scenarios due to their high computational complexity and poor scalability on large data.  In this paper, we propose a novel and scalable approach for multi-relational factorization based on consensus optimization. Our model, called ConsMRF, is based on the Alternating Direction Method of Multipliers (ADMM) framework, which enables us to optimize each target relation using a smaller set of parameters than the state-of-the-art competitors in this task.  Due to ADMM's nature, ConsMRF can be easily parallelized which makes it suitable for large multi-relational data. Experiments on large Web datasets - derived from DBpedia, Wikipedia and YAGO - show the efficiency and performance improvement of ConsMRF over strong competitors. In addition, ConsMRF near-linear scalability indicates great potential to tackle Web-scale problem sizes.", "text": "data. best solution optimization problem compromise performance relations. although models evaluated multi-target settings none explicitly investigates problem optimize target relation individually instead learning optimal performance compromise relations. decoupled target speciﬁc features multitarget factorization addresses drawback. learns models optimized single target relation regularized minimizing loss relations data. problem approach twofold number parameters grow fast number relations runtime complexity quadratic number relations. coupled auxiliary target speciﬁc features multi-target factorization alleviates ﬁrst problem sharing parameters among models second problem still persists. besides catsmf needs estimate relation weights process problematic e.g. setting weights model selection might infeasible even moderate number relations. hence catsmf still scalable enough handle large-scale multi-relational problems. paper propose consmrf novel approach multi-relational factorization based consensus optimization. consmrf deﬁnes target speciﬁc parameters regularizes global consensus variable competitors catsmf iterate auxiliary relations target data. thus consmrf lower runtime costs still featuring predictive quality achieved target speciﬁc models. addition that thanks learning algorithm based alternating direction method multipliers consmrf training parallelized shared memory distributed environment allowing scale large problems. propose consmrf novel approach large scale multi-relational factorization. consmrf based consensus optimization optimizes target relation speciﬁcally efﬁciently state-of-the-art competitors; introduction complex graph structure different relations edge types motivated large body research tackling challenge mining multi-relational data presence noise partial inconsistencies ambiguities duplicate entities. state-of-the-art advances ﬁeld relevant many applications link prediction resource description framework mining entity linking recommender systems natural language processing however paradigms still needed statistical computational inference large multi-relational datasets like ones produced massive scale projects google’s knowledge graph yago semantic initiatives dbpedia factorization models considered state-of-the-art approaches statistical relational learning exhibited high predictive performance factorization models multi-relational data associate entities relations latent feature vectors model predictions unknown relationships operations vectors optimizing predictions number relations seen prediction task multiple target variables. example multi-target models support information retrieval tasks linked open data bases like dbpedia providing estimates facts neither explicitly stated knowledge base inferred logical entailment enabling probabilistic queries databases another example context social recommender systems services interested recommending instance news items user also recommending users potential friends. state-of-the-art factorization models approach multitarget prediction task sharing parameters used target relations. instances approaches rescal multiple order factorization share entity speciﬁc parameters among relations e-mail e.diaz-avilesacm.org schmidt-thieme information systems machine learning university hildesheim hildesheim germany. e-mail schmidt-thiemeismll.de factorization models multi-relational learning factorization models deﬁne matrix r|e|×k rows respective k-dimensional feature vector entity addition models associate relation matrix rk×k. thus prediction corresponds multi-relational factorization approaches differ parametrize relation feature matrices early models like collective matrix factorization employ relation features i.e. viewed deﬁning identity matrix. lead poor prediction quality specially prediction different relations pair entities same. cope issue different approaches associate latent features relations. simplest approach includes relation features deﬁne diagonal matrix model equivalent parafac tensor decomposition semantic matching energy model also uses approach although slightly different prediction function using full matrix approach used rescal multiple order factorization localized factor model finally approaches exist deal higher arity relations e.g. coupled matrix tensor factorization metafac purposes work focus binary relations; however concepts described easily applied higher order relations. note none aforementioned state-of-the-art approaches make distinction target auxiliary relations parameters predicting targets i.e. learned parameters compromise performance targets speciﬁc one. catsmf efﬁciently consmrf address drawback. catsmf decoupled target speciﬁc features multi-target factorization coupled auxiliary target speciﬁc features multi-target factorization combine idea shared parameters learn individual entity embeddings different target relations. catsmf achieve state-of-the-art results statistical relational learning tasks comparison rescal however learning model number parameters grows fast number relations issue catsmf solves sharing parameters among models different targets. spite that catsmf still estimate relation weights problematic e.g. setting model selection might infeasible even moderate number relations. hence catsmf still scalable efﬁciently handle large-scale multi-relational problems. approach consmrf multi-relational factorization efﬁcient competitors catsmf since require relation weights catsmf parallelized straightforward manner. conduct extensive experiments real-world datasets derived dbpedia wikipedia yago demonstrate consmrf achieves state-of-the-art predictive performance that time scales large data. throughout paper uppercase bold face letters like denote matrices lowercase boldface vectors e.g. i-th matrix denoted scalars denoted non-boldface letters e.g. finally denote sets calligraphic letters like multi-relational learning relational data comprise relations among entities paper assume relations binary relationships subject object dataset given relation described multi-relational model associates relation prediction function characterized model parameters given training data task parameters test error previously unseen test data many multi-relational datasets however consist positive-only instances e.g. tuples type like linked open data true triples observed. case standard setting optimize pairwise ranking function tensor matrix completion control systems regression hierarchical interactions power systems computational advertising however best knowledge consmrf ﬁrst parallel algorithm learning multi-relational factorization models admm framework. admm based learning algorithm enables consmrf outperform state-of-the-art competitors catsmf also gracefully scale large datasets. parallel distributed algorithms factorization models developed single relation datasets e.g. recommender systems. state-of-the-art approaches like nomad dsgd ccd++ ds-admm work well problems like recommender systems relation available entity types i.e. user items. strong assumptions data schema make parallelization approaches generalizable multi-relational case. consmrf hand partitions data relationwise allowing parallel processing them. property makes consmrf attractive applications need mine data many relations. recent work turbo-smt proposes generate subsamples whole data learn model subsample combine ﬁnal step. learning model sample carried parallel. framework however leverage target-speciﬁc features improve prediction quality like consmrf does. approach consensus multirelational factorization order scale large amounts relational data without sacriﬁcing predictive accuracy approach needed exploits target speciﬁc parameters improved prediction accuracy efﬁciently parallelized distributed. address points propose approach based framework consensus optimization. approach takes different road catsmf model learned data target relation parameters regularized global consensus variable advantages approach three-fold; learning model speciﬁc target relation efﬁcient catsmf since needs iterate data relation; information relation ﬂows models means variable model easily distributed assigning relation different machine i.e. without requiring duplication training data. discussed before strong disadvantage approach parameters predicting relation learned exploiting information relations. order alleviate this introduce global entity feature matrix called consensus variable used make sure different parameters converge value. consequence hard constraint solution problem equivalent problem thus cannot exploit fully target speciﬁc parameters. solve softening constraints instead solving problem minimize lagrangian. consensus multi-relational factorization problem formulated optimizing multiple relations order illustrate approach introduce running example used across paper. consider social media website users follow users friends users consume products e.g. read news items. example entity types namely users news items three relations follows social relationship product consumption e.g. reading news items. task given existing past data recommend friends users follow items consumed. suitable approach associate different matrix target relation prediction functions optimized speciﬁc target. naive approach would factorize relation individually problem approach model learned relation exploit available information others. instance social circle user well taste valuable predictors might interested follow. alleviates optimizing parameters relation relations data thus solving following problem figure illustrates would applied social media website example. note model target relation learned whole data model trivially parallelized long worker access whole training data. disadvantage approach additional amount parameters needed. figure shows catsmf alleviates issue fig. catsmf consmrf social network example. picture illustrates data needed optimize parameters target relation. gray boxes indicate available training data relation white boxes model parameters. involves performing optimization task. also updates depend loss regularization functions. section show updates pairwise loss function look like. algorithm summarizes whole process figure illustrates running example. initializing parameters entity relation latent factors updated using stochastic gradient descent speciﬁed algorithm important state update relation independent update relation thus loop line easily parallelized. that since parallel worker processes portion data updates local variables algorithm implemented shared memory updating iteration updating requires solving optimization problem given equation avoid overﬁtting regularize parameters using l-regularization notice framework optimize relational model variety loss functions ideally approximate evaluation criterion. since relational learning problems evaluated using ranking measures reasonable optimize models pairwise ranking function. optimization criterion proposed bpr-opt smooth approximation area curve thus enabling optimization standard gradient-based approaches. also previous work provided empirical evidence effective optimization criterion task approached since feasible closed form solution aforementioned problem resort approximate solution means proven scale gracefully large datasets procedure depicted algorithm algorithm starts randomly sampling observed data point uniformly random recall deﬁnition additional object section objects associated subject relation i.e. dr}. sampling objects performed follows object sampled uniformly random gets accepted another sample drawn otherwise. sampled algorithm continues updating respective parameters opposite direction gradient. also parallelized however ad-hoc parallelization inefﬁcient dealing high number relations observed empirical evaluation. number convergence criteria suggested admm algorithms found empirically early stopping strategy checking performance training works well problem algorithm terminate better understand model consider social media example presented before. seen figure model learned assigning worker relation. ﬁrst step worker optimizes locally parallel local variables. means following problems solved parallel theory consensus optimization admm method consmrf deﬁnes global entity feature matrix regularizes parameters given relation using following term encodes information relations seen equation consmrf approach major advantages catsmf involve nested summations auxiliary relations; therefore scales better number relations data avoids potentially cumbersome setting hyperparameters finally resorting parallelization consmrf also efﬁcient. main reason because worker needs access data. might problem shared memory setting distributed environment whole data needs replicated node. hand consmrf requires worker access data relation assigned thus data duplication necessary. experimental evaluation section assess behavior consmrf practical applications terms predictive performance scalability. compare consmrf state-ofthe-art competitors catsmf rescal standard canonical decomposition ﬁrst describe datasets protocol experimental setting conclude section results discussion empirical study. datasets experiments used three datasets collected dbpedia wikipedia yago whose statistics summarized table datasets described follows. dbpedia central interlinking-hubs emerging data makes really attractive evaluate multi-relational learning approaches. dataset comprises triples sample dbpedia properties english. contains entities relations regarding music domain namely artist genre composer associated_band associated_musical_artist. wikipedia-svo highest number relations among benchmark datasets multi-relational tasks. contains subject-verb-object triples extracted million wikipedia articles verbs play role relationship. consists triples relations entities. yago huge semantic knowledge base derived wikipedia wordnet geonames. dataset made relation consmrf catsmf models traditional factorization models like rescal deﬁne entity feature matrix relation feature matrix relation parameters learned optimizing loss function like equation learned latent features ones provide best performance compromise relations data. already stated drawback model relation cannot learn anything information relations whole point multi-matrix factorization. catsmf solve problem using information relations introducing regularization term. regularization term relation written regularization weights auxiliary parameters learned regularization purposes never used making predictions test data. shown strategy leads better predictive performance evaluation protocol experimental settings dataset split training validation test set. first randomly select positive tuples assign test set. then randomly sample remaining positive tuples form validation set. rest tuples used training. expect good model score true facts higher false ones thus dealing ranking task leads following evaluation protocol based relation entity test experiments executed gnu/linux machine running centos version equipped intel xeon-phi .ghz processor ram. consmrf implemented using eigen library openmp parallelization. regularization weights learning rate consmrf catsmf initialized adjusted using adagrad policy catsmf relation weights searched range high number relations wikipedia-svo yago dataset searched aforementioned interval finally consmrf penalty parameter searched range results discussion evaluation used diagonal matrices consmrf model. compare catsmf approaches also using diagonal matrices relation features well complete sharing approach equivalent standard canonical decomposition optimized sgd. models relation loss used bpr-opt approximates measure. keep results perspective also added rescal evaluation well known state-of-the-art multi-relational factorization model. results evaluation seen figure consmrf achieves best scores datasets wikipedia-svo. important measure models optimized exception rescal optimized l-loss. words consmrf able achieve better scores measure optimized promising result given framework general enough allow different loss functions equation consmrf performs slightly worse catsmf precision recall added advantage parallel algorithm thus able scale larger datasets. finally unlike catsmf consmrf require careful tuning relation weights αtr. scalability. demonstrate consmrf ability scale large datasets report runtime performance here. table shows total training time using core catsmf consmrf. observe total runtime methods comparable dbpedia dataset consmrf much lower runtime yago wikipedia-svo. note speedups wikipedia-svo much higher speedups yago conﬁrming consmrf scales gracefully number relations unlike catsmf dmf. closer comparison runtime performance consmrf catsmf seen figure ﬁgure shows learning curves methods machine using core. note consmrf converges much faster catsmf datasets higher number relations namely wikipedia-svo yago dbpedia dataset relations convergence speed three methods comparable finally figure shows consmrf scale number cores. plot total training wall-clock time seconds number cores used. worth noting speedups achieved consmrf limited factors namely unbalanced workload synchronization costs. given distinct relations node optimizing |dtrain work learning since algorithm synchronous cores might need wait others ﬁnish work. possible avoid problem would look asynchronous admm approaches sensitivity hyperparameter. method consmrf introduces hyperparameter controls extent target speciﬁc parameters regularized global entity latent features means higher values tend diminish effect target speciﬁc parameters since forces similar values finally note acts step size parameters thus another side effect large values lead numerical problems cause algorithm diverge. explains drop performance seen figure figure sensitivity consmrf hyperparameter three datasets seen figure reproducibility experiments. datasets used experiments publicly available. reference implementation consmrf made available download upon paper acceptance. previous work shown multi-relational factorization models optimize speciﬁcally target relation achieve better predictive performance. work taken idea employing target speciﬁc parameters step means consensus optimization alternating direction method multipliers novel method consmrf takes advantage predictive power target speciﬁc parameters simple efﬁcient algorithm capable scaling large datasets. shown consmrf achieve state-of-theart performance much less time. addition best knowledge consmrf ﬁrst principled method able parallelize learning multi-relational factorization models. consmrf require careful optimization large number hyperparameters balance contribution different relations target prediction advantage previous approaches like catsmf. partitioning problem across relations work distribution different threads might unbalanced especially much data relations others. future work plan achieve even better runtime performance improvements exploiting different strategies data partitioning order obtain balanced workload thus even greater speedup parallelization. references nickel tresp h.-p. kriegel factorizing yago scalable machine learning linked data proceedings international conference world wide ser. drumond rendle schmidt-thieme predicting triples incomplete knowledge bases tensor factorization proceedings annual symposium applied computing ser. krohn-grimberghe drumond freudenthaler schmidt-thieme multi-relational matrix factorization using bayesian personalized ranking social network data proceedings ﬁfth international conference search data mining wsdm dong gabrilovich heitz horn murphy strohmann zhang knowledge vault webscale approach probabilistic knowledge fusion proceedings sigkdd international conference knowledge discovery data mining ser. drumond diaz-aviles schmidt-thieme nejdl optimizing multi-relational factorization models multiple target relations proceedings international conference information knowledge management ser. cikm nickel murphy tresp gabrilovich review relational machine learning knowledge graphs multi-relational link prediction automated knowledge graph construction arxiv preprint arxiv. nickel tresp h.-p. kriegel three-way model collective learning multi-relational data proceedings international conference machine learning agarwal b.-c. chen long localized factor models multi-context recommendation proceedings sigkdd international conference knowledge discovery data mining ser. h.-f. c.-j. hsieh vishwanathan dhillon nomad non-locking stochastic multi-machine algorithm asynchronous decentralized matrix completion proc. vldb endow. vol. gemulla nijkamp haas sismanis largescale matrix factorization distributed stochastic gradient descent proc. international conference knowledge discovery data mining ser. h.-f. c.-j. hsieh dhillon scalable coordinate descent approaches parallel matrix factorization recommender systems proceedings ieee international conference data mining ser. icdm z.-q. x.-j. w.-j. distributed stochastic admm matrix factorization proceedings international conference information knowledge management ser. cikm boyd parikh peleato eckstein distributed optimization statistical learning alternating direction method multipliers found. trends mach. learn. jan. musialski wonka tensor completion estimating missing values visual data ieee transactions pattern analysis machine intelligence rendle freudenthaler gantner schmidt-thieme bayesian personalized ranking implicit feedback proceedings twenty-fifth conference uncertainty artiﬁcial intelligence ser. cremonesi koren turrin performance recommender algorithms top-n recommendation tasks proceedings fourth conference recommender systems ser. recsys", "year": 2016}