{"title": "Fairness in Supervised Learning: An Information Theoretic Approach", "tag": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML"], "abstract": "Automated decision making systems are increasingly being used in real-world applications. In these systems for the most part, the decision rules are derived by minimizing the training error on the available historical data. Therefore, if there is a bias related to a sensitive attribute such as gender, race, religion, etc. in the data, say, due to cultural/historical discriminatory practices against a certain demographic, the system could continue discrimination in decisions by including the said bias in its decision rule. We present an information theoretic framework for designing fair predictors from data, which aim to prevent discrimination against a specified sensitive attribute in a supervised learning setting. We use equalized odds as the criterion for discrimination, which demands that the prediction should be independent of the protected attribute conditioned on the actual label. To ensure fairness and generalization simultaneously, we compress the data to an auxiliary variable, which is used for the prediction task. This auxiliary variable is chosen such that it is decontaminated from the discriminatory attribute in the sense of equalized odds. The final predictor is obtained by applying a Bayesian decision rule to the auxiliary variable.", "text": "abstract—automated decision making systems increasingly used real-world applications. systems part decision rules derived minimizing training error available historical data. therefore bias related sensitive attribute gender race religion etc. data cultural/historical discriminatory practices certain demographic system could continue discrimination decisions including said bias decision rule. present information theoretic framework designing fair predictors data prevent discrimination speciﬁed sensitive attribute supervised learning setting. equalized odds criterion discrimination demands prediction independent protected attribute conditioned actual label. ensure fairness generalization simultaneously compress data auxiliary variable used prediction task. auxiliary variable chosen decontaminated discriminatory attribute sense equalized odds. ﬁnal predictor obtained applying bayesian decision rule auxiliary variable. automated decision making systems based statistical inference learning increasingly common wide range real-world applications health care enforcement education ﬁnance. systems trained based historical data might biased towards certain attributes data points hence data without noticing possible biases could result discrimination deﬁned gratuitous distinction individuals different sensitive attribute. attributes include race religion referred protected attributes literature. example justice system courts features criminals race years jail etc. estimate possible recidivism–future arrest. considering features court assigns score in-jail individual decides whether release person. score exceeds certain limit safe release individual. instance noted angwin analysis risk scores criminal justice system–the compas risk tool–are biased negatively towards african-americans. showed risk score unjustiﬁably shows high risk recidivism african-american people compared actually another example authors studied accuracy gender representation online image searches. results indicate instance google image search c.e.o. percent depicted results women even though percent u.s. chef executives women; search telemarketer percent people depicted female occupation evenly split women. interesting connection problem fairness differential privacy differential privacy problem tries hide identity individuals fairness problem goal hide information protected attribute. details regarding connection presented different criteria assessing discrimination suggested literature. commonly used criterion so-called demographic parity requires predictor statistically independent protected attribute. denoting protected attribute prediction respectively demographic parity requires model satisfy demographic parity variants used several works scenarios criterion fails provide fairness demographics example case hiring employee majority applicants certain demographic force decision making system independent demographic system pick equal number applicants demographic. therefore system admit lower qualiﬁed individual smaller demographic guarantee percentages hired people different demographics matches. moreover denoting true label cases image search example correlated protected attribute therefore demographic parity forces independent criterion satisﬁed ideal predictor returning example hiring employee measure implies among qualiﬁed applicants probability hiring people different demographics same. people different demographics qualiﬁed qualiﬁed system hire equal probability. also note unlike demographic parity equalized odds allows ideal predictor paper present framework designing fair predictors data. utilize information theoretic approach model information content variables system relative another. equalized odds criterion assess discrimination. proposed scheme data variable ﬁrst mapped auxiliary variable decontaminate discriminatory attribute well ensuring generalization. design auxiliary variable input variable true label seek compact representation contains certain level information variable maximizes auxiliary variable turn used input prediction task. similar framework based joint statistics variables rather functional forms; hence formulation general. furthermore many cases functional form score underlying training data public. formulation instance) allows arbitrary cardinality implies multi-level protected attributes labels. cast task ﬁnding fair predictor optimization problem propose iterative solution solving problem. observe proposed solution necessarily converge levels fairness. suggests given requirement accuracy predictor certain levels fairness achievable. somewhat similar idea approach presented authors used intermediate representation space elements called prototypes. however besides fact work demographic parity used measure discrimination method used choosing prototypes quite different. speciﬁcally main approach avoid overﬁtting learning process limiting number prototypes achieve goal controlling information auxiliary variable data. approach extended deep variational auto-encoders priors encourage independence between sensitive latent factors variation. fig. graphical model proposed framework. denote protected attribute rest attributes true label respectively. compressed representor used designing prediction procedure. additionally propose optimization must solved address fairness issue. section propose iterative approach solving optimization problem introduced. concluding remarks presented section consider purely observational setting train predictor labeled data. sample attributes includes protected attributes gender race religion etc. protected attributes denoted denote rest attributes. denote true label prediction label instance example regarding risk recidivism explained section represents race individual represents features individual determines whether he/she committed crimes released jail. graphical model setup depicted figure seen ﬁgure correlated given independent true label property essential otherwise protected attribute fact direct cause label using attribute prediction process considered discriminatory. generalization since real-life application number available samples ﬁnite prevent overﬁtting constraint hypothesis space. compress variable auxiliary variable turn used prediction task. also choose contaminated discrimination sense equalized odds deﬁned following. variable. propose apply bayesian empirical risk minimization decision rule work prediction task. obtain mechanism generating auxiliary variable seek compact representation maximizes utility/quality prediction contains certain level information variable essence similar goal information bottleneck method maximizing corresponds maximizing utility keeping bounded could viewed regularization rejects complex hypotheses ensure generalization. note fact present fairness accuracy compactness mutual information provides setting need requirement cardinality variables stated earlier goal learning scheme produce compressed representor much information true label possible fair sense deﬁnition relax equalized odds requirement allow certain amount information variable conditioned reason choice become clear section iii. therefore objective mechanism pu|x maximizes well iii. solving fairness optimization problem section propose solution fairness optimization problem presented section lagrangian problem follows characterization solution problem given theorem below. slight abuse notation following clear context drop subscripts notation probability distributions. theorem values certain range arbitrary value stationary point lagrangian given equation conditional distribution pu|x obtained follows. note equation depends functions hence solution given theorem implicit. solution problem iterative algorithm described following. t-th iteration algorithm obtains values using previous stated before obtaining decontaminated variable variable used prediction task. utilize bayesian decision rule described following. alphabet variable alphabet variables quantify quality decision deﬁne loss function determines cost predicting true label decisions based auxiliary variable statistically related true label. denote decision designing fair predictors data information theoretic machinery. equalized odds used criterion discrimination demands prediction independent protected attribute conditioned actual label. proposed scheme data variable ﬁrst mapped auxiliary variable decontaminate discriminatory attribute well ensuring generalization. modeled task designing auxiliary variable optimization problem aims force variable fair sense equalized odds maximizes mutual information auxiliary variable true label whilst keeping information variable contains data limited. proposed iterative solution solving optimization problem. observed proposed solution necessarily converge levels fairness. suggests given requirement accuracy predictor certain levels fairness achievable. ﬁnal predictor obtained applying bayesian decision rule auxiliary variable. finding exact bound achievable level fairness well applying proposed method real data considered future work. general guarantee algorithm converges global minimum lagrangian. nevertheless theorem shows algorithm converges stationary ﬁxed point. note since achieving global optimum guaranteed initiate algorithm several different starting distributions. theorem certain range values parameter algorithm converges stationary ﬁxed point lagrangian given equation appendix proof. fact convergence occurs certain range values parameter suggests given requirement accuracy predictor certain levels fairness achievable. imply inherent bound level fairness algorithm achieve conclusion could obtained existing works. studied problem fairness supervised learning motivated fact automated decision making systems inherit biases related sensitive attributes gender race religion etc. historical data trained presented framework terms convex hence second derivative respect positive. second term convex setting sufﬁciently small second derivative expression brackets respect kept positive. therefore exists βmax βmax expression brackets convex hence step algorithm value decreases. moreover since ﬁrst three terms linear combinations kl-divergences hence nonnegative lower bounded constant. therefore values βmax algorithm converges stationary point. matuszek munson unequal representation gender stereotypes image search results occupations proceedings annual conference human factors computing systems. kalantari sankar sarwate optimal differential privacy mechanisms hamming distortion structured source classes information theory ieee international symposium ieee feldman friedler moeller scheidegger venkatasubramanian certifying removing disparate impact proceedings sigkdd international conference knowledge discovery data mining.", "year": 2018}