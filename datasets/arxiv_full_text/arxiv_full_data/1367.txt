{"title": "Pixel Recurrent Neural Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.", "text": "modeling distribution natural images landmark problem unsupervised learning. task requires image model expressive tractable scalable. present deep neural network sequentially predicts pixels image along spatial dimensions. method models discrete probability pixel values encodes complete dependencies image. architectural novelties include fast twodimensional recurrent layers effective residual connections deep recurrent networks. achieve log-likelihood scores natural images considerably better previous state art. main results also provide benchmarks diverse imagenet dataset. samples generated model appear crisp varied globally coherent. generative image modeling central problem unsupervised learning. probabilistic density models used wide variety tasks range image compression forms reconstruction image inpainting deblurring generation images. model conditioned external information possible applications also include creating images based text descriptions simulating future frames planning task. great advantages generative modeling practically endless amounts image data available learn from. however images high dimensional highly structured estimating distribution natural images extremely challenging. eling building complex expressive models also tractable scalable. trade-off resulted large variety generative models advantages. work focuses stochastic latent variable models vae’s extract meaningful representations often come intractable inference step hinder performance. effective approach tractably model joint distribution pixels image cast product conditional distributions; approach adopted autoregressive models nade fully visible neural networks factorization turns joint modeling problem sequence problem learns predict next pixel given previously generated pixels. model highly nonlinear longrange correlations pixels complex conditional distributions result highly expressive sequence model necessary. recurrent neural networks powerful models offer compact shared parametrization series conditional distributions. rnns shown excel hard sequence problems ranging handwriting generation character prediction machine translation two-dimensional produced promising results modeling grayscale images textures contributions paper follows. section design types pixelrnns corresponding types lstm layers; describe purely convolutional pixelcnn fastest architecture; design multi-scale version pixelrnn. section show relative beneﬁts using discrete softmax distribution models adopting residual connections lstm layers. next test models mnist cifar- show obtain loglikelihood scores considerably better previous results. also provide results large-scale imagenet dataset resized pixels; knowledge likelihood values generative models previously reported dataset. finally give qualitative evaluation samples generated pixelrnns. estimate distribution natural images used tractably compute likelihood images generate ones. network scans image time pixel time within row. pixel predicts conditional distribution possible pixel values given scanned context. figure illustrates process. joint distribution image pixels factorized product conditional distributions. parameters used predictions shared across pixel positions image. capture generation process theis bethge propose two-dimensional lstm network starts left pixel proceeds towards bottom right pixel. advantage lstm network effectively handles long-range dependencies central object scene understanding. two-dimensional structure ensures signals well propagated left-to-right topto-bottom directions. goal assign probability image formed pixels. write image onedimensional sequence pixels taken image row. estimate joint distribution write product conditional distributions pixels figure left generate pixel conditions previously generated pixels left center generate pixel multi-scale case also condition subsampled image pixels right diagram connectivity inside masked convolution. ﬁrst layer channels connected previous channels context connected itself. subsequent layers channels also connected themselves. large-scale modeling natural images. resulting pixelrnns composed twelve fast two-dimensional long short-term memory layers. layers lstm units state adopt convolution compute states along spatial dimensions data. design types layers. ﬁrst type lstm layer convolution applied along row; similar technique described second type diagonal bilstm layer convolution applied novel fashion along diagonals image. networks also incorporate residual connections around lstm layers; observe helps training pixelrnn twelve layers depth. also consider second simpliﬁed architecture shares core components pixelrnn. observe convolutional neural networks also used sequence model ﬁxed dependency range using masked convolutions. pixelcnn architecture fully convolutional network ﬁfteen layers preserves spatial resolution input throughout layers outputs conditional distribution location. pixelrnn pixelcnn capture full generality pixel inter-dependencies without introducing independence assumptions e.g. latent variable models. dependencies also maintained color values within individual pixel. furthermore contrast previous approaches model pixels continuous values gregor model pixels discrete values using multinomial distribution implemented simple softmax layer. observe approach gives representational training advantages models. previous approaches continuous distribution values pixels image uria contrast model discrete distribution every conditional distribution equation multinomial modeled softmax layer. channel variable simply takes distinct values. discrete distribution representationally simple advantage arbitrarily multimodal without prior shape experimentally also discrete distribution easy learn produce better performance compared continuous distribution section describe architectural components compose pixelrnn. sections describe types lstm layers convolutions compute states along spatial dimensions. section describe incorporate residual connections improve training pixelrnn many lstm layers. section describe softmax layer computes discrete joint distribution colors masking technique ensures proper conditioning scheme. section describe pixelcnn architecture. finally section describe multi-scale architecture. lstm unidirectional layer processes image bottom computing features whole once; computation performed one-dimensional convolution. pixel layer captures roughly triangular context pixel shown figure kernel onefigure diagonal bilstm allow parallelization along diagonals input skewed offseting position respect previous row. spatial layer computed left right column column output shifted back original size. convolution uses kernel size dimensional convolution size larger value broader context captured. weight sharing convolution ensures translation invariance computed features along row. computation proceeds follows. lstm layer input-to-state component recurrent state-to-state component together determine four gates inside lstm core. enhance parallelization lstm input-to-state component ﬁrst computed entire two-dimensional input map; convolution used follow row-wise orientation lstm itself. convolution masked include valid context produces tensor size representing four gate vectors position input number output feature maps. size input represents convolution operation elementwise multiplication. weights kernel weights state-to-state input-to-state components latter precomputed described above. case output forget input gates activation logistic sigmoid function whereas content gate tanh function. step computes state entire input map. lstm triangular receptive ﬁeld unable capture entire available context. train pixelrnns twelve layers depth. means increase convergence speed propagate signals directly network deploy residual connections lstm layer next. figure shows diagram residual blocks. input pixelrnn lstm layer features. input-to-state component reduces number features producing features gate. applying recurrent layer output upsampled back features position convolution input added output map. method related previous approaches gating along depth recurrent network advantage requiring additional gates. apart residual connections also learnable skip connections layer output. experiments evaluate relative effectiveness residual layer-to-output skip connections. features input position every layer network split three parts corresponding channels. predicting channel current pixel generated pixels left used context. predicting channel value channel also used context addition previously generated pixels. likewise channel values channels used. restrict connections network dependencies apply mask inputto-state convolutions purely convolutional layers pixelrnn. types masks indicate mask mask shown figure mask applied ﬁrst convolutional layer pixelrnn restricts connections neighboring pixels colors current pixels already predicted. hand mask applied subsequent input-to-state convolutional transitions relaxes restrictions mask also allowing connection color itself. masks easily implemented zeroing corresponding weights input-to-state convolutions update. simidiagonal bilstm designed parallelize computation capture entire available context image size. directions layer scans image diagonal fashion starting corner reaching opposite corner bottom. step computation computes lstm state along diagonal image. figure illustrates computation resulting receptive ﬁeld. diagonal computation proceeds follows. ﬁrst skew input space makes easy apply convolutions along diagonals. skewing operation offsets input position respect previous illustrated figure results size point compute input-to-state state-to-state components diagonal bilstm. directions input-to-state component simply convolution contributes four gates lstm core; operation generates tensor. state-to-state recurrent component computed column-wise convolution kernel size step takes previous hidden cell states combines contribution input-to-state component produces next hidden cell states deﬁned equation output feature skewed back removing offset positions. computation repeated directions. given output maps prevent layer seeing future pixels right output shifted added left output map. besides reaching full dependency ﬁeld diagonal bilstm additional advantage uses convolutional kernel size processes minimal amount information step yielding highly nonlinear computation. kernel sizes larger particularly useful broaden already global receptive ﬁeld diagonal bilstm. diagonal lstm layers potentially unbounded dependency range within receptive ﬁeld. comes computational cost state needs computed sequentially. simple workaround make receptive ﬁeld large unbounded. standard convolutional layers capture bounded receptive ﬁeld compute features pixel positions once. pixelcnn uses multiple convolutional layers preserve spatial resolution; pooling layers used. masks adopted convolutions avoid seeing future context; masks previously also used non-convolutional models made note advantage parallelization pixelcnn pixelrnn available training evaluating test images. image generation process sequential kinds networks sampled pixel needs given input back network. multi-scale pixelrnn composed unconditional pixelrnn conditional pixelrnns. unconditional network ﬁrst generates standard smaller image subsampled original image. conditional network takes image additional input generates larger image shown figure conditional network similar standard pixelrnn layers biased upsampled version small image. upsampling biasing processes deﬁned follows. upsampling process uses convolutional network deconvolutional layers construct enlarged feature size number features output upsampling network. then biasing process layer conditional pixelrnn simply maps conditioning added input-to-state corresponding layer; performed using unmasked convolution. larger image generated usual. section give speciﬁcations pixelrnns used experiments. four types networks pixelrnn based lstm based diagonal bilstm fully convolutional multiscale one. table speciﬁes layer single-scale networks. ﬁrst layer convolution uses mask type types lstm networks variable number recurrent layers. input-to-state convolution layer uses mask type whereas state-to-state convolution masked. pixelcnn uses convolutions size mask type feature passed couple layers consisting rectiﬁed linear unit convolution. cifar- imagenet experiments layers feature maps; mnist experiment layers feature maps. residual layer-to-output connections used across layers three networks. networks used experiments following hyperparameters. mnist diagonal bilstm layers value cifar- diagonal bilstms layers number units. pixelcnn layers imagenet adopt layer lstm units imagenet layer lstm units; latter model residual connections. section describe experiments results. begin describing evaluate compare results. section give details training. give results relative effectiveness architectural components best results mnist cifar- imagenet datasets. models trained evaluated loglikelihood loss function coming discrete distribution. although natural image data usually modeled continuous distributions using density functions compare results previous following way. literature currently best practice realvalued noise pixel values dequantize data using density functions uniform noise added log-likelihoods continuous discrete models directly comparable case values discrete distribution piecewiseuniform continuous function constant value every interval corresponding distribution log-likelihood original discrete distribution mnist report negative log-likelihood nats common practice literature. cifar- imagenet report negative log-likelihoods bits dimension. total discrete log-likelihood normalized dimensionality images numbers interpretable number bits compression scheme based model would need compress every color value practice also small overhead arithmetic coding. models trained gpus using torch toolbox. different parameter update rules tried rmsprop gives best convergence performance used experiments. learning rate schedules manually every dataset highest values allowed fast convergence. batch sizes also vary different datasets. smaller datasets mnist cifar- smaller batch sizes images seems regularize models. imagenet large batch size allowed memory; corresponds images/batch imagenet images/batch imagenet. apart scaling centering images input network don’t preprocessing augmentation. multinomial loss function pixel color values categories. pixelrnn models learn initial recurrent state network. apart intuitive easy implement using softmax discrete pixel values instead mixture density approach continuous pixel values gives better results. lstm model softmax output distribution obtain bits/dim cifar validation set. model mixture conditional gaussian scale mixtures obtain bits/dim. figure show softmax activations model. although don’t embed prior information meaning relations color categories e.g. pixel values neighbors distributions predicted model meaningful multimodal skewed peaked long tailed. also note values often much higher probability frequent. another advantage discrete distribution worry parts distribution mass lying outside interval something typically happens continuous distributions. another core component networks residual connections. table show results residual connections standard skip connections both -layer cifar- lstm model. using residual connections effective using skip connections; using also effective preserves advantage. figure samples models trained cifar- imagenet images. general models capture local spatial dependencies relatively well. imagenet model seems better capturing global structures cifar- model. imagenet model larger trained much data explains qualitative difference samples. although goal work model natural images large scale also tried model binary version mnist good sanity check previous dataset compare with. table report performance diagonal bilstm model previous published results. knowledge best reported result mnist far. next test models cifar- dataset table lists results models previously published approaches. results obtained without data augmentation. proposed networks diagonal bilstm best performance followed lstm pixelcnn. coincides size respective receptive ﬁelds diagonal bilstm global view lstm partially occluded view pixelcnn sees fewest pixels context. suggests effectively capturing large receptive ﬁeld important. figure shows cifar- samples generated figure samples models trained imagenet images. left normal model right multi-scale model. single-scale model trained images less able capture global structure model. multi-scale model seems resolve problem. although models similar performance log-likelihood samples right seem globally coherent. genet log-likelihood performance table imagenet current pixelrnns appear overﬁt validation performance improved size depth. main constraint model size currently computation time memory. figure image completions sampled model trained imagenet images. note diversity completions high attributed log-likelihood loss function used generative model encourages models high entropy. sampled model easily generate millions different completions. also interesting textures water wood shrubbery also inputed relative well likely resized different algorithm used imagenet images. imagenet images less blurry means neighboring pixels less correlated thus less predictable. downsampling method inﬂuence compression performance made used downsampled images available. figure shows samples drawn model trained imagenet. figure shows samples model without multi-scale paper signiﬁcantly improve build upon deep recurrent neural networks generative models natural images. described novel two-dimensional lstm layers lstm diagonal bilstm scale easily larger datasets. models trained model pixel values. treated pixel values discrete random variables using softmax layer conditional distributions. employed masked convolutions allow pixelrnns model full dependencies color channels. proposed evaluated architectural improvements models resulting pixelrnns lstm layers. shown pixelrnns signiﬁcantly improve state mnist cifar- datasets. also provide benchmarks generative image modeling imagenet dataset. based samples completions drawn models conclude pixelrnns able model spatially local long-range correlations able produce images sharp coherent. given models improve make larger practically unlimited data available train computation larger models likely improve results. authors would like thank shakir mohamed guillaume desjardins helpful input paper lucas theis alex graves karen simonyan lasse espeholt danilo rezende karol gregor danihelka insightful discussions. graves alex schmidhuber j¨urgen. ofﬂine handwriting recognition multidimensional recurrent neural networks. advances neural information processing systems gregor karol danihelka mnih andriy blundell charles wierstra daan. deep autoregressive networks. proceedings international conference machine learning gregor karol danihelka graves alex wierstra daan. draw recurrent neural network image generation. proceedings international conference machine learning raiko tapani kyunghyun bengio yoshua. iterative neural autoregressive distribution esadvances neural information timator nade-k. processing systems oord a¨aron schrauwen benjamin. factoring variations natural images deep gaussian mixture models. advances neural information processing systems zhang chen guoguo dong kaisheng khudanpur sanjeev glass james. highway long shortterm memory rnns distant speech recognition. proceedings international conference acoustics speech signal processing rezende danilo mohamed shakir wierstra daan. stochastic backpropagation approximate inference proceedings deep generative models. international conference machine learning russakovsky olga deng krause jonathan satheesh sanjeev sean huang zhiheng karpathy andrej khosla aditya bernstein michael berg alexander fei-fei imagenet large scale visual recognition challenge. international journal computer vision salimans kingma diederik welling max. markov chain monte carlo variational inference bridging gap. proceedings international conference machine learning sohl-dickstein jascha weiss eric maheswaranathan niru ganguli surya. deep unsupervised learning using nonequilibrium thermodynamics. proceedings international conference machine learning stollenga marijn byeon wonmin liwicki marcus schmidhuber juergen. parallel multi-dimensional lstm application fast biomedical volumetric imadvances neural information segmentation. processing systems sutskever ilya martens james hinton geoffrey generating text recurrent neural networks. proceedings international conference machine learning", "year": 2016}