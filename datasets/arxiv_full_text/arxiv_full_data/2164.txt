{"title": "A Deep Learning Approach for Joint Video Frame and Reward Prediction in  Atari Games", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Reinforcement learning is concerned with identifying reward-maximizing behaviour policies in environments that are initially unknown. State-of-the-art reinforcement learning approaches, such as deep Q-networks, are model-free and learn to act effectively across a wide range of environments such as Atari games, but require huge amounts of data. Model-based techniques are more data-efficient, but need to acquire explicit knowledge about the environment.  In this paper, we take a step towards using model-based techniques in environments with a high-dimensional visual state space by demonstrating that it is possible to learn system dynamics and the reward structure jointly. Our contribution is to extend a recently developed deep neural network for video frame prediction in Atari games to enable reward prediction as well. To this end, we phrase a joint optimization problem for minimizing both video frame and reward reconstruction loss, and adapt network parameters accordingly. Empirical evaluations on five Atari games demonstrate accurate cumulative reward prediction of up to 200 frames. We consider these results as opening up important directions for model-based reinforcement learning in complex, initially unknown environments.", "text": "reinforcement learning concerned identifying reward-maximizing behaviour policies environments initially unknown. stateof-the-art learning approaches deep q-networks model-free learn effectively across wide range environments atari games require huge amounts data. model-based techniques data-efﬁcient need acquire explicit knowledge environment. paper take step towards using modelbased techniques environments highdimensional visual state space demonstrating possible learn system dynamics reward structure jointly. contribution extend recently developed deep neural network video frame prediction atari games enable reward prediction well. phrase joint optimization problem minimizing video frame reward reconstruction loss adapt network parameters accordingly. empirical evaluations atari games demonstrate accurate cumulative reward prediction frames. consider results opening important directions model-based reinforcement learning complex initially unknown environments. reinforcement learning concerned ﬁnding optimal behaviour policies maximize agents’ future reward. approaches divided model-free model-based. model-free settings agents learn trial error explicitly capture dynamics environment structure reward function. state-of-the-art model-free approaches deep q-networks effectively approximate so-called q-values i.e. expected cumulative future reward taking speciﬁc actions given state using deep neural networks. impressive effectiveness approaches comes ability learn complex policies directly high-dimensional inputs despite effectiveness model-free approaches require substantial amount training data collected direct interactions environment limits applicability sampling data expensive additionally model-free requires access reward observations training problematic environments sparse reward structure—unless coupled explicit exploration mechanism. second alternative algorithms model-based. here agents explicitly gather statistics environment reward—in narrow deﬁnition statistics comprise environment dynamics reward function. recent work model-based techniques successfully used learn statistics cumulative future rewards improve exploration resulting data efﬁcient learning compared model-free approaches. accurate model true environment dynamics true reward function available model-based approaches planning monte-carlo tree search outperform model-free state-of-the-art approaches open question whether effective model-based possible complex settings environment dynamics reward function initially unknown agent acquire knowledge experience. paper take step towards addressing question extending recent work video frame prediction effectively learns system dynamics arcade learning environment atari games enable reward prediction well. resulting approach deep convolutional neural network enables joint prediction future states rewards using single latent representation. network parameters trained minimizing joint optimization objective minimizing video frame reward reconstruction loss. joint prediction model necessary prerequisite model-based algorithms dyna learning planning montecarlo tree search evaluate approach atari games. empirical results demonstrate successful prediction cumulative rewards roughly frames. complement quantitative results qualitative error analysis visualizing example predictions. results ﬁrst demonstrate feasibility using learned dynamics reward model model-based rest paper structured follows. section discuss related work motivate approach further. section describe network architecture training procedure. section present results cumulative reward prediction. section conclude outline future work. lines research related work presented paper model-based optimal control theory. model-based utilizes given learned model aspect task e.g. reduce data exploration requirements optimal control theory describes mathematical principles deriving control policies continuous action spaces maximize cumulative future reward scenarios known system dynamics known reward structure recent interest combining principles optimal control theory model-based learning settings information system dynamics available priori instead acquired visual data general idea behind approaches learn compressed latent representation visual state space images autoencoder networks utilize acquired latent representation infer system dynamics. system dynamics used specify planning problem solved optimization techniques derive optimal policies. introduce approach learning system dynamics visual data jointly training variational autoencoder state prediction model operates autoencoder’s compressed latent state representation. similar approach jointly learning compressed state representation predictive model pursued devise sequential approach ﬁrst learns latent state representation visual data subsequently exploits latent representation augment robot’s initial state space describing joint angles end-effector positions. augmented state space used improve estimates local dynamics planning. approaches presented assume knowledge functional form true reward signal hence directly applicable settings like reward function initially unknown. planning settings therefore necessitates learning system dynamics reward function order infer optimal behavioral policies. recent work introduced approach learning environment dynamics pixel images demonstrated enabled successful video frame prediction frames. current paper extend recent work enable reward prediction well modifying network’s architecture training objective accordingly. modiﬁcation training objective bears positive side effect since network must optimize compound loss consisting video frame reconstruction loss reward loss reward-relevant aspects video frames reconstruction loss alone might insensitive explicitly captured optimization objective. subsequent section elucidate approach well extensions detail. deep network proposed video frame prediction atari games aims learning function predicts video frame next time step given current history frames st−h+t time horizon current action taken agent— section extend work enable joint video frame reward prediction network anticipates current reward rt—see sections video-frame-predictive architecture comprises three information-processing stages encoding stage maps input frames compressed latent representation transformation stage integrates current action compressed latent representation decoding stage maps compressed latent representation predicted next frame—see figure initial encoding stage sequence convolutional forward operations current frame history st−h+t—a three-dimensional tensor—to compressed feature vector henc transformation stage converts compressed feature vector henc actionconditional representation hdec vectorized form integrating current action current action figure network architecture joint video frame reward prediction. architecture comprises three stages encoding stage mapping current input frames compressed latent representation transformation stage integrating current action latent representation element-wise vector multiplication denoted ﬁnal predictive stage reconstructing frame next time step current reward. network uses three different types neuron layers combination three different types activation functions dimensional extend individual layers either depicted beneath within layers. network part coloured highlights extension reward prediction. represented one-hot vector length varying game game since least actions ale. integration current action compressed feature vector includes element-wise vector multiplication—depicted figure —with particularity neuron layers involved element-wise multiplication layers entire network without bias parameters section finally decoding stage performs series forward deconvolutional operations mapping actionconditional representation hdec current frame history st−h+t current action predicted video frame next time step note necessitates reshape operation beginning decoding cascade order transform vectorized hidden representation three-dimensional tensor. whole network uses linear rectiﬁed linear units only. experiments following video frames processed network grey-scale images down-sampled fullresolution atari images ale. following history frame time horizon section detail proposed network architecture joint video frame reward prediction. model assumes ternary rewards result reward clipping line original game scores integers vary signiﬁcantly different atari games corresponding original rewards clipped assume three values negative rewards reward positive rewards. reward clipping rewards represented vectors one-hot encoding size figure extension video-frame-predictive architecture enable reward prediction highlighted red. additional softmax layer predict current reward information contained action-conditional encoding hdec motivation behind extension twofold. first extension makes possible jointly train network compound objective emphasizes video frame reconstruction reward prediction thus encourages network abstract away reward-relevant features reconstruction loss alone might insensitive. second formulation facilitates future model reward prediction virtual roll-outs compressed latent space without computational expensive necessity reconstructing video frames explicitly— note requires another shortcut predictive model hdec following previous work actions chosen agent every fourth frame repeated frames skipped. skipped frames repeated actions hence part data sets used train test predictive network original reward values accumulated four frames clipping. training advances. training details hyperparameter settings sections network parameters updated stochastic gradient descent derivatives training objective computed backpropagation time evaluations investigate cumulative reward predictions quantitatively qualitatively different atari games quantitative analysis comprises evaluating cumulative reward prediction error—see section qualitative analysis comprises visualizations example predictions seaquest—see section quantitative evaluation examines whether joint model system dynamics reward function results shared latent representation enables accurate cumulative reward prediction. assess cumulative reward prediction test sets consisting approximately video frames game including actions rewards. network evaluated trajectories—suitable analyze -step ahead prediction—drawn randomly test set. look ahead prediction measured terms cumulative reward error difference ground truth cumulative reward predicted cumulative reward. game results empirical distributions cumulative reward error—one distribution look ahead step— consisting samples compare model predictions baseline model samples rewards marginal reward distribution observed test game. note negative reward values absent games investigated here. figure illustrates empirical cumulative reward error distributions games network model blue baseline model together median percentiles cumulative reward error look ahead steps across games observe joint video frame reward prediction model accurately predicts future cumulative rewards least look ahead steps predicts future rewards substantially accurately baseline model. evidenced cumulative reward error distributions maintain unimodal form mode zero ﬂatten quickly distributions random-prediction baseline model. best results achieved freeway q*bert probability zero cumulative reward error look ahead steps still around respectively—see figure note collected agent playing atari game index trajectories time index samples within trajectory parameter denotes number trajectories training minibatch respectively parameter denotes length individual trajectory. case trained dqn-agents according collected trajectory samples training ﬁnished. original training objective consists video frame reconstruction loss terms squared loss function aimed minimizing square l-norm difference vector ground truth image action-conditional reconstruction. extend training objective enable joint reward prediction. results compound training loss consisting original video frame reconstruction loss reward prediction loss—given cross entropy loss proven value classiﬁcation problems ground truth reward predicted reward denotes k-step look ahead frame prediction target video frame denotes k-step look ahead probability values reward-predicting softmax layer—depicted figure—with target reward vector t+k. note reward values ternary reward clipping. parameter controls trade-off video frame reconstruction reward loss. parameter time horizon parameter determines often single trajectory sample unrolled future determines look ahead prediction horizon dictating network predicts future using video frame predicted output input next time step. following apply curriculum learning scheme successively increasing course training network initially learns predict short time horizon becomes ﬁne-tuned longer-term predictions look ahead steps correspond frames underlying agent collecting trajectory samples training testing model skipped every fourth frame choosing action—see section lowest performance obtained seaquest probability zero cumulative reward error steps around begins ﬂatten soon thereafter—see figure running emulator frequency steps correspond second real-time game play frame skipping. since model capable predicting steps ahead less second model enables real-time planning could therefore utilized online fashion. turn attention error analysis. look ahead step errors become prominent differs substantially game game overall model underestimates cumulative reward. seen asymmetry towards positive cumulative reward error values inspecting percentile intervals ﬁrst plot game figure identify likely cause stochastic transitions inherent games. considering seaquest running example objects divers submarines enter scene randomly right left time essential impact rewards agent potentially collect. ground truth trajectories agent’s actions reactions objects. predicted future trajectory deviates ground truth targeted actions shooting miss target leading underestimating true reward. analyze effect detail section experiments conducted triplicate different initial random seeds. different initial random seeds signiﬁcant impact cumulative reward prediction games except freeway—see section detailed analysis. discussed results concerning reward prediction only. appendix also evaluate joint performance reward video frame prediction test terms optimization objective authors report successful video frame reconstruction approximately steps observe similar results—see section finally could also ascertain using joint training objective joint video frame reward prediction beneﬁcial comparing alternative training methods separate objectives video frame reconstruction reward prediction. conducted additional experiments baseline models. ﬁrst baseline model uses network architecture figure decoupled training objective reward prediction part trained using latent representation video frame prediction part input—this means gradient updates respect reward prediction impact parameters video frame prediction. second baseline model uses decoupled architecture completely separate convolutional networks—one video frame prediction reward prediction— without shared parameters. overall results additional experiments joint training objective decoupled architecture work better decoupled training objective. added beneﬁt joint training objective decoupled architecture signiﬁcant reduction overall number parameters shared network layers section details. previous section identiﬁed stochasticity state transitions likely cause relatively performance long-term cumulative reward prediction games seaquest. seaquest objects randomly enter scene non-deterministic fashion. errors predicting events result predicted possible futures match actually observed future states resulting inaccurate reward predictions. here support hypothesis visualizations seaquest illustrating joint video frame reward prediction single network steps —see figure ground truth video frames compared predicted video frames terms error maps. error maps emphasize difference between ground truth predicted frames squared error values pixels black white depending whether objects absent present mistake network’s prediction. actions ground truth rewards model-predicted rewards shown state transitions. peculiarities prediction shown red. step model predicts reward mistake agent barely misses target. steps report model predicts reward correctly time step. steps depict problems caused objects randomly entering scene right model cannot predict. steps show problems predict rewards steps rewards attached objects model failed notice entering scene earlier. paper extended recent work video frame prediction atari games enable reward prediction. approach used jointly predict video frames cumulative rewards horizon approximately frames different games achieved best results freeway q*bert probability zero cumulative reward error frames still around respectively worst results seaquest probability zero cumulative reward error frames around compared model random prediction baseline model well sophisticated models separate objectives video frame reward prediction ascertaining beneﬁts joint training objective. study general line research using autoencoder networks learn latent representation visual data extends line research showing autoencoder networks capable learning combined representation system dynamics reward function reinforcement learning settings high-dimensional visual state spaces—a ﬁrst step towards applying model-based techniques environments reward function initially known. positive results open intriguing directions future work. long-term goal integration model-based model-free approaches effective interactive learning planning complex environments. directions achieving long-standing challenge include dyna method uses predictive model artiﬁcially augment expensive training data shown lead substantial reductions data requirements tabular approaches. recently dyna-style algorithm combined environments continuous action space known reward function opposed actions discrete atari game’s reward function unknown. alternatively model could utilized planning monte-carlo tree search recently similar approach proposed work used jointly learn system dynamics reward function atari domain applied context planning monte-carlo tree search procedure could compete state-of-theart results achieved model-free baseline compounding errors predictions. observe compounding errors approach instead demonstrated reliable joint video frame reward prediction frames establishing hence necessary prerequisite planning high-dimensional environments like atari games unknown dynamics reward function. hypothesize model-based reinforcement learning agents using proposed model joint video frame reward prediction presented work particularly beneﬁcial multi-task life-long learning scenarios reward function changes environment dynamics stationary. testing hypothesis requires ﬂexible learning framework reward function artiﬁcial environment changed experimenter arbitrary fashion possible environment reward function ﬁxed game. learning environment providing ﬂexibility recently released malm¨o platform minecraft researchers create user-deﬁned environments tasks order evaluate performance artiﬁcial agents. joint video frame reward prediction also improve upon exploration high-dimensional environments sparse reward signals. seminal work authors could demonstrate improved exploration atari games encouraging agent visit novel predicted states dissimilar states visited before. authors propose model-based exploration scheme triggers intrinsic reward signal whenever agent enters state high prediction error according state prediction model yielding impressive exploration behaviour environments like vizdoom super mario bros. importantly aforementioned exploration schemes include reward prediction model could improved upon encouraging agent visit novel states potentially reward-yielding high reward prediction error. paper aims establish necessary prerequisite model-based learning environments unknown dynamics unknown reward function need learnt visual input. model-based techniques applied past scenarios non-visual state spaces reward function known system dynamics system dynamics approximated locally linear gaussian assumptions approaches sample-efﬁcient unsuited highdimensional state representations. extensions model non-deterministic state transitions dropout variational autoencoders promising direction alleviate limitations provoked random events. pseudorandom events likely cause biased reward prediction severe seaquest section predicting alternative versions future could address bias reward prediction process. work similar recent studies model-based learning advantage approach potentially enable efﬁcient planning low-dimensional latent spaces observed state space high-dimensional. figure cumulative reward error look ahead steps different atari games. plots game. plot game shows median percentiles cumulative reward error evolve look ahead steps model baseline model samples rewards marginal reward distribution test vertical slice concise representation corresponds single empirical distribution cumulative reward error. depict every ﬁfth look ahead step compound plots models. empirical error distributions demonstrate successful cumulative reward prediction least steps games evidenced zero-centered unimodal shape ﬁrst column compound plot game. figure example predictions seaquest. ground truth video frames model predictions error maps emphasizing differences ground truth predicted frames—in form squared error pixel values—are compared column-wise. error maps highlight objects black white respectively depending whether objects absent mistake present mistake model’s prediction. actions taken agent well ground truth rewards reward predictions shown video error frames. peculiarities prediction process marked red. ﬁgure demonstrates predictive model fails anticipate objects randomly enter scene right rewards associated objects. references bellemare naddaf veness bowling arcade learning environment evaluation platform general agents. journal artiﬁcial intelligence research browne powley whitehouse lucas cowling rohlfshagen tavener perez samothrakis colton survey monte carlo tree search methods. ieee transactions computational intelligence games dosovitskiy springenberg brox learning generate chairs convolutional neural networks. proceedings ieee conference computer vision pattern recognition glorot bengio understanding difﬁculty training deep feedforward neural networks. proceedings international conference artiﬁcial intelligence statistics gregor danihelka graves rezende wierstra draw recurrent neural network image generation. proceedings international conference machine learning singh lewis wang deep learning real-time atari game play using ofﬂine monte-carlo tree search planning. advances neural information processing systems johnson hofmann hutton bignell malmo platform artiﬁcial intelligence experimentation. proceedings international joint conference artiﬁcial intelligence lange riedmiller voigtl¨ander autonomous reinforcement learning visual input data real proceedings international world application. joint conference neural networks mnih kavukcuoglu silver rusu veness bellemare graves riedmiller fidjeland ostrovski petersen beattie sadik antonoglou king kumaran wierstra legg hassabis human-level control deep reinforcement learning. nature ranzato huang boureau lecun unsupervised learning invariant feature hierarchies proceedings applications object recognition. ieee conference computer vision pattern recognition rezende mohamed wierstra stochastic backpropagation approximate inference deep generative models. proceedings international conference machine learning simard steinkraus platt best practices convolutional neural networks applied visual document analysis. proceedings international conference document analysis recognition srivastava mansimov salakhutdinov unsupervised learning video representations using lstms. proceedings international conference machine learning sutton integrated architectures learning planning reacting based approximating dynamic programming. proceedings international conference machine learning watter springenberg boedecker riedmiller embed control locally linear latent dynamics model control images. advances neural information processing systems weber racaniere reichert buesing guez rezende puigdomenech badia vinyals heess pascanu battaglia silver wierstra imagination-augmented agents deep reinforcement learning. arxiv performed experiments python chainer adhered instructions close possible. trajectory samples learning network parameters obtained previously trained agent according dataset training comprised around video frames game addition actions chosen agent rewards collected game play. video frames used network input grey-scale images pixel values down-sampled full-resolution images. applied preprocessing step dividing pixel subtracting mean pixel values image leading ﬁnal pixel values detailed network architecture shown figure main paper. weights network initialized according except layers participate element-wise multiplication figure weights action-processing layer initialized uniformly range weights layer receiving latent encoding input video frames initialized uniformly range training performed minibatch iterations curriculum learning scheme increasing look ahead parameter every iterations increasing look ahead parameter ﬁrst time iterations minibatch size also altered learning rate parameter updates throughout entire curriculum scheme time horizon parameter determining number times single trajectory unrolled future optimizer updating weights adam gradient momentum squared gradient momentum epsilon parameter evaluation mode network outputs clipped strong activations could accumulate roll-out time network. experiments modiﬁed reward prediction loss slightly order prevent exploding gradient values replacing term ﬁrst-order taylor approximation p-values smaller e−—a similar technique used improve stability optimization algorithm. identify optimal values reward weight performed initial experiments pacman without applying aforementioned curriculum learning scheme instead using ﬁxed look ahead parameter evaluated effect different λ-values training objective identiﬁed conducting experiments—see section identifying optimal reward weight conducted additional initial experiments without curriculum learning ﬁxed look ahead parameter different atari games used paper. observed periodic oscillations reward prediction loss training objective seaquest ﬁxed adding gradient clipping threshold parameter optimization procedure— experiments investigating effect gradient clipping seaquest reported section ﬁne-tuning effect curriculum learning training objective ﬁnal experiments shown section analysed atari games. identify optimal values reward weight conducted initial experiments pacman without curriculum learning ﬁxed look ahead horizon tested four different λ-values investigated frame reconstruction loss reward loss training objective evolve minibatch iterations—see figure best results obtained whereas values lead signiﬁcantly slower convergence worse overall training performance respectively. figure —which solved adding gradient clipping optimization procedure—see second third column figure tested different values gradient clipping threshold worked value oscillation vanished completely. ﬁnal experiments curriculum learning networks trained minibatch iterations total look ahead parameter gradually increased every iterations networks hence initially trained one-step ahead prediction later ﬁne-tuned further-step ahead prediction. figure shows training objective evolves iterations. characteristic bumps training objective every iterations training evolves demonstrate improvements long-term predictions games except freeway training objective assumed already values within ﬁrst iterations might therefore insensitive ﬁne-tuning curriculum learning. conducted three different experiments game different initial random seeds. effect different initial random seeds cumulative reward error summarized figure reports median percentiles cumulative reward error evolve look ahead steps different experiments game. note results ﬁrst column figure shown figure main paper together detailed analysis depicting empirical cumulative reward error distributions look ahead steps. random initial seed seem signiﬁcant impact cumulative reward prediction except freeway network third experiment starts considerably overestimate cumulative rewards around look ahead steps. order investigate reward overestimation freeway further analyse visualizations joint video frame reward prediction particular seed results shown figure peculiar situation occurs predicted look ahead steps. freeway agent’s cross busy road bottom without bumping order receive reward. agent bumps agent propelled downwards away reward-yielding top. propelled downwards movement happens even agent tries move upwards. exactly kind situation depicted beginning figure occurs particular prediction steps. predictive model figure effect reward weight training loss pacman. four panels depicts experiment different reward weight panel shows training loss evolves minibatch iterations terms subplots reporting video frame reconstruction reward loss respectively. experiment conducted three times different initial random seeds depicted blue green red. graphs smoothed exponential window size however able correctly predict aforementioned downwards movement caused agent hitting highlighted throughout steps documenting increasing ground truth predicted agent position propelled downwards movement ground truth agent continues. course prediction network model assumes agent reach reward-yielding side road early results sequence erroneous positive reward predictions throughout steps side effect seemingly predictive model loses track objects scene. concluding ﬁnding serve possible explanation cumulative reward overestimation particular experiment freeway. main paper analysis focuses evaluating well model serves purpose cumulative reward prediction. here evaluate network performance terms video frame reconstruction loss well reward prediction loss test following analysis conducted game sample minibatches size underlying test compute test loss look ahead steps formula presented main paper section used learning network parameters without averaging look ahead steps illustrate test loss function look ahead steps— statistics analysis plotted figure best overall test loss achieved freeway initial look ahead steps q*bert accordance results cumulative reward prediction main paper. also line results main paper ﬁnding reward loss test worse seaquest pacman space invaders compared q*bert freeway. worst video frame reconstruction loss observed space invaders compliance authors report objects scene moving period time steps hard predict network taking last frames last steps input future predictions. ﬁrst sight might seem surprising reward prediction loss space invaders signiﬁcantly lower seaquest pacman long-term ahead prediction despite higher frame reconstruction loss space invaders. possible explanation paradox might frequency rewards collected—this frefigure effect gradient clipping training loss seaquest. three panels compare experiments reward clipping reward clipping using threshold values respectively. subplots within panel similar figure display ﬁrst evolution compound training loss addition frame reconstruction reward loss. experiments conducted seed containing outlier freeway figure joint objective works equally well decoupled architecture added beneﬁt less parameters required shared network architecture. shared network architecture single latent representation encoding dynamics instantaneous reward signal might also advantageous future work computationally efﬁcient planning lower-dimensional latent spaces. quency signiﬁcantly higher seaquest pacman space invaders. reward prediction model bias zero—as indicated paper—might therefore less often rewards collected lower frequency hence yielding lower reward reconstruction loss. could furthermore ascertain using joint objective video frame reconstruction reward prediction beneﬁcial. conducted experiments baseline approaches using separate objectives video frame reward prediction. ﬁrst baseline uses architecture main paper decoupled training objective. means reward prediction part network shown figure trained using hidden representation video frame prediction part input. importantly gradient updates respect reward prediction longer affect video frame prediction parameters would training conducted joint objective. second baseline uses decoupled network architecture different convolutional networks video frame reward prediction trained separately another. video frame prediction network decoupled architecture modelled according depicted figure without reward prediction part highlighted whereas reward prediction network modelled similar figure encoding stage action-conditioned transformation stage reward prediction layer without video frame decoding stage. results shown figure overall training decoupled objective signiﬁcantly worse compared training joint objective training decoupled architecture games except seaquest well baseline model experiment. refers different game column refers different experiment game initialized different random seed. ﬁrst column ﬁgure presented figure main paper explaining results detail additionally illustrating empirical distributions cumulative reward error look ahead steps. figure loss test look ahead steps. reports loss test look ahead steps different game. ﬁrst column illustrates compound loss consisting video frame reconstruction loss reward prediction loss loss test computed according similar training loss learning network parameters however different look ahead parameter different minibatch size without averaging look ahead steps since plot test loss function look ahead steps. game test loss computed minibatches resulting empirical distribution loss values look ahead step. ﬁgure shows mean median percentiles well minimum maximum elements empirical distributions. figure effect different training methods cumulative reward error. plots show cumulative reward error evolves look ahead steps terms median percentiles network models well random prediction baseline model refers different game. column refers different training method. ﬁrst column refers training joint objective video frame reward prediction outlined main paper presented third column figure —note contains outlier freeway. second column refers training decoupled objective reward prediction part network figure trained separately using hidden state video frame prediction model input. third column refers training decoupled architecture separate convolutional networks video frame reconstruction reward prediction. overall result training decoupled objective works worse compared using joint training objective decoupled architecture. joint objective works equally well decoupled architecture requires signiﬁcantly less parameters shared network architecture.", "year": 2016}