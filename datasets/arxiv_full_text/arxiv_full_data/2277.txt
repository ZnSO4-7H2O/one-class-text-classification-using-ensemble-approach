{"title": "Building Robust Deep Neural Networks for Road Sign Detection", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Deep Neural Networks are built to generalize outside of training set in mind by using techniques such as regularization, early stopping and dropout. But considerations to make them more resilient to adversarial examples are rarely taken. As deep neural networks become more prevalent in mission-critical and real-time systems, miscreants start to attack them by intentionally making deep neural networks to misclassify an object of one type to be seen as another type. This can be catastrophic in some scenarios where the classification of a deep neural network can lead to a fatal decision by a machine. In this work, we used GTSRB dataset to craft adversarial samples by Fast Gradient Sign Method and Jacobian Saliency Method, used those crafted adversarial samples to attack another Deep Convolutional Neural Network and built the attacked network to be more resilient against adversarial attacks by making it more robust by Defensive Distillation and Adversarial Training", "text": "abstract—deep neural networks built generalize outside training mind using techniques regularization early stopping dropout. considerations make resilient adversarial examples rarely taken. deep neural networks become prevalent mission critical real time systems miscreants start attack intentionally making deep neural networks misclassify object type seen another type. catastrophic scenarios classiﬁcation deep neural network lead fatal decision machine. work used gtsrb dataset craft adversarial samples fast gradient sign method jacobian saliency method used crafted adversarial samples attack another deep convolutional neural network built attacked network resilient adversarial attacks making robust defensive distillation adversarial training index terms—road sign classiﬁcation gtsrb dataset non-targeted adversarial attack target adversarial attack adversarial sample crafting defensive distillation adversarial training robust deep neural networks. availability computational resources abundance data huge resurgence using deep neural networks object recognition classiﬁcation several machine learning models including state-of-the-art deep neural networks consistently misclassify adversarial examples inputs formed applying small intentionally engineered worst-case perturbations input images. perturbations indiscernible humans make deep neural networks make wrong classiﬁcations high conﬁdence. problem becomes concerning advent self-driving cars automatic detection classiﬁcation road signs path planning adjusting speed driving behaviors. convolutional neural network detects road signs self-driving adversarial inputs even though obvious human classify correctly network make egregious misclassiﬁcation road sign. result self-driving cars making erroneous decisions. work ways create adversarial examples road sign images explored order fool state-of-the-art neural networks effort build robust neural networks resilient attacks made. section previous work done related adversarial examples addressed. explanations methods used craft adversarial examples ways used build robust neural networks resilient adversarial samples presented section dataset used data augmentation processes also described experimental results shown section ﬁnally discussions weakness work well possible future extensions work discussed section finally scope work concluded section work nguyen inception fooling state-of-the-art neural networks. work shown deep neural networks easily fooled classify images recognized humans belonging particular classes high conﬁdence. ﬁrst showed state-of-the-art machine learning models including neural networks vulnerable adversarial examples. based work presented generate adversarial examples fast gradient sign method adversarial training result regularization dropout adversarial examples generalize across different deep neural network models. explores reason behind adversarial attacks presented imperfections training phase deep neural networks make vulnerable adversarial samples. formalizes space adversaries deep neural networks introduces novel class algorithms craft adversarial samples. addressed issue black-box adversarial attacks aligns problem space work aiming address. adversarial samples crafted perturbing pixels image explored adversarial examples transferred physical world adversarial printed road signs grafﬁti like road signs seem like vandalized neural networks grafﬁti overlays lead classiﬁcation astray correct prediction. nonetheless recent work stated standard detectors fasterrcnn yolo fooled physical adversarial stop signs. based method knowledge distillation described papernot proposed method making neural networks resilient adversarial samples stated defense adversarial samples defensive distillation work. since ﬁeld adversarial sample crafting defense adversarial atsecond objective second step pipeline craft adversarial samples based gtsrb dataset. adversarial samples used attack deepcnn without knowing architecture weights. order craft adversarial examples another built. call adversarialcnn. detailed architecture adversarialcnn depicted table adversarialcnn used craft adversarial samples using fast gradient sign method jacobian-based saliency method adversarial samples generated ﬁrst analyzed whether perturbations visually perceptible best hyperparameters produce least visually perceptible adversarial samples picked generate samples attack deepcnn. ﬁnal step pipeline work defense adversarial attacks methods called adversarial training defensive distillation methods used make deepcnn robust adversarial attacks. deepcnn adversarialcnn trained scratch without weights initialization pretrained networks. reason training scratch emulate hybrid black attack close possible. since models trained scratch model knowledge model. therefore adversarialcnn generating samples knowing input dataset output classes gtsrb dataset. reason adversarialcnn still knowledge dataset restricts approach full black attack. motivation behind building cnns different architectures follows assume adversary wants attack neural network trafﬁc sign classiﬁcation self-driving car. adversary know architecture weights neural network knows input output pairs. example attacker knows given stop sign image network predicts class label stop sign. therefore attacker build replica network different architecture generate adversarial samples potentially attack deeper complex network black fashion. compare architecture deepcnn adversarialcnn seen deepcnn deeper reprsentational power. reason making adversarialcnn less complex less powerful make task generating adversarial examples hard possible attack deeper complex model. emulates realistic scenario deployed neural network likely complex compared replica network miscreant would built attacking dataset work german trafﬁc sign recognition benchmark dataset used popular benchmark deep learning problems. dataset composed images image belonging classes. dataset split training fig. pipeline methodology implemented work crafting adversarial samples attacking state-of-the-art convolutional neural network building robust neural network road sign detection. tacks fairly research topic machine learning various literature debating effectiveness different methods. work serves connecting different ends literature attacking deep neural networks adversarial samples defending adversarial attacks verifying whether defensive methods work dataset different datasets used literature. methodology pipeline three main objectives work building state-of-the-art trafﬁc sign classiﬁcation deep convolutional neural network crafting adversarial samples gtsrb dataset fooling state-of-the-art network making network resilient attacks. methodology ﬂowchart figure depicts scope creating robust neural network built upon objectives. ﬁrst objective ﬁrst step pipeline build classiﬁer near state-of-the-art performance test gtsrb dataset. order satisfy objective trained tested convolutional neural network detailed architecture deepcnn depicted table deepcnn trained epochs optimizer learning rate momentum learning rate decay. deepcnn network adversary wants possible generate training examples applying transformations using since infeasible generate training data manual automatic data augmentation process done applying small random perturbations base points using ones order obtain different transformation matrices. image generated process shown figure network architecture detailed architectures adversarialcnn deepcnn described table table networks take -channel image width pixels height pixels. adversarialcnn fairly shallow convolutional neural network compared deepcnn. main motivation behind intentionally making adversarialcnn less complex deepcnn depicted section deepcnn penultimate layer called temperature layer tunable parameter layer implementation concept depicted section shape kernel size number ﬁlters stride padding zero-padding relu kernel size number ﬁlters stride padding padding relu kernel size number ﬁlters stride padding padding relu flatten nodes nodes fig. histogram samples classes. class imbalance visible original dataset histogram samples class data augmentation mean samples class chosen cutoff augmenting samples classes lower mean samples class. testing set. since building convolutional neural network ﬁxed input size decided work images size furthermore images smaller size removed. since high class imbalance training samples seen histogram presented figure decision generating data classes quantity samples taken. data augmentation well known data machine learning algorithms access effective are. common data augmentation performing afﬁne transformations image. afﬁne transformation transformation expressed matrix multiplication vector addition. used perform rotations translations scale operations images basically relation images using following matrix shape kernel size number ﬁlters stride padding zero padding relu kernel size number ﬁlters stride padding zero padding relu window size stride probability kernel size number ﬁlters stride padding zero padding relu kernel size number ﬁlters stride padding zero padding adversarial sample input crafted impact deep learning algorithms output integrity. could input deliberately built cause artiﬁcial neural network misclassify input different class supposed least reduce output conﬁdence. attacks could targeted untargeted. targeted attack aims cause particular output class input untargeted attack aims misclassify output class regardless crafting adversarial samples require modiﬁcation training process created model fully trained. linear structure neural networks suggests weakness linear perturbations images equation explains phenomenon small perturbation plus original image affects predictions classiﬁer larger sized vector results substantial change classiﬁer. fig. fast gradient sign method example km/h road sign perturbations misclassiﬁed km/h road sign. perturbation image normalized ampliﬁed data. oracle mode neither training data network architecture known adversary adversary access model oracle. adversary output supplied inputs. samples mode adversary collection pairs input output related classiﬁer. fast gradient sign method fast gradient sign method mathematical method generating adversarial examples derivation parameters neural network equation formulates perturbation taking derivative cost function direction perturbation nominal size cost function conﬁdence label image resulting perturbations added equation equation deﬁned gradient image certain pixel figure shows image example equations generates adversarial example misclassiﬁes km/h road sign km/h road sign. examination adversarial example itself user would difﬁculties discerning whether adversarial image. images different perceptible adversarial examples. user perceptibly deﬁned ability perceive adversarial attack image. fgsm user perceptibly depends size feature vector choice parameter feature vector image dimension imperceptible adversarial examples easier craft larger dimension images. jacobian-based saliency method jacobian-based saliency method ﬁrst introduced papernot method based identifying small pixels candidates perturbation order perform class targeted attack. equation describes jacobian-based saliency works. function learned trained neural network legitimate sample targeted class idea behind defensive distillation based knowledge distillation ﬁrst coined hinton knowledge distillation training procedure model trained produce entropic outputs using original one-hot encoded training labels second model uses entropic soft target outputs training label. original work main motivation behind knowledge distillation distill knowledge bigger network smaller network deployed mobile devices still equivalent representative power bigger network. case defensive distillation need smaller model main goal compression robust networks. therefore setting defensive distillation network architecture trained twice; ﬁrst original one-hot encoded labels produce entropic predictions tuning temperature parameter second using entropic soft targets training labels back network retrained scratch labels. distillation makes ﬁnal model’s responses smoother therefore works even models size. using network architecture product outputs back network training sound counterintuitive reason works ﬁrst model trained hard labels provides soft labels used train second model. implicitly makes network less conﬁdent predictions. network less conﬁdent predictions useful network making wrong predictions. desirable network less conﬁdent wrong predictions network conﬁdent wrong predictions. even network making wrong predictions better make wrong predictions semantically similar actual class actual class showing second third prediction. figure shows resulting predictions conﬁdent network figure shows resulting predictions less conﬁdent network. second third predictions less conﬁdent network semantically related actual class label compared second third predictions conﬁdent classiﬁer. therefore setting adversarial sample crafting small tugs perturbations lead network make conﬁdent incorrect classiﬁcations adversary sharp edges manifold class boundaries. temperature perform distillation convolutional neural network network whose output layer softmax ﬁrst trained original dataset. description original network described section consider logit outputs produced last hidden layer right trasformed normalized probabilities softmax function outputs obtained describe probability likely data class within softmax layer given neuron corresponding class indexed computes component small perturbation added objective perform optimization minimized still satisfying equation figure illustrates example adding perturbation original image entails calculating forward derivative neural network order build saliency distinguishes potential features perturbation. perturbation would leads desired adversarial output. approach category attack research able also produce category attack high rate success generating adversarial samples different architecture. summarize approach processed processed using jacobian-based saliency approach craft category attacks mentioned jacobian-based saliency approach assumes knowing network architecture weight parameters order build saliency identiﬁes input features candidate perturbation research show even without knowing network architecture able craft adversarial samples access training data even though attacker know architecture weights network wants attack attacker build replica network classiﬁcation task. attacker would able craft adversarial samples using knowledge network architecture weights. building robust neural networks defensive distillation defensive distillation smooths models decision surface adversarial directions exploited adversary parameter tunable parameter implemented additional layer softmax ﬁnal fully connected layer size nodes. additional layer needs perform element-wise division output vector fully connected layer. adversarial training using adversarial examples generated fast gradient sign method jacobian saliency method split training adversarial testing adversarial gtsrb dataset train test split. training adversarial samples back neural network training samples. adversarial training similar brute force approach solving robust neural networks generating adversarial samples expensive scalable. therefore adversarial training used supplement help defensive distillation scalable since needs parameter tuned. results experimental results based stated methodology obtained discussed separately adversarial crafting attack deepcnn defensive distillation adversarial training. adversarial examples fast gradient sign method adversarial neural network described section trained gtsrb data. resulting accuracy network back-propagation network initiate fast gradient sign method attack help cleverhans library. adversarial examples generated changing parameter step figure shows resulting adversarial examples generated images increasing last column explains whether image either classiﬁed missclassiﬁed. increasing changes magnitude perturbation well increasing chances misclassiﬁcation image. direction perturbation pixel deﬁned equation changing ﬁgure observe direction perturbation constant. words perturbed image retained constant pattern iteration perceptibility means whether user classify image attack neural network. result dependent image subjectivity person viewing image. compare images qualitatively authors assessment image left perceptible image right imperceptible. another result consider shown figure plots accuracy neural network increasing epsilons. accuracy deﬁned based number images correctly classiﬁed total images tests. exponential decay accuracy increasing \u0001’s. fig. adversarial example images fast gradient sign method increasing parameters pixel change pattern similar image. magnitudes perturbation increase increasing fig. accuracy adversarial examples fast gradient sign method. accuracy tested adversarialcnn adversarial examples. increasing exponentially decays accuracy neural network simple neural network generate adversarial examples uses parameters described section trained using gtsrb data three channels rgb. order generate adversarial samples ﬁrst chose samples class tried make targeted attack classes. total generated samples. success rate percentage adversarial samples successfully classiﬁed adversarial target class distortion percentage pixels modiﬁed legitimate sample obtain adversarial sample average. figure shows three examples successfully generated targeted adversarial samples deep attack deepcnn initial training step deepcnn temperature parameter temperature parameter temperature layer analogous identity mapping. moreover network restricted produce entropic softmax predictions therefore epochs network becomes conﬁdent predictions. summary training testing cross entropy losses percent correct accuracy entropy softmax predictions tabulated table deep achieved high test accuracy entropy softmax outputs training images indicates network conﬁdent making correct predictions also making wrong predictions. example deepcnn making conﬁdent predictions seen figure used adversarial examples generated fast gradient sign method jacobian saliency method test deepcnn. test accuracy went cross entropy loss rose table summarizes difference test statistics legitimates samples adversarial samples. figure shows samples road signs initially correctly classiﬁed misclassiﬁed perturbed fast gradient sign method jacobian saliency attack. indicates adversarial examples generate different network knowledge architecture weights another network still attack network certain extent. fig. examples conﬁdent predictions less conﬁdent predictions. produced deepcnn networks high conﬁdence tend make conﬁdent correct predictions well conﬁdent wrong predictions. produced deepcnn networks conﬁdence tend make less conﬁdent correct predictions well less conﬁdent wrong predictions. even though ﬁrst prediction wrong prediction semantically close actual ground truth shows second prediction network. shard knowledge networks training data training labels. signiﬁes primary purpose trying attack deep neural networks particular classiﬁcation works without knowing architecture weights parameters network. defensive distillation adversarial training order train deepcnn defensive distillation temperature parameter forces network produce entropic outputs. example softmax outputs distilled network seen figure trained defensive distillation even network making mistake correct prediction. opposed network trained without distillation network conﬁdent ﬁrst prediction second prediction semantically away correct class training deepcnn defensive distillation method network’s output accuracy test went considerably. network making entropic outputs seen table entropy training softmax fig. adversarial samples successfully fooled deepcnn misclassify. left column adversarial samples jacobian saliency method. right column adversarial samples fast gradient sign method. original samples road signs correctly classiﬁed deepcnn. right network trained defensive distillation network adversarial training set. using adversarial training network trained defensive distillation. testing adversarial samples deterioration test accuracy cross entropy loss less dramatic compared deepcnn without defensive distillation adversarial training. effect seen table samples misclassifed deepcnn defensive distillation adversarial training correctly classiﬁed defensive distillation adversarial training seen figure defensive distillation smoothen model learned deepcnn training helping model generalize better samples outside training dataset. distillation generates smoother classiﬁer models reducing sensitivity input perturbations. smoother classiﬁers found resilient adversarial samples improved class generalizability properties. methods crafting adversarial examples described paper successful generating black attack deep neural network. disadvantages method absence measurement parameter user perceptibility. parameters fgsm distortion rate jacobian saliency chosen perceptibility depends original image itself. extensive studies parameterizing perceptibility adversarial perturbafurthermore comparisons made visual differences fgsm method jacobian saliency method. ﬁrst method applies minuscule perturbations almost pixels image latter applies distinct perturbation small amount pixels. either advantageous disadvantageous depending situation. example authors used distinct modiﬁcations physical adversarial examples easier apply. defense adversarial samples analogous reducing variance mode regularization adversarial examples traditional overﬁtting problem previous work showed wide variety traditional regularization methods including dropout weight decay either fail defend adversarial examples seriously harming accuracy original task. another weakness making robust neural networks work defensive distillation adversarial training test accuracy cannot reach original test accuracy non-adversarial samples. moreover early stopping respect threshold parameter affects well defensive distillation work adversarial samples. work built complete pipeline crafting adversarial samples building deep convolutional neural network achieves near state-of-the-art test accuracy gtsrb dataset using adversarial examples attack near state-of-the-art network without prior knowledge architecture building robust neural network resilient adversarial examples. successfully crafted adversarial samples gtsrb dataset using fast gradient sign method jacobian saliency method. using adversarial samples able fool different network black manner since access internal weights architecture network. finally successfully made network robust defensive distillation adversarial training. work serves aggregated implementation work done previous literature adversarial sample crafting defense adversarial attack methods. work also differs form previous work adversarial sample crafting defense nicolas papernot patrick mcdaniel somesh matt fredrikson berkay celik ananthram swami. limitations deep learning adversarial settings. security privacy ieee european symposium pages ieee nicolas papernot patrick mcdaniel somesh ananthram swami. distillation defense adversarial perturbations deep neural networks. security privacy ieee symposium pages ieee nguyen jason yosinski jeff clune. deep neural networks easily fooled high conﬁdence predictions unrecognizable images. proceedings ieee conference computer vision pattern recognition pages christian szegedy wojciech zaremba ilya sutskever joan bruna dumitru erhan goodfellow fergus. intriguing properties neural networks. arxiv preprint arxiv. nicolas papernot patrick mcdaniel goodfellow somesh berkay celik ananthram swami. practical black-box attacks deep learning systems using adversarial examples. arxiv preprint arxiv. alexey kurakin goodfellow samy bengio. adversarial examples physical world. arxiv preprint arxiv. ivan evtimov kevin eykholt earlence fernandes tadayoshi kohno atul prakash amir rahmati dawn song. robust physical-world attacks machine learning models. arxiv preprint arxiv. jiajun hussein sibai evan fabry david forsyth. need worry adversarial examples object detection autonomous vehicles. arxiv preprint arxiv. shaoqing kaiming ross girshick jian sun. faster r-cnn towards real-time object detection region proposal advances neural information processing systems networks. pages johannes stallkamp marc schlipsing salmen christian igel. german trafﬁc sign recognition benchmark multiclass classiﬁcation competition. ieee international joint conference neural networks pages somesh matt fredrikson berkay celik ananthram swami nicolas papernot patrick mcdaniel. limitations deep learning adversarial settings. security privacy ieee symposium ieee goodfellow reuben feinman fartash faghri alexander matyasko karen hambardzumyan yi-lin juang alexey kurakin ryan sheatsley abhibhav garg yen-chen nicolas papernot nicholas carlini. cleverhans adversarial machine learning library. arxiv preprint arxiv.", "year": 2017}