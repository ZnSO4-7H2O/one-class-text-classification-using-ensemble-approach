{"title": "Near-Optimal Active Learning of Multi-Output Gaussian Processes", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "This paper addresses the problem of active learning of a multi-output Gaussian process (MOGP) model representing multiple types of coexisting correlated environmental phenomena. In contrast to existing works, our active learning problem involves selecting not just the most informative sampling locations to be observed but also the types of measurements at each selected location for minimizing the predictive uncertainty (i.e., posterior joint entropy) of a target phenomenon of interest given a sampling budget. Unfortunately, such an entropy criterion scales poorly in the numbers of candidate sampling locations and selected observations when optimized. To resolve this issue, we first exploit a structure common to sparse MOGP models for deriving a novel active learning criterion. Then, we exploit a relaxed form of submodularity property of our new criterion for devising a polynomial-time approximation algorithm that guarantees a constant-factor approximation of that achieved by the optimal set of selected observations. Empirical evaluation on real-world datasets shows that our proposed approach outperforms existing algorithms for active learning of MOGP and single-output GP models.", "text": "exploited improving prediction. example monitor soil pollution heavy metal complex time-consuming extraction soil samples alleviated supplementing prediction correlated auxiliary types soil measurements easier sample similarly monitor algal bloom coastal ocean plankton abundance correlates well auxiliary types ocean measurements sampled readily. examples real-world applications include remote sensing trafﬁc monitoring monitoring groundwater indoor environmental quality precision agriculture among others. practical examples motivate need design develop active learning algorithm selects informative sampling locations observed also types measurements selected location minimizing predictive uncertainty unobserved areas target phenomenon given sampling budget focus work here. achieve this model types coexisting phenomena jointly multi-output gaussian process allows spatial correlation structure type phenomenon cross-correlation structure different types phenomena formally characterized. importantly unlike non-probabilistic multivariate regression methods probabilistic mogp regression model allows predictive uncertainty target phenomenon formally quantiﬁed consequently exploited deriving active learning criterion. work differs multivariate spatial sampling algorithms improve prediction types coexisting phenomena existing active learning algorithms sampling measurements target phenomenon extended applied straightforwardly detailed section paperaddresses problem active learning multi-output gaussian process model representing multiple types coexisting correlated environmental phenomena. contrast existing works active learning problem involves selecting informative sampling locations observed also types measurements selected location minimizing predictive uncertainty target phenomenon interest given sampling budget. unfortunately entropy criterion scales poorly numbers candidate sampling locations selected observations optimized. resolve issue ﬁrst exploit structure common sparse mogp models deriving novel active learning criterion. then exploit relaxed form submodularity property criterion devising polynomial-time approximation algorithm guarantees constant-factor approximation achieved optimal selected observations. empirical evaluation real-world datasets shows proposed approach outperforms existing algorithms active learning mogp single-output models. many budget-constrained environmental sensing monitoring applications real world active learning/sensing attractive frugal alternative passive high-resolution sampling spatially varying target phenomenon interest. different latter active learning aims select gather informative observations modeling predicting spatially varying phenomenon given sampling budget constraints practice target phenomenon often coexists correlates well auxiliary type phenomena whose measurements spatially correlated less noisy and/or less tedious sample consequently model. consider utilizing entropy criterion measure predictive uncertainty target phenomenon widely used active learning single-output model. unfortunately mogp model criterion scales poorly number candidate sampling locations target phenomenon even number selected observations optimized resolve scalability issue ﬁrst exploit structure common unifying framework sparse mogp models deriving novel active learning criterion then deﬁne relaxed notion submodularity called \u0001-submodularity exploit \u0001-submodularity property criterion devising polynomial-time approximation algorithm guarantees constant-factor approximation achieved optimal selected observations empirically evaluate performance proposed algorithm using three real-world datasets multi-output gaussian process convolved mogp regression. number mogp models co-kriging parameter sharing linear model coregionalization proposed handle multiple types correlated outputs. generalization called convolved mogp model empirically demonstrated outperform others mogp model choice approximation whose structure exploited deriving active learning criterion turn efﬁcient approximation algorithm detailed later. types coexisting phenomena deﬁned vary realization cmogp domain corresponding sampling locations location associated noisy realized output measurement observed i}x∈d type then measurement type deﬁned convolution smoothing kernel latent measurement function corrupted additive noise {l}x∈d {yxi}xi∈d+ also every ﬁnite subset {yxi}xi∈d+ follows multivariate original notion submodularity used theoretically guarantee performance algorithms active learning single-output model. ease exposition consider single latent function. note however multiple latent functions used improve ﬁdelity modeling shown importantly proposed algorithm theoretical results remain valid multiple latent functions. gaussian distribution. fully speciﬁed prior mean exi] covariance σxixj covxi yxj] latter characterizes spatial correlation structure type phenomenon cross-correlation structure different types phenomena speciﬁcally {l}x∈d prior covariance σsin signal variance controlling intensity measurements type diagonal precision matrices controlling respectively degrees correlation latent measurements cross-correlation latent type measurements denotes column vector comprising components value then σxixj= σsjn supposing column vector realized measurements tuples observed locations corresponding measurement types cmogp regression model exploit observations provide gaussian predictive distribution measurements tuples unobserved locations corresponding measurement types following posterior mean vector covariance matrix xixj)xi∈axj∈a sparse cmogp regression. limitation cmogp model poor scalability number observations computing gaussian predictive distribution requires inverting incurs time. improve scalability unifying framework sparse cmogp regression models deterministic training conditional fully independent training conditional partially independent training conditional approximations exploit vector inducing measurements small inducing locations |d|) approximate measurement also share structural properties exploited deriving active learning criterion turn efﬁcient approximation algorithm measurements condidifferent types conditionally independent given pitc used sparse cmogp regression model work since others unifying framework impose assumptions. structural properties pitc utilize covariance matrix covariance components σxix σsin transpose block-diagonal matrix constructed diagonal blocks σaa|u matrix σaiai|u note computing require inducing locations observed. also covariance matrix approximated reduced-rank matrix summed resulting sparsiﬁed residual matrix using matrix inversion lemma invert approximated covariance matrix applying algebraic manipulations computing incurs time case evenly distributed observations among types. recall section entropy criterion used measure predictive uncertainty unobserved areas target phenomenon. using cmogp model gaussian posterior joint entropy measurements tuples unobserved locations corresponding measurement types expressed terms posterior covariance matrix σzz|x independent realized measurements index denote type measurements target phenomenon. then active learning cmogp model tuples sampling locations corresponding measurement types observed minimize posterior joint entropy type measurements remaining unobserved locations target phenomenon ﬁnite tuples candidate sampling locations target phenomenon corresponding measurement type available selected observation. however evaluating term incurs time prohibitively expensive target phenomenon spanned large number |vt| candidate sampling locations. auxiliary types phenomena missing ignored computational difﬁculty eased instead solving well-known maximum entropy sampling problem maxxt|xt|=n proven equivalent using chain rule entropy noting constant. evaluating term incurs time independent |vt|. equivalence result fact extended applied minimizing predictive uncertainty types coexisting phenomena exploited multivariate spatial sampling algorithms deﬁned similar manner measurement type equivalence result also follows chain rule entropy fact constant. unfortunately straightforward derive equivalence result active learning problem target phenomenon interest coexists auxiliary types phenomena consider maximizing longer equivalent minimizing entropy terms necessarily constant. exploiting sparse cmogp model structure. derive equivalence result considering instead constant entropy conditioned inducing measurements used sparse cmogp regression models then using chain rule entropy structural property shared sparse cmogp regression models unifying framework described section proven equivalent conditional mutual information yvt\\xt given novel active learning criterion exhibits interesting exploration-exploitation trade-off inducing measurements viewed latent structure sparse cmogp model induce conditional independence properties hand maximizing term aims select tuples locations uncertain measurements target phenomenon corresponding type observed given latent model structure hand minimizing term aims select tuples observed rely less measurements yvt\\xt type remaining unobserved locations target phenomenon infer latent model structure since yvt\\xt won’t sampled. supposing |vt| evaluating active learning criterion incurs time every one-off cost time contrast computing original criterion requires time every costly especially number selected observations much less number |vt| candidate sampling locations target phenomenon example tight sampling budget large sampling domain usually occurs practice. trick achieving computational advantage inherited approximation algorithm described next. novel active learning criterion optimized still suffers poor scalability number selected observations like criterion involves evaluating prohibitively large number candidate selections sampling locations corresponding measurement types however unlike criterion possible devise efﬁcient approximation algorithm theoretical performance guarantee optimize criterion main contribution work paper. idea proposed approximation algorithm greedily select next tuple sampling location corresponding measurement type observed maximally increases criterion iterate till tuples selected observation. speciﬁcally h−i+i denote active learning criterion augmented positive constant make non-negative. additive constant simply technical necessity proving performance guarantee affect outcome optimal selection then approximation algorithm greedily selects next tuple sampling location corresponding measurement type maximizes xi∈v xi∈v hxi|yx δihxi|yx∪vt\\xt) kronecker delta value otherwise. derivation appendix algorithm updates iterates greedy selection update till intuition understanding algorithm choose observing sampling location uncertain measurement target phenomenon auxiliary type inducing largest reduction predictive uncertainty measurements remaining unobserved locations target phenomenon since hxi|yx hxi|yx∪vt\\xt) hxi}). also interesting ﬁgure whether approximation algorithm avoid selecting tuples certain auxiliary type formally analyze conditions elucidated following result assuming absence suppressor variables hxi|yx hxi|yx∪vt\\xt logx vt\\xt)) v-t\\x-t. proof relies following assumption absence suppressor variables holds many practical cases conditioning make correlated proposition reveals signal-to-noise ratio auxiliary type and/or cross correlation measurements target phenomenon auxiliary type small greedy criterion relow turns small value hence causing algorithm avoid selecting tuples auxiliary type theorem approximation algorithm incurs |vt|) time. proof appendix approximation algorithm incurs quartic time number selected observations cubic time number |vt| candidate sampling locations target phenomenon. performance guarantee. theoretically guarantee performance approximation algorithm ﬁrst motivate need deﬁne relaxed notion submodularity. submodular function exhibits natural diminishing returns property adding element input increment function value decreases larger input set. maximize nondecreasing submodular function work nemhauser wolsey fisher proposed greedy algorithm guaranteeing -factor approximation achieved optimal input set. main difﬁculty proving submodularity lies mutual information term conditioned works shown submodularity conditional mutual information imposing conditional independence assumptions practice strong assumptions xi|yvt\\xt x-t) severely violate correlation structure multiple types coexisting phenomena overkill correlation structure fact preserved fair extent relaxing assumptions consequently entails relaxed form submodularity performance guarantee similar nemhauser wolsey fisher derived approximation algorithm. deﬁnition function \u0001-submodular {a}) section evaluates predictive performance approximation algorithm empirically three real-world datasets jura dataset contains concentrations heavy metals collected locations swiss jura region; gilgai dataset contains electrical conductivity chloride content generated line transect survey locations gilgai territory south wales australia; indoor environmental quality dataset contains temperature light readings taken temperature sensors light sensors deployed intel berkeley research lab. sampling locations jura datasets shown appendix performance m-greedy compared maximum variance/entropy algorithm greedily selects next location corresponding measurement type maximum posterior variance/entropy iteration; greedy maximum entropy mutual information sampling algorithms gathering observations target phenomenon. experiments k-means used select inducing locations clustering possible locations available selected observation clusters cluster center corresponds element hyper-parameters mogp single-output models learned using data maximum likelihood estimation dataset observations type randomly selected form test tuples candidate sampling locations corresponding type therefore become less auxiliary types. root mean squared error metric µxt|x used evaluate performance tested algorithms. experimental results averaged random test sets. fair comparison measurements types normalized using training prediction active learning. jura dataset. three types correlated lg-cd lg-zn measurements used experiment; take measurements remove strong skewness proposed standard statistical practice measurement types smallest largest signal-to-noise ratios type submodular relaxes example conditional independence assumption allows practice expected small since conditioning monotonically decreases posterior variance expected large tuples remaining unobserved locations target phenomenon tends informative enough make σpitc small hence non-negative variance reduction term small. furthermore given small realized controlling discretization domain candidate sampling locations. example reﬁning discretization variance reduction term decreases shown submodular many practical cases. give another example lemma realize controlling discretization every pair selected observations sufﬁciently apart. easy derive information never hurts bound entropy entails nondecreasing hxi|yx δihxi|yx∪vt\\xt) hxi|yx hxi|yx∪vt\\xt) ﬁrst inn∗ hxi|ya) equality requires πeσpitcxixi|a reasonable practice ubiquitous noise. combining result lemma yields performance guarantee theorem given holds approximation algorithm guaranteed select s.t. similar well-known result nemhauser wolsey fisher except exploiting \u0001-submodularity lemma instead submodularity. finally present discretization scheme satisﬁes smallest discretization width construct tuples candidate sampling locations corresponding measurement types every pair tuples least distance apart candidate location thus corresponding type. construction constrains algorithm select observations sparsely across spatial domain sufﬁciently many neighboring tuples remaining unobserved locations target phenomenon keep σpitc small hence variance reduction term small. previous theoretical results still hold used instead result gives minimum value satisfy figs. show respectively results tested algorithms lg-cd type observed rmse m-greedy decreases rapidly m-var especially observations auxiliary types selected algorithm selects observations auxiliary types induce largest reduction predictive uncertainty measurements remaining unobserved locations target phenomenon contrast m-var select observations reduce predictive uncertainty auxiliary types phenomena directly achieve active learning problem. increasing m-greedy m-var reach smaller rmses mgreedy achieve faster much less observations. shown figs. m-greedy performs much better s-var s-mi means observations correlated auxiliary types indeed used improve prediction target phenomenon. finally comparing results between figs. rmse m-greedy type decreases faster lg-cd type especially beginning higher-quality measurements gilgai dataset. experiment lg-cl contents depth used jointly types target phenomena electrical conductivity easier measure depths used auxiliary type. fig. shows results average rmse lg-cl types similar results jura dataset types target phenomena rmse m-greedy still decreases rapidly increasing m-var achieves much smaller rmse s-var s-mi; results s-var s-mi also averaged independent single-output predictions lg-cl content depths. dataset. fig. shows results light type observations similar jura gilgai datasets rmse m-greedy decreases faster algorithms. importantly number observations m-greedy achieves much smaller rmse s-var s-mi sample target phenomenon. m-greedy selects observations auxiliary type less noisy demonstrates advantage s-var s-mi type measurements noisy figure graphs rmses observations lgcl types gilgai dataset light type dataset. existing works active learning multiple output measurement types driven mogp model formally characterized cross-correlation structure different types phenomena spatial sampling algorithms simply modeled auxiliary phenomenon noisy perturbation target phenomenon assumed latent differs work here. multi-task active learning active transfer learning algorithms considered prediction type phenomenon task used auxiliary tasks help learn target task. mtal algorithm zhang requires relations different classiﬁcation tasks manually speciﬁed highly non-trivial achieve practice applicable mogp regression. active learning algorithms used active learning criteria speciﬁc classiﬁcation recommendation tasks cannot readily tailored mogp regression. paper describes novel efﬁcient algorithm active learning mogp model. resolve issue poor scalability optimizing conventional entropy criterion exploit structure common unifying framework sparse mogp models deriving novel active learning criterion then exploit \u0001-submodularity property criterion devising polynomialtime approximation algorithm guarantees constantfactor approximation achieved optimal selected observations empirical evaluation three real-world datasets shows approximation algorithm m-greedy outperforms existing algorithms active learning mogp single-output models especially measurements target phenomenon noisy auxiliary types. future work plan extend approach generalizing non-myopic active learning single-output mogps improving scalability data parallelization online learning stochastic variational inference acknowledgments. research carried", "year": 2015}