{"title": "Reversible Architectures for Arbitrarily Deep Residual Neural Networks", "tag": ["cs.CV", "stat.ML"], "abstract": "Recently, deep residual networks have been successfully applied in many computer vision and natural language processing tasks, pushing the state-of-the-art performance with deeper and wider architectures. In this work, we interpret deep residual networks as ordinary differential equations (ODEs), which have long been studied in mathematics and physics with rich theoretical and empirical success. From this interpretation, we develop a theoretical framework on stability and reversibility of deep neural networks, and derive three reversible neural network architectures that can go arbitrarily deep in theory. The reversibility property allows a memory-efficient implementation, which does not need to store the activations for most hidden layers. Together with the stability of our architectures, this enables training deeper networks using only modest computational resources. We provide both theoretical analyses and empirical results. Experimental results demonstrate the efficacy of our architectures against several strong baselines on CIFAR-10, CIFAR-100 and STL-10 with superior or on-par state-of-the-art performance. Furthermore, we show our architectures yield superior results when trained using fewer training data.", "text": "recently deep residual networks successfully applied many computer vision natural language processing tasks pushing state-of-the-art performance deeper wider architectures. work interpret deep residual networks ordinary differential equations long studied mathematics physics rich theoretical empirical success. interpretation develop theoretical framework stability reversibility deep neural networks derive three reversible neural network architectures arbitrarily deep theory. reversibility property allows memoryefﬁcient implementation need store activations hidden layers. together stability architectures enables training deeper networks using modest computational resources. provide theoretical analyses empirical results. experimental results demonstrate efﬁcacy architectures several strong baselines cifar- cifar- stl- superior on-par state-of-the-art performance. furthermore show architectures yield superior results trained using fewer training data. deep learning powers many research areas impacts various aspects society computer vision natural language processing biology e-commerce. recent progress designing architectures deep networks accelerated trend among successful architectures deep residual network variants widely used many computer vision applications natural language processing tasks however still theoretical analyses guidelines designing training resnet. contrast recent interest deep residual networks system ordinary differential equations special kinds dynamical systems long studied mathematics physics rich theoretical empirical copyright association advancement artiﬁcial intelligence rights reserved. success connection nonlinear odes deep resnets established recent works continuous interpretation resnets dynamical systems allows adaption existing theory numerical techniques odes deep learning. example paper introduces concept stable networks arbitrarily long. however deep networks simple single-layer convolution building blocks proposed architectures reversible simple numerical examples provided. work aims overcoming drawbacks investigates efﬁcacy practicability stable architectures derived dynamical systems perspective. work connect deep resnets odes closely propose three stable reversible architectures. show three architectures governed stable well-posed odes. particular approach allows train arbitrarily long networks using minimal memory storage. illustrate intrinsic reversibility architectures theoretical analysis empirical results. reversibility property easily leads memoryefﬁcient implementation need store activations hidden layers. together stability allows train almost arbitrarily deep architectures using modest computational resources. remainder paper organized follows. discuss related work sec. sec. review notion reversibility stability resnets present three architectures regularization functional. sec. show efﬁcacy networks using three common classiﬁcation benchmarks architectures achieve comparable even superior accuracy particular generalize better limited number labeled training data used. sec. conclude paper. thus problem learning network parameters equivalent solving parameter estimation problem optimal control problem involving system cases interpreted system partial differential equations problems rich theoretical computational framework including techniques guarantee stable networks using appropriate functions discretization forward propagation process theoretical frameworks optimization parameters methods computing gradient solution respect reversible architectures reversible numerical methods dynamical systems allow simulation dynamic going ﬁnal time initial time vice versa. reversible numerical methods commonly used context hyperbolic pdes various methods proposed compared theoretical framework reversible methods strongly tied issues stability. fact show here every method algebraically reversible numerically stable. strong implication practical applicability reversible methods deep neural networks. recently various reversible neural networks proposed different purposes based different architectures. recent work inverts feed-forward reproduces input features values ﬁnal layers. suggests deep neural networks reversible generative model reverse feed-forward provide theoretical connection model-based compressive sensing cnns. nice uses invertible non-linear transformation data distribution latent space resulting distribution factorizes yielding good generative models. besides implications reversibility deep generative models property used developing memory-efﬁcient algorithms. instance revnet inspired nice develops variant resnet layer’s activations reconstructed next layer’s. allows avoid storing activations hidden layers except layers stride larger one. show later physicallyinspired network architectures also reversible property derive memory-efﬁcient implementations. introduce three reversible architectures deep neural networks discuss stability. capitalize link resnets odes guarantee stability forward propagation process well-posedness learning problem. finally present regularization functionals favor smooth time dynamics. figure hamiltonian reversible block. first input feature equally channel-wise split operations described performed resulting zj+. finally concatenated output block. residual neural networks extensions resnets deep neural networks obtained stacking simple residual blocks simple residual network block written here values features layer layer’s network parameters. goal training learn network parameters represents discrete dynamical system. early review neural networks dynamical systems presented resnets broadly applied many domains including computer vision tasks image recognition object detection semantic segmentation visual reasoning natural language processing tasks speech synthesis speech recognition machine translation besides broadening application domain resnet successors focus improving accuracy stability saving memory accelerating training process instance resnxt introduces homogeneous multi-branch architecture increase accuracy. stochastic depth reduces training time increases accuracy randomly dropping subset layers bypassing identity function. interprets resnet discretization differential equation whose parameters learned training process. process forward propagation viewed simulating nonlinear dynamics take initial data hard classify moves ﬁnal state classiﬁed easily using e.g. linear classiﬁer. fundamental question needs addressed conditions forward propagation well-posed? question important main reasons. first instability forward propagation means solution highly sensitive data perturbation given computations done single precision cause serious artifacts instabilities ﬁnal results. second training unstable networks difﬁcult practice although impossible prove instability many local minima. ﬁrst review issue stability. dynamical system stable small change input data leads small change ﬁnal result. better characterize this assume small perturbation initial data assume change propagated throughnetwork. question would change time change characterized lyapunov exponent measures difference trajectories nonlinear dynamical system given initial conditions. lyapunov exponent deﬁned exponent measures difference observation allows generate networks guaranteed stable. emphasized stability forward propagation necessary obtain stable networks generalize well sufﬁcient. fact real parts eigenvalues negative large shows differences input features decay exponentially time. complicates learning problem therefore consider architectures lead jacobians purely imaginary eigenvalues. discuss three networks inspired different physical interpretations. partitions features activation function network parameters convolutional neural networks convolution operator convolution transpose operator respectively. shown jacobian matrix satisﬁes condition thus stable well-posed. authors also demonstrate performance small dataset. however numerical experiments found representability one-layer architecture limited. according universal approximation theorem two-layer neural network approximate monotonically-increasing continuous function compact set. recent work shows simple two-layer neural networks already perfect ﬁnite sample expressivity soon number parameters exceeds number data points. therefore propose extend following two-layer structure principle linear operator used within hamiltonian framework. however since numerical experiments consider images choose convolution operator transpose. rewriting matrix form gives different ways partitioning input features including checkerboard partition channel-wise partition work equal channel-wise partition ﬁrst half channels input second half diag) derivative activation function. eigenvalues imaginary therefore satisﬁed forward propagation neural network stable well-posed. commonly used discretization technique hamiltonian systems verlet method reads choose hamiltonian blocks illustrate fig. similar resnet hamiltonian reversible network built ﬁrst concatenating blocks units concatenating units network. illustration architecture provided fig. gives following forward propagation obtained forward euler step. guarantee stability single layer function contain anti-symmetric linear operator next straightforward show midpoint method reversible however possible potentially double layer midpoint network difﬁcult ensure stability network. explore leapfrog network next. leapfrog network stable leapfrog network seen special case hamiltonian network kernels identity matrix activation identity function. leapfrog network involves derivatives time reads σy+b) discretized example using conservative leapfrog discretization uses following symmetric approximation second derivative time reversible architectures stability architecture called reversible allows reconstruction activations going beginning. reversible numerical methods odes studied context hyperbolic differential equations reversibility discovered recently machine learning community reversible techniques enable memory-efﬁcient implementations network requires storage last activations only. ﬁrst demonstrate reversibility leapfrog network. assume given last states yn−. then using straight-forward compute revnet midpoint represent reversible networks algebraically reversible practice without restrictions residual functions. illustrate consider simple linear case revnet simple case reads evaluate methods three standard classiﬁcation benchmarks compare state-of-the-art results literature. furthermore investigate robustness method amount training data decrease train deep network layers. datasets baselines cifar- cifar- cifar- dataset consists training images testing images classes image resolution. cifar- dataset uses image data train-test split cifar- classes. common data augmentation techniques including padding four zeros around image random cropping random horizontal ﬂipping image standardization. state-of-the-art methods resnet revnet used baseline methods. stl- stl- dataset image recognition dataset classes image resolutions contains training images test images. thus compared cifar- class fewer labeled training samples higher image resolution. used data augmentation cifar- except padding zeros around images. neural network architecture speciﬁcations provide neural network architecture speciﬁcations here. implementation details appendix. networks contain units unit blocks. also convolution layer beginning network fully connected layer end. hamiltonian networks convolution layers block total number layers midpoint leapfrog convolution layers block total number layers ﬁrst block unit feature size halved number ﬁlters doubled. perform downsampling average pooling increase number ﬁlters padding zeros. obvious stable every choice indeed example positive solution grow every layer exhibiting unstable behavior. possible obtain stable solutions sufﬁciently small. role hamiltonian network. analysis plays role reversibility. unstable networks either forward backward propagation consists exponentially growing mode. computation single precision gradient grossly inaccurate. thus every choice functions lead reasonable network practice control needed network grow exponentially neither forward backwards. arbitrarily deep residual neural networks three architectures proposed used arbitrary depth since dissipation. implies signal input system decay even arbitrarily long networks. thus signals propagate system inﬁnite network depth. also experimented slightly dissipative networks networks attenuate signal layer yielded results comparable ones obtained networks proposed here. regularization regularization plays vital role serving parameter tuning deep neural network training help improve generalization performance besides commonly used weight decay also weight smoothness decay. since interpret forward propagation hamiltonian network time-dependent nonlinear dynamic process prefer convolution weights smooth time using regularization functional table main results different architectures cifar- cifar-. compare three dynamical system inspired neural networks state-of-the-art methods resnet revnet please note revnet three architectures much memory-efﬁcient resnet. main results analysis cifar- cifar- show main results different architectures cifar-/ table three architectures achieve comparable performance resnet revnet term accuracy using similar number model parameters. compared resnet architectures memory efﬁcient reversible thus need store activations layers. compared revnet models reversible also stable theoretically proved sec. show later stable property makes models robust small amount training data arbitrarily deep. stl- main results stl- shown table compared state-of-the-art results architectures achieve better accuracy. robustness training data subsampling sometimes labeled data expensive obtain. thus desirable design architectures generalize well trained examples. verify intuition stable architectures generalize well conducted extensive numerical experiments using cifar- datasets decreasing number training data. focus behavior neural network architecture face data subsampling instead improving state-of-the-art results. therefore intentionally simple architectures blocks units number ﬁlters comparison resnet baseline. cifar- much training data stl- randomly subsample training data cifar- stl-. test data remains unchanged. cifar- fig. shows result cifar- decreasing number examples training data hamiltonian network performs consistently better terms accuracy resnet achieving higher accuracy trained using original training data. stl- result shown fig. hamiltonian consistently achieves better accuracy resnet average improvement around training -layer hamiltonian demonstrate stability memory-efﬁciency hamiltonian network arbitrary depth explore -layer architecture cifar-. aggressively deep resnet also trained cifar- layers accuracy result shown last table compared original resnet architecture uses half parameters obtains better accuracy. since hamiltonian network intrinsically stable guaranteed issue exploding vanishing gradient. easily train arbitrarily deep hamiltonian network without difﬁculty optimization. implementation reversible architecture memory efﬁcient enables layer hamiltonian model running single machine memory. present three stable reversible architectures connect stable deep residual neural networks yield well-posed learning problems. exploit intrinsic reversibility property obtain memory-efﬁcient implementation need store activations hidden layers. together stability forward propagation allows training deeper architectures limited computational resources. evaluate methods three publicly available datasets several state-of-the-art methods. experimental results demonstrate efﬁcacy method superior on-par state-of-the-art performance. moreover small amount training data architectures achieve better accuracy compared widely used state-of-the-art resnet. attribute robustness small amount training data intrinsic stability hamiltonian neural network architecture. implementation details method implemented using tensorflow library cifar-/ stl- experiments evaluated desktop intel quad-core single nvidia gpu. cifar- cifar- experiments ﬁxed mini-batch size training test data except hamiltonian- uses batch-size learning rate initialized decayed factor training epochs. total training step weight decay constant weight smoothness decay momentum stl- experiments mini-batch size learning rate initialized decayed factor training epochs. total training step weight decay constant weight smoothness decay momentum", "year": 2017}