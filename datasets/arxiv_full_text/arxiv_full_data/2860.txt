{"title": "Recurrent Predictive State Policy Networks", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We introduce Recurrent Predictive State Policy (RPSP) networks, a recurrent architecture that brings insights from predictive state representations to reinforcement learning in partially observable environments. Predictive state policy networks consist of a recursive filter, which keeps track of a belief about the state of the environment, and a reactive policy that directly maps beliefs to actions, to maximize the cumulative reward. The recursive filter leverages predictive state representations (PSRs) (Rosencrantz and Gordon, 2004; Sun et al., 2016) by modeling predictive state-- a prediction of the distribution of future observations conditioned on history and future actions. This representation gives rise to a rich class of statistically consistent algorithms (Hefny et al., 2018) to initialize the recursive filter. Predictive state serves as an equivalent representation of a belief state. Therefore, the policy component of the RPSP-network can be purely reactive, simplifying training while still allowing optimal behaviour. Moreover, we use the PSR interpretation during training as well, by incorporating prediction error in the loss function. The entire network (recursive filter and reactive policy) is still differentiable and can be trained using gradient based methods. We optimize our policy using a combination of policy gradient based on rewards (Williams, 1992) and gradient descent based on prediction error. We show the efficacy of RPSP-networks under partial observability on a set of robotic control tasks from OpenAI Gym. We empirically show that RPSP-networks perform well compared with memory-preserving networks such as GRUs, as well as finite memory models, being the overall best performing method.", "text": "recently signiﬁcant progress deep reinforcement learning deep reinforcement learning combines deep networks high level representation policy reinforcement learning optimization method allows end-to-end training. traditional applications deep learning rely standard architectures sigmoid activations rectiﬁed linear units emerging trend using composite architectures contain parts explicitly resembling algorithms kalman ﬁltering value iteration shown architectures outperform standard neural networks. work focus partially observable environments agent full access state environment partial observations thereof. agent maintain instead distribution states i.e. belief state based entire history observations actions. standard approach problem employ recurrent architectures long-short-term-memory gated recurrent units however recurrent models difﬁcult train non-convexity hidden states lack statistical meaning hard interpret. models based predictive state representations offer alternative method construct surrogate belief state partially observable environment. models represent state expectation sufﬁcient statistics future observations conditioned history future actions. predictive state models admit efﬁcient learning algorithms theoretical guarantees. moreover successive application predictive state update procedure results recursive computation graph differentiable respect model parameters. therefore treat predictive state models recurrent networks apply backpropagation introduce recurrent predictive state policy networks recurrent architecture brings insights predictive state representations reinforcement learning partially observable environments. predictive state policy networks consist recursive ﬁlter keeps track belief state environment reactive policy directly maps beliefs actions maximize cumulative reward. recursive ﬁlter leverages predictive state representations modeling predictive state prediction distribution future observations conditioned history future actions. representation gives rise rich class statistically consistent algorithms initialize recursive ﬁlter. predictive state serves equivalent representation belief state. therefore policy component rpsp-network purely reactive simplifying training still allowing optimal behaviour. moreover interpretation training well incorporating prediction error loss function. entire network still differentiable trained using gradient based methods. optimize policy using combination policy gradient based rewards gradient descent based prediction error. show efﬁcacy rpsp-networks partial observability robotic control tasks openai gym. empirically show rpsp-networks perform well compared memory-preserving networks grus well ﬁnite memory models overall *equal contribution machine learning department carnegie mellon university pittsburgh robotics institute carnegie mellon university pittsburgh isr/it instituto superior t´ecnico lisbon portugal paul allen school computer science engineering university washington seattle usa. correspondence ahmed hefny <ahefnycs.cmu.edu> zita marinho <zmarinhocmu.edu>. time optimize model parameters. insight construct recurrent predictive state policy network special recurrent architecture consists predictive state model acting recursive ﬁlter keep track predictive state feed-forward neural network directly maps predictive states actions. conﬁguration results recurrent policy recurrent part implemented instead lstm gru. predictive states sufﬁcient summary history observations actions reactive policy rich enough information make decisions access true belief state. number motivations architecture recursive ﬁlter rpsp-networks fully differentiable meaning good initialization obtained spectral learning methods reﬁne rpsp-nets using gradient descent. network trained end-to-end example using policy gradients reinforcement learning setting supervised learning imitation learning setting work focus former. discuss predictive state model component control component presented learning algorithm presented describe experimental setup results control tasks evaluate performance reinforcement learning using predictive state policy networks multiple partially observable environments continuous observations actions. assume agent interacting environment episodes episode consists time steps agent takes action observes observation reward agent chooses actions based stochastic policy parameterized parameter vector would like improve major approaches policy modeling optimization. ﬁrst value function-based approach seek learn function evaluate value action state optimal policy given function agent greedily based estimated values. second approach direct policy optimization learn function directly predict optimal actions function optimized maximize using policy gradient methods derivative-free methods focus direct policy optimization approach robust noisy continuous environments modeling uncertainty provide class policy functions combines recurrent reinforcement learning recent advances modeling partially observable environments using predictive state representations previous attempts combine predictive state models policy learning. boots proposed method planning partially observable environments. method ﬁrst learns trajectories collected using explorative blind policy. predictive states estimated considered states fully observable markov decision process. value function learned states using least squares temporal difference point-based value iteration main disadvantage approaches assumes one-time initialization propose mechanism update model based subsequent experience. hamilton proposed iterative method simultaneously learn predictive states q-function. azizzadenesheli proposed tensor decomposition method estimate parameters discrete partially observable markov decision process common limitation aforementioned methods restricted discrete actions also shown psrs beneﬁt greatly local optimization moment-based initialization venkatraman proposed predictive state decoders lstm network trained mixed objective function order obtain high cumulative rewards accurately predicting future observations. proposed rpsp networks alleviate limitations previous approaches supports continuous observations actions uses recurrent state tracker consistent initialization supports end-to-end training initialization. section give brief introduction predictive state representations constitute state tracking component model discuss relationship recurrent neural networks rnns. given history observations actions typical computes hidden state using recursive update equation given state predict observations function latent function connects states output unknown learned. case output could predicted observations used prediction figure predictive state models deﬁne similar recursive state update. however state speciﬁc interpretation encodes conditional distribution future observations ott+k− conditioned future actions att+k−. denote representation state predictive state. main characteristic predictive state deﬁned entirely terms observable quantities. mapping predictive state prediction given implied choice features. suitable features mapping fully known simple learn consistently. contrast general mapping unknown typically requires non-convex optimization learned. characteristic allows efﬁcient consistent learning models predictive states reduction supervised learning follow predictive state controlled model formulation hefny however alternative methods predictive state inference machines could contemplated. length-k depends observability system. system k-observable maintaining predictive state equivalent maintaining distribution system’s latent state. state extension linear wext applied obtain extended state state deﬁnes conditional distribution extended window observations actions. wext parameter learned. figure depicts state update. conditioning function fcond depends representation example discrete system could represent conditional probability tables fcond amounts applying bayes rule. extension continuous systems achieved using hilbert space embedding distributions fcond uses kernel bayes rule work rffpsr model proposed observation action features based random fourier features kernel projected lower dimensional subspace using randomized denote feature function. conditioning function fcond kernel bayes rule observation function linear learning psrs carried steps initialization procedure using method moments local optimization procedure using gradient descent. initialization initialization procedure exploits fact represented terms observable quantities since wext linear using denotes features extracted previous observations actions hidden states estimating expectations sides done solving supervised regression subproblem. given predictions regression solving wext becomes another linear regression problem. here follow two-stage regression proposed hefny wext computed perform ﬁltering obtain predictive states estimated states learn mapping predicted observations wpred results another regression subproblem section detailed derivation. rffpsr linear regression subproblems ensures two-stage regression procedure free local optima. local optimization although initialization procedure consistent based method moments hence necessarily statistically efﬁcient. therefore beneﬁt local optimization. downey hefny note deﬁnes recursive computation graph similar introduce proposed class policies recurrent predictive state policies section formally describe components describe policy learning algorithm rpsps consist fundamental components state tracking component models state system able predict future observations; reactive policy component maps states actions shown figure state tracking component based formulation described reactive policy consider stochastic non-linear policy maps predictive state distribution actions parametrized θre. similar schulman assume gaussian distribution parameters scheme. hyper-parameters determine importance expected return prediction error respectively. discussed detail noting rpsp special type recurrent network policy possible adapt policy gradient methods joint loss following subsections propose different update variants. minimize -step prediction error opposed general k-future prediction error recommended avoid biased estimates induced causal statistical correlations performing on-policy updates non-blind policy use. figure rpsp network predictive state updated linear extension wext followed non-linear conditioning fcond. linear predictor wpred used predict observations used regularize training loss feed-forward reactive policy maps predictive states distribution actions. rpsp thus stochastic recurrent policy recurrent part corresponding psr. parameters consist parts parameters θpsr wext wpred} reactive policy parameters following section describe parameters learned. detailed algorithm learning rpsp performed phases. ﬁrst phase execute exploration policy collect dataset used initialize described worth noting initialization procedure depends observations rather rewards. particularly useful environments informative reward signals infrequent. second phase starting initial random reactive policy iteratively collect trajectories using current policy update parameters reactive policy predictive model θpsr wext wpred} depicted algorithm distribution trajectories induced policy updating parameters seek difﬁcult make sense values specially gradient magnitudes respective losses comparable. reason propose principled approach ﬁnding relative weights. user-given value dynamically adjusted maintain property gradient loss weighted unit variance maintain variance gradient loss exponential averaging adjust weights. evaluate rpsp-network’s performance collection reinforcement learning tasks using openai mujoco environments. default environment settings. consider partially observable environments angles joints agent visible network without velocities. proposed models consider rpsp predictive component based rffpsr described rffpsr random fourier features observation action sequences followed dimensionality reduction step dimensions. report results best choice initialize rpsp stage regression batch initial trajectories experiment joint vrpg optimization described alternating optimization rpspvrpg gradient normalization described additionally consider extended variation concatenates predictive state window previous observations extended form predictive state learning succeeded perfectly extra information would unnecessary; however observe practice including observations help model learn faster stably. later results section report rpsp variant performs best. provide detailed comparison models appendix. competing models compare models ﬁnite memory model gated recurrent units ﬁnite memory models analogous rpsp replace replace cumulative trajectory reward γj−trj computing cumulative reward starting reduce variance baseline estimates expected reward-to-go conditioned current policy. implementation assume parameter vector estimated using linear regression. given batch trajectories stochastic gradient obtained replacing expectation empirical expectation trajectories batch. stochastic gradient prediction error obtained using backpropagation time estimate gradients compute update parameters trough gradient descent algorithm appendix. section describe method utilizes recently proposed trust region policy optimization alternative vanilla policy gradient methods shown superior performance practice uses natural gradient update enforces constraint encourages small changes policy trpo step. constraint results smoother changes policy parameters. policy induced reward-to-go baseline functions deﬁned possible extend trpo joint loss observed trpo tends computationally intensive recurrent architectures. instead resort following alternating optimization iteration trpo update reactive policy parameters involve feedforward network. then gradient step described update parameters θpsr algorithm appendix. figure empirical average return epochs finite memory model grus best performing rpsp joint optimization best performing rpsp alternate optimization four environments. rpsp variations ﬁxed parameters without prediction regularization random initialization comparison prediction regularization rpsp graphs shifted right reﬂect extra trajectories initialization. cumulative rewards model linear baseline variance reduction state model used predictor variable. evaluation setup algorithm number iterations based environment after iteration compute average return riter batch trajectories predictive state window past observations. tried three variants window size respectively compare grus -dimensional hidden states. optimize network parameters using rllab implementation trpo different learning rates figure empirical average return trials batch trajectories time steps hopper. robustness observation gaussian noise best rpsp alternate loss finite memory model environment number samples batch maximum length episode cart-pole swimmer hopper walkerd respectively. rpsp found step size performs well vrpg alternating optimization environments. reactive policy contains hidden layer nodes relu activation. models report results choice hyper-parameters resulted highest mean cumulative reward results discussion performance iterations figure shows empirical average return amount interaction environment measured time steps. rpsp networks plots shifted account initial trajectories used initialize rpsp ﬁltering layer. amount shifting equivalent trajectories. observe rpsp networks perform well every environment competing outperforming model terms learning speed ﬁnal reward exception cart-pole larger. report cumulative reward environments table except cart-pole come variant rpsp best performing model. swimmer best performing model statistically better model hopper best rpsp model performs statistically better models walkerd rpsp outperforms baselines cart-pole rpsp model performs better model statistically signiﬁcantly different model. also note rpsp-alt provides similar performance joint optimization converges faster. effect proposed contributions rpsp model based number components state tracking using consistent initialization using two-stage regression end-to-end training state tracker policy using observation prediction loss regularize training. conducted experiments verify beneﬁt component. ﬁrst experiment test three variants rpsp randomly initialized another ﬁxed initial value third train rpsp network without prediction loss regularization figure demonstrates variants inferior model showing importance two-stage initialization end-to-end training observation prediction loss respectively. second experiment replace initialized using backpropagation time. analogous predictive state decoders proposed observation prediction loss included optimizing policy network figure shows model inferior model initialization procedure consistent suffer local optima. effect observation noise also investigated effect propose rpsp-networks combining ideas predictive state representations recurrent networks reinforcement learning. learning algorithms provide statistically consistent initialization state tracking component propose gradient-based methods maximize expected return reducing prediction error. references azizzadenesheli kamyar lazaric alessandro anandkumar animashree. reinforcement learning pomdp’s using spectral methods. corr abs/. http//arxiv.org/abs/.. bojarski mariusz testa davide dworakowski daniel firner bernhard flepp beat goyal prasoon jackel lawrence monfort mathew muller zhang jiakai zhang zhao jake zieba karol. learning self-driving cars. corr abs/. boots byron gordon geoffrey predictive state advances neural temporal difference learning. information processing systems annual conference neural information processing systems boots byron gretton arthur gordon geoffrey hilbert space embeddings predictive state representations. proc. intl. conf. uncertainty artiﬁcial intelligence downey carlton hefny ahmed boots byron gordon geoffrey boyue. predictive state recurrent neural networks. guyon luxburg bengio wallach fergus vishwanathan garnett advances neural information processing systems curran associates inc. duan chen houthooft rein schulman john abbeel pieter. benchmarking deep reinforcement learning continuous control. proceedings international conference international conference machine learning volume icml’ greensmith evan bartlett peter baxter jonathan. variance reduction techniques gradient estimates reinforcement learning. proceedings international conference neural information processing systems natural synthetic nips’ cambridge press. haarnoja tuomas ajay anurag levine sergey abbeel pieter. backprop learning discriminative deterministic state estimators. sugiyama luxburg guyon garnett advances neural information processing systems curran associates inc. hefny ahmed downey carlton gordon geoffrey supervised learning dynamical system learning. cortes lawrence sugiyama garnett advances neural information processing systems kyunghyun merrienboer bart g¨ulc¸ehre aglar bahdanau dzmitry bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder-decoder statistical machine transsutton mcallester singh mansour policy gradient methods reinforcement learning function approximation. advances neural information processing systems press tamar aviv levine sergey abbeel pieter thomas garrett. value iteration networks. advances neural information processing systems annual conference neural information processing systems december barcelona spain venkatraman arun rhinehart nicholas pinto lerrel boots byron kitani kris bagnell. james andrew. predictive state decoders encoding proceedings adfuture recurrent networks. vances neural information processing systems mnih volodymyr kavukcuoglu koray silver david graves alex antonoglou ioannis wierstra daan riedmiller martin. playing atari deep reinforcement learning http//arxiv.org/abs/ cite arxiv.comment nips deep learning workshop ross st´ephane gordon geoffrey bagnell drew. reduction imitation learning structured prediction no-regret online learning. proceedings fourteenth international conference artiﬁcial intelligence statistics aistats fort lauderdale april schulman john levine sergey abbeel pieter jordan michael moritz philipp. trust region policy optimization. blei david bach francis proceedings international conference machine learning jmlr workshop conference proceedings silver david huang maddison chris guez arthur sifre laurent driessche george schrittwieser julian antonoglou ioannis panneershelvam veda lanctot marc dieleman sander grewe dominik nham john kalchbrenner sutskever ilya lillicrap timothy leach madeleine kavukcuoglu koray graepel thore hassabis demis. mastering game deep neural networks tree search. nature january venkatraman arun boots byron bagnell andrew. learning ﬁlter predictive state inference machines. proceedings international conference machine learning icml york city june stage- regression rffpsrs derived either joint regression action/observation pairs using solving regularized least squares problem full derivation refer hefny second step provided ﬁltering function tracks predictive states time clarity provide pseudo-code joint alternating update steps deﬁned step algorithm section show joint vrpg update step algorithm alternating update algorithm rpsp-networks rnns deﬁne recursive models able retain information previously observed inputs. bptt learning predictive states psrs bears many similarities bptt training hidden states lstms grus. cases state updated series alternate linear non-linear transformations. predictive states linear transformation wext represents system prediction expectations futures expectations extended features section describe process initializing state tracking part rpsp-networks. derive state ﬁlter equation rffpsr representation using -stage regression complete derivation refer hefny received environment time together compose entire history time {at− ot−}. consider future sequence consecutive k-observations ott+k− deﬁne feature mappings shown figure immediate future extended future actions observations assume future expectations linear transformation extended future expectations figure temporal correlation expectations cannot directly learn mapping wext empirical estimates using linear regression since noise also correlated. alternatively turn instrumental variable regression history used instrument since correlated observables noise. predictive extended states ﬁrst computing possibly non-linear regression histories predictive extended statistics hilbert space embedding non-linear regression computed kernel bayes rule subsequently linearly regress denoised extended state denoised predictive state using least squares approach ease explanation partition extended states parts derived skipped future observations prediction future observations could trained based interpretation. reason rpsps statistically driven form initialization obtained using moment matching technique good theoretical guarantees. contrast rnns exploit heuristics improved initialization however deﬁning best strategy guarantee good performance. paper proposed initialization provides guarantees inﬁnite data assumption section investigate effect using different variants rpsp networks test random initialization predictive layer provide experimental evidence baselines. rpsp optimizers next compare several rpsp variants environments. test rpsp variants joint alternate loss predictive states augmented predictive states ﬁrst variant standard vanilla rpsp second variant rpsp augmented state representation concatenate previous window observations predictive state provide complete comparison rpsp models using augmented states observations environments figure compare joint optimization alternating approach extended predictive states window observations provide better results particular joint optimization. extension might mitigate prediction errors improving information carried ﬁltering states. finite memory models next present ﬁnite memory models used baselines figure shows ﬁnite memory models three different window sizes environments. report main comparison best environment baselines section report results obtained grus using best learning rate figure shows results using different number hidden units environments. non-linear transformation place usual activation functions replaced fcond conditions current action observation update expectation future statistics worth noting transformations represent non-linear state updates rnns form update deﬁned choice representation state. hilbert space embeddings corresponds conditioning using kernel bayes rule additional source linearity representation itself. consider linear transformations wpred wext refer transformations kernel representations hilbert space embeddings. psrs also deﬁne computation graphs parameters optimized leveraging states system. predictive states leverage history like lstms/grus psrs also memory since learn track reproducing kernel hilbert space space distributions future observations based past histories. psrs provide additional beneﬁt clear interpretation figure predictive ﬁlter regularization effect walkerd cartpole swimmer environments. rpsp predictive regularization rpsp ﬁxed ﬁlter parameters rpsp without predictive regularization loss", "year": 2018}