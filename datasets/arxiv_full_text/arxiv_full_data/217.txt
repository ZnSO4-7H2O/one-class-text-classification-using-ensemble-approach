{"title": "Towards Bayesian Deep Learning: A Framework and Some Existing Methods", "tag": ["stat.ML", "cs.CV", "cs.LG", "cs.NE"], "abstract": "While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, the subsequent tasks that involve inference, reasoning and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This paper proposes a general framework for Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this paper, we also discuss the relationship and differences between Bayesian deep learning and other related topics like Bayesian treatment of neural networks.", "text": "abstract—while perception tasks visual object recognition text understanding play important role human intelligence subsequent tasks involve inference reasoning planning require even higher level intelligence. past years seen major advances many perception tasks using deep learning models. higher-level inference however probabilistic graphical models bayesian nature still powerful ﬂexible. achieve integrated intelligence involves perception inference naturally desirable tightly integrate deep learning bayesian models within principled probabilistic framework call bayesian deep learning. uniﬁed framework perception text images using deep learning boost performance higher-level inference return feedback inference process able enhance perception text images. paper proposes general framework bayesian deep learning reviews recent applications recommender systems topic models control. paper also discuss relationship differences bayesian deep learning related topics bayesian treatment neural networks. many perception tasks including seeing hearing recognition) undoubtedly fundamental tasks functioning comprehensive artiﬁcial intelligence data engineering system. however order build real ai/de system simply able read hear enough. should possess ability think. take medical diagnosis example. besides seeing visible symptoms hearing descriptions patients doctor look relations among symptoms preferably infer corresponding etiology. doctor provide medical advice patients. example although abilities seeing hearing allow doctor acquire information patients thinking part deﬁnes doctor. speciﬁcally ability think could involve causal inference logic deduction dealing uncertainty apparently beyond capability conventional deep learning methods. fortunately another type models probabilistic graphical models excels causal inference dealing uncertainty. problem good deep learning models perception tasks. address problem therefore natural choice tightly integrate deep learning within principled probabilistic framework call bayesian deep learning paper. wang department computer science engineering hong kong university science technology. e-mail hwangazcse.ust.hk dit-yan yeung department computer science engineering hong kong university science technology. e-mail dyyeungcse.ust.hk tight principled integration perception tasks inference tasks regarded whole beneﬁt other. example above able medical image could help doctor’s diagnosis inference. hand diagnosis inference return help understanding medical image. suppose doctor sure dark spot medical image however able infer etiology symptoms disease help better decide whether dark spot tumor not. achieve high accuracy recommender systems need fully understand content items analyze proﬁle preferences users evaluate similarity among users deep learning good ﬁrst subtask excels two. besides fact better understanding item content would help analysis user proﬁles estimated similarity among users could also provide valuable information understanding item content return. order fully utilize bidirectional effect boost recommendation accuracy might wish unify deep learning single principled probabilistic framework seen besides recommender systems need also arise dealing control non-linear dynamic systems images input. consider controlling complex dynamical system according live video stream received camera. problem transformed iteratively performing tasks perception images control based dynamic models. perception task taken care using multiple layers simple nonlinear transformation control task usually needs sophisticated models like hidden markov models kalman ﬁlters feedback loop completed course challenges applying real-world tasks. first nontrivial design efﬁcient bayesian formulation neural networks reasonable time complexity. line work pioneered widely adopted lack scalability. fortunately recent advances direction seem shed light practical adoption bayesian neural networks. second challenge ensure efﬁcient effective information exchange perception component task-speciﬁc component. ideally ﬁrst-order second-order information able back forth components. natural represent perception component seamlessly connect task-speciﬁc done paper give comprehensive overview models applications like recommender systems topic models control. rest paper organized follows section provide review basic deep learning models. section covers main concepts techniques pgm. sections serve background next section section proposes uniﬁed framework surveys models applied areas recommender systems topic models. section discusses future research issues concludes paper. deep learning deep learning normally refers neural networks layers. better understand deep learning start simplest type neural networks multilayer perceptrons example show conventional deep learning works. that review several types deep learning models based mlp. multilayer perceptron essentially multilayer perceptron sequence parametric nonlinear transformations. suppose want train multilayer perceptron perform regression task maps vector dimensions vector dimensions. denote input matrix j-th denoted m-dimensional vector representing data point. target denoted similarly denotes d-dimensional vector. problem learning l-layer multilayer perceptron formulated following optimization problem refer bayesian treatment neural networks bayesian neural networks. term bayesian deep learning retained refer complex bayesian models perception component task-speciﬁc component. fact actions chosen control model affect received video stream return. enable effective iterative process perception task control task need two-way information exchange them. perception component would basis control component estimates states control component built-in dynamic model would able predict future trajectory cases suitable choice mentioned examples above particularly useful tasks involve understanding content inference/reasoning among variables. complex tasks perception component responsible understanding models probabilistic relationship among different variables. furthermore interaction components creates synergy boosts performance. apart major advantage providing principled unifying deep learning another beneﬁt comes implicit regularization built bdl. imposing prior hidden units parameters deﬁning neural network model parameters specifying causal inference degree avoid overﬁtting especially sufﬁcient data. usually model consists components perception component bayesian formulation certain type neural networks task-speciﬁc component describes relationship among different hidden observed variables using pgm. regularization crucial both. neural networks usually large numbers free parameters need regularized properly. regularization techniques weight decay dropout shown effective improving performance neural networks bayesian interpretations terms task-speciﬁc component expert knowledge prior information kind regularization incorporated model prior imposed guide model data scarce. another advantage using complex tasks provides principled bayesian approach handling parameter uncertainty. applied complex tasks three kinds parameter uncertainties need taken account representing unknown parameters using distributions instead point estimates offers promising framework handle three kinds uncertainty uniﬁed way. worth noting third uncertainty could handled uniﬁed framework bdl. train perception component task-speciﬁc component separately equivalent assuming uncertainty exchanging information components. sdae feedforward neural network learning representations input data learning predict clean input output shown figure hidden layer middle i.e. ﬁgure constrained bottleneck learn compact representations. difference traditional sdae input layer corrupted version clean input data. essentially sdae solves following optimization problem sdae regarded multilayer perceptron regression tasks described previous section. input corrupted version data target clean version data example data matrix randomly entries nutshell sdae learns neural network takes noisy data input recovers clean data last layer. ‘denoising’ means. normally output middle layer i.e. figure would used compactly represent data. deep learning models commonly used deep learning models include convolutional neural networks apply convolution operators pooling operators process image video data recurrent neural networks recurrent computation imitate human memory restricted boltzmann machines undirected probabilistic neural networks binary hidden visible layers. note vast literature deep learning neural networks. introduction section intends serve background bdl. readers referred comprehensive survey details. probabilistic graphical models probabilistic graphical models diagrammatic representations relationships among them. similar graph contains nodes links nodes represent random variables links express probabilistic relationships among them. models pointed main types pgms directed pgms undirected pgms although exist hybrid ones. paper mainly focus directed pgms. details undirected pgms readers referred element-wise sigmoid function matrix +exp regularization parameter denotes frobenius norm. purpose imposing allow nonlinear transformation. normally transformations like tanh used alternatives sigmoid function. hidden units. easily computed given. since given data need learn here. usually done using backpropagation stochastic gradient descent compute gradients objective function respect denote value objective function compute gradients using chain rule regularization terms omitted. element-wise product denoted mean matlab operation matrices. practice small part data compute gradients update. called stochastic gradient descent. autoencoder feedforward neural network encode input compact representation reconstruct input learned representation. simplest form autoencoder multilayer perceptron bottleneck layer middle. idea autoencoders around decades abundant variants autoencoders proposed enhance representation learning including sparse contractive denoising details please refer nice recent book deep learning introduce kind multilayer denoising known stacked denoising autoencoders example classic example would latent dirichlet allocation used topic model analyze generation words topics documents. usually comes graphical representation model generative process depict story random variables generated step step. figure shows graphical model corresponding generative process follows generative process gives story random variables generated. graphical model figure shaded node denotes observed variables others latent variables parameters model deﬁned learning algorithms applied automatically learn latent variables parameters. bayesian nature like easy extend incorporate information perform tasks. example different variants topic models based proposed. authors proposed incorporate temporal information extends assuming correlations among topics. make possible process large datasets extends batch mode online setting. recommender systems extends incorporate rating information make recommendations. model extended incorporate social information inference learning strictly speaking process ﬁnding parameters called learning process ﬁnding latent variables given parameters called inference. however given observed variables learning inference often intertwined. usually learning inference would alternate updates latent variables updates parameters learning inference completed would parameters document arrives learned perform inference alone topic proportions document. various learning inference algorithms available pgm. among them cost-effective probably maximum posteriori amounts maximizing posterior probability latent variable. using learning process equivalent minimizing objective function regularization. famous example probabilistic matrix factorization learning graphical model equivalent factorization large matrix low-rank matrices regularization. efﬁcient gives point estimates latent variables order take uncertainty account harness full power bayesian models would resort bayesian treatments variational inference markov chain monte carlo example original uses variational inference approximate true posterior factorized variational distributions learning latent variables parameters boils minimizing kl-divergence variational distributions true posterior distributions. besides variational inference another choice bayesian treatment mcmc. example mcmc algorithms proposed learn posterior distributions lda. bayesian deep learning background deep learning ready introduce general framework concrete examples bdl. speciﬁcally section list recent models applications recommender systems topic models. summary models shown table general framework mentioned section principled probabilistic framework seamlessly integrated components perception component task-speciﬁc component. figure shows simple model example. part inside rectangle left represents perception component part inside blue rectangle right task-speciﬁc component. typically perception component would probabilistic formulation deep learning model multiple nonlinear processing layers represented chain structure pgm. nodes edges perception component relatively simple task-speciﬁc component often describe complex distributions relationships among variables three sets variables three sets variables model perception variables hinge variables task variables paper denote perception variables variables perception component. usually would include weights neurons probabilistic formulation deep learning model. fig. example bdl. rectangle left indicates perception component blue rectangle right indicates task-speciﬁc component. hinge variable {j}. denote hinge variables variables directly interact perception component task-speciﬁc component. table shows hinge variables listed models. task variables i.e. variables task-speciﬁc component without direct relation perception component denoted i.i.d. requirement note hinge variables always task-speciﬁc component. normally connections hinge variables perception component i.i.d. convenience parallel computation perception component. example related corresponding although mandatory models meeting requirement would signiﬁcantly increase efﬁciency parallel computation model training. variance related mentioned section motivations model uncertainty exchanging information perception component task-speciﬁc component boils modeling uncertainty related example kind uncertainty reﬂected variance conditional density equation according shown above terms model ﬂexibility normally models properly regularized model would outperform model superior model. table show types variance different models. note although model table speciﬁc type always adjust models devise counterparts types. example table model easily adjust devise counterparts. authors compare performance ﬁnds former performs signiﬁcantly better meaning sophisticatedly modeling uncertainty components essential performance. criterion implies conventional variational inference mcmc methods applicable. usually online version needed sgd-based methods work either unless inference needed typically large number free parameters perception component. means methods based laplace approximation realistic since involve computation hessian matrix scales quadratically number free parameters. denoted output layer sdae denoted j-by-kl matrix number units layer similar denoted xlj∗. weight matrix bias vector respectively layer wl∗n denotes column number layers. convenience denote collection layers weight matrices biases. note l/-layer sdae corresponds l-layer network. generalized note goes inﬁnity gaussian distribution equation become dirac delta distribution centered sigmoid function. model degenerate bayesian formulation sdae. call generalized sdae. note ﬁrst layers network encoder last layers decoder. maximization posterior probability equivalent minimization reconstruction error weight decay taken consideration. hyperparameters conﬁdence parameter similar note middle layer serves bridge ratings content information. middle layer along latent offset enables simultaneously learn effective feature representation capture note generation clean input part generative process bayesian sdae generation noise-corrupted input artiﬁcial noise injection process help sdae learn robust feature representation. despite successful applications deep learning natural language processing computer vision attempts made develop deep learning models authors restricted boltzmann machines instead conventional matrix factorization formulation perform extends work incorporating user-user item-item correlations. although methods involve deep learning actually belong cf-based methods incorporate content information crucial accurate recommendation. authors low-rank matrix factorization last weight layer deep network signiﬁcantly reduce number model parameters speed training however classiﬁcation instead recommendation tasks. music recommendation directly conventional deep belief networks assist representation learning content information deep learning components models deterministic without modeling noise hence less robust. models achieve performance boost mainly loosely coupled methods without exploiting interaction content information ratings. besides linked directly rating matrix means models perform poorly serious overﬁtting ratings sparse. collaborative deep learning address challenges above hierarchical bayesian model called collaborative deep learning novel tightly coupled method introduced based bayesian formulation sdae tightly couples deep representation learning content information collaborative ﬁltering rating matrix allowing two-way interaction two. experiments show signiﬁcantly outperforms state art. following text start introduction notation used presentation cdl. review design learning cdl. notation problem formulation similar work recommendation task considered takes implicit feedback training test data. entire collection items represented j-by-b matrix bag-of-words vector xcj∗ item based vocabulary size users deﬁne i-by-j binary rating matrix i×j. example dataset citeulike-a user article personal library otherwise. given part ratings content information problem predict ratings note although current form focuses movie recommendation article recommendation like section general enough handle recommendation tasks fig. left graphical model cdl. part inside dashed rectangle represents sdae. example sdae shown. right graphical model degenerated cdl. part inside dashed rectangle represents encoder sdae. example sdae shown right. note although still decoder sdae vanishes. prevent clutter omit variables except graphical models. optimization perspective third term objective function equivalent multi-layer perceptron using latent item vectors target fourth term equivalent sdae minimizing reconstruction error. perspective neural networks approaches positive inﬁnity training probabilistic graphical model figure would degenerate simultaneously training neural networks overlaid together common input layer different output layers shown figure note second network much complex typical neural networks involvement rating matrix. ratio λn/λv approaches positive inﬁnity degenerate two-step model latent representation learned using sdae directly ctr. interaction perception component task-speciﬁc component one-way meaning perception component affected task-speciﬁc component. another extreme happens λn/λv goes zero decoder sdae essentially vanishes. right figure graphical model degenerated λn/λv goes zero. demonstrated experiments predictive performance suffer greatly extreme cases veriﬁes information task-speciﬁc component improve perception component mutual boosting effect crucial bdl. graphical model approaches positive inﬁnity shown figure where notational simplicity place note according deﬁnition section perception variables {{wl}{bl}{xl} hinge variables task variables learning based model above parameters could treated random variables fully bayesian methods markov chain monte carlo variational approximation methods applied. however treatment typically incurs high computational cost. consequently uses em-style algorithm obtaining estimates bayesian collaborative deep learning besides estimates sampling-based algorithm bayesian treatment also proposed algorithm turns bayesian generalized version well-known back-propagation learning algorithm. list conditional densities follows interestingly goes inﬁnity adaptive rejection metropolis sampling used sampling turns bayesian generalized version speciﬁcally figure shows getting gradient loss function point next sample would drawn region line alternating update local optimum several commonly used techniques using momentum term applied alleviate local optimum problem. note carefully designed model minimize overhead seamlessly combining perception component task-speciﬁc component. computational complexity perception component task-speciﬁc component number non-zero entries rating matrix computational complexity whole model signiﬁcant overhead introduced. prediction observed test data. similar uses point estimates calculate predicted rating t|d] denotes expectation operation. words approximate predicted rating similar algorithms used learn parameters cdr. reported using ranking objective leads signiﬁcant improvement recommendation performance. following deﬁnition section cdr’s perception variables {{wl}{bl}{xl} hinge variables task variables symmetric collaborative deep learning models like focus deep learning component modeling item content. besides content information items attributes users sometimes contain much important information. therefore desirable extend model user attributes well call variant symmetric cdl. example using extra msdae user attributes adds extra terms equation user attributes) collection different corrupted versions yci∗ k-time repeated version yci∗ transformation matrix user latent factors number user attributes. similar marginalized solution given parameters discussion ﬁrst hierarchical bayesian model bridge state-of-the-art deep learning models performing deep learning collaboratively variants simultaneously extract effective deep feature representation content capture similarity implicit relationship items perception component task-speciﬁc component able interact create synergy boost recommendation accuracy. learned representation also used tasks recommendation. unlike previous deep learning models simple target classiﬁcation reconstruction cdl-based models complex target probabilistic framework. equivalent probabilistic version sample curve loss function tangent line would added better approximate distribution corresponding joint log-likelihood. that samples would drawn region lines. sampling besides searching local optima using gradients algorithm also takes variance consideration. called bayesian generalized back-propagation. marginalized collaborative deep learning sdae corrupted input goes encoding decoding recover clean input. usually different epochs training different corrupted versions input. hence generally sdae needs enough epochs training sufﬁcient corrupted versions input. marginalized sdae seeks avoid marginalizing corrupted input obtaining closed-form solutions directly. sense msdae computationally efﬁcient sdae. solver expectation equation provided note linear one-layer case generalized nonlinear multi-layer case using techniques marginalized perception variables hinge variables task variables collaborative deep ranking assumes collaborative ﬁltering setting model ratings directly. however output recommender systems often ranked list means would natural ranking rather ratings objective. motivation collaborative deep ranking proposed jointly perform representation learning collaborative ranking. corresponding generative process except step replaced with learned representation vector item denotes relational latent matrix column relational latent vector item note equation matrix variate normal distribution deﬁned operator denotes kronecker product matrices denotes trace matrix laplacian matrix incorporating relational information. diagonal matrix adjacency matrix representing relational information binary entries indicating links items. indicates link item item otherwise. denotes product gaussian performance bdl. cdl-based models above exchange achieved assuming gaussian distributions connect hinge variables variables perception component generative process perception variable) simple effective efﬁcient computation. among cdl-based models table three models others models according deﬁnition section since veriﬁed signiﬁcantly outperforms counterpart expect extra performance boosts counterparts three models. besides efﬁcient information exchange designs models also meet i.i.d. requirement distribution concerning hinge variables discussed section hence easily parallelizable. models introduced later alternative designs enable efﬁcient i.i.d. information exchange components bdl. relational stacked denoising autoencoders topic models problem statement notation assume denoting items content item besides denote k-dimensional identity matrix denote relational latent matrix representing relational properties item j-by-b matrix represents clean input sdae noise-corrupted matrix size denoted besides denote output layer sdae j-by-kl matrix denoted xlj∗ weight matrix bias vector layer wl∗n denotes column number layers. shorthand refer collection weight matrices biases layers note l/-layer sdae corresponds l-layer network. mentioned rsdae formulated novel probabilistic model seamlessly integrate layered representation learning relational information available. model simultaneously learn feature representation content information relation items. graphical model rsdae shown figure generative process listed follows table shows recall different methods dataset movielens-plot learned representation used recommendation rsdae signiﬁcantly outperforms sdae means relational information task-speciﬁc component crucial performance boost. please refer details. deep poisson factor analysis sigmoid belief networks poisson distribution support nonnegative integers known natural choice model counts. therefore desirable building block topic models motivation proposed model dubbed poisson factor analysis latent nonnegative matrix factorization poisson distributions. poisson factor analysis assumes discrete n-by-p matrix containing word counts documents vocabulary size nutshell described using following equation denotes factor loading matrix factor analysis k-th encoding importance word topic n-by-k matrix factor score matrix n-th containing topic proportions document n-by-k matrix latent binary matrix n-th deﬁning topics associated document different priors correspond different models. example dirichlet priors all-one matrix would recover beta-bernoulli prior leads negative binomial focused topic model model deep-structured prior based sigmoid belief networks imposed form deep model topic modeling. number layers corresponds equation entry matrix n-th xnpk count word comes topic document perception variables {{hl}{wl}{bl}} hinge variables task variables {{φk}{rk} weight matrix containing columns bias vector containing entries xlj∗ note ﬁrst term corresponds matrix variate distribution equation simple manipulation k∗lask∗ denotes k-th maximizing equivalent making closer item item linked {{xl} xc{wl}{bl}} hinge variables task variables {a}. naive approach solve linear system setting λr−xt unfortunately complexity single update. similar steepest descent method used iteratively update given learn layer using back-propagation algorithm. alternating update local optimum found. also techniques including momentum term help avoid trapped local optimum. computational topic hierarchy would facilitate accurate modeling words topics providing valuable information learning inter-document relations. hand accurately modeling words topics inter-document relations could help discovery topic hierarchy learning compact document representations. information exchange mechanism bdl-based topic models different section example sbn-based dpfa model exchange natural since bottom layer relationship inherently probabilistic shown equation means additional assumptions distribution necessary. sbn-based dpfa model equivalent assuming generated dirac delta distribution centered bottom layer hence dpfa models table models according deﬁnition section worth noting rsdae model hinge variable others perception variables) naively modifying model counterpart would violate i.i.d. requirement section consider controlling complex dynamical system according live video stream received camera. solving control problem iteration tasks perception images control based dynamic models. perception task taken care using multiple layers simple nonlinear transformation control task usually needs sophisticated models hidden markov models kalman ﬁlters enable effective iterative process perception task control task two-way information exchange often necessary. perception component would basis control component estimates states hand control component built-in dynamic model would able predict future trajectory reversing perception process. example proposed bdl-based model performs control based received images generative process follows learning using bayesian conditional density filtering efﬁcient learning algorithms needed bayesian treatments deep pfa. proposed online version mcmc called bayesian conditional density ﬁltering learn global parameters local variables conditional densities used gibbs updates follows xnpk|− multi φk|− θnk|− gamma nk|− δber learning using stochastic gradient thermostats alternative learning deep stochastic gradient n´ose-hoover thermostats accurate scalable. speciﬁcally following stochastic differential equations used log-posterior model. indexes time denotes standard wiener process. thermostats variable make sure system constant temperature. injected variance constant. deep poisson factor analysis restricted boltzmann machine similar deep above restricted boltzmann machine used place used equation would deﬁned using energy learning similar algorithms deep used. speciﬁcally sampling process would alternate {{φk}{γk} {{wl}{bl}}. {{φk}{γk} similar conditional density sbn-based dpfa used. {{wl}{bl}} contrastive divergence algorithm. discussion bdl-based topic models perception component responsible inferring topic hierarchy documents task-speciﬁc component charge modeling word generation topic generation word-topic relation inter-document relation. synergy components comes bidirectional interaction them. hand knowledge worth noting terms information exchange components bdl-based control model uses different mechanism ones section section uses neural networks separately parameterize mean covariance hinge variables ﬂexible models section gaussian distributions ﬁxed variance also used. note bdl-based control model model since covariance assumed diagonal model still meets i.i.d. requirement section conclusions future research paper identiﬁed current trend merging probabilistic graphical models neural networks proposed framework reviewed relevant recent work strives combine merits organically integrating single principled probabilistic framework. learn parameters several algorithms proposed ranging block coordinate descent bayesian conditional density ﬁltering stochastic gradient thermostats stochastic gradient variational bayes. gained popularity success recent promising advances deep learning. since many real-world tasks involve perception inference natural choice harnessing perception ability inference ability pgm. although current applications focus recommender systems topic models stochastic optimal control future expect increasing number applications link prediction community detection active learning bayesian reinforcement learning many complex tasks need interaction perception causal inference. complex tasks interconnected perception components task-speciﬁc components possesses great performance-boosting potential. besides advances efﬁcient bayesian neural networks important component expected scalable. geoffrey hinton. training products experts minimizing contrastive divergence. neural computation geoffrey hinton drew camp. keeping neural networks simple minimizing description length weights. colt pages takamitsu matsubara vicencc g´omez hilbert kappen. latent kullback leibler control continuous-state systems using probabilistic graphical models. pages porteous david newman alexander ihler arthur asuncion padhraic smyth welling. fast collapsed gibbs sampling latent dirichlet allocation. pages salah rifai pascal vincent xavier muller xavier glorot yoshua bengio. contractive auto-encoders explicit invariance feature extraction. icml pages tara sainath brian kingsbury vikas sindhwani ebru arisoy bhuvana ramabhadran. low-rank matrix factorization deep neural network training high-dimensional output targets. icassp pages pascal vincent hugo larochelle isabelle lajoie yoshua bengio pierre-antoine manzagol. stacked denoising autoencoders learning useful representations deep network local denoising criterion. jmlr manuel watter jost springenberg joschka boedecker martin riedmiller. embed control locally linear latent dynamics nips pages model control images. haochao ying liang chen yuwen xiong jian collaborative deep ranking hybrid pair-wise recommendation algorithm implicit feedback. pakdd pages fuzheng zhang nicholas jing yuan defu lian xing wei-ying collaborative knowledge base embedding recommender systems. pages wang received b.sc. degree computer science shanghai jiao tong university china. currently ph.d student department computer science engineering hong kong university science technology. research interests statistical machine learning data mining. received hong kong fellowship microsoft research asia fellowship baidu research fellowship achievements statistical machine learning dit-yan yeung received beng degree electrical engineering mphil degree computer science university hong kong degree computer science university southern california. started academic career assistant professor illinois institute technology chicago. joined hong kong university science technology full professor department computer science engineering joint appointment department electronic computer engineering. research interests computational statistical approaches machine learning artiﬁcial intelligence.", "year": 2016}