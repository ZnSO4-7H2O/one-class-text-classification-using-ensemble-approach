{"title": "Sparse Online Learning via Truncated Gradient", "tag": ["cs.LG", "cs.AI"], "abstract": "We propose a general method called truncated gradient to induce sparsity in the weights of online learning algorithms with convex loss functions. This method has several essential properties: The degree of sparsity is continuous -- a parameter controls the rate of sparsification from no sparsification to total sparsification. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular $L_1$-regularization method in the batch setting. We prove that small rates of sparsification result in only small additional regret with respect to typical online learning guarantees. The approach works well empirically. We apply the approach to several datasets and find that for datasets with large numbers of features, substantial sparsity is discoverable.", "text": "propose general method called truncated gradient induce sparsity weights online learning algorithms convex loss functions. method several essential properties approach theoretically motivated instance regarded online counterpart popular l-regularization method batch setting. prove small rates sparsi cation result small additional regret respect typical online learning guarantees. concerned machine learning large datasets. example largest dataset sparse examples features using bytes. setting many common approaches fail simply cannot load dataset memory ciently cient. roughly approaches work simply adding regularization gradient online weight update work gradients induce sparsity. essential culty gradient update form oats. pairs little reason expect gradient update accidentally produce sparsity. simply rounding weights problematic weight small useless small updated rounding techniques also play havoc standard online learning guarantees. black-box wrapper approaches eliminate features test impact elimination cient enough. approaches typically algorithm many times particularly undesirable large datasets. lasso algorithm commonly used achieve regularization linear regression. algorithm work automatically online fashion. formulations regularization. consider loss function convex input/output pair. convex constraint formulation appropriately chosen formulations equivalent. convex constraint formulation simple online version using projection idea requires projection weight ball every online step. operation cult implement ciently large-scale data examples sparse features large number features. situation require number operations online step linear respect number nonzero features independent total number features. method works soft-regularization formulation satis requirement. additional details found section addition regularization formulation family online algorithms consider paper also include nonconvex sparsi cation techniques. operates decaying weights previous examples rounding weights zero become small. forgetron stated kernelized online algorithms concerned simple linear setting. applied linear kernel forgetron computationally space competitive approaches operating directly feature weights. high level approach take weight decay default value. simple approach enjoys strong performance guarantee discussed section instance algorithm never performs much worse standard online learning algorithm additional loss sparsi cation controlled continuously single real-valued parameter. theory gives family algorithms convex loss functions inducing sparsity\u0016one online learning algorithm. instantiate square loss show deal sparse examples ciently section mentioned introduction mainly interested sparse online methods large scale problems sparse features. problems algorithm satisfy following requirements theoretical results stating much sparsity achieved using method generally require additional assumptions practice. consequently rely experiments section show method achieves good sparsity practice. compare approach others including regularization small data well online rounding cients zero. sub-gradient respect variable parameter often referred learning rate. analysis consider constant learning rate simplicity. theory might desirable decaying learning rate becomes smaller increases called no-regret bound without knowing advance. however known advance select constant accordingly regret vanishes since focus sparsity choose learning rate method widely used online learning moreover argued cient even solving batch problems repeatedly online algorithm training data multiple times. example idea successfully applied solve large-scale standard formulations scenario outlined introduction online learning methods suitable traditional batch learning methods. however main drawback achieve sparsity address paper. note literature particular update rule often referred gradient descent stochastic gradient descent variants exponentiated gradient descent since focus paper sparsity versus shall consider modi cations simplicity. section examine several methods achieving sparsity online learning. idea simple cient rounding natural method. consider full online implementation another method online counterpart regularization batch learning. shall ideas closely related. perform standard stochastic gradient descent rule round updated cients toward zero. e\u001bect remove nonzero small components weight vector. general take especially small since step modi small amount. cient zero remains small online update rounding operation pulls back zero. consequently rounding done every steps case nonzero cients cient time threshold however large training stage need keep many nonzero features intermediate steps rounded zero. extreme case simply round cients solve storage problem training phase. sensitivity choosing appropriate main drawback method; another drawback lack theoretical guarantee online performance. plus rounding used simple baseline. note method produce sparse weights online. therefore handle large-scale problems cannot keep features memory. order obtain online version simple rounding rule observe direct rounding zero aggressive. less aggressive version shrink cient zero smaller amount. call idea truncated gradient. general larger parameters sparsity incurred. extra truncation method lead sparse solutions rmed experiments described later. experiments degree sparsity discovered varies problem. parameter controls sparsity achieved algorithm. note update rule identical standard stochastic gradient descent rule. general perform truncation every steps. integer integer gravity parameter reason perform aggressive truncation gravity parameter steps. potentially lead better sparsity. descent regularization di\u001berent naive application stochastic gradient descent rule added regularization term. pointed introduction latter fails rarely leads sparsity. theory shows even sparsi cation prediction performance still comparable standard online learning algorithm. following develop general regret bound general method also shows regret depend sparsi cation parameter assumption assume convex exist non-negative constants rd+. linear prediction problems general loss function form following common loss functions corresponding choices parameters assumption supx main result theorem parameterized proof left appendix. specializing particular losses yields several corollaries. applied least squares loss given later corollary state theorem constant learning rate mentioned earlier possible obtain result variable learning rate decays increases. although lead no-regret bound without knowing advance introduces extra complexity presentation main idea. since focus sparsity rather optimizing learning rate include result clarity. known advance bound simply take depends easily estimated. remove dependency trivial upper bound used leading penalty general case cannot remove non-convex penalty solving non-convex formulation hard possible replace right-hand side simple online update rule. still naturally expects right-hand side penalty much smaller corresponding penalty especially many components close therefore situation potentially yield theorem also implies trade-o\u001b sparsity regret performance. simply consider case constant. small less sparsity regret term right-hand side also small. large able achieve sparsity regret right-hand side also becomes words l+gw regularized loss regularized regret small implies procedure regarded online counterpart l-regularization methods. stochastic setting examples drawn underlying distribution sparse online gradient method proposed paper solves regularization problem. stochastic-gradient-based online learning methods used solve large-scale batch optimization problems often quite successfully setting training examples one-by-one online fashion repeat multiple times training data. section analyze performance procedure using theorem simplify analysis instead assuming data assume additional data point drawn training data randomly equal probability. corresponds standard stochastic optimization setting observed samples underlying distributions. following result simple consequence theorem choose random stopping time inequalities says average also solves regularization problem approximately. therefore experiment last solution instead aggregated solution since -norm regularization frequently used achieve sparsity batch learning setting connection -norm regularization regarded alternative justi cation sparse-online algorithm developed paper. method section directly applied least squares regression. leads algorithm implements sparsi cation square loss according equation description superscripted symbol denote j-th component vector clarity also drop index although keep choice gravity parameters open algorithm description practice consider following choice many online learning situations small subset features nonzero values example thus desirable deal sparsity small subset rather features simultaneously inducing sparsity feature weights. moreover important store features non-zero cients bounded average square loss best weight vector plus term related size decays additive o\u001bset controlled sparsity threshold gravity parameter prediction clipped interval implying loss function square loss unclipped predictions outside dynamic range. instead update constant value equivalent gradient linear loss function. learning rate vowpal wabbit controllable supporting decay well constant learning rate program operates entirely online fashion memory footprint essentially weight vector even amount data large. mentioned earlier would like algorithm's computational complexity depend linearly number nonzero features example rather total number features. approach took store time-stamp feature time-stamp initialized index example feature nonzero time. online learning simply went nonzero features example could \u0010simulate\u0011 shrinkage batch mode. weights updated time stamps reset lazy-update idea delaying shrinkage calculation needed cient implementation truncated gradient. speci cally instead using update rule weight shrunk weights nonzero feature di\u001berently following note lazy-update trick maintaining time-stamp information applied algorithms given section cient rounding algorithm instance nonzero feature example perform regular gradient descent square speci using operations independent total number features. standard implementation relies sorting weights requires operations total number features. complexity unacceptable purpose. however shall point important recent work authors proposed cient online -projection method. idea balanced tree keep track weights allows cient threshold nding tree updates operations average although algorithm still weak dependency applicable large scale practical applications. theoretical analysis presented paper shows obtain meaningful regret bound picking arbitrary useful resulting method much simpler implement computationally cient online step. moreover method allows non-convex updates closely related simple cient rounding idea. complexity implementing balanced tree strategy shall compare paper. applied vowpal wabbit ciently implemented sparsify option described previous section selection datasets including eleven datasets repository much larger dataset private large-scale dataset big_ads related interest prediction. datasets useful benchmark purposes big_ads interesting since embody real-world datasets large numbers features many less informative making predictions others. datasets summarized table datasets used many features expected large fraction features useful making predictions. comparison purposes well better demonstrate behavior algorithm also added random binary features datasets. feature value probability otherwise. experiments interested much reduction number features possible without a\u001becting learning performance signi cantly; speci cally require accuracy reduced classi cation tasks total square loss increased regression tasks. common practice allowed algorithm training data multiple passes decaying learning rate. dataset performed -fold cross validation training identify best parameters including learning rate sparsi cation rate number passes training decay learning rate across passes. parameters used train vowpal wabbit whole training set. finally learned classi er/regressor evaluated test set. figure shows fraction reduced features sparsi cation applied dataset. datasets randomly added features vowpal wabbit able reduce number features fraction except dataset reduction observed. less satisfying result might improved extensive parameter search cross validation. however tolerate decrease accuracy cross validation vowpal wabbit able achieve reduction indicating large reduction still possible tiny additional cost accuracy loss. slightly aggressive sparsi cation test-set accuracy drops accuracy without sparsi cation even original datasets without arti cially added features vowpal wabbit manages lter less useful features maintaining level performance. example dataset reduction achieved. compared results above seems e\u001bective feature reductions occur datasets large number less useful features exactly sparsi cation needed. features removed sparsi cation process indicating e\u001bectiveness algorithm real-life problems. able many parameters cross validation size rcv. expected reduction possible thorough parameter search performed. previous results exercise full power approach presented applied datasets standard lasso regularization computationally viable. also applied approach large non-public dataset big_ads goal predicting clicked given context information here accepting increase classi cation error allows reduce number features factor decrease number features. classi cation tasks also study sparsi cation solution a\u001bects standard metric classi cation. using sets parameters -fold cross validation described above criterion a\u001bected signi cantly sparsi cation cases actually slightly improved. reason figure plot showing amount features left sparsi cation dataset. result fraction features left performance changed sparsi cation. second result sparsi cation random features added example. big_ads second column since experiment useful. sparsi cation method remove features could confused vowpal wabbit ratios without sparsi cation classi cation tasks plotted figures often case ratios argued before using value larger desired truncated gradient rounding algorithms. advantage empirically demonstrated here. particular algorithms. before cross validation used select parameters rounding algorithm including learning rate number passes data training learning rate decay training passes. figure plot showing ratio sparsi cation used sparsi cation used. process figure used determine empirically good parameters. result original dataset second result modi dataset random features added example. figure gives number-of-feature plots data point generated running respective algorithms using di\u001berent value observations place. first results verify observation behavior truncated gradient similar rounding algorithm. second results suggest that practice desired truncated gradient next experiments compares truncated gradient descent algorithms regarding abilities tradeo\u001b feature sparsi cation performance. again focus metric classi cation tasks except wpdc. algorithms comparison include figure gives results. first observed truncated gradient consistently competitive online algorithms signi cantly outperformed problems. suggests e\u001bectiveness truncated gradient. second interesting observe qualitative behavior truncated gradient often similar lasso especially sparse weight vectors allowed consistent theorem showing relation algorithms. however lasso usually worse performance allowed number nonzero weights large case lasso seems contrast truncated gradient robust tting. robustness online learning often attributed early stopping extensively discussed literature finally worth emphasizing experiments subsection shed light relative strengths algorithms terms feature sparsi cation. large datasets big_ads truncated gradient cient rounding sub-gradient algorithms applicable large-scale problems sparse features. shown argued rounding algorithm quite work robustly problems subgradient algorithm lead sparsity general training. paper covers sparsi cation technique large-scale online learning strong theoretical guarantees. algorithm truncated gradient natural extension lasso-style regression online-learning setting. theorem proves technique sound never harms performance much compared standard stochastic gradient descent adversarial situations. furthermore show asymptotic solution instance algorithm essentially equivalent lasso regression thus justifying algorithm's ability produce sparse weight vectors number features intractably large. theorem veri experimentally number problems. cases especially problems many irrelevant features approach achieves order magnitude reduction number features.", "year": 2008}