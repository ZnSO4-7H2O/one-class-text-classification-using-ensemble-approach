{"title": "Shortcut-Stacked Sentence Encoders for Multi-Domain Inference", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "We present a simple sequential sentence encoder for multi-domain natural language inference. Our encoder is based on stacked bidirectional LSTM-RNNs with shortcut connections and fine-tuning of word embeddings. The overall supervised model uses the above encoder to encode two input sentences into two vectors, and then uses a classifier over the vector combination to label the relationship between these two sentences as that of entailment, contradiction, or neural. Our Shortcut-Stacked sentence encoders achieve strong improvements over existing encoders on matched and mismatched multi-domain natural language inference (top non-ensemble single-model result in the EMNLP RepEval 2017 Shared Task (Nangia et al., 2017)). Moreover, they achieve the new state-of-the-art encoding result on the original SNLI dataset (Bowman et al., 2015).", "text": "present simple sequential sentence encoder language inference. encoder based stacked bidirectional lstm-rnns shortcut connections ﬁne-tuning word embeddings. overall supervised model uses encoder encode input sentences vectors uses classiﬁer vector combination label relationship sentences entailment contradiction neural. shortcut-stacked sentence encoders achieve strong improvements existing encoders matched mismatched multi-domain natural language inference moreover achieve state-of-theart encoding result original snli dataset natural language inference recognizing textual entailment fundamental semantic task ﬁeld natural language processing. problem determine whether given hypothesis sentence logically inferred given premise sentence. recently released datasets stanford natural language inference corpus multi-genre natural language inference corpus encouraged several end-to-end neural network approaches also served evaluation resource general representation learning natural language. depending whether model ﬁrst encode sentence ﬁxed-length vector without incorporating information sentence several proposed models categorized groups encoding-based models tree-based encoders stack-augmented parser-interpreter neural network bowman joint pairwise models cross-features between sentences encode them enhanced sequential inference model chen bilateral multiperspective matching model wang moreover common sentence encoders classiﬁed tree-based encoders spinn bowman mentioned before sequential encoders bilstm model bowman paper follow former approach encoding-based models propose novel simple sequential sentence encoder multinli problem. encoder require syntactic information sentence. also contain attention memory structure. basically stacked bidirectional lstm-rnn shortcut connections word embedding ﬁne-tuning. overall supervised model uses shortcutstacked encoders encode input sentences vectors classiﬁer vector combination label relationship sentences entailment contradiction neural conneau simple shortcut-stacked encoders achieve strong improvements existing encoders multi-layered shortcutconnected properties matched mismatched evaluation settings multi-domain natural language inference well original snli dataset. single-model result emnlp repeval multi-nli shared task state-of-the-art encoding-based results snli dataset github code link https//github.com/ easonnie/multinli_encoder model mainly consists separate components sentence encoder entailment classiﬁer. sentence encoder compresses source sentence vector representation classiﬁer makes three-way classiﬁcation based vectors source sentences. model follows ‘encoding-based rule’ i.e. encoder encode source sentence ﬁxed length vector without information function based sentence order fully explore generalization sentence encoder encoder applied premise hypothesis shared parameters projecting space. setting follows idea siamese networks bromley figure shows sentence encoder sentence encoder simply composed multiple stacked bidirectional lstm layers shortcut connections followed pooling layer. bilstmi represent bilstm layer deﬁned input next lstm-rnn layer simply output sequence previous lstm-rnn layer. settings input sequences bilstm layer concatenated outputs previous layers plus original word embedding sequence. gives shortcut connection style setup related widely used idea residual connections cnns computer vision highway networks rnns speech processing shortcut connections hierarchical multitasking learning case feed previous layers’ output sequences well word embedding sequence every layer. represent words source sentence. assume word embedding vector initialized using pre-trained vector embeddings then input bilstm layer time deﬁned then assuming layers bilstm ﬁnal vector representation obtained applying row-max-pool output last bilstm layer similar conneau ﬁnal layer deﬁned closest encoder architecture conneau whose model consists single-layer bilstm max-pooling layer treat starting point. experiments demonstrate enhancements stacked-birnn shortcut connections provide signiﬁcant gains baseline entailment classiﬁer obtain vector representation premise hypothesis sentence apply three matching methods vectors concatenation element-wise distance elementwise product vectors concatenate three match vectors vector representations premise hypothesis respectively. matching vector deﬁned datasets instructed repeval multi-nli shared task training data multinli combined randomly selected samples snli training resampled epoch) ﬁnal training models; cross-domain in-domain multi-nli development sets model selection. snli test results table train snli training mization batch size. starting learning rate half decay every epochs. number hidden units classiﬁer dropout layer also applied output layer dropout rate used pre-trained glove vectors initialize word embeddings. tuning decisions word embedding training strategy hyperparameters dimension number layers bilstm activation type number layers explained section ablation analysis results investigate effectiveness enhancement components overall model. ablation results shown tables based multi-nli development sets. finally table shows results different encoders snli multi-nli test sets. first table shows performance changes different number bilstm layers varying dimension size. dimension size bilstm layer referring dimension hidden state forward backward lstm-rnns. shown added layer model improves accuracy achieve substantial improvement accuracy matched mismatched settings compared single-layer bilstm conneau experimented layers dimensions each model still potential improve result further larger dimension layers. next table show shortcut connections among bilstm layers also important contributor accuracy improvement demonstrates simply stacking bilstm layers sufﬁcient next table show ﬁne-tuning word embeddings also improves results in-domain task cross-domain tasks hence models trained word embeddings ﬁne-tuned. last ablation table shows classiﬁer layers relu preferable options. thus setting strongest encoder. multi-nli snli test results finally table report test results mnli snli. first multi-nli improve substantially cbow bilstm encoder baselines reported dataset paper also show ﬁnal shortcut-based stacked encoder achieves around improvement compared layer bilstm-max encoder second last shortcut-encoder also singe-model result emnlp repeval shared task leaderboard. snli compare shortcutstacked encoder current state-of-the-art encoders snli leaderboard also compare recent bilstm-max encoder conneau served model’s -layer starting point. results indicate ‘our shortcut-stacked encoder’ surnote ‘our bilstm-max encoder’ results second-last obtained using reimplementation conneau model; version better likely classiﬁer optimizer settings. passes previous state-of-the-art encoders achieves best encoding-based result snli suggesting general effectiveness simple shortcut-connected stacked layers sentence encoders. explored various simple combinations connections bilstm-rnn layered architectures developed shortcut-stacked sentence encoder natural language inference. model single result emnlp repeval multi-nli shared task also surpasses state-of-the-art encoders snli dataset. future work also evaluating effectiveness shortcut-stacked sentence encoders several semantic tasks. later experiments found residual connection achieve similar accuracies fewer number parameters compared shortcut connection. therefore order reduce model size also follow snli leaderboard settings performed additional snli experiments shortcut connections replaced residual connections input next bilstm layer concatenation word embedding summation outputs previous layers table shows residual-connection snli test results parameter comparison shortcut-connection models table results snli fewer-parameter residual-stacked encoder models. model bilstm-stacked layers layer. param column denotes number parameters millions. references samuel bowman gabor angeli christopher potts christopher manning. large annotated corpus learning natural language inferproceedings conference ence. empirical methods natural language processing association computational linguistics. samuel bowman gauthier abhinav rastogi raghav gupta christopher manning christopher potts. fast uniﬁed model parsing sentence understanding. arxiv preprint arxiv. jane bromley isabelle guyon yann lecun eduard s¨ackinger roopak shah. signature veriﬁcation using siamese time delay neural network. advances neural information processing systems. pages alexis conneau douwe kiela holger schwenk loic barrault antoine bordes. supervised learning universal sentence representations arxiv preprint natural language inference data. arxiv. kazuma hashimoto caiming xiong yoshimasa tsuruoka richard socher. joint many-task model growing neural network multiple tasks. arxiv preprint arxiv. kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recogproceedings ieee conference nition. computer vision pattern recognition. pages nikita nangia adina williams angeliki lazaridou samuel bowman. repeval shared task multi-genre natural language inference proceedings sentence representations. jeffrey pennington richard socher christopher manning. glove global vectors word representation. empirical methods natural language processing pages http//www.aclweb.org/anthology/d-. zhang guoguo chen dong kaisheng yaco sanjeev khudanpur james glass. highway long short-term memory rnns distant speech acoustics speech signal prorecognition. cessing ieee international conference ieee pages", "year": 2017}