{"title": "A Network-based End-to-End Trainable Task-oriented Dialogue System", "tag": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring costly labelled datasets to solve a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. The results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.", "text": "teaching machines accomplish tasks conversing naturally humans challenging. currently developing taskoriented dialogue systems requires creating multiple components typically involves either large amount handcrafting acquiring costly labelled datasets solve statistical learning problem component. work introduce neural network-based text-in textend-to-end trainable goal-oriented dialogue system along collecting dialogue data based novel pipe-lined wizard-of-oz framework. approach allows develop dialogue systems easily without making many assumptions task hand. results show model converse human subjects naturally whilst helping accomplish tasks restaurant search domain. building task-oriented dialogue system hotel booking technical support service difﬁcult application-speciﬁc usually limited availability training data. mitigate problem recent machine learning approaches task-oriented dialogue system design cast problem partially observable markov decision process using reinforcement learning train dialogue policies online interactions real users however language understanding language generation modules still rely supervised learning therefore need corpora train furthermore make tractable state action space must carefully designed restrict expressive power learnability model. also reward functions needed train models difﬁcult design hard measure run-time spectrum sequence sequence learning inspired several efforts build end-to-end trainable non-task-oriented conversational systems family approaches treats dialogue source target sequence transduction problem applying encoder network encode user query distributed vector representing semantics conditions decoder network generate system response. models typically require large amount data train. allow creation effective chatbot type systems lack capability supporting domain speciﬁc tasks example able interact databases aggregate useful information responses. work propose neural network-based model task-oriented dialogue systems balancing strengths weaknesses research communities model end-to-end trainable still modularly connected; directly model user goal nevertheless still learns accomplish required task providing relevant appropriate responses turn; explicit representation database attributes uses achieve high task success rate distributed representation user intent allow ambiguous inputs; uses delexicalisation weight tying strategy reduce data required train model still maintains high degree freedom larger amounts data become available. show proposed model performs given task competitively across several metrics trained hundred dialogues. order train model target application introduce novel pipe-lined data collection mechanism inspired wizard-of-oz paradigm collect human-human dialogue corpora crowd-sourcing. found process simple enables fast data collection online development costs. probable values belief state form query search result along intent representation belief state transformed combined policy network form single vector representing next system action. system action vector used condition response generation network generates required system output token token skeletal form. ﬁnal system response formed substituting actual values database entries skeletal sentence structure. detailed description component given below. treat dialogue sequence sequence mapping problem augmented dialogue history current database search outcome shown figure turn system takes sequence tokens user input converts internal representations distributed representation generated intent network probability distribution slot-value pairs called belief state generated belief trackers. database operator selects intent network viewed encoder sequence-to-sequence learning framework whose encode sequence input tokens distributed vector representation every turn typically long short-term memory network used last time step hidden layer taken representation figure tied jordan-type belief tracker delexicalised feature extractor. output feature extractor concatenation top-level sentence embedding several levels intermediate ngram-like embeddings however value cannot delexicalised input ngram-like embeddings padded zeros. zero vectors convolution operation make sure representation layer length. output tracker belief tracking provides core task-oriented spoken dialogue system current state-of-the-art belief trackers discriminative models recurrent neural networks directly hypotheses belief states although work focus text-based dialogue systems retain belief tracking core system because enables sequence freeform natural language sentences mapped ﬁxed slot-value pairs used query viewed simple version semantic parser keeping track dialogue state avoids learning unnecessarily complicated long-term dependencies inputs; uses smart weight tying strategy greatly reduce data required train model provides inherent robustness simpliﬁes future extension spoken systems. tribution values informable slot binary distribution requestable slot. slot ontology specialised tracker tracker jordantype feature extractor shown figure like mrkši´c weights together value vary features updating pre-softmax activation update equations given slot vector matrix bias terms scalar parameters. probability user mentioned slot turn calculated substituting numerator equation order model discourse context turn feature vector every token represented embedding size derived -hot input vector. order make tracker aware delexicalisation applied slot value slot-value specialised operator extracts level sentence representation also intermediate n-gram-like embeddings determined position delexicalised token utterance. multiple matches observed corresponding embeddings summed. hand match particular slot value empty n-gram embeddings padded zeros. order keep track position delexicalised tokens sides sentence padded zeros convolution operation. number vectors determined ﬁlter size layer. overall process extracting several layers position-speciﬁc features visualised figure belief tracker described based henderson modiﬁcations probabilities informable requestable slots values output recurrent memory block removed since appears offer beneﬁt task n-gram feature extractor replaced extractor described above. introducing slot-based belief trackers essentially intermediate labels system compared training pure end-to-end system. later paper show tracker components critical achieving task success. also show additional annotation requirement introduce successfully mitigated using novel pipe-lined wizard-of-oz data collection framework. value vector entities indicates corresponding entity consistent query addition entirely null associated entity pointer maintained identiﬁes matching entities selected random. entity pointer updated current entity longer matches search criteria; otherwise stays same. entity referenced entity pointer used form ﬁnal system response described section policy network policy network viewed glue binds system modules together. output single vector representing system action inputs comprised intent network belief state truth value vector since generation network generates appropriate sentence forms individual probabilities categorical values informable belief state immaterial summed together form summary belief vector slot represented three components summed value probabilities probability user said \"don’t care\" slot probability slot mentioned. similarly truth value vector number matching entities matters identity. vector therefore compressed -bin -hot encoding represents different degrees matching finally policy network output generated three-way matrix transformation generation network uses action vector condition language generator generates template-like sentences token token based language model probabilities based given ontology designed webpages amazon mechanical turk wizards users users given task specifying characteristics particular entity must asked type natural language sentences fulﬁl task. wizards given form record information conveyed last user turn search table showing available matching entities database. note forms contain labels needed train slot-based belief trackers. table automatically updated every time wizard submits information. based updated table wizard types appropriate system response dialogue continues. order enable large-scale parallel data collection avoid distracting latencies inherent conventional scenarios users wizards asked contribute single turn dialogue. ensure coherence consistency users wizards must review previous turns dialogue contribute turns. thus dialogues progress pipe-line. many dialogues active parallel worker ever wait response party dialogue. despite fact multiple workers contribute dialogue observe dialogues generally coherent diverse. furthermore turn-level data collection strategy seems encourage workers learn correct based previous turns. paper system designed assist users restaurant cambridge area. three informable slots users constrain search requestable slots user value restaurant offered. restaurants based domain hits total roughly days collected dialogue turns. cleaning data approximately dialogues total total cost collecting dataset usd. slot value) hidden layer. output token sequence generated generic tokens replaced actual values replacing delexicalised slots random sampling list surface forms e.g. <s.food> food type food replacing delexicalised values actual attribute values entity currently selected pointer. similar spirit latent predictor network token generation process augmented pointer networks transfer entity speciﬁc information response. attentive generation network instead decoding responses directly static action vector attention-based mechanism used dynamically aggregate source embeddings output step work explore attention mechanism combine tracker belief states i.e. computed output step arguably greatest bottleneck statistical approaches dialogue system development collection appropriate training data especially true task-oriented dialogue systems. serban catalogued existing corpora developing conversational agents. corpora useful bootstrapping task-oriented dialogue systems in-domain data essential. mitigate problem propose novel crowdsourcing version wizard-of-oz paradigm collecting domain-speciﬁc corpora. ﬁxed tracker parameters remaining parts model trained using cross entropy errors generation network language model output token targets predictions respectively turn output step treated dialogue batch used stochastic gradient decent small regularisation term train model. collected corpus partitioned training validation testing sets ratio early stopping implemented based validation regularisation gradient clipping hidden layer sizes weights randomly initialised including word embeddings. vocabulary size around input output rare words words delexicalised removed. used three convolutional layers cnns work ﬁlter sizes pooling operations applied ﬁnal convolution layer. order decode without length bias decoded system response based average probability tokens {log p/jt} model parameters user input length machine response. contrast also investigated criterion increase diversity additional scores delexicalised tokens encourage task completion. weighted decoding strategy following objective function weights selected validation modelled standalone lstm language model. used simple heuristic scoring function designed reward giving appropriate information penalise spuriously providing unsolicited information. applied beam search beamwidth equal search stops sentence token generated. order obtain language variability deployed model decoding obtained candidates randomly sampled system response. tracker performance table shows evaluation trackers’ performance. delexicalisation type trackers n-gram type trackers achieve high precision n-gram tracker worse recall. result suggests compared simple ngrams type trackers better generalise sentences long distance dependencies complex syntactic structures. corpus-based evaluation evaluated end-to-end system ﬁrst performing corpusbased evaluation model used predict system response held-out test set. three evaluation metrics used bleu score entity matching rate objective task success rate calculated entity matching rate determining whether actual selected entity dialogue matches task speciﬁed user. dialogue marked successful offered entity matches system answered associated information requests user. computed bleu scores template-like output sentences lexicalising entity value substitution. give additional reward requestable slot requested corresponding delexicalised slot value token generated. give additional penalty informable slot never mentioned corresponding delexicalised value token generated details scoring please table table shows result corpus-based evaluation averaging randomly initialised networks. baseline block shows baseline models ﬁrst simple turn-level sequence sequence model second introduces additional recurrence model dependency dialogue history following serban seen incorporation recurrence improves bleu score. however baseline task success matching rates cannot computed since models make provision database. variant block table shows variants proposed end-to-end model. ﬁrst requestable trackers used informable trackers. hence burden modelling user requests falls intent network alone. found without explicitly modelling user requests model performs poorly task completion even though offer correct entity time. data help here; however found incorporation explicit internal semantic representation full model efﬁcient extremely effective. second variant lstm intent network replaced cnn. achieves competitive bleu score task success still quite poor think encodes intent capturing several local features lacks global view sentence easily result unexpected overﬁt. full model block shows performance proposed model different decoding strategies. ﬁrst shows result decoding using average likelihood term second uses weighted decoding strategy seen weighted decoding strategy provide signiﬁcant improvement bleu score greatly improve task success rate term contributes improvement injects additional task-speciﬁc information decoding. despite this effective elegant improve performance attention-based mechanism dynamically aggregate tracker beliefs gives slight improvement bleu score gain task success finally improve incorporating weighted decoding attention models aside used t-sne produce reduced dimension view action embeddings plotted labelled ﬁrst three generated output words ﬁgure shown figure clear clusters based system intent types even though explicitly model using dialogue acts. order assess operational performance tested model using paid subjects recruited amazon mechanical turk. judge asked follow given task rate model’s performance. assessed subjective success rate perceived comprehension ability naturalness response scale full model attention weighted decoding used system tested total dialogues. seen table average subjective success rate means system able complete majority tasks. moreover comprehension ability naturalness scores averaged handcrafted modular baseline system consisting handcrafted semantic parser rulebased policy belief tracker templatebased generator. result seen table system achieved task success rate suggests strong baseline even though components handengineered. dialogues tested system considered better handcrafted system metrics compared. although systems achieved similar success rates system efﬁcient provided engaging conversation moreover comprehension ability naturalness system also rated higher suggests learned system perceived natural hand-designed system. supervision signals modest corpus training data. paper also presented novel crowdsourced data collection framework inspired wizard-of-oz paradigm. demonstrated pipe-lined parallel organisation collection framework enables good quality task-oriented dialogue data collected quickly modest cost. experimental assessment dialogue system showed learned model interact efﬁciently naturally human subjects complete application-speciﬁc task. best knowledge ﬁrst end-to-end nnbased model conduct meaningful dialogues task-oriented application. however still much work left current model text-based dialogue system directly handle noisy speech recognition inputs user conﬁrmation uncertain. indeed extent type model scaled much larger wider domains remains open question hope pursue work. figure user webpage. worker plays user given task follow. mturk he/she needs type appropriate sentence carry dialogue looking task description dialogue history. figure wizard page. wizard’s slightly complex worker needs dialogue history form interpreting user input turn type appropriate response based history result search result updated form submitted. form divided informable slots requestable slots contains labels need train trackers. table additional term delexicalised tokens using weighted decoding observed means corresponding tracker highest probability either mentioned dontcare value observed mean highest probability categorical values. positive score encourages generation token negative score discourages <s.food> <s.area>... informable slot token <v.food> <v.area>... informable value token requestable slot token <s.phone><s.address>... requestable value token <v.phone><v.address>... kyunghyun bart merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder–decoder statistical machine translation. emnlp pages doha qatar october. acl. jiwei michel galley chris brockett jianfeng bill dolan. diversitypromoting objective function neural conversation models. naacl-hlt pages diego california june. acl. wang ling phil blunsom edward grefenstette karl moritz hermann tomáš koˇciský fumin wang andrew senior. latent predictor networks code generation. pages berlin germany august. acl. tomáš mikolov martin karaﬁat lukáš burget ˇcernocký sanjeev khudanpur. recurrent neural network based laninterspeech pages guage model. makuhari japan. isca. milica gaši´c catherine breslin matthew henderson dongho martin szummer blaise thomson pirros tsiakoulis steve young. on-line policy optimisation bayesian spoken dialogue systems human interaction. icassp pages may. diarmuid séaghdha blaise thomson milica gaši´c pei-hao david vandyke tsung-hsien steve young. multi-domain dialog state tracking pages using recurrent neural networks. beijing china july. acl. matthew henderson blaise thomson steve young. word-based dialog state tracking recurrent neural networks. sigdial pages philadelphia june. acl. karl moritz hermann tomás kociský edward grefenstette lasse espeholt mustafa suleyman phil blunsom. teaching machines read comprehend. nips pages montreal canada. press. iulian vlad serban alessandro sordoni yoshua bengio aaron courville joelle pineau. hierarchical neural network generative models movie dialogues. arxiv preprint.. tsung-hsien milica gaši´c nikola mrkši´c pei-hao david vandyke steve young. multi-domain neural network language generation spoken dialogue systems. naacl-hlt pages diego california june. acl. kaisheng baolin peng zhang dong geoffrey zweig yangyang shi. spoken language understanding using long shortterm memory neural networks. ieee pages december. steve young milica gaši´c simon keizer françois mairesse jost schatzmann blaise thomson hidden information state model practical framework pomdp-based spoken dialogue management. computer speech language. pei-hao david vandyke milica gasic dongho nikola mrksic tsung-hsien steve young. learning real users rating dialogue success neural networks reinforcement learning spoken dialogue systems. interspeech pages dresden germany. isca. pei-hao milica gasic nikola mrkši´c lina rojas barahona stefan ultes david vandyke tsung-hsien steve young. on-line active reward learning policy optimisation spoken dialogue systems. pages berlin germany august. acl. tsung-hsien aaron heidel hung tsao lin-shan lee. recurrent neural network based language model personalization social network crowdsourcing. interspeech pages lyon france. isca. tsung-hsien milica gaši´c dongho nikola mrkši´c pei-hao david vandyke steve young. stochastic language generation dialogue using recurrent neural networks convolutional sentence reranking. sigdial pages prague czech republic september. acl. tsung-hsien milica gaši´c nikola mrkši´c pei-hao david vandyke steve young. semantically conditioned lstm-based natural language generation spoken emnlp pages dialogue systems. lisbon portugal september. acl. restaurant area serves vietnamese food thanh binh serves vietnamese food type anything else help phone number post code number area code anything else good thank using cambridge restaurant system looking moderately priced halal food sorry halal restaurants moderate price range would like something else european food galleria serves european food type moderate price range address postcode address bridge street area code anything else help today thanks galleria european restaurant moderate price range would like phone thank thank using cambridge restaurant system good hungry indonesian food sorry indonesian restaurants would like different food indian? restaurants serving indian types food preference side town east side please sitar tandoori indian restaurant east side town would like information address phone number please sitar tandoori located high street cherry hinton cherry hinton phone number okay thank thank goodbye hello cuisine would like afghan sorry afghan restaurants would like something else korean little seoul serves korean food located regent street city centre expensive? little seoul serves korean food expensive price range area little seoul located centre area town would like number telephone number anything else help today goodbye thank using cambridge restaurant system goodbye want restaurant east part town serves creative food sorry creative restaurants east side town would like different restaurant indian food? curry prince indian restaurant east town would like phone number address phone number? curry prince located newmarket road ditton phone anything else help today thank goodbye thank using cambridge restaurant system good food interested english sorry english restaurants would like different food type chinese restaurants serving chinese type cuisine preference area south part good luck chinese food takeaway chinese restaurant south town would like phone number address price range good luck chinese food takeaway located cherry hinton road cherry hinton anything else help price range? good luck chinese food takeaway expensively priced chinese restaurant south area would like information goodbye thank using cambridge restaurant system good gastropub anywhere around here? gastropub restaurants would like information west side? sorry gastropub restaurants west side town would like something else east? royal standard gastropub restaurant east town what’s address? address mill road city centre thanks thank using cambridge restaurant system good", "year": 2016}