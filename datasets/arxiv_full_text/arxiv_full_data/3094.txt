{"title": "Unsupervised Feature Learning for low-level Local Image Descriptors", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Unsupervised feature learning has shown impressive results for a wide range of input modalities, in particular for object classification tasks in computer vision. Using a large amount of unlabeled data, unsupervised feature learning methods are utilized to construct high-level representations that are discriminative enough for subsequently trained supervised classification algorithms. However, it has never been \\emph{quantitatively} investigated yet how well unsupervised learning methods can find \\emph{low-level representations} for image patches without any additional supervision. In this paper we examine the performance of pure unsupervised methods on a low-level correspondence task, a problem that is central to many Computer Vision applications. We find that a special type of Restricted Boltzmann Machines (RBMs) performs comparably to hand-crafted descriptors. Additionally, a simple binarization scheme produces compact representations that perform better than several state-of-the-art descriptors.", "text": "unsupervised feature learning shown impressive results wide range input modalities particular object classiﬁcation tasks computer vision. using large amount unlabeled data unsupervised feature learning methods utilized construct high-level representations discriminative enough subsequently trained supervised classiﬁcation algorithms. however never quantitatively investigated well unsupervised learning methods low-level representations image patches without additional supervision. paper examine performance pure unsupervised methods low-level correspondence task problem central many computer vision applications. special type restricted boltzmann machines performs comparably hand-crafted descriptors. additionally simple binarization scheme produces compact representations perform better several state-of-the-art descriptors. paper tackle recent computer vision dataset viewpoint unsupervised feature learning. another dataset? already enough datasets serve well evaluating feature learning algorithms. particular feature learning image data several well-established benchmarks exist caltech- cifar- norb name few. notably benchmarks object classiﬁcation tasks. unsupervised learning algorithms evaluated considering well subsequent supervised classiﬁcation algorithm performs high-level features found aggregating learned low-level representations think mingling steps makes difﬁcult assess quality unsupervised algorithms. direct needed evaluate methods preferably subsequent supervised learning step completely optional. odds methodology evaluating unsupervised learning algorithms. general object classiﬁcation tasks always based orientationscale-rectiﬁed pictures objects themes ﬁrmly centered middle. looking dataset possible show unsupervised feature learning beneﬁcial wide range computer vision tasks beyond object classiﬁcation like tracking stereo vision panoramic stitching structure motion. might argue object classiﬁcation acts good proxy tasks hypothesis shown correct either theoretically empirical evidence. instead chose general direct task applied low-level representations matching representations i.e. determining data samples similar given learned representation. matching image descriptors central problem computer vision hand-crafted descriptors always evaluated respect task given dataset labeled correspondences supervised learning approaches representations accompanying distance metric optimized respect induced similarity measure. remarkable hand-engineered descriptors perform well task without need learn measure representations supervised manner. best knowledge never investigated whether many unsupervised learning algorithms developed last couple years match performance without relying supervision signals. propose additional benchmark unsupervised learning algorithms introduce learning algorithm. rather investigate performance gaussian sparse variant mean covariance without supervised learning respect matching task. turns mcrbm performs comparably hand-engineered feature descriptors. fact using simple heuristic mcrbm produces compact binary descriptor performs better several state-of-the-art hand-crafted descriptors. begin brief description dataset used evaluating matching task followed section details training procedure. section present results quantitatively qualitatively also mention models tested analyzed overall performance. section concludes brief summary outlook future work. review grbms spgrbms mcrbms provided appendix section completeness. related work similar spirit work like interested behavior unsupervised learning approaches without supervised steps afterwards. whereas investigate high-level representations. learns compact binary representation deep autoencoder order fast content-based image search again representations studied respect capabilities model high-level object concepts. additionally various algorithms learn high-level correspondences studied recent years. finding low-level image descriptors excellent machine learning task even hand-designed descriptors many free parameters cannot optimized manually. given ground truth data correspondences performance supervised learning algorithms impressive recently boosted learning image gradient-based weak learners shown excellent results dataset used paper. section related work space supervised metric learning. heart paper recently introduced dataset discriminative learning local image descriptors attempts foster learning optimal low-level image representations using large realistic training patch correspondences. dataset based million image patches three different scenes statue liberty notre dame yosemite’s half dome patches sampled around interest points detected difference gaussians normalized respect scale orientation. shown figure dataset wide variation lighting conditions viewpoints scales. dataset contains also approximately million image correspondences. correspondences between image patches established dense surface models obtained stereo matching exact procedure establish correspondences involved described detail actual correspondences used identiﬁed patch correspondences show substantial perspective distortions resulting much realistic dataset previous approaches dataset appears similar earlier benchmark authors correspondences novel dataset resemble much harder problem. error rate detection correct matches sift descriptor raises error rate evaluating patch similarity pixel space raises least respectively) figure patch correspondences liberty dataset. note wide variation lighting viewpoint level detail. patches centered interest points otherwise considered random e.g. reasonable notion object boundary possible. figure taken example. order facilitate comparison various descriptor algorithms large predetermined match/non-match patch pairs provided. every scene sets comprising pairs available. don’t argue dataset subsumes substitutes previously mentioned benchmarks. instead think serve complement those. constitutes excellent testbed unsupervised learning algorithms experiments considering self-taught learning effects semi-supervised learning supervised transfer learning input distributions varying degree similarity effect enhancing dataset arbitrary image patches around keypoints conducted controlled environment. furthermore end-to-end trained systems classiﬁcation problems evaluated respect type data distribution task. different models trained unsupervised fashion available patches. train scene evaluate performance test every scene. allows investigate self-taught learning paradigm also train three scenes jointly evaluate every scene individually. grbm spgrbm differ setting sparsity penalty settings same. compute approximate gradient log-likelihood recently proposed rmsprop method gradient ascent method. compared standard minibatch gradient ascent rmsprop efﬁcient method respect training time necessary learn good representations takes half training time necessary standard minibatch gradient ascent. learning parameters ﬁrst scale image patches pixels. preprocess training samples subtracting vectors’ mean dividing standard deviation elements. common practice visual data corresponds local brightness contrast normalization. gives also theoretical justiﬁcation preprocessing step necessary learn reasonable precision matrix preprocessing scheme allows grbm spgrbm achieve good results. addition important learn λ—setting identity matrix common practice also produces dissatisfying error rates. note originally considered learning mostly important wants good density model data. grbm spgrbm hidden units. elements initialized according biases initialized rmsprop uses learning rate decay factor minibatch size train models epochs spgrbm sparsity target sparsity penalty spgrbm sensitive settings —setting high results dead representations results deteriorate drastically. mcrbm training performed using code resample patches pixels. samples preprocessed subtracting mean followed whitening retains variance. overall training procedure identical described train architectures total epochs however updating started epoch consider different mcrbm architectures ﬁrst mean units factors covariance units. constrained ﬁxed topography. denote architecture mcrbm. second architecture concerned learning compact representations mean units factors covariance units. initialized two-dimensional topography takes neighborhoods factors stride equal denote model mcrbm. consumer grade takes hours train ﬁrst architecture samples hours train second architecture number samples. results presented section follow evaluation procedure every scene notredame half dome labeled dataset image pairs assess quality trained model scene. order save space present curves show results terms error rate percent incorrect matches true matches found computing respective distances pairs test threshold determined matching pairs distance threshold. non-matching pairs distance threshold considered incorrect matches. table consists subtables. table presents error rates grbm spgrbm mcrbm limitations size representations placed. table considers descriptors overall small memory footprint. grbm spgrbm activations hidden units given preprocessed input patch descriptor accordance manually designed descriptors. many rely distributions intensity gradients edge directions structural information encoded covariance units table error rates i.e. percent incorrect matches true matches found. numbers grbm spgrbm mcrbms given within ±.%. every subtable indicated entry method column denotes descriptor algorithm. descriptor algorithms require learning represented line. numbers columns labeled error rates method respective test scene. supervised algorithms evaluated scene trained training ly/nd/hd encompasses million patches three scenes; setting possible unsupervised learning methods. error rates several unsupervised algorithms without restricting size learned representation. grbm spgrbm mcrbm learn descriptors dimensionality denotes error rates method respect normalization descriptor distance. results compact descriptors. brief brisk binary descriptors surf real valued descriptor dimensions. binboost itq-sift d-brief learn compact binary descriptors supervision. numbers brief brisk surf binboost itq-sift distance widely used comparing image descriptors. considering generative nature models follow general argumentation choose manhattan distance denoted text also consider normalization schemes patch representations given visible input grbm mcrbm compute features resemble parameters independent bernoulli random variables. therefore consider jensen-shannon divergence alternative similarity measure. finally binary descriptors hamming distance. sift landmark image feature matching. good performance important basic ingredients many different kinds computer vision algorithms. serves baseline evaluating models. vlfeat compute sift descriptors. performance sift descriptor -normalized reported table ﬁrst entry. normalization provides better results normalization normalization all. sift performs descriptor sampling certain scale relative difference gaussians peak. order achieve good results essential optimize scale parameter every dataset. table concerned evaluating compact descriptors ﬁrst entry shows performance sift used -byte descriptor distance. table shows sift performs better three unsupervised methods. mcrbm performs similar sift trained half dome albeit cost times larger descriptor representation. compact binary descriptor based mcrbm performs remarkably well comparable even better several state-of-the-art descriptors table last entry. discuss detail several aspects results following paragraphs. grbm spgrbm spgrbm performs considerably better non-sparse version necessarily expected unlike e.g. classiﬁcation sparse representations considered problematic respect evaluating distances directly. lifetime sparsity beneﬁcial setting compared strictly enforced population sparsity. plan investigate issue detail future work comparing spgrbm cardinality restricted boltzman machines dataset. self-taught paradigm would expect performance model trained liberty dataset evaluated notre dame scene noticeably better performance model trained half dome evaluated architectural datasets. however observe. particular mcrbm opposite training natural scene data leads much better performance assumed optimal setting. jensen-shannon divergence grbm spgrbm perform poorly jensenshannon divergence similarity therefore don’t report numbers table. similar results mcrbm equally bad. however scales constant results respect improve noticeably table last entry. performance half dome dataset still good scaling factor learned also plan future work. compact binary descriptor successful ﬁnding good compact representation either grbm spgrbm. finding compact representations kind input data done multiple layers nonlinearities even layers learn relatively good compact descriptors. features binarized representation made even compact order suitable binarization threshold employ following simple heuristic training dataset ﬁnished histogram activations training median histogram threshold. brieﬂy comment developed ﬁlters unsurprisingly spgrbm mcrbm learn gabor like ﬁlters. closer look make interesting observations figure shows diagonal elements spgrbm. computing latent representation input scaled matrix which visualized image resembles gaussian dented center location keypoint every image patch. mcrbm also builds ﬁlters around keypoint figure shows unusual ﬁlters centered around keypoint bear strong resemblance discriminative projections learned supervised dataset typical ﬁlters learned spgrbm. filters mcrbm. pixelwise inverted standard deviations learned spgrbm plotted image input patch elementwise multiplied image computing latent representation. ﬁgure generated training patches better visibility qualitative results appear patches. mcrbm also learns variants log-polar ﬁlters centered around keypoint. similar ﬁlters found optimizing correspondence problem supervised setting. several ﬁlters shown subﬁgure taken finally basic keypoint ﬁlters combined garbor ﬁlters placed close center; garbor ﬁlters systematically arranged around keypoint ﬁlters. figure qualitatively ﬁlters figure resemble log-polar ﬁlters used several state-of-the-art feature designs focused keypoint ﬁlters often combined gabor ﬁlters placed vicinity center garbor ﬁlters appear center. mcrbm trained ﬁxed topography sees gabor ﬁlters systematically arranged around keypoint also trained several unsupervised feature learning models grbm nonlinear rectiﬁed hidden units various kinds autoencoders denoising autoencoders) kstart paper suggesting unsupervised feature learning evaluated without using subsequent supervised algorithms directly respect capacity good low-level image descriptors. recently introduced dataset discriminatively learning low-level local image descriptors proposed suitable benchmark evaluation scheme complements nicely existing benchmarks. demonstrate mcrbm learns real-valued binary descriptors perform comparably even better several state-of-the-art methods dataset. future work plan evaluate deeper architectures combined sparse convolutional features dataset. moreover ongoing work investigates several algorithms supervised correspondence learning presented dataset.", "year": 2013}