{"title": "A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale  SVM Training", "tag": ["cs.CV", "cs.AI", "cs.LG", "math.OC", "stat.ML"], "abstract": "Recently, there has been a renewed interest in the machine learning community for variants of a sparse greedy approximation procedure for concave optimization known as {the Frank-Wolfe (FW) method}. In particular, this procedure has been successfully applied to train large-scale instances of non-linear Support Vector Machines (SVMs). Specializing FW to SVM training has allowed to obtain efficient algorithms but also important theoretical results, including convergence analysis of training algorithms and new characterizations of model sparsity.  In this paper, we present and analyze a novel variant of the FW method based on a new way to perform away steps, a classic strategy used to accelerate the convergence of the basic FW procedure. Our formulation and analysis is focused on a general concave maximization problem on the simplex. However, the specialization of our algorithm to quadratic forms is strongly related to some classic methods in computational geometry, namely the Gilbert and MDM algorithms.  On the theoretical side, we demonstrate that the method matches the guarantees in terms of convergence rate and number of iterations obtained by using classic away steps. In particular, the method enjoys a linear rate of convergence, a result that has been recently proved for MDM on quadratic forms.  On the practical side, we provide experiments on several classification datasets, and evaluate the results using statistical tests. Experiments show that our method is faster than the FW method with classic away steps, and works well even in the cases in which classic away steps slow down the algorithm. Furthermore, these improvements are obtained without sacrificing the predictive accuracy of the obtained SVM model.", "text": "recently renewed interest machine learning community variants sparse greedy approximation procedure concave optimization known frank-wolfe method. particular procedure successfully applied train large-scale instances non-linear support vector machines specializing training allowed obtain eﬃcient algorithms also important theoretical results including convergence analysis training algorithms characterizations model sparsity. paper present analyze novel variant method based perform away steps classic strategy used accelerate convergence basic procedure. formulation analysis focused general concave maximization problem simplex. however specialization algorithm quadratic forms strongly related classic methods computational geometry namely gilbert algorithms. theoretical side demonstrate method matches guarantees terms convergence rate number iterations obtained using classic away steps. particular method enjoys linear rate convergence result recently proved quadratic forms. practical side provide experiments several classiﬁcation datasets evaluate results using statistical tests. experiments show method faster method classic away steps works well even cases classic away steps slow algorithm. furthermore improvements obtained withsacriﬁcing predictive accuracy obtained model. problem encompasses several models used machine learning including hard-margin support vector machines l-loss svms binary classiﬁcation regression novelty detection noted researchers methods focus paper diﬀerent ﬁelds approximate solutions problem obtained using quite simple iterative procedures. instance yildirim presents iterative algorithms task approximating minimum enclosing ball points. ahipasaoglu propose similar methods solve minimum volume enclosing ellipsoid problems. zhang studies similar techniques convex approximation estimation mixture models. methods nowadays identiﬁed variants general approximation procedure maximizing diﬀerentiable concave function simplex traces back frank wolfe recently analyzed clarkson jaggi modern perspective. nutshell iteration method moves solution towards direction along linearized objective function increases rapidly still feasible. procedure related idea coreset coined context computational geometry denoting subset data suﬃces obtain approximation solution whole dataset given precision clarkson’s framework uniﬁes diverse results regarding existence small coresets diﬀerent instances problem ideas used characterize sparsity svms convergence properties training algorithms geometric formulations problem. algorithm studied paper obtained incorporating type away step basic method. loosely speaking instead moving solution towards direction along linearized objective function increases away step moves solution away direction along linearized objective function decreases. strategy suggested wolfe improve convergence rate method leading variant original algorithm called modiﬁed frank-wolfe method demonstrated linearly convergent general assumptions properties problem however found classic away steps improve signiﬁcantly running times method machine learning problems. similar conclusion obtained ouyang gray contrast approach experimentally improves methods shows theoretical guarantees least good mfw. applications learning training non-linear svms large datasets challenging eﬀective interior point methods devised special circumstances kernels admit low-rank factorizations however methods suitable large-scale problems general scenario mainly memory constraints general interior point method needs memory time matrix inversions prohibitive even medium-scale problems. among traditional methods devised cope problem active methods sequential minimal optimization well-known alternatives among practitioners. indeed algorithms choice widely known libraries svmlight libsvm respectively. linear kernel case stochastic gradient descent specialized sub-gradient methods like pegasos stochastic dual coordinate ascent lately gained popularity community approximate eﬃcient alternatives classic solutions large-scale problems non-linear case eﬀective methods deal large datasets recently devised focusing formulations problem applying methods. ﬁrst work specialize variant method training probably tsang given labelled examples {+−} denotes input space index adopt so-called lsvm formulation model built solving following optimization problem yiyjk yiyj δij/c kernel function used model regularization parameter problem clearly problem formulation adopted mainly eﬃciency using functional eqn. possible exploit framework introduced developed solve learning problem easily. note also problem positive deﬁnite thus strictly concave. borrowing coreset-based algorithm computational geometry authors obtain total number iterations needed identify coreset i.e. approximation l-svm model arbitrary precision bounded independently size dataset. iterative structure algorithm follows easily size coreset also bounded similar result regarding linear svms trained sdca recently demonstrated strictly speaking special case method address general form problem normalization constraint quadratic form required this easily seen writing positive semi-deﬁnite matrices multiple identity column vector whose components labels gram matrix ˜kij hadamard componentwise product. latter properties imply particular size examples required represent solution i.e. number support vectors model also independent size dataset improvement previous lower bounds support size bound grows linearly size dataset. obtained training algorithm also exhibits linear running times number examples. remarkable results context non-linear models support needs explicitly stored memory implement predictions determines cost classiﬁcation decision terms testing time. addition combination procedure certain sampling techniques allows obtain sub-linear time approximation algorithms practice method found competitive well-known software using non-linear kernels several papers recently stressed eﬃciency coresetbased methods machine learning. authors investigate direct application method large-scale non-linear training demonstrating running times signiﬁcantly improved long minor loss accuracy acceptable. variations algorithm based geometrical reformulations learning problem stochastic variants method applications training data streams structural svms also proposed. contributions present method endowed type optimization step devised overcome diﬃculties observed classic approach preserving intuition beneﬁts behind introduction away steps. theoretical side formulate analyze algorithm general case problem demonstrating method matches guarantees terms convergence rate number iterations obtained using classic away steps. particular show method converges linearly optimal value objective function achieves predetermined accuracy iterations. focusing quadratic objectives turns method strongly related gilbert mitchell-demnyanov-malozemov algorithms classic methods computational geometry methods well-known machine learning properties particular rate convergence focus recent research practical side specialize algorithm training perform detailed experiments several classiﬁcation problems. conclude algorithm improves running times existing approaches without statistically signiﬁcant diﬀerence terms prediction accuracy. particular show method faster methods statistically faster addition show rigorous probability identifying good point given iteration depends size sampling. sub-linear procedure guaranteeing constant success probability studied though seems results non-linear case provided kernels. method faster equal method signiﬁcantly slower i.e. classic away steps fail. addition method competitive signiﬁcantly slower i.e. classic away steps work algorithm works well. thus method represents robust alternative implement away steps enjoying strong theoretical guarantees providing signiﬁcant improvements practice. organization paper organized follows. section give overview methods introduce basic concepts required analysis. section present method including minor variant provide details specialization svms. analysis convergence provided section section discuss relation proposed method classic geometric approaches quadratic objective. experiments problems presented section finally section closes paper concluding remarks. addition technical results required proofs section reported appendix. notation optimal solution problem denoted sequence approximations solution problem abbreviated {αk}k. indices denoted face unit simplex corresponding indices subset points term active face indicates face corresponding non-zero indices current solution term optimal face denoted indicates face corresponding optimal solution vector denotes i-th vector canonical basis. frank-wolfe methods method computes sequence approximations {αk}k solution problem iterating convergence following steps. first linear approximation current iterate performed order ascent direction since lies easy linear approximation step reduces largest coordinate gradient i.e. argmaxi ∇gi. iterate moved towards seeking best feasible improvement objective function. procedure summarized algorithm rest paper refer ascent vertex used method. optimality measures stopping condition shown method globally convergent rather weak assumptions properties objective function guaranteed hold problem addition shown iterates procedure satisfy constant related second derivative convergence rate slow compared methods. however simplicity procedure implies amount computation iteration usually small. kind tradeoﬀ favorable large-scale applications testiﬁed example widespread adoption method context svms multiplicative constant primal-dual eqn. primal measure approximation eqn. metrics employed analyze convergence algorithm advantage respect former depend optimal value objective function. therefore explicitly monitored execution algorithm adopted implement stopping given tolerance parameter. note strong duality condition implies therefore algorithm stops iteration also note also eqn. implies method ﬁnds solution fullﬁling iterations. clarkson recently shown also iterates thus solution found method using stopping condition guaranteed close optimum primally dually iterations. analysis presented paper make following notion ﬁrst condition guarantees ∆-approximate solution close optimum primally dually. addition second condition ensures active face primal-dual computed active coordinate largest computed among coordinates gradient. implies also solution almost optimal face simplex deﬁned non-zero indices. sparsity solutions coresets main points interest method sparsity solutions ﬁnds. observed that contrast methods projected reduced gradient methods algorithm modiﬁes coordinate previous iterate step. starting solution non-zero coordinates iterate non-zero entries. therefore previous remarks convergence method show exist solutions space-complexity good approximations problem even much larger. properties essential context training non-linear svms. case non-zero coordinate represents support training example needs explicitly stored memory execution algorithm. addition test complexity non-linear svms proportional number non-zero coordinates determines cost iteration training time cost classiﬁcation decision testing time. existence sparse approximate solutions problem linked idea ε-coreset ﬁrst described geometrical problems ε-coreset property smallest ball containing expanded factor resulting ball contains problem solved solution close solution existence ε-coresets size problem ﬁrst demonstrated bădoiu clarkson note large-scale applications much smaller cardinality clarkson provides deﬁnition coreset applies general setting problem basically ε-coreset problem subset indices spanning face compute good approximate solution. existence small ε-coresets implies existence sparse solutions optimal respective active faces. practical consequence result would possibility solving large instances working small variables original problem. deﬁnition ε-coreset problem indices solution discussed method guaranteed ε-coreset iterations problem demonstrated able coreset special cases e.g. polytope distance problems however general iterations required. instead computationally intensive modiﬁcation presented algorithm generally known fully corrective variant job. note algorithm needs solve optimization problem increasing size iteration. considered generalized version well-known bădoiu-clarkson method compute mebs computational geometry knowledge corresponds ﬁrst variant method applied problems boosting convergence using away-steps well-known method often exhibits tendency stagnate near solution resulting slow convergence rate discussed problem explained geometrically. near solution gradient tendency become nearly orthogonal face simplex spanned therefore little improvement achieved moving towards ascent vertex however since solution optimal reasonable think solution improved working face spanned actually algorithm works approximate optimality exploring next ascent direction. shown convergence method boosted introducing type optimization step. short idea that instead moving towards point maximizing local linear approximation move away point current face minimizing iteration choice options made determining directions promising. since point must current active face easy linear approximation step reduces smallest active coordinate gradient i.e. argminj∈ik ∇gj. whole procedure known modiﬁed frank-wolfe method summarized algorithm rest paper refer descent vertex away direction used method respectively. perform line-search argmaxλ∈ update {i∗}. perform line-search λaway argmaxλ∈ clip line-search parameter λaway max) perform away step λaway. {i∗}. λaway αkj∗ {j∗}. contrast method sub-linear rate convergence expected general shown asymptotically exhibits linear convergence solution problem assumptions form objective function feasible addition algorithm potential compute sparser solutions practice since contrast method allows reducing coordinates step. adaptations svms context learning work tsang arguably ﬁrst point properties algorithms obtained applying methods formulations ﬁtting problem work relies equivalence problem problem holds normalization assumption kernel function employed model exploiting equivalence adapting bădoiu-clarkson algorithm computing problem training non-linear svms algorithm called core vector machine obtained enjoys remarkable theoretical properties competitive performance practice first number support vectors model obtained constant tolerance parameter method. therefore space complexity model independent size dimensionality training set. second number iterations algorithm termination also independent size dimensionality training set. determine overall time complexity method note algorithm requires search point representing best ascent direction current approximation objective function operation also performed methods. searching among training points requires number kernel evaluk mqk) cardinality since ations order obtain overall time complexity linear number examples improving super-linear time complexity reported empirically popular methods like train svms large however complexity iteration still become prohibitive practice. sampling technique called probabilistic speedup proposed overcome obstacle. technique also used implement leading training algorithms overall time complexity independent number training examples. practice index computed random subset coordinates constant. overall complexity iteration thereby reduced order major improvement previous estimate since generally refer details speed-up technique. methods task training svms. advantage algorithms algorithm rely analytical steps. result training iteration becomes signiﬁcantly cheaper iteration depend external numerical solver. practice training algorithm might probably require iterations order obtain solution within predeﬁned tolerance criterion work iteration signiﬁcantly smaller. trade-oﬀ shown worthwhile dealing large-scale applications authors show adopting algorithms running times signiﬁcantly improved long minor loss accuracy acceptable. analysis presented possible conclude approach enjoys similar theoretical guarantees namely linear time number examples number iterations independent number examples. sampling technique speed-up computation introduced used methods well order obtain overall time complexities independent number training patterns. closely related work kumar yildirim present specialization method problems adopting geometrical formulation studied approach reformulates problem minimum polytope distance problem. obtained method properties also strongly related work gartner jaggi authors authors show method well coreset framework introduced applied currently used hard soft margin variants arbitrary kernels obtain approximate training algorithms needing number iterations independent number attributes training examples. ouyang gray propose stochastic variant methods online learning l-svms obtaining comparable sometimes better accuracies state-of-the-art batch online algorithms training svms. similar technique recently proposed allow smooth general online convex optimization sub-linear regret bounds variants method proposed introduced training svms data streams. authors adapted method train svms structured outputs like graphs combinatorial objects obtaining algorithm outperforms competing structural solvers. described previous sections basic method modiﬁed order avoid stagnation near solution obtaining algorithm guaranteed rate convergence. previous remarks method suggest algorithm terminate faster sparser solutions. practice however method always fast could expect theory. instance experimental results reported minimum volume enclosing ellipsoid problems respectively show tight improvements obtained using enhanced method respect basic approach. concerns problem training svms results conﬁrm using statistical tests systematically better indeed sometimes slower. similarly authors argue away steps provide clear advantage respect standard method. possible interpretation results given looking implements away steps keep feasibility i.e. ensure satisﬁed. basic idea approach include alternative getting away descent vertex current face decreasing j∗-th weight instead going toward ascent vertex would increase i∗-th weight choice mutually exclusive. algorithm decides work around lose opportunity explore promising direction feasible space vice-versa. hand away step performed weights active vertices uniformly scaled keep feasibility. scheme considerably perturb current approximation since weights modiﬁed importantly increase weights vertices belong optimal face away steps method thus prone increase need away steps eliminate spurious points here introduce type away step devised circumvent problems preserving advantages mfw. discuss variants method obtained using ﬁrst second order approximations objective function iteration respectively. away step increase weight vertices active face corresponding descent vertices. points correspond spurious points need removed active face reach optimal face problem. away step moves current solution away direction simultaneously direction toward step. moves away descent vertex also gets closer ascent vertex iteration. step actually written superposition separate steps ﬁrst term right-hand side represents standard toward step method second term away step considered approach. note term disappears components corresponding updated leaving rest current solution unchanged. type away step called swap step substitutes away steps algorithm procedure summarized algorithm note deliberately include steps represent computational tasks deﬁnitions simplify convergence analysis next section. choose type step perform criterion cannot employed method. method employs ﬁrst order approximation current iterate predict value objective function next iterate. denotes search direction computed. step gives largest value selected. however swap step always gives larger value value obtained using toward step. indeed value using swap step clip line-search parameter λswap λswap αkj∗ mark iteration swap-drop step. λswap λswap mark iteration swap-add step. perform swap step λswap. {i∗}. swap-drop step performed {j∗}. always larger ∇gj∗ swap step would always since preferred using ﬁrst-order information predict objective function value. address problem observe method computes exact line-search search direction selected using thus formulate method computing line-search deciding type step perform. design requires perform line-searches instead one. however estimation objective function value next iterate accurate. discuss section regarding adaptation procedure problem computation particularly simple objective function problem computations analytical. furthermore exact computation δswap involve terms already computed figure sketch search directions used swap methods. representation search directions explored algorithms dmfw dswap respectively. note swap eﬀective reducing weight descent vertex however also descent vertex update side eﬀect increasing weight another descent vertex. avoided swap update increases weight corresponding best ascent vertex. methods introduced previously make ﬁrst-order approximations objective function order determine direction toward current iterate moved. here consider possibility using second-order information. assume objective function twice diﬀerentiable second-order taylor approximation neighborhood hessian matrix negative semi-deﬁnite. finding best ascent direction would thus require computation quadratic form dt∇gd. since matrix highly dense usually case applications employing ﬁrst order relaxation frankwolfe methods makes sense order obtain lighter iterations. however note search direction swap step dswap yields particularly simple expression order determine best pair thus need evaluate three entries hessian matrix. however still computationally hard task iteration since would need consider m|ik| pairs points order take step. thus adopt strategy used second-order version proposed ascent index ﬁrst-order swap search index active maximizes improvement second order approximation call obtained procedure second-order swap denote swap-o next sections. worth note approximation exact quadratic objective functions case problem note also case line-search along ascent direction deﬁned closed-form solution. indeed again negative semi-deﬁniteness non-negative. naturally need restrict value interval order obtain feasible solution next step. thus modify algorithm speciﬁed algorithm notes adaptation training provide analytical expressions computations required algorithm algorithm applied problem similar expressions follow quadratic objective function. exception term ki∗j∗ computations already performed compute choose descent vertex conclude that compared procedure swap method adapted problem involves computation additional term entry kernel matrix deﬁning problem. start demonstrating global convergence swap method. then analyze rate convergence towards optimum. purpose adapt analysis presented ahipasaoglu using framework using observations concerning improvement objective function iteration swap method able prove algorithm converges linearly optimal value objective function. theoretical point view results show swap enjoys mathematical properties method. finally provide bounds number iterations required fulﬁll stopping condition eqn. demonstrate algorithm stops iterations independently number variables coincides number training examples problem provide proofs ﬁrst-order swap method described algorithm however convergence results follow easily secondorder variant well. statements proofs technical results used section found appendix. hypotheses imposed yildirim ahipasaoglu study convergence frank-wolfe methods problem minimum volume enclosing ellipsoid problem respectively. remark robinson’s condition general version classical second order suﬃcient condition solution isolated local extremum i.e. locally unique referring case constrained maximization problem concave objective result requires conditions fulﬁlled point hessian lagrangian function behaves negative deﬁnite matrix along directions belonging critical cone point specialized quadratic problem simplex i.e. problem form condition assumes form additional analysis plays role convergence analysis essentially describes conditions stationary points small perturbation problem neighborhood solution original problem. also step proofs linear convergence provided method. however assumptions diﬃcult satisfy practice. particular quite strong assumption cannot guaranteed general. note assumption implies mean value theorem also shown that problem strongly concave strong suﬃcient condition robinson holds i.e. implies particular satisﬁed wolfe dual l-svm problem. fact used kumar yildirim demonstrate linear convergence method problems match quadratic objectives. convergence method specialized quadratic program also studied implicit assumption demonstrate linear convergence gram matrix involved quadratic form positive deﬁnite. remark easy that quadratic objectives linear constraints implies robinson condition. recently linear convergence variants method convex optimization polytopes demonstrated ingredient proof global convergence proposition suppose hypothesis satisﬁed. starting feasible algorithm produces series iterates {αk}k converges solution problem unique {αk}k converges proof. observation swap search directions algorithm satisfy note result holds swap algorithm note also convergence analysis purposes assume suﬃciently large. follows fact algorithm converges globally iterate generated algorithm always feasible. ﬁrst fact follows becomes arbitrarily small suﬃciently large second fact follows since improvement objective function iteration algorithm quantity iteration onwards lower predeﬁned constant particular note that thus lemma states suﬃciently large iterate produced swap-add step ∆-approximate solution proposition suppose hypotheses hold. solution problem then suﬃciently large iteration marked swap-add algorithm produces iterate satisfying inequality proof. lemma shows suﬃciently large iterate produced algorithm swap-add step ∆-approximate solution lδk. addition since swap globally convergent chosen arbitrarily small. thus large enough conditions lemma hold lδk. eqn. exists constant proof. number steps number swap-add steps number swap-drop steps number steps include points coreset steps swap-add steps points coreset. sometimes include points sometimes not. clearly thus previous inequality number steps drop points coreset cannot greater number steps points coreset plus number points coreset initialization combining last inequalities leads +a+i therefore proposition states exist subsequence iterates {αk}k produced algorithm {g}k converges linearly optimal value objective function problem subsequence obtained dropping {αk}k iterates corresponding swap-drop steps objective function value decrease. thanks proposition know steps aﬀect overall complexity bound number iterations needed achieve given accuracy. note converse true. algorithm stop even improvement objective function last iteration greater happens proposed termination criterion fundamin mentally looks possible improvement standard steps. proof. denote number iterations algorithm ﬁrst iterate primal-dual satisﬁes ﬁrst satisﬁes since total improvement objective function cannot greater improvement objective function given swapadd step least lemma bound follows multiplying factor comes fact total number iterations times number swap-add iterations note previous proof based minimal improvement step. results holds general swap step performed unconstrained swap yields larger improvement. also possible provide logarithmic bound however case multiplicative additive constants depend thus result alone cannot infer important property overall complexity algorithm bounded independently problem size. furthermore comparable larger guarantee obtained bound tighter given eqn. proof result state completeness found last years number authors proposed training svms ﬁrst reducing task computational geometry problem applying dedicated algorithm obtain exact approximate solution. approaches indeed specialized versions methods. instance so-called gilbert method used train svms approaching task minimum norm problem nearest point problem noted method equivalent gilbert method geometric problems. thus implementation details specializations gilbert methods problem coincide. similarly bădoiu-clarkson algorithm used train svms admit reduction minimum enclosing ball problem nowadays well-understood algorithm nothing else fully corrective method applied finally algorithms proposed direct applications methods models admit interpretation algorithms proposed corresponding application problems admit interpretation mnp. recently mitchell-demnyanov-malozemov algorithm another classic geometric algorithm solve mnps npps found essentially equivalent algorithm devised speciﬁcally svms similar quadratic programs section show method proposed paper closely related gilbert algorithms applied mnps npps. indeed speciﬁc problems swap considered gilbert method possibility performing steps. polytope problems svms problem cast instance consists ﬁnding point polytope nearest origin minimizez subject case polytope convex hull ﬁnite points product space admits following formulation matrix points arranged columns. feasible solution problem yields feasible solution original problem setting feasible space problem corresponds unit simplex objective function convex. thus instance general problem studied paper furthermore hard instance problem similarly instance space spanned points corresponds feature associated kernel used svm. therefore algorithms devised solve mnps adapted train l-svms vice-versa. equivalence ﬁrst pointed used build training algorithm formulations admit similar geometric interpretations table instance hard-margin svms shown equivalent i.e. problem computing pair nearest points polytopes minimizezz subject variants l-svm considered transformed classic hard-margin thus admit interpretation soft margin l-svms ν-svms essentially npps additional constraints form authors equivalences gilbert algorithms classic iterative methods used solve mnps gilbert algorithm mitchell-demnyanovmalozemov algorithm methods easily adapted solve npps means called minkowski diﬀerence trick iteration gilbert algorithm problem written λkzi∗ chosen performing exact line-search minimize shown specializations gilbert algorithm method problem coincide. note ∇ggi zαk. therefore applied iteration method written αk+λkei∗ argmaxi. setting iteration translates λkzi∗ obtained line search argmini exactly gilbert method does. gilbert method algorithm updates towards direction given point zi∗. however search direction determined using additional point polytope corresponding iteration algorithm written computed line search. recently shown well-known algorithm devised train svms solve similar quadratic programs essentially equivalent considering svms reduce npps. diﬀerence that npps chooses polytope correspond class problem. instead choose diﬀerent classes. diﬀerence however disappears applications mnps since case geometric problem involves polytope. corresponds away vertex used swap method. thus algorithm updates current iterate using vertices polytope would considered specialization swap problem equivalent problem zj∗. swap goes toward step identical method iteration. equivalence gilbert methods conclude swap identical gilbert iteration decides explore away direction. otherwise search direction used swap simplex translates dswap mnp. update problem corresponds updating computing line search. exactly direction procedure step size used mdm. conclude that applied polytope problems swap equivalent gilbert method possibility performing steps. sense swap kind hybrid gilbert-mdm presented analyzed problems beyond npp. considering equivalence problems also state problems swap method possibility performing steps. however again presentation analysis limited quadratic forms. minor variant method using second order information uses essentially criterion proposed improve original platt’s well known introduction that general gilbert method converges sub-linearly specialization method quadratic program improve rate convergence however recently shown algorithm converges linearly assumptions structure problem include positive deﬁniteness matrix paper analysis addresses general maximization problem simplex results rely hypotheses slightly general used sense asking positive deﬁnite matrix equivalent asking positive deﬁnite hessian turn implies robinson condition used analysis. section present several experiments conducted benchmark classiﬁcation datasets evaluate performance proposed methods related approaches practice. case multi-category classiﬁcation problems adopt one-versusone approach note cases number examples necessarily reﬂect complexity training problems addressed. example according mnist datasets similar size. however mnist problem classes largest binary problem solve scheme around training examples. problem contrast binary thus whole dataset needs handled simultaneously. reason also report table size mmax largest binary subproblem size mmin smallest binary subproblem decomposition. initialization parameters initialization swap methods computation starting solution adopted method proposed approach starting solution obtained solving problem random subset training patterns. indices corresponding data points zero. used points initialization algorithms. scale parameter relatively small datasets pendigits usps parameter determined together parameter svms using -fold cross-validation logarithmic grid ﬁrst collection values corresponds parameter second parameter large-scale datasets determined using default method employed i.e. average squared distance among training patterns. parameter determined logarithmic grid using validation consisting randomly computed training-set. emphasize paper determine optimal parameter values ﬁne-tuning algorithm seek best possible accuracy. compare performance presented methods analyze behavior manner consistent theoretical analysis. therefore necessary perform experiments conditions given dataset. optimization problem solved algorithm. reason deliberately avoided using diﬀerent next sections report test accuracies training times model sizes obtained classiﬁcation problems table test accuracy intend fraction correctly classiﬁed test instances. training time time seconds required obtain model training set. times diﬀer order magnitude among diﬀerent methods logarithmic scale present ﬁgures. model size number training examples non-zero weights training process number support vectors model. obtain detailed comparison compute speed-ups obtained frank-wolfe based algorithms respect method. speed-up method respect measured training time algorithm training time method measured seconds. similarly speed-up swap swap-o methods respect measured respectively training time method swap swap-o. addition quantify diﬀerence testing performance respect method. denote accuracy accuracy method relative diﬀerence accuracy incurred quantiﬁed similarly diﬀerences testing performance corresponding methods swap swap-o measured testing accuracies swap swap-o methods respectively. computational environment experiments conducted personal computer .ghz quad core running gnu/linux. algorithms implemented based source code available experiments dataset collection dataset collection series classiﬁcation problems extracted webpage categorization dataset ﬁrst appeared platt’s paper sequential minimal optimization training svms number training patterns instance collection grows approximately number training patterns ﬁrst dataset. scheme makes series amenable studying performance scalability diﬀerent training algorithms. figures report test accuracies training times model sizes obtained collection. note times depicted logarithmic scale. figure figure conﬁrm frank-wolfe based methods slightly less accurate exhibit running times scale considerably better number training figure illustrates main points paper theoretical advantages method basic routine often correspond improvement practical performance. collection problems actually extreme case always signiﬁcantly slower contrast proposed methods faster competitive method. figure observe speed-ups method seem increase monotonically number training patterns increases ranging faster faster cvm. speed-ups corresponding method contrast signiﬁcantly limited. swap algorithm clearly competitive speed-up largest dataset. swap endow basic frank-wolfe procedure awaysteps both contrast oﬀer guarantee rate convergence. however away steps implemented swap swap-o work signiﬁcantly better collection datasets. swap-o however perform better swap series. argue standard away steps provide signiﬁcant advantage particular problem proved resulting slowest algorithm. since swap-o invests time ﬁnding good away direction ﬁnding solution takes time comparison simpler swap seems provide better compromise away toward steps. regards accuracy slightly accurate swap turn slightly accurate time. swap-o often outperforms three methods approaching accuracy cvm. note however relative diﬀerences testing accuracy time note ﬁnally less accurate among frank-wolfe based methods. concerns model sizes note additional computational time incurred swap-o methods compensated improved ability smaller models. figure actually shows faster methods swap obtain time smaller models. finally size models found signiﬁcantly larger proposed methods. addition percentage training data used method build model seem decrease signiﬁcantly series progresses. experiments adult dataset collection adult dataset collection series problems derived census database. goal predict whether individual’s income exceeded us/year based personal data. like datasets collection designed purpose analyzing scalability figures depict accuracies running times model sizes obtained collection. times depicted logarithmic scale. results conﬁrm frank-wolfe based methods tend faster algorithm number examples becomes larger. figure shows swap swap-o always faster reaching speed-ups respectively. figure shows addition times frank-wolfe based methods achieve testing performance greater equal cvm. note speed-ups obtained method experiment signiﬁcantly smaller obtained collection. largest speed-up achieved algorithm sixth dataset collection. contrast methods investigated paper swap swap-o always show speed-ups larger running faster cases. compute median speed-up among datasets collection results swap swap-o respectively. contrast method achieves median conclude proposed methods order magnitude faster basic method experiment. previous remark suggests away steps useful speed algorithm towards optimal face problem. conﬁrm observation examining performance method experiment. figure shows method always faster result contrasts previous experiment always slower conclude experiment algorithms incorporating away steps signiﬁcantly faster algorithms not. note proposed methods swap swap-o always faster mfw. regards testing accuracy time slightly less accurate frank-wolfe methods experiment. swap always obtains accuracy greater equal case accuracy greater equal mfw. swap-o time accurate mfw. conclude additional running time incurred methods compensated better accuracy series datasets. simplify presentation analysis datasets separated groups medium-scale large-scale datasets. dataset included ﬁrst group largest binary subproblem addressed lower training examples included second group otherwise. according criterion datasets letter pendigits usps reuters mnist together ﬁrst group datasets shuttle ijcnn usps-ext kdd-pc kddfull included second group. results dataset protein presented/analyzed independently accuracies training times signiﬁcantly diﬀerent results medium-scale group. note problems using experiment already used compare algorithms train svms times model sizes depicted logarithmic scale. examining figure observe slight advantage terms testing accuracy. addition conﬁrm accuracy swap swap-o methods tends closest best observed performance. method often least accurate among frank-wolfe based algorithms. note compute diﬀerence accuracy respect always obtain results lower results figure show swap swap-o methods time faster cvm. speed-up achieved methods becomes signiﬁcant size training grows peaks around largest datasets. diﬀerences among frankwolfe methods depend size problem. among medium-scale datasets methods achieve running times order magnitude. speed-ups large-scale group clearly signiﬁcant medians swap swap-o respectively. advantage methods explored paper standard routines summarized follows. methods sometimes faster swap swap-o case advantage tight. often however methods improve signiﬁcant speed-ups. particular tends signiﬁcantly outperformed cases works better. cases performance methods tends competitive better. medium-scale problems methods evenly matched performance slight advantage swap-o mfw. large-scale group swap swap-o tend outperform signiﬁcantly. results protein dataset deserve particular comment. dataset around examples distributed classes leads binary subproblems around examples. according size problem included group medium-scale datasets seen frank-wolfe algorithms obtain fairly similar small speed-ups. protein problem however methods obtain peculiar results. method achieves speed-up cvm. however standard runs faster faster suggests problem away steps signiﬁcantly help algorithm solution problem quickly. since methods tend better aways steps work observe important improvements using proposed methods. indeed respective speed-ups statistical tests section perform statistical tests assess signiﬁcance experimental results reported paper. adopt guidelines suggested ﬁrst conduct multiple test determine whether hypothesis algorithms perform equally rejected not. then conduct separate binary tests compare performances algorithm other. binary tests adopt wilcoxon signed-ranks test method. multiple test non-parametric friedman test. demsar recommends tests safe alternatives classical parametric t-tests compare classiﬁers multiple datasets. conduct binary test pair algorithms. main hypothesis paper swap method outperforms methods terms training time without signiﬁcant diﬀerences terms predictive accuracy. contrast claim signiﬁcant diﬀerences between methods observed practice also observed swap method signiﬁcantly outperforms sometimes expense little test accuracy. finally observed swap-o usually exhibits larger running times swap method outperforms based methods terms predictive power. regards comparison proposed methods apparent advantage terms running time other. thus conduct two-tailed test running times adopt one-tailed test testing accuracy. considering observations above design binary tests table table also report p-values corresponding test. reproducibility concerns p-values computed using statistical software wilcoxon signed-ranks test exact p-values preferred asymptotic ones. pratt method handle ties employed default. case friedman test iman davenport’s correction adopted suggested cases implement one-sided alternative hypotheses others two-sided tests. two-sided test preferred one-sided alternative it’s enough double p-value reported here. vice versa one-sided test preferred two-sided test it’s enough halve p-value reported here. time swap equally fast swap faster time swap equally fast swap faster time swap equally fast swap faster time equally fast diﬀerent times time swap-o equally fast diﬀerent times time swap-o p-value equally fast swap-o faster time swap-o p-value equally fast swap-o faster time swap-o swap p-value equally fast swap faster equally fast cannot rejected. adopting signiﬁcance level running times swap method found signiﬁcantly diﬀerent baseline methods null hypothesis rejected favor alternative hypothesis swap method faster. signiﬁcance level better hypotheses swap-o method fast rejected favor conclusion swap-o method faster. empirical data however insuﬃcient reject hypothesis swap-o method fast swap methods. regards testing accuracy swap found equally accurate reasonable signiﬁcance levels contrast hypothesis swap-o method similar accuracies swap rejected favor conclusion swap-o accurate. figure left testing accuracies medium-scale datasets letter pendigits usps reuters mnist. right testing accuracies large dataset collection shuttle ijcnnx usps-ext kdd-pc kdd-full. figure left model sizes medium-scale datasets letter pendigits usps reuters mnist. right model sizes large dataset collection shuttle ijcnn usps-ext kdd-pc kdd-full. figure left running times medium-scale datasets letter pendigits usps reuters mnist. right running times large dataset collection shuttle ijcnn usps-ext kdd-pc kdd-full. experiments non-normalized kernels solving classiﬁcation problem using svms requires select kernel function. since optimal kernel given application cannot speciﬁed priori capability training method work family kernels important feature. order illustrate proposed methods obtain eﬀective models even kernel satisfy conditions required conduct experiments using homogenous second order polynomial kernel xj). here parameter estimated inverse average squared figures summarize results obtained datasets used section. test accuracies training times comparable obtained using gaussian kernel. noted algorithm cannot used train using kernel selected experiment thus incorporate frank-wolfe based methods ﬁgures. results demonstrate capability methods used kernels satisfying normalization condition imposed cvm. main contribution paper twofold. theoretical side proposed variant method general problem maximizing concave function unit simplex introducing novel perform away steps method devised boost convergence. practical side demonstrated approach eﬀective improving performance state-of-the-art learners large datasets expanding research methods machine learning problems. presented variants procedure swap swap-o provided thorough theoretical analysis. first demonstrated converge globally. second showed swap swap-o asymptotically exhibit linear rate convergence case method main additional property respect standard method. finally proved achieve primal-dual lower given tolerance iterations independently dimensionality feasible space number examples problems. carried extensive performance evaluation experiments variants algorithm. obtained results demonstrated that contrast method approach provides useful robust alternative method training svms. often proposed methods swap swap-o improved performance mfw. swap method faster datasets adult collection collection protein problem. large-scale group figure swap outperformed datasets collection datasets large-scale group figure swap-o also faster protein problem slightly faster medium-scale problems figure conclusion swap swap-o faster found statistically signiﬁcant signiﬁcance levels better. often swap method improved order magnitude sometimes orders magnitude. addition cases faster advantage less signiﬁcant improvements techniques mfw. proposed methods also faster basic method several times. example swap median times faster adult collection swap-o times faster. similar results observed shuttle protein datasets. found conclusion swap faster statistically signiﬁcant critical value around contrast able reject hypothesis lead similar training times. similarly cannot conclude swap-o diﬀerent running times. cases away steps signiﬁcantly speed-up method. examples adult collection shuttle protein datasets. cases swap method competitive faster signiﬁcantly faster cases classic away steps fail. achieves cases noticeably worse running times. instance observed behavior collection usps-ext kdd-pc datasets. cases swap method clearly faster mfw. addition competitive fastest algorithm conclude swap method expected faster cases classic away steps eﬀectively boost convergence method also competitive away steps fail. thus swap robust alternative cvm. point view swap-o method less appealing. even swap-o outperforms signiﬁcantly standard away steps useful technique seems fail often cases fails. knew away steps going useful given problem swap-o would algorithm choice. however since cannot predict advance swap-o less reliable practice. point line since concave hessian matrix negative semi-deﬁnite last term always non-positive. obtain bound need bound norm simplex. largest absolute value eigenvalue matrix. therefore obtain following bound remark swap-drop step algorithm cannot bound improvement objective function clipped value step-size λswap arbitrarily small. however hard show objective function value decrease.", "year": 2013}