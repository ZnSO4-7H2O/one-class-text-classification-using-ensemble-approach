{"title": "A Generative Model of Words and Relationships from Multiple Sources", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Neural language models are a powerful tool to embed words into semantic vector spaces. However, learning such models generally relies on the availability of abundant and diverse training examples. In highly specialised domains this requirement may not be met due to difficulties in obtaining a large corpus, or the limited range of expression in average use. Such domains may encode prior knowledge about entities in a knowledge base or ontology. We propose a generative model which integrates evidence from diverse data sources, enabling the sharing of semantic information. We achieve this by generalising the concept of co-occurrence from distributional semantics to include other relationships between entities or words, which we model as affine transformations on the embedding space. We demonstrate the effectiveness of this approach by outperforming recent models on a link prediction task and demonstrating its ability to profit from partially or fully unobserved data training labels. We further demonstrate the usefulness of learning from different data sources with overlapping vocabularies.", "text": "web-scale data abundant training data available models case generic language. specialised language domains true. example medical text data often contains protected health information necessitating access restrictions potentially limiting corpus size obtainable single institution resulting corpus less tens millions sentences billions google n-grams. addition this specialised domains expect certain prior knowledge reader. doctor never mention anastrazole aromatase inhibitor example communicate sparsely assuming reader shares training terminology. cases likely even larger quantities data required sensitive nature data makes difﬁcult attain. fortunately specialised disciplines often create expressive ontologies form annotated relationships terms semantic type animal derived domain-speciﬁc knowledge anemia associated disease leukemia. observe relationships thought additional contexts co-occurrence statistics drawn; diseases associated leukemia arguably share common context even cooccur sentence would like structured information improve quality learned embeddings information content regularize embedding space cases data abundance obtaining explicit representation relationships vector space. tackle assuming relationship operator transforms words relationship-speciﬁc way. intuitively action operators distort shape space effectively allowing words multiple representations without requiring full parameters possible sense. intended effect underlying embedding twofold encourage solution sensitive domain would achieved using unstructured information heterogeneous sources information compensate sparsity data. addition neural language models powerful tool embed words semantic vector spaces. however learning models generally relies availability abundant diverse training examples. highly specialised domains requirement difﬁculties obtaining large corpus limited range expression average use. domains encode prior knowledge entities knowledge base ontology. propose generative model integrates evidence diverse data sources enabling sharing semantic information. achieve generalising concept co-occurrence distributional semantics include relationships entities words model afﬁne transformations embedding space. demonstrate effectiveness approach outperforming recent models link prediction task demonstrating ability proﬁt partially fully unobserved data training labels. demonstrate usefulness learning different data sources overlapping vocabularies. deep problem natural language processing model semantic relatedness words drawing evidence text spoken language well knowledge graphs ontologies. successful modelling approach obtain embedding words metric space semantic relatedness reﬂected closeness space. paradigm obtaining embedding neural language model traditionally draws local co-occurence statistics sequences words obtain encoding words vectors space whose geometry respects linguistic semantic features. core concept behind procedure distributional hypothesis language; sahlgren semantics inferred examining context word. relies availability large corpus diverse sentences word’s typical context accurately estimated. copyright association advancement artiﬁcial intelligence rights reserved. less-recent years. bengio described neural architecture predict next word sequence using distributed representations overcome curse dimensionality. since then much work devoted obtaining understanding applying distributed language representations. model wordvec mikolov explicitly relies distributional hypothesis semantics attempting predict surrounding context word either neighbouring words average environment note later model section idealised version skip-gram wordvec special case model relationship; appears sentence with. practice wordvec uses distinct objective function replacing full softmax approximation intended avoid computing normalising factor. retain probabilistic interpretation approximating gradients partition function allowing follow true model gradient maintaining tractability. furthermore learning joint distribution facilitates imputation generation data dealing missing data making predictions using model itself. note generative approach language also explored andreas ghahramani concern relationships. relational data also used learn distributed representations entities knowledge graphs entities correspond mapped words. general approach implicitly embed graph structure vertex embeddings rules traversing bordes scored similarity entities given relationship distance transformation using pairs relationship-speciﬁc matrices. socher describe neural network architecture complex scoring function noting previous method allow interactions entities. transe model bordes represents relationships translations motivated tree representation hierarchical relationships observations linear composition entities appears preserve semantic meaning approaches uniquely concerned relational data however consider distributional semantics free text. faruqui johansson nieto pi˜na describe methods modify pre-existing word embeddings align evidence derived knowledge base although models learn representations novo. similar spirit work weston entities belonging structured database identiﬁed unstructured text order obtain embeddings useful relation prediction. however learn separate scoring functions data source. approach also employed fried dredze wang cases separate objectives used incorporate different data sources combining skip-gram objective mikolov transe obfigure unify structured unstructured data sources considering functional relationships form co-occurrence considering sentence cooccurrence another type functional relationship. thus model source-agnostic uses true triples evidence obtain embedding entities relationships. this since relationship operators part space learning functions apply word regardless source allowing link prediction entities knowledge base. attempt model higher-order language structure grammar syntax consider generative model distance terms embedded space describes probability co-occurrence given relationship. this learn joint distribution pairs relationships questions ‘what probability anemia appearing sentence imatinib?’ ‘what probability anemia disease associated leukemia?’ introduces ﬂexibility subsequent analyses require generative approach. paper laid follows related work describe relevant prior work concerning embedding words relationships place contributions context. modelling describe detail probabilistic model inference learning strategy including link code. experiments show array experimental results quantitatively demonstrate different aspects model datasets using wordnet wikipedia sources supervised semi-supervised unsupervised settings summarising ﬁndings discussion section. partition function normalisation factor here joint posterior space captured model pasrt e−e. parameters case representations words relationship matrices; vj}r∈relationships energy function problematic however trivially minimised making norms vectors tend inﬁnity. partition function provides global regularizer sufﬁcient avoid norm growth training. therefore energy function negative cosine similarity suffer weakness; also natural choice cosine similarity standard method evaluating word vector similarities. energy minimisation therefore equal ﬁnding embedding angle related entities minimised appropriately transformed relational space. would simple deﬁne complex energy function choosing different functional representation focus work afﬁne case. jective bordes method uses single energy function joint space word pairs relationships combining ‘distributional objective’ relational data considering free-text co-occurrences another type relationship. mentioned several approaches integrating graphs embedding procedures. graphs derived knowledge bases ontologies forms graphs also exploited related efforts example using constituency trees obtain sentence-level embeddings motivation work similar spirit multitask transfer learning evgeniou pontil widmer r¨atsch transfer learning takes advantage data related similar typically supervised learning task improving accuracy speciﬁc learning task. case unsupervised learning task embedding words relationships vector space would like data another task improve learned embeddings word co-occurrence relationships. understood case unsupervised transfer learning tackle using principled generative model. finally note recent extension wordvec full sentences using fast generative model exceeds scope model terms sentence modeling explicitly model latent relationships tackle transfer learning heterogeneous data sources. consider probability distribution triplets source word relationship target word. note refer ‘words’ could represent entity relationship hold without altering mathematical formulation could refer multiple-word entities even non-lexical objects. without loss generality proceed refer words. following mikolov learn representations word represents word appears source word appearing target. relationships altering action vector space grcs). allowing arbitrary afﬁne transformation combine bilinear form socher translation operators bordes goldberg levy provide motivation using representations word. extend observing words similar representations share paradigmatic relationship exchangeable sentences tend co-occur. conversely words syntagmatic relationship tend co-occur thus seek enforce syntagmatic relationships transitivity obtain paradigmatic relationships vectors. implementation provide algorithm python since runtime takes place vector operations developing gpu-optimised version. adam adapt learning rates improve numerical stability. used recommended hyperparameters paper; unless otherwise stated hyperparameters speciﬁc model were dimension batch size learning rate parameter types three rounds gibbs sampling obtain model samples. proceed explore model settings. first entity vector embedding problem wordnet consists fully observed triplets words relationships. second case demonstrate power semisupervised extension algorithm task. show adding relationship data lead better embeddings adding unstructured text lead better relationship predictions. finally demonstrate algorithm also identify latent relationships lead better word embeddings. data structured data wordnet dataset described socher available http// stanford.io/ienoyh. contains words types relationships. training data consists true triples derived additional version dataset stripping sense words reduced vocabulary words. note procedure likely makes prediction data difﬁcult every word receives representation. order produce aligned vocabulary unstructured data source taken english wikipedia extracted text using wikiextractor greedily identiﬁed wordnet -grams wikipedia text. words considered sentence context appeared within word window. pairs words appeared wordnet vocabulary included. drew pool training triples wordnet relationships triples wikipedia check choice strip sense valid also created version wikipedia dataset word tagged common sense wordnet training corpus. found signiﬁcantly impact results chose continue sense-stripped version preferring collapse wordnet identities assigning possibly-incorrect senses words wikipedia. given smooth energy function ﬁrst term easily obtained second term problematic. term derived partition function intractable evaluate practice owing double size vocabulary order circumvent intractability resort techniques used train restricted boltzmann machines stochastic maximum likelihood also known persistent contrastive divergence contrastive divergence gradient partition function estimated using samples drawn model distribution seeded current training example however many rounds sampling required obtain good samples. retains persistent markov chain model samples across gradient evaluations assuming underlying distribution changes slowly enough allow markov chain chain mix. gibbs sampling iteratively using conditional distributions variables obtain model samples. interestingly model gracefully deal missing elements observed triplets learning achieved considering partially observed triple superposition possible completions triple weighted conditional probability given observed elements using produces gradient weighted sum. fully-observed case weighting simply spike observed state. similarly model predict missing values simple inference step. properties make joint distribution attractive practical offsetting conceptual difﬁculty training. experiments exploit properties principled semi-supervised unsupervised learning parwordnet prediction task used model solve basic prediction task described socher case model must differentiate true false triples false triples obtained corrupting entry triple e.g. doesn’t appear training data). ‘truth’ triple evaluated energy relationshipspeciﬁc cut-off chosen maximizing accuracy validation learning explicit representations entities wordnet approach closely follows ‘entity vector’ task socher contrasted ‘word vector’ task representation learned word entity representations obtained averaging word vectors. elected perform task conﬁdent composition phrases averaging well-justiﬁed. using validation select early stopping point epochs obtain test accuracy auroc ‘neural tensor model’ described socher achieves accuracy around task although note simpler bilinear model also described socher achieves closer energy function employ. improved performance exhibited simpler bilinear model also noted yang baselines reported socher single layer model without interaction term hadamard model model bordes learns separate left right relationship operators element triple. outperformed bilinear models figure socher details. hence model outperforms previous methods preliminary test model also considered freebase task described socher initial testing yielded accuracy comparable result best-performing model chose explore dataset however entities mostly proper nouns thus seemed unlikely beneﬁt additional semantic data. semi-supervised learning wordnet next tested semi-supervised learning capabilities algorithm consider task before omit label information training instead posterior probabilities inference. trained algorithm subset training data measured accuracy classifying true false relationships before. fully-observed case used subset fully-observed data semi-supervised learning also used remaining data masking type relationship between pairs. figure report accuracy different labelled/unlabelled fractions otherwise dataset. semi-supervised method consistently performs better fully observed method analysed figure semi-supervised learning improves learned embeddings tested semi-supervised extension approach entity relationship learning task described socher previous subsection. following socher predict triple true using energy score. trained algorithm subset training data fully-supervised version used subset fully labelled data semi-supervised learning addition remaining data type relationship pairs unknown. semi-supervised method consistently performs better fully supervised method adding unstructured data relationship prediction task test unstructured text data improve prediction task structured data scarce augmented subsampled triples wordnet examples wikipedia varied weight associated gradients learning. task predict whether given triple true example wordnet described previously. figure shows accuracy task amount structured data vary. improvement associated unstructured data compared accuracy including free text data quite consistently improves classiﬁcation accuracy particularly structured data scarce. figure unstructured data helps relationship learning addition training known relationships unstructured data wikipedia varying weight training. before goal predict triple true using energy score. validation used determine threshold triple considered ‘true’. solid line denotes average three independent experimental runs; shaded areas show range results. plot right shows difference accuracy gave highest accuracy validation set. signiﬁcance marked asterisk. unstructured wikipedia improve relationship learning cases labelled relationship data scarce. formed embeddings inputs supervised multi-class classiﬁer. task given triple predict given vector formed concatenating random forest classiﬁer trained wordnet validation using ﬁve-fold cross-validation. avoid testing training data perform procedure relationship time removing training data triples containing relationship. figure shows score multi-class classiﬁer left-out relationship different combinations data sizes. relationships including unstructured data improves embeddings also trained wordvec much larger wikipedia-only dataset trained classiﬁer vectors; results shown black lines. approach yields consistently higher score suggesting even data unrelated relationships provides information produce vectors semantically richer overall. figure relationship data improves learned embeddings apply algorithm scarce wikipedia co-occurences varying amounts additional unrelated relationship data test quality embedding measuring accuracy task related nine relationships used eight relationships together wikipedia data learn representations used subsequent supervised learning task predict remaining ninth relationship based representations using random forests. black lines denote results wordvec trained wikipedia-only dataset sentences. relationships use. using model relationship model effectively reverts wordvec. however budget relationships model additional parameters available learn afﬁne transformations space differentiate distances meaning interact word embeddings without ﬁxing priori. intuition want test whether textual context alone substructure capture latent variables. generate training million word co-occurrences wikipedia train different models number latent relationships. inspired earlier experiments testing utility supplanting wordnet training data wikipedia examples decide test ability model purely trained wikipedia learn word relationship representations predictive wordnet triplets without seen data wordnet. baseline start test well word embeddings context alone perform indicated leftmost figure proceed train models latent relationships. observe that especially relationship prediction tasks including ﬂexibility model produces noticeable increase score task. since evaluate embeddings alone effect must shift content vectors cannot explained additional parameters introduced latent relationships. note consistent explanation phenomenon model discovers contextual subclasses indicative wordnet-type relationships. observation opens doors explorations hypothesis regarding contextual subclasses unsupervised relationships learning different types co-occurrence data. note perform exhaustive search hyperparameter space; better settings exist sought future work. nonetheless although absolute improvement score yielded method modest encouraged model’s ability exploit latent variables way. figure unsupervised learning latent relationships improves embeddings train fully unsupervised algorithm possible latent relationships million wikipedia sentences. initialisation random without prior knowledge. test quality resulting embeddings supervised learning nine wordnet relationships random forests. depending relationship hand multiple latent relationships training leads slightly consistently better accuracies using computed embeddings every nine relationships also average. hence resulting embeddings using unsupervisedly learned latent relationships said higher quality. again black lines show results using wordvec. presented probabilistic generative model words relationships them. estimating parameters model stochastic gradient descent obtain vector matrix representations words relationships respectively. make learning tractable persistent contrastive divergence gibbs sampling entity types approximate gradients partition function. model uses energy function contains idealised wordvec model special case. augmenting embedding space considering relationships arbitrary afﬁne transformations combine beneﬁts previous models. addition formulation generative model distinct allows ﬂexible especially missing data semiunsupervised setting. motivated domain-settings structured unstructured data scarce illustrated model combines data sources improve quality embeddings supporting ﬁndings direction. ising afﬁne transformations include nonlinear maps. choice cosine similarity energy function also developed note function insensitive small deviations angle therefore produce looser clusters synonyms. nonlinearity could also introduced energy using example splines. furthermore intend encode capacity richer transfer structured information sources graphs prior knowledge model. current model take advantage local properties graphs purpose explicit encoding nested distributed relationships. limitation model conceptual inability embed whole sentences recurrent complex neural language models offer many avenues pursue extensions model tackle this. particularly interesting direction achieve would combination work could principle integrated model include relationships. intended future application model exploratory semantic data analysis domain-speciﬁc pools knowledge. combining prior knowledge unstructured information infer example edges knowledge graphs. promising ﬁeld medical language processing retrospective exploratory data analysis boost understanding complex relational mechanisms inherent multimodal observations speciﬁc medical knowledge form umls used strong regulariser. indeed initial experiments combining clinical text notes relational data umls concepts semmeddb demonsrated utility combined approach predict functional relationship medical concepts example cisplatin treats diabetes. process expanding investigation. work funded memorial hospital sloan kettering institute additional support s.l.h. provided tri-institutional training program computational biology medicine.", "year": 2015}