{"title": "KnightCap: A chess program that learns by combining TD(lambda) with  game-tree search", "tag": ["cs.LG", "cs.AI", "I.2.6"], "abstract": "In this paper we present TDLeaf(lambda), a variation on the TD(lambda) algorithm that enables it to be used in conjunction with game-tree search. We present some experiments in which our chess program ``KnightCap'' used TDLeaf(lambda) to learn its evaluation function while playing on the Free Internet Chess Server (FICS, fics.onenet.net). The main success we report is that KnightCap improved from a 1650 rating to a 2150 rating in just 308 games and 3 days of play. As a reference, a rating of 1650 corresponds to about level B human play (on a scale from E (1000) to A (1800)), while 2150 is human master level. We discuss some of the reasons for this success, principle among them being the use of on-line, rather than self-play.", "text": "paper present tdleaf variation algorithm enables used conjunction game-tree search. present experiments chess program knightcap used tdleaf learn evaluation function playing free internet chess server main success report knightcap improved rating rating games days play. reference rating corresponds level human play human master level. discuss reasons success principle among on-line rather self-play. temporal difference learning ﬁrst introduced samuel later extended formalized sutton algorithm elegant technique approximating expected long term future cost stochastic dynamical system function current state. mapping states future cost implemented parameterized function approximator neural network. parameters updated online after state transition possibly batch updates several state transitions. goal algorithm improve cost estimates number observed state transitions associated costs increases. perhaps remarkable success tesauro’s td-gammon neural network backgammon player trained scratch using simulated selfplay. td-gammon competitive best human backgammon players td-gammon neural network played dual role predictor expected cost-to-go position means select moves. position next move chosen greedily evaluating positions reachable current state selecting move leading position smallest expected cost. parameters neural network updated according algorithm game. many authors discussed peculiarities backgammon make particularly suitable temporal difference learning self-play principle among speed play td-gammon learnt several hundred thousand games self-play representation smoothness evaluation backgammon position reasonably smooth function position making easier good neural network approximation stochasticity backgammon random game forces least minimal amount exploration search space. td-gammon original form searched oneply ahead feel list appended with shallow search good enough humans. possible reasons this; either gain searching deeper backgammon humans simply incapable searching deeply tdgammon competing pool shallow searchers. although know psychological studies investigating depth humans search backgammon plausible combination high branching factor random move generation makes quite difﬁcult search two-ply ahead. particular random move generation effectively prevents selective search forward pruning enforces lower bound branching factor move. contrast ﬁnding representation chess othello allows small neural network order moves one-ply near human performance difﬁcult task. seems games reliable tactical evaluation difﬁcult achieve without deep lookahead. deep lookahead invariably involves kind minimax search turn requires exponential increase number positions evaluated search depth increases computational cost evaluation function ruling expensive evaluation functions neural networks. consequently chess othello programs linear evaluation functions paper introduce tdleaf variation algorithm used learn evaluation function deep minimax search. tdleaf identical except instead operating positions occur game operates leaf nodes principal variation minimax search position test effectiveness tdleaf incorporated chess program—knightcap. knightcap particularly rich board representation enabling relatively fast computation sophisticated positional features although achieved cost speed trained knightcap’s linear evaluation function using tdleaf playing free internet chess server internet chess club internet play used avoid premature convergence difﬁculties associated self-play.the main success story report starting evaluation function coefﬁcients zero except values pieces knightcap went -rated player -rated player three days games. knightcap ongoing project features added evaluation function time. tdleaf internet play tune coefﬁcients features. remainder paper organized follows. section describe algorithm applies games. tdleaf algorithm described section experimental results internet-play knightcap given section section contains discussion concluding remarks. denote possible board positions game. play proceeds series moves discrete time steps time agent ﬁnds position available moves actions agent chooses action makes transition state probability position board agent’s move opponent’s response. game over agent receives scalar reward typically draw loss. ease notation assume games ﬁxed length denote reward received game. assume agent chooses actions according function current state expected reward state given large state spaces possible store value every instead might approximate using parameterized function class example linear function splines neural networks etc. assumed differentiable function parameters parameter vector minimizes measure error approximation algorithm describe designed exactly that. successive parameter updates according algorithm should time lead improved predictions expected reward provided actions independent parameter vector shown linear algorithm converges near-optimal parameter vector unfortunately guarantee non-linear depends argument’s sake assume action taken state leads predetermined state denote approximation found choose actions state picking action whose successor state minimizes opponent’s expected reward strategy used td-gammon. unfortunately games like othello chess difﬁcult accurately evaluate position looking move ahead. programs games employ form minimax search. minimax search builds tree position examining possible moves computer position possible moves opponent possible moves computer predetermined depth leaf nodes tree evaluated using heuristic evaluation function resulting scores propagated back tree choosing stage move leads best position player move. ﬁgure example game tree minimax evaluation. reference ﬁgure note evaluation assigned root node evaluation leaf node principal variation; sequence moves taken root leaf side chooses best available move. denote evaluation obtained state applying leaf nodes depth minimax search parameter vector good approximation expected reward achieve apply algorithm ˜jd. sequence posi∇ vector partial derivatives respect parameters. positive parameter controls learning rate would typically annealed towards zero course long series games. parameter controls extent temporal differences propagate backwards time. this compare equation consider term contributing sums equations parameter vector adjusted move ˜j—the predicted reward time t—closer ˜j—the predicted reward time contrast adjusts parameter vector away move predicted reward time step closer ﬁnal reward time step values zero interpolate behaviors. note equivalent gradient descent figure search tree non-unique principal variation case derivative root node respect parameters leaf-node evaluation function multi-valued either except transpositions collisions likely extremely rare tdleaf ignore choosing leaf node arbitrarily available candidates. note also continuous function whenever continuous function implies even pairs undeﬁned multi-valued. thus still arbitrarily choose particular value happens land points. based observations modiﬁed algorithm take account minimax search almost trivial instead working root positions algorithm applied leaf positions found minimax search root positions. call algorithm tdleaf. full details given ﬁgure section describe outcome several experiments tdleaf algorithm used train weights linear evaluation function chess program knightcap. knightcap reasonably sophisticated computer chess program unix systems. standard algorithmic features modern chess programs tend well number features much less common. details knightcap including source code wwwsyseng.anu.edu.au/lsg. figure full breadth -ply search tree illustrating minimax rule propagating values. leaf nodes given score evaluation function scores propagated back tree assigning opponent’s internal node minimum children’s values internal nodes maximum children’s values. principle variation sequence best moves either side starting root node illustrated dashed line ﬁgure. note score root node evaluation leaf node principal variation. ties siblings derivative score respect parameters problem equation necessarily differentiable function values even everywhere differentiable. values ties minimax search i.e. best move available positions along principal variation means principal variation unique thus evaluation assigned root node evaluation number leaf nodes. main experiment took knightcap’s evaluation function material parameters zero. material parameters initialized standard computer values pawn knight bishop rook queen. parameter settings knightcap started free internet chess server human computer opponents. played knightcap games without modifying evaluation function reasonable idea rating. games blitz rating b-grade human performance although course kind game knightcap plays material parameters different human play level turned tdleaf learning algorithm learning rate value chosen heuristically based typical delay moves error takes effect high enough ensure rapid modiﬁcation parameters. couple minor modiﬁcations algorithm made ensured small ﬂuctuations relative values leaf nodes produce large temporal differences tdleaf calculations). outcome game loss draw. ensure value equivalent matanhhβ modiﬁed following way. negative values left unchanged decrease evaluation position next viewed mistake. however positive values occur simply opponent made blunder. avoid knightcap trying learn predict opponent’s blunders positive temporal differences zero unless knightcap predicted opponent’s move. later experiment positive temporal differences zero knightcap predict opponent’s move opponent rated less knightcap. predicting stronger opponent’s blunders useful skill although whether made difference clear. store whole position hash table small memory really hurts performance). another reason also portion performing paramater updates every four games rather every game. plots various parameters function number games played shown figure plot contains three graphs corresponding three different stages evaluation function opening middle ending. finally compared performance knightcap learnt weight knightcap’s performance hand-coded weights playing versions icc. hand-coded weights close performance learnt weights also tested result allowing knightcap learn starting hand-coded weights case seems knightcap performs better starting material values conducting tests verify results. however surprising learning good quality hand-crafted parameters better learning material parameters. particular handcrafted parameters high values take long time learn normal playing conditions particularly rarely active principal leaves. within games knightcap’s rating risen increase points three days level comparable human masters. point knightcap’s performance began plateau primarily opening book repeatedly play weak lines. since implemented opening book learning algorithm knightcap plays rating major internet chess server chessclub.com often beats international masters blitz. also knightcap automatically learns parameters able large number features evaluation function knightcap currently operates features extra evaluation power knightcap easily beats versions crafty restricted search deep itself. however caveat optimistic assessment knightcap routinely gets crushed faster programs searching deeply. quite unlikely easily ﬁxed simply modifying evaluation function since work able predict tactics statically something seems difﬁcult could effective algorithm learning search selectively would potential greater improvement. note twice repeated learning experiment found similar rate improvement ﬁnal performance level. rating function number games repeat runs shown ﬁgure note case knightcap took mearly twice long reach mark partly operating limited memory game point memory increased memory intensive variant learning knightcap must there appears systematic difference around points servers peak rating roughly corresponds peak fics. transferred knightcap strong players playing there. players fics prefer play opponents similar strength knightcap’s opponents improved did. effect guiding knightcap along path weight space strong weights. knightcap learning on-line self-play. advantage on-line play great deal information provided opponent’s moves. particular stronger opponent knightcap shown positions could forced mis-evaluated evaluation function. course self-play knightcap also discover positions misevaluated kinds positions relevant strong play opponents. setting view information provided opponent’s moves partially solving exploration part exploration/exploitation tradeoff. good initial conditions. second experiment knightcap’s coefﬁcients initialised value pawn. value pawn needs positive knightcap because used many places code example deem search converged .∗pawn. thus parameters equal value value pawn. playing initial weight settings knightcap blitz rating around games fics knightcap’s rating improved point gain. much slower improvement original experiment. know whether coefﬁcients would eventually converged good values clear experiment starting near good weights important fast convergence. interesting avenue exploration effect learning rate. initial evaluation function completely wrong would justiﬁcation setting early knightcap tries predict outcome game evaluations later moves non-material weights initially zero even small changes weights could cause large changes relative ordering materially equal positions. hence even games knightcap playing substantially better game chess. gammon signiﬁcant reason randomness backgammon ensures high probability different games substantially different sequences moves also speed play td-gammon ensured learning could take place several hundred-thousand games. unfortunately chess programs slow chess deterministic game self-play deterministic algorithm tends result large number substantially similar games. problem games seen self-play representative games played practice however knightcap’s self-play games non-zero material weights different kind games humans level would play. demonstrate learning self-play knightcap effective learning real opponents another experiment material parameters initialised zero again time knightcap learnt playing itself. games played resulting version good version learnt fics games weight values ﬁxed. self-play version scored good fics version. simultaneously work presented here beal smith reported positive results using essentially tdleaf self-play learning parameters evaluation function computed material balance. however comparing performance on-line players primarily investigating whether weights would converge sensible values least good naive values introduced tdleaf variant suitable training evaluation function used minimax search. extra requirement algorithm leafnodes principal variations stored throughout game. presented experiments chess evaluation function trained b-grade master level using tdleaf on-line play mixture human computer opponents. experiments show importance on-line sampling deterministic game chess need start near good solution fast convergence although near still clear. theoretical side recently shown converges linear evaluation functions interesting avenue investigation would determine whether tdleaf similar convergence properties. thanks several anonymous referees helpful remarks. jonathan baxter supported australian postdoctoral fellowship. weaver supported australian postgraduate research award.", "year": 1999}