{"title": "Shedding Light on the Asymmetric Learning Capability of AdaBoost", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "In this paper, we propose a different insight to analyze AdaBoost. This analysis reveals that, beyond some preconceptions, AdaBoost can be directly used as an asymmetric learning algorithm, preserving all its theoretical properties. A novel class-conditional description of AdaBoost, which models the actual asymmetric behavior of the algorithm, is presented.", "text": "paper propose diﬀerent insight analyze adaboost. analysis reveals that beyond preconceptions adaboost directly used asymmetric learning algorithm preserving theoretical properties. novel class-conditional description adaboost models actual asymmetric behavior algorithm presented. asymmetry present many real world pattern recognition applications. medical diagnosis disaster prediction biometrics fraud detection etc. obviously diﬀerent costs associated diﬀerent kinds mistakes implicitly related decision. asymmetry connected direct cost mistake. many problems unbalanced class priors classes extremely frequent easier sample. kind problems require classiﬁers capable focusing attention rare class instead trying hypothesis general well data retical guarantees tendency non-overﬁtting promising practical results focused interest family algorithms theoretical practical points view. literature several modiﬁcations adaboost proposed deal asymmetry viola jones face detector framework validation modify adaboost strong classiﬁer threshold order trade false positive detection rates. nevertheless stated clear whether change preserves original training generalization guarantees adaboost weak classiﬁers selection optimal asymmetric task proposed algorithms reach asymmetry based direct manipulations weight distribution update rule. heuristic modiﬁcations algorithm full reformulation adaboost asymmetric classiﬁcation problems. hand recent asymmetric boosting algorithm ﬁnds solution problem based statistical view boosting result theoretically solid ﬁnal algorithm complex computing demanding original adaboost. eventhough studies mention incorporation unbalanced initial weights could lead cost-sensitive version adaboost subsequent works insist enough reach eﬀective asymmetry swelling number diﬀerent asymmetric boosting algorithm variants. meanwhile standard adaboost remains explained uniform initial weight distribution best knowledge formal explanation consequences using asymmetric initial weights adaboost provided either think light must shed order clarify paper propose perspective analyze adaboost class-conditional way. analysis suggests that unbalanced class-conditional initialization weight distribution adaboost itself theoretically sound asymmetric classiﬁcation algorithm. based class error decomposition analysis oﬀers model understand adaboost behavior really deals asymmetry additive round-by-round scheme. fact weights initialization modify data distribution seen learner easily shown shape error bound sets adaboost minimization goal. point work merely analysis adaboost unchanged. consequence algorithm theoretical properties remain intact clearly reported modiﬁcations literature. analysis inspired generalized derivation schapire singer close original specially intuitive illustrative purpose. statistical view boosting subsequent controversy left aside although analogous conclusion could derived next section describe adaboost original algorithm relationship asymmetry. section detail novel class-dependant interpretation analysis experimental results show actual asymmetric behavior adaboost. finally section includes main conclusions drawn analysis. given training examples ﬁrst positives i=m+ adaboost boosting algorithm whose goal learning strong classiﬁer based ensemble weak classiﬁers combined weighted voting scheme. achieve this weight distribution deﬁned whole training set. learning round weak learner selects best classiﬁer according weight distribution weak classiﬁer added ensemble weighted goodness parameter depending correlation term every weak classiﬁer selected weight distribution updated according performance following rule actual distribution). process repeated iteratively ﬁxed number rounds reached obtained strong classiﬁer achieves performance goal. following procedure used schapire singer ﬁnal bound training error obtained adaboost expressed additive minimization seen ﬁnding round round weak hypothesis maximizes maximizing correlation labels predictions weighted sake simplicity clarity analysis focus discrete version algorithm. case weak hypothesis binary minimization process equivalent selecting weak classiﬁer less weighted error case last inequality becomes equality parameter rewritten terms adaboost usually seen learning procedure driven misclassiﬁcation training set. sense exponential bound minimize must deﬁned following guidelines proposed graphically visualize bounding process figure figure adaboost exponential training error bound. horizontal axis represents absolute value ﬁnal score strong classiﬁer negative sign errors positive correct classiﬁcations. vertical axis loss related misclassiﬁcation exponential bound. classes biased prevalent class otherwise. consequently adaboost couldn’t cost-sensitive algorithm unless training data resampled accordingly. based seemingly balanced nature several modiﬁcations adaboost proposed order adapt algorithm cost-sensitive problems. based modifying weight update rule asymmetric way. however clear changes aﬀect theoretical properties adaboost since mentioned above update rule consequence minimization process arbitrary starting point perspective supported fact adaboost usually explained ﬁxed uniform initial weight distribution nevertheless initial works freund schapire leave distribution free controlled learner. explanation algorithm section deliberately didn’t mention anything initialization weight distribution. would really happen initial distribution generic one? changes initial distribution used deal costrelated utility functions cost-sensitive weight initializations bonded diﬀerent changes weight update rule also used karakoulas shawe-taylor ting viola jones proposed ﬁrst modiﬁcation adaboost equivalent asymmetric modiﬁcation initial weights. nevertheless discard approximation arguing induced asymmetry fully absorbed ﬁrst round remaining rest process entirely symmetric. ﬁnal proposal fairly spreading desired asymmetry among predeﬁned number rounds. though widely appreciated easily shown error bounded minimized adaboost actually weighted error depending initial weight distribution. change regard usual bound initial uniform weights taken summation generic initial weights must kept inside summation bounding process rest process remains identical explained schapire singer consequently guaranteeing theoretical properties adaboost regard training generalization errors. section show novel class-conditional interpretation model adaboost. generalized analysis shed light classdependant behavior adaboost sketched previous section. bearing mind round variable parameters minimize using procedure analogous proposed schapire singer minimization exactly original case process entirely performed terms class-conditional parameters allows obtain next expression error minimized round round actually original adaboost formulation respectively. hand derivation equivalent original exception weights decomposed three parameters hand derivation process obtain equivalences appropriately replaced original adaboost expressions lead ones. initial weight decomposition analysis allows decouple global weight distribution information levels always mixed original adaboost formulation class level asymmetry parameter models global cost positive class negative one. practical point view parameter used introduce asymmetry strong classiﬁer. example level class-conditional initial weight distributions model relative relevance example inside class. separate distributions isolated asymmetry problem. class level deﬁnes eﬀective asymmetry demanded current round. seen global desired asymmetry modulated past asymmetric behavior classiﬁer depends previous rounds. example level related weighted error current weak classiﬁer. weight distributions dt−) updated round round encode eﬀective relative relevance example totally apart class behavior. depends previous current rounds. eﬀective asymmetry round depend asymmetry previous ones adaboost goal iteratively weak hypothesis which given predecessors best helps global asymmetry minimizing training error bound. asymmetry reached round-by-round adaptive without previous restriction ﬁnal number rounds. error bound interpretation open door modiﬁcations adaboost based example tuning global past asymmetry contributions order achieve diﬀerent asymmetric behaviors along rounds. change regarding algorithm description usually found literature initial weight distribution necessarily uniform. here initialize terms asymmetry parameter class-conditional distributions uniform order illustrate analysis empirical results asymmetric behavior adaboost unbalanced initial weight distributions performed three kinds experiments. experiments deﬁned asymmetric error cost-sensitive error classiﬁer weighted average positives negatives error rates same weighted average false negatives false positives rates. ﬁrst used separable figure positives concentrated circular area negatives surround them following uniform distribution cases. weak classiﬁers stumps linear two-dimensional space. adaboost behavior training diﬀerent asymmetries shown figure that asymmetry grows positive error bound respective positive training/test errors tend lower negative error bound respective negative training/test errors tend higher. behavior doesn’t prevent classiﬁer asymptotically improving round round approaching zero training error classiﬁers separable nature classiﬁcation problem. advantage approach error evolution follows unbalanced behavior allowing user stop training iteration theoretical conﬁdence minimized error bound desired asymmetry matter iteration philosophy). useful ﬂexible building cascaded classiﬁers ones proposed also experiment non-separable shown figure diﬀerent asymmetries that overlapping classes error curves tend working point diﬀerent previous experiment. case obtained behaviors clearly asymmetric along whole evolution boosted classiﬁers degree asymmetry eﬀectively managed parameter. finally also conducted extensive experiment using synthetic real datasets obtain numerical results verifying hypothesis. strategy followed leave-one cross-validation. thus iteratively selecting every example dataset classiﬁer trained remaining elements tested selected one. procedure repeated examples datasets desired parameters overall performance ﬁgures computed. tables summarize obtained performance synthetic dataset overlapping figure real asymmetric datasets training errors test errors rounds adaboost training diﬀerent asymmetries using without overlapping figure figure classiﬁcation results test overlapping diﬀerent asymmetries. figure true positives marked true negatives. however case cyan colored marks represent positive classiﬁcations blue ones represent negative classiﬁcations. previous sections reveal adaboost asymmetric learning algorithm following original additive round-by-round updating behavior. proposed change perspective yields several consequences initial weight distribution distribution seen ﬁrst weak classiﬁer. distribution weighs global error bound minimized adaboost. asymmetry initial weight distribution eﬀective introduce asymmetry strong classiﬁer goal. kind asymmetry asymptotic whole classiﬁer number training rounds ﬂexible original case among advantages makes possible strong classiﬁer trained whatever round consider certainty error bound minimized taking desired global asymmetry account. moreover specially useful cascaded classiﬁers used object detection stage must markedly asymmetric short possible order improve rejecting eﬃciency asymmetry reached without changing weight update rule opposed asymmetric adaboost modiﬁcations literature. argued modiﬁcation needed adaboost updates weights examples diﬀerent classes distinguishing correctly incorrectly classiﬁed ones. true must taken account that ﬁrst weight distribution update adaboost must selected ﬁrst weak classiﬁer goodness parameter according initial weight distribution stores desired asymmetry information. consequently implicitly encode asymmetry information parameters ones manage update rule. result asymmetry indirectly present usual weight update rule seen section subsequent iterations seen round-by-round asymmetry adaptive process. additional class-dependant change weight update rule emphasize less controlled described asymmetric behavior cases clear would aﬀect theoretical properties adaboost. paper introduced insight asymmetric learning capabilities adaboost symmetric case seen particularization beyond preconceptions needed change regard usual formulation initial weights initialized. shown using novel class-conditional interpretation error bound asymmetric behavior reached asymptotic number rounds works whole algorithm additive round-by-round way. weight update rule doesn’t need changed formal guarantees remain intact. error bound interpretation also useful develop adaboost modiﬁcations based adjusting diﬀerent asymmetry components presented algorithm. adaboost", "year": 2015}