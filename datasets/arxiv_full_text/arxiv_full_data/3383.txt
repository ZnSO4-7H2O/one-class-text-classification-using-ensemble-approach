{"title": "Generative and Discriminative Voxel Modeling with Convolutional Neural  Networks", "tag": ["cs.CV", "cs.HC", "cs.LG", "stat.ML"], "abstract": "When working with three-dimensional data, choice of representation is key. We explore voxel-based models, and present evidence for the viability of voxellated representations in applications including shape modeling and object classification. Our key contributions are methods for training voxel-based variational autoencoders, a user interface for exploring the latent space learned by the autoencoder, and a deep convolutional neural network architecture for object classification. We address challenges unique to voxel-based representations, and empirically evaluate our models on the ModelNet benchmark, where we demonstrate a 51.5% relative improvement in the state of the art for object classification.", "text": "working three-dimensional data choice representation key. explore voxel-based models present evidence viability voxellated representations applications including shape modeling object classiﬁcation. contributions methods training voxel-based variational autoencoders user interface exploring latent space learned autoencoder deep convolutional neural network architecture object classiﬁcation. address challenges unique voxel-based representations empirically evaluate models modelnet benchmark demonstrate relative improvement state object classiﬁcation. data offer computer vision systems rich view world also pose unique challenges particularly applications understanding surrounding environment critical. particular data point cloud extracted rgb-d image polygonal mesh guaranteed arranged regular grid making unsuitable high-performance machine learning algorithms convolutional neural networks deep convnets currently used state-of-the-art systems number tasks computer vision date three recent systems achieve state-of-the-art performance object recognition modelnet benchmark made convnets pre-trained imagenet evaluated using multiple rendered object views. voxel models wherein object shape represented binary occupancy grid provide representation suitable convnets present number difﬁculties. addition third spatial dimension regular grid comes corresponding computational cost curse dimensionality central issue limiting available resolution voxel grid. resolution grids make difﬁcult differentiate similar shapesand toss texture information available renderings equivalent dimesnionality. shallow convnets evaluated modelnet benchmark generally outperformed multi-view convnets. despite challenges posit deep convnets viable modeling voxel-based objects generative discriminative tasks. work present deep convnet architectures generative discriminative voxel modeling explore issues speciﬁc voxel-based representations. generative methods display high ﬁdelity shape interpolation discriminative methods outperform current state relative modelnet modelnet benchmarks. interpolating binary grids permits obvious mathematical interpretation; wish learn voxellated objects relate another require model capable reasoning abstract feature space captures salient factors variation. work select variational autoencoder probabilistic framework learns inference network input space descriptive latent variables generative network maps latent space back input space. training network infer latent variables describe underlying factors variation objects gain ability smoothly transition objects interpolating object’s latent description reconstructing using decoder network. model implemented theano lasagne comprises encoder network latent layer decoder network displayed figure encoder network consists convolutional layers fully connected layer followed linear projection fully connected layer latent layer. decoder network identical inverted architecture weights tied encoder’s. convolutional layer bank ﬁlters starting ﬁlters layer furthest latents doubling subsequent layer. layers exponential linear unit nonlinearity exception ﬁnal layer uses sigmoid nonlinearity. output element ﬁnal layer interpreted predicted probability voxel present given location. downsampling encoder network accomplished strided convolutions every second layer. upsampling decoder network accomplished fractionally strided convolutions implemented gradient equivalent strided convolution every second layer. network initialized glorot initialization output layer batch normalized. variance mean parameters latent layer individually batch normalized output latent layer training still stochastic parameterization trick. loss function consists divergence prior latents weight regularization reconstruction error specialized form binary cross-entropy standard loss target value output network output element. derivative respect severely diminishes approaches result vanishing gradients training. additionally standard weights false positives false negatives equally; voxel grid training data empty network conﬁdently plunge local optimum standard outputting negatives. make modiﬁcations improve training. first change range target output respectively. change increases magnitude loss gradient throughout domain reducing probability vanishing gradients. second hyperparameter weights relative importance false positives false negatives training strongly penalizing false negatives reducing penalty false positives. setting high results noisy reconstructions setting results reconstructions neglect salient object details structure. model trained using stochastic gradient descent nesterov momentum epochs reconstruction error held-out validation bottoms out. learning rate ﬁrst epoch increased data augmented adding random translations horizontal ﬂips training example training noisy uncorrupted copy instance randomly shufﬂed. training network reconstruct corrupted uncorrupted data force learn invariance small structural variations. ﬁrst validate modiﬁcation comparing validation errors identically initialized networks trained standard trained modiﬁcation. reconstruction error plotted training epochs figure interestingly note validation error lower training error particular training run. experiment number different model architectures training regimes converging ﬁnal method detailed above. particular experiment augmenting training objective adding -unit fully connected softmax layer classiﬁcation parallel latent estimation well denoising objective neither result observable performance improvement. present graphical user interface modeled interface implemented using allows user drag center object interpolates four different objects supports class-unconditional random shape generation. interpolant endpoint models randomly selected modelnet test runtime inference reconstruction real time laptop graphics card. figure shows screenshot interface video interface action available online. voxel-based convnets ﬁrst applied object recognition voxnet shallow volumetric convnet architecture also used orion wherein classiﬁcation task augmented orientation estimation task. recent extension fusionnets combines convnets trained voxellated models pre-trained convnets ﬁne-tuned rendered object views. model designed line approaches used high performance convnets object classiﬁcation. approach inception-style modules batch normalization residual connections pre-activation stochastic network depth contrast previous convnet approaches used shallow networks train networks layers take advantage increased expressivity comes model depth. compared fusionnets model requires signiﬁcantly fewer parameters fewer object views code train test models publicly available. initial tests vanilla convnets adopted simple inception-style architecture. intuition behind design maximize number possible \"pathways\" information propagate network still maintaining simplicity efﬁciency. non-downsampling layers concatenate equal numbers ﬁlters allowing network choose taking weighted average featuremaps previous layer focusing spatial relationships downsampling layers stack convolutions strided pooling operations using average pooling concatenate features strided convolutions. intent downsampling layers network learn best relative weighting various downsampling methods maximizing propagation information still producing compact representation. ﬁnal model nine layers deep four voxception blocks three voxception downsample blocks followed fully connected layers softmax nonlinearity. voxception-resnet architecture based resnet architecture concatenates resnet bottleneck block standard resnet block single inception-style block improve parameter efﬁciency early layers path block half many ﬁlters ﬁnal layer. downsampling accomplished voxceptiondownsample blocks change resnet model. change order application rectifying nonlinearities batch normalization obtain pre-activation blocks. finally stochastically drop non-residual paths blocks keep probability linearly decreased ﬁrst layer ﬁnal layer used weighting value instead drop probability test time. best-performing architecture shown figure consists initial convolutional layer four main units containing three stacked blocks voxception-downsample block ﬁnal convolution residual connection keep probability global pooling layer fully-connected layers. number ﬁlters begins doubled downsampling block. deepest path network layers batch normalized trained using nesterov momentum momentum value ﬁnal softmax nonlinearity exponential linear units used activations throughout network. change binary voxel range encourage network attention positive entries. experiment multiple learning rate decay schemes dividing learning rate factor every time validation loss bottoms effective annealing constant rate number minibatches epochs. initial hyperparameter studies validated using held-out class-balanced tenth training ﬁnal evaluation annealing schedule ﬁxed entire training used. epoch train copies example copies randomly ﬂipped horizontal axis and/or translated done voxnet. ﬁxed random seed scheme ensure different training runs make identically augmented datasets. versions training rotations instance rotations instance. best performing models warm network training twelve epochs -rotation training anneal learning rate ﬁne-tune -rotation training set. testing measured predictions single view averaged predictions across rotated copies instance. found data augmentation essential training deeper networks especially voxception-resnet. produce simple ensemble summing predictions models voxception model. single training epoch using batch size full -rotation augmented dataset takes around hours single titan models require around days training converge. also experiment treating different rotations separate channels single instance found training model look single orientation averaging predictions across rotated versions instance yielded better performance. suspect because without otherwise changing model architecture rotationchannel network must still pass information equivalent representational bottlenecks opposed able separately evaluate rotation individual instances. additionally experimented variety values range binary voxel grid including adaptive method wherein numerical value positive entries equal times percentage grid occupied object methods signiﬁcantly alter performance. experimented stacking xxxxxx blocks place convolutions found noticeably affect performance training time. tried adam adamax optimizers; although using optimization schemes caused training error bottom quickly validation error improve training error suggesting model rapidly overﬁtting training set. also found aggressively downsampling early network replacing initial convolution strided convolution enabled train signiﬁcantly deeper models networks underperformed layer counterparts. theorize placing early representational bottleneck causes network toss features might otherwise learn keep propagate deeper layers placing upper bound performance simply provide sufﬁcient data train networks depth. reconstruction accuracy fully-trained evaluated modelnet test displayed table model attains true positive true negative reconstruction accuracy modelnet test indicating learns reconstruct high ﬁdelity tends slightly overestimate probability voxel present. reconstruction interpolation examples displayed figure alongside class-unconditional random samples. accuracy discriminative models evaluated compared competing approaches table best single model obtains accuracy modelnet test accuracy modelnet test respectively better previous published results competitive current state art. best ensemble models obtains state-of-the accuracy modelnet subsets improving state relative respectively. best model obtains accuracy tested using single view object. best ensemble models trained solely modelnet obtains accuracy also better previously published result; report modelnet accuracy models trained modelnet consistency previous results. network achieves passable reconstruction accuracy learns smoothly interpolate arbitrary previously unseen shapes. network additionally capable generating random shapes consistent structure indicating learned latent space successful disentangling factors structural variation though shapes. network performs well dense objects particularly thick dense objects sofas toilets occasionally struggles reconstruct objects long thin members tables chairs. suspect features small activate receptive ﬁeld appropriate latents lost favor denser features weigh heavily loss function suggest imposing additional \"local\" reconstruction function measures reconstruction accuracy subsets voxel grid network learns reconstruct features regardless small relative entire object. network also struggles reproduce crisp edges preferring output smooth rounded edges. posit analogous vanilla tend output images blurry edges rather crisp edges avoid overconﬁdently making incorrect predictions. system capable smoothly interpolating reconstructions indicating learns representation captures underlying factors structural variation. example interpolating objects class slightly different orientation model make minimal changes output interpolation rather completely deconstructing reconstructing output interpolating drastically different objects interface exhibits ﬂowing water effect wherein preexisting voxels appear smoothly shift shapes rather appearing random seen figure samples generated model shown last figure samples consistently bear semblance structure free-ﬂoating voxels suggesting decoder network learned maintain output voxel connectivity regardless latent conﬁguration. major limitation generated samples however resemble real objects. hypothesize training deeper expressive model modelnet dataset augmenting latent vector class-conditional vector would enable generation objects clearly belong particular class leave investigation future work. model takes advantage increased expressivity associated substantially increased depth achieve signiﬁcant improvements modelnet classiﬁcation benchmark. found averaging predictions across rotations critical achieving performance even taking predictions single view resulted passable modelnet accuracy even ensemble models predicting single view achieves accuracy. also found ensembling predictions averaged rotations resulted even higher test performance modelnet suspect result general claim main results. best single model competitive with outperform previous state modelnet orion augments classiﬁcation task rotation estimation task. suspect deeper models perform quite well trained smaller subset lack data training instances modelnet compared instances modelnet. additionally orion incorporates class-speciﬁc priors determine precisely rotations train eschew favor generality. hypothesis upper bound model depth dependent amount available data consistent observations signiﬁcant data augmentation required train -layer model. also note highest performance modelnet came models trained modelnet despite modelnet containing additional modelnet instances. interesting transfer learning perspective learning distinguish wider variety classes model learns better discriminate given subset classes. believe still remains plenty low-hanging fruit gained investigating deep convnets object classiﬁcation provide several suggestions improvement change voxel grid real-valued based percentage space occupied instance grid element. could also used allow represent complex shapes ﬁtting corner-connected polyhedron volume equal predicted occupancy percentage. different voxception architectures downsampling methods activation functions. experiments focused primarily high-level network architecture likely effective ways compose voxception-resnet block. presented voxel-based variational autoencoder graphical user interface exploring latent space generative models along voxel-based deep convolutional neural network classiﬁcation. methods take account challenges speciﬁc voxel representations demonstrate viability voxel representations discriminative tasks improving state modelnet classiﬁcation task large margins. research made possible grants support renishaw edinburgh centre robotics. work presented herein also partially funded european programme beaconing project grant agreement", "year": 2016}