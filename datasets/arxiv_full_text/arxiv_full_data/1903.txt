{"title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural  Networks", "tag": ["cs.LG", "cs.CL", "cs.CV"], "abstract": "Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the MSCOCO image captioning challenge, 2015.", "text": "recurrent neural networks trained produce sequences tokens given input exempliﬁed recent results machine translation image captioning. current approach training consists maximizing likelihood token sequence given current state previous token. inference unknown previous token replaced token generated model itself. discrepancy training inference yield errors accumulate quickly along generated sequence. propose curriculum learning strategy gently change training process fully guided scheme using true previous token towards less guided scheme mostly uses generated token instead. experiments several sequence prediction tasks show approach yields signiﬁcant improvements. moreover used successfully winning entry mscoco image captioning challenge recurrent neural networks used process sequences either input output both. known hard train long term dependencies data versions like long short-term memory better suited this. fact recently shown impressive performance several sequence prediction problems including machine translation contextual parsing image captioning even video description paper consider problems attempt generate sequence tokens variable size problem machine translation goal translate given sentence source language target language. also consider problems input necessarily sequence like image captioning problem goal generate textual description given image. cases recurrent neural networks generally trained maximize likelihood generating target sequence tokens given input. practice done maximizing likelihood target token given current state model previous target token helps model learn kind language model target tokens. however inference true previous target tokens unavailable thus replaced tokens generated model itself yielding discrepancy model used training inference. discrepancy mitigated beam search heuristic maintaining several generated target sequences continuous state space models like recurrent neural networks dynamic programming approach effective number sequences considered remains small even beam search. main problem mistakes made early sequence generation process input model quickly ampliﬁed model might part state space never seen training time. here propose curriculum learning approach gently bridge training inference sequence prediction tasks using recurrent neural networks. propose change training process order gradually force model deal mistakes would inference. model explores training thus robust correct mistakes inference learned training. show experimentally approach yields better performance several sequence prediction tasks. paper organized follows section present proposed approach better train sequence prediction tasks recurrent neural networks; followed section draws links related approaches. present experimental results section conclude section considering supervised tasks training given terms input/output pairs input either static dynamic target output sequence variable number tokens belong ﬁxed known dictionary. sequence length represented tokens latter term equation estimated recurrent neural network parameters introducing state vector function previous state previous output token i.e. often implemented linear projection state vector vector scores token output dictionary followed softmax transformation ensure scores properly normalized usually non-linear function combines previous state previous output order produce current state. means model focuses learning output next token given current state model previous token. thus model represents probability distribution sequences general form unlike conditional random fields models assume independence outputs different time steps given latent variable states. capacity model limited representational capacity recurrent feedforward layers. lstms ability learn long range structure especially well suited task make possible learn rich distributions sequences. order learn variable length sequences special token <eos> signiﬁes sequence added dictionary model. training <eos> concatenated sequence. inference model generates tokens generates <eos>. training recurrent neural networks solve tasks usually accomplished using mini-batch stochastic gradient descent look parameters maximizes likelihood producing correct target sequence given input data training pairs inference model generate full sequence given generating token time advancing time step. <eos> token generated signiﬁes sequence. process time model needs input output token last time step order produce since access true previous token instead either select likely given model sample according searching sequence highest probability given expensive combinatorial growth number sequences. instead beam searching procedure generate best sequences. maintaining heap best candidate sequences. time step candidates generated extending candidate token adding heap. step heap re-pruned keep candidates. beam searching truncated sequences added best sequences returned. beam search often used discrete state based models like hidden markov models dynamic programming used harder efﬁciently continuous state based models like recurrent neural networks since factor followed state paths continuous space hence actual number candidates kept beam search decoding small. cases wrong decision taken time model part state space different visited training distribution doesn’t know worse easily lead cumulative decisions classic problem sequential gibbs sampling type approaches sampling future samples inﬂuence past. propose sampling mechanism randomly decide training whether ˆyt−. assuming mini-batch based stochastic gradient descent approach every token predict mini-batch training algorithm propose coin true previous token probability estimate coming model probability estimate model obtained sampling token according probability distribution modeled taken maxs process illustrated figure model trained exactly before model trained setting inference. propose curriculum learning strategy other intuitively beginning training sampling model would yield random token since model well trained could lead slow convergence selecting often true previous token help; hand training favor sampling model often corresponds true inference situation expects model already good enough handle sample reasonable tokens. note experiments ﬂipped coin every token. also tried coin sequence results much worse probably consecutive errors ampliﬁed ﬁrst rounds training. thus propose schedule decrease function itself similar manner used decrease learning rate modern stochastic gradient descent approaches. examples schedules seen figure follows call approach scheduled sampling. note sample previous token ˆyt− model training could back-propagate gradient losses times decision. done experiments described paper left future work. searn proposed tackle problems supervised training examples might different actual test examples example made sequence decisions like acting complex environment mistakes model early sequential decision process might compound yield poor global performance. proposed approach involves meta-algorithm meta-iteration trains model according current policy applies test modiﬁes next iteration policy order account previous decisions errors. policy thus combination previous actual behavior model. comparison searn related ideas proposed approach completely online single model trained policy slowly evolves training instead batch approach makes much faster train furthermore searn proposed context reinforcement learning consider supervised learning setting trained using stochastic gradient descent overall objective. approaches considered problem ranking perspective particular parsing tasks target output tree. case authors proposed beam search training inference phases aligned. training beam used best current estimate model compared guided solution using ranking loss. unfortunately feasible using model like recurrent neural network state sequence cannot factored easily thus beam search hard efﬁciently training time finally proposed online algorithm parsing problems adapts targets dynamic oracle takes account decisions model. trained model perceptron thus state-based like recurrent neural network probability choosing truth ﬁxed training. describe section experiments three different tasks order show scheduled sampling helpful different settings. report results image captioning constituency parsing speech recognition. image captioning attracted attention past year. task formulated mapping image onto sequence words describing content natural language proposed approaches employ form recurrent network structure simple decoding schemes notable exception system proposed directly optimize likelihood caption given image instead proposes pipelined approach. since image many valid captions evaluation task still open problem. attempts made design metrics positively correlate human evaluation common tools published mscoco team used mscoco dataset train model. trained images report results separate development additional images. image corpus different captions training procedure picks random creates mini-batch examples optimizes objective function deﬁned image preprocessed pretrained convolutional neural network similar described resulting image embedding treated ﬁrst word model starts generating language. recurrent neural network generating words lstm layer hidden units input words represented embedding vectors size number words dictionary used inverse sigmoid decay schedule scheduled sampling approach. table shows results various metrics development set. metrics variant estimating overlap obtained sequence words target one. since target captions image best result always chosen. best knowledge baseline results consistent current state-of-the-art task. dropout helped terms likelihood negative impact real metrics. hand scheduled sampling successfully trained model resilient failures training inference mismatch likely yielded higher quality captions according metrics. ensembling models also yielded better performance baseline schedule sampling approach. also interesting note model trained always sampling dubbed always sampling table yielded poor performance expected model hard time learning task case. also trained model scheduled sampling instead sampling model sampled uniform distribution order verify important build current model performance boost simple form regularization. called uniform scheduled sampling results better baseline good proposed approach. also experimented ﬂipping coin sequence instead token results poor always sampling approach. another less obvious connection any-to-sequence paradigm constituency parsing. recent work proposed interpretation parse tree sequence linear operations build tree. linearization procedure allowed train model sentence onto parse tree without modiﬁcation any-to-sequence formulation. trained model layer lstm cells words represented embedding vectors size used attention mechanism similar described helps considering next output token produce focus part input sequence applying softmax lstm state vectors corresponding input sequence. input word dictionary contained around words target dictionary contained symbols used describe tree. used inverse sigmoid decay schedule scheduled sampling approach. parsing quite different image captioning function learn almost deterministic. contrast image large number valid captions sentences unique parse tree thus model operates almost deterministically seen observing train test perplexities extremely compared image captioning different operating regime makes interesting comparison would expect baseline algorithm make many mistakes. however seen table scheduled sampling positive effect additive dropout. table report score development also emphasize training instances overﬁtting contributes largely performance system. whether effect sampling training helps regard overﬁtting training/inference mismatch unclear result positive additive dropout. again model trained always sampling instead using groundtruth previous token input yielded results fact resulting trees often valid trees speech recognition experiments used slightly different setting rest paper. training example input/output pair sequence input vectors x··· sequence tokens y··· aligned corresponding here acoustic features represented ﬁlter bank spectra frame corresponding target. targets used hmm-state labels generated gmm-hmm recipe using kaldi toolkit could well phoneme labels. setting different experiments model used following vector dimensionality ht’s extra token added dictionary represent start sequence. generated data experiments using timit corpus kaldi toolkit described standard conﬁgurations used experiments dimensional ﬁlter banks ﬁrst second order temporal derivatives used inputs frame. dimensional targets generated time frame using forced alignment transcripts using trained gmm-hmm system. training validation test sets sequences respectively average length frames. validation used choose best epoch training model parameters epoch used evaluate test set. trained models layers lstm cells softmax layer conﬁgurations baseline conﬁguration ground truth always model conﬁguration model predictions last time step three scheduled sampling conﬁgurations ramped linearly maximum value minimum value epochs kept constant ﬁnal value. conﬁguration trained models report average performance them. training model done frame targets gmm. baseline conﬁgurations typically reached best validation accuracy approximately epochs whereas sampling models reached best accuracy approximately epochs validation accuracy decreased. probably trained models exact account gradient sampling probabilities sampled targets. future effort tackling problem improve results. testing done ﬁnding best sequence beam search decoding computing error rate sequences. also report next step error rate models validation summarize performance models training objective. table shows summary results seen baseline performs better next step prediction models sample tokens input. expected since former access groundtruth. however seen models trained sampling perform better baseline decoding. also seen problem always sampling model performs quite well. hypothesize nature dataset. hmm-aligned states correlation state appears target several frames states constrained subset states. next step prediction groundtruth labels task ends paying disproportionate attention structure labels enough acoustics input thus achieves good next step prediction error groundtruth sequence acoustic information able exploit acoustic information sufﬁciently groundtruth sequence model testing conditions training condition make good predictions. model prediction ends exploiting information acoustic signal effectively ignores predictions inﬂuence next step prediction. thus test time performs well training. model attention model predicts phone sequences directly instead highly redundant state sequences would suffer problem would need exploit acoustic signal language model sufﬁciently make predictions. nevertheless even setting adding scheduled sampling still helped improve decoding frame error rate. note typically speech recognition experiments hmms decode predictions neural networks hybrid model. avoid using altogether hence advantage smoothing results architecture language models. thus results directly comparable typical hybrid model results. table frame error rate speech recognition experiments. next step prediction ground truth predict next target like done training. decoding experiments beam searching done best sequence. report results four different linear schedulings sampling ramped linearly baseline model ground truth. section analysis results. using recurrent neural networks predict sequences tokens many useful applications like machine translation image description. however current approach training them predicting token time conditioned state previous correct token different actually thus prone accumulation errors along decision paths. paper proposed curriculum learning approach slowly change training objective easy task previous token known realistic provided model itself. experiments several sequence prediction tasks yield performance improvements incurring longer training times. future work includes back-propagating errors sampling decisions well exploring better sampling strategies including conditioning conﬁdence measure model itself.", "year": 2015}