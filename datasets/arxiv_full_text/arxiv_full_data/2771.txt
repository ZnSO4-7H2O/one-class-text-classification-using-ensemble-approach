{"title": "Classification with Costly Features using Deep Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We study a classification problem where each feature can be acquired for a cost and the goal is to optimize the trade-off between classification precision and the total feature cost. We frame the problem as a sequential decision-making problem, where we classify one sample in each episode. At each step, an agent can use values of acquired features to decide whether to purchase another one or whether to classify the sample. We use vanilla Double Deep Q-learning, a standard reinforcement learning technique, to find a classification policy. We show that this generic approach outperforms Adapt-Gbrt, currently the best-performing algorithm developed specifically for classification with costly features.", "text": "abstract study classification problem feature acquired cost goal optimize trade-off classification precision total feature cost. frame problem sequential decision-making problem classify sample episode. step agent values acquired features decide whether purchase another whether classify sample. vanilla double deep q-learning standard reinforcement learning technique find classification policy. show generic approach outperforms adapt-gbrt currently best-performing algorithm developed specifically classification costly features. keywords costly features; reinforcement learning; deep q-learning; adaptive feature acquisition; datum-wise classification; prediction budget introduction classification costly features problem algorithm classify sample reveal features defined cost. sample treated independently sample algorithm sequentially selects features base decision acquire values already revealed. inherently different samples different subset features selected. algorithm make trade-off classification precision total incurred cost goal reach highest precision lowest total cost possible. classification costly features studied years practical problem many real cases. let’s name network security analysis expensive human operator examines network traffic queries different analytic services makes decision whether current sample malicious benign. another example medical diagnosis doctor make examinations decide correct treatment robot sensors localize itself measurement depletes battery little. problem sequential decision-making problem such naturally tackled reinforcement learning algorithms. view classification process sample episode agent sequentially decides whether acquire another feature which whether enough information classify sample. step agent base decision features acquired far. actions requesting feature build upon prior work uses q-learning type problem uses linear regression approximate function. replace linear regression recent advances deep learning base method deep q-network double q-learning although algorithm extended multiple techniques show even basic double algorithm outperform state-of-the-art techniques specifically tailored domain. using standard algorithm also brings advantages wide community rapid progress. also algorithm adapt changes non-stationary environments commonly present real-world problems. compare current state-of-the-art algorithm adaptgbrt latest algorithm sequence problem-specific algorithms classification costly features demonstrate several datasets algorithm achieves performance. main contribution paper revisit replace linear regression recently introduced successful deep technique show outperforms state-of-the-art algorithms several datasets. problem definition view problem sequential decision-making problem step agent selects feature view makes class prediction. standard reinforcement learning setting agent explores environment actions observes rewards states. represent environment partially observable markov decision process episode corresponds classification sample dataset. pomdp defined state space actions reward function transition function described below. sample drawn data distribution vector contains feature values value feature number features class. function mapping feature real-valued cost power represents sample currently selected features agent given observed state consist selected parts without label. observed state pairs every selected feature action possible classification actions feature selecting actions classification actions terminate episode agent receives reward case correct classification otherwise. feature selecting actions reveal corresponding value agent receives negative reward −λc. currently available feature selecting actions limited features selected. feature cost factor. altering value make trade-off precision average cost. higher forces agent prefer lower cost shorter episodes precision vice versa. properties make environment inherently episodic maximal length episode goal find policy sequentially selects features eventually decides class maximizing expected total reward. observed state feature values original state could missing multiple different states observed state agent working observed states therefore cannot predict state result agent experiences stochastic transitions rewards although defined environment deterministic initial choice sample. following sections mainly agent’s point view work observed states expectations probabilities. reason useful overload notation transition function also work observed states. give transition probabilities next observed states action taken observed state similarly reward function return expected reward taking action observed state deep q-learning method estimates state-action value function neural network. function expresses expected discounted reward starting observed state performing action following policy afterwards. often written recursive form discount factor controls importance future rewards. useful either non-episodic domains function approximation used although environment guaranteed terminate discount factor help learning preserved algorithm parameter. greedy policy argmaxa dimensional finite state space function precisely found dynamic programming. however precise solution feasible case high-dimensional continuous state space. overcome issue approximation neural networks often employed leading deep q-learning inspired dynamic programming neural network approximates function minimizing sides transitions empirically experienced agent following greedy policy argmaxa transitions denote current state taken action experienced reward expectation converge following state formally looking parameters iteratively minimize loss function batch transitions error decreases approximated function converges however practice method proved unstable several techniques exist stabilize work implement experience replay double q-learning soft-target network well established community sufficiently demonstrate algorithm outperforms stateof-the-art algorithms. experience replay allows efficient usage acquired transitions often expensive get. also overcomes issues induced online learning transitions come highly correlated sequence. experience replay circular buffer used store recent transitions step number sampled form batch optimization. samples less correlated compared online learning replayed multiple times. algorithm implement environment according defined pomdp episode corresponds randomly drawn sample training dataset defines it’s initial state observed time state form created. neural networks accept numeric input observed state mapped tuple vector masked vector original contains values acquired zeros unknown values neural network architecture follows parameters shared actions network accepts observed state outputs values actions. consists input layer accepting state three hidden fully connected layers relu activation linear output layer output action. mainly performance reasons simulate parallel independent environments once. compared full control environment enables efficient implementation. iteration simulate step environments generating transitions once. algorithmic perspective approach similar however synchronously. target network technique decouples current network values update targets. target values generated network itself induces unwanted feedback causes oscillations instability. target network another network used parallel network values used formula parameters reflect delay. here techniques used. either parameters number optimization steps slowly follows computation continues number target network factor leading soft-target network work. double q-learning technique reduce maximization bias induced formula builds upon target network technique combinines estimates here maximizing action taken value target network further ϵ-greedy policy enforce exploration. policy behaves greedily time picks random action probability parameter linearly decreased time towards zero. exact parameters used implementation summarized table appendix. experienced transitions stored circular buffer sampled randomly form batch rmsprop momentum parameter gradient normalization norm exceeds optimize loss function learning rate decay exponentially time. exact parameters differ dataset summarized table optimization weights target network updated overview algorithm seen algorithm details environment simulation algorithm value discount factor environment episodic short average length episodes makes sense consider full returns end. undiscounted returns theory yield better strategy choose features. however discount factor helpful case large number features help stability function approximation considering future states much. could also provide accelerate learning process study effect paper leave future work. promote exploration ϵ-greedy policy. exploration rate starts defined initial value linearly decreased time. exact values differ based dataset described table appendix. reward correct classification error. cost feature selecting actions dataset dependent normalized feature cost factor satisfies pracice rewards feature selecting actions −λc. environment optimal function also bounded domain knowledge clip target +γqϕ) experimentally found approach avoids initial explosion predicted values weights greatly speeds stabilizes learning process. opposed rather large batches optimization step. batch contains transitions. simulate environments step means generated transition replayed times average. found number works well enables efficient hardware. theoretical analysis analyze algorithm theoretically derive following interesting facts could help better understand algorithm lead future improvements. since typically small means values actions close variance sufficiently reduced actions distinguishable. achieved either sufficiently small learning rate large batch size. following proposition need define concept samples consistent observed state. sample consis disclosed tent observed state equal subset samples values dataset consistent observed state proof appendix. words true value classification action ratio classes corresponding action samples consistent current observed state minus one. fact could directly used minimize variance classification actions directly learning correct value instead stochastic rewards converge correct value. however would require additional computation leave future work. experimental results evaluate algorithm three datasets miniboone particle identification forest covertype cifar- prepared split datasets training/validation/testing sets according setting comparing results. datasets explicit cost associated features uniform cost used. multiple values used cost factor different values correspond different average feature cost. information datasets summarized table directly compare prior algorithms adapt-gbrt budgetprune decision trees based algorithms. report algorithms already shown perform worse retrieved performance published results available. machine equipped intel xeon nvidia titan used measure time needed algorithm converge report table feel important factor practical usage. unfortunately prior report learning time needed algorithms therefore cannot compare note algorithm training needs parameter features classes train size validation size test size network size training time transitions classes datasets binarized comparable results algorithm uses neural networks evaluate accuracy plain neural network classifier report baseline. accuracy provides estimate upper bound algorithm approach. baseline network used three layers relu activation size algorithm particular dataset output layer output class softmax activation. comparison achieved trade-off classification accuracy feature cost shown figure exact size neural network layers number transitions needed converge training time reported table miniboone dataset compact well suited fast experiments classes real-valued features. algorithm operates whole range giving different tradevalue. point performs better compared algorithms shown figure accuracy features average almost reaches accuracy baseline network features. forest covertype dataset originally samples classes however prior algorithms reported results reduced version samples classes. comparison report results reduced dataset. dataset features real-valued categorical. categorical features one-hot encoded binary vectors length values used actual features dataset. learning time dataset longer miniboone dataset. compared prior algorithm performs better value another interesting fact algorithm’s best classification accuracy exceeds accuracy baseline neural network. suggests algorithm might work sort regularization. figure shows comparison figure experiments forest dataset. performance training test datasets compared plain neural network based classifier using features. performance different versions forest dataset. show difference training dataset size binarized original multiclass problem. algorithm baseline classifier suggests features actually hurt performance. baseline classifier using features reach performance algorithm neither training testing dataset. cifar- image dataset goal determine object picture. dataset preprocessed classes binarized according dataset contains realvalued features. algorithm performs better prior point uses features average that adapt-gbrt algorithm takes lead. algorithm converges limit baseline classifier indicating feedforward networks suitable problem. however could arrange features grid pattern easily extend algorithm convolutional networks known work well type problems approach work maintain architecture datasets. figure observe values initial observed state evolve training multiclass forest dataset. algorithm quickly estimates correct values classification actions since value dependent distribution classes dataset. values feature selection actions change slowly dependence next state. values close other predicted proposition figure shows evolution average reward episode length accuracy small validation dataset learning. initial policy algorithm quickly finds corresponds always selecting class highest ratio dataset. expectable since strategy achieves optimal accuracy without feature. point average accuracy reward slowly increase supremum. figure middle figure shows evolution average number acquired features equivalent average episode length case. algorithm starts selecting features first subsequently decreasing amount. already noted values classification actions estimated quickly range since dependent next state. however values feature selection actions change slowly result value higher values classification actions beginning learning. makes algorithm sequentially select features taking classification action. however algorithm finds core subset features important subsequently features improve accuracy shown figure figure show histogram number used features different samples multiclass forest dataset. confirms algorithm selects different subset features sample since every episode length present. according histogram samples divided three categories easy average difficult. easy samples classified almost immediately soon identified. average samples form majority require features. group difficult samples also significant size samples require features dataset. rest samples belonging obvious category distributed between. next study effect neural network size performance. computational efficiency miniboone dataset. trade-off accuracy average feature cost controlled parameter shown figure higher forces algorithm select fewer features average consequence accuracy falls down. also report effect figure learning progress multiclass forest dataset learned values actions initial state average reward length accuracy learning validation dataset. point axis corresponds learning steps. graph truncated would axis full training. related work similar work approach also used q-learning. however algorithm worked limited linear approximation resulted inferior performance. instead neural networks allowing algorithm much wider range complex problems. authors recurrent neural network uses attention select blocks features classifies fixed number steps. compared work decisions network hard explain. hand q-learning based algorithm produces semantically meaningful values action. moreover standard algorithm directly benefit enhancements made community. figure effect neural network size algorithm performance miniboone dataset. different selected values affect accuracy feature cost trade-off. comparison different neural network sizes. smaller network achieves worse overall performance. figure histogram used features different samples multiclass forest dataset different three peaks corresponding easy average difficult samples. concrete instantiation adapt-gbrt uses gradient boosted regression trees model either random forests rbf-svm powerful model. essentially algorithm restricted models could even instantiated form algorithm. different algorithms employed linear programming domain algorithm presented uses select model best accuracy lowest cost pre-trained models different features. algorithm also chooses model based complexity sample similarly authors propose reduce problem finding different disjoint subsets features used together macrofeatures. macro-features form graph solved dynamic programming based algorithm. algorithm finding different subsets features complementary algorithm could possibly used jointly improve performance. reference uses fixed order features reveal increasingly complex models them. however order features computed assumed manually. hand algorithm restricted fixed order features also find significance automatically. early work analyzes problem similar definition classification costly features. however algorithms introduced require memorization training examples scalable many domains. plethora tree-based algorithms article implements adapt-gbrt algorithm represents state-of-the-art directly compare algorithm uses separate models gating function selects appropriate given sample. high-prediction cost model classifies samples regardless cost lowprediction cost model adaptively approximates model regions input space achieves adequate accuracy. conclusion formulate problem classification costly features generic sequential decision-making problem. solve vanilla double algorithm show even basic algorithm exceeds performance prior-art algorithms miniboone forest covertype dataset partially exceeds performance cifar- dataset. however architecture algorithm easily extended improvement basic algorithm dueling prioritized experience replay distributional great speed-up could possibly achieved using longer traces combination complementary algorithms could also improve performance. acknowledgments used research donated nvidia corporation. access computing storage facilities owned parties projects contributing national grid infrastructure metacentrum provided programme \"projects large research development innovations infrastructures\" greatly appreciated. parameter network size parallel agents replay buffer size batch size training steps* soft-target factor ϵstart† ϵend ϵsteps lrstart‡ lrend lrsteps lrfactor step means simulating agents step. epsilon controls exploration ϵ-greedy policy. learning first step follows maximal action better action. second step derived definition function. third step need realize single expectation well pair expectations data samples. sample follow exact policy would policy misclassification cost averaged samples. furthermore policy asks feature cost feature. therefore expectation samples reaching average feature cost included values higher feature cost included values hence overall expected value states higher expected value states s′∗. classification actions terminate episode value merely expected reward. ratio class samples consistent observed state correct classification happens expectation probability reward incorrect classification happens probability reward references gabriella contardo ludovic denoyer thierry artieres. recurrent neural networks adaptive feature acquisition. international conference neural information processing. springer gabriel dulac-arnold ludovic denoyer philippe preux patrick gallinari. datum-wise classification sequential approach sparsity. joint european conference machine learning knowledge discovery databases. springer vincent françois-lavet raphael fonteneau damien ernst. discount deep reinforcement learning towards dynamic strategies. arxiv preprint arxiv. matteo hessel joseph modayil hado hasselt schaul georg ostrovski dabney horgan bilal piot mohammad azar david silver. rainbow combining improvements deep reinforcement learning. arxiv preprint arxiv. alex krizhevsky ilya sutskever geoffrey hinton. imagenet classification deep convolutional neural networks. advances neural information processing systems. matt kusner wenlin chen quan zhou zhixiang eddie kilian weinberger yixin chen. feature-cost sensitive learning submodular trees classifiers. aaai. timothy lillicrap jonathan hunt alexander pritzel nicolas heess erez yuval tassa david silver daan wierstra. continuous control deep reinforcement learning. arxiv preprint arxiv. volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. international conference machine learning. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature rémi munos stepleton anna harutyunyan marc bellemare. safe efficient off-policy reinforcement learning. advances neural information processing systems. tijmen tieleman geoffrey hinton. lecture .-rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning kirill trapeznikov venkatesh saligrama. supervised sequential classification budget constraints. artificial intelligence statistics. hado hasselt arthur guez david silver. deep reinforcement joseph wang tolga bolukbasi kirill trapeznikov venkatesh saligrama. model selection linear programming. european conference computer vision. springer joseph wang kirill trapeznikov venkatesh saligrama. efficient learning directed acyclic graph resource constrained prediction. advances neural information processing systems. zhixiang eddie matt kusner kilian weinberger minmin chen olivier chapelle. classifier cascades trees minimizing feature evaluation cost. journal machine learning research", "year": 2017}