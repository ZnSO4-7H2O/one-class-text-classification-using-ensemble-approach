{"title": "Hindsight policy gradients", "tag": ["cs.LG", "cs.AI", "cs.NE", "cs.RO"], "abstract": "Goal-conditional policies allow reinforcement learning agents to pursue specific goals during different episodes. In addition to their potential to generalize desired behavior to unseen goals, such policies may also help in defining options for arbitrary subgoals, enabling higher-level planning. While trying to achieve a specific goal, an agent may also be able to exploit information about the degree to which it has achieved alternative goals. Reinforcement learning agents have only recently been endowed with such capacity for hindsight, which is highly valuable in environments with sparse rewards. In this paper, we show how hindsight can be introduced to likelihood-ratio policy gradient methods, generalizing this capacity to an entire class of highly successful algorithms. Our preliminary experiments suggest that hindsight may increase the sample efficiency of policy gradient methods.", "text": "goal-conditional policies allow reinforcement learning agents pursue speciﬁc goals different episodes. addition potential generalize desired behavior unseen goals policies also help deﬁning options arbitrary subgoals enabling higher-level planning. trying achieve speciﬁc goal agent also able exploit information degree achieved alternative goals. reinforcement learning agents recently endowed capacity hindsight highly valuable environments sparse rewards. paper show hindsight introduced likelihood-ratio policy gradient methods generalizing capacity entire class highly successful algorithms. preliminary experiments suggest hindsight increase sample efﬁciency policy gradient methods. traditional reinforcement learning setting agent interacts environment sequence episodes observing states acting according policy ideally maximizes expected cumulative reward agent required achieve speciﬁc goal episode goal encoded part states possibly allowing generalization desired behavior goals never encountered before. recent examples learning goal-conditional behavior include works silva schmidhuber srivastava kupcsik deisenroth fabisch metzen schaul held interesting application goal-conditional policies hierarchical reinforcement learning help deﬁning options arbitrary subgoals case instead planning perform sequence actions agent could plan achieve sequence subgoals abstracting details involved lower-level decisions. recent examples approach include works vezhnevets kulkarni trying achieve speciﬁc goal agent also able exploit information degree achieved alternative goals. example expect traveller learn arrive eventual destination whether intended destination. capacity hindsight introduced andrychowicz off-policy reinforcement learning algorithms rely experience replay proven particularly important environments sparse rewards. paper show importance sampling used introduce hindsight likelihoodratio policy gradient methods generalizing idea highly successful class reinforcement learning algorithms achieve state-of-the-art results many tasks importance sampling previously applied policy gradient methods order efﬁciently reuse information obtained earlier policies contrast approach attempts efﬁciently learn different goals using information obtained current policy speciﬁc goal. preliminary experiments suggest hindsight indeed increase sample efﬁciency policy gradient methods policy gradients. consider agent interacts environment sequence episodes lasts exactly time steps. agent receives goal beginning episode. time step agent observes state receives reward chooses action setting policy gradient method represent policy probability distribution actions given state goal. objective ﬁnding parameters policy achieve maximum expected return simplicity notation consider ﬁnite state action goal spaces. denote random variables upper case letters assignments variables lower case letters. denote trajectory. assume probability trajectory given goal policy parameterized given hindsight policy gradients. alternatively suppose easily able compute state goal equivalent assumption made andrychowicz case possible evaluate trajectory obtained trying achieve goal alternative goal arbitrary goal importance sampling allows rewriting gradient expected return refer estimate term hindsight policy gradient evaluates trajectory goals intended. although would possible sample goals compute estimate δhpg paper focus variants estimate particularly simple compute environments chose experiments presented sec. ﬁrst variant uniform hindsight policy gradient δuhpg given empirical probability function states trajectory comes additional assumption goals represent states. variant also ignores likelihood-ratio performing updates trajectory equally likely given goal. perhaps surprisingly variant achieves excellent results preliminary experiments. emphasize variants general unbiased estimates hindsight policy gradient studied future work. environments. performed preliminary experiments simple environments allowed systematic hyperparameter search aggregating results across large number runs. empty grid environment agent starts episode upper left corner grid goal achieve randomly chosen position grid. actions allow agent move four cardinal directions stay position state goal represented pair integers reward zero agent goal otherwise. episode lasts time steps. ﬂipping environment agent starts episode state represented bits goal achieve randomly chosen state. actions allow agent bits individually remain state. reward zero agent goal state otherwise. episode lasts time steps. andrychowicz similar environment evaluate hindsight approach. environments important characteristics common. firstly optimal behavior corresponds reaching goal state fast possible staying there. secondly uhpg/ahpg computed using states observed trajectory. policy optimization. policy represented feedforward neural network single hyperbolic tangent hidden layer softmax output layer. parameters updated using adam uhpg scaled inverse cumulative likelihood-ratio across goals avoid large discrepancies step sizes. hyperparameter search. technique evaluated runs lasting episodes every combination environment hidden layer size learning rate average return computed individual average used select best combination technique corresponding standard deviation subtracted. criterion discourages hyperparameter combinations large ﬂuctuations performance empty grid techniques settle hidden layer size learning rate except ahpg learning rate ﬂipping environment learning rate uhpg ahpg; hidden layer size ahpg uhpg. analysis. results additional runs using hyperparameters described summarized figures also include average performance technique. learning curves represent average smoothed returns across runs. smoothed returns obtained using moving average across episodes window size results indicate ahpg achieves better sample efﬁciency specially empty grid. besides substituting distribution goals empirical distribution states trajectory variant ignores fact trajectory obtained trying achieve goal unlikely trying achieve goal although could potentially disrupt behavior policy trajectories likely occur trying achieve risk appears outweighed beneﬁts hindsight environments considered. uhpg achieves comparable performance empty grid. explained fact likelihood-ratio becomes small goals intended policy begins exhibit distinct behaviors distinct goals likelihood-ratio always zero goals intended uhpg would become equivalent off-goal likelihood-ratio longer vanishes ﬂipping environment likely shorter episodes. environment uhpg also appears outperform ahpg early training. experiments required investigate behavior. introduced techniques enable optimizing goal-conditional policies using hindsight. context hindsight refers capacity exploit information degree arbitrary goal achieved another goal intended. prior work hindsight limited off-policy reinforcement learning algorithms rely experience replay preliminary experiments suggest hindsight increase sample efﬁciency policy gradient methods. future work study properties proposed techniques include improvements commonly found policy gradient methods perform comprehensive experiments challenging environments. would like thank sjoerd steenkiste klaus greff valuable feedback. research supported swiss national science foundation capes also grateful nvidia corporation donating dgx- machine donating minsky machine.", "year": 2017}