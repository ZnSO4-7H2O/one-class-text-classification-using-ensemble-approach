{"title": "An associative memory for the on-line recognition and prediction of  temporal sequences", "tag": ["cs.NE", "cs.AI"], "abstract": "This paper presents the design of an associative memory with feedback that is capable of on-line temporal sequence learning. A framework for on-line sequence learning has been proposed, and different sequence learning models have been analysed according to this framework. The network model is an associative memory with a separate store for the sequence context of a symbol. A sparse distributed memory is used to gain scalability. The context store combines the functionality of a neural layer with a shift register. The sensitivity of the machine to the sequence context is controllable, resulting in different characteristic behaviours. The model can store and predict on-line sequences of various types and length. Numerical simulations on the model have been carried out to determine its properties.", "text": "abstract— paper presents design associative capable on-line temporal memory feedback sequence learning. framework on-line sequence learning proposed different sequence learning models analysed according framework. network model associative memory separate store sequence context symbol. sparse distributed memory used gain scalability. context store combines functionality neural layer shift register. sensitivity machine sequence context controllable resulting different characteristic behaviours. model store predict online sequences various types length. numerical simulations model carried determine properties. many real world problems sequential nature time order events important. time built operation neural network either implicitly convolving weights samples inputs explicitly treating explicit part input thus giving spatial representation. sequence refer time order discrete symbols. approximation temporal coding. sequence machine system capable storing retrieving temporal sequences. paper develop model on-line sequence learning using hebbian one-step learning associative memories. associative memories autonomous learning capability based localised mechanism. optimal model used based variant kanerva’s sparse distributed memory intrinsically scalable shown good information efﬁciency proposed framework on-line sequence learning section tried develop optimal model based framework. also looked approaches sequence learning issue encoding past context. sequence learning problem divided sequence generation recognition prediction paper dealing problem getting best prediction single presentation sequence perfect learning patterns generating sequence many trials dealt schmidhuber elman others. treat whole sequence long string without break trying learn subsequences long unbroken sequence. three symbols. possible sequences etc. associative memories learn single associations. train memory write association give later stage recover even input give slightly corrupted noise e.g. rather building memory remember sequences need representation context symbol sequence. different sequences certain symbols common e.g. abcde wxydz. associative memory learn single associations learn sequences cannot decide successor symbol sequences different successors learning sequences recall give input cannot decide output basis present input alone. needs idea context well. thus basic sequence machine needs four components input output main memory context memory fig. gives design basic sequence machine. learning sequence machine on-line off-line. on-line learning implies separation reading writing phases. sequence input memory symbol time machine calculates output based input context using associations formerly written memory. output ‘prediction’ next input sequence. however actual next input different predicted value learns predict right value next time. off-line learning reading writing phases well separated. memory learns associations writing phase. ideal on-line sequence machine look back recent inputs necessary unique context deciding next character predicted. machine able ‘lock-on’ converge context seen earlier learn association not. inﬁnite look-back able distinguish different contexts. various approaches sequence learning general tried people past. examples recurrent nets back-propagation schmidhuber hopﬁeld nets temporal difference reinforcement learning hidden markov models self organising maps competitive nets etc. inspiration biology also used develop models short term memory sequence learning subject interest variety domains well sequence machines developed varied tasks music composition protein sequence classiﬁcation robot movements grammar learning etc. sequence learning models classiﬁed various ways architecture encoding decoding schemes static adaptive memory representation state history memory learning algorithm used closed loop open etc. mozer proposed classiﬁcation schemes classiﬁed many existing models way. schmidhuber’s method trained efﬁciently remember error signals long time lags. reinforcement learning gradient methods generally favoured literature especially practical applications concerned. encoding problem represent encode history sequence effective recover whole sequence associative memory presenting context cue. plate dealt problem encoding higher-level associations ﬁxed length vector detail. tino dealt dynamics rnn’s random initialised weights also developed prediction fractal machine similar hidden markov model encoded sequence structure points hypercube could predict well shot. models store state context entire history neural layer equivalent non-linear function past states. combine models mentioned show combined model performs better either simulations. choose particular approach many others mentioned earlier partly simplicity suitability on-line learning speed ease implementation associative memories partly number approaches thought representing cases although convoluted kernel form. symbol presented input on-line sequence memory memory learn association context input calculate output based this. divide process following three steps steps incorporate prediction learning memory seen similar input context before write anything memory expected next output predicted. hand given association writes memory. case predicted output might incorrect memory learn give correct prediction next time association presented. models memory associates context input symbol. represented vectors. rank-ordered n-of-m code exactly total neurons active order give valid code ﬁring order active neurons signiﬁcant. thorpe ﬁrst used rank-ordered codes work n-ofcodes self error-correcting ordered codes information content unordered binary ones. represent code vector order captured reducing weight successive neuron geometric ratio. thus example -of- code representing neuron ﬁring order represented symbol input alphabet given ﬁxed encoding. real valued associative memory learns associations context input vectors. function training algorithm weights weight matrix maximum outer product vectors associated weight matrix. decoding output input vectors similar similarity measured taking product vectors. principle associative memory non-negative real valued binary weights used model used modiﬁed kanerva sparse distributed memory using rank-ordered n-of-m codes encoding. original n-of-m consisted layers neurons address decoder layer whose primary purpose cast input symbol high dimensional space make linearly separable second correlation-matrix layer called data store associates ﬁrst symbol decoded ﬁrst neural layer second symbol. learning takes place layer weights ﬁrst address decoder layer stay constant. number address decoder neurons much greater number input neurons. memories proved scalable error tolerant large size address decoder layer makes memories scalable else would identical correlation matrix memories. operation described follows first data store gets input rank-ordered nof-m encoded word encoder symbols sequence stored. context outputs address decoder whose outputs data store too. data store writes association address decoder output input data. that context outputs back context layer along encoded input context layer generates context. finally context address decoder whose outputs feed data store before. data store calculates ﬁnal outputs decoded. thus whole operation proceeds discretely input character. sequence machine three primary components encoder encode input characters neural sequence memory decoder decode neural memory output back characters. encoder decoder translate input symbols desired rank-ordered n-of-m code back. characters sequence time encoder. encoder converts character appropriate neural code input neural memory. decoder decodes neural memory outputs. encoder unique encoding character input alphabet. implemented neurons encoder represented single neural layer ﬁxed weights mappings characters neural code ﬁxed behaves like lookup table. decoder similar encoder except inputs outputs reversed. however decoder must also ability distinguish characters errors errors look like characters must threshold signals weak interpreted characters. purpose decoder output closest matching character neural code also check code sufﬁciently close stored characters. represent context could ﬁxed length time window past associate next output inputs time window done time delay neural nets memory acts like shift register. relating model on-line learning framework described section step context obtained adding input shifted version context. fig. shows design shift register model. advantage using shift register model retrieve rest stored sequence length giving inputs starting middle sequence. disadvantage using model time window ﬁxed size number common symbols might greater size. shift register forgets context beyond look-back example -shift register remember previous characters best. context linear function previous inputs. recent inputs made important ones multiplying context value constant another common approach separate ‘context’ neural layer represent entire history sequence rather ﬁxed-length time window. separate neural layer would store representation context past history sequence rather last symbols shift register model. memory give input symbol want output according sequences previously learnt memory determined present input well output context layer non-linear encoding past inputs. fig. gives structure context layer based model. relating separate context neural layer model framework section step context output context neural layer whose inputs context present input symbol. context important bits input replace context context bits shifted less important bits context. thus input bits shifted context bits down. thus part represents shift register. conducted tests sequence machines described above analyse behaviour different kinds sequences. tests on-line learning symbol sequences described framework section memory expected learn previously unseen sequences single pass. three kinds sequence machine comparing namely shift register context neural layer output context neural layer ﬁxed weights whose inputs fed-back previous context input. inﬂuence context modulated multiplying context inputs neural layer constant context sensitivity factor. thus context encodes entire past history ‘state’ sequence. model resembles ﬁnite state machine used elman jordan models theoretically give unlimited look-back entire history sequence stored memory. however problem context neural layer model retrieve sequence need start retrieval beginning sequence. solve this effect context modulated using ﬁxed modulation factor. ensure past history slowly forgotten present inputs greater role past determining next outputs ensure noisy input symbol middle sequence memory learn mode beginning memory recall mode mess future outputs. shift register model separate context layer model advantages disadvantages stated sections respectively. combine memory model using separate context layer modulated context context determined input shifted version present context. context modiﬁed following step context passed ﬁxed scrambler scrambles deterministically step scrambled version context multiplied scaling factor added rank-ordered input code maximum components chosen make context. ensures table shows experiment subsequence length repeated. results averaged runs. alphabet length total sequence length memory sizes optimised parameters last experiment. combined model performs better gets perfect recalls others shift register never gets perfect recall look-back smaller number repeated characters another experiment shown fig. vary context sensitivity factor memory performance. three clear zones. machine sensitive context performs badly. means context given equal priority current input performs quite well errors. zones combined model effectively behaves like shift register. found repeated training much machine cases single pass gave fairly good results since memory number writings memory writing cases interference noise high. however cases predicted incorrectly memory performance improved repeated training sequence. context layer model contexts different time memory used experiments size address decoder neurons context neurons data memory neurons. code used -of- code. length context kept double input shift register model look-back ﬁrst experiment compare three models sequence machine analyse performance different sequence lengths. alphabet size therefore sequences characters bound include repeats. fig. comparison performance three types sequence memories shift register neural layer combined model. optimal parameters used. combined model performs better others. figure shows results ﬁrst experiment. point ﬁgure started blank memory input sequence twice. memory learns sequence ﬁrst presentation input second time check predicted output sequence accurate prediction parameters respective models optimised. neural layer combined model. combined model performs best three obtains near perfect recall. also found alphabet length gets smaller three curves diverge three memories respond differently number repeated characters sequence. alphabet three perform nearly same close perfect recall. another experiment tried study behaviour three models sequence certain number characters common. sequence type subsequences different lengths ‘common’ common subsequence. surprisingly shift register model look-back could discriminate between ‘common’ subsequences failed predict next characters. thus developed neural network model capable on-line learning recall symbol sequences. currently investigating possibilities memories implemented real time asynchronous spiking neurons. work also needs done develop suitable applications model used. built context-based associative memory inﬂuence context dynamically tuned. experiments measured performance memory storing large sequences symbols recalling successfully. tributed mem. using n-of-m codes neural networks vol. sequence learning. springer-verlag hochreiter schmidhuber long short-term memory neural baddeley working memory. oxford university press fortin agster eichenbaum critical role hippocampus memory sequences events nature neuroscience vol. mozer. neural network music composition prediction exploring beneﬁts psychoacoustic constraints multiscale processing connection science vol. cathy berry shivakumar mclarty neural networks full-scale protein sequence classiﬁcation sequence encoding singular value decomposition machine learning vol.", "year": 2006}