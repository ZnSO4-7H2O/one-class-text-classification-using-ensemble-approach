{"title": "Self-Adaptation of Activity Recognition Systems to New Sensors", "tag": ["cs.CV", "cs.LG", "stat.ML", "68T05", "K.3.2"], "abstract": "Traditional activity recognition systems work on the basis of training, taking a fixed set of sensors into account. In this article, we focus on the question how pattern recognition can leverage new information sources without any, or with minimal user input. Thus, we present an approach for opportunistic activity recognition, where ubiquitous sensors lead to dynamically changing input spaces. Our method is a variation of well-established principles of machine learning, relying on unsupervised clustering to discover structure in data and inferring cluster labels from a small number of labeled dates in a semi-supervised manner. Elaborating the challenges, evaluations of over 3000 sensor combinations from three multi-user experiments are presented in detail and show the potential benefit of our approach.", "text": "abstract traditional activity recognition systems work basis training taking ﬁxed sensors account. article focus question pattern recognition leverage information sources without minimal user input. thus present approach opportunistic activity recognition ubiquitous sensors lead dynamically changing input spaces. method variation well-established principles machine learning relying unsupervised clustering discover structure data inferring cluster labels small number labeled dates semi-supervised manner. elaborating challenges evaluations sensor combinations three multiuser experiments presented detail show potential beneﬁt approach. keywords opportunistic activity recognition unsupervised learning semi-supervised learning classiﬁer adaptation today state-of-the-art approaches activity context recognition typically assume ﬁxed narrowly deﬁned system conﬁgurations dedicated often also narrowly deﬁned tasks. systems work sensors known training phase cannot adapt sensors environment. turn sensors evermore present life although always available. moving around person face highly instrumented environments places little intelligent infrastructure. concerning on-body sensing user carry varying collection sensor enabled devices diﬀerent dynamically varying body locations thus order realize full potential systems need take advantage devices happen environment taking account current placement relevance. previous work investigated on-body position orientation on-body sensors inferred position shifts tolerated sensor replace another started work challenge seamless sensor january preprint submitted computational intelligence neuroscience integration. precisely means answer question sensor’s data integrated existing activity recognition system runtime order improve recognition process. extending system used sensors uses many challenges. instance training data expensive thus cannot expect data labeled. moreover activity performed multiple diﬀerent ways i.e. training data vary signiﬁcantly other. article tackle problems presenting method include sensors existing system unsupervised manner. achieved using concepts techniques ﬁeld unsupervised learning. nutshell improve current n-dimensional system assuming structure dimension related class distribution. course assumption always hold leading possible downgrade system performance. order counter issue evaluate balance possible gain conformance class distribution sensor data. also deal variance training data applying standard machine learning techniques bagging. improvement guaranteed show practice method good harm providing overall improvement even labeling data scarce. furthermore study performance presence user input used validate system. article presents ﬁrst important step towards method self-adapting multisensor systems. experiments extend one-dimensional two-dimensional system answer question actually integrate sensor became available. questions focus ongoing research ﬁeld. remainder article organized follows section analyzes related work section sketches challenges solution ideas. evaluation strategy section basic self-adaptation method explained section sections focus four concerted extensions basic method. section summarizes ﬁndings gives outlook future research. incorporation information sources interpreted form transductive transfer learning described ﬁeld covers general problem transferring knowledge learned domain another eventually even diﬀerent task. within authors description domain comprises feature space marginal probability distribution occurrence samples therein well task i.e. classiﬁcation composed label space objective predictive function dimensionality input spaces sensor adaptation domains diﬀerent whereas classiﬁcation task stays same even utilization information sources. even though authors give restrictive deﬁnition domain adaptation usual approach considered special case area. duan proposed domain adaptation method heterogeneous feature spaces. ﬁrst project source target feature spaces common subspace apply standard learning algorithm. authors achieved promising results computer vision data dimensional feature spaces. contrast work explicitly assumes addition additional sensors i.e. target space contains additional features common subspace equal source feature space. furthermore feature spaces activity recognition scenarios usually signiﬁcantly less dimensions dimension related signal physical sensor features often selected manually. need large amount annotated training data widely recognized major issue practical deployment activity recognition systems. consequence diﬀerent approaches studied overcome problem. line work looks unsupervised discovery structure sensor data others attempt develop activity models online information common sense data base wordnet general resources beyond fully unsupervised approaches also considerable interest semisupervised systems limit amount data needed training general ﬁelds semi-supervised learning lower extent also active learning relevant work presented article. makes labeled unlabeled data train prediction model. therefore falls unsupervised learning supervised learning based generative models probabilistic models density separation algorithms graph-based methods instance. best known example second transductive support vector machines build connection density model data discriminative decision boundary putting boundary sparse regions. tsvm uses unlabeled data semi-supervised manner. typical example third class laplacian graph based models called lapalacian graphs semi-supervised learning. detailed overview semi-supervised learning given ﬁeld membership query learning stream-based active learning pool-based active learning important learning paradigms. relevant generate artiﬁcial samples cannot understood labeled human experts similar work focuses sample streams. sample typically drawn once system must decide whether query expert discard sample builds ranking given pool unlabeled samples depending certain selection strategy chooses samples must labeled. number diﬀerent selection strategies exist purpose four main categories distinguished uncertainty sampling strategies select samples considered classiﬁer uncertain density weighting strategies consider samples’ distribution input space estimated error reduction strategies reducing generalization error classiﬁer directly diversity sampling strategies prevent selection redundant samples sample selected query round applied co-clustering train classiﬁer unlabeled documents target domain labels documents source domain known. co-clustering reveal features similar help classifying items diﬀerent domains capable using features improve classiﬁcation. co-training applicable case source target feature spaces conditionally independent goal extend training data expand classiﬁer extended input space. closest work uses sensors behavioral assumptions train system take advantage sensor. however unlike work consider adding sensor existing system. related work methodological level none articles mentioned addresses problem integrating sensors existing system. consequently work aims continuous self-improvement process part activity recognition systems. context reﬂects motivation systems increasingly dynamic heterogeneous system compositions within scope research initiatives dealing self-management autonomous self-adaptation described e.g. order able deal adaptive sensor integration terms self-adaptation activity recognition systems propose combine eﬀorts domain semisupervised learning approach uses classiﬁer trained features provide labeled data points dimensional feature space. sake simplicity better understanding consider transition one-dimensional two-dimensional space here. start classiﬁer trained data single sensor assume classiﬁer performing reasonably well still regions feature space separation classes poor. method crucial known regions feature space classiﬁer conﬁdent many misclassiﬁcations. tree classiﬁer trivially given purity leaves retained training. certain stage second sensor appears sensor potentially improve overall class separation. collect cluster data points space remaining question assign labels clusters. project clusters onto space sensor consider regions space onto points single cluster projected. assumption structure corresponding class distribution conclude class distribution within regions representative class distribution corresponding clusters. seen fig. performance gain comes clusters partially projected onto regions classiﬁer conﬁdent partially onto regions produces many errors. former case classiﬁer provides labels clusters. latter case classiﬁer outperforms classiﬁer. note basic method necessarily require one-to-one correspondence clusters classes. example consider region space corresponds mixture classes. assume region corresponds clusters mixture classes. follows using cluster information reduces probability misclassiﬁcation region. clearly method works well sample clusters fig. distributions carefully constructed. real data sets ideal distributions unlikely. applied data naive method described able achieve improvement clusters label using pure regions classiﬁer appear together impure region could improved. furthermore variation distributions samples signiﬁcant even lead performance decrease. remainder section systematically outline challenges basic method requires cluster region where projected onto space overlap cluster. unfortunately always hold. clusters hidden shown fig. assign labels hidden clusters propose similarity search. ﬁrst assign labels hidden clusters. attempt assign labels hidden clusters that labels propagated cluster’s data points projection data points onto space approximates original distribution good possible. figure illustration methods assigning labels feature space clusters. dashed lines boundaries tree trained according cluster membership training data. another distribution-related issue addresses speciﬁc choice tree classiﬁer stores class distributions leaf level. means order able assign class distribution cluster cluster must cluster appearing leaf. unfortunately real data often situation clusters projected onto distinct areas space areas fall within leaf making class distribution assignment impossible. address problem create additional tree feature space using purity respect cluster membership rather class membership splitting criterion. original training data classiﬁer assign class distributions leaves turn used assign distributions clusters. machine learning algorithms build assumption training data reasonably representative break assumption substantially violated. activity recognition training data variance often problem costly obtain large amount labeled data users often large variations humans perform even fairly simple actions. time method particularly sensitive quality training data leverages small areas feature space label larger ones using small areas derive labels larger ones eﬀectively means artiﬁcially reducing sample size thus making estimator sensitive sample variance. practice would often less data cluster labeling. training data happens high variance representative addition sensor fail improve performance actually drastically reduce classiﬁcation accuracy entire feature space. described later found erroneous conﬁgurations signiﬁcant problem real-world activity recognition data. fact additional measures taken risk ending erroneous conﬁgurations worsen recognition performance nearly neutralizes potential improvement integration additional sensor. solve problem developed three approaches discarding conﬁgurations high probability leading performance degradation. first every assignment class distributions clusters compute plausibility gain values. plausibility based projections labeled clusters onto feature space. compares class distribution results projection class distribution original training data thus oﬀers measure plausible guessed labels are. gain compares purity labeled clusters purity corresponding leaves original classiﬁer. estimation potential particular cluster labeling improving system performance. larger discrepancy measures likely speciﬁc cluster labeling caused sample variance correspond true class distribution. overall gain plausibility proved useful deciding whether makes sense particular cluster labeling not. however well possible solution high plausibility high gain distorted sample variance worsen rather improve performance additional measure address problem rely bagging. derive cluster structure class distribution diﬀerent subsets training data created original randomly leaving replicating individual samples consider clusters class distribution assignments signiﬁcantly vary subsets unstable respect training data variance penalize plausibility value. ﬁnal measure consider user-provided additional labels detect erroneous conﬁgurations. apply both original classiﬁer classiﬁer generated method data points compare output. output diﬀers user provide label check classiﬁer would improve worsen performance reject classiﬁer erroneous makes pre-deﬁned number mistakes within certain number data points. looking points classiﬁers diﬀer even user-provided labels high probability spotting erroneous conﬁgurations little probability mistakenly rejecting improve classiﬁcation rate relevant factor. correct number clusters managed avoid suboptimal conﬁgurations related local minima. general neither assumed. thus rely hierarchical clustering algorithm provide possible conﬁgurations choose promising generating cluster labels conﬁguration evaluating plausibility gain metrics. base work described article tree classiﬁer. tree classiﬁer region classiﬁer represented single leaf classiﬁer tree ancestor nodes deﬁne region’s boundaries additional dimensions easily added simply replacing leaf subtree adds boundaries extra axis without aﬀecting regions. label inferring assigning class distributions clusters using areas feature space onto single cluster projected involves creating tree trained according cluster rather class membership evaluate method data previously published data sets bicycle repair data maintenance data opportunity data three data sets study diﬀerent sensor combinations users executing diﬀerent activities individual activity instances. sensors question widely varying recognition rates mean individual sensor mean combination sensors trained fully supervised manner baseline. features sensors described allow mean variance. setting decouple problem investigated work issue feature choice complex inﬂuence recognition success. time features commonly used motion-related activity recognition. given unknown sensor system would certainly able identify sophisticated features would work known work reasonably well across many sensors. evaluate method combination diﬀerent sensors; train base recognition system ﬁrst integrate second sensor. mentioned earlier treat feature single sensor evaluation. thus also consider improving classiﬁer another feature sensor even feature diﬀerent axis sensor. furthermore includes cross-modality situations system trained sensor improved sensor completely diﬀerent modality order avoid improving shortcomings badly trained recognition system chose parameters base classiﬁer individually combination base system comparison system trained features perform best training set. evaluation purposes select trained decision tree classiﬁer base classiﬁer. normalize features real data sets unit interval evaluation need three separate data sets training adapting testing. training data needed training initial classiﬁer uses feature labels. data need adapting original classiﬁer features labels data testing performance features plus labels. stated otherwise size adapting test data sets half size training data set. sensor combination evaluate diﬀerent splittings shuﬄing data time splitting three sets. order assess structure extended feature space apply agglomerative hierarchical clustering algorithm. resulting cluster tree gives possibility explore structure diﬀerent levels detail without deciding number clusters yet. traverse cluster tree down starting clusters splitting cluster step maximum number clusters reached. step attempt infer label distribution cluster based information original classiﬁer model build adapted classiﬁer model extended feature space utilizing label information clusters. adapted models rated based possible gain classiﬁcation accuracy best selected. method rating diﬀerent solutions allows automatically ﬁnding appropriate number clusters also makes possible choose helpful among diﬀerent features available sensor even among diﬀerent sensors. described section basic method assigning labels clusters based projecting clustered data points back onto original feature space. identify regions original feature space single cluster projected onto multiple clusters projected onto. refer single-cluster multi-cluster regions single-cluster regions class distribution found original model region directly related cluster used infer cluster label distribution. multi-cluster regions however clear class distribution region correctly partitioned among involved clusters unless distribution pure i.e. contains single class only. ﬁrst challenge distinguishing singlemulti-cluster regions essence classiﬁcation task itself. found decision tree classiﬁer good choice solving problem. amount pruning controls granularity stability noise. thus actual process cluster labeling project clustered data points onto original feature space train decision tree based cluster membership. regions tree purity preset purity threshold treated single-cluster regions. label assignment back training data used original classiﬁer. estimate class density leaves tree trained clusters correspond single-cluster regions. ﬁnal step label distribution transferred corresponding clusters normalized. clusters appear single-cluster regions hand label distributions assigned them distributions used extend classiﬁcation model dimension. obviously extension makes sense multi-cluster regions original feature space since single-cluster regions class distribution identical feature spaces. furthermore extension possible multi-cluster regions clusters labeled. explained section fig. clusters hidden points cluster projected onto single cluster region thus multi-cluster regions clusters labeled split according cluster membership points. decision tree classiﬁer means leaf original classiﬁer falls multi-cluster region replaced subtree leaf subtree corresponding diﬀerent cluster. essence method amounts generalizing class distribution small subset points entire cluster subset taken. generalization becoming valid conditions must fulﬁlled. first class distribution must homogeneous throughout cluster i.e. split separate regions class distribution must region. note necessary points within cluster belong class instead probability point belonging given class must everywhere within cluster. main questions does feature space contain enough correspondence structure class distribution clusters homogeneous class distribution? clustering algorithm able clusters?. second subset points labeling must good representation class distribution within cluster. main concerns sample size relation sample variance actual homogeneity distribution within cluster. thus envisage situation where overall class distribution within cluster homogeneous enough justify assigning single distribution cluster small single-cluster regions collect labels variations would considerable. clearly regions used derivation class distribution. sensor used create feature space contains additional information classiﬁcation problem hand well possible requirements fulﬁlled. however means guaranteed. thus example fundamental reason additional sensor result distribution spatially well separated clusters separation boundary classes located within clusters instead them. furthermore even feature space exists cluster structure satisﬁes conditions guarantee clustering algorithm unfortunately applying method distribution satisfy conditions general leads signiﬁcant performance decrease time proposed system able deal arbitrary additional sensor without prior knowledge class distribution feature space. problem herewith that given clustering label distribution feature space exactly determine quality label assignment. consequence system relies following heuristic derivation plausibility based observation that—while inclusion sensor changes classiﬁcation individual data points—the overall class distribution within region feature space remain unchanged. thus propose take data points extended feature space probability class within region ˆpri probability class region approximated data extended feature space classiﬁed adapted classiﬁer. plausibility normalized maximal possible distance class distributions. second measure estimate potential classiﬁcation accuracy gain. since consider tree classiﬁer deﬁned increase purity achieve extending classiﬁer sensor. described create extended classiﬁer splitting leaves classiﬁcation tree according membership clusters feature space. leaf tree compute purity training data best case leaves region pure hence purity would however leaf would still classify points incorrectly relative size leaf would larger probability corresponding class original training data within region. note multiple leaves considered together contribute class. hence purity contribution leaf maximally amount probability corresponding class original training data. thus obtain purity resulting leaves region histogram intersection class distribution initial class distribution gain value computed separately region original classiﬁer summed weighted average whole solution. weights relative amount training data points region. given solution plausible gain value allows quantify utility increasing classiﬁcation performance. therefore measure used comparing solutions diﬀerent cluster levels discarding ones plausible enough. choose plausible solution highest gain best number clusters. furthermore gain measure also help quantify utility diﬀerent sensors features case multiple available would help selecting ones best contribute problem. parameter denotes minimal number data instances allowed leaf cluster-based decision tree built method order labels clusters. decreasing parameter means allowing ﬁne-grained search single-cluster regions needed reveal labels clusters overlapped others. decreasing often leads single-cluster regions also increases sensitivity noise. results three real data sets summarized fig. histograms display number sensor combinations accuracy change unsupervised change accuracy achieved method. display number combinations desirable undesirable results. regarding fig. lowering purity threshold clearly reducing number grade undesirable results keeping nearly desirable ones. lowest threshold number desirable results strongly reduced too. eﬀect also visible results less grained cluster-based trees rising parameter i.e. increasing minimal allowed leaf size cluster-based tree used ﬁnding cluster labels reduces undesirable results also many desirable ones. judging histograms blue result fig. seems reasonable. features many high grade desirable results median undesirable ones smaller median accuracy change. figure results variants diﬀerent parameterizations three real-world data sets plot shows results four plausibility thresholds four levels user provided labels respectively. bounding boxes positive negative results highlighted incl. percentiles. section describe extension system addresses hidden cluster problem step fig. core idea perform exhaustive search possible labellings hidden clusters maximizing plausibility overall conﬁguration. search done level multi-cluster regions original feature space illustration principle consider multi-cluster region original feature space clusters projected onto feature space already class distributions assigned need estimating label distribution unlabeled cluster based fact plausibility deﬁned maximal distribution original training data identical distribution resulting projection points respective clusters onto feature space. course method work clusters projected onto multi-cluster region contains unlabeled cluster case diﬀerence distributions correspond class distributions union clusters information distribution within individual clusters. possible solution stems fact that general cluster projected onto multi-cluster region. thus cluster also projected onto region within cluster unlabeled cluster assigned distribution. means within unlabeled cluster also assigned distribution. general case consider dependency graph rather cluster pairs individual regions. dependency graph node cluster edges connecting pair clusters appear together region. iteratively traverse dependency graph starting nodes label propagating information along edges label nodes method terminates propagating information newly labeled cluster lead nodes assigned distribution. note termination necessarily mean unambiguous distributions found clusters. however results presented next section show signiﬁcant number clusters labeled method clearly outperforms naive approach described previous section. ﬁnal concern respect similarity search computational complexity. complete search possible labellings leads combinatoric explosion classes clusters candidates verify region. however perform complete search general. connected components dependency graph form disjoint groups clusters. sets regions groups clusters appear disjoint too. means labeling clusters group aﬀect labeling cluster search best labeling group clusters individually. thus maximally candidates verify |gi| denotes number clusters i-th group. maximal size connected components expected small activity recognition scenarios. related maximal number classes initial classiﬁer confuses. evaluation real world data sets proceed described section figs. provide detailed overview results diﬀerent levels purity thresholds applied. results conﬁrm observations previous section similarity search leads higher improvement also carries higher risk performance decrease faced data violates basic assumptions method built. thus example plth=. improve resulting accuracy cases results similarity search method presented previous section reveal potentially signiﬁcant performance improvement also sensitivity noise data sets. here show well-known bagging technique adapted approach prevent method misled noise sample variance. small change data produces diﬀerent solution i.e. cluster getting labeled diﬀerently clustering deviates strong indication either ambiguous situation over-ﬁtting cases want avoid. thus ﬁltering solutions desirable exactly achieved bagging. thus starting given unlabeled data form replicas size drawing instance random replacement. instance appear repeated times replica. method individually replicated data aggregate resulting classiﬁers. least certain percentage classiﬁers agree majority result taken. otherwise fall back result initial classiﬁer without improvement. case decision tree classiﬁers base experiments merge aggregated classiﬁers resulting bagging step single decision tree. simpliﬁes classiﬁcation afterwards single tree hast traversed. method integrating sensor splits regions original classiﬁer leaves decision tree along dimension feature space. thus leaf replaced subtree providing leaves split region. merging aggregated classiﬁers ﬁrst overlay subtrees region successively assign majority label split regions least bagging threshold percent subtrees agree assign initial label region split regions. adjacent split regions identical label joined. improvement obtained real sensor data presented figs. relevant results obtained threshold threshold bagging similarity search results range comparable observed simple method described section i.e. less negative also less positive cases hand threshold negative results reduced much positive ones. thus positive results nearly times negative results also plth cases negative accuracy change cases achieve improvement average. summary combination similarity search bagging brings system line doing much good harm vision makes useful real-life applications. extend process depicted fig. particular extra step cluster labels determined label inferring similarity search methods. note that unlike classical semi-supervised methods expect enough labeled instances directly assign class distributions clusters. instead assume clusters labeled user-provided information function anchors increasing eﬀectiveness similarity search. described section similarity search assign class distribution unlabeled cluster unlabeled cluster multi-cluster region. time considers dependency graphs clusters nodes multi cluster regions edges. information newly assigned distribution propagated along graph reducing number unlabeled clusters multi cluster regions. thus best labeling single cluster cascade leading clusters previously fully unlabeled dependency graph assigned correct distributions. obviously default case. cases cascade eﬀect all. however assumption average signiﬁcant multiplicative eﬀect small number user provided labels lead signiﬁcant performance improvement. applied real world data sets positive eﬀect pre-labeling also clearly visible. fig. shows results pre-labeling combination similarity search method number cases negative accuracy change decreases four labels eight labels time number positive outcomes increase four labels eight labels. improvement particularly well visible improvements goes labels four eight. pre-labeling method previous section meant increase number clusters similarity search method label—which essentially means increasing amount improvement system generate. reduction number sensor combinations caused performance decrease by-product additional correctly labeled clusters oﬀset wrong labels statistics. contrast section present method using user-provided labels directly detect discard classiﬁer variants lead performance degradation. general idea based observation assess whether classiﬁer using additional sensor increases decreases classiﬁcation accuracy need consider data points classiﬁer disagree. consequence actively user labels instances classiﬁers disagree. reject classiﬁer fails test criterion deﬁned correct least certain predeﬁned number times. general keep number labels small consider instances disagreement classiﬁer require correct least cases. fault reduction method applied right cluster conﬁgurations labeled assigned ﬁnal gain plausibility values without fault reduction solution best gain/plausibility trade-oﬀ would chosen solutions discarded. fault reduction take best solutions parallel classiﬁer described above. solution fails test criterion discarded. solutions discarded consider sensor combination useful otherwise pick best gain/plausibility trade-oﬀ surviving classiﬁers. alike methods presented article fault reduction heuristic works often guaranteed always right. thus classiﬁer that overall leads signiﬁcant performance degradation instances acquire labels happen ones right. hand reject good classiﬁer picking instances wrong. however shown below strength method theoretic model signiﬁcantly skewed favor keeping good classiﬁers rejecting poor ones given random sample sequences. function better performing classiﬁers likely accepted perform worse initial classiﬁer eﬀect ampliﬁed increasing number tested instances time classiﬁers small improvement tend rejected well. larger overlap classiﬁers strongly ﬁltered even classiﬁers small improvement higher probability accepted maximal possible improvement restricted overlap. identical classiﬁers function undeﬁned. case disagreement instances test evaluation purposes select user-provided labels randomly achieved selecting ﬁrst instances adaptation meet method’s requirements augmenting ground truth measure amount user provided information number disagreements classiﬁers considered test criterion. note since running test several classiﬁers necessarily equal number labels. best case equal worst case need diﬀerent labels classiﬁers. thus evaluation provide average number labels system used setting number disagreements histograms fig. conﬁrm eﬀect real-world data sets. average four labels already reduces number faults still keeping occasions accuracy improved. userprovided labels faults decrease without aﬀecting positive outcomes much moreover combining pre-labeling fault reduction labels positive outcomes even increased negative ones. article presented evaluated novel techniques enable activity recognition systems self-adapt sensor information becoming available environment. techniques show promising results real data details) aware practical applicability still quite limited. first extension sensor second investigated. practice multi-sensor systems used transition high dimensional space anything trivial. second approach consists limited number heuristics applied speciﬁc classiﬁer type theoretical underpinning comparison methods. finally many important aspects feature selection online adaptation long term evaluation overall system architecture investigated. recognized this deﬁned roadmap future research addressing following challenges appropriate features tasks chosen autonomously? assuming generative probabilistic classiﬁers static system behavior shall used self-adaption realized? assuming discriminative dynamic classiﬁers shall used solution extended? humans integrated run-time eﬃcient eﬀective means active learning techniques? ensure continuous modiﬁcations system conﬁguration lead long term improvement unbounded performance degradation overall system? quality metrics needed various techniques evaluated case studies? altogether research provide technologies self-improvement self-healing multi-modal sensor systems ﬁeld work supported eu-funded opportunity project authors would like thank tobias reitmaier providing information active semi-supervised learning. work funded german research foundation within project organic computing techniques run-time selfadaptation multi-modal activity recognition systems", "year": 2017}