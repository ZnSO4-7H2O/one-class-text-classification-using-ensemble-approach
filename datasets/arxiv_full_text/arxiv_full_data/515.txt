{"title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "We propose a technique for learning representations of parser states in transition-based dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks---the stack LSTM. Like the conventional stack data structures used in transition-based parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser's state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance.", "text": "extend last line work learning representations parser state sensitive complete contents parser’s state complete input buffer complete history parser actions complete contents stack partially constructed syntactic structures. global sensitivity state contrasts previous work transitionbased dependency parsing uses narrow view parsing state constructing representations although parser integrates large amounts information representation used prediction time step constructed incrementally therefore parsing training time remain linear length input sentence. technical innovation lets variation recurrent neural networks long short-term memory units parsing model uses three stack lstms representing input representing stack partial syntactic trees representing history parse actions encode parser states since stack partial syntactic trees contain individual tokens partial syntactic structures representations individual tree fragments computed compositionally recursive neural networks. parameters learned backpropagation obtain state-of-the-art results chinese english dependency parsing tasks propose technique learning representations parser states transitionbased dependency parsers. primary innovation control structure sequence-to-sequence neural networks— stack lstm. like conventional stack data structures used transitionbased parsing elements pushed popped stack constant time addition lstm maintains continuous space embedding stack contents. lets formulate efﬁcient parsing model captures three facets parser’s state unbounded look-ahead buffer incoming words complete history actions taken parser complete contents stack partially built tree fragments including internal structures. standard backpropagation techniques used training yield state-of-the-art parsing performance. transition-based dependency parsing formalizes parsing problem series decisions read words sequentially buffer combine incrementally syntactic structures formalization attractive since number operations required build projective parse tree linear length sentence making transition-based parsing computationally efﬁcient relative graphgrammarbased formalisms. challenge transitionbased parsing modeling action taken unboundedly many states encountered parser progresses. section provide brief review lstms deﬁne stack lstms notation. follow convention vectors written lowercase boldface letters matrices written uppercase boldface letters scalars written lowercase letters structured objects sequences discrete symbols written lowercase bold italic letters discussion dimensionality deferred experiments section long short-term memories lstms variant recurrent neural networks designed cope vanishing gradient problem inherent rnns rnns read vector time step compute state applying linear concatenation previous time step’s state input passing logistic sigmoid nonlinearity. although rnns principle model long-range dependencies training difﬁcult practice since repeated application squashing nonlinearity step results exponential decay error signal time. lstms address extra memory cell constructed linear combination previous state signal input. lstm cells process inputs three multiplicative gates control proportion current input pass memory cell proportion previous memory cell forget updated value memory cell input computed follows component-wise logistic sigmoid function component-wise product. value lstm time step controlled third gate applied result application nonlinearity stack long short-term memories conventional lstms model sequences leftto-right order. innovation augment lstm stack pointer. like conventional lstm inputs always added right-most position stack lstms current location stack pointer determines cell lstm provides computing memory cell contents. addition adding elements sequence stack lstm provides operation moves stack pointer previous element thus lstm understood stack implemented contents never overwritten push always adds entry list contains back-pointer previous updates stack pointer. control structure schematized figure querying output vector stack pointer points continuous-space summary contents current stack conﬁguration available. refer value stack summary. goldberg propose similar stack construction prevent stack operations invalidating existing references stack beam-search parser must maintain priority queue stacks. figure stack lstm extends conventional left-to-right lstm addition stack pointer ﬁgure shows three conﬁgurations stack single element result operation result applying push operation boxes lowest rows represent stack contents inputs lstm upper rows outputs lstm middle rows memory cells gates. arrows represent function applications refer speciﬁcs. although architecture best knowledge novel reminiscent recurrent neural network pushdown automaton added external stack memory rnn. however architecture provides embedding complete contents stack whereas made stack visible rnn. turn problem learning representations dependency parsers. preserve standard data structures transition-based dependency parser namely buffer words processed stack partially constructed syntactic elements. stack element augmented continuous-space vector embedding representing word case syntactic dependents. additionally introduce third stack represent history actions taken parser. stacks associated stack lstm provides encoding current contents. full architecture illustrated figure review components turn. dependency parser initialized pushing words representations input sentence reverse order onto ﬁrst word root symbol bottom contain emptystack token. time step parser computes composite representation stack states uses predict action take updates stacks. processing completes empty contains elements representing full parse tree headed root symbol empty-stack symbol history operations taken parser. learned parameter matrix stack lstm encoding input buffer stack lstm encoding stack lstm encoding bias term passed component-wise rectiﬁed linear unit nonlinearity figure parser state computation encountered parsing sentence overhasty decision made. designates stack partially constructed dependency subtrees lstm encoding; buffer words remaining processed lstm encoding; stack representing history actions taken parser. linearly transformed passed relu nonlinearity produce parser state embedding afﬁne transformation embedding passed softmax layer give distribution parsing decisions taken. column vector representing embedding parser action bias term action represents valid actions taken given current contents stack buffer. since encodes information previous decisions made parser chain rule invoked write probability valid sequence parse actions conditional input arc-standard? arc-standard transitions parse sentence left right using stack store partially built syntactic structures buffer keeps incoming tokens parsed. parsing algorithm chooses action conﬁguration means score. arc-standard parsing dependency tree constructed bottom-up right-dependents head attached subtree dependent fully parsed. since parser recursively computes representations tree fragments construction order guarantees syntactic structure used modify head algorithm another head dependent structure. means evaluate composed representations tree fragments incrementally; discuss strategy token embeddings oovs represent input token concatenate three vectors learned vector representation word type ﬁxed vector representation neural language model learned representation token provided auxiliary input parser. figure parser transitions indicating action applied stack buffer resulting stack buffer states. bold symbols indicate embeddings words relations script symbols indicate corresponding words relations. figure token embedding words decision present parser’s training data language model data overhasty adjective present parser’s training data present data. architecture lets deal ﬂexibly outof-vocabulary words—both limited parsing data present pretraining words both. ensure estimates oovs parsing training data stochastically replace singleton word type parsing training data token training iteration. pretrained word embeddings. veritable cottage industry exists creating word embeddings meaning numerous pretraining options ˜wlm available. however syntax modeling problems embedding approaches discard order perform less well therefore used variant skip n-gram model introduced ling named structured skip n-gram different parameters used predict context word depending position relative target word. hyperparameters model skip n-gram model deﬁned wordvec window size used negative sampling rate epochs unannotated corpora described composition functions recursive neural network models enable complex phrases represented compositionally terms parts relations link follow previous line work embedding dependency tree fragments present stack vector space token embeddings discussed above. particular challenge syntactic head general arbitrary number dependents. simplify parameterization composition function combine headmodiﬁer pairs time building complicated structures order reduced parser illustrated figure node expanded syntactic tree value computed function three arguments syntactic head dependent syntactic relation satisﬁed deﬁne concatenating vector embeddings head dependent relation applying linear operator component-wise nonlinearity follows trained parser maximize conditional log-likelihood treebank parses given sentences. implementation constructs computation graph sentence runs forwardbackpropagation obtain gradients dimensions chosen based intuitively reasonable values conﬁrmed development data performed well. future work might carefully optimize parameters; reported architecture strikes balance minimizing computational expense ﬁnding solutions work. data used data setup chen manning namely english chinese parsing task. baseline conﬁguration chosen since likewise used neural parameterization predict actions arc-standard transition-based parser. english used stanford dependencency treebank used closest model published splits. part-of-speech tags predicted using stanford tagger accuracy treebank contains negligible amount non-projective arcs representation depenfigure computed redency subtree cursively applying composition functions case head modiﬁer relation triples. multiple dependents single head recursive branching order imposed order parser’s reduce operations objective respect model parameters. computations single parsing model single thread cpu. using dimensions discussed next section required hours reach convergence held-out set. parameter optimization performed using stochastic gradient descent initial learning rate learning rate updated pass training data number epochs completed. momentum used. mitigate effects exploding gradients clipped norm gradient applying weight update rule penalty dimensionality. full version parsing model sets dimensionalities follows. lstm hidden states size layers lstms stack. embeddings parser actions used composition functions dimensions output embedding size dimensions. pretained word embeddings dimensions dimensions learned word embeddings experimental conﬁgurations report results experimental conﬁgurations language well chen manning baseline. full stack lstm parsing model stack lstm parsing model without tags stack lstm parsing model without pretrained language model embeddings stack lstm parsing model uses head words stack instead composed representations full parsing model rather lstm classical recurrent neural network used results following chen manning exclude punctuation symbols evaluation. tables show comparable results chen manning show model better model development test set. various ablated conditions report. exception −pos condition chinese parsing task underperform baseline although still obtain reasonable parsing performance limited case. note predicted tags english little value—suggesting think parsing sentences directly without ﬁrst tagging them. also using composed representations dependency tree fragments outperforms using representations head words alone implications theories headedness. finally lstms outperform baselines classical rnns still quite capable learning good representations. effect beam size. beam search determined minimal impact scores therefore results report used greedy decoding—chen manning likewise report results greedy decoding. ﬁnding line previous work generates sequences recurrent networks although vinyals report much substantial improvements beam search grammar foreign language parser. approach ties together several strands previous work. first several kinds stack memories proposed augment neural architectures. proposed neural network external stack memory based recurrent neural networks. contrast model entire contents stack summarized single value model network could contents stack. mikkulainen proposed architecture stack summary feature although stack control learned latent variable. variety authors used neural networks predict parser actions shift-reduce parsers. earliest attempt aware mayberry miikkulainen resurgence interest neural networks resulted several applications transition-based dependency parsers works conditioning structure manually crafted sensitive certain properties state conditioning global state object. like stenetorp used recursively composed representations tree fragments neural networks also used learn representations chart parsing lstms also recently demonstrated mechanism learning represent parse structure.vinyals proposed phrasestructure parser based lstms operated ﬁrst reading entire input sentence obtain vector representation generating bracketing structures sequentially conditioned representation. although superﬁcially similar model approach number disadvantages. first relied large amount semi-supervised training data generated parsing large unannotated corpus off-the-shelf parser. second recognized stack-like shiftreduce parser control provided useful information made word stack visible training decoding. third although impressive feat learning entire parse tree represented vector seems formulation makes problem unnecessarily difﬁcult. finally work understood progression toward using larger contexts parsing. exhaustive summary beyond scope paper important milestones tradition cube pruning efﬁciently include nonlocal features discriminative chart reranking approximate decoding techniques based relaxations graph-based parsing include higherorder features randomized hill-climbing methods enable arbitrary nonlocal features global discriminative parsing models since parser sensitive part input history stack contents similar spirit last approach permits truly arbitrary features. presented stack lstms recurrent neural networks sequences push operations used implement state-of-theart transition-based dependency parser. conclude remarking stack memory offers intriguing possibilities learning solve general information processing problems here learned observable stack manipulation operations computed embeddings ﬁnal parser states used prediction. however could reversed giving device learns construct context-free programs given observed outputs; application would unsupervised parsing. extension work would make alternative architectures explicit external memory neural turing machines memory networks however models without supervision stack operations formidable computational challenges must solved sampling techniques techniques reinforcement learning promise making intriguing avenue future work. authors would like thank lingpeng kong jacob eisenstein comments earlier version draft danqi chen assistance parsing datasets. work sponsored part army research laboratory army research ofﬁce contract/grant number wnf--- part career grant iis-. miguel ballesteros supported european commission contract numbers fp-ict hria- sreerupa giles guozheng sun. learning context-free grammars capabilities limitations recurrent neural network external stack memory. proc. cognitive science society. andr´e martins noah smith eric xing pedro aguiar m´ario figueiredo. turboparsers dependency parsing approximate variational inference. proc. emnlp. joakim nivre. incrementality deterministic dependency parsing. proceedings workshop incremental parsing bringing engineering cognition together. richard socher eric huang jeffrey pennington andrew christopher manning. dynamic pooling unfolding recursive autoencoders paraphrase detection. proc. nips. richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. proc. emnlp. huihsin tseng pichuan chang galen andrew daniel jurafsky christopher manning. conditional random ﬁeld word proc. segmenter sighan bakeoff fourth sighan workshop chinese language processing.", "year": 2015}