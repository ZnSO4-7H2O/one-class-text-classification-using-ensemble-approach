{"title": "On Inductive Abilities of Latent Factor Models for Relational Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Latent factor models are increasingly popular for modeling multi-relational knowledge graphs. By their vectorial nature, it is not only hard to interpret why this class of models works so well, but also to understand where they fail and how they might be improved. We conduct an experimental survey of state-of-the-art models, not towards a purely comparative end, but as a means to get insight about their inductive abilities. To assess the strengths and weaknesses of each model, we create simple tasks that exhibit first, atomic properties of binary relations, and then, common inter-relational inference through synthetic genealogies. Based on these experimental results, we propose new research directions to improve on existing models.", "text": "latent factor models increasingly popular modeling multi-relational knowledge graphs. vectorial nature hard interpret class models works well also understand fail might improved. conduct experimental survey state-of-the-art models towards purely comparative means insight inductive abilities. assess strengths weaknesses model create simple tasks exhibit ﬁrst atomic properties binary relations then common inter-relational inference synthetic genealogies. based experimental results propose research directions improve existing models. many machine learning ﬁelds research drifting away ﬁrst-order logic methods. time drift justiﬁed better predictive performances scalability methods. especially true link prediction core problem statistical relational learning latent factor models became popular logic-based models link prediction knowledge graphs—also known knowledge graph completion— operates predicates pairs entities objects knowledge graphs. diﬀerent predicate symbol called relation grounded relation called fact. example given entities alice relations mother grandmother mother mother true facts grandmother also true. inferring last fact ﬁrst however requires knowing mother one’s mother one’s grandmother expressed ﬁrst-order logic-based link prediction consists using observed facts logical rules infer truth unobserved facts. achieved deterministically logical deduction probabilistically cope uncertainty data beyond known problems complexity brittleness obvious limitation arises setup logical rules knowledge graph relations required inference many knowledge graphs provide observed facts case must either handcraft rules learn them generally inductive logic programming methods latent factor models suﬀer limitation learned model never represented explicitly symbolic rather vectorial embeddings entities relations. representations make model diﬃcult interpret although show better predictive abilities explored well models able overcome absence logical rules inference abilities diﬀer logic-based models. evaluate state-of-the-art latent factor models relational learning synthetic tasks designed target speciﬁc inference ability well discover structure data. interested evaluating inductive abilities models ability cope uncertainty design synthetic experiments noise-free deterministic data. choice favorable setup deterministic logical inference clariﬁes approach followed paper purpose evaluate latent factor models means point weaknesses stimulate research towards models suﬀer combinatorial complexity— advocated bottou computational complexity namely polynomiality could turn criterion machine intelligence beyond complexity could also argue explicitly learning logical expressions tackle knowledge graph completion that when solving given problem avoid solving general problem intermediate step ﬁrst evaluate models three main properties binary relations reﬂexivity symmetry transitivity combinations. experimentally testing ability learn patterns facts robustness missing data. tasks represent real reasoning family genealogies. data explore diﬀerent types training/testing splits diﬀerent types inference. remainder paper organized follows. ﬁrst review literature section presenting formally link-prediction task evaluated latent factor models optimization procedure section experiments learning properties relations presented along results section experiments description results family genealogies reported section finally propose research directions results section artiﬁcial intelligence becoming driven empirical successes quest principled formalisation reasoning making empirical science theoretical one. experimental design skill empirical scientists well-designed experiment expose model limitations enable improving them. indeed seeking falsiﬁcation best deﬁnition science machine learning extremely simple come experiment fail. however less easy think brings informative failure—when thinks failing experiment all. babi data proposing prerequisite tasks reasoning natural language example informative experiment speciﬁc reasoning type task targets. inspired idea work designed simple tasks relational learning assess basic properties relations well simple reasonings kinship relations. learning knowledge graphs generally relational data problem artiﬁcial intelligence many contributions made using inductive logic programming relational data last decades handling inference probabilistically gave birth statistical relational learning ﬁeld link prediction always main problems ﬁeld. diﬀerent probabilistic logic-based inference models proposed main contribution along line research probably markov logic networks mlns take input ﬁrst-order rules facts build markov random ﬁeld facts co-occuring possible groundings formulae learn weight rules represents likeliness applied inference time. proposals followed purely probabilistic approach link-prediction problem recently drawn attention wider community. driven standard data representation semantic resource description framework various knowledge graphs—also called knowledge bases— collaboratively automatically created recent years dbpedia freebase google knowledge vault since netﬂix challenge latent factor models taken advantage probabilistic symbolic approaches link-prediction task. terms prediction performances ﬁrst also scalability. rise predictive performances speed enabled many applications including automated personal assistants recommender systems statistical models learning knowledge graphs summarized recent review among latent factor models. discuss models detail following section. notable latent factor model tested paper holographic embeddings model shown equivalent complex model complex model detailed next section. also latent factor model proposed jenatton included combination uni- bitrigram terms evaluated separate models understand contribution modeling choice diﬀerent situations. latent models actually factorization models. among variety neural-network models including neural tensor networks multi-layer perceptron used dong survey models work focus latent factor models models expressed factorization knowledge graph represented tensor. cessing tasks entailment equivalence. natural logic theoretical framework natural language inference uses natural language strings logical symbols. bouchard compared squared logistic losses learning transitive sequential synthetic relations singh also investigated synthetic examples relational learning diﬀerent latent factor models. following diﬀerent goal approaches formalised encoding logical operations tensor operations. smolensky applied babi data reasoning tasks grefenstette general boolean operations. advances bringing worlds together include work rockt¨aschel demeester latent factor model used well logical rules. error-term rules added classical latent factor objective function. rockt¨aschel riedel fully diﬀerentiable neural theorem prover used handle facts rules whereas minervini adversarial training wang cohen learned ﬁrst-order logic embeddings formulae learned ilp. similar proposals integrating logical knowledge distributional representations words include work lewis steedman conversely yang learn latent factor model facts only extract rules learned embeddings. proposed projections subject object entity embeddings conserve transitivity symmetry. section formally deﬁnes link-prediction problem knowledge graphs well notations used throughout paper. introduce state-of-the-art models tested experimental sections. knowledge graph stores facts entities relations form facts also write triples relation subject object entities fact associated truth value yrso corresponding truth value yrso false facts attribute value denote possible triples given entity relation entities denoted number relations |r|. complex matrix cn×m written slight abuse notation entities relations write corresponding rows embedding matrices following present detail model scoring functions parameters compare experimental survey. chose compare popular best-performing link-prediction models. models’ scoring functions parameters summarized table controlled hyper-parameter z++. start introducing natural model general decomposition tensors canonical-polyadic decomposition also know candecomp parafac canonical-polyadic decomposition involves latent matrix dimension decomposed tensor case three latent matrices order tensor. dimension learned representations also often called rank decomposition. scoring function model general tensor decomposition though really tailored problem since tensor stack square matrices rows columns represent underlying objects entities. indeed completely decorrelated rescal rescal diﬀers decomposition points embedding entity instead embedding entities subject another entities objects; relation represented matrix embedding instead vector. scoring function rescal ﬁrst model propose unique embeddings entities—simultaneously bordes —which yielded signiﬁcant performance improvement since unique entity embeddings adopted subsequent models. however matrix representations relations makes scoring function time space complexity quadratic rank decomposition. also leads potential overﬁtting. pairwise nature gives model advantage non-compositional pairs entities. however memory complexity quadratic number entities practice unobserved pairs entities stored memory useless. though also weak point model cannot predict scores unobserved pairs entities since learns pairwise representations. transe transe model imposes geometrical structural bias model subject entity vector close object entity vector translated relation vector. given q-norm embedding space matrix relations. deriving norm scoring function exposes another perspective model unravels factorial nature gives bilinear terms explored garc´ıa-dur´an latent dimensions’ information relation embeddings—modeling symmetric relations similar cousin related implies strong geometrical constraint entity embeddings diﬀerence must orthogonal relation embedding model thus make trade-oﬀ correctly modelling symmetry relation zeroing relation embedding altering much entity embeddings meet orthogonality requirement distmult distmult model seen simpliﬁcation rescal model unique representation entities kept representation relations brought back vectors instead matrices complex complex model seen complexvalued version distmult model. latent matrices entities relations complex domain instead real domain. scoring function real part trilinear product entities relation embeddings model solves symmetry problem distmult slightly diﬀerent representations entities subject object complex conjugate. representations still tightly linked yields good generalization properties unlike slight diﬀerence allows model retain vectorial representation relations thus linear time space complexity unlike rescal without loss expressiveness—complex able decompose exactly possible knowledge graphs previously mentioned used logistic function provides better results compared squared error minimized negative log-likelihood regularization applied entity-wise relation-wise vector embeddings considered model loss optimized stochastic gradient descent mini-batches adagrad initial learning rate early stopping average precision decreased validation calculated every epochs. regularization parameter models evaluated using average precision classically used ﬁeld also computed average precision deterministic logic inference engine applying corresponding rules used generate data set. fact test probability fact logically deduced true facts training validation sets deduced false otherwise. transe model trained norms results reported best performing experiment. original paper regularization parameters instead enforced entity embeddings unit results evaluated average precision factorization rank models best validated kept. average precisions macro-averaged runs error bars show standard deviation runs. assess whether latent factor models able generalize data without ﬁrst-order logic rules generate synthetic data rules evaluate model classical training validation test splitting data. proportion positives negatives respected across sets. ﬁrst consider rules corresponding relation properties rules corresponding inter-relations reasonings genealogical data. also explore robustness missing data well diﬀerent training/testing splits data. keeping data deterministic simple also allows write corresponding logical rules experiment simulate test metrics perfect induction would yield upper-bound performance method. data sets made available. relations knowledge graphs diﬀerent names diﬀerent areas mathematics. logicians call binary predicates boolean-valued functions variables. theorists binary endorelations operate elements single diﬀerent combinations properties deﬁne basic building blocks theory equivalence relations reﬂexive symmetric transitive relations partial orders reﬂexive antisymmetric transitive relations examples given table many common examples combinations knowledge graphs many hierarchical similarity relations. example relations older father strict hierarchies thus antisymmetric irreﬂexive. transitive whereas makes diﬀerence inference time. similarly symmetric relations has-the-same-parents-as friend sibling’s parents also makes ﬁrst relation transitive whereas friend’s friends necessarily yours. note makes has-the-same-parents-as relation reﬂexive—it thus equivalence relation. relational learning models must able handle relations exhibit possible combinations properties since common imply diﬀerent types reasoning already acknowledged bordes given relation reﬂexive irreﬂexive neither; symmetric antisymmetric neither; transitive possible combinations. however address cases little interest none properties true reﬂexivity irreﬂexivity true irreﬂexive symmetric transitive case consistent possibility facts false irreﬂexive transitive case must either false antisymmetric—and thus corresponds already existing case—to consistent. indeed observes true facts application transitivity rule must true explains inconsistency cases irreﬂexive. leaves cases interest. evaluate ability ﬁrst learn relation individually single relation knowledge graph jointly. joint case note since relation generated independently signal shared across relations would help predicting facts relation facts another relation thus ability learn relation patterns tested. proportion observed facts generally small real knowledge graphs. assess models robustness missing data also reduce proportion training learning diﬀerent relations jointly. results averaged -fold cross-validation training validation test individual learning case. joint learning case training percentage varies validation size kept constant test contains remaining samples—between first results identical models whether relations reﬂexive irreﬂexive neither tells reﬂexivity irreﬂexivity important practice remove quality prediction latent factor models. report results diﬀerent combinations symmetry/antisymmetry transitivity main text. results combinations including reﬂexivity irreﬂexivity reported appendix figure shows average precision model generated symmetric antisymmetric relations. surprisingly simple relations observed data complex rescal manage learn symmetric antisymmetric patterns non-negligible advantage complex model. moreover higher ranks rescal starts overﬁtting average precision decreases presumably quadratic number parameters respect rank since model probably fails uncorrelated representations entities subject objects makes unable model symmetry antisymmetry. distmult unsurprisingly fails non-symmetric cases symmetric nature scoring function thus succeeds symmetric case. unexpectedly transe model hard time antisymmetry performs well symmetric relation zeroing relation embedding explained section model cannot actually generalize single relation case single embedding entity pair. figure shows results symmetric transitive antisymmetric transitive relations figure transitive relations. almost models except model distmult non-symmetric cases perfectly generalize low-rank. tells transitivity actually brings much structure data makes problem easy latent factor models data observed. state-of-the-art latent factor models surprisingly unable model basic properties binary relations. though time relations learnt together also much less observed facts. next assess models ability learn relations together robustness sparse observations gradually decreasing size training set. figure shows results relations jointly learned diﬀerent proportions training expected scores drop the—deterministic logic—upper-bound latent factor models widen decrease training data. complex proves robust missing data match logical inference training data. rescal overﬁts rank increasing best performing model training rank suggests richer relation representations entity representations parameters proﬁtable learning relation properties little data. however reason variance transe models seem sensitive missing data curves progressively away rescal’s percentage observed data decreasing. distmult symmetric model models four settings relations symmetric. since relation generated independently observed entity pair relations help model thus fails too. latent factor models cannot match logical inference suggesting number examples suﬃcient learn properties. finally last setting training best models still points best achievable average precision showing need large amount training data correctly learn basic properties binary relations. results taken cautiously experiment state general least facts observed order learn properties correctly. indeed relations completely uncorrelated real knowledge graphs generally correlated thus share information. also often machine learning ratio number parameters number data points informative figure joint learning relations entities symmetric antisymmetric transitive symmetric transitive antisymmetric transitive. average precision factorization rank ranging model. top-left train top-right train bottom-left train bottom-right train set. generated family trees corresponding kinship relations facts designed three diﬀerent splits data. three splits assess diﬀerent inductive properties latent models giving less supporting facts training set. predicting family relationships task popularised hinton’s kinship data generated synthetic families testing link-prediction models also recently proposed public dataset generated families intertwined want family disjoint ones true fact entities diﬀerent families below. propose generate family relations synthetic family trees namely mother father husband wife daughter brother sister uncle aunt nephew niece cousin grandfather grandson grandmother granddaughter. sample families independently span three generations unique couple three children couple random husbands wives parents added middle generation. figure shows example family tree. whole data totals entities— persons family—and relations mentioned above. family thus consists true false facts. within traditional families feature married heterosexual couples divorce children ﬁgure relations mother father daughter suﬃcient deduce remaining ones. examples rules allow deducing relations main ones shown table fact devise three diﬀerent splits data. fact sets contain main relations mother father daughter denoted ωmain facts involving relations ωother. thus family other. figure draws corresponding tensor subset observed facts. finally sampling function uniformly random figure tensor representation observed subsets family experiments. part dark orange represents sets containing four relations mother father daughter part light orange represents relations. figure tensor representation three diﬀerent splits. green sets always contained training ωtrain whereas blue sets split among training validation test sets. sets mainly serve control experiment splits. second split evaluating whether latent factor models able leverage information. ensure relations mother father daughter families training split remaining ones training validation thirdly assess ability latent factor models transfer knowledge learnt family another disjoint entities. split training always contains relations four families plus mother father daughter ﬁfth family relations ﬁfth family split other). split explore diﬀerent values also main entirely observed family plus main relations ﬁfth one. observe makes sense last split. latent factor models expected inductive abilities would able understand genealogical reasoning four ﬁrst families learned information correctly predict relations ﬁfth family four main ones. note last splits deterministic logic inference system makes perfect predictions—given rules ones shown table —for value number facts training validation test sets split summarized table similar splits data already proposed evaluate rule-based inference models able transfer reasoning disjoint sets entities. interestingly data sets ﬁrst random split evaluate quantity training data needed learn reason genealogies. figure shows average precision model ranks ranging value complex rescal able generalize almost perfectly observed data ﬁrst tells models indeed capable learn genealogical reasonings. many relations antisymmetric surprise distmult transe cannot reach perfect predictions already failed antisymmetric synthetic relation. complex model generalizes quickly small ranks outrun rescal— small ranks—and transe percentage observed data decreases conjecture transe’s robustness bilinear terms especially involves subject object embeddings—e eo—as shown section give high scores pairs entities belonging family. rescal richer representations relations matrices probably help here long rank clearly causes overﬁtting. decomposition scores drop quickly proportion observed data unrelated subject object representations. model fails again simple reason relations exclusive given pair entities indeed father true example none relations true—at least families. hence model predict score fact true triple involving support decision. also troubles next splits reason. note split split recall mother father daughter relations always train families. value ranging corresponds proportion relations also training set. test validation sets composed relations. compared random split setting figure performances models decrease slowly percentage observed data. shows latent factor models able information provided four relations others deduced. rescal clearly best model values long big. exhibits behavior seems equilibria distributed around pivotal average precision suddenly drops high variance predictions around complex also seems show lighter overﬁtting high values however given rules deduce relations four main ones recall logical inference engine able reach average precision one. though improvement compared random split setup large logical inference still wide showing latent factor models troubles making link four main relations ones limited training data available. could imbalance number relation training split introduces biasing entity embeddings towards better reconstruction main relations detriment generalization remaining ones. weighting facts accordance preponderance relation dataset could improve performances here. figure average precision factorization rank ranging model families experiment family split. top-left top-right middle-left middle-right bottom mean models able exploit additional information? conjecture better results ranging partly relation imbalance problem—explained previous split—being much smaller here relations four families given training set. ensure models indeed generalized four perfectly informed families reduced proportion relations ﬁfth family training zero—which thus constitute whole validation test sets. though models provided four perfectly informed families needed facts predict missing ones ﬁfth family fail last setting shown bottom plot figure rescal transe resist better models last setting families never involved together observed fact ysro) ysro) relation thus learning embeddings link share embedding relation involved scoring functions interpretation also supported rescal scores beneﬁts higher number parameters relation representations rk×k increases amount information shared formally bring problem light re-frame tensor approximation problem system inequalities. true triples score false triples sake example consider factorizing relations sister grandfather families persons each using distmult model. observing true fact sister true fact sister allows deduce grandfather observed blocks—and block wish recover—are symmetric here expressiveness issue using distmult. decomposing tensor distmult model true facts probability false facts probability amounts solving following system inequalities however easy check arbitrary solutions system necessarily satisfy system hence necessarily predict grandfather facts correctly. also would true even added families like relations fully observed would constraints explains models fail family split nothing encourages less constrained entities embeddings resemble ones similar constrained entities; adding examples constrained entities help. overall complex model proved stable generalization abilities across synthetic experiments. models showed good ability learn basic relation properties except antisymmetry complex succeeded. said decreasing size training joint learning relation properties best models points average precision behind best possible score. improving models towards learning basic binary relation properties less data thus seems promising direction. models showed advantages speciﬁc settings. rescal transe showed good robustness data missing family experiments thanks bilinear terms transe rich matrix relation representations rescal. model experiments pairwise terms known give advantage non-compositional pairs entities diﬀerent possible combinations seem promising. behaviour rescal complex symmetric antisymmetric experiments suggests encoding patterns complex conjugation stable using non-commutative matrix product. rescal’s matrix representations relations helped family experiments long rank high suggesting might middle ground found parametric representation relations. using tridiagonal pentadiagonal symmetric matrices relation representations within complex model could answer problems. combining scoring functions transe models complex could also lead robust model. combination bilinear trilinear terms already explored within real-valued models also vectorial weights term well combining diﬀerent pairwise terms yielded better performance cases. main defect latent factor models experimental survey points ability transfer knowledge disjoint entities shown last family split real knowledge graphs might fully disjoint subsets rather less-connected sub-graphs eﬀect likely appear too. believe improving ability latent factor models key. already-pursued harness problem enable latent factor models make logic rules already said rules always available thus latent factor models improved order ability learn disjoint subsets still operating without rules. intuitively sharing parameters across entity representations could also solve issue used bayesian clustered factorization models though models known scalability issues. possible scalable implement factorization matrix expressed low-rank factorization already proposed relation embeddings another could suited regularization whole matrix proposals regularized setting facts involving entity necessary infer facts known rules. recent works started tackling problem verga proposed solution model expressing entity pair embeddings combinations relation embeddings appear. hamaguchi used graph neural networks handle unseen entities test time. evidence split family experiments also pointed potential problem imbalance distribution relations across facts train test sets distributed diﬀerently. correcting imbalance down-weighting facts involving frequent relations could solution well sharing parametrization relations. non-mentioned aspect problem paper theoretical learnability logic formulas ﬁeld extensively covered however logic learnability latent factor models speciﬁcally studied. recently established links sign-matrices complexity—speciﬁcally sign-rank —and vc-dimension open door theoretical study possible extensions tensor case. said theoretical guarantees generally come condition training test sets drawn distribution case last splits family experiments theoretical analysis learnability cases might require theoretical framework statistical learning. experimentally surveyed state-of-the-art latent factor models link prediction knowledge graphs order assess ability learn binary relation properties genealogical relations directly observed facts well robustness missing data. latent factor models yield good performances ﬁrst case diﬃculties second one. speciﬁcally show models reason generally meant logical inference engines unable transfer predictive abilities disjoint subsets entities. diﬀerent behaviors models experimental setup suggest possible enhancements research directions including combining them well exposes model’s advantages limitations.", "year": 2017}