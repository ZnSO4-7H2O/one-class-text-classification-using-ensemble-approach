{"title": "Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative  Refinement", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "We propose a conditional non-autoregressive neural sequence model based on iterative refinement. The proposed model is designed based on the principles of latent variable models and denoising autoencoders, and is generally applicable to any sequence generation task. We extensively evaluate the proposed model on machine translation (En-De and En-Ro) and image caption generation, and observe that it significantly speeds up decoding while maintaining the generation quality comparable to the autoregressive counterpart.", "text": "propose conditional non-autoregressive neural sequence model based iterative reﬁnement. proposed model designed based principles latent variable models denoising autoencoders generally applicable sequence generation task. extensively evaluate proposed model machine translation image caption generation observe signiﬁcantly speeds decoding maintaining generation quality comparable autoregressive counterpart. conditional neural sequence modeling become facto standard variety tasks much recent success built successful autoregressive sequence modeling probability target sequence factorized product conditional probabilities next symbols given preceding ones. despite success neural autoregressive modeling often non-markovian nonlinear weakness decoding i.e. ﬁnding likely sequence. intractability must resort suboptimal approximate decoding sequential nature decoding cannot easily parallelized results large latency motivated recent investigation non-autoregressive neural sequence modeling context machine translation oord context speech synthesis. paper propose non-autoregressive neural sequence model based iterative reﬁnement generally applicable sequence generation task beyond machine translation. proposed model viewed latent variable model conditional denoising autoencoder. thus propose learning algorithm hybrid lower-bound maximization reconstruction error minimization. design iterative inference strategy adaptive number steps minimize generation latency without sacriﬁcing generation quality. extensively evaluate proposed conditional nonautoregressive sequence model compare autoregressive counterpart using state-of-the-art transformer machine translation image caption generation. case machine translation proposed deterministic non-autoregressive models able decode approximately faster beam search autoregressive counterparts maintaining translation quality. image caption generation observe approximately faster decoding respectively maintaining caption quality. sequence modeling deep learning largely focused autoregressive modeling sequence. given sequence form neural network parametrize conditional distribution variable given preceding variables i.e. instance recurrent neural network. approach become facto standard language modeling augmented extra conditioning variable becomes conditional sequence modeling serves basis many recent advances instance machine translation speech recognition made. hand issue canceled decoding arbitrarily large autoregressive model approximate decoding algorithm exist non-autoregressive model indeed shown case recently oord fig. graphical illustration view. propose non-autoregressive neural sequence model. similarly recent works introduce latent variables implicitly capture dependencies among target variables. however remove stochastic behavior interpreting latent variable model introduced immediately below process iterative reﬁnement. goal capture dependencies among target symbols given source sentence without autoregression. address introducing l-many intermediate random variables marginalizing product term inside summation modelled shared deep neural network takes input source sentence outputs conditional distribution target vocabulary figure non-autoregressive sequence modeling eliminates decoding expense potentially larger modeling thereby compensates potentially-larger modeling gap. cessing. weakness shows especially decode likely sequence trained model i.e. maxy known polynomial algorithm solving exactly practitioners relied approximate decoding algorithms greedy decoding beam search noisy parallel approximate decoding continuous relaxation among these beam search become method choice superior performance greedy decoding however comes substantial computational overhead solution issue slow decoding recent works attempted non-autoregressive sequence modeling. modiﬁed recently proposed transformer non-autoregressive machine translation oord convolutional network non-autoregressive modeling waveform. non-autoregressive modeling aims factorizing distribution target sequence given source product conditionally independent per-step distributions breaking dependency among target variables across time. break dependency allows trivially likely target sequence effectively bypass computational overhead sub-optimality decoding autoregressive sequence model. modeling decoding desirable property non-autoregressive neural sequence modeling however comes also potential performance degradation fact potential dependency among target variables must captured neural network models factorized conditionals potential modeling underlying true model neural sequence model could larger non-autogressive model autoregressive one. formulation intermediate random variables could anonymous. however constrain type output order share underlying parametrized neural network. constraint allows view conditional single-step reﬁning rough target sequence entire chain conditionals l-step iterative reﬁnement. furthermore sharing parameters across reﬁnement steps enables dynamically adapt number iterations input important substantially reduces amount time required decoding later experiments. corruption process little consensus best corruption process sequence especially discrete tokens. work corruption process proposed hill recently become widely adopted reference target corrupted probability decided corrupt either replace y∗t+ token replace token uniformly selected vocabulary unique tokens random swap y∗t+. done sequentially although possible train proposed nonautoregressive sequence model using either cost functions propose stochastically cost functions. randomly replacing term jdae words distillation context machine translation oord context speech generation recently discovered important knowledge distillation successfully train non-autoregressive sequence model. following also knowledge distillation replacing reference target training example target generated well-trained autoregressive counterpart. replacement cost function model architecture remain unchanged. length prediction minor difference autoregressive non-autoregressive models former naturally models length target sequence without arbitrary upper-bound latter not. hence necessary separately model length target sequence. training simply length reference target sequence. proposed approach could instead viewed learning conditional denoising autoencoder known capture gradient log-density implicitly learn direction output space maximizes underlying true data-generating distribution output space discrete much theoretical analysis alain bengio strictly applicable. however view attractive serves alternative foundation designing learning algorithm. start corruption process introduces noise correct output given source reference translation sample corrupted target acts input conditional then goal learning maximize log-probability original reference given corrupted version. maximize evaluated non-autoregressive autoregressive approaches found non-autoregressive approach signiﬁcantly lags behind autoregressive variants. approach similar shared stack neural applied multiple times given input sequence. however differs approach iteration output reﬁned version previous iteration. recent paper relevant proposed work. order capture dependencies among target variables introduced sequence discrete latent variables. instead marginalizing supervised learning train inference network latent variables using off-the-shelf word alignment tool unlike approach require extra supervision. achieve best result stochastically sample latent variables rerank corresponding target sequences external autoregressive model. stark contrast proposed approach fully deterministic rely extra reranking mechanism. furthermore approach require additional ﬁnetuning reverse kl-divergence. parallel wavenet simultaneously oord presented successful nonautoregressive sequence model speech waveform. inverse autoregressive sequence independent random variables target sequence. order maximize performance found helpful apply multiple times similarly iterative reﬁnement strategy. approach however restricted continuous target variables proposed approach principle could applied discrete continuous variables. deliberation network deliberation network proposed recently incorporates idea reﬁnement neural machine translation. deliberation network consists autoregressive decoders. second decoder takes account translation generated ﬁrst decoder. approach signiﬁcantly expands allowing many steps reﬁnement. three transformer-based network blocks implement model right side ﬁrst block encodes input second block models ﬁrst conditional ﬁnal block shared across iterative reﬁnement steps modeling blocks conditionals except initial modeled single shared neural network reﬁnement performed many iterations necessary potentially number iterations used training predeﬁned stopping criterion met. criterion instance based either amount change target sequence iteration amount change conditional log-probabilities −log computational budget. experiments later observe ﬁrst criterion reasonable choice machine translation image caption generation. related work non-autoregressive neural machine translation schwenk proposed continuous-space translation model estimate conditional distribution target phrase given source phrase dropping conditional dependencies among target tokens cstm found improve output machine translation system reranking n-best list. evaluation however limited reranking short phrase pairs only. table generation quality speed tokens/sec↑ images/sec↑ translation image captioning respectively. generation speed measured assuming sentence-by-sentence generation reported based cpu. translation tasks speed measured en→{dero}. stands autoregressive models. beam width. idec number reﬁnement steps taken decoding. adaptive refers adaptive number reﬁnement steps. terms generation quality measured terms bleu generation efﬁciency measured terms tokens images second translation image captioning respectively. machine translation choose three translation datasets different sizes iwslt’ en↔de wmt’ en↔ro wmt’ en↔de whose training sets consist sentence pairs respectively. tokenize corpora using script moses segment word subword units using byte pair encoding shared tokens iwslt’ en-de wmt’ en-ro wmt’ en-de respectively. wmt’ en-de newstest- newstest- development test sets. wmt’ en-ro newsdev- newstest- development test sets. iwslt’ en-de test validation. closely follow setting case iwslt’ en-de small model wmt’ en-de wmt’ en-ro base transformer addition warm-up learning rate scheduling also experimented annealing learning rate linearly label smoothing average multiple checkpointed models. decisions made based preliminary experiments. train model either single nvidia single nvidia minibatch consisting approximately tokens. decoder takes input padded/shortened according length corresponding reference target sequence. reﬁnement step decoder takes input predicted target sequence sequence ﬁnal activation vectors previous step. position embedding vector target token ˆyl− activation vector simply summed. preliminary experiments experimented excluding activation vector previous step underperformed current setup. evaluate proposed approach distinct sequence modeling tasks machine translation image caption generation. compare proposed nonautoregressive model autoregressive counterpart make important observations results table first generation quality improves across tasks reﬁnement steps idec even beyond number iterations used training supports interpretation proposed approach conditional denoising autoencoder sec. verify this decoding wmt’ iterations. shown fig. quality improves certain number iterations thereon stagnates eventually drops. similar behavior observed earlier another generative model using iterative reﬁnement leave future work investigate. second generation efﬁciency decreases reﬁnements made. together ﬁrst observation suggests proposed approach allows make smooth trade-off quality speed. further observe adaptive iteration scheme works well achieving near-best generation quality signiﬁcantly lower computational overhead. also observe speedup decoding proposed approach much clearer cpu. consequence highly parallel computation proposed non-autoregressive model better suited gpus showcasing potential using nonautoregressive model specialized hardware parallel computation google’s tpus used publicly available splits used previous image caption generation work consisting training images validation images test images. image pre-transformed -dimensional feature vectors using resnet- pretrained imagenet average vectors copied many times match length target sentence form initial input decoder base transformer except nlayer instead train model single nvidia minibatch consisting approximately tokens. target length prediction formulate target length prediction classiﬁcation problem predicting difference target source lengths translation target length image captioning. hidden vectors nlayer layers encoder summed softmax classiﬁer afﬁne transformation. length predictor trained separately main non-autoregressive model trained used decoding. figure decoding latencies terms sec/sentence using different decoding algorithms iwslt’ en→de. decoding non-autoregressive model largely constant respect sentence length latency decoding autoregressive model increases linearly. note y-axis logarithmic scale. training inference adam optimizer meaning four steps iterative reﬁnement pdae based validation performance. main non-autogressive sequence model target length predictor trained decode ﬁrst predicting target length running iterative reﬁnement steps outputs consecutive iterations assess effectiveness adaptive scheme also test ﬁxed number steps seitdem habe sieben h¨auser nachbarschaft lichtern versorgt funktionierenen wirklich seven homes since neighborhood lights really functional seven homes neighborhood lights really functional seven homes neighborhood lights really functional providing seven homes neighborhood lights really functional providing seven homes neighborhood lights good functional since seven homes around community really working sehr gl¨ucklich damals ziemlich ungew¨ohnlich nachrichten meistens deprimierten looked happy pretty unusual news usually depressing looked happy pretty unusual news depressing looked happy pretty unusual news mostly depressing looked happy pretty unusual time news mostly depressing looked happy pretty unusual time news mostly depressing smile face unusual news mostly depressed furchtlos sein heißt f¨ur mich heute ehrlich sein honest today fearless honest today fearless honest today fearless honest today fearless honest today today fearless means honest table three sample de→en translations proposed nonautoregressive sequence model. source sentences development iwslt’. ﬁrst iteration corresponds decoder thereon decoder repeatedly applied reﬁne translation. subsequences changes across reﬁnement steps underlined. dation evident wmt’ en-de. wmt’ en-de clearly distinguished datasets aspects. first average target length training wmt’ en-de longer compared iwslt’ en-de wmt’ en-ro coco require unreasonably many reﬁnement steps transformer blocks decoder order capture long-term dependencies target sequence. second wmt’ much training examples compared datasets. clear aspects negatively affect non-autoregressive model leave analysis future investigation. lastly encouraging observe proposed nonautoregressive model works well image caption generation. result conﬁrms generality approach beyond machine translation unlike explicitly designed tested machine translation oord speech synthesis. decoding latency better understand observed decoding speedup plot average seconds sentence fig. measured sequentially decoding sentence time. expected decoding autoregressive model linearly slows length sentence grows decoding proposed nonautoregressive model ﬁxed number iterations constant complexity. adaptive scheme described sec. automatically increases number reﬁnement steps length source sentence increases suggesting scheme captures amount information input well. increase latency however less severe compared decoding autoregressive model. ablative experiments iwslt’ en-de investigate impact different components proposed non-autoregressive sequence model. results presented table training first observe beneﬁcial multiple iterations reﬁnement training. using four iterations bleu score improved approximately points directions. also notice necessary proposed hybrid learning strategy maximize improvement iterations training lastly knowledge distillation found crucial close proposed deterministic non-autoregressive sequence model autoregressive counterpart echoing observations oord inference noticed many instances repetition model output preliminary experiments decided remove repeating consecutive symbols simple post—processing routine. table removing repeating consecutive symbols improves quality suggests proposed iterative reﬁnement enough remove repetitions own. investigation development necessary properly tackle issue leave future work. woman standing playing tennis tennis racquet woman standing tennis court tennis racquet woman standing tennis court racquet woman standing tennis court holding racquet reference captions tour parked curb waiting city parked side hotel rain parked awning next brick sidewalk parked curb front building double decked sits parked awning reference captions female tennis player black playing tennis woman standing tennis court holding racquet female tennis player preparing serve ball woman holding tennis racket court woman getting ready reach tennis ball ground table sample image captions proposed non-autoregressive sequence model. images development coco. ﬁrst iteration decoder subsequent ones decoder subsequences changes across reﬁnement steps underlined. qualitative analysis machine translation table present three sample translations iterative reﬁnement steps development iwslt’ expected sequence generated ﬁrst iteration mostly rough iteratively reﬁned multiple steps. inspecting underlined sequences iteration monotonically improve translation overall modiﬁes translation towards reference sentence. missing words added unnecessary words dropped. instance second example. second iteration removes unnecessary were fourth iteration inserts word mostly. phrase time gradually added word time. image caption generation table shows examples image caption generation proposed nonautoregressive sequence model. case observe iteration captures details input image. ﬁrst example described yellow ﬁrst iteration subsequent iterations reﬁne yellow black bus. similarly road reﬁned noticing details parking lanes. notice behavior second example well. ﬁrst iteration specify place woman standing ﬁxed immediately second iteration standing tennis court. ﬁnal fourth iteration proposed model captures fact woman holding racquet. autoregressive neural sequence model based idea iterative reﬁnement. designed learning algorithm specialized proposed approach interpreting entire model latent variable model reﬁnement step denoising autoencoder. implemented approach using recently proposed transformer state-of-the-art sequence-to-sequence model evaluated tasks machine translation image caption generation. tasks able show proposed non-autoregressive model performs closely autoregressive counterpart signiﬁcant speedup decoding. qualitative analysis revealed proposed iterative reﬁnement indeed reﬁnes target sequence gradually multiple steps. despite promising results observed proposed non-autoregressive neural sequence model outperformed autoregressive counterpart terms quality generated sequences. believe following directions pursued future narrow gap. first deterministic lower-bound replaced tighter bound. second investigate corruption processes understand better impact choice generation quality. lastly work sequence-to-sequence model architectures could yield better results non-autoregressive sequence modeling. thank support adeptmind ebay tencent nvidia. work partly supported samsung advanced institute technology samsung electronics also thank jiatao valuable feedback. chiu chung-cheng sainath tara yonghui prabhavalkar rohit nguyen patrick chen zhifeng kannan anjuli weiss kanishka gonina katya state-of-the-art speech recognition sequenceto-sequence models. arxiv preprint arxiv. kyunghyun courville aaron bengio yoshua. describing multimedia content using attention-based encoder-decoder networks. ieee transactions multimedia chorowski bahdanau dzmitry serdyuk dmitriy kyunghyun bengio yoshua. attention-based models speech recognition. advances neural information processing systems deng dong socher richard li-jia imagenet large-scale hierarchical fei-fei image database. computer vision pattern recognition cvpr ieee conference ieee kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition jouppi norman young cliff patil nishant patterson david agrawal gaurav bajwa raminder bates sarah bhatia suresh boden borchers indatacenter performance analysis tensor processing unit. proceedings annual international symposium computer architecture kingma diederik salimans jozefowicz rafal chen sutskever ilya welling max. improved variational inference inverse autoregressive ﬂow. advances neural information processing systems koehn philipp hoang hieu birch alexandra callisonburch chris federico marcello bertoldi nicola cowan brooke shen wade moran christine zens richard moses open source toolkit statisproceedings tical machine translation. annual meeting interactive poster demonstration sessions association computational linguistics raiko tapani kyunghyun bengio yoshua. iterative neural autoregressive distribution esadvances neural information timator nade-k. processing systems mikolov tom´aˇs karaﬁ´at martin burget luk´aˇs ˇcernock`y khudanpur sanjeev. recurrent neural network based language model. eleventh annual conference international speech communication association oord aaron dieleman sander heiga simonyan karen vinyals oriol graves alex kalchbrenner senior andrew kavukcuoglu koray. wavenet generative model audio. arxiv preprint arxiv. oord aaron yazhe babuschkin igor simonyan karen vinyals oriol kavukcuoglu koray driessche george lockhart edward cobo parallel wavenet luis stimberg florian arxiv preprint fast high-ﬁdelity speech synthesis. arxiv. sennrich rico haddow barry birch alexandra. neural machine translation rare words subword units. proceedings annual meeting association computational linguistics vaswani ashish shazeer noam parmar niki uszkoreit jakob jones llion gomez aidan kaiser łukasz polosukhin illia. attention need. advances neural information processing systems papineni kishore roukos salim ward todd wei-jing. bleu method automatic evaluation machine translation. proceedings annual meeting association computational linguistics association computational linguistics yingce tian lijun jianxin nenghai tie-yan. deliberation networks sequence generation beyond one-pass decoding. advances neural information processing systems", "year": 2018}