{"title": "Distributed Representations of Words and Phrases and their  Compositionality", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.", "text": "recently introduced continuous skip-gram model efﬁcient method learning high-quality distributed vector representations capture large number precise syntactic semantic word relationships. paper present several extensions improve quality vectors training speed. subsampling frequent words obtain signiﬁcant speedup also learn regular word representations. also describe simple alternative hierarchical softmax called negative sampling. inherent limitation word representations indifference word order inability represent idiomatic phrases. example meanings canada cannot easily combined obtain canada. motivated example present simple method ﬁnding phrases text show learning good vector representations millions phrases possible. distributed representations words vector space help learning algorithms achieve better performance natural language processing tasks grouping similar words. earliest word representations dates back rumelhart hinton williams idea since applied statistical language modeling considerable success follow work includes applications automatic speech recognition machine translation wide range tasks recently mikolov introduced skip-gram model efﬁcient method learning highquality vector representations words large amounts unstructured text data. unlike previously used neural network architectures learning word vectors training skipgram model involve dense matrix multiplications. makes training extremely efﬁcient optimized single-machine implementation train billion words day. word representations computed using neural networks interesting learned vectors explicitly encode many linguistic regularities patterns. somewhat surprisingly many patterns represented linear translations. example result vector calculation closer word vector paper present several extensions original skip-gram model. show subsampling frequent words training results signiﬁcant speedup improves accuracy representations less frequent words. addition present simpliﬁed variant noise contrastive estimation training skip-gram model results faster training better vector representations frequent words compared complex hierarchical softmax used prior work word representations limited inability represent idiomatic phrases compositions individual words. example boston globe newspaper natural combination meanings boston globe. therefore using vectors represent whole phrases makes skip-gram model considerably expressive. techniques represent meaning sentences composing word vectors recursive autoencoders would also beneﬁt using phrase vectors instead word vectors. extension word based phrase based models relatively simple. first identify large number phrases using data-driven approach treat phrases individual tokens training. evaluate quality phrase vectors developed test analogical reasoning tasks contains words phrases. typical analogy pair test montrealmontreal canadienstorontotoronto maple leafs. considered answered correctly nearest representation vec. finally describe another interesting property skip-gram model. found simple vector addition often produce meaningful results. example close close vec. compositionality suggests non-obvious degree language understanding obtained using basic mathematical operations word vector representations. training objective skip-gram model word representations useful predicting surrounding words sentence document. formally given sequence training words objective skip-gram model maximize average probability computationally efﬁcient approximation full softmax hierarchical softmax. context neural network language models ﬁrst introduced morin bengio main advantage instead evaluating output nodes neural network obtain probability distribution needed evaluate nodes. hierarchical softmax uses binary tree representation output layer words leaves node explicitly represents relative probabilities child nodes. deﬁne random walk assigns probabilities words. precisely word reached appropriate path root tree. j-th node path root length path root addition inner node arbitrary ﬁxed child true otherwise. hierarchical softmax deﬁnes follows implies cost computing proportional average greater also unlike standard softmax formulation skip-gram word hierarchical softmax formulation assigns representations representation word representation every inner node binary tree. structure tree used hierarchical softmax considerable effect performance. mnih hinton explored number methods constructing tree structure effect training time resulting model accuracy work binary huffman tree assigns short codes frequent words results fast training. observed grouping words together frequency works well simple speedup technique neural network based language models alternative hierarchical softmax noise contrastive estimation introduced gutmann hyvarinen applied language modeling mnih posits good model able differentiate data noise means logistic regression. similar hinge loss used collobert weston trained models ranking data noise. shown approximately maximize probability softmax skipgram model concerned learning high-quality vector representations free simplify long vector representations retain quality. deﬁne negative sampling objective figure two-dimensional projection -dimensional skip-gram vectors countries capital cities. ﬁgure illustrates ability model automatically organize concepts learn implicitly relationships them training provide supervised information capital city means. used replace every term skip-gram objective. thus task distinguish target word draws noise distribution using logistic regression negative samples data sample. experiments indicate values range useful small training datasets large datasets small main difference negative sampling needs samples numerical probabilities noise distribution negative sampling uses samples. approximately maximizes probability softmax property important application. noise distribution free parameter. investigated number choices found unigram distribution raised power //z) outperformed signiﬁcantly unigram uniform distributions every task tried including language modeling large corpora frequent words easily occur hundreds millions times words usually provide less information value rare words. example skip-gram model beneﬁts observing co-occurrences france paris beneﬁts much less observing frequent co-occurrences france nearly every word co-occurs frequently within sentence the. idea also applied opposite direction; vector representations frequent words change signiﬁcantly training several million examples. table accuracy various skip-gram -dimensional models analogical reasoning task deﬁned neg-k stands negative sampling negative samples positive sample; stands noise contrastive estimation hs-huffman stands hierarchical softmax frequency-based huffman codes. frequency word chosen threshold typically around chose subsampling formula aggressively subsamples words whose frequency greater preserving ranking frequencies. although subsampling formula chosen heuristically found work well practice. accelerates learning even signiﬁcantly improves accuracy learned vectors rare words shown following sections. section evaluate hierarchical softmax noise contrastive estimation negative sampling subsampling training words. used analogical reasoning task introduced mikolov task consists analogies germany berlin france solved ﬁnding vector closest according cosine distance speciﬁc example considered answered correctly paris. task broad categories syntactic analogies semantic analogies country capital city relationship. training skip-gram models used large dataset consisting various news articles discarded vocabulary words occurred less times training data resulted vocabulary size performance various skip-gram models word analogy test reported table table shows negative sampling outperforms hierarchical softmax analogical reasoning task even slightly better performance noise contrastive estimation. subsampling frequent words improves training speed several times makes word representations signiﬁcantly accurate. argued linearity skip-gram model makes vectors suitable linear analogical reasoning results mikolov also show vectors learned standard sigmoidal recurrent neural networks improve task signiﬁcantly amount training data increases suggesting non-linear models also preference linear structure word representations. discussed earlier many phrases meaning simple composition meanings individual words. learn vector representation phrases ﬁrst words appear frequently together infrequently contexts. example york times toronto maple leafs replaced unique tokens training data bigram this remain unchanged. form many reasonable phrases without greatly increasing size vocabulary; theory train skip-gram model using n-grams would memory intensive. many techniques previously developed identify phrases text; however scope work compare them. decided simple data-driven approach phrases formed based unigram bigram counts using used discounting coefﬁcient prevents many phrases consisting infrequent words formed. bigrams score chosen threshold used phrases. typically passes training data decreasing threshold value allowing longer phrases consists several words formed. evaluate quality phrase representations using analogical reasoning task involves phrases. table shows examples categories analogies used task. dataset publicly available web. starting news data previous experiments ﬁrst constructed phrase based training corpus trained several skip-gram models using different hyperparameters. before used vector dimensionality context size setting already achieves good performance phrase dataset allowed quickly compare negative sampling hierarchical softmax without subsampling frequent tokens. results summarized table results show negative sampling achieves respectable accuracy even using achieves considerably better performance. surprisingly found hierarchical softmax achieve lower performance trained without subsampling became best performing method downsampled frequent words. shows subsampling result faster training also improve accuracy least cases. maximize accuracy phrase analogy task increased amount training data using dataset billion words. used hierarchical softmax dimensionality entire sentence context. resulted model reached accuracy achieved lower accuracy reduced size training dataset words suggests large amount training data crucial. gain insight different representations learned different models inspect manually nearest neighbours infrequent phrases using various models. table show sample comparison. consistently previous results seems best representations phrases learned model hierarchical softmax subsampling. demonstrated word phrase representations learned skip-gram model exhibit linear structure makes possible perform precise analogical reasoning using simple vector arithmetics. interestingly found skip-gram representations exhibit another kind linear structure makes possible meaningfully combine words element-wise addition vector representations. phenomenon illustrated table additive property vectors explained inspecting training objective. word vectors linear relationship inputs softmax nonlinearity. word vectors trained predict surrounding words sentence vectors seen representing distribution context word appears. values related logarithmically probabilities computed output layer word vectors related product context distributions. product works function words assigned high probabilities word vectors high probability words probability. thus volga river appears frequently sentence together words russian river word vectors result feature vector close vector volga river. many authors previously worked neural network based representations words published resulting models comparison amongst well known authors collobert weston turian mnih hinton downloaded word vectors web. mikolov already evaluated word representations word analogy task skip-gram models achieved best performance huge margin. table examples closest tokens given various well known models skip-gram model trained phrases using billion training words. empty cell means word vocabulary. give insight difference quality learned vectors provide empirical comparison showing nearest neighbours infrequent words table examples show skip-gram model trained large corpus visibly outperforms models quality learned representations. attributed part fact model trained billion words three orders magnitude data typical size used prior work. interestingly although training much larger training time skip-gram model fraction time complexity required previous model architectures. work several contributions. show train distributed representations words phrases skip-gram model demonstrate representations exhibit linear structure makes precise analogical reasoning possible. techniques introduced paper used also training continuous bag-of-words model introduced successfully trained models several orders magnitude data previously published models thanks computationally efﬁcient model architecture. results great improvement quality learned word phrase representations especially rare entities. also found subsampling frequent words results faster training signiﬁcantly better representations uncommon words. another contribution paper negative sampling algorithm extremely simple training method learns accurate representations especially frequent words. choice training algorithm hyper-parameter selection task speciﬁc decision found different problems different optimal hyperparameter conﬁgurations. experiments crucial decisions affect performance choice model architecture size vectors subsampling rate size training window. interesting result work word vectors somewhat meaningfully combined using simple vector addition. another approach learning representations phrases presented paper simply represent phrases single token. combination approaches gives powerful simple represent longer pieces text having minimal computational complexity. work thus seen complementary existing approach attempts represent phrases using recursive matrix-vector operations", "year": 2013}