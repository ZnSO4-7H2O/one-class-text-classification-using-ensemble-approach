{"title": "The Projected Power Method: An Efficient Algorithm for Joint Alignment  from Pairwise Differences", "tag": ["cs.IT", "cs.CV", "cs.LG", "math.IT", "math.OC", "stat.ML"], "abstract": "Various applications involve assigning discrete label values to a collection of objects based on some pairwise noisy data. Due to the discrete---and hence nonconvex---structure of the problem, computing the optimal assignment (e.g.~maximum likelihood assignment) becomes intractable at first sight. This paper makes progress towards efficient computation by focusing on a concrete joint alignment problem---that is, the problem of recovering $n$ discrete variables $x_i \\in \\{1,\\cdots, m\\}$, $1\\leq i\\leq n$ given noisy observations of their modulo differences $\\{x_i - x_j~\\mathsf{mod}~m\\}$. We propose a low-complexity and model-free procedure, which operates in a lifted space by representing distinct label values in orthogonal directions, and which attempts to optimize quadratic functions over hypercubes. Starting with a first guess computed via a spectral method, the algorithm successively refines the iterates via projected power iterations. We prove that for a broad class of statistical models, the proposed projected power method makes no error---and hence converges to the maximum likelihood estimate---in a suitable regime. Numerical experiments have been carried out on both synthetic and real data to demonstrate the practicality of our algorithm. We expect this algorithmic framework to be effective for a broad range of discrete assignment problems.", "text": "various applications involve assigning discrete label values collection objects based pairwise noisy data. discrete—and hence nonconvex—structure problem computing optimal assignment becomes intractable ﬁrst sight. paper makes progress towards eﬃcient computation focusing concrete joint alignment problem—that problem recovering discrete variables {··· given noisy observations modulo diﬀerences propose low-complexity model-free procedure operates lifted space representing distinct label values orthogonal directions attempts optimize quadratic functions hypercubes. starting ﬁrst guess computed spectral method algorithm successively reﬁnes iterates projected power iterations. prove broad class statistical models proposed projected power method makes error—and hence converges maximum likelihood estimate—in suitable regime. numerical experiments carried synthetic real data demonstrate practicality algorithm. expect algorithmic framework eﬀective broad range discrete assignment problems. nonconvex optimization nonconvex optimization permeates almost ﬁelds science engineering applications. instance consider structured recovery problem wishes recover structured inputs ≤i≤n noisy samples recovery procedure often involves solving optimization problem objective function measures well candidate samples. unfortunately program highly nonconvex depending choices goodness-of-ﬁt measure well feasible contrast convex optimization become cornerstone modern algorithm design nonconvex problems general daunting solve. part challenges arises existence local stationary points; fact oftentimes even checking local optimality feasible point proves np-hard. despite general intractability recent years seen progress nonconvex procedures several classes problems including low-rank matrix recovery phase retrieval dictionary learning blind deconvolution empirical risk minimization name few. example learned several problems kind provably enjoy benign geometric structure sample complexity sufﬁciently large sense local stationary points become saddle ∗department statistics stanford university stanford u.s.a. †department electrical engineering princeton university princeton u.s.a. ‡department mathematics stanford university stanford u.s.a. points diﬃcult escape problem solving certain random systems quadratic equations phenomenon arises long number equations sample size exceeds order denoting number unknowns also learned possible minimize certain non-convex random functionals—closely associated famous phase retrieval problem—even multiple local minima problems reasonably large basin attraction around global solution ﬁrst-order method converges geometrically fast. importantly existence basin often guaranteed even challenging regime minimal sample complexity. take phase retrieval problem example basin exists soon sample size order motivates development eﬃcient two-stage paradigm consists carefully-designed initialization scheme enter basin followed iterative reﬁnement procedure expected converge within logarithmic number iterations also related ideas matrix completion. present work extend knowledge nonconvex optimization studying class assignment problems represented ﬁnite alphabet detailed next subsection. unlike aforementioned problems like phase retrieval inherently continuous nature work preoccupied input space discrete already nonconvex start with. would like contribute understanding possible solve setting. joint alignment problem paper primarily focuses following joint discrete alignment problem. consider collection variables {xi}≤i≤n variable take diﬀerent possible values namely {··· imagine obtain pairwise diﬀerence samples {yij symmetric index noisy measurement modulo diﬀerence incident variables joint alignment problem ﬁnds applications multiple domains. begin with binary case deserves special attention reduces graph partitioning problem. instance community detection scenario wishes partition users clusters variables {xi} recover indicate cluster assignments user represents friendship users allows model example haplotype phasing problem arising computational this geometric property alone suﬃcient ensure rapid convergence algorithm. symmetric implies speciﬁcally impossible distinguish sets inputs {xi}≤i≤n }≤i≤n }≤i≤n genomics another example problem water-fat separation magnetic resonance imaging crucial step determine image pixel phasor possible candidates represented respectively. task takes input pre-computed pairwise cost functions provides information whether pixels details. moving beyond binary case problem motivated need jointly aligning multiple images/shapes/pictures arises various ﬁelds. imagine sequence images physical instance represents orientation camera taking image. variety computer vision tasks structural biology applications rely upon joint alignment images; equivalently joint recovery camera orientations associated image. practically often easier estimate relative camera orientation pair images using features problem boils this jointly aggregate pairwise information order improve collection camera pose estimates? contributions work propose solve problem novel model-free nonconvex procedure. informally procedure starts lifting variable higher dimensions distinct values represented orthogonal directions encodes goodness-of-ﬁt measure matrix. representation allows recast constrained quadratic program equivalently constrained principal component analysis problem. attempt optimization means projected power iterations following initial guess obtained suitable lowrank factorization. procedure proves eﬀective broad family statistical models might interesting many boolean assignment problems beyond joint alignment. section present nonconvex procedure solve nonconvex problem entails series projected power iterations higher-dimensional space. follows algorithm termed projected power method matrix representation formulation admits alternative matrix representation often amenable computation. begin with state {··· represented binary-valued vector representation appealing simplicity objective function regardless landscape allows focus quadratic optimization rather optimizing function directly. families also lead problem single simple important family obtained enforcing global scaling oﬀset speciﬁcally solution remains unchanged replaced algorithm interpret quadratic program ﬁnding principal component subject certain structural constraints. motivates tackle constrained problem means power method assistance appropriate regularization enforce structural constraints. precisely consider following procedure starts suitable initialization follows update rule denoting index largest entry fact justiﬁcation. advantage computational eﬃciency expensive step iteration lies matrix multiplication completed nearly linear time i.e. time arises fact block circulant compute matrix-vector product using m-point ffts. projection step performed ﬂops sortingbased algorithm hence much cheaper matrix-vector multiplication given occurs applications. remark alternatively take best rank- approximation debiased input matrix ldebias deﬁned computed slightly faster manner. remark natural question arises whether algorithm works arbitrary initial point. question studied special stochastic block models shows conditions second-order critical points correspond truth hence arbitrary initialization works. however condition presented therein much stringent optimal threshold moreover unclear whether local algorithm like achieve optimal computation time without proper initialization. would interesting future investigation. main motivation comes low-rank structure input matrix shall shortly many scenarios data matrix approximately rank samples noise-free. therefore low-rank approximation serves denoised version data expected reveal much information truth. low-rank factorization step performed eﬃciently method orthogonal iteration power iteration consists matrix product form well decomposition matrix rnm×m. matrix product computed ﬂops assistance m-point ffts whereas decomposition takes time summary power iteration runs time consequently matrix product constitutes main computational cost decomposition becomes bottleneck m/n. noteworthy initialization reﬁnement propose model-free make assumptions data model. whole algorithm summarized algorithm course question sequence {µt} defer section proposed two-step algorithm based proper initialization followed successive projection onto product simplices paradigm solving class discrete optimization problems. detail next section provably eﬀective family statistical models. hand remark exist many algorithms similar ﬂavor tackle generalized eigenproblems including limited sparse water-fat separation hidden clique problem phase synchronization cone-constrained automatic network analysis algorithms variants projected power method combine proper power iterations additional procedures promote sparsity enforce feasibility constraints. instance deshpande show simple models cone-constrained eﬃciently computed using generalized projected power method provided cone constraint convex. current work adds instance growing family nonconvex methods. algorithm projected power method. input input matrix ≤ij≤n; scaling factors {µt}t≥. initialize deﬁned random column best rank-m approximation loop section explores performance guarantees projected power method. assume obtained random sampling observation rate pobs included independently probability pobs assumed independent measurement noise. addition assume samples {yij independently generated. independence noise assumption hold reality serves starting point develop quantitative theoretical understanding eﬀectiveness projected power method. also common assumption literature assumptions mind exactly given representing log-likelihood candidate solution given outcome yij. ﬁnding much practical computing directly also capable achieving nearly identical statistical accuracy variety scenarios. block sparsity vector {hi}≤i≤n deﬁned denoted i{·} indicator function. since hope recover global oﬀset deﬁne misclassiﬁcation rate normalized block sparsity estimation error modulo global shift random corruption model goal accommodate general class noise models helpful start concrete simple example—termed random corruption model—such unif uniform distribution term parameter non-corruption rate since probability observation behaves like random noise carrying information whatsoever. single-parameter model write else apart mathematical simplicity random corruption model somehow corresponds worstcase situation since uniform noise enjoys highest entropy among distributions ﬁxed range thus forming reasonable benchmark practitioners. show guaranteed work even non-corruption rate vanishingly small corresponds scenario almost acquired measurements behave like random noise. formal statement this theorem consider random corruption model input matrix given suppose pobs suﬃciently large constants exists absolute constant probability approaching scales iterates algorithm obey remark throughout largest singular value fact often replace usually good choice unless employ debiased version instead typically corresponds direct current component could excessively large. addition note computed spectral initialization result result extra computational cost. remark seen section stronger version error contraction arises uniform result sense occurs simultaneously obeying regardless preceding iterates {z··· statistical dependency {yij}. particular {e··· em}n hence forms sequence feasible iterates increasing accuracy. case iterates become accurate whenever according theorem convergence ground truth expected iterations. together per-iteration cost shows computational complexity iterative stage nearly optimal since even reading data likelihood values take time order |ω|. happens soon corruption rate exceed uncovering remarkable ability tolerate correct dense input errors. irrespective whether independent data {yij} not. therefore often suﬃces power method constant number iterations initialization stage completed statistically optimal revealed following converse result. theorem consider random corruption model ﬁxed suppose pobs suﬃciently large constant mentioned before binary case bears similarity community detection problem presence communities. arguably popular model community detection stochastic block model vertices within cluster connected edge probability asymptotic limits exact partial recovery extensively studied note however primary focus community detection lies sparse regime logarithmic sparse regime n/n) contrast joint alignment problem measurements often considerably denser. however theoretical results cover dense regime e.g. facilitate comparison consider case reduces random corruption model pobs easily verify derive matches recovery threshold given general noise models theoretical guarantees develop random corruption model special instances general results. subsection cover general class noise models feasibility accurate recovery necessarily depends noise distribution precisely distinguishability output distributions {yij} given distinct inputs. particular distributions {pl}≤l<m we’d like emphasize represents distribution conditional alternatively also l-shifted distribution noise given proceed main ﬁndings. simplify matters shall concern primarily kind noise distributions obeying following assumption. assumption miny bounded away remark replace miny miny assumption however allowed scale n—which case section .—then prefactor cannot dropped. words assumption ensures noise density exceedingly lower average density point. reason introduce assumption two-fold. begin with enables preclude case entries l—or equivalently log-likelihoods—are wild. instance resulting computational instability. reason simplify analysis exposition slightly making easier readers. note however assumption crucial dropped means slight modiﬁcation algorithm detailed later. roughly speaking assumption states mutual distances possible output distributions {pl}≤l≤m within reasonable dynamic range cannot pair considerably separated pairs. alternatively understood variation log-likelihood ratio show later often governed divergence corresponding distributions. point view assumption tells submatrix signiﬁcantly volatile remaining parts often leads enhanced stability computing power iteration. theorem immediate consequence following theorem. theorem assume pobs suﬃciently large constants assumptions exist absolute constants probability tending scales iterates algorithm input matrix obey remark alternatively theorem stated terms divergence metrics like squared hellinger distance speciﬁcally theorem holds minimum squared hellinger distance obeys recovery condition non-asymptotic takes form minimum divergence criterion. consistent understanding hardness exact recovery often arises diﬀerentiating minimally separated output distributions. within projected power iterations returns estimate absolutely error soon minimum divergence exceeds threshold. threshold remarkably small pobs large equivalently many pairwise measurements available. theorem accommodates broad class noise models. highlight examples illustrate generality. begin with self-evident random corruption model belongs class klmax/klmin beyond simple model list important families satisfy assumption receive broad practical interest. list however means exhaustive. recall algorithm attempts constrained principal component enable successful recovery would naturally hope structure data matrix reveal much information truth. limit large samples helpful start looking mean given consequently column knowledge largest entries block reveals relative positions across {xi}. take column example ﬁrst blocks column attain maximum values entries telling given noisy nature acquired data would need ensure true structure stands noise. hinges upon understanding serve reasonably good proxy power iterations. since interested identifying largest entries signal contained block—which essentially mean separation largest second largest entries—is size preceding performance guarantee turns information theoretically optimal asymptotic regime. fact divergence threshold given theorem arbitrarily close information limit level every procedure bound fail minimax sense. formalize ﬁnding converse result extension removing assumption return assumption mentioned before exceedingly small might result unstable loglikelihoods suggests regularize data running algorithm. alternative introduce little entropy samples regularize noise density namely small level random noise yield eﬀectively bumps ς/m. propose algorithm using data {˜yij} ˜pl} leading following performance guarantee. theorem take suﬃciently small constant suppose algorithm operates upon {˜yij} ˜pl}. theorem holds without assumption proof. appendix extension large-m case study focused case alphabet size scale however shortage situations large cannot treated ﬁxed constant. encouraging news algorithm appears surprisingly competitive large-m case well. again begin random corruption model analysis developed ﬁxed immediately applies here. theorem suppose pobs suﬃciently large constant theorem continues hold probability least long replaced main message theorem error correction capability proposed method improves number unknowns grows. quantitative bound implies successful recovery even overwhelming fraction measurements corrupted. notably exceedingly large theorem might shed light continuous joint alignment problem. particular cases worth emphasizing coincides setting studied orthogonal group name synchronization shown leading eigenvector pobs certain data matrix becomes positively correlated truth long addition generalized power method—which equivalent projected gradient descent— provably converges solution nonconvex least-squares estimation long size noise threshold comes exact recovery wang prove semideﬁnite relaxation succeeds long constant threshold irrespective contrast exact recovery performance approach—which operates lifted discrete space rather so—improves allowing arbitrarily small suﬃciently large. hand model reminiscent general robust problem consists recovering low-rank matrix fraction observed entries corrupted. learned literature perfect reconstruction feasible tractable even though dominant portion observed entries suﬀer random corruption consistent ﬁnding theorem preceding spike model probability density measurement experiences impulse around truth. variety realistic scenarios however noise density might smooth rather spiky. smoothness conditions modeled enforcing rule sharp jump. satisfy condition take npobs view theorem sense uncovers resolution estimator smooth noise model constrain input domain unit interval letting represent grid points respectively recover variable resolution notably discrete random corruption model investigated prior literature best theoretical support derived convex programming. speciﬁcally shown convex relaxation guaranteed work soon comparison stringent recovery condition develop logarithmic factor. furthermore theorem immediate consequence general result theorem assume pobs suﬃciently large constant suppose replaced ldebias computing initial guess demonstrate proof. thus left-hand side regarded signal-to-noiseratio experienced block lij. recovery criterion thus terms lower threshold vanishingly small regime considered theorem note general alignment problem studied well although focus therein show stability semideﬁnite relaxation presence random vertex noise. regime npobs/ performance improves increases. noteworthy none polynomial algorithms proposed prior works achieves optimal scaling remains seen whether arises drawback algorithms existence inherent information-computation gap. section examines empirical performance projected power method synthetic instances real image data. statistical assumptions underlying theory typically hold practical applications numerical experiments show developed based statistical models enjoy favorable performances applied real datasets. synthetic experiments begin with conduct series monte carlo trials various problem sizes random corruption model speciﬁcally vary number unknowns input corruption rate alphabet size observation rate pobs throughout. tuple monte carlo trials conducted. trial draw uniformly random generate measurements {yij}≤ij≤n according record misclassiﬁcation rate algorithm mean empirical misclassiﬁcation rate calculated averaging monte carlo trials. fig. depicts mean empirical misclassiﬁcation rate accounts choices scaling factors particular solid lines locate asymptotic phase transitions exact recovery predicted theory. cases empirical phase transition curves come closer analytical prediction problem size increases. controls ﬂatness noise density. vary parameters take pobs experiment choices scaling factors mean misclassiﬁcation rate reported fig. empirical phase transition matches theory well. joint shape alignment next return motivating application—i.e. joint image/shape alignment—of work validate applicability datasets drawn shapenet repository chair dataset plane dataset speciﬁcally shapes taken dataset randomly sample points shape input features. shape rotated plane random continuous angle since shapes datasets high quality noise perturb shape data adding independent gaussian noise coordinate point perturbed data inputs. makes task challenging; instance resulting chair dataset around apply projected power method discretize angular domain points represents angle j◦/. following procedure adopted compute pairwise cost using nearest-neighbor distance metric; precise average nearest-neighbor squared distance samples shapes respectively. pairwise cost functions widely rotated used computer graphics vision regard assuming average nearest-neighbor distance follows gaussian distribution. careful readers might remark speciﬁed {yij} experiment. practically oftentimes access pairwise potential/cost functions rather {yij}. fortunately need algorithm proxy fig. shows ﬁrst representative shapes joint alignment chair dataset. shapes aligned reasonably good manner. quantitatively fig. displays cumulative distributions absolute angular estimation errors datasets. also reported fig. performance semideﬁnite programming —that matchlift algorithm presented note angular errors measured distance un-discretized angles {θi} hence somewhat continuous. estimates joint graph matching applicable combinatorial problems beyond joint alignment. present example called joint graph matching consider collection images containing feature points suppose exists one-to-one correspondence between feature points pair images. many oﬀ-the-shelf algorithms able compute feature correspondence points images joint matching problem concerns recovery collection globally consistent feature matches given noisy pairwise matches. mathematically think ground truth permutation matrices rm×m}≤i≤n representing feature mapping image reference true feature correspondence images represented provided pairwise matches features images encoded rm×m noisy version goal recover {xi}—up global permutation—given pairwise observations {lij}. detailed problem formulations well theoretical guarantees convex relaxation. problem diﬀers joint alignment ground truth permutation matrix. light this make modiﬁcations algorithm maintain iterates ≤i≤n matrices replace projects rm×m permutation matrices corresponds hard rounding power iterations initial guess rnm×m taken projection random column block ﬁrst apply benchmark image datasets house dataset consisting images house hotel dataset consisting images hotel. image contains feature points labeled consistently across images. initial pairwise matches obtained jonker-volgenant algorithm mismatching rates house dataset. algorithm allows lower mismatching rate house representative results dataset depicted fig. next turn three shape datasets hand dataset containing shapes fourleg dataset containing shapes human dataset containing shapes drawn collection shrec feature points hand fourleg human datasets respectively follow shape sampling pairwise matching procedures described evaluate matching performance report fraction output matches whose normalized geodesic errors threshold ranging sake comparisons plot fig. quality initial matches matches returned projected power method well matches returned semideﬁnite relaxation computation runtime reported table numerical results demonstrate projected power method signiﬁcantly faster achieving joint matching performance competitive sdp. figure comparisons input matches outputs house hotel datasets representative images shown dataset. yellow dots refer manually labeled feature points green lines represent matches consistent ground truth. projection onto standard simplex firstly algorithm involves projection onto standard simplex light this single several elementary facts concerning follows. throughout norm vector fact suppose words fact claims global oﬀset alter projection fact reveals large scaling factor results suﬃcient separation largest entry remaining ones. fig. graphical illustration. properties likelihood ratios next study log-likelihood ratio statistics. ﬁrst result makes connection divergence properties log-likelihood ratio. below distributions supported total variation distance deﬁned lemma consider probability distributions ﬁnite block random matrices additionally data matrix assumed independent blocks. thus crucial control ﬂuctuation random block matrices following lemma proves useful. lemma ≤ij≤n random symmetric block matrix {mij rm×m independently generated. suppose maxij p{mij pobs pobs n/n. probability exceeding establish performance guarantees two-stage algorithm reverse order. speciﬁcally demonstrate section iterative reﬁnement stage achieves exact recovery provided initial guess reasonably close truth. analysis initialization deferred section error contraction section mainly consists establishing following claim concerns error contraction iterative reﬁnement presence appropriate initial guess. iteration produces accurate estimate long iterates stay within reasonable neighborhood surrounding below term neighborhood basin attraction. fact initial guess successfully lands within basin subsequent iterates never jump this observe obeying furthermore emphasize theorem uniform result namely holds simultaneously within basin regardless whether independent data {yij} not. consequently theory analyses remain valid initialization schemes produce suitable ﬁrst guess. rest section thus devoted establishing theorem proofs scenarios—the ﬁxed case large case—follow almost identical arguments hence shall merge analyses. analysis outline steps proof theorem continuing helpful introduce additional assumptions notation used throughout. assume without loss generality. shall denote {hi} ≤i≤n {wi} ≤i≤n deﬁned vector ≤l≤m metric important because fact projection block onto standard simplex returns correct solution—that e—as long suﬃciently large. such show vector given obeys here take positive constant independent follows input matrix takes either original form debiased form version tailored random corruption model discussed section klmax/klmin bounded suﬃciently small suﬃciently large. concludes treatment large-error regime. small-error regime. turn second regime obeying similarly convenient high probability follows deﬁnition small-error regime. recall klmax/klmin bounded according assumption picking suﬃciently small constants applying lemma arrive summarize established claim —and hence error contraction—as long ﬁxed condition satisﬁed conditions hold. interestingly simplify case pobs leading matching condition theorem choice scaling factor proved result scaling factor condition given theorem conclude analysis theorem theorem remains convert conditions terms singular value consequences random corruption models obtained qualitative behavior iterative stage general models specialize random corruption model continuing straightforward compute metrics finally would adjust scaling factor accordingly. straightforward show scaling factor condition translated input matrix employed. observe continues hold long also verify come back assess performance spectral initialization establishing theorem below. similar deﬁnition introduce counterpart distance modulo global oﬀset produce decent estimate discussed before reveals structure truth. follows ﬁrst prove result general specialize three choices considered theorem. usual suppose without loss generality section proves minimax lower bound claimed theorem done apply random corruption model immediately establishes theorem using exactly calculation section prove theorem suﬃces analyze maximum likelihood rule minimizes bayesian error probability impose uniform prior possible inputs. continuing provide asymptotic estimate tail exponent likelihood ratio test proves crucial bounding probability error decoding. lemma {pn}n≥ {qn}n≥ sequences probability measures ﬁxed ﬁnite minny minny bounded away {yjn n}n≥ triangular array independent random variables deﬁne indicates tends regime bounded away klmin that regime always extra noise decrease klmin signiﬁcantly improving success probability results contradiction. moreover bounded away follows lemma consider {··· small constant ﬁrst single subset local likelihood ratio score—when restricted samples subgraph induced v—is suﬃciently large. precisely take last line results elementary inequality e−x. holds note according chernoﬀ bound number samples linking ω}|) npobs high probability provided npobs suﬃciently large suﬃciently large. taken collectively lemma yield ﬁrst condition consequence long suﬃciently small. remains verify second condition npobs suﬃciently large constant connected least pobs|v| vertices high probability meaning number random variables involved developed eﬃcient nonconvex paradigm class discrete assignment problems. numerous questions leave open might interesting future investigation. instance seen fig. fig. algorithm returns reasonably good estimates even information limits. natural question this characterize accuracy algorithm satisﬁed approximate solutions? addition work assumes index pairwise samples drawn uniformly random. depending application scenarios might encounter measurement patterns cannot modeled random manner; example samples might come nearby objects hence sampling pattern might highly local determine performance algorithm general sampling moreover log-likelihood functions incorporate data matrix might imperfect. study could help understand stability algorithm presence model mismatch. returning assumption remark assumption imposed primarily computational concern. fact klmax/klmin exceedingly large might actually favorable case information theoretic viewpoint indicates hypothesis corresponding klmax much easier preclude compared hypotheses. would interesting establish rigorously performance without assumption case becomes suboptimal shall modify algorithm adaptive general class noise models. moving beyond joint alignment interested seeing potential beneﬁts discrete problems. instance joint alignment problem falls category maximum posteriori inference discrete markov random ﬁeld spans numerous applications including segmentation object detection error correcting codes speciﬁcally consider discrete variables given unitary potential functions {ψi}≤i≤n well collection pairwise potential functions {ψij}∈g graph goal compute assignment similar introduce vector represent matrix rm×m encode pairwise log-potential function unitary potential function also encoded diagonal matrix rm×m such expect eﬀective solving many instances inference problems. questions amounts ﬁnding appropriate initialization allows eﬃcient exploitation unitary prior belief leave future work. partially supported grant dms- math award simons foundation. supported award. thank qixing huang motivating discussions join image alignment problem. chen grateful qixing huang leonidas guibas helpful discussions joint graph matching. tempting invoke matrix bernstein inequality analyze random block matrices loses logarithmic factor comparison bound advertised lemma turns would better resort talagrand’s inequality means talagrand’s inequality introduction. proposition form product holds probability measure. equipped norm supxl∈ωl -lipschitz convex function deﬁne respect exist absolute constants independent log-likelihood ratio statistics. main ingredient control establish following lemma. lemma consider sequences probability distributions {pi} {qi} ﬁnite generate independent random variables", "year": 2016}