{"title": "Fine-Grained Visual Categorization via Multi-stage Metric Learning", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Fine-grained visual categorization (FGVC) is to categorize objects into subordinate classes instead of basic classes. One major challenge in FGVC is the co-occurrence of two issues: 1) many subordinate classes are highly correlated and are difficult to distinguish, and 2) there exists the large intra-class variation (e.g., due to object pose). This paper proposes to explicitly address the above two issues via distance metric learning (DML). DML addresses the first issue by learning an embedding so that data points from the same class will be pulled together while those from different classes should be pushed apart from each other; and it addresses the second issue by allowing the flexibility that only a portion of the neighbors (not all data points) from the same class need to be pulled together. However, feature representation of an image is often high dimensional, and DML is known to have difficulty in dealing with high dimensional feature vectors since it would require $\\mathcal{O}(d^2)$ for storage and $\\mathcal{O}(d^3)$ for optimization. To this end, we proposed a multi-stage metric learning framework that divides the large-scale high dimensional learning problem to a series of simple subproblems, achieving $\\mathcal{O}(d)$ computational complexity. The empirical study with FVGC benchmark datasets verifies that our method is both effective and efficient compared to the state-of-the-art FGVC approaches.", "text": "fine-grained visual categorization categorize objects subordinate classes instead basic classes. major challenge fgvc co-occurrence issues many subordinate classes highly correlated difﬁcult distinguish exists large intra-class variation paper proposes explicitly address issues distance metric learning addresses ﬁrst issue learning embedding data points class pulled together different classes pushed apart other; addresses second issue allowing ﬂexibility portion neighbors class need pulled together. however feature representation image often high dimensional known difﬁculty dealing high difine-grained visual categorization aims distinguish objects subordinate classes. example images classiﬁed different breeds dogs chihuahua samoyed challenge fgvc handle co-occurrence somewhat contradictory requirements needs distinguish many similar classes part localization extract image level representations popular choices include features fisher vectors etc. recent development train convolutional neural network large-scale image dataset trained model extract features so-called deep learning features demonstrated state-of-the-art performance fgvc datasets note difﬁculties training directly fgvc datasets existing fgvc benchmarks often small paper simply take state-of-the-art deep learning features without operators focus studying better classiﬁcation approach address aforementioned co-occurring requirements fgvc. classiﬁcation step many existing fgvc methods directly learn single classiﬁer ﬁne-grained class using one-vs-all strategy apparently strategy scale well number ﬁne-grained classes number subordinate classes fgvc could large additionally one-vs-all scheme address ﬁrst issue issues namely makes efforts separate different classes without modeling intra-class variation. paper proposes distance metric learning approach aiming explicitly handle co-occurring requirements single metric. fig. illustrates works fgvc. learns distance metric pulls neighboring data points class close pushes data points different classes apart. varying neighborhood size learning metric able effectively handle tradeoff interclass intra-class variation. learned metric knearest neighbor classiﬁer applied class assignment test image. although numerous algorithms developed limited dimensional data dimensionality image data representation usually higher straightforward approach toward high dimensional reduce dimensionality data using methods principle component analysis random projection main problem dimensionality reduction methods unable take account supervised information result subspaces identiﬁed dimensionality reduction methods usually suboptimal. large number constraints large number training constraints usually required avoid overﬁtting high dimensional dml. total number triplet constraints could number examples. computational challenge learn matrix size dimensionality data study. number variables leads computational challenges ﬁnding optimal metric. first results slower convergence rate solving related optimization problem second ensure learned metric positive semi-deﬁnitive algorithms require every iteration optimization projecting intermediate solution onto cone expensive operation complexity storage limitation expensive simply save number variables memory. example study would take store completed metric memory adds complexity already difﬁcult optimization problem. work propose multi-stage metric learning framework high dimensional explicitly addresses challenges. first deal large number constraints used high dimensional divide original optimization problem multiple stages. stage small subset constraints difﬁcult classiﬁed currently learned metric adaptively sampled used improve learned metric. setting regularizer appropriately prove ﬁnal solution optimized appeared constraints. second handle computational challenge subproblem extend theory dual random projection originally developed linear classiﬁcation problems dml. proposed method enjoys efﬁciency random projection hand learns distance metric size contrast dimensionality reduction methods learn metric reduced space. finally handle storage problem propose maintain rank copy learned metric randomized algorithm rank matrix approximation. accelerates whole learning process also regularizes learned metric avoid overﬁtting. extensive comparisons benchmark fgvc datasets verify effectiveness efﬁciency proposed method. rest paper organized follows section summarizes related work dml. section describes details proposed method. section shows results empirical study section concludes work future directions. many algorithms developed detailed review found survey papers based pairwise constraints others focus optimizing triplet constraints paper adopt triplet constraints exactly serve purpose addressing second issue fgvc. although numerous studies devoted examined challenges high dimensional dml. common approach high dimensional number summation terms extremely large making difﬁcult effectively solve optimization problem although learning active help reduce number constraints number active constraints still large since many images fgvc different categories visually similar leading many mistakes. address challenge divide learning process multiple stages. s-th stage distance metric learned last stage. sample subset active triplet constraints difﬁcult classiﬁed given sampled triplet constraints update distance metric solving following optimization problem project data dimensional space learn metric space reduced dimension often leads suboptimal performance. alternative approach assume rank writing tall rectangle matrix rank ﬁxed advance applying methods. instead learning approaches directly learn data. main shortcoming approach solve non-convex optimization problem making computationally less attractive. several recent studies address high dimensional assuming sparse. although resolving storage problem still suffer high cost optimizing variables. multi-stage metric learning proposed algorithm focuses triplet constraints pull small portion nearest examples class together collection training images class assignment given distance metric distance data points measured triplet conlet straints derived training examples since share class constraint result optimal disdm loss function penalizes objective function appears effective optimization hinge loss keeping beneﬁt large margin main computational challenge comes constraint address challenge following projection paradigm ﬁrst learns metric without constraint projects cone learning process. although allows recover distance metric original dimensional space dual variables expensive impossible save memory since large fgvc address challenge instead saving propose save rank approximation speciﬁcally ﬁrst eigenvalues corresponding eigenvectors. approximate rank matrix ll⊤. different existing methods directly optimize obtain ﬁrst decompose avoid suboptimal solution. unlike requires storage space takes space save could arbitrary value. addition rank metric accelerates sampling step reducing cost computing distance rank also popular regularizer remark theorem demonstrates metric learned last stage optimized constraints stages. therefore original problem could divided several subproblems affordable number active constraints. fig. summaries framework multi-stage learning procedure. solve high dimensional subproblem dual random projection technique. simplify analysis investigate subproblem ﬁrst stage following stages could analyzed way. introducing convex conjugate dual problem dual variable matrix deﬁned abi. αtat setting gradient respect zero. rd×m gaussian random matrices number random projections triplet constraint project representation dimensional space decaf features extracted image representations experiments. although activation deep convolutional network trained imagenet outperforms conventional visual features many general tasks concatenate features last three fully connected layers dimension resulting features apply proposed algorithm learn distance metric learned metric together smoothed knearest neighbor classiﬁer variant k-nn predict class assignments test examples. different conventional k-nn ﬁrst obtains reference centers class clustering training images class clusters. then computes query’s distance class soft distances test image corresponding reference centers assigns test image avoid overﬁtting learning high dimensional metric however issue efﬁciently compute eigenvectors eigenvalues stage. particularly challenging case even computed explicitly large size. second exploit randomized theory efﬁciently compute eigen-decomposition speciﬁcally rd×q gaussian random matrix. according overwhelming probability eigenvectors subspace spanned column vectors provided constant independent limitation method requires appearance matrix computing keeping whole matrix unaffordable here. fortunately replacing according approximate eigenvectors size computed efﬁciently since sparse matrix. overall computational cost proposed algorithm rank approximation linear note sparse matrix cumulated stages. alg. summarizes steps proposed approach rank approximation stand eigen decomposition matrix. note distributed computing particularly effective realization algorithm matrix multiplications accomplished parallel helpful also large. class shortest distance. efﬁcient predicting especially large-scale training performance similar conventional one. refer classiﬁcation approach based metric learned proposed algorithm smoothed k-nn msml smoothed k-nn euclidean distance original space euclid. although size covariance matrix large rank small number training examples thus computed explicitly. state-of-the-art algorithm i.e. lmnn preprocess also included comparison. one-vs-all strategy based implementation liblinear used baseline fgvc regularization parameter varied also include state-of-the-art results fgvc evaluation. parameters used msml empirically number random projections number random combinations applied lmnn reduce dimensionality before metric learned. lmnn implemented code original authors recommended parameters used ensure baseline method fully exploits training data maximum number iterations lmnn parameter values used throughout experiments. training/test splits provided datasets. mean accuracy standard evaluation metric fgvc used evaluate classiﬁcation performance. experiments single machine .ghz cores memory. cats&dogs contains images species images class training rest test. table summaries results. first observe msml accurate baseline lsvm. surprising distance metric learned training examples class assignments. contrast one-vs-all approach used lsvm classiﬁcation function class learned examples class assignment second method performs signiﬁcantly better baseline method indicating unsupervised dimension reduction method result suboptimal solutions dml. fig. compares images similar query images using metric learned proposed algorithm based metric learned lmnn euclid observe images class query found metric learned msml lmnn. example msml able capture difference species lmnn returns similar images wrong class assignments. third msml overwhelming performance compared state-of-the-art fgvc approaches. although method using ground truth head bounding segmentation achieves msml better image information shows advantage proposed method. finally takes less second extract decaf features image based implementation simple segmentation operator costs seconds reported study making proposed method fgvc appealing. evaluate performance msml extremely high dimensional features concatenate conventional features using pipeline visual feature extraction outlined speciﬁcally extract features different scales encode dimensional feature dictionary method pooling strategy used aggregate local features single vector representation. finally features extracted image total dimension msml combined features denoted msml+ improves performance table note time extracting high dimensional conventional features second image still much cheaper segmentation localization operator. ﬂowers oxford ﬂowers dataset ﬂower species consists images classes. class images training rest test. table shows results different methods. similar conclusion baseline methods. msml outperforms lsvm lmnn signiﬁcantly. although lsvm already performs well msml improves accuracy. additionally observed even performances state-of-the-art methods segmentation operators much worse msml. note figure examples retrieved images. ﬁrst column indicates query images highlighted green bounding boxes. columns include similar images measured euclid. columns show metric lmnn. columns metric msml. images columns highlighted bounding boxes share category queries blue bounding boxes not. birds caltech-uscd-- birds dataset bird species classes images class roughly images training. version ground truth bounding box. table compares proposed method state-of-the-art baselines. first obvious performance msml significantly better baseline methods observation above. second although symb combines segmentation localization msml outperforms without time consuming operator. third symb* ali* mirror training images improve performances msml even better without trick. finally msml outperforms method combining decaf features models fact studies fgvc ignore choosing appropriate base classiﬁer simply adopt linear one-vs-all strategy. comparison also report result mirroring training images denoted msml+*. provides uses hand annotated segmentations followed multiple kernel msml outperforms without supervised information conﬁrms effectiveness proposed method. fig. illustrates changing trend test mean accuracy number stages increases. observe msml converges fast veriﬁes multi-stage division essential proposed framework. another improvement msml+ shown table illustrate capacity msml exploring correlation among classes makes effective simple one-vs-all classiﬁer fgvc conduct additional experiment. randomly select classes birds target classes test images target classes evaluation. learning metric besides training images target classes sample classes unselected ones auxiliary classes training images auxiliary classes additional training examples dml. fig. compares performance lsvm msml increasing number auxiliary classes. surprising observe performance lsvm decreases little since unable explore supervision information auxiliary classes improve classiﬁcation accuracy target classes auxiliary classes intensify class imbalance problem. contrast performance msml improves signiﬁcantly increasing auxiliary classes indicating msml capable effectively exploring training data auxiliary classes therefore particularly suitable fgvc. s-dogs stanford species dataset contains classes images images class used training. since subset imagenet decaf model trained from report result table reference. package state-of-the-art algorithm solving linear implemented mostly core part lmnn also implemented time feature extraction included shared methods comparison. running time msml includes operational cost table summarizes results comparison. first takes msml time complete computation compared lmnn. msml employs stochastic optimization method optimal distance metric lmnn batch learning method. second observe proposed method signiﬁcantly efﬁcient lsvm datasets. high computational cost lsvm mostly comes aspects. first lsvm train classiﬁcation model class becomes signiﬁcantly slower number classes large. second fact images different classes visually similar makes computationally difﬁcult optimal linear classiﬁer separate images class images classes. contrast training time msml independently number classes making appropriate fgvc. finally running time msml+ features doubles msml veriﬁes proposed method linear dimensionality conclusion paper propose multi-stage metric learning framework high dimensional fgvc problem addresses challenges arising high dimensional dml. speciﬁcally divides original problem multiple stages handle challenge arising many triplet constraints extends theory dual random projection address computational challenge high dimensional data develops randomized rank matrix approximation algorithm storage challenge. empirical study shows proposed method general purpose features yields performance signiﬁcantly better state-of-the-art approaches fgvc. future plan combine proposed algorithm segmentation localization improve performance fgvc. additionally since proposed", "year": 2014}