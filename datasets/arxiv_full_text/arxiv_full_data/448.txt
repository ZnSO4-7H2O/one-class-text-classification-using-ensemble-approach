{"title": "Learning Natural Language Inference with LSTM", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "Natural language inference (NLI) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate learning-centered methods such as deep neural networks for natural language inference (NLI). In this paper, we propose a special long short-term memory (LSTM) architecture for NLI. Our model builds on top of a recently proposed neural attention model for NLI but is based on a significantly different idea. Instead of deriving sentence embeddings for the premise and the hypothesis to be used for classification, our solution uses a match-LSTM to perform word-by-word matching of the hypothesis with the premise. This LSTM is able to place more emphasis on important word-level matching results. In particular, we observe that this LSTM remembers important mismatches that are critical for predicting the contradiction or the neutral relationship label. On the SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the state of the art.", "text": "natural language inference fundamentally important task natural language processing many applications. recently released stanford natural language inference corpus made possible develop evaluate learning-centered methods deep neural networks natural language inference paper propose special long short-term memory architecture nli. model builds recently proposed neural attention model based signiﬁcantly different idea. instead deriving sentence embeddings premise hypothesis used classiﬁcation solution uses match-lstm perform wordby-word matching hypothesis premise. lstm able place emphasis important word-level matching results. particular observe lstm remembers important mismatches critical predicting contradiction neutral relationship label. snli corpus model achieves accuracy outperforming state art. natural language inference problem determining whether premise sentence infer another hypothesis sentence fundamentally important problem applications many tasks including question answering semantic search automatic text summarization. much interest past decade especially surrounding pascal recognizing textual entailment challenge existing solutions range shallow approaches based lexical similarities advanced methods consider syntax perform explicit sentence alignment formal logic recently bowman released stanford natural language inference corpus purpose encouraging learning-centered approaches nli. corpus contains around sentence pairs three labels entailment contradiction neutral. size corpus makes feasible train deep neural network models typically require large amount training data. bowman tested straightforward architecture deep neural networks nli. architecture premise hypothesis represented sentence embedding vector. vectors multi-layer neural network train classiﬁer. bowman achieved accuracy long short-term memory networks used obtain sentence embeddings. recent work rockt¨aschel improved performance applying neural attention model. basic architecture still based sentence embeddings premise hypothesis difference embedding premise takes consideration alignment premise hypothesis. so-called attention-weighted representation premise shown help push accuracy limitation aforementioned models reduce premise hypothesis single embedding vector matching them; i.e. embedding vectors perform sentence-level matching. however word phrase-level matching results equally important. example matching between stop words sentences likely contribute much ﬁnal prediction. also hypothesis contradict premise single word phrase-level mismatch sufﬁcient matching results less important intuition hard captured directly match sentence embeddings. paper propose lstm-based architecture learning natural language inference. different previous models prediction based whole sentence embeddings premise hypothesis. instead lstm perform word-by-word matching hypothesis premise. lstm sequentially processes hypothesis position tries match current word hypothesis attention-weighted representation premise. matching results critical ﬁnal prediction remembered lstm less important matching results forgotten. refer architecture matchlstm mlstm short. show mlstm model achieves accuracy snli corpus outperforming state art. furthermore analyses learned parameters show mlstm architecture indeed pick important word-level matching results need remembered ﬁnal prediction. particular observe good wordlevel matching results generally forgotten important mismatches often indicate contradiction neutral relationship tend remembered. review lstm. review word-by-word attention model rockt¨aschel best performing model. finally present mlstm architecture natural language inference. background lstm ﬁrst brieﬂy review lstm lstm special form recurrent neural networks process sequence data. lstm uses gate vectors position control passing information along sequence thus improves modeling long-range dependencies. different variations lstms present adopted rockt¨aschel speciﬁcally denote input sequence position internal vectors including input gate forget gate output gate memory cell vectors used together generate d-dimensional hidden state follows tanh tanh sigmoid function elementwise multiplication vectors rd×lv* rd×d weight matrices vectors learned. neural attention model natural language inference task sentences premise hypothesis. embedding vector corresponding word. goal predict label indicates relationship paper assume entailment contradiction neutral. representation whole premise. rockt¨aschel used represents whole premise together approximately regarded aggregated representation hypothesis predict label model although neural attention model rockt¨aschel achieved better results bowman limitations. first model still uses single vector representation premise namely match entire hypothesis. speculate instead attention-weighted representations premise matching i.e. position match hidden state hypothesis hypothesis could achieve better matching results. done using position takes input determines well overall matching sentences current position. produce single vector representing matching entire sentences. model rockt¨aschel explicitly allow place emphasis important matching results premise hypothesis down-weight less critical ones. example matching stop words presumably less important matching content words. also matching results particularly critical making ﬁnal prediction thus remembered. example consider premise jumping frisbee snow. hypothesis washes face whiskers front paw. sequentially process hypothesis subject hypothesis match subject premise high probability believe contradiction. mismatch remembered. pothesis) last cell state ﬁrst lstm denote resulting hidden states corresponding respectively. main idea word-by-word attention model rockt¨aschel introduce series attention-weighted combinations hidden states premise combination particular word hypothesis. denote attention vector word hypothesis. speciﬁcally deﬁned follows attention-weighted premise essentially tries model relevant parts premise respect i.e. word hypothesis. rockt¨aschel built model {ak}n deﬁning following hidden states word-by-word attention model rockt¨aschel different underlying model same. presentation close bahdanau attention vectors corresponding context vectors paper. portant matching results remembered lstm non-essential ones forgotten. concatenation attention-weighted version premise word hypothesis hidden state word itself input mlstm. besides difference lstm architecture also introduce changes model rockt¨aschel first insert special word null premise allow words hypothesis aligned null. inspired common practice machine translation. speciﬁcally introduce vector ﬁxed vector dimension represents null used derive attention vectors {ak}n third words pre-trained word embeddings take average embeddings words surrounding unseen word within window size approximation embedding unseen word. update figure ﬁgure depicts model rockt¨aschel bottom ﬁgure depicts model. represents hidden states note model represents weighted version premise only model represents matching premise hypothesis position speciﬁcally model works follows. first similar rockt¨aschel process premise hypothesis using lstms feed last cell state premise lstm hypothesis. need lstm hypothesis encode knowledge premise match premise hypothesis using hidden states lstms. again represent hidden states. word embedding learning model. although crude approximation reduces number parameters need update turns still achieve better performance rockt¨aschel experiment settings data snli corpus test effectiveness model. original data contains sentence pairs labeled following relationships entailment contradiction neutral indicates lack consensus human annotators. discard sentence pairs labeled keep remaining ones experiments. pairs training pairs development pairs testing. follows data partition used bowman experiments. perform three-class classiﬁcation accuracy evaluation metric. parameters adam method hyperparameters optimization. initial learning rate decay ratio iteration. batch size experiment dimension hidden states. methods comparison mainly want compare model word-by-word attention model rockt¨aschel model achieved state-of-the-art performance snli corpus. ensure fair comparison besides comparing accuracy reported rockt¨aschel also re-implemented model report performance implementation. also consider variations model. speciﬁcally following models implemented tested experiments word-by-word attention implementation word-by-word attention model rockt¨aschel dimension hidden states differences implementation original implementation rockt¨aschel following also null token premise matching. feed last cell state lstm premise lstm hypothesis keep consistent implementation model. word representation also glove word embeddings update word embeddings. unseen words adopt strategy described section mlstm bi-lstm sentence modeling model except derive hidden states sentences bi-lstms instead lstms. implement model whether bilstms allow better align sentences. mlstm mlstm model mlstm word embedding model except directly word embedding vectors instead hidden states model. case attention vector weighted experiment setting hypothesize effectiveness model largely related mlstm architecture rather lstms process original sentences. table experiment results terms accuracy. dimension hidden states. |θ|w+m total number parameters |θ|m number parameters excluding word embeddings. note models last section implemented results taken directly previous papers. note also models last section update word embeddings |θ|w+m |θ|m. three columns right accuracies trained models training data development data test data respectively. table three examples sentence pairs different relationship labels. second hypothesis contradiction mentions completely different event. third hypothesis neutral premise phrase with owner cannot inferred premise. data set. compare mlstm model implementation word-by-word attention model rockt¨aschel setting performance test data higher model also tested statistical signiﬁcance found improvement statistically signiﬁcant level. performance mlstm bi-lstm sentence modeling compared model standard lstm sentence modeling shows using bi-lstm process original sentences helps difference small complexity bi-lstm much higher lstm. therefore increased experiment bilstm sentence modeling. interestingly experimented mlstm model using pre-trained word embeddings instead lstmgenerated hidden states initial representations premise hypothesis able achieve accuracy test data still better previously reported state art. suggests mlstm architecture coupled attention model works well regardless whether lstm process original sentences. task three-way classiﬁcation problem better understand errors also show confusion matrix results obtained mlstm model table confusion neutral entailment neutral contradiction entailment contradiction. shows neutral relatively hard capture. obtain better understanding proposed model actually performs matching between premise hypothesis conduct following analyses. first look learned word-by-word alignment weights check whether soft alignment makes sense. done rockt¨aschel look values various gate vectors mlstm. looking values check whether model able differentiate important less important word-level matching results whether model forgets certain matching results remembers certain ones. conduct analyses choose three examples display various learned parameter values. three sentence pairs share premise different hypotheses different recall degree word hypothesis aligned word premise. also recall weights conﬁgured means weights plots three plots alignment weights generally make sense. example example animal strongly aligned aligned frisbee. phrase cold weather aligned snow. example also strongly aligned game aligned frisbee. example strongly aligned washes aligned jumping. appear matching results wrong. however likely best match among words premise show later match actually strong indication contradiction sentences. explanation applies match washes jumping. also observe words aligned null token inserted. example word hypothesis example correspond word premise therefore aligned null. words face whiskers example owner example also aligned null. intuitively important content words hypothesis aligned null likely relationship label either contradiction neutral. next look values learned gate vectors mlstm three examples. show values setting plots corresponds dimensions. again darker color indicates higher value. input gate controls whether input current position used deriving ﬁnal hidden state current position. three plots input gates observe generally stop words prepositions articles input gates lower values suggesting matching words less important. hand content words nouns verbs verify observation above compute average input gate values stop words content words. former average value standard deviation latter average value standard deviation shows indeed generally stop words lower input gate values. interestingly also stop words higher input gate values critical classiﬁcation task. example negation word average input gate value standard deviation next look forget gates. recall forget gate controls importance previous cell state deriving ﬁnal hidden state current position. higher values forget gate indicate need remember previous cell state pass whereas lower values indicate probably forget previous cell. three plots forget gates overall colors lightest example entailment. suggests hypothesis entailment premise mlstm tends forget previous matching results. hand example example contradiction neutral generally darker colors. particular example colors consistently dark starting word hypothesis end. believe explanation mlstm processes ﬁrst three words hypothesis washes sees matching washes jumping strong indication contradiction therefore matching results need remembered mlstm ﬁnal prediction. also checked forget gates sentence pairs test data computing average forget gate values standard deviations entailment neutral contradiction respecbased observations above hypothesize mlstm works follows. remembers important mismatches useful predicting contradiction neutral relationship forgets good matching results. mlstm important mismatch remembered ﬁnal classiﬁer likely predict entailment default. otherwise depending kind mismatch remembered classiﬁer predict either contradiction neutral. much work natural language inference. shallow methods rely mostly lexical similarities shown robust. example bowman experimented lexicalized classiﬁer-based method uses lexical information achieves accuracy snli corpus. advanced methods syntactic structures sentences help matching them. example mehdad applied syntactic-semantic tree kernels recognizing textual entailment. inference essentially logic problem methods based formal logic natural logic also proposed. comprehensive review existing work found book dagan work relevant recently proposed neural attention model-based method rockt¨aschel detailed previous sections. neural attention models recently applied natural language processing tasks including machine translation abstractive summarization question answering rockt¨aschel showed neural attention model could help derive betsnli corpus used studies. besides work bowman rockt¨aschel studies used snli corpus. vendrov used skip-thought model proposed kiros task reported accuracy test data. used treebased encoders obtain sentence embeddings achieved accuracy paper proposed special lstm architecture task natural language inference. based recent work rockt¨aschel ﬁrst used neural attention models derive attention-weighted vector representations premise. designed match-lstm processes hypothesis word word trying match hypothesis premise. last hidden state mlstm used predicting relationship premise hypothesis. experiments snli corpus showed mlstm model outperformed state-of-the-art performance reported data set. moreover closer analyses gate vectors revealed mlstm indeed remembers passes important matching results typically mismatches indicate contradiction neutral relationship premise hypothesis. large number parameters learn inevitable limitation model large training data needed learn good model parameters. indeed preliminary experiments applying mlstm sick corpus smaller textual entailment benchmark data give good results. believe model learns everything scratch except using pre-trained word embeddings. future direction would incorporate resources paraphrase database learning process. bill maccartney michel galley christopher manning. phrase-based alignment model natural language inference. proceedings conference empirical methods natural language processing. marco marelli stefano menini marco baroni luisa bentivogli raffaella bernardi roberto zamparelli. sick cure evaluation compositional distributional semantic proceedings ninth international models. conference language resources evaluation. yashar mehdad alessandro moschitti fabio massiomo zanzotto. semker syntactic/semantic kernels recognizing proceedings text analtextual entailment. ysis conference. richard socher christopher manning. glove global vectors word representation. proceedings conference empirical methods natural language processing. edward grefenstette karl moritz hermann tom´aˇs koˇcisk`y phil blunsom. reasoning entailment neural attention. proceedings international conference learning representations. alexander rush sumit chopra jason weston. neural attention model abstractive sentence summarization. proceedings conference empirical methods natural language processing.", "year": 2015}