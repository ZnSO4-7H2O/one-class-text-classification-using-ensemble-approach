{"title": "Randomized Nonnegative Matrix Factorization", "tag": ["stat.ML", "cs.CV"], "abstract": "Nonnegative matrix factorization (NMF) is a powerful tool for data mining. However, the emergence of `big data' has severely challenged our ability to compute this fundamental decomposition using deterministic algorithms. This paper presents a randomized hierarchical alternating least squares (HALS) algorithm to compute the NMF. By deriving a smaller matrix from the nonnegative input data, a more efficient nonnegative decomposition can be computed. Our algorithm scales to big data applications while attaining a near-optimal factorization, i.e., the algorithm scales with the target rank of the data rather than the ambient dimension of measurement space. The proposed algorithm is evaluated using synthetic and real world data and shows substantial speedups compared to deterministic HALS.", "text": "nonnegative matrix factorization powerful tool data mining. however emergence ‘big data’ severely challenged ability compute fundamental decomposition using deterministic algorithms. paper presents randomized hierarchical alternating least squares algorithm compute nmf. deriving smaller matrix nonnegative input data eﬃcient nonnegative decomposition computed. algorithm scales data applications attaining near-optimal factorization i.e. algorithm scales target rank data rather ambient dimension measurement space. proposed algorithm evaluated using synthetic real world data shows substantial speedups compared deterministic hals. elsevier ltd. rights reserved. techniques dimensionality reduction principal component analysis essential analysis highdimensional data. methods take advantage redundancies data order low-rank parsimonious model describing underlying structure input data. indeed core machine learning assumption lowrank structures embedded high-dimensional data dimension reduction techniques basis vectors represent data linear combination lower-dimensional space. enables identiﬁcation features eﬃcient analysis high-dimensional data. signiﬁcant drawback commonly-used dimensionality reduction techniques permit positive negative terms components. many data analysis applications negative terms fail hold physically meaningful interpretation. example images represented grid nonnegative pixel intensity values. context negative terms principal components interpretation. address problem researchers proposed restricting basis vectors nonnegative terms paradigm called nonnegative matrix factorization emerged powerful dimension reduction technique. versatile tool allows computation sparse physically meaningful factors describe coherent structures within data. prominent applications areas image processing information retrieval gene expression analysis instance surveys berry gillis however computationally intensive becomes infeasible massive data. hence innovations reduce computational demands increasingly important ‘big data’. randomized methods linear algebra recently introduced ease computational demands posed classical matrix factorizations inspired ideas tepper sapiro proposed compressed accelerated algorithms based idea bilateral random projections compressed algorithms reduce computational load considerably often fail converge many experiments. follow probabilistic approach matrix approximations formulated halko speciﬁcally propose randomized hierarchical alternating least squares algorithm compute nmf. demonstrate randomized algorithm eases computational challenges posed massive data assuming input data feature low-rank structure. experiments show algorithm reliable attains near-optimal factorization. further manuscript accompanied open-software package ristretto written python allows reproduction results manuscript organized follows first section brieﬂy reviews well basic concept randomized matrix algorithms. then section describes randomized variant hals algorithm. followed empirical evaluation section synthetic real world data used demonstrate performance algorithm. finally section concludes manuscript. low-rank approximations fundamental widely used tools data analysis dimensionality reduction data compression. goal methods matrices much lower rank approximate high-dimensional matrix target rank approximation denoted integer min{m ubiquitous example tools singular value decomposition ﬁnds exact solution problem least-square sense optimality property similar methods desirable many scientiﬁc applications resulting factors guaranteed physically meaningful many others. imposes orthogonality constraints factors leading holistic rather parts-based representation input data. further basis vectors popular decompositions mixed sign. thus natural formulate alternative factorizations optimal least-square sense preserve useful properties sparsity nonnegativity. properties found nmf. roots traced back work paatero tapper seung independently introduced popularized concept context psychology several years later. formally attempts solve equation additional nonnegativity constraints constraints enforce input data expressed additive linear combination. leads sparse parts-based features appearing decomposition intuitive interpretation. example components face image dataset reveal individual nose mouth features whereas components yield holistic features known ‘eigenfaces’. though bears desirable property interpretability optimization problem inherently nonconvex ill-posed. general convexiﬁcation exists simplify optimization meaning exact unique solution guaranteed diﬀerent algorithms therefore produce distinct decompositions minimize objective function. denotes frobenius norm matrix. however optimization problem equation nonconvex respect factors resolve this algorithms divide problem simpler subproblems closed-form solutions. convex subproblem solved keeping factor ﬁxed updating other alternating iterating convergence. popular techniques minimize subproblems method multiplicative updates proposed seung procedure essentially rescaled version gradient descent. simple implementation comes expense much slower convergence. appealing alternating least squares methods variants. among these hals proves highly eﬃcient without exhaustive would also like point interesting work gillis glineur well proposed improved accelerated hals algorithms computing nmf. another approach compute based idea column subset selection. method columns input matrix chosen form factor matrix denotes index set. factor matrix found solving following optimization problem context approach appealing input matrix separable means must possible select basis vectors columns input matrix case selecting actual columns preserves underlying structure data allows meaningful interpretation. assumption intrinsic many applications e.g. document classiﬁcation blind hyperspectral unmixing however approach limited potential applications data dense noisy. separable schemes unique obtained various algorithms. finding meaningful column subset explored decomposition extracts columns best describe data. addition decomposition leverages statistical signiﬁcance columns rows improve interpretability leading near-optimal decompositions. another interesting algorithm compute near-separable proposed zhou algorithm ﬁnds conical hulls smaller subproblems computed parallel details ongoing research column selection algorithms refer reader wang zhang boutsidis woodruﬀ wang ‘big data’ probabilistic methods become indispensable computing low-rank matrix approximations. central concept utilize randomness order form surrogate matrix captures essential information high-dimensional input matrix. assumes input matrix features low-rank structure i.e. eﬀective rank smaller ambient dimensions. following halko probabilistic framework low-rank approximations proceeds follows. matrix without loss generality assume first approximate range provides best possible basis least-square sense near-optimal basis obtained using random projections rn×k random test matrix. recall target rank approximation denoted integer assumed typically entries independently identically drawn standard normal distribution. next qr-decomposition used form matrix rm×k orthogonal columns. thus matrix forms near-optimal normal basis input matrix high-dimensional data pose computational challenge deterministic nonnegative matrix factorization despite modern optimization techniques. indeed costs solving optimization problem formulated equation prohibitive. motivation randomness strategy ease computational demands extracting low-rank features high-dimensional data. speciﬁcally randomized hierarchical alternating least squares algorithm formulated eﬃciently compute nonnegative matrix factorization. block coordinate descent methods universal approach algorithmic optimization iterative methods block components optimize respect remaining components. following philosophy hals algorithm unbundles original problem sequence simpler optimization problems. allows eﬃcient computation suppose update ﬁxing terms except block comprised column thus subproblem essentially reduced smaller minimization. hals approximately minimizes cost function equation respect remaining components process preserves geometric structure euclidean sense. smaller matrix many applications suﬃcient construct desired low-rank approximation. approximation quality controlled oversampling concept power iterations here denotes oversampling parameter number additional power iterations. follows increases error tends towards best possible approximation error i.e. singular value σk+. rigorous error analysis provided halko refer reader surveys halko mahoney drineas mahoney martinsson detailed discussions randomized algorithms. implementations randomized related lowrank approximations szlam voronin martinsson erichson remark power iterations help improve quality basis matrix idea pre-process input matrix order sample matrix faster decaying singular value spectrum. therefore equation replaced speciﬁes number power iterations. drawback however additional passes input matrix required. note direct implementation power iteration numerically unstable. thus subspace iterations used instead remark performance algorithms depends procedure used initializing factor matrices. refer langville excellent discussion topic. proceed simply initializing factor matrices gaussian entries negative elements remark predeﬁning maximum number iterations satisfactory practice. instead stop algorithm suitable measure reached convergence tolerance. discussion stopping criteria instance gillis glineur algorithm terminated convergence rate smaller stopping condition following quantity stopping criteria ˜wv)/v max) elementwise maximum operator rotate low-dimensional space stopping criterion maximum number iterations reached return nonnegative factor matrices rm×k rk×n employing randomness reformulate optimization problem equation low-dimensional optimization problem. speciﬁcally high-dimensional input matrix replaced surrogate matrix formed described section thus yield impose following constraint note nonnegativity constraints need apply highdimensional factor matrix necessarily factor matrix rotated back high-dimensional space using following approximate relationship thus equation solved approximately since further reason nonnegative entries low-dimensional projection decrease objective function equation proof similar cohen demonstrated. computational steps summarized algorithm remark entries random test matrix drawn independently uniform distribution interval nonnegative random entries perform better gaussian distributed entries seem natural choice context nonnegative data. remark oversampling required good basis matrix realworld data often exact rank. thus instead computing random projections compute order form basis matrix speciﬁcally procedure increases probability approximately captures column space experiments show small oversampling values achieve good approximation results. following evaluate proposed randomized algorithm compare deterministic hals algorithm implemented scikit-learn package througout experiments oversampling parameter number subspace iterations randomized algorithm. further tolerance parameter stopping condition limit number iterations maximum comparison also provide results compressed algorithm using default settings proposed tepper sapiro computations performed laptop intel core quad-core .ghz fast memory. practice often great interest extract features data order characterize underlying structure preprocess data. extracted features used instance visualize data classiﬁcation. following mnist database handwritten digits comprises training testing images demonstration image patch dimension represents digit figure shows ﬁrst dominant basis images computed deterministic randomized hals well svd. compared holistic features found computes parts-based representation digits. hence digit formed additive combination individual parts. furthermore basis images simple interpret. table summarizes computational results shows randomized algorithms achieve near-optimal reconstruction error signiﬁcantly reducing computation. speciﬁcally costs iteration randomized hals algorithms substantially lower deterministic algorithm. next investigate question whether quality features computed randomized algorithm suﬃcient classiﬁcation. basis images ﬁrst project data low-dimensional space k-nearest-neighbors method classiﬁcation. results training test samples shown table reconstruction error randomized compressed algorithm slightly poorer deterministic hals algorithm. interestingly similar classiﬁcation performance terms precision recall f-score achieved. note design best possible classiﬁer note tepper sapiro also used active alternating direction method multipliers computing nmf. methods perform better algorithm several empirical experiments faced numerical issues applications interest hence present results compressed algorithm following. further tepper sapiro used vanilla ﬂavored implementation power scheme; tends distort basis roundoﬀ errors practice recommended implement power scheme concept subspace iterations. fig. dominant basis images extracted mnist dataset using deterministic randomized hals algorithm well unlike provides parts-based features characterizing underlying structure data. note data mean centered. table classiﬁcation results mnist dataset using k-nearestneighbors method. results show overall predictive accuracy randomized deterministic features similar. hyperspectral unmixing separate pixel spectra collection so-called endmembers abundances. speciﬁcally endmembers represent pure materials abundances reﬂect fraction endmember present pixel represents simple linear mixing model blind form following popular ‘urban’ hyperspectral image demonstration hyperspectral image dimension pixels corresponding area meters. further image consists spectral bands; however omit several channels dense water vapor atmospheric eﬀects. thus bands following analysis aims automatically extract four endmembers asphalt grass tree roof. figure shows dominant basis images reﬂecting four endmembers well corresponding abundance map. deterministic randomized algorithms faithfully reﬂect four endmembers. feature extraction facial images essential preprocessing step facial recognition. ordinary approach task ﬁrst studied kirby sirovich later turk pentland resulting basis images represent ‘shadows’ faces so-called eigenfaces. however instead learning holistic representations seems natural learn parts-based representation facial data. ﬁrst demonstrated seminal work seung using nmf. corresponding basis images robust occlusions easier interpret. following downsampled cropped yale face database dataset comprises grayscale images cropped aligned. image dimension thus yield data matrix dimension vectorizing stacking images. figure shows dominant features extracted deterministic randomized hals characterize facial features. randomized algorithm achieves substantial speed-up factor reconstruction error remains near-optimal shown table comparison compressed algorithm performs slightly poorer overall. table summary computational results yale face database randomized hals algorithm achieves substantial speedup factor baseline speedup deterministic hals. target rank hyperspectral imaging often used order determine materials underlying processes present scene. uses data collected across electromagnetic spectrum. diﬃculty however pixels hyperspectral images commonly consist mixture several materials. thus fig. dominant basis images abundance maps extracted ‘urban’ hyperspectral image. extracted spectral signatures randomized deterministic algorithms similar abundance maps show slight diﬀerences speciﬁcally peaks spectra. table summary computational results hyperspectral image. randomized hals achieves speedup attaining relative error deterministic algorithm. target rank baseline speedup deterministic hals. table quantiﬁes results. randomized hals compressed algorithms require iterations converge compared deterministic hals. speedup randomized hals considerable factor whereas performance compressed weak speedup error. synthetic nonnegative data contextualize computational performance algorithms. speciﬁcally construct low-rank matrices consisting nonnegative elements drawn gaussian distribution. first compute low-rank matrices dimension rank figure shows relative error timings speedup varying target-ranks averaged runs. first notice randomized hals algorithm shows signiﬁcant speedup deterministic hals algorithm factor achieving high accuracy. here limited maximum number iterations randomized deterministic hals algorithm. required even higher accuracy could achieved allowing larger number iterations. algorithm known require larger number iterations. thus allowed maximum number iterations despite large number iterations compressed algorithms shows patchy performance comparison. results small target ranks satisfactory algorithm diﬃculties approximating factors larger ranks high accuracy. further compressed algorithm converge matrix. next evaluation repeated presence additive nonnegative gaussian noise. results shown figure show similar picture. before randomized algorithm outperforms deterministic showing near-optimal relative error. compressed algorithm shows performance issues above. massive nonnegative data poses computational challenge deterministic algorithms. however randomized algorithms promising alternative computing approximate nonnegative factorization. computational complexity proposed randomized algorithm scales target rank rather ambient dimension measurement space. hence computational advantage becomes pronounced increasing dimensions input matrix. randomized hals algorithm substantially reduces computational demands achieving near-optimal reconstruction results. thereby trade-oﬀ precision speed controlled concept oversampling number power iterations. prose computation subspace iterations default values. settings show good performance throughout experiments. overall randomized hals algorithm shows considerable advantages deterministic hals previously proposed compressed algorithm. future research investigate gpu-accelerated implementation proposed randomized hals algorithm. furthermore exciting research direction extend presented ideas nonnegative tensor factorization using randomized framework proposed erichson fig. randomized hals algorithm outperforms tall-and-skinny synthetic data matrices rank compressed algorithm fails converge baseline speedup deterministic hals. mahoney m.w. drineas matrix decompositions improved data analysis. proceedings national academy sciences martinsson p.g. randomized methods matrix computations pedregosa varoquaux gramfort michel thirion grisel blondel prettenhofer weiss dubourg vanderplas passos cournapeau brucher perrot duchesnay scikit-learn machine learning python. journal machine learning research szlam kluger tygert implementation randomized algorithm principal component analysis. preprint arxiv. tepper sapiro compressed nonnegative matrix factorization fast accurate. ieee transactions signal processing turk m.a. pentland a.p. face recognition using eigenfaces proceedings computer vision pattern recognition ieee. udell townsend nice latent variable models log-rank. arxiv voronin martinsson p.g. rsvdpack subroutines computing partial singular value decompositions randomized sampling single core multi core architectures. preprint arxiv. wang zhang improving matrix decomposition nyström approximation adaptive sampling. journal machine learning research zhou bian divide-and-conquer anchoring nearseparable nonnegative matrix factorization completion high dimensions data mining ieee international conference ieee.", "year": 2017}