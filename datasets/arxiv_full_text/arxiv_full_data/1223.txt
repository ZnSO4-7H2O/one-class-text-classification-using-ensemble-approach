{"title": "Creation of a Deep Convolutional Auto-Encoder in Caffe", "tag": ["cs.NE", "cs.CV", "cs.LG", "68Txx", "F.1.1; I.2.6; I.5.1"], "abstract": "The development of a deep (stacked) convolutional auto-encoder in the Caffe deep learning framework is presented in this paper. We describe simple principles which we used to create this model in Caffe. The proposed model of convolutional auto-encoder does not have pooling/unpooling layers yet. The results of our experimental research show comparable accuracy of dimensionality reduction in comparison with a classic auto-encoder on the example of MNIST dataset.", "text": "abstract development deep convolutional auto-encoder caffe deep learning framework presented paper. describe simple principles used create model caffe. proposed model convolutional autoencoder pooling/unpooling layers yet. results experimental research show comparable accuracy dimensionality reduction comparison classic auto-encoder example mnist dataset. convolutional auto-encoder wanted architectures deep learning research. auto-encoder based encoder-decoder paradigm input first transformed typically lower-dimensional space expanded reproduce initial data trained unsupervised fashion allowing extract generally useful features unlabeled data detect remove input redundancies present essential aspects analyzing data robust discriminative representations auto-encoders unsupervised learning methods widely used many scientific industrial applications solving mainly dimensionality reduction unsupervised pretraining tasks. compared architecture classic stacked auto-encoder better suited image processing tasks fully utilizes properties convolutional neural networks proven provide better results noisy shifted corrupted image data theoretical issues developments well described many research papers modern deep learning frameworks i.e. convnet theano+lasagne torch caffe others become popular tools deep learning research community since provide fast deployment state-of-the-art deep learning models along appropriate training strategies allowing rapid research commercial applications. interest apply deep learning technologies namely image processing neuroscience field. chosen caffe deep learning framework mainly reasons description deep neural network pretty straightforward several existing solutions/attempts develop research model different platforms best knowledge authors current implementation caffe yet. issue implementation permanently active caffe user group implementations one-layer convolutional restricted boltzmann machine matlab. mike swarbrick implementation theano/lasagne deep team dollar prize national data science bowl data science competition goal classify images plankton reported pretrain convolutional layers network approach final winning architecture since receive substantial improvement pre-training. also implementations examples torch deep learning framework recently implemented based torch recently implemented theano/keras examples neon deep learning framework goal paper present first results practical implementation model caffe deep learning framework well experimental research proposed model example mnist dataset simple visualization technique helped receive results. examples caffe models solve task dimensionality reduction. first classic stacked auto-encoder proposed hinton second siamese network proposed lecun classic auto-encoder model well researched trains purely unsupervised fashion. siamese network consists lenet architectures coupled siamese ended contrastive loss function. siamese network trains semi-supervised fashion since forming training label couple input images saving channels images belong class otherwise. visualizations example eliminated pooling-unpooling layers added non-linear activation function <sigmoid> case convolutional deconvolution layer noticed developed model similar classic auto-encoder model difference first fully-connected layers encoder part replaced convolutional layers last fully-connected layers decoder part replaced deconvolution layers. architecture developed model caffe depicted fig. taking account similarity classic auto-encoder developed model used following principles research model symmetric terms total size feature maps number neurons hidden layers encoder decoder parts. sizes numbers decrease layer layer encoder part increase decoder part similarly classic auto-encoder. sizes numbers less minimal values allowing handling size input data informational point view; similarly example classic auto-encoder caffe model used loss functions <sigmoid_cross_entropy_loss> <euclidean_loss>. experimental research shown loss functions separately provide good convergence results; visualization values trainable filters feature maps hidden units layer layer allows better understanding data converted/processed layer layer main purpose activation function convolutional/deconvolution layer non-linear nature data processing convolutional/deconvolution multiplication visualization showed huge rise convolutional/deconvolution operations encoder/decoder parts layer layer prevents model desirable convergence learning. activation functions drops resulting value feature maps interval kept values feature maps decoder part smaller order provide good convergence whole model. well-known fact good generalization properties neural networks depend ratio trainable parameters size dimension input data. therefore necessary perform experiments existing model classic auto-encoder order find better architecture provides better generalization properties. then similar size terms total size feature maps number neurons hidden layers could used create caffe implementations classic auto-encoder siamese network solve dimensionality reduction task encoding test mnist -dimensional space depicted fig. fig. respectively. ollowing success lenet architecture paper showed experimentally convolutional+pooling better representation convolutional filters furthermore classification results started construct scheme conv-pool-conv-pool encoder part deconv-unpool-deconv-unpool decoder part. also inspired work hyeonwoo used nonofficial caffe distribution implemented unpooling layer still absent official caffe distribution. model train previous experiments therefore eliminated pooling-unpooling layers consideration. masci showed convolutional architectures without max-pooling layers give worse results architectures without pooling appropriate unpooling layers definitely working architectures good point start first simpler working architecture increase complexity model. model good generalization properties. cannot compare classic auto-encoder basis number trainable parameters convolutional network size much fewer trainable parameters created architecture stable. since different runs neural network show different learning convergences depending random initialization weights/biases stable architecture mean convergence results within several runs model; practical implementation principles experimental results presented next section. experimental researches fulfilled workstation operated ubuntu operation system. workstation equipped -core inter xeon processor geforce gpu. fermi architecture cuda cores computing capability version caffe distributed used experiments. training presented models performed mode thus core device utilized caffe training. results experimental tests different sizes classic auto-encoder caffe examples presented table sure hinton presented best architecture science paper mentioned above wanted generalization properties smaller bigger architectures order experience create model. number trainable parameters architecture specified first column. table next tables calculations provided case dimensions. size mnist training dataset calculated examples elements elements. ratio mnist size number training parameters specified second column. trained architecture three fashions neurons last hidden layer encoder part corresponds -dimensional space encoding respectively. according hinton architectures called -dimensional auto-encoders. three runs architecture values loss functions separated presented training black test set. last column specified number classes evaluated visually three runs architecture. example number means classes total correctly classified. ‘correctly classified’ mean color class mixed color another class course subjective sentiment. learning parameters solver classic auto-encoder left specified caffe examples. parameters were solver_type base_lr lr_policy \"step\" gamma stepsize momentum weight_decay results table specified training iterations. training time architecture ----n--- third table minutes. table architectures bigger ratio data/ namely better generalization properties provide lower values loss functions training testing better visualization results. necessary note case classic fully-connected auto-encoder researched above number trainable parameters equal number connections. case since convolutional/deconvolution layers much fewer trainable parameters sizes convolutional deconvolution kernels trainable parameters case cae. therefore order build operated term size total number elements feature maps number neurons hidden layers encoder decoder parts. paper present architectures model more-or-less adjusted size best architecture classic auto-encoder model increased size series experimental tests model table contains architecture parameters developed models. models convolution kernels conv conv encoder part came back deconvolution kernels deconv deconv decoder part chosen size deconvolution kernels order restore size mnist image difference models conv/deconv layers number neurons fullyconnected layers encoder decoder parts. third column table proposed models practically symmetric terms total number elements feature maps number neurons hidden layers encoder decoder parts. number trainable parameters models details calculation specified last column table table respectively. since deconvolution operation nature convolution used approach calculate number trainable parameters encoder decoder parts. calculations easily checked calling caffe matlab using command <caffe;>. decoder part purpose deconvolution layer deconvneur corresponds term third column table transform feature maps last deconvolution layer deconv restored image size original pixels case mnist. explanation caffe user group tables proposed models practically symmetric terms total number elements feature maps number neurons hidden layers also terms number trainable parameters encoder decoder parts. comparison similarly-sized classic auto-encoder model developed shows times fewer trainable parameters. developed models presented table organized similarly table evaluate -dimensional caes number dimensions corresponds number neurons last hidden layer ipencode encoder part. results table specified training iterations. comparison tables shows model provides minimum values loss functions well number visualization classes best architecture classic autoencoder table experimental tests model showed better values loss functions reached training slightly better visualization results comparison model visualizations showing model solves dimensionality reduction task encoding test mnist space depicted figs. similarly used t-sne technique visualize dimensional data produced dimensional caes. cases i.e. research classical auto-encoder developed reformatted original mnist dataset format since data format perfectly supported matlab. learning parameters solver developed architecture were solver_type base_lr lr_policy \"fixed\" weight_decay several experiments changing architecture learning parameters stable architectures. tried different initializations weights biases presented results provided {type layers <weight_filler {type \"constant\"}> \"xavier\"}> convolutional/deconvolutional layers <weight_filler {type \"gaussian\" sparse fully-connected layers. also tried <relu> activation functions instead <sigmoid> surprisingly received worse results. architectures. training times model model running training iterations minutes respectively. quick reference collected learning parameters classic autoencoder developed table mentioned above experimental research visualized feature maps outputs hidden layer neurons order understand data processing inside cae. implementation visualization simple straightforward thanks matlab wrapper caffe. created .prototxt files corresponding number layers fig. training appropriate .caffemodel file call caffe matlab using .prototxt files argument. received values produced layer visualized. example showing model encodes decodes digit depicted fig. left upper picture original image pixels right bottom picture name <deconvneursig> restored image pixels. title picture contains following information digit visualized convolutional auto-encoders allow using desirable properties convolutional neural networks image data processing tasks working within unsupervised learning paradigm. results experimental research show comparable accuracy dimensionality reduction task compared classic auto-encoder example mnist dataset. creation convolutional autoencoder used well-known principles mentioned section above used many machine learning researchers every day. nevertheless believe approach research results presented paper help researchers general caffe user group particular create efficient deep neural network architectures future. application developed deep convolutional auto-encoder tasks neuroscience field creation architectures pooling/unpooling layers directions future research. would like thank caffe developers creating powerful framework deep machine learning research. thank hyeonwoo david silver discussions results presented paper eric chalmers editorial help robert sutherland help financial support. ranzato huang y.-l. boureau lecun unsupervised learning invariant feature hierarchies applications object recognition ieee conference computer vision pattern recognition vincent larochelle bengio p.a. manzagol extracting composing robust features denoising autoencoders proceedings international conference machine learning example digits mnist) picture number size appropriate layer internally represented caffe name appropriate layer corresponding names layers fig. also calculated minimum maximum values conv/deconv layer specified square brackets titles appropriate pictures. allowed understand failed experiments outputs deconv deconv layers saturated therefore pixels restored image value loss values training appropriate .prototxt files developed along matlab scripts used visualizations published caffe user group luczak’s web-page necessary note developed model working version caffe used/distributed model latest version seems newer versions caffe developers changed syntax layer descriptions layers layer layers’ types convolution convolution etc. fully-connected layers -dimensional array -dimensional previous version. deal issues necessary change syntax .prototxt files accordingly change dimensionality last fully-connected layer first deconvolution layer decoder part using <reshape> layer follows <layer {name \"reshape\" type \"reshape\" bottom \"ipdecode\" \"ipdecodesh\" reshape_param shape }}}>. hadsell chopra lecun dimensionality reduction learning invariant mapping ieee computer society conference computer vision pattern recognition changed/added references solutions appeared founded added references specify exact links .prototxt files published. corrected inaccuracy table calculation number trainable parameters. improved text several paragraphs including better explanation fig. tips developed newest version caffe.", "year": 2015}