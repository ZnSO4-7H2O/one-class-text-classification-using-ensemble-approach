{"title": "Acquiring Target Stacking Skills by Goal-Parameterized Deep  Reinforcement Learning", "tag": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "abstract": "Understanding physical phenomena is a key component of human intelligence and enables physical interaction with previously unseen environments. In this paper, we study how an artificial agent can autonomously acquire this intuition through interaction with the environment. We created a synthetic block stacking environment with physics simulation in which the agent can learn a policy end-to-end through trial and error. Thereby, we bypass to explicitly model physical knowledge within the policy. We are specifically interested in tasks that require the agent to reach a given goal state that may be different for every new trial. To this end, we propose a deep reinforcement learning framework that learns policies which are parametrized by a goal. We validated the model on a toy example navigating in a grid world with different target positions and in a block stacking task with different target structures of the final tower. In contrast to prior work, our policies show better generalization across different goals.", "text": "understanding physical phenomena component human intelligence enables physical interaction previously unseen environments. paper study artiﬁcial agent autonomously acquire intuition interaction environment. created synthetic block stacking environment physics simulation agent learn policy endto-end trial error. thereby bypass explicitly model physical knowledge within policy. speciﬁcally interested tasks require agent reach given goal state different every trial. propose deep reinforcement learning framework learns policies parametrized goal. validated model example navigating grid world different target positions block stacking task different target structures ﬁnal tower. contrast prior work policies show better generalization across different goals. understanding predicting physical phenomena daily life important component human intelligence. ability enables effortlessly manipulate objects unseen conditions. open question kind knowledge represented kind models could explain human manipulation behavior paper concerned question artiﬁcial agent autonomously acquire physical interaction skills trial error. recently researcher attempted build computational models capturing essence physical events machine learning methods sensory inputs little work investigate knowledge captured model directly applied manipulation. work learn block stacking trial-and-error bypassing explicitly model corresponding physics knowledge. purpose build synthetic environment physics simulation agent move stack blocks observe different outcomes actions. apply deep reinforcement learning directly acquire block stacking skill end-to-end fashion. previous work focuses learning policies ﬁxed task introduce goal-parameterized policies facilitate generalization learned skill different targets. study problem aforementioned block stacking task agent reproduce tower shown particular learn single model guide agent build different shapes request. generally intended conventional reinforcement learning formulations policy typically optimized reach speciﬁc goal. learning framework varying goals given agent input. ﬁrst validated extended model example agent navigate gridworld. both location start point randomized episode. observed good generalization performance. apply framework block stacking task. show execution depends desired target structure observe promising results generalization across different goals. humans possess amazing ability perceive understand ubiquitous physical phenomena occurring daily life. research psychology seeks understand ability develops. baillargeon suggest infants acquire knowledge physical events young observing events including support events others. interestingly recent work denil authors introduce basic tasks require learning agent estimate physical properties objects interactive simulated environment learn perform experiments strategically discover hidden properties analogy human’s development physics knowledge. battaglia proposes intuitive physics simulation engine internal mechanism type ability found close correlation behavior patterns human subjects’ several psychological tasks. recently increasing interest equipping artiﬁcial agents ability letting learn physical concepts visual data. mottaghi understanding dynamic events governed laws newtonian physics proto-typical motion scenarios exemplars. fragkiadaki analyze billiard table scenarios learn dynamics observation explicit object notion. alternative approach based boundary extrapolation bhattacharyya addresses similar settings without imposing object notion. aims understand physical properties objects based explicit physical simulation. mottaghi proposes reason containers behavior liquids inside single image. moreover lerer propose using visual model predict stability falling trajectories simple block scenes. investigate prediction performance image-based models changes trained block stacking scenes larger variety. examine human’s prediction adapts variation generated scenes compare learned visual model. work requires signiﬁcant amounts simulated physically-realistic data train large-capacity deep models. another interesting question explored psychology knowledge physical events affects guides human’s actual interaction objects yildirim clear machine model trained physics understanding directly applied real-world interactions object accomplish manipulation tasks. makes ﬁrst attempt along direction extending previous work stability classiﬁcation. task robot place wooden block existing structure maintaining stability. placement candidates ﬁrst generated evaluated visual stability classiﬁer predicted stable placements executed robot. paper reinforcement learning used learn end-to-end model directly experience collected interaction physically-realistic environment. majority work reinforcement learning focuses solving task single goal. however also tasks goal change every trial. obvious directly apply model learned towards speciﬁc goal different one. early idea proposed kaelbling maze navigation problem goal changes. author introduces analogous formulation q-learning using shortest path replacement value functions. major limitations framework formulated tabular form practical application complex states introduced shortest path speciﬁc maze navigation setting hence cannot easily adapt handle task like different targets stacking serving general solution type problem. contrast propose goal-parameterized model integrate goal information general learning-based framework facilitates generalization across different goals. model shown work navigation task target stacking. notably schaul also propose integrate goal information learning. however learn embedding state goal allow generalization reinforcement learning setup. process incorporated horde framework sutton agent learns towards different goals. work introduce dedicated embedding learning instead resort end-to-end approach function approximator learn direct mapping sensory observations actions allows generalization across different goals. introduce manipulation task target stacking. task image target structure made stacked blocks provided. given number blocks target structure goal reproduce structure shown image. manipulation primitives task include moving placing blocks. inspired scenario young children learn stack blocks different shapes given example structure. want explore artiﬁcial agent acquire skill trial error. task instance target structure generated image provided agent along number blocks. blocks ﬁxed orientation. sequence block orientations reproducing target feasible. agent attempts construct target structure placing blocks given sequence. spawning location block randomized along boundary environment. sample task instance shown figure goal-speciﬁc widely-used benchmark deep reinforcement learning algorithm atari games made popular mnih game collection large variety games deﬁned single goal speciﬁc goal enforced particular point time. example breakout player tries bounce many bricks possible. enduro player tries pass many cars possible simultaneously avoiding cars. instance unlikely optimal another task instance different target structure. contrast games type move likely work similar scenes. argument also applies research platforms richer visuals like vizdoom longer sequences target stacking requires looking ahead longer time horizon simultaneously ensure stability similarity target structure. different learning poke objective select motion primitive optimal next action. also different work reasons placement block. rich physics bounded besides stacking assigned target shape agent needs learn move block without colliding environment existing structure choose block’s placement wisely collapse current structure. agent prior knowledge this. needs learn everything scratch observing consequence actions. deep reinforcement learning agent requires learn larger number samples. enable this build simulated environment agent interact physical-realistic task instances. keep essential parts task current stage simulated environment remains abstraction real-world robotics scenario. generally requires integration multiple modules full-ﬂedged working system toussaint scope paper. detail simulated stacking environment implemented pandad bullet physics engine. block size follows ratio denote length width height respectively. ignore impact block placement focus resulting stability entire structure. block makes contact existing structure treated releasing block placement. episode moving block collides environment boundary existing structure terminate current episode. further block placement causes resulting structure collapse also episode. stability simulated similar comparing change displacement across blocks pre-selected small threshold within fraction time. blocks’ displacements threshold structure deemed stable otherwise unstable. simplify setting constrain action {left right down}. physics simulation runs however considering cost simulation contact moving block boundary existing structure. otherwise current block moving without actual physics simulation. reduce appearance difference caused varying perspective environment rendered using orthographic projection. figure shows example images. environment provides user-friendly python interface used test different reinforcement learning agents. time publication release implementation environment. major characteristic task requires goal-speciﬁc planning given similar states different objectives optimal move different. extend typical reinforcement learning formulation incorporate additional goal information. typical reinforcement learning setting agent interacts environment time observes state takes action receives reward transits state st+. common goal reinforcement learning agent maximize cumulative reward. commonly formalized form value function expected rewards γirt+i+|st actions taken respect policy state directly search optimal q-value function. recently incorporating deep neural network function approximator q-function shown impressive results across variety atari games. task apply deep network uses deep neural network approximating action-value function mapping input state action values. particular important improvements proposed mnih learning process including experience replay agent stores observed transitions memory buffer time uniformly samples memory update network target network agent maintains networks loss function current estimator function surrogate true function. current estimator parameters constantly updated. surrogate parameters updated every certain number steps current estimator network otherwise kept ﬁxed. shown figure contrast original model state action used estimate q-value model include current goal network produce estimate. call model goal-parametrized network note don’t apply frame-skipping technique used atari games allowing agent sees selects actions every frame last action repeated skipped frames. suit task particular moving block getting close existing structure simply repeating action decided previous frame cause unintended collision collapse. further explore reward shaping task providing prompt intermediate reward. types reward shaping included overlap ratio distance transform. overlap ratio state target overlap ratio counted ratio intersected foreground region target foreground region figure reward shaping used target stacking. overlap ratio target. gray area middle ﬁgure denotes intersected foreground region current target scene overlap ratio ratio areas two. distance distance transform target. middle ﬁgure denotes distance transform target shown left. distance current scene target distances masked current scene distance transform. evaluate proposed gdqn model navigation task target stacking compare base model integrate goal information. addition include result gdqn model different ways reward shaping target stacking task. example introduce type navigation task classic gridworld environment. locations starting point goal randomized episode. agent needs reach goal four possible actions {left right down}. action make agent grid leave stay location. episode terminates agent reaches goal. agent receive reward reaching current goal. different sizes gridworld tested training epoch size steps smaller gridworld larger test sizes agents epochs \u0001-greedy anneals linearly ﬁrst epochs ﬁxed thereafter. memory buffer size annealing length i.e. smaller gridworld buffer size equals length epochs training steps whereas larger buffer size steps. measure proportion episodes test epoch reaches goal shortest distance success ratio. results shown table best agents throughout training process. simple task relative small state space gets performance running average policy across goals addressing task contrast gdqn parametrized speciﬁcally include goal information achieves signiﬁcant better results sizes environment. groups target structures consisted different number blocks scene shown figure within group target shapes random target picked beginning individual episode. training epoch consists steps test epoch steps. similar setting example agents epochs anneals ﬁrst epochs memory buffer size long annealing steps steps. computed average overlap ratio success rate ﬁnished stacking episodes test epoch. overlap ratio deﬁned reward shaping equation simply measures scene assigned target scene. tells relative completion stacked structure comparison assigned target structure higher value better completion target. maximum suggests completely reproduction target. success rate counts ratio many episodes complete exact shape assigned total number episodes ﬁnished test epoch. absolute metric counting overall successful stacking. results shown table best agents throughout training process. groups metrics observe gdqn outperforms showing importance integrating goal information. general blocks task difﬁcult becomes. small number blocks scene single policy learned averages target shapes still work extent. however introducing blocks scene becomes difﬁcult averaged model handle. result already signiﬁcant decrease performance increasing blocks number whereas gdqn’s performance decreases slightly blocks scene longer reproduce single target gdqn parametrized speciﬁcally include goal information still though success rate basic gdqn relatively average overlap ratio still holds pretty well also reward shaping improves gdqn model particular distance transform boost performance overlap ratio. table results target stacking. gdqn denotes different ways reward shaping described previous section overlap ratio distance transform. metrics stands average overlap ratio average success rate. create synthetic block stacking environment physics simulation agent learn block stacking end-to-end trial error bypassing explicitly model corresponding physics knowledge. introduce target stacking task agent stacks blocks reproduces tower shown image. task presents distinct type challenge requiring agent reach given goal state different every trial. therefore propose goal-parametrized gdqn model plan respect speciﬁc goal allowing better generalization across different goals. validate model navigation task classic gridworld environment different start goal positions block stacking task different target structures. proposed model shows good performance tasks. references pulkit agrawal ashvin nair pieter abbeel jitendra malik sergey levine. learning poke poking experiential learning intuitive physics. advances neural information processing systems misha denil pulkit agrawal tejas kulkarni erez peter battaglia nando freitas. learning perform physics experiments deep reinforcement learning. arxiv preprint arxiv. michał kempka marek wydmuch grzegorz runc jakub toczek wojciech ja´skowski. vizdoom doom-based research platform visual reinforcement learning. computational intelligence games ieee conference ieee volodymyr mnih koray kavukcuoglu david silver alex graves ioannis antonoglou daan wierstra martin riedmiller. playing atari deep reinforcement learning. arxiv preprint arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature schaul daniel horgan karol gregor david silver. universal value function approximators. proceedings international conference machine learning richard sutton joseph modayil michael delp thomas degris patrick pilarski adam white doina precup. horde scalable real-time architecture learning knowledge unsupervised sensorimotor interaction. international conference autonomous agents multiagent systems-volume international foundation autonomous agents multiagent systems marc toussaint nils plath tobias lang nikolay jetchev. integrated motor control planning grasping high-level reasoning blocks world using probabilistic inference. robotics automation ieee international conference ieee ilker yildirim tobias gerstenberg basil saeed marc toussaint josh tenenbaum. physical problem solving joint planning symbolic geometric dynamic constraints. arxiv preprint arxiv.", "year": 2017}