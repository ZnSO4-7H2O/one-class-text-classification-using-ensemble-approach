{"title": "Distributed Representations of Sentences and Documents", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.", "text": "many machine learning algorithms require input represented ﬁxed-length feature vector. comes texts common ﬁxed-length features bag-of-words. despite popularity bag-of-words features major weaknesses lose ordering words also ignore semantics words. example powerful strong paris equally distant. paper propose paragraph vector unsupervised algorithm learns ﬁxed-length feature representations variable-length pieces texts sentences paragraphs documents. algorithm represents document dense vector trained predict words document. construction gives algorithm potential overcome weaknesses bag-ofwords models. empirical results show paragraph vectors outperform bag-of-words models well techniques text representations. finally achieve state-of-the-art results several text classiﬁcation sentiment analysis tasks. text classiﬁcation clustering play important role many applications document retrieval search spam ﬁltering. heart applications machine learning algorithms logistic regression kmeans. algorithms typically require text input represented ﬁxed-length vector. perhaps common ﬁxed-length vector representation texts bag-of-words bag-of-n-grams simplicity efﬁciency often surprising accuracy. tages. word order lost thus different sentences exactly representation long words used. even though bag-of-n-grams considers word order short context suffers data sparsity high dimensionality. bag-of-words bagof-n-grams little sense semantics words formally distances words. means words powerful strong paris equally distant despite fact semantically powerful closer strong paris. paper propose paragraph vector unsupervised framework learns continuous distributed vector representations pieces texts. texts variable-length ranging sentences documents. name paragraph vector emphasize fact method applied variable-length pieces texts anything phrase sentence large document. model vector representation trained useful predicting words paragraph. precisely concatenate paragraph vector several word vectors paragraph predict following word given context. word vectors paragraph vectors trained stochastic gradient descent backpropagation paragraph vectors unique among paragraphs word vectors shared. prediction time paragraph vectors inferred ﬁxing word vectors training paragraph vector convergence. technique inspired recent work learning vector representations words using neural networks formulation word represented vector concatenated averaged word vectors context resulting vector used predict words context. example neural network language model proposed uses concatenation several previous word vectors form input neural network tries predict next word. outcome model trained word vectors mapped vector space figure.a framework learning word vectors. context three words used predict fourth word input words mapped columns matrix predict output word. practice hierarchical softmax preferred softmax fast training. work structure hierarical softmax binary huffman tree short codes assigned frequent words. good speedup trick common words accessed quickly. binary huffman code hierarchy neural network based word vectors usually trained using stochastic gradient descent gradient obtained backpropagation type models commonly known neural particular implementation neural network based algorithm training word vectors available code.google.com/p/wordvec/ following successful techniques researchers tried extend models beyond word level achieve phrase-level sentence-level representations instance simple approach using weighted average words document. sophisticated approach combining word vectors order given parse tree sentence using matrix-vector operations approaches weaknesses. ﬁrst approach weighted averaging word vectors loses word order standard bag-of-words models second approach using parse tree combine word vectors shown work sentences relies parsing. paragraph vector capable constructing representations input sequences variable length. unlike previous approaches general applicable texts length sentences paragraphs documents. require task-speciﬁc tuning word weighting function rely parse trees. paper present experiments several benchmark datasets demonstrate advantages paragraph vector. example sentiment analysis task achieve stateof-the-art results better complex methods yielding relative improvement terms error rate. text classiﬁcation task method convincingly beats bag-of-words models giving relative improvement section introduces concept distributed vector representation words. well known framework learning word vectors shown figure task predict word given words context. framework every word mapped unique vector represented column matrix column indexed position word vocabulary. concatenation vectors used features prediction next word sentence. example powerful strong close other whereas powerful paris distant. difference word vectors also carry meaning. example word vectors used answer analogy questions using simple vector algebra king woman queen also possible learn linear matrix translate words phrases languages properties make word vectors attractive many natural language processing tasks language modeling natural language understanding statistical machine translation image understanding relational extraction approach learning paragraph vectors inspired methods learning word vectors. inspiration word vectors asked contribute prediction task next word sentence. despite fact word vectors initialized randomly eventually capture semantics indirect result prediction task. idea paragraph vectors similar manner. paragraph vectors also asked contribute prediction task next word given many contexts sampled paragraph. paragraph vector framework every paragraph mapped unique vector represented column matrix every word also mapped unique vector represented column matrix paragraph vector word vectors averaged concatenated predict next word context. experiments concatenation method combine vectors. paragraph token thought another word. acts memory remembers missing current context topic paragraph. reason often call model distributed memory model paragraph vectors contexts ﬁxed-length sampled sliding window paragraph. paragraph vector shared across contexts generated paragraph across paragraphs. word vector matrix however shared across paragraphs. i.e. vector powerful paragraphs. stochastic gradient descent gradient obtained backpropagation. every step stochastic gradient descent sample ﬁxed-length context random paragraph compute error gradient network figure gradient update parameters model. prediction time needs perform inference step compute paragraph vector paragraph. also obtained gradient descent. step parameters rest model word vectors softmax weights ﬁxed. suppose paragraphs corpus words vocabulary want learn paragraph vectors paragraph mapped dimensions word mapped dimensions model total parameters even though number parameters large large updates training typically sparse thus efﬁcient. figure.a framework learning paragraph vector. framework similar framework presented figure change additional paragraph token mapped vector matrix model concatenation average vector context three words used predict fourth word. paragraph vector represents missing information current context memory topic paragraph. trained paragraph vectors used features paragraph feed features directly conventional machine learning techniques logistic regression support vector machines k-means. summary algorithm stages training word vectors softmax weights paragraph vectors already seen paragraphs; inference stage paragraph vectors paragraphs adding columns gradient descending holding ﬁxed. make prediction particular labels using standard classiﬁer e.g. logistic regression. paragraph vectors also address weaknesses bag-of-words models. first inherit important property word vectors semantics words. space powerful closer strong paris. second advantage paragraph vectors take consideration word order least small context n-gram model large would important n-gram model preserves information paragraph including word order. said model perhaps better bag-of-n-grams model n-grams model would create high-dimensional representation tends generalize poorly. method considers concatenation paragraph vector word vectors predict next word text window. another ignore context words input force model predict words randomly sampled paragraph output. reality means iteration stochastic gradient descent sample text window sample random word text window form classiﬁcation task given paragraph vector. technique shown figure name version distributed words version paragraph vector opposed distributed memory version paragraph vector previous section. addition conceptually simple model requires store less data. need store softmax weights opposed softmax weights word vectors previous model. model also similar experiments paragraph vector combination vectors learned standard paragraph vector distributed memory learned paragraph vector distributed words pv-dm alone usually works well tasks combination pv-dbow usually consistent across many tasks therefore strongly recommended. perform experiments better understand behavior paragraph vectors. achieve this benchmark paragraph vector text understanding problems require ﬁxed-length vector representations paragraphs sentiment analysis information retrieval. sentiment analysis datasets stanford sentiment treebank dataset imdb dataset documents datasets differ signiﬁcantly lengths every example socher dataset single sentence every example maas dataset consists several sentences. dataset comes detailed labels sentences subphrases scale. achieve this socher used stanford parser parse sentence subphrases. subphrases labeled human annotators sentences labeled. labeled phrases dataset. dataset downloaded http//nlp.stanford.edu/sentiment/ tasks baselines authors propose ways benchmarking. first could consider -way ﬁne-grained classiﬁcation task labels {very negative negative neutral positive positive} -way coarse-grained classiﬁcation task labels {negative positive}. axis variation terms whether label entire sentence phrases sentence. work consider labeling full sentences. socher apply several methods dataset recursive neural tensor network works much better bag-of-words model. argued movie reviews often short compositionality plays important role deciding whether review positive negative well similarity words given rather tiny size training set. experimental protocols follow experimental protocols described make available labeled data model subphrase treated independent sentence learn representations subphrases training set. test time freeze vector representation word learn representations sentences using gradient descent. vector representations test sentences learned feed logistic regression predict movie rating. experiments cross validate window size using validation optimal window size vector presented classiﬁer concatenation vectors pv-dbow pv-dm. pv-dbow learned vector representations dimensions. pv-dm learned vector representations dimensions words paragraphs. predict word concatenate paragraph vectors word vectors. special characters treated normal word. paragraph less words pre-pad special null word symbol. results report error rates different methods table ﬁrst highlight table bag-ofwords bag-of-n-grams models perform poorly. simply averaging word vectors improve results. bag-of-words models consider sentence composed therefore fail recognize many sophisticated linguistic phenomena instance sarcasm. results also show na¨ıve bayes svms bigram na¨ıve bayes word vector averaging recursive neural network matrix vector-rnn recursive neural tensor network paragraph vector method performs better baselines e.g. recursive networks despite fact require parsing. coarse-grained classiﬁcation task method absolute improvement terms error rates. translates relative improvement. previous techniques work sentences paragraphs/documents several sentences. instance recursive neural tensor network based parsing sentence unclear combine representations many sentences. techniques therefore restricted work sentences paragraphs documents. method require parsing thus produce representation long document consisting many sentences. advantage makes method general approaches. following experiment imdb dataset demonstrates advantage. dataset imdb dataset ﬁrst proposed maas benchmark sentiment analysis. dataset consists movie reviews taken imdb. aspect dataset movie review several sentences. labeled training instances labeled test instances unlabeled training instances. types labels positive negative. labels balanced training test set. dataset downloaded http//ai.stanford.edu/ amaas/data/sentiment/index.html experimental protocols learn word vectors paragraph vectors using training documents paragraph vectors labeled instances neural network hidden layer units logistic classiﬁer learn predict sentiment. test time given test sentence freeze rest network learn paragraph vectors test reviews gradient descent. vectors learned feed neural network predict sentiment reviews. hyperparameters paragraph vector model selected manner previous task. particular cross validate window size optimal window size words. vector presented classiﬁer concatenation vectors pvdbow pv-dm. pv-dbow learned vector representations dimensions. pv-dm learned vector representations dimensions words documents. predict word concatenate paragraph vectors word vectors. special characters treated normal word. document less words pre-pad special null word symbol. results results paragraph vector baselines reported table seen table long documents bag-of-words models perform quite well difﬁcult improve upon using word vectors. signiﬁcant improvement happened work combine restricted boltzmann machines model bag-ofwords. combination models yields improvement approximately terms error rates. another signiﬁcant improvement comes work among many variations tried nbsvm bigram features works best yields considerable improvement terms error rate. here dataset paragraphs ﬁrst results returned search engine given popular queries. paragraphs also known snippet summarizes content page page matches query. collection derive dataset test vector representations paragraphs. query create triplet paragraphs paragraphs results query whereas third paragraph randomly sampled paragraph rest collection goal identify three paragraphs results query. achieve this paragraph vectors compute distances paragraphs. better representation achieves small distance pairs paragraphs query larg distance pairs paragraphs different queries. benchmark four methods compute features paragraphs bag-of-words bag-of-bigrams averaging word vectors paragraph vector. improve bag-of-bigrams also learn weighting matrix distance between ﬁrst paragraphs minimized whereas distance ﬁrst third paragraph maximized record number times method produces smaller distance ﬁrst paragraphs ﬁrst third paragraph. error made method produce desirable distance metric triplet paragraphs. results paragraph vector baselines reported table task tf-idf weighting performs better counts therefore report results methods tf-idf weighting. results show paragraph vector works well gives relative improvement terms error rate. fact paragraph vector method signiﬁcantly outperforms words bigrams suggests proposed method useful capturing semantics input text. table.the performance paragraph vector bag-of-words models information retrieval task. weighted bag-ofbigrams method learn linear matrix tfidf bigram features maximizes distance ﬁrst third paragraph minimizes distance ﬁrst second paragraph. pv-dm consistently better pv-dbow. pvdm alone achieve results close many results paper example imdb pv-dm achieves combination pv-dm pv-dbow often work consistently better therefore recommended. paragraph vector expensive done parallel test time. average implementation takes minutes compute paragraph vectors imdb test using core machine distributed representations words ﬁrst proposed become successful paradigm especially statistical language modeling word vectors used applications word representation named entity recognition word sense disambiguation parsing tagging machine translation distributed representations phrases sentences also focus socher methods typically require parsing shown work sentence-level representations. obvious extend methods beyond single sentences. methods also supervised thus require labeled data work well. paragraph vector approach computing paragraph vectors gradient descent bears resemblance successful paradigm computer vision known fisher kernels basic construction fisher kernels gradient vector unsupervised generative model. described paragraph vector unsupervised learning algorithm learns vector representations variablelength pieces texts sentences documents. vector representations learned predict surrounding words contexts sampled paragraph. experiments several text classiﬁcation tasks stanford treebank imdb sentiment analysis datasets show method competitive state-of-the-art methods. good performance demonstrates merits paragraph vector capturing semantics paragraphs. fact paragraph vectors potential overcome many weaknesses bag-of-words models. although focus work represent texts method applied learn representations sequential data. non-text domains parsing available expect paragraph vector strong alternative bag-of-words bag-of-n-grams models.", "year": 2014}