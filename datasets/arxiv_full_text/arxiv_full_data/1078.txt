{"title": "Diagonal RNNs in Symbolic Music Modeling", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "In this paper, we propose a new Recurrent Neural Network (RNN) architecture. The novelty is simple: We use diagonal recurrent matrices instead of full. This results in better test likelihood and faster convergence compared to regular full RNNs in most of our experiments. We show the benefits of using diagonal recurrent matrices with popularly used LSTM and GRU architectures as well as with the vanilla RNN architecture, on four standard symbolic music datasets.", "text": "paper propose recurrent neural network architecture. novelty simple diagonal recurrent matrices instead full. results better test likelihood faster convergence compared regular full rnns experiments. show beneﬁts using diagonal recurrent matrices popularly used lstm architectures well vanilla architecture four standard symbolic music datasets. recent resurgence neural networks recurrent neural networks utilized variety sequence learning applications great success. examples include language modeling machine translation handwriting recognition speech recognition symbolic music modeling paper empirically show symbolic music modeling using diagonal recurrent matrix rnns results signiﬁcant improvement terms convergence speed test likelihood. inspiration idea comes multivariate gaussian mixture models gaussian mixture modeling known using diagonal covariance matrix often results better generalization performance increased numerical stability reduced computational complexity adapt idea rnns using diagonal recurrent matrices. investigate consequences using diagonal recurrent matrices vanilla rnns popular long short term memory networks gated recurrent units empirically observe using diagonal recurrent matrices results improvement convergence speed training resulting test likelihood three models four standard symbolic music modeling datasets. hidden state vector hidden units input vector time rk×l input matrix transforms input dimensional space rk×k recurrent matrix transforms previous state. finally bias vector. note that practice recursion either followed output stage outputs another recursion obtain multi-layer recurrent neural network. hidden layer non-linearity usually chosen hyperbolic tangent. choice output nonlinearity dependent application typically softmax sigmoid function. despite simplicity original form usually preferred practice well known gradient vanishing problem people often involved architectures lstms grus alleviate vanishing gradient issue using gates ﬁlter information enable modeling long-term dependencies. denotes element-wise product sigmoid function forget gate write gate zeros vector current state depends solely candidate vector extreme ones vector state recursion forms mapping state function past inputs current input. intuition recurrent matrix interacts inputs functionally temporarily ignore non-linearities although equation sacriﬁces generality gives notion matrix effects overall transformation input transformation matrix inputs transformed multiple application matrices exponentiated matrices weights inputs. question weights applied are? input transformations sensible since want project inputs dimensional space. transformations recurrent weights rather arbitrary multiple plausible forms straightforward alternative recursion equation considering linear transformations diagonal scalar constant alternatives recurrent matrix similar different cases gaussian covariance matrices paper explore diagonal alternative full matrices. carried unchanged similarly determines much contributes candidate state notice ones vector zeros vector architecture reduces vrnn architecture equation finally note omitted biases equations reduce notation clutter. omit bias terms also rest paper. lstm network much related network above. addition gates output gate control output forget gate decoupled gates blend previous state candidate state also notice application tangent hyperbolic yielding output. prevents output assuming values large magnitudes. experimentally shown output non-linearity crucial lstm performance. time recurrent term length vector instead matrix. note element wise multiplying previous state vector equivalent matrix-vector multiplication wdiaght− wdiag diagonal matrix diagonal entries vector hence name diagonal rnns. involved lstm architectures also modify recurrent matrices gates. results following network architecture last thing note using diagonal matrix completely eliminate ability neural network model inter-dimensional correlations since projection matrix gets applied input furthermore networks typically dense output layer. noted aforementioned used perframe negative log-likelihood measure evaluate models. negative log-likelihood essentially crossentropy predictions ground truth. frame negative log-likelihood given following expression trained vrnns lstms grus full symbolic midi diagonal music datasets. http//www-etud.iro.umontreal.ca/˜boulanni/icml originally used paper learning goal predict next frame given sequence using past frames. datasets divided training test validation sets. performance measured per-frame negative log-likelihood sequences test set. datasets ordered increasing size chorales piano-midi nottingham musedata. apply transposition center datasets around center optional preprocessing indicated used provided piano roll sequences provided aforementioned converted binary masks entry note played corresponding pitch time. also eliminated pitch bins activity given dataset. large size experiments limited maximum sequence length take advantage parallelization noticed operation alter results signiﬁcantly. randomly sampled hyper-parameter conﬁgurations model dataset optimizer. report test accuracies conﬁgurations ranked according performance validation set. random hyper-parameter conﬁguration trained given model iterations. experiments different optimizers. overall different models different datasets different optimizers means obtained training runs iterations each. trained models nvidia tesla gpus. optimizers used adam optimizer default parameters speciﬁed corresponding paper rmsprop used sigmoid output layer models. used mild dropout accordance keep probability input output layers. used xavier initialization cases. sampled hyper-parameters corresponding ranges follows figures show training iterations negative test log-likelihoods hyperparameter conﬁgurations chorales piano-midi nottingham musedata datasets respectively. show negative log-likelihoods obtained test respect training iterations hyper-parameter conﬁgurations ranked validation according performance attained last iteration. rows show training iterations adam optimizer bottom rows show iterations rmsprop optimizer. curves show negative log-likelihood averaged conﬁgurations cyan curves full model black curves diagonal models. violin plots show distribution test negative log-likelihoods conﬁgurations. also show average number parameters used models corresponding conﬁgurations legends ﬁgures. minimum negative log-likelihood values obtained model using adam rmsprop optimizers summarized table using diagonal recurrent matrices results improvement test likelihoods almost cases explored paper. beneﬁts extremely pronounced adam optimizer rmsprop optimizer also improvements training speed ﬁnal test likelihoods. fact figure training iterations test negative log-likelihoods chorales dataset full diagonal models. adam optimizer bottom rmsprop. black curves diagonal models cyan curves full models. left column vrnn middle column lstm right column gru. legends show average number parameters used models caption also applies figures corresponding datasets. modiﬁcation results improvement three different models different optimizers strongly suggests using diagonal recurrent matrices suitable modeling symbolic music datasets potentially useful applications. except nottingham dataset using diagonal recurrent matrix results improvement ﬁnal test likelihood cases. although ﬁnal negative likelihoods nottingham dataset larger diagonal models still improvement training speed cases black curves cyan curves part. average number parameters utilized diagonal models cases smaller full models cases observe diagonal models achieve comparable performance using fewer parameters. glorot bengio understanding difﬁculty training deep feedforward neural networks proceedings thirteenth international conference intelligence statistics aistats artiﬁcial chia laguna resort sardinia italy available http//www.jmlr.org/proceedings/papers/v/glorota.html sutskever vinyals sequence sequence learning neural networks proceedings international conference neural information processing systems ser. nips’. cambridge press boulanger-lewandowski bengio vincent modeling temporal dependencies high-dimensional sequences application polyphonic music generation transcription. icml pascanu mikolov bengio difﬁculty training recurrent neural networks proceedings international conference machine learning atlanta june available http//jmlr.org/proceedings/papers/v/pascanu.html", "year": 2017}