{"title": "PMLB: A Large Benchmark Suite for Machine Learning Evaluation and  Comparison", "tag": ["cs.LG", "cs.AI"], "abstract": "The selection, development, or comparison of machine learning methods in data mining can be a difficult task based on the target problem and goals of a particular study. Numerous publicly available real-world and simulated benchmark datasets have emerged from different sources, but their organization and adoption as standards have been inconsistent. As such, selecting and curating specific benchmarks remains an unnecessary burden on machine learning practitioners and data scientists. The present study introduces an accessible, curated, and developing public benchmark resource to facilitate identification of the strengths and weaknesses of different machine learning methodologies. We compare meta-features among the current set of benchmark datasets in this resource to characterize the diversity of available data. Finally, we apply a number of established machine learning methods to the entire benchmark suite and analyze how datasets and algorithms cluster in terms of performance. This work is an important first step towards understanding the limitations of popular benchmarking suites and developing a resource that connects existing benchmarking standards to more diverse and efficient standards in the future.", "text": "selection development comparison machine learning methods data mining diﬃcult task based target problem goals particular study. numerous publicly available real-world simulated benchmark datasets emerged diﬀerent sources organization adoption standards inconsistent. such selecting curating speciﬁc benchmarks remains unnecessary burden machine learning practitioners data scientists. present study introduces accessible curated developing public benchmark resource facilitate identiﬁcation strengths weaknesses diﬀerent machine learning methodologies. compare meta-features among current benchmark datasets resource characterize diversity available data. finally apply number established machine learning methods entire benchmark suite analyze datasets algorithms cluster terms performance. work important ﬁrst step towards understanding limitations popular benchmarking suites developing resource connects existing benchmarking standards diverse eﬃcient standards future. term benchmarking used machine learning refer evaluation comparison methods regarding ability learn patterns ‘benchmark’ datasets applied ‘standards’. benchmarking could thought simply sanity check conﬁrm method successfully runs expected reliably simple patterns existing methods known identify hastie rigorous view benchmarking approach identify respective strengths weaknesses given methodology contrast others caruana niculescumizil comparisons could made range evaluation metrics e.g. power detect signal prediction accuracy computational complexity model interpretability. approach benchmarking would important demonstrating methodological abilities simply guide selection appropriate method given problem. benchmark datasets typically take three forms. ﬁrst accessible well-studied real-world data taken diﬀerent real-world problem domains interest. second simulated data data artiﬁcially generated often ‘look’ like real-world data known underlying patterns. example gametes genetic-data simulation software generates epistatic patterns association ‘mock’ single nucleotide polymorphism data urbanowicz third form data deﬁne data also artiﬁcially generated known embedded pattern without emphasis representing real-world data e.g. parity multiplexer problems blum koza worth noting term ‘toy dataset’ often used describe small simple dataset examples included algorithm software. benchmark repositories datasets emerged popular others still lacks central comprehensive concise benchmark datasets accentuate strengths weaknesses established methods. individual studies often restrict benchmarking eﬀorts various reasons example based comparing variants algorithm interest. genetic programming community also previously discussed appropriate benchmarking comparing methodologies o’neill mcdermott white benchmarking eﬀorts focus speciﬁc application interest e.g. traﬃc sign detection stallkamp narrowly deﬁned problem type e.g. classiﬁcation -way epistatic interactions moore scope benchmarking also limited practical computational requirements. currently number challenges make diﬃcult benchmark methods useful globally accepted manner. overwhelming number publications reference benchmark datasets however surprisingly publications discuss topic appropriate benchmarking general. additionally collecting curating real-world benchmark datasets remains challenge many researchers. although repositories repository lichman kaggle goldbloom provide dozens real-world datasets download free datasets come myriad formats require considerable preprocessing methods applied them. result many benchmark datasets unused simply diﬃcult preprocess. further real-world benchmarks derived many diﬀerent problem domains strict data science perspective many benchmarks repositories similar meta-features representative diﬀerent real-world problems represent diverse assembly data science problems. issue raised previously; applying datasets benchmarks noted scope included datasets limited method evaluation suggested repositories expanded segal another challenge benchmarking researchers often handful datasets evaluating methods make diﬃcult properly compare method state-of-the-art methods. example datasets handpicked highlight strengths proposed method failing demonstrate proposed method’s potential weaknesses. result although method perform well handful datasets fail generalize broader range problems. submit important clearly identify limitations algorithm benchmarking practices something often overlooked. always need identify generate custom benchmarks specialized problem domains e.g. physical activity monitoring data reiss stricker dynamical systems simulation cava vital bioinformatics community comprehensive benchmark suite compare contrast methods. towards goal present study introduces penn machine learning benchmark publicly available dataset suite initialized real-world simulated benchmark datasets evaluating supervised classiﬁcation methods. pmlb includes datasets many most-used benchmark suites keel alcal´a repository lichman addition collecting data resources pmlb standardizes format data provides useful interfaces fetching datasets directly web. initial pmlb repository meant comprehensive; includes mainly real-world datasets excludes regression datasets well datasets missing values. chosen focus initial assessment available datasets classiﬁcation. paper includes highlevel analysis properties founding pmlb datasets feature counts class imbalance etc. further evaluate performance standard statistical methods scikit-learn pedregosa full pmlb datasets. assess diversity benchmark datasets perspective meta-features well based predictive performance methods applied. beyond introducing simpliﬁed resource benchmarks study designed provide insight limitations currently utilized benchmarks direct expansion curation future improved pmlb dataset suite eﬃciently comprehensivly allows comparison methods. work provides important ﬁrst step assembly eﬀective diverse benchmarking standards integrating real-world simulated datasets generalized evaluation comparison. compiled penn machine learning benchmark datasets wide range existing benchmark suites including repository lichman kaggle goldbloom keel alcal´a meta-learning benchmark reif such pmlb includes real-world benchmark datasets commonly used benchmarking studies. instance data row. categorical features labels non-numerical encodings replaced numerical equivalents additionally every dataset dependent variable column labeled class. finally benchmark datasets missing data excluded pmlb many algorithms cannot handle missing data standard implementations wished avoid imposing particular data imputation method initial study. discriminating categorical ordinal features continuous features determined automatically based whether variable considered ‘ﬂoat’ pandas dataframe pan. corresponds perfectly balanced classes value approaching corresponds extreme class imbalance i.e. nearly instances class value. imbalance calculated measuring squared distance class’s instance proportion perfect balance dataset datasets instances features fairly balanced class distribution. roughly half datasets binary classiﬁcation problems whereas remaining half multiclass classiﬁcation problems ranging classes. datasets datasets discrete continuous features include binary features contain continuous features. worth noting pmlb datasets cover broad range application areas including biomedical studies signal processing image classiﬁcation among others. make pmlb datasets easier access published open source python interface pmlb pypi. interface provides simple fetch data function returns dataset pmlb pandas dataframe. example fetch clean dataset clean data variable contain data frame clean dataset class column corresponds class labels remaining columns features. fetch data function several caching preprocessing options documented pmlb repository. gaussian na¨ıve bayes bernoulli na¨ıve bayes multinomial na¨ıve bayes logistic regression linear classiﬁer trained stochastic gradient descent support vector classiﬁer linear polynomial sigmoid kernel passive aggressive classiﬁer k-nearest neighbor decision tree random forest extra random forest adaboost gradient tree boosting methods evaluated using balanced accuracy velez urbanowicz moore scoring metric normalized version accuracy accounts class imbalance calculating accuracy per-class basis averaging per-class accuracies. information methods hastie scikit-learn documentation pedregosa evaluated method ﬁrst scaled features datasets subtracting mean scaling features unit variance. scaling step necessary methods k-nearest neighbor classiﬁer assumes datasets scaled appropriately beforehand. method data set. process resulted total million evaluations methods data sets. comprehensive parameter search used expert knowledge methods decide parameters parameter values evaluate. complete code running experiment available online. noted diﬀerent number parameters algorithm every algorithm number evaluations. order characterize datasets pmlb clustered based metafeatures section analyze datasets based performance section identiﬁes datasets solved high accuracy well datasets appear universally easy hard diﬀerent algorithms model accurately versus ones appear particularly useful highlighting diﬀerential algorithm performance. used k-means cluster normalized meta-features datasets clusters visualized along ﬁrst principal component axes figure number clusters chosen compromise interpretability clusters adequate separation clustered datasets deﬁned silhouette score. figure includes clusters centered outlier datasets clusters compared detail according mean values dataset meta-features cluster figure clusters contain datasets separated endpoint type i.e. cluster comprised binary classiﬁcation problems whereas cluster comprised multiclass classiﬁcation problems. cluster made datasets relatively high numbers features cluster contains datasets high imbalance classes data set. finally cluster reserved dataset exceptionally high number instances clustering analysis thus reﬂects fairly intuitive ways challenges presented particular dataset categorized namely large numbers instances large numbers features high class imbalance binary versus multiclass classiﬁcation. figure summarizes results biclustering balanced accuracy tuned models according method dataset using spectral biclustering algorithm kluger methods datasets grouped contiguous biclusters order expose relationships models datasets. figure presents balanced accuracy. figure preserves clustering presents deviation mean balanced accuracy among figure biclustering models datasets according balanced accuracy models using best parameter settings. deviation mean balanced accuracy across models. highlights datasets upon methods performed similarly versus certain methods performed better worse others. identiﬁes boundaries contiguous biclusters identiﬁed based ml-wise clusters data-wise clusters. methods order clearly identify datasets upon methods perform similarly methods performed better others. figure simply delineates identiﬁed biclusters deﬁned balanced accuracy biclustering figure preserved interesting note methods tend group according underlying approach; example gaussian multinomial na¨ıve bayes methods cluster together logistic regression passive aggressive cluster together tree-based methods decision tree extra trees random forest also form separate cluster. datasets derived origin observed cluster certain instances. example dataset cluster contains gametes data sets; cluster contains mfeats datasets breast cancer datasets; cluster includes wine quality datasets several thyroid-related datasets figure allows interpret utility certain datasets terms diﬃculty across methods across classes methods. example light-blue stripes balanced accuracy indicate none models achieve good performance datasets correspond gametes epistasis datasets known diﬃcult lack univariate correlations features classes high amount noise. contrast nearly every method solves dataset high degree accuracy simple linear correlations features classes noise. clusters datasets methods reveal contrasts performance. dataset cluster cluster contain single dataset parity problem corresponding dataset figure unique problem method must able quantify whether number features given binary value even order correctly classify instance. result methods consider main eﬀect features independently able solve contrast methods high capacity interactions features well contrast also seen cluster contains several datasets strong interactions features observe contrast methods make assumptions linear independence across cluster datasets. contrasting figure helps diﬀerentiate diﬀerences overperformance given datasets diﬀerences performance based selected methodology. important observation reasonably large proportion benchmarks included study yielded similar performance spectrum methods applied. likely signals identiﬁed datasets either universally easy diﬃcult detect. furthermore datasets variable performance observed often group datasets clustered together similar signature better average/worse average performance overall current suite datasets span reasonable range diﬃculty tested approaches. figure shows distribution scores tuned method dataset suite sorted best balanced accuracy score achieved method. left-most dataset corresponds clean mentioned above right-most analcatdata dmft maximum accuracy score methods tested. approximately half current suite classiﬁed balanced accuracy higher nearly datasets classiﬁed balanced accuracy higher. thus although range model ﬁdelity observed datasets biased towards problems solved higher balanced accuracy. primary goal paper introduce ongoing research project benchmarking methods. speciﬁcally collected curated datasets popular data repositories introduced pmlb evolving benchmark standards comparing evaluating diﬀerent methods. apart repository itself conducted comprehensive analysis performance numerous standard methods used baseline evaluating comparing newly developed methods assessed diversity existing benchmark datasets identify shortcomings addressed subsequent addition benchmarks future release. simplicity diversity ultimate priorities pmlb suite. motivated clean standardize presentation datasets repository develop simple interface fetching data include datasets multiple sources. interestingly analyzed meta-features datasets pmlb found datasets fall handful categories based feature types class imbalance dimensionality numbers classes. also found biclustering performance diﬀerent algorithms datasets could observe classes problems algorithms work well poorly conjunction. course pmlb fully comprehensive benchmark suite supervised classiﬁcation methods. instance currently excludes datasets missing values regression tasks pmlb handful highly imbalanced datasets. approach adding diversity pursued keel repository augment existing datasets simulated missingness noise. however propose avoid adding multiple variants dataset instead identify simulate datasets varying properties meta-features expand pmlb suite gaps underrepresented problem types data science perspective. present study plan performance comparisons diversity methods order identify limited benchmark standards able diversely identify methodological advantages disadvantages. expect future work lead comprehensive benchmark tool better researchers discovering strengths weaknesses methods ultimately lead thorough—and honest—comparisons methods. thank andreas m¨uller valuable input development project. also thank penn medicine academic computing services computing resources. work supported national institutes health grants warren center network data science.", "year": 2017}