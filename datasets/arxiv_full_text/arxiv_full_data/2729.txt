{"title": "A Framework for Generalizing Graph-based Representation Learning Methods", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.SI"], "abstract": "Random walks are at the heart of many existing deep learning algorithms for graph data. However, such algorithms have many limitations that arise from the use of random walks, e.g., the features resulting from these methods are unable to transfer to new nodes and graphs as they are tied to node identity. In this work, we introduce the notion of attributed random walks which serves as a basis for generalizing existing methods such as DeepWalk, node2vec, and many others that leverage random walks. Our proposed framework enables these methods to be more widely applicable for both transductive and inductive learning as well as for use on graphs with attributes (if available). This is achieved by learning functions that generalize to new nodes and graphs. We show that our proposed framework is effective with an average AUC improvement of 16.1% while requiring on average 853 times less space than existing methods on a variety of graphs from several domains.", "text": "random walks heart many existing deep learning algorithms graph data. however algorithms many limitations arise random walks e.g. features resulting methods unable transfer nodes graphs tied node identity. work introduce notion attributed random walks serves basis generalizing existing methods deepwalk nodevec many others leverage random walks. proposed framework enables methods widely applicable transductive inductive learning well graphs attributes achieved learning functions generalize nodes graphs. show proposed framework effective average improvement requiring average times less space existing methods variety graphs several domains. keywords random walk representation learning inductive learning deep learning attributed graphs graphs ubiquitous allow model entities dependencies between them. graph data often observed directly natural world constructed non-relational data deriving metric space entities retaining signiﬁcant edges learning useful feature representation graph data lies heart success many machine learning tasks node classiﬁcation anomaly detection link prediction dynamic network analysis community detection role discovery visualization sensemaking graph classiﬁcation network alignment many existing techniques random walks basis learning features estimating parameters graph model downstream prediction task. examples include recent node embedding methods deepwalk copyright association advancement artiﬁcial intelligence rights reserved. nodevec well graph-based deep learning algorithms. however simple random walk used methods fundamentally tied identity node. three main disadvantages. first approaches inherently transductive generalize unseen nodes graphs. furthermore unable used graph-based transfer learning tasks acrossnetwork classiﬁcation graph similarity matching second space-efﬁcient feature vector learned node impractical large graphs. third approaches lack support attributed graphs. includes graphs intrinsic attributes gender well structural features higher-order subgraph counts e.g. number -cliques node participates. make methods generally applicable introduce reinterpretation notion random walk tied node identity instead based function maps node attribute vector type. using reinterpretation basis propose framework naturally generalizes many existing methods. framework provides number important advantages. first learned features generalize nodes across graphs therefore used transfer learning tasks across-network link prediction classiﬁcation. proposed approach supports transductive inductive learning. second generalized approach inherently space-efﬁcient since embeddings learned types therefore requires significantly less space existing methods. third generalized approach supports learning attributed graphs. however stress approach require graphs input attributes since derived graph structure. furthermore approach shown effective average improvement requiring average less space existing methods variety graphs. contributions paper proposes framework generalizing many existing algorithms making broadly applicable attributed graphs inductive learning. following properties space-efﬁcient requires average less space function mapping nodes types based attribute matrix function learned automatically deﬁned manually user. framework general ﬂexible arbitrary function maps nodes types based attribute matrix graphs attributed simply derive extracting graph features based structure graph types deﬁned function maps nodes types nodevec embedding deep graph algorithms notice node mapped unique identiﬁer identiﬁes speciﬁc node used random walk. however attributed random walk framework nodes type. binary operator concatenation among others. second class functions learned solving objective function. includes functions based low-rank factorization matrix form fuvt factor matrices rn×f rk×f rank linear non-linear function. formally section formally introduce attributed random walk framework serve basis generalizing many existing deep graph models embedding methods. framework consists general components function maps nodes types based matrix attributes. function deﬁned learned automatically. note given input computed based structure graph. details section function mapping nodes types given directed graph matrix attributes ﬁrst component framework maps nodes types often much smaller i.e. formally length random walk neighbors random walks function maps attribute vector corresponding type types type assigned nodes attribute matrix rows represent nodes columns represent attributes k-dimensional attribute vector node n-dimensional vector attribute transformation hyperparameter matrix type embeddings d-dimensional embedding type figure simple illustrative example. example shows potential instantiation general framework. particular step computes number triangles -stars node participates whereas step transforms individual attribute using logarithmic binning. next step derives types using simple function representing concatenation node’s attribute values resulting types among nodes. finally step derives attributed random walks used learn embeddings. section discussion. observe gives mapping latent feature space input attribute space represents attributes latent features therefore tied particular nodes generally attributes. many methods avoid issue selecting appropriate subset attributes apriori robust selection different sets attributes compared simple functions aside straightforward formulate function learning problem automate choice function framework naturally supports attributed non-attributed graphs attribute matrix derived extracting graph features based structure graph learned automatically using inductive feature learning approach deepgl attributed random walks existing deep learning models embedding algorithms graphs simple random walks based node identity features learned methods fundamentally tied identity node therefore unable generalize nodes well graph-based transfer learning tasks across-network learning among limitations. section introduce notion attributed random walks demonstrate serve basis generalizing many existing deep graph models embedding algorithms. deﬁnition walk length deﬁned sequence indices note nodes walk necessarily distinct. replace traditional notion walk appropriate useful notion attributed walk deﬁned deﬁnition k-dimensional vector node attributed walk length deﬁned sequence adjacent node types associated sequence indices function maps input vector node corresponding type type sequence simply node types occur walk. framework allows uniform non-uniform attributed random walks. also straightforward bias attributed random walk arbitrary fashion. notion attributed walk extended various ways given arbitrary embedding method deep graph model uses simple random walks based node identity generalize replacing traditional notion random walk proposed notion attributed random walks. show existing methods actually special case proposed framework uniquely identiﬁes node suppose graph nodes mapped types using arbitrary function select recover original method special case framework. intuitively node must assigned unique type uniquely identiﬁes node since observe actually extreme case framework corresponds exactly original method three advantages arise existing methods generalized proposed framework. first generalized method naturally supports attributed graphs. second features learned generalize nodes across graphs therefore naturally inductive able used transfer learning tasks. finally learned embeddings signiﬁcantly space-efﬁcient space complexity compared previous methods require space embedding dimensionality. emphasize proposed framework general ﬂexible; example discussed section represents simple instantiation framework. overview steps involved speciﬁc instantiation shown figure summarized succinctly follows step compute and/or select attributes given directed graph ﬁrst step compute attributes using graph structure step transform attributes next transform attribute vector. goal values individual attribute vector smaller values figure attribute transformed using logarithmic binning. convenience initial replaced transformed attribute. step attributed random walks finally generate attributed random walks using node types attribute random walks represent sequences node types used learn embeddings. instance attributed random walks used section learn type embedding matrix perspective general framework steps figure correspond ﬁrst main component maps nodes types function step corresponds last component generates attributed random walks representing sequences node types. logarithmic binning assigns ﬁrst nodes smallest attribute value assigns fraction remaining unassigned nodes smallest value however framework could used generalize node embedding deep graph model leverages traditional random walks. algorithm generalized nodevec procedure generalizednodevec possibly attributes embedding dimensions walks node walk length context size return in-out function nodevec algorithm following advantages naturally supports attributed graphs learns features generalize nodes well extraction another arbitrary graph thus able used transfer learning tasks space-efﬁcient learning embedding type opposed node. generalized nodevec algorithm shown alg. particular replace notion random walk nodevec appropriate notion attributed random walk nodes mapped types using arbitrary function deﬁned learned; section details. aside alg. assumes function deﬁned learned apriori. note attribute matrix given input derive features based graph structure however even attributes given input also choose derive additional structural features graphlets append attributed random walk routine generalized nodevec algorithm shown alg. given start node ﬁrst obtain type start node given function initialize attributed walk node appended back preserve order). afterwards line sets current node lines repeated steps. finally attributed random walk length starting node returned line also leverage alg. generalizing deep graph models node embedding methods. straightforward alg. generalization nodevec since allow recover original nodevec algorithm special case framework. furthermore deepwalk also special case implementation details implementation evaluate node store hash table giving time lookup. allows derive line alg. constant time. construct hash table takes time obtain given space store efﬁciently using hash table. experimental setup unless otherwise mentioned experiments logarithmic binning experiments simple function represents concatenation attribute values node attribute vector particular deﬁned concatenation operator. experiments searched subsets graphlet attributes shown figure however approach trivially handles categories attributes including structural attributes intrinsic/self-attributes relational features derived using graph along existing structural self-attributes. evaluate generalized nodevec approach presented section leverages attributed random walk framework number popular methods including nodevec deepwalk line approach nodevec hyperparameters grid search mentioned logistic regression penalty. model selected using -fold cross-validation labeled data. experiments repeated random seed initializations. results statistically signiﬁcant pvalue unless otherwise mentioned evaluate models. data made available networkrepository figure using node types useful revealing interesting insights graph data including nodes structurally similar. node types alone reveal nodes structurally similar node color encodes type. number triangles wedges used nodes identical types grouped; uses binning text discussion. node mapping experiments section investigates intermediate representation deﬁned arbitrary function maps nodes types. recall simplicity deﬁned concatenation attribute values given node attributes correspond graphlet counts figure strikingly observe figure even simple mappings alone useful effective understanding important structural behavioral characteristics nodes even generating attributed random walks learning embedding based them. particular nodes assigned type figure obey strict notion feature. strict since nodes belong type identical feature vectors. however figure captures relaxed notion structural equivalance called structural similarity result surprising since embedding learned mapping intermediate representation hence validates intermediate representation mapping nodes types also demonstrating effectiveness grouping structurally similar nodes. comparison section compares proposed approach embedding methods link prediction. given partially observed graph fraction missing edges link prediction task predict missing edges. generate labeled dataset edges done positive examples obtained removing edges randomly whereas negative examples generated randomly sampling equal number node pairs connected edge i.e. node pair method learn features using remaining graph consists positive examples. using feature representations method learn model predict whether given edge test exists not. notice node embedding methods deepwalk nodevec require node appear least edge training graph otherwise methods unable derive features nodes. signiﬁcant limitation prohibits many real-world applications. method signiﬁcantly better methods summarize gain/loss predictive performance methods figure cases method achieves better predictive performance methods across wide variety graphs different characteristics. overall mean product binary operators give best results average gain predictive performance respectively. space-efﬁcient embeddings investigate space-efﬁciency learned embeddings proposed framework intermediate representation. observe embedding method implements proposed attributed random walk framework learns embedding distinct node type worst case embedding learned nodes graph recover original method special case. general best embedding often lies extremes therefore embedding learned method implementing framework often orders magnitude smaller size since function maps type experiments logarithmic binning deﬁned concatenation logarithmically binned attribute values. embeddings learned using different figure space requirements proposed approach orders magnitude less existing methods. note relative gain/loss best baseline method size embedding σmin embedding smallest size among baseline methods attribute sets used input mapping nodes types. represent variations method. baseline method minimum space shown thus methods space horizontal line. subsets attributes instance indicates node types derived using number -stars triangles given node participates evaluate space-efﬁciency various methods measure space required store embedding learned method. figure summarize reduction space approach compared methods. cases embeddings learned approach require signiﬁcantly less space thus space-efﬁcient. speciﬁcally embeddings approach require average times less space best method averaged across graphs. addition table provides number types derived using various attribute combinations. recent embedding techniques graphs largely based popular skip-gram model originally introduced learning vector representations words natural language processing domain. particular deepwalk used approach embed nodes co-occurrence frequencies pairs short random walks preserved. recently nodevec introduced hyperparameters deepwalk tune depth breadth random walks. approaches becoming increasingly popular shown outperform number existing methods. methods based simple random walks thus well-suited generalization using attributed random walk framework. hand methods skipgraph make simple random walks learn embeddings entire graphs methods used graph-level tasks graph classiﬁcation clustering. since methods still based simple random walks straightforward generalize using proposed framework. ever planetoid embedding-based approach semisupervised learning structural features. rossi proposed inductive approach networks called deepgl learns relational functions representing compositions operators applied initial graph features recently hamilton proposed similar approach also aggregates features node neighborhoods. however approaches based random-walks. heterogeneous networks also recently considered well attributed networks particular huang proposed approach attributed networks labels whereas yang used text features learn node representations liang proposed semi-supervised approach networks outliers bojchevski proposed unsupervised rankbased approach recently coley introduced convolutional approach attributed molecular graphs learns graph embeddings opposed node embeddings. however approaches inductive space-efﬁcient. work also related uniform non-uniform random walks graphs random walks heart many important applications ranking community detection recommendation link prediction inﬂuence modeling search engines image segmentation routing wireless sensor networks time-series forecasting applications techniques also beneﬁt notion attributed random walks. make existing methods generally applicable work proposed ﬂexible framework based notion attributed random walks. framework serves basis generalizing existing techniques attributed graphs unseen nodes graph-based transfer learning tasks allowing signiﬁcantly larger graphs inherent space-efﬁciency approach. instead learning individual embeddings node approach learns embeddings type based functions feature vectors types. allows inductive transductive learning. furthermore framework shown following desired properties space-efﬁcient accurate inductive able support graphs attributes finally approach guaranteed perform least well original methods since recovered special case framework.", "year": 2017}