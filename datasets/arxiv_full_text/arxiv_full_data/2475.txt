{"title": "Adaptive Lambda Least-Squares Temporal Difference Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Temporal Difference learning or TD($\\lambda$) is a fundamental algorithm in the field of reinforcement learning. However, setting TD's $\\lambda$ parameter, which controls the timescale of TD updates, is generally left up to the practitioner. We formalize the $\\lambda$ selection problem as a bias-variance trade-off where the solution is the value of $\\lambda$ that leads to the smallest Mean Squared Value Error (MSVE). To solve this trade-off we suggest applying Leave-One-Trajectory-Out Cross-Validation (LOTO-CV) to search the space of $\\lambda$ values. Unfortunately, this approach is too computationally expensive for most practical applications. For Least Squares TD (LSTD) we show that LOTO-CV can be implemented efficiently to automatically tune $\\lambda$ and apply function optimization methods to efficiently search the space of $\\lambda$ values. The resulting algorithm, ALLSTD, is parameter free and our experiments demonstrate that ALLSTD is significantly computationally faster than the na\\\"{i}ve LOTO-CV implementation while achieving similar performance.", "text": "temporal difference learning fundamental algorithm ﬁeld reinforcement learning. however setting td’s parameter controls timescale updates generally left practitioner. formalize selection problem bias-variance trade-off solution value leads smallest mean squared value error solve tradesuggest applying leave-one-trajectory-out crossvalidation search space values. unfortunately approach computationally expensive practical applications. least squares show loto-cv implemented efﬁciently automatically tune apply function optimization methods efﬁciently search space values. resulting algorithm allstdis parameter free experiments demonstrate allstd signiﬁcantly computationally faster na¨ıve loto-cv implementation achieving similar performance. problem policy evaluation important industrial applications accurately measuring performance existing production system lead large gains temporal difference learning fundamental policy evaluation algorithm derived context reinforcement learning variants used sarsa lspi many popular algorithms. algorithm estimates value function policy parameterized averages estimates value function future timesteps. induces bias-variance trade-off. even though tuning signiﬁcant impact performance previous work generally left problem tuning practitioner paper consider problem automatically tuning data-driven way. deﬁning problem ﬁrst step deﬁning mean best choice take value minimizes msve solution bias-variance trade-off. proposed solution intuitive approach estimate ﬁnite chose minimizes estimate mse. score values could estimate loss training scores misleading overﬁtting. alternative approach would estimate cross validation particular supervized learning setting leave-one-out gives almost unbiased estimate loss develop leave-one-trajectory-out unfortunately loto-cv computationally expensive many practical applications. efﬁcient cross-validation show loto-cv efﬁciently implemented framework least squares recursive lstd). combining ideas propose adaptive least-squares temporal difference learning na¨ıve implementation loto-cv requires evaluations lstd allstd requires evaluations number trajectories |λ|. experiments demonstrate proposed algorithm effective selecting minimize mse. addition experiments demonstrate proposed algorithm signiﬁcantly computationally faster na¨ıve implementation. contributions main contributions work formalize selection problem ﬁnding value leads smallest mean squared value error markov decision process countable states ﬁnite actions maps state-action pair probability transitioning single timestep dimensional vector mapping state scalar reward discount factor. assume given function remainder paper focus lstd rather rlstd clarity rlstd additional initial variance parameter lstd gives exact least squares solutions note however similar approaches analysis applied rlstd. parameter effectively controls timescale updates performed. induces bias-variance tradeoff longer timescales tend result high variance shorter timescales introduce bias. paper solution trade-off value produces parameters minimize mean squared value error distribution states. ﬁnite natural choice perform leaveone-out cross-validation select minimizes mse. unlike typical supervised learning setting individual sampled transitions correlated. loo-cv errors potentially biased. however since trajectories independent propose leave-one-trajectory-out na¨ıve implementation would perform loto-cv parameter would mean running lstd times parameter value thus total time loto-cv parameter values signiﬁcantly need solve lstd times parameter value. ﬁrst decrease computational cost associated loto-cv single parameter value. consider methods reduce cost associated solving lstd different values rather solving value separately. maps state d-dimensional vector denote dimensional matrix column state stochastic policy denote probability policy executes action state given policy deﬁne value function drop subscript clear context. computational complexity lstd term solving inverse term cost associated building matrix. reduce total computational complexity using recursive lstd refer rlstd. instead computing solving inverse rlstd recursively updates estimate using sherman-morrison formula deﬁnition invertable matrix column vectors +vm−u shermanmorrison formula given state observation vectors timesteps dur× matrix columns next state observation vectors timesteps during trajectory pseudo-code given algorithm allstd takes input trajectories ﬁnite values candidate values. mean squared value error notice loto-cv error depends computed parameters important property allows compare error different choices parameters known loto-cv error trajectory computed time. respectively derive straightforwardly time. however deriving must done carefully. ﬁrst derive update matrix recursively using sherman-morrison formula remove transition sample trajectory. policy policy used generate trajectories uniform random policy actions left right. features state encoded using -hot representation -dimensional feature vector. thus value function exactly representable. figure compares root msve random walk domain. notice allstd na¨ıveallstd achieve roughly error level lstd rlstd. domain narrow performance lstd lstd allstd na¨ıveallstd achieve performance comparable lstd. figure compares average execution time algorithm seconds lstd rlstd simply time required compute lstd rlstd different values respectively. shown reference actually make decision value use. allstd signiﬁcantly faster na¨ıveallstd takes roughly computational time solving lstd different values. domain game played board tiles. tiles either empty assigned positive number. tiles larger numbers acquired merging tiles number. immediate reward merged tile numbers. policy policy used generate trajectories uniform random policy four actions down left right. features state represented dimensional vector value taken value corresponding tile used value empty tiles. linear space rich enough capture true value function. figure compares root msve allstd na¨ıveallstd achieve roughly error level lstd rlstd perform significantly better lstd rlstd small number trajectories. theorem dataset trajectories generated following policy initial state distribution n→∞νπ−aφµ−min νπ−θφµ proposed algorithm allstd maps dataset vector compared msve computation time allstd na¨ıve implementation loto-cv three domains random walk mountain car. baseline compared algorithms lstd rlstd best worst ﬁxed choices hindsight denote lstd lstd rlstd rlstd. experiments generated independent trials. random walk domains discount factor mountain domain discount factor domain random walk random walk domain chain states organized left right. agent always starts middle state move left right state except leftmost rightmost states absorbing. agent receives reward unless enters rightmost state receives reward episode terminates. figure random walk domain relationship amount training data root msve trajectories varied. allstd achieves performance na¨ıvecv+lstd. training time seconds trajectories varied. allstd approximately order magnitude faster na¨ıvecv+lstd. figure relationship amount training data root msve trajectories varied. allstd achieves performance na¨ıvecv+lstd. training time seconds trajectories varied. allstd approximately order magnitude faster na¨ıvecv+lstd. figure mountain domain relationship amount training data root msve trajectories varied. allstd achieves performance na¨ıvecv+lstd. training time seconds trajectories varied. allstd approximately order magnitude faster na¨ıvecv+lstd. reverse action selected otherwise represents location represents velocity car. features feature space -dimensional vector ﬁrst element location second element velocity car. thus linear space rich enough capture true value function. figure shows root msve function trajectories. unlike previous domains achieves smallest error even small trajectories. difference likely poor feature representation used favors monte-carlo return figure compares root msve mountain domain. poor feature representation difference lstd lstd large. allstd na¨ıveallstd achieve roughly performance lstd rlstd. introduced allstd efﬁciently select minimize root msve data-driven way. similar approach allstd found work downey sanner introduces bayesian model averaging approach update however approach comparable allstd clear extended domains function approximation required estimate value. konidaris thomas introduce γ-returns ω-returns respectively offer alternative weightings t-step returns. however approaches designed estimate value single point rather value function. furthermore assume bias introduced t-step returns thomas brunskill introduce magic algorithm attempts account bias t-step returns algorithm still designed estimate value point. allstd designed estimate value function data-driven minimize root msve. white white introduce λ-greedy algorithm adapting per-state based estimating bias variance. however approximation bias variance needed state apply λ-greedy. approximating values accurately equivalent solving original policy evaluation problem approach suggested work white white introduces several additional parameters. allstd hand parameter free algorithm. furthermore none previous approaches suggest using loto-cv tune show loto-cv efﬁciently implemented lstd family algorithms. focused on-policy evaluation biasvariance trade-off controlled even extreme off-policy evaluation problems. thus interesting area future work would applying allstd off-policy evaluation possible identify good values without evaluating trajectories. bandit-like algorithm could applied determine many trajectories evaluate different values also interesting note efﬁcient cross-validation trick could used tune parameters parameter controlling regularization. paper focused selecting single global value however possible reduce estimation error learning values specialized different regions state space adapting different regions state-space challenging increases search space identifying good values could improve prediction accuracy regions state space high variance little data.", "year": 2016}