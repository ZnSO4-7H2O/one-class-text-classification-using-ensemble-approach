{"title": "From neural PCA to deep unsupervised learning", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "A network supporting deep unsupervised learning is presented. The network is an autoencoder with lateral shortcut connections from the encoder to decoder at each level of the hierarchy. The lateral shortcut connections allow the higher levels of the hierarchy to focus on abstract invariant features. While standard autoencoders are analogous to latent variable models with a single layer of stochastic variables, the proposed network is analogous to hierarchical latent variables models. Learning combines denoising autoencoder and denoising sources separation frameworks. Each layer of the network contributes to the cost function a term which measures the distance of the representations produced by the encoder and the decoder. Since training signals originate from all levels of the network, all layers can learn efficiently even in deep networks. The speedup offered by cost terms from higher levels of the hierarchy and the ability to learn invariant features are demonstrated in experiments.", "text": "network supporting deep unsupervised learning presented. network autoencoder lateral shortcut connections encoder decoder level hierarchy. lateral shortcut connections allow higher levels hierarchy focus abstract invariant features. standard autoencoders analogous latent variable models single layer stochastic variables proposed network analogous hierarchical latent variables models. learning combines denoising autoencoder denoising sources separation frameworks. layer network contributes cost function term measures distance representations produced encoder decoder. since training signals originate levels network layers learn efﬁciently even deep networks. speedup offered cost terms higher levels hierarchy ability learn invariant features demonstrated experiments. ever since hubel wiesel published ﬁndings hierarchy increasingly abstract invariant visual features neocortex researchers trying mimic hierarchy feature extraction stages artiﬁcial neural networks. early example neocognitron fukushima nowadays estimating deep feature extraction hierachies called deep learning topic received attention hinton salakhutdinov hinton proposed unsupervised pre-training scheme made subsequent supervised learning efﬁcient deeper network before. somewhat embarrasing ﬁeld though recently purely supervised learning achieved good better results unsupervised pre-training schemes classiﬁcation problems ﬁnding producing labels samples hard. many cases plenty unlabeled data exist seems obvious using improve results. instance plenty unlabeled images available image classiﬁcation tasks vastly bits information statistical structure input images labels. unsupervised learning aims representing structure input data often means features. resulting features used input classiﬁcation tasks initialization supervised learning. consider example imagenet classiﬁcation problem krizhevsky tackled. target classes label carries less bits information. compare amount information contained images used input. impossible exactly many bits information image carries certainly several orders magnitude bits. argued reason unsupervised learning able improve results current versions incompatible supervised learning. problem many unsupervised learning methods represent much information original data possible whereas supervised learning tries ﬁlter information irrelevant task hand. chapter presents unsupervised learning network whose properties make good supervised learning. first learning based minimizing cost function much stochastic gradient descent supervised feedforward networks. learning therefore continue alongside supervised learning rather restricted pre-training phase. second network discard information higher layers leave details lower layers represent. means approach allows supervised learning select relevant features. proposed unsupervised learning ﬁlter noise selected features come features related selected features. section explains adding lateral shortcut connections autoencoder gives representational capacity hierarchical latent variable models. means higher layers longer need represent details concentrate abstract invariant representations. model structure called ladder network vertical paths connected horizontal lateral connections regular intervals. learning deep autoencoders slow since training signals need travel long distance decoder output decoder encoder. lateral shortcuts tend slow even since shortcuts learn ﬁrst shunting training signals along longer paths. section explains remedied adding training targets level hierarchy. novel idea combine denoising source separation framework training denoising functions remove injected noise experiments presented section demonstrate higher levels ladder network discard information focus invariant representations training targets higher layers speed learning. results presented promising preliminary. section discusses potential extensions related work. argued earlier unsupervised learning needs tolerate discarding information order work well supervised learning. many unsupervised learning methods good class models stands exception hierarchical latent variable models. unfortunately derivation quite complicated often involves approximations compromise performance. simpler alternative offered autoencoders also beneﬁt compatible standard supervised feedforward networks. would promising candidate combining supervised unsupervised learning unfortunately autoencoders normally correspond latent variable models single layer stochastic variables tolerate discarding information. section summarizes complementary roles supervised unsupervised learning reviews latent variable models relation standard autoencoder networks proposes network structure ladder network whose lateral shortcut connections give representational capacity hierarchical latent variable models. consider roles supervised unsupervised learning particular task classiﬁcation prediction regression. assume input-output pairs used supervised learning unlabeled samples lacking output inputs available inputs information outputs. general typical setup means unsupervised learning used anything works precious bits information available output samples reserved tasks unsupervised learning cannot handle. main role supervised learning clear enough ﬁgure type representations relevant task hand. supervised learning because deﬁnition unsupervised learning detailed information task. obvious role traditional unsupervised learning pre-processing pretraining step supervised learning. however question unsupervised learning supervised learning kicked important many problems much information inputs cannot possibly fully summarized unsupervised learning ﬁrst. rather would useful unsupervised learning continue tuning representations even supervised learning started tune relevant features ﬁlter irrelevant ones. combination supervised unsupervised learning known semi-supervised learning. argued earlier happy marriage unsupervised learning content discarding information concentrating features supervised learning deems relevant. unsupervised learning able efﬁciently features correlate predict features selected supervised learning. improves generalization samples. example consider learning recognize face. suppose supervised learning ﬁgured important feature classifying faces non-faces samples. unsupervised learning available unlabeled samples features correlate selected one. features could instance detector nose brow mouth features improve generalization face detector cases feature missing instance eyes closed occluded sunglasses. speciﬁcally unsupervised learning must keep pushing features representation intended supervised learning simply features carry information inputs. behavior reasonable pre-training pre-processing step compatible semi-supervised learning. words unsupervised learning knows features relevant reasonable select features carry much information possible inputs. however supervised learning starts showing preference features others unsupervised learning follow suite present kind supervised learning seems interested many unsupervised learning methods framed latent variable models unknown latent varibles assumed generate observed data common special case continuous variables latent variables predict mean observations denotes probability density function noise term inference unknown latent variables parameters based simply minimizing mismatch observed reconstruction generally probabilistic modeling. models deﬁned eqs. layer latent variables tries represent everything represent data. models trouble letting piece information since would increase reconstruction error. also many cases abstract invariant feature cannot reduce reconstruction error alone without plenty accompanying details position orientation size details need represented alongside relevant feature show beneﬁt reducing reconstruction error. means latent variable models single layer latent variables trouble discarding information focusing abstract invariant features. superscript refers variables layer observations taken equation deﬁning latent variables higher levels longer need represent everything. lower levels take care representing details higher levels focus selected features abstract not. generally higher-level variables represent properties distribution variance binary variables sigmoid units often used representing dependency exact inference computing posterior probability unknown variables typically mathematically intractable. instead approximate inference techniques variational bayesian methods employed amount approximating intractable exact posterior probability simpler tractable approximation. learning corresponds iteratively minimizing cost function respect posterior approximation. instance case continuous latent variables posterior could approximated gaussian diagonal covariance. unknown variable mean variance would estimated course learning. posterior means variances typically depend values lower higher layers inference would therefore proceed iteratively. hierarchical latent variable models easy deﬁne learn problem would solved. unfortunately hierarchical models often require complex probabilistic methods train them. often involve approximations compromise performance limited restricted model structures mathematically tractable. also many training schemes require latent variable values updated layer-wise combining bottom-up information top-down priors. slows propagation information network. autoencoder networks resemble many ways single-layer latent variable models. idea inference process mapping observations corresponding latent variables called hidden unit activations modeled encoder network mapping back observations modeled decoder network mappings called encoder decoder mappings respectively. connection latent variable models analogous mappings called recognition reconstruction mappings. learning autoencoders based minimizing difference observation vector reconstruction minimizing cost ˆx|| respect parameteres remainder chapter mappings assumed parameters omitted brevity. typically course learning layers added previously trained network. adding training last layer training continue supervised manner using mappings deﬁne multi-layer feedforward network minimizing squared distance actual outputs desired targets outputs. tempting assume hierarchical version autoencoder eqs. corresponds somehow hierarchical latent variable model unfortunately case because intermediate hidden layers called deterministic variables hierarchical latent variable model requires called stochastic variables. difference stochastic variables independent representational capacity. matter priors tell stochastic latent variables overrule bits information reconstruction. contrast deterministic variables zero bits information assuming deterministic mappings implemented layered networks correspond hidden layers mappings stochastic variables order this going take inference structure hierarchical latent variable model main difference inference combines information bottom-up likelihood top-down prior depends top-down information. words cannot information representation receive information bottom-up path. shortcut connection bottom-up encoder path modiﬁed top-down decoder path recover information missing words higher layers need represent details. also mapping learn combine abstract information higher levels face detailed information position orientation size lower layers. means higher layers focus representing abstract invariant features seem relevant task hand detailed information. figure shows roughly inference structure hierarchical latent variable model compares standard autoencoder ladder network. note combines information bottom-up top-down paths ladder network not. direct path inputs highest layer means training signals highest layers propagate directly network supervised learning. gradient propagation already combines information bottom-up activations top-down gradients need extra mixing information. general problem deep models error function input layer output layer many parts network away source training signals. fact ladder model shown fig. trained fashion regular autoencoders minimizing difference problem becomes worse. shortcut connection chance contributing reconstruction leaving shrinking share error higher layers. standard autoencoders force training signals pass levels hierarchy even learning multiple layers nonlinear functions difﬁcult slow. contrast hierarchical latent variable models cost functions stochastic variables. since ladder network shares many properties hierarchical latent variable models seems reasonable introduce training signals level hierarchy ladder network. section shows done combining denoising source separation framework training denoising functions remove injected noise shortcut connections part solution since adding actually short-circuits standard autoencoder learning mapping simply copy shortcut input output incentive higher layers. related inability standard autoencoder training learn over-complete representations. fortunately denoising autoencoders overcome problem simply adding noise inputs explained section figure inference structure hierarchical latent variable model compared standard autoencoder proposed ladder network. details inference latent variable models often complex posterior distribution approximation complex posterior mean overall picture approximately shown left. since information standard autoencoder network highest layer needs represent details input intermediate hidden layer activations cannot independently represent information receive information highest layer. ladder network constrast lateral connections layer give chance represent information independently higher layers. also abstract invariant representations higher levels interpreted context detailed information without higher levels represent details. order develop system learning distributed rather guided gradients propagating single error term shall turn attention competitive unsupervised learning. starting point algorithms study neural principal component analysis learning rule utilize second order statistics input data principal component projections. slight nonlinear modiﬁcations made learning rule input data whitened method becomes sensitive higher-order statistics performs independent component analysis nonlinearity used algorithm interpreted constrast function measures non-gaussianity source distribution. interpretation originally given popular fastica algorithm however alternative view nonlinearity interpreted denoising function. hyv¨arinen derived maximum likelihood estimate going follow derivation valpola pajunen showed nonlinearity interpreted expectation step expectation maximization algorithm. overall nonlinear learning rule combined input whitening orthogonalization projections interpreted efﬁcient approximation expectation maximization algorithm applied linear latent variable model tuned interpretation development denoising source separation framework algorithm method optimizing parametric mappings latent variable models. operates alternating step step step assumes mapping ﬁxed updates posterior distribution step reverse updating mapping assuming posterior distribution ﬁxed. algorithm resulting assumptions essentially nonlinear learning rule. input data whitened step becomes simple matrix multiplication nonlinear learning rule amounting essentially hebbian learning. nonlinearity interpretation expected value latent variables given noisy observations denoising. crucial topic learning deep models cost function directly refer input latent variable denoised version. hierarchical model mean layer contributes terms cost function bringing source training signals close parameters layer. nonlinear learning rule needs additional constraint implements competition latent variables could otherwise converge values. case linear model easiest approach require orthogonal. apply nonlinear models. instead possible require covariance matrix latent variables unit matrix denoising function used derived prior distribution latent variables. many techniques learning distributions particularly useful technique directly learns denoising function proposed vincent connection autoencoders. idea corrupt inputs autoencoder noise network reconstruct original uncorrupted inputs. forces autoencoder learn denoise corrupted inputs. bengio showed possible sample called denoising autoencoders simply iterating corruption denoising. distribution denoised samples converges original data distribution training denoising function learns cancel diffusion resulting corruption input data. diffusive forces proportional average carry samples areas high density towards lower densities. denoising function learns oppose this force opposite sign. sampling starts given distribution combined steps corruption denoising produce average samples disappears diffusion caused corruption exactly cancels caused denoising sample distribution follows original training distribution. bengio suggested sampling efﬁcient hierarchical models corruption takes place inputs levels encoder path called networks generative stochastic networks surprising denoising functions even possible derive probability estimates data. note denoising function loses information absolute probability conserves information relative probabilities logarithm ﬁrst turns multiplication summation constant normalization term disappears differentiation. representation bears similarity energy-based probability models relative probabilities readily accessed. turns however model reconstruct missing data turned probability density estimator using input erasure corruption autoencoder thus used deriving normalized probability estimates even denoising function loses information normalization factor probability. purposes particularly important feature denoising autoencoders handle over-complete representations including shortcut connections reason enough network simply copy inputs outputs since inputs corrupted noise. rather network representation makes removing noise easy possible. ready derive learning rule distributed cost function ladder network. basic idea apply denoising autoencoder recursively. starting point standard learning denoising function learns remove noise injecting corrupt assume denoising function uses internal variables implementing multi-layer mapping derivation suggests cost functions used learning encoding mappings layers layers below derived assuming ﬁxed. problematic means cannot single consistent cost function whole learning process learning continuously alternate learning different layers. like hierarchical latent variable models higher level priors offer guidance lower-level forward mappings since gradients propagate backward along encoding path model fully compatible supervised learning standard supervised cost function simply added top-most layer measuring distance target output. ﬁnal thing must take care decorrelation term needed algorithms. recall minimized constant. minimization respect actually typically promotes decorrelation amounts regression extra information used reduce reconstruction error. minimization respect promotes ﬁnding projections predicted well possible since mutual information symmetric therefore also help predicting features long entropy hidden unit activations kept collapsing avoiding trivial solution constant. kronecker delta. words measures squares difference however cost function distinguish small large eigenvalues viewpoint keeping dss-style learning collapsing representation small eigenvalues pose problem. analyse situation note sound measure information content variable determinant covariance matrix measures square volume cuboid whose sides length determined standard deviations distribution along eigenvectors. since determinant matrix equals product eigenvalues logarithm determinant equals logarithms eigenvalues latter equality follows fact analytical function deﬁned square matri= eλke− therefore power series expansion matrix turns power series expansion eigenvalues. note matrix logarithm logarithm elements matrix. note twice differentiable cost functions minimized second-order behaviour close minimum simpler works well sufﬁciently close however avoid potential problems used experiments presented chapter. algorithms require decorrelation output representation also important decorrelate inputs. normally denoising autoencoders ﬁxed input cost functions higher layers inﬂuence input mappings creates bias towards pca-type solutions. amount noise injected relatively smaller projections variance larger. terms therefore smaller network extracts mainly projections larger variance pca-type solutions. desirable cases often not. learning parameters mappings based minimizing simple solution apply gradient descent basically optimization method used example nonlinear conjugate gradient quasi-newton methods. whichever method chosen existence single cost function minimized guarantees learning converges long minimization method performs properly. figure depicts computational diagram cost function ladder network. paths going upward share mappings difference inputs corrupted path corrupted noise. layer network adds term cost function measuring well clean activations reconstructed corrupted activations. forward computations information observations towards cost function terms along arrows. learning gradients cost function terms opposite direction. training signals arriving along clean path correspond dssstyle learning training signals denoising path corrupted path correspond type learning taking place denoising autoencoders. terms promote unit covariance zero mean clean activations respectively shown. required keeping dss-style learning collapsing representations functions clean path only. perspective learning deep networks important mapping close cost function terms means learning efﬁcient even propagating gradients mappings would efﬁcient. figure ladder network’s cost computations illustrated. clean path shares exactly mappings corrupted path. difference corruption noise added corrupted path. resulting corrupted activations denoted layer cost function term measures distance clean activations reconstructions terms measure well activations normalized shown. three following sections gradually develop two-layered ladder network learn abstract invariant features. first simple distributions modeled. linear model hidden layer. finally second layer added models correlations variances ﬁrst layer activations. experiments used learning rules described eqs. hyperparameters automatically adjusted keep smallest eigenvalue hyperparameter value start simple experiment elucidates relation prior distribution activations denoising functions illustrated fig. three different distributions tested super-gaussian sub-gaussian gaussian distribution. unit variance zero mean. plot shows different results overlaid other. e−√|x| logarithmic scale behaves −√|x| plus constant. sub-gaussian distribution generated scaling sinusoidal signal obtain variable unit variance. distribution figure illustration connection marginal distribution denoising function. left three different probability distributions shown logarithmic scale. super-gaussian sub-gaussian gaussian distribution. distribution unit variance zero mean. right denoising functions trained remove corruptive noise unit variance. gaussian case theoretically optimal solution ˜x/. plot shows different random samples plotted other. note plots showing data plotted vertical axis help side-by-side comparison denoising function. experiment model hidden layers therefore forward functions denoising function implemented single hidden neuron tanh activation bypass connection variance noise used corrupting small corruption noise would mean denoising function laplacian input would scaled step function. case since large variance input. tends smoothen denoising function. move simple linear model serves example statistical modeling translated function approximation framework. also gives intuition lateral connections help higher levels focus relevant features. linear independent component analysis data assumed linear mixture independent identically distributed sources. unlike principal component analysis mixing restricted orthogonal sources non-gaussian marginal distributions recovered. source gaussian distribution sources cannot expected recovered remain mixed. however subspace spanned gaussian sources recoverable separate non-gaussian sources. limitation unless extra information available sources recovered scaling permutation. scaling usually ﬁxed assuming source distributions unit variance. still leave permutation sign sources ambiguous. dataset generated linearly mixing samples sources observations. elements mixing matrix sampled zero-mean gaussian distribution. souces distributions previous example super-gaussian sub-gaussian gaussian sources. data whitened purpose experiments demonstrate normal autoencoders bias towards solution even cost function terms used. experiments zero. model hidden layer nonlinearity hidden layer denoising simpliﬁed version model used previous experiment. essentially zero mean observations need bias terms model. mappings model follows note underlying assumption models sources independent. incorporated model making denoising hidden layer unit-wise. also lateral linear mapping matrix model covariance structure observation data. means hidden layer able focus representing non-gaussian sources. experiments verify model structure deﬁned eqs. indeed able recover original sources used generating observed mixtures apparent studying normalized loading matrix measures much original sources contribute values hidden units. loading matrix obtained product waorig unmixing matrix learned model aorig original mixing matrix. rows matrix measure much contribution original source recovered hidden neuron activation. normalized loading matrix rows scaled squares vectors unit lengths. successful unmixing characterized single dominant loading measured average angle vector dominant source. experiments typical value around corresponds contribution dominant source. denoising mappings source depend distribution source expected. particular sign parameter determined supersub-gaussianity sources. hidden units non-gaussian sources model also represent gaussian sources preference non-gaussian sources. expected lateral mapping lowest level network already represent gaussian structure. words model performs expected. interesting happens lateral mapping missing. since reconstruction contain information present hidden units network strong pressure conserve much information possible. essentially dominant mode operation network primarily extracts subspace spanned eigenvectors data covariance matrix corresponding largest eigenvalues. network secondarily align representation along independent components. independent components happen align principal subspace wins ica. experiment instance network hidden units able retrieve non-gaussian sources loadings averaging contrast best loadings exactly setting without averaged number iterations. signiﬁcantly different random mappings yield average loadings around turned network able better convereged tremendously slowly requiring times iterations still even network seemingly converged best loadings averaged clearly better random even best loading worse worst loading lateral connections used. network’s tendency extract principal subspace seen analysing large portion subspace spanned falls outside subspace spanned largest eigenvectors data covariance matrix. network whereas network lacking fair noted pre-whitening inputs restores autoencoder’s ability recover independent components allows nonlinear learning rule perform independent component analysis rather principal subspace analysis. however complex cases easy normalize away information wanted. shall move expanding model adding layer capture nonlinear dependencies remaining hidden unit activations model. makes sense usually impossible produce truly statistically independent components simply computing different linear projections observations. even resulting feature activations lack linear correlations normally higher-order dependencies features typical example variances activations correlated underlying cause feature activation likely generate activations too. order network structure could represent correlated variances recall optimal denoising gaussian variable prior variance gaussian corruption noise experiments section demonstrate need represent correlations variances hidden unit activations modulatory connections layer give enough ﬂexibility forward mapping highest layer. network representations highest layer. moreover lateral connections middle layer play crucial role allow higher layers focus representing higher-order correlations. unlike case model previous section could replaced whitening inputs. experiments also demonstrate cost function terms higher layers speed learning. dataset samples generated random linear mixture sources experiment. time however sources gaussian changing variance. variances sources determined higher-order variance sources used group four sources. four groups words four higher-order variance sources determined variances sources. groups dependent sources mean data follows model used independent subspace analysis variance sources sampled gaussian distribution variance lower-level sources obtained computing contain indices lower-level sources modulated variance source since four non-overlapping groups four sources note although sources sampled gaussian distribution marginal distribution super-gaussian since variance changing. experiments dataset pre-whitened. figure squares normalized loading matrix hierarchical ladder network. black corresponds white zero. evident plot four subspace cleanly separated remain internally mixed. distribution sources within subspace spherically symmetric makes impossible determine rotation within subspace. reﬂected blocky structure matrix loadings. hidden units ordered appropriately reveal block structure. experiments veriﬁed network managed separate individual source subspaces learned model correlations variances different sources ﬁgures correspond experiment dimension ﬁrst layer second layer network used modeling hidden units. ﬁgures show results training iterations. results relatively good already iterations iterations network practically converged. figure shows squares loadings elementwise squares matrix waorig scaled squares hidden unit. since data generated modulating variance gaussian sources subspace data distribution spherically symmetric within subspace. reﬂected blocky appearance matrix loadings network settled random rotation within subspace. note rotation indeterminacy property input data. experiments model data previous section readily separated sources figure illustration denoising functions learned hierarchical ladder network. plot shows term sigmoid behaves function hidden neurons takes nonzero values. shows sigmoid changes function hidden units belong subspace whose sigmoid shown. rest plots correspond hidden units groups figure depicts learned denoising functions. plot shows term sigmoid belonging ﬁrst hidden unit behaves function hidden neurons sigmoid term function hidden neuron activations second layer hidden unit activations. theoretically sigmoid term modulating mapping function norm activation vector subspace function readily seen plots second layer activations apparently learned represent quadratic terms ﬁrst layer activations sigmoid indeed appears function norm activation vector ﬁrst subspace. data used sigmoid term learned neglect hidden units developed function variance features developing second layer model improve reconstruction combined variance alone cannot anything direction reconstruction changed. without shortcut connections ladder model highest layer hidden units could learned represent variance sources would enough space also represent activations also needed make variance sources. higher layers details recovered denoising proceeds highest layers towards lowest. another important question whether terms cost function originating higher layers network really useful learning. investigate this simulations different datasets random initializations network. turned particular model higher layer cost function term important could speed learning considerably particularly early stages learning. expected crucial combine proper decorrelation term success model measured value reached. note priori clear adding cost function terms could help reduce nevertheless turned case. iteration network consistently reached lower value iteration minimizing alone subsequent learning continued approximately pace seems reasonable denoising autoencoders able optimize model initialized close sensible solution. another interesting ﬁnding third improvement seems attributable decorrelation term able speed learning alone without despite initializing network mappings ensure representations start decorrelated. whereas speedup pronounced beginning learning speedup offered important middle phases learning. presumably representations start diverging uncorrelated initialization gradually. optimum cost function addition extra term make situation worse viewpoint minimizing therefore likely optimally weights gradually decreased throughout training could zero ﬁnal ﬁnetuning phase. simplicity kept ﬁxed simulations presented here. experiments veriﬁed ladder model lateral shortcut connections cost function terms every level hierarchy indeed able learn abstract invariant features efﬁciently. although networks studied layers important representations abstract invariant already second layer. fact model layers linear features ﬁrst variance features second corresponds roughly architecture simple complex cells found hubel wiesel recently kingma obtained good results model combined idea hierarchical latent variable models. model included trainable recognition mapping addition generative model overall structure resembles autoencoder. additive interactions included encoder decoder remains seen whether approach extended include complex interactions modulatory interaction somewhat similar approach motivated variational autoencoders target propagation proposed bengio also includes exchange information decoder encoder mappings every level hierarchy though learning. appealing features approach taken replaces probabilistic modeling function approximation using simple cost function. experiments network learned extract higher-level sources captured dependencies ﬁrst-level sources. model corresponded independent subspace analysis model tailored input data structure. forward mapping general network. experiments denoising mapping somewhat limited structure mainly simplify analysis results. interesting study whether also replaced general mapping. another important avenue research take advantage machinery developed related methods sampling calculating probability densities making multiple rounds corruption denoising learning particularly ability sample model useful. order make full possibilities corruption procedure extended. simple gaussian noise added inputs. noise could added every layer better support sampling could also involve masking elements input vector completely different types corruption needed different times might possible extend denoising functions handle different types corruption might possible relearn denoising functions keeping previously learned forward mappings ﬁxed. crucial aspect ladder network captures essential features inference structure hierarchical latent variable models. along line possible extend model support even complex inferences take place kalman ﬁlters. model studied yli-krekola takes even step further implements dynamical biasing process gives rise emergent attention-like selection information similar fashion model suggested deco rolls model studied yli-krekola derived framework already structure reminiscent ladder architecture presented here. model otherwise elegant prone overﬁt lateral connections exaggerate feedback loops units. using tricks here injecting noise beneﬁt learning lateral top-down denoising might possible learn lateral connections reliably. chapter ladder network structure proposed autoencoder networks. network’s lateral shortcut connections give layer representational capacity stochastic latent variables hierarchical latent variable models. allows higher levels network discard information focus representing abstract invariant features. order support efﬁcient unsupervised learning deep ladder networks type cost function proposed. aspect layer network contributes terms cost function. means every mapping network receives training signals directly term measures local reconstruction errors. addition immediate training information network also propagates gradient information throughout network. means also possible terms correspond supervised learning. price higher-level cost function needs matched decorrelation term prevents representation collapsing. analogous competition used unsupervised competitive learning. additionally often useful decorrelate inputs otherwise network biased towards ﬁnding solution. preliminary experiments veriﬁed network able learn abstract invariant features extra terms cost function speed learning. experiments support notion model scales deep models works well together supervised learning much larger experiments still required verify claims. would like thank tapani raiko antti rasmus yoshua bengio useful discussions. antti rasmus running experiments parallel work input different versions performed invaluable. j¨urgen schmidhuber kyunghyun miquel perell´o nieto made available collections citations saved plenty time preparing manuscript. last certainly least would like thank erkki creating environment ideas underly work presented able develop. erkki always supported research. example shown possible follow intuition designing unsupervised learning algorithms also always emphasized importance rigorous analysis convergence properties resulting algorithms. without combination pioneering work neural nonlinear learning rule none research reported would gotten far.", "year": 2014}