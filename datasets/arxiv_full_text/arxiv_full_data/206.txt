{"title": "Self-informed neural network structure learning", "tag": ["stat.ML", "cs.CV", "cs.LG", "cs.NE"], "abstract": "We study the problem of large scale, multi-label visual recognition with a large number of possible classes. We propose a method for augmenting a trained neural network classifier with auxiliary capacity in a manner designed to significantly improve upon an already well-performing model, while minimally impacting its computational footprint. Using the predictions of the network itself as a descriptor for assessing visual similarity, we define a partitioning of the label space into groups of visually similar entities. We then augment the network with auxilliary hidden layer pathways with connectivity only to these groups of label units. We report a significant improvement in mean average precision on a large-scale object recognition task with the augmented model, while increasing the number of multiply-adds by less than 3%.", "text": "david warde-farley d´epartement d’informatique recherche op´erationelle universit´e montr´eal montreal quebec canada wardefariro.umontreal.ca study problem large scale multi-label visual recognition large number possible classes. propose method augmenting trained neural network classiﬁer auxiliary capacity manner designed signiﬁcantly improve upon already well-performing model minimally impacting computational footprint. using predictions network descriptor assessing visual similarity deﬁne partitioning label space groups visually similar entities. augment network auxilliary hidden layer pathways connectivity groups label units. report signiﬁcant improvement mean average precision large-scale object recognition task augmented model increasing number multiply-adds less context large scale visual recognition uncommon state-of-the-art convolutional networks trained days weeks convergence performing exhaustive architecture search quite challenging computationally expensive. furthermore satisfactory architecture discovered extremely difﬁcult improve upon; small changes architecture often decrease performance improve architectures containing fully-connected layers naively increasing dimensionality layers increases number parameters quadratically increasing computational workload tendency towards overﬁtting. settings domain interest comprises thousands classes improving performance speciﬁc subdomains prove challenging jointly learned features succeed overall task average sufﬁcient correctly identifying long tail classes making ﬁne-grained distinctions similar entities. side information form metadata example freebase often roughly corresponds kind similarity would make correct classiﬁcation challenging. context object classiﬁcation visually similar entities belong vastly different high-level categories whereas entities high-level semantic category bear little resemblance another visually. traditional approach building increasingly accurate classiﬁers average predictions large ensemble. case neural networks common approach layers making existing layers signiﬁcantly larger possibly additional regularization. strategies present signiﬁcant problem runtime-sensitive production environments classiﬁer must rapidly evaluated matter milliseconds comply service-level agreements. therefore often desirable increase classiﬁer’s capacity signiﬁcantly improves performance minimally impacting computational resources required evaluate classiﬁer; however immediately obvious satisfy competing objectives. present method judiciously adding capacity trained neural network using network’s predictions held-out data inform augmentation network’s structure. demonstrate efﬁcacy method using signiﬁcantly improve upon performance state-of-the-art industrial object recognition pipeline based szegedy less extra computational overhead. given trained network evaluate network held dataset order compute confusion matrix. apply spectral clustering generate partitioning possible labels. augment trained network’s structure adding additional stacks fully connected layers connected parallel pre-existing stack fully-connected layers. output auxiliary head connected weight matrix subset output units corresponding label clusters discovered spectral clustering. train augmented network initializing pre-existing portions network parameters original network randomly initializing remaining portions. train holding pre-existing weights biases ﬁxed learning hidden layer weights portions retraining classiﬁer layer’s weights. allows training focus making good auxiliary capacity rather adapting pre-initialized weights compensate presence hidden units. note also possible ﬁne-tune whole network training augmented section though perform ﬁne-tuning experiments described below. method seen similar spirit mixture experts approach jacobs rather jointly learning gating function well experts gated employ starting point strong generalist network whose outputs inform decisions specialist networks deploy different subsets classes. specialists also train original data input rather higher-level feature representation output original network’s convolutional layers. recent work distillation building earlier work termed model compression emphasizes idea great deal valuable information gleaned non-maximal predictions neural network classiﬁers. distillation makes averaged overall predictions several expensive-to-evaluate neural networks soft targets order train single network predict correct label mimic overall predictions ensemble closely possible. hinton predictions model itself however knowledge pursuit carefully adding capacity single already trained network rather mimicking performance many networks one. approach arguably complementary could conceivably applied distilling ensemble single mimic network order improve ﬁne-grained performance. base model consists inception architecture employed googlenet plus fully connected hidden layers rectiﬁed linear units each. output layer consists logistic units class. using seemingly large value order recover annotations large fraction possible classes least example hold-out set. term detection class context ground truth class confusion entry matrix thus encodes fraction time class confused class hold-out set. symmetrize either matrix apply spectral clustering using similarity matrix following formulation experiments specialist subnetworks consisted layers relus each. evaluate balanced test number classes image. confusion co-detection cases compare network identical capacity topology labels randomly permuted order assess importance particular partitioning discovered carefully controlling number parameters learned. lingonberry rooibos persimmon rutabaga banana family ensete apple viola shamrock walnut beech poppy kimjongilia chicory leaf melon grain juniper spruce birch family hawthorn guava gooseberry tick pouchong bonsai caraway fennel anemone maple sugar table examples partial sets labels grouped together performing spectral clustering base network’s confusions based scoring predictions. ﬁrst appears aviation-related second focusing mainly food third broadly concerned plant-related entities. methods improve upon base network ground truth appears provide signiﬁcant edge. best performing network specialist heads increases number multiply-adds required evaluation billion billion modest increase also provide figure evaluation best performing network imagenet -class test subset classes mapped classes imagenet task results thus directly comparable results obtained imagenet training set; direct comparison left follow-up work. presented simple general method improving upon trained neural network classiﬁers carefully adding capacity groups output classes trained model considers similar. demonstrate results computer vision task assumption underlying approach plan extend domains follow-up work. capacity label group. seemingly relevant factors include cardinality group amount training data available labels contained therein; however difﬁculty discrimination task necessarily scale either these. case particular convolutional network described obvious best place connect auxiliary stacks hidden layers following last convolutional layer. capacity therefore arguably discriminative knowledge network contained fully connected layers appealing part network augmentation purposes seems natural. nonetheless possible layers group-speciﬁc convolutional feature maps could beneﬁcial well. note augmentation procedure could also theoretically applied once necessarily location. subsequent clustering retraining step could potentially identify complementary division label space capturing information. finally seen small step towards conditional computation envisioned bengio wherein relevant pathways large network conditionally activated based task relevance. focused relatively large gains computationally inexpensive targeted augmentations. similar strategies could pave towards networks much higher capacity specialists evaluated necessary. bollacker kurt evans colin paritosh praveen sturge taylor jamie. freebase collaboratively created graph database structuring human knowledge. proceedings sigmod international conference management data krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems overfeat integrated recognition localization detection using convolutional networks. international conference learning representations cbls april", "year": 2014}