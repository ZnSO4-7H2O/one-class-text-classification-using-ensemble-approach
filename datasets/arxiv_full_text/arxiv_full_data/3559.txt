{"title": "Generalized Zero-Shot Learning via Synthesized Examples", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We present a generative framework for generalized zero-shot learning where the training and test classes are not necessarily disjoint. Built upon a variational autoencoder based architecture, consisting of a probabilistic encoder and a probabilistic conditional decoder, our model can generate novel exemplars from seen/unseen classes, given their respective class attributes. These exemplars can subsequently be used to train any off-the-shelf classification model. One of the key aspects of our encoder-decoder architecture is a feedback-driven mechanism in which a discriminator (a multivariate regressor) learns to map the generated exemplars to the corresponding class attribute vectors, leading to an improved generator. Our model's ability to generate and leverage examples from unseen classes to train the classification model naturally helps to mitigate the bias towards predicting seen classes in generalized zero-shot learning settings. Through a comprehensive set of experiments, we show that our model outperforms several state-of-the-art methods, on several benchmark datasets, for both standard as well as generalized zero-shot learning.", "text": "tion classiﬁers seen classes combination weights typically deﬁned using similarity scores unseen seen class learning probability distribution seen class extrapolating unseen class distributions using class-attribute information detailed discussion related work provided related work section. although aforementioned models shown considerable promise various benchmark datasets limitation models that test time highly biased towards predicting seen classes model learned using labeled data seen classes. issue models usually evaluated restricted setting training test classes assumed disjoint i.e. test examples come unseen classes search space limited unseen classes only. challenging setting training test classes disjoint known generalized zero-shot learning considered formidable problem setting. recent work shown accuracies approaches drops signiﬁcantly setting. work take generative approach problem naturally helps address generalized problem. approach based generative model synthesize exemplars unseen classes subsequently training off-the-shelf classiﬁcation model using synthesized exemplars. approach motivated similar spirit recent work synthesizing exemplars shown lead improved performance especially gzsl setting. exemplar generation develop generative model based conditional variational autoencoder architecture latent code instance augmented class-attribute vector. architecture coupled discriminator learns mapping generator’s output class-attribute. feedback helps improve generator encouraging generate exemplars highly discriminative nature. moreover dispresent generative framework generalized zeroshot learning training test classes necessarily disjoint. built upon variational autoencoder based architecture consisting probabilistic encoder probabilistic conditional decoder model generate novel exemplars seen/unseen classes given respective class attributes. exemplars subsequently used train off-the-shelf classiﬁcation model. aspects encoder-decoder architecture feedback-driven mechanism discriminator learns generated exemplars corresponding class attribute vectors leading improved generator. model’s ability generate leverage examples unseen classes train classiﬁcation model naturally helps mitigate bias towards predicting seen classes generalized zeroshot learning settings. comprehensive experiments show model outperforms several state-of-the-art methods several benchmark datasets standard well generalized zero-shot learning. ability correctly categorize objects previously unseen classes requirement truly autonomous object discovery system. zero-shot learning learning paradigm tries fulﬁl desideratum leveraging auxiliary information available seen/unseen class. models usually assume information given form class attribute vectors textual descriptions classes. typical approaches taken existing models roughly categorized following learning mapping instance space class-attribute space predicting class unseen class test instance ﬁnding closest class-attribute vector deﬁning classiﬁer unseen class weighted combinafigure illustration model like conditional probabilistic encoder-generator reconstruct noisy version original input. feedback mechanism loss attribute regressor well encoder improve reconstruction capability decoder network. dotted lines denote feedback loss orange line points desired location reconstruction feature space. test time predictions made using off-the-shelf classiﬁer trained synthesized examples. criminator also allows operate semi-supervised settings incorporating unlabeled examples know class label model learned used generate exemplars unseen class given classattributes exemplars used labeled training examples train off-the-shelf classiﬁcation model. notably since classiﬁcation model trained using labeled examples seen well unseen classes test time biased towards predicting seen classes gzsl setting also evidenced recent work leveraging synthesized unseen class examples also note that proposed framework ﬁnal classiﬁcation stage directly predicts actual class label test example opposed predicting classattribute vector necessitates nearest neighbor search class label. appealing because nearest neighbor search approach commonly used methods known suffers issues hubness problem problem assume seen classes labeled training examples unseen classes labeled training examples. test examples either exclusively unseen classes seen unseen classes work focus gzsl setting although model applies settings class assume given respective class-attribute vectors {ac}s+u attribute vector class models usually based leveraging class-attribute information transfer information seen classes unseen classes. note seen class labeled training example equivalently denoted ayn} feature vector class-attribute vector pair example. therefore assuming total examples seen classes training data seen classes collectively denoted ayn}ns goal learn classiﬁcation model using learned model predict labels test examples. model shown pictorial illustration fig. fig. based variational autoencoder architecture consists probabilistic encoder model parameters probabilistic decoder model parameters model generator also conditioned class-attribute vector enables synthesize exemplars class simply specifying corresponding classattribute vector along unstructured code work also note that training classiﬁer either original labeled examples seen classes also augmented examples using additional exemplars seen classes well. describe model architecture detail. model shown block-diagram fig. based variational autoencoder consisting encoder decoder/generator note that unlike standard generator’s input consists latent code well class-attribute vector similar conditional architecture enabling generated exemplars different classes distinguishable other. however compared traditional cvae architecture differences motivated goal generator generate exemplars given class surrogate real examples class consists discriminator learns real example generatorsynthesized example corresponding class-attribute vector backpropagating regressor’s loss helps improve generator ensuring generated exemplars representative associated class. regressor also enables using model semisupervised setting training examples labels/class-attribute vectors. examples class-attribute vector replaced output distribution regressor model architecture draws inspiration recent work controllable text generation goal generate text certain desired characteristics positive/negative sentiment specifying binary attribute. contrast setting model conditioned class-attribute vector enabling smoothly transition generation seen unseen classes varying class-attribute vector. moreover focus work text generation goal leverage framework solve generalized problem. model also consists mapping decoder’s output class-attribute vector; mapping learned discriminator multivariate regression model parameters maps decoder’s output respective class-attribute vector feedforward network regressor plays roles model provides feedback generator results generation exemplars discriminated easily; allows using unlabeled examples during training computing probability distribution class-attribute vector. note that proposed model example inﬂuenced sources latent code represents unstructured component class-attribute vector represents structured component. describe model components detail section training final classiﬁer generative model learned generate labeled exemplars class ﬁrst generating unstructured component randomly prior specifying class exemplar generated generating example using generator. generate ﬁxed number exemplars class generated exemplars ﬁnally used train discriminative classiﬁer support vector machine softmax classiﬁer. since stage utilizes labeled examples seen unseen classes approach inherently robust bias towards seen classes also evidenced recent conditional architecture consists conditional generator parameters responsible generating exemplars subsequently used train ﬁnal classiﬁcation model. hence training needs designed generated class conditional distribution nicely approximates true distribution. denoting encoder parameters regressor output distribution loss function given ﬁrst term r.h.s. generator’s reconstruction error second term promotes posterior close prior. model encoder conditional decoder/generator regressor gaussian distributions. also note factorization joint distribution overall latent code components consistent attempt learning disentangled representation discriminator-driven learning described sec. discriminator/regressor improve generator backpropagating error encourages generation exemplars coherent class-attribute perform using couple loss functions. ﬁrst simply assumes regressor optimal parameters reason regressing correct value poor generation generator loss encourages generator create samples regressed attribute vector discriminator correct. also additional term acts regularizer encourages generator generate good class-speciﬁc sample even random drawn prior combined class-attribute akin semi-supervised learning. ensures quality synthesized exemplars used train ﬁnal classiﬁer true data. loss functions help increase coherence classattribute also need enforce independence unstructured component class-attribute encoder ensure sampling distribution obtained figure proposed architecture zero-shot set-up. block represents feedforward neural network. encoder link stochastic similar vae. line blue present direction loss backpropagating improve generator. red-lines present loss direction inference network discriminator/regressor deﬁned probabilistic model parameters feedforward neural network learns example corresponding class-attribute vector regressor learned using sources data note unsupervised loss computed taking latent code sampled prior along class-attribute vector sampled empirical distribution generating exemplar generator distribution taking distributions. expectation w.r.t. phase attributes seen well unseen classes. optimization problem ﬁrst step alternating optimization procedure. optimizes regressor parameters make regressor predict correct class-attribute vector even noisy signal note that step assume generator distribution ﬁxed. overall learning objective conditional autoencoder weighted combination components discussed above effectively optimizing parameters ensure good density estimation. weight hyperparameters tuned optimal reconstruction. completes second step alternating optimization. learning received signiﬁcant amount interest recently. lack space possible comprehensively cover work area. section provide overview representative methods traditional well generalized zsl. early works based learning direct/indirect mapping instances class-attributes. mapping applied test data ﬁrst predict class-attribute vector used predict class ﬁnding similar attribute vector. another popular approach based learning shared embedding seen unseen class instances class-attribute space projection nearest neighbor methods used similar class attribute vector test instance corresponds likely class. conceptually simple easy implement methods suffer shortcomings hubness problem similar vein embedding instances attribute/semantic space approaches learn mapping instances semantic space learning bilinear compatibility function instances attribute space using ranking loss optimizing structural loss learn bilinear compatibility. embedding based methods also extended learn non-linear multi-modal embeddings. using fact class-attributes used compute relationships seen unseen classes number methods proposed based representing parameters representing unseen class similarity-weighted combination parameters representing seen classes generalized problem training test classes disjoint considerably challenging compared traditional recent focus design methods work robustly setting without biased towards predicting seen classes. generative models promising setting. ways models solve gzsl problem generating synthetic labeled examples unseen classes using examples train classiﬁcation model. following approach similar spirit work number recent works tried synthesized examples seen well unseen classes perform generalized zero-shot task. synthesize samples class approximating class conditional distribution unseen classes based learned class conditional distributions seen classes corresponding attribute vectors. hand perform adversarial training train generators domain adapted samples perform classiﬁcation. finally ability generate exemplars unseen class training classiﬁcation models also help mitigate domain-shift problem encountered traditional methods distribution seen classes unseen classes same. given labeled examples seen classes synthetic labeled examples unseen classes supervised/semi-supervised domain adaptation methods readily applied address domain shift problem. despite signiﬁcant amount progress past years would also like point differences evaluation protocols evaluating models often make hard fair comparison various methods. recent work guidelines choosing data-splits evaluations protocols ensure fair comparisons. experimental settings strictly adhere guidelines much possible. test effectiveness model conduct extensive evaluation several benchmark datasets compare various state-of-the-art models. note baselines also include recently proposed methods based exemplar generation report results following benchmark datasets also following guidelines offered evaluating models set). class human-provided -dimensional class-attribute vector. since images original dataset available used features. recently updated version dataset images made available. completeness evaluate model datasets henceforth referred scene recognition dataset comprises scenes. setting widely used split seen classes unseen classes. dataset ﬁne-grained images attributes available image level. combine attributes images class obtain class-level attributes training. caltech ucsd birds dataset consists classes ﬁne-grained images birds. given split unseen seen classes. like dataset attributes available image level average across class class-attributes. imagenet also evaluate zero-shot classiﬁcation accuracy large-scale imagenet dataset. setup involves training using images class ilsvrc data testing non-overlapping classes ilsvrc data. unlike datasets googlenet features dataset. datasets statistics summarized table human-curated attributes class available datasets wordvec representations class class-attribute vector imagenet. mentioned datasets evaluate method commonly used standard split well split proposed recent work evaluation protocols zsl. version dataset also proposed therefore report results dataset well dataset. datasets suggested resnet features images. extra ﬁne-tuning done improve image features. network optimized based loss function discussed sec. using adam optimizer. learning started pretraining using loss followed joint alternating-training regressor encodergenerator pair optimizing loss convergence. hyerparameters chosen based train-validation split used training model complete data. though small variability results based values hyperparameters used λreg worked well experiments. important note architecture used across datasets extra data/feature engineering used/performed improve accuracies thus showcasing efﬁcacy training schedule task. encoder realised using two-hidden layer feedforward network decoder regressor modeled feedforward networks consisting hidden layer hidden units each. evaluation methodology recent guidelines evaluation methodology methods proposed stressed reasons previously reported results might spurious. believe proposed changes evaluation fair hence follow policy report results. fairness comparison reproduced results mentioned shall discuss experimental setup settings experiment with gzsl. datasets available parts labeled examples seen classes unlabeled examples unseen classes generalized zero shot learning gzsl setting involves performing classiﬁcation test examples seen unseen classes prior distinction them. this perform random split dataset obtain test. split done ensuring examples classes. train model train. trained samples synthesized classes using generative model. samples ﬁnally used train multi-class linear svm. examples test used calculate average per-class accuracy deﬁned gzsl setting evaluation measures denoted accy average per-class classiﬁcation test using classiﬁer trained accy average per-class classiﬁcation test using classiﬁer trained following harmonic mean deﬁned average per-class top- accuracy evaluated compiled tables respectively. number samples synthesized hyper-parameter chosen balance evaluation time accuracy. results clearly demonstrate model signiﬁcantly mitigate gzsl issue bias towards seen classes number previous approaches tend suffer outperforms previous approaches unseen class test accuracy accy well harmonic mean measure quantities aggregate performance across seen unseen test classes. conventional setting ﬁrst train generative model synthesize samples unseen classes ﬁnally train multi-class linear using generated training data unseen classes. used predict classes test examples average per-class accuracy accy reported table table improvements consistent across scale complexity images dataset. evident improvement large-scale imagenet dataset well complex ﬁne-grained datasets like cub. large number classes relatively fewer training examples class dataset hamper performance method. quantitative results reported gzsl demonstrate samples generated generative model good quality effective classiﬁcation tasks. gain insight quality generated samples compare empirical distribution generated samples unseen classes empirical distribution real samples classes. done taking generated real examples embedding dimensions using t-sne. shown fig. unseen classes empirical distributions generated real samples overlap signiﬁcantly corroborating model’s ability generates samples look like samples true distribution. finally also perform experiment assess varying number generated examples class affects classiﬁcation accuracy. this vary number generated examples class range examples off-the-shelf classiﬁers linear kernel nearest neighbors. shown fig. expected classiﬁcation accuracies increase increasing number generated examples asymptotes fairly quickly indicating usually small number generated examples sufﬁcient learn fairly accurate classiﬁer. seen classes). exemplars used classiﬁcation model. approach naturally helps solve gzsl problem since learned classiﬁcation model solely dependent labeled data seen classes also leverages synthesized examples unseen classes. model easily leverage unlabeled examples seen and/or unseen classes therefore also operate semi-supervised setting. model results presented strongly demonstrate effectiveness learning continuous space models signiﬁcant power generating exemplars representative true distribution. style generative model case extending adversarial training enhance generated exemplar quality terms sharpness realness noted also believe predictive power regressor naturally improve performance transductive settings exploring part future work. also extended online settings few-shot learning small number acquired labeled samples classes used improving model. presented robust generative framework solve generalized zero shot learning problem. using conditional based architecture augmenting discriminator-driven feedback mechanism enables model generate high-quality class-speciﬁc exemplars unseen classes (and desired also", "year": 2017}