{"title": "Kernel Approximation Methods for Speech Recognition", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "We study large-scale kernel methods for acoustic modeling in speech recognition and compare their performance to deep neural networks (DNNs). We perform experiments on four speech recognition datasets, including the TIMIT and Broadcast News benchmark tasks, and compare these two types of models on frame-level performance metrics (accuracy, cross-entropy), as well as on recognition metrics (word/character error rate). In order to scale kernel methods to these large datasets, we use the random Fourier feature method of Rahimi and Recht (2007). We propose two novel techniques for improving the performance of kernel acoustic models. First, in order to reduce the number of random features required by kernel models, we propose a simple but effective method for feature selection. The method is able to explore a large number of non-linear features while maintaining a compact model more efficiently than existing approaches. Second, we present a number of frame-level metrics which correlate very strongly with recognition performance when computed on the heldout set; we take advantage of these correlations by monitoring these metrics during training in order to decide when to stop learning. This technique can noticeably improve the recognition performance of both DNN and kernel models, while narrowing the gap between them. Additionally, we show that the linear bottleneck method of Sainath et al. (2013) improves the performance of our kernel models significantly, in addition to speeding up training and making the models more compact. Together, these three methods dramatically improve the performance of kernel acoustic models, making their performance comparable to DNNs on the tasks we explored.", "text": "study large-scale kernel methods acoustic modeling speech recognition compare performance deep neural networks perform experiments four speech recognition datasets including timit broadcast news benchmark tasks compare types models frame-level performance metrics well recognition metrics order scale kernel methods large datasets random fourier feature method rahimi recht propose novel techniques improving performance kernel acoustic models. first order reduce number random features required kernel models propose simple effective method feature selection. method able explore large number non-linear features maintaining compact model efﬁciently existing approaches. second present number frame-level metrics correlate strongly recognition performance computed heldout set; take advantage correlations monitoring metrics training order decide stop learning. technique noticeably improve recognition performance kernel models narrowing them. additionally show linear bottleneck method sainath improves performance kernel models signiﬁcantly addition speeding training making models compact. together three methods dramatically improve performance kernel acoustic models making performance comparable dnns tasks explored. recent years deep learning techniques signiﬁcantly advanced state-of-the-art performance automatic speech recognition achieving large drops word error rates deep neural networks able gracefully scale large datasets successfully leverage additional data achieve strong empirical performance. stark contrast kernel methods attractive powerful modeling highly nonlinear data well theoretical learning guarantees tractability scale well. particular data sets size size kernel matrix makes training prohibitively slow typical size resulting models makes deployment impractical. much recent effort devoted development approximations kernel methods primarily nystr¨om approximation random feature expansion methods yield explicit feature representations linear learning methods provide good approximations original non-linear kernel method. however successful applications methods alone head-on comparisons dnns except efforts limited scope paper investigate empirically kernel methods scaled tackle typical tasks. focus four datasets iarpa babel program cantonese bengali limited language packs -hour subset broadcast news timit present several results first show kernel methods efﬁciently scaled large-scale tasks using above-mentioned random fourier feature technique contribution demonstrate practical utility method constructing large-scale classiﬁers acoustic modeling. second found leveraging novel techniques discussed paper kernel-based acoustic models generally competitive layer-wise discriminatively pre-trained dnn-based models order attain strong performance kernel acoustic models developed methods. first propose simple feature selection algorithm effectively reduces number random features required. iteratively select features large pools random features using learned weights selection criterion. clear beneﬁts subsequent training selected features considerably faster training entire pool random features resulting model also much smaller. certain kernels feature selection approach—which applied level random features—can regarded non-linear method feature selection level input features observation motivate design kernel function. second present several novel frame-level metrics correlate strongly token error rate thus monitored heldout training order determine stop learning. using method achieve notable gains kernels dnns. method partially mitigates well-known problem acoustic modeling; namely training criterion often align well true objective case noticed although kernel models would often attain similar cross-entropy values heldout dnns would generally perform better sometimes wide margin terms ter. although sequence training techniques also used address issue computationally expensive generally depend frame-level training initialization; thus proposed method used conjunction existing sequence training techniques providing better initial model. lastly demonstrate importance using linear bottleneck parameter matrix kernel models. method improve performance kernel models signiﬁcantly also makes training faster reduces size models learned. paper builds previous works provide comparisons kernel acoustic models datasets additionally present entropy regularized loss metric show using model selection criterion yield improvements. present feature selection algorithm described paper along experiments datasets comparisons dnns also performed. work current paper builds existing work several ways. first provide extensive experiments including results benchmark timit dataset addition updated results datasets second extended work erll presenting larger metrics correlate strongly ter; additional metrics help explain unusual correlation erll well poor correlation cross-entropy ter. paper show metrics evaluated heldout training order decide decay learning rate stop training. lastly provide extensive experiments showing importance using linear bottleneck attaining strong performance kernel methods. rest paper organized follows. review related work provide background kernel approximation methods well acoustic modeling present feature selection algorithm present several novel metrics correlate strongly show used training improve performance. report extensive experiments comparing dnns kernel methods including results using methods discussed above. conclude scaling kernel methods long-standing actively studied problem kernels sparse feature expansions sonnenburg franc show efﬁciently scale kernel svms datasets million training samples using sparse vector operations parameter updates. approximating kernels constructing explicit ﬁnite-dimensional feature representations products representations approximate kernel function emerged powerful technique nystr¨om method constructs feature maps arbitrary kernels low-rank decomposition kernel matrix shift-invariant kernels random fourier feature technique rahimi recht uses random projections order generate features. random projections also used approximate wider range kernels many recent works developed speed-up random fourier feature approach kernel approximation. line work attempts reduce time needed compute random feature expansions imposing structure random projection matrix also possible doubly-stochastic methods speed-up stochastic gradient training models based random features despite much progress kernel approximation reported empirical studies techniques speech recognition tasks however tasks relatively small-scale part detailed evaluation methods large-scale tasks together thorough comparison dnns lacking. work ﬁlls tackling challenging large-scale acoustic modeling problems deep neural networks achieve strong performance additionally provide number important improvements kernel methods boost performance signiﬁcantly. contribution work introduce feature selection method works well conjunction random fourier features context large-scale multi-class classiﬁcation problems. recent work feature selection methods random fourier features binary classiﬁcation regression problems includes sparse random features algorithm algorithm coordinate descent method smooth convex optimization problems space non-linear features step involves solving batch ℓ-regularized convex optimization problem randomly generated non-linear features here ℓ-regularization cause learned solution depend subset generated features. drawback approach computational burden fully solving many batch optimization problems prohibitive large data sets. attempts implement online variant method using fobos ℓ/ℓ-regularization multi-class setting observed strong regularization required obtain intermediate sparsity turn severely hurt prediction performance. effectively regularization strong made learning meaningless selected features basically random. approach selecting random features efﬁcient directly ensures sparsity regularization. another improvement propose alters frame-level training acoustic model order improve recognition performance ﬁnal model. methods typically referred sequence training techniques share goal tuning acoustic model purpose improving recognition performance. number different sequence training criteria proposed including maximum mutual information boosted minimum phone error minimum bayes risk methods though originally proposed training gaussian mixture model acoustic models also used neural network acoustic models nonetheless methods quite computationally expensive typically initialized acoustic model trained frame-level cross-entropy criterion. method contrast simple making small change frame-level training process. furthermore used conjunction above-mentioned sequence training techniques providing better initial model. recently povey showed possible train acoustic model using sequence-training methods lattice-free version criterion. future work would like much kernel models beneﬁt various sequence training methods mentioned above relative dnns. work also contributes debate relative strengths deep shallow neural networks. explained section many types kernels understood shallow neural networks. such comparing kernel methods dnns also sense comparing shallow deep neural networks. much literature topic. classic results show deep shallow neural networks universal approximators meaning approximate real-valued continuous function bounded support arbitrary degree precision however number papers argued exist functions deep neural networks express exponentially fewer parameters shallow neural networks caruana authors show performance shallow neural networks increased considerably training match outputs deep neural networks. showing kernel methods compete dnns large-scale speech recognition tasks paper adds credence argument shallow networks perform deep networks. kernel methods broadly speaking machine learning techniques either explicitly implicitly data input space feature space linear model learned. kernel function deﬁned function takes input returns dot-product corresponding points denote feature space standard kernel methods avoid inference generally high-dimensional even inﬁnite-dimensional space. instead solve dual problem using n-by-n kernel matrix containing values kernel function applied pairs training points. greater kernel trick provides nice computational advantage. however exceedingly large size kernel matrix makes training impractical. rahimi recht address problem leveraging bochner’s theorem classical result harmonic analysis order provide fast approximate positive-deﬁnite shift-invariant kernel motivates sampling-based approach approximating kernel function. concretely draw independently distribution independently uniform distribution parameters approximate kernel follows efﬁciency relative standard kernel methods large datasets. learning representation relatively efﬁcient provided less number training samples example experiments million million training samples often leads good performance. rahimi recht prove number important theoretical results random high probability feature approximations. first show compact subset bounded diameter. claim follow-up work authors prove generalization bound models learned using random features. show high-probability excess risk assumed using approximation relative using oracle kernel model bounded details). given generalization error model trained using exact kernel methods known within oracle model implies worst case random features required order approximated model achieve generalization performance comparable exact kernel model. empirically however fewer features often needed order attain strong performance neural network acoustic models provide conditional probability distribution possible acoustic states conditioned acoustic frame encoded feature representation. acoustic states correspond context-dependent phoneme states modern speech recognition systems number states order acoustic model used within probabilistic systems decoding speech signals word sequences. typically probability model used hidden markov model model’s emission transition probabilities provided acoustic model together language model. bayes’ rule order compute probability emitting certain acoustic feature vector state given output neural network note ignored inference time doesn’t affect relative scores assigned different word sequences simply prior probability state viterbi algorithm used determine likely word sequence overview using hmms speech recognition). label take value corresponding context-dependent phonetic state label parameter matrix learned. note also include bias term appending equation above. model equation seen shallow neural network following properties parameters inputs hidden units randomly learned; hidden units activation function; parameters hidden units output units learned softmax function used normalize outputs network. figure visual representation model architecture. number phonetic state labels large. signiﬁcantly increase number parameters reduce number linear bottleneck layer hidden layer output layer; linear bottleneck corresponds low-rank factorization parameter matrix particularly important kernel models number trainable parameters number random features number output classes. using linear bottleneck size reduced signiﬁcantly less min. strictly decreases capacity resulting model unfortunately rendering optimization problem non-convex. section ﬁrst motivate describe proposed feature selection algorithm. introduce sparse gaussian kernel performs well conjunction feature selection algorithm. proposed random feature selection method shown algorithm based general iterative framework. iteration random features generated added current features; subset features selected rest discarded. selection process works follows ﬁrst model trained current features using single pass stochastic gradient descent subset training data. then features whose corresponding rows largest norms kept. note weights corresponding feature model case using linear bottleneck decompose perform training using decomposition. complete training given iteration compute select features based ℓ-norms rows feature selection method following advantages overall computational cost mild requires passes subsets data size fact experiments sufﬁcient moreover method able explore large number non-linear features maintaining compact model. dt/t learning algorithm exposed random features throughout feature selection process; initialize parameter matrix learn weights rd×c using single pass randomly selected training examples using projection vectors biases generate random fourier features. amongst rows highest norm}. latter method solves ℓ/ℓ-regularized problem stochastic fashion alternating taking unregularized stochastic gradient descent steps shrinking rows parameter matrix; time parameters shrunk rows whose ℓ-norms threshold training completes solution likely rows zero point features corresponding rows discarded. method hand take many consecutive unregularized steps thereafter choose discard rows whose ℓ-norm threshold. mentioned related work section attempts using fobos feature selection failed magnitude regularization parameter needed order produce sparse model large dominated learning process; result models learned performed badly selected features essentially random. disadvantage method index used selection misrepresent features’ actual predictive utilities. instance presence random feature increase decrease weights random features relative would feature present. alternative would consider features isolation features time would signiﬁcantly computationally expensive. example would require passes data relative passes would prohibitive large values. empirically inﬂuence additional random features selection criterion tolerable still possible select useful features method. recall table laplacian kernel sampling distribution used random fourier draw two-sided tail distribution hence typically contain entries much larger rest. property sampling distribution implies many random features generated effectively concentrate input features. thus regard random features non-linear combinations small number original input features. thus proposed feature selection method effectively picks useful non-linear interactions small sets input features. also directly construct sparse non-linear combinations input features. instead relying properties cauchy distribution actually choose small number coordinates uniformly random choose random vector always zero positions outside non-linearity applied sparse random vector chosen. compared random fourier feature approximation laplacian kernel vectors chosen truly sparse make random feature expansion computationally efﬁcient apply note random fourier features sparse sampling distributions fact correspond shiftinvariant kernels rather different laplacian kernel. instance non-zero entries vector composed elements kernel equation puts equal emphasis input feature subsets size however feature selection process effectively bias distribution feature subsets concentrate small family input feature subsets. discussed introduction well-known problem training acoustic models; namely training criterion perfectly correlate true objective consequently lowering cross-entropy performance heldout necessarily result better performance. example noticed dnns often attaining stronger performance kernel models even though comparable cross-entropy performance. order partially address problem section present several metrics whose empirical correlation amongst fully trained kernel models trained high. leverage metrics training evaluating heldout epoch order decide decay learning rate stop training note reason metrics proxies instead directly using expensive compute development set. entropy regularized loss loss rewards models conﬁdent considering weighted cross entropy loss average entropy model heldout data. speciﬁcally deﬁne loss follows effectively loss ensures single example contributes loss. small positive number loss similar normal loss values close affecting loss dramatically values close notice metrics simplify standard loss. figure show plots empirical correlations metrics values function metric’s hyperparameters based models trained. speciﬁcally fully train large number kernel models evaluate performance models development well compute heldout performance models terms metrics described precise models used tables compute empirical correlations values plot function metric’s hyperparameters. note top-k loss plot correlation function fraction seen plots certain ranges values metric hyperparameters correlation metrics quite high. example correlations entropy regularized loss bengali cantonese timit respectively. compared correlations cross-entropy objective. based analysis reasonable thing would heldout entropy regularized loss stopping criteria instead standard cross-entropy loss. practice results training continuing past point lowest heldout cross-entropy producing models lower heldout entropy hopefully lower well. similarly could heldout entropy regularized loss instead heldout cross-entropy order decide decay learning rate. experiments. results reported section experiments. note could also used metrics purpose chose erll since observed attained high correlation values models trained across datasets section ﬁrst provide description datasets evaluation criteria. give overview training procedure provide details regarding hyperparameter choices. present experimental results comparing performance kernel approximation methods dnns demonstrating effectiveness using linear bottlenecks performing feature selection using early stopping criteria bringing ter. lastly take deeper look dynamics feature selection process. train dnns kernel-based multinomial logistic regression models described predict context-dependent state labels acoustic feature vectors. test methods four datasets. dataset partitioned four training heldout development test set. heldout tune hyperparameters training procedure decoding development using ibm’s attila speech recognition toolkit select small subset models perform best terms tune acoustic model weight order optimize relative contributions language model acoustic model ﬁnal score system assigns given word sequence. finally decode test using select group models bengali limited language packs. pack contains training development sets approximately hours approximately hour test set. designate training data heldout set. training heldout development test sets contain different speakers. babel data challenging two-person conversations people know well recorded telephone channels speakers wide variety acoustic environments including moving vehicles public places. result contains many natural phenomena mispronunciations disﬂuencies laughter rapid speech background noise channel variability. additional challenge babel data available training language models acoustic transcripts comparatively small. third dataset -hour subset broadcast news well-studied benchmark task community hours audio used training hours used heldout set. development devf dataset provided consists hours broadcast news various news shows. darpa ears english broadcast news evaluation test consisting -minute conversations. last dataset timit contains recordings speakers various english dialects reciting sentences total hours speech. training consists data speakers reciting sentences development consists speech speakers. evaluation core test consists utterances total speakers reference exact features labels divisions dataset huang allows direct comparison results theirs. language models n-gram language models estimated using modiﬁed kneser-ney smoothing values bengali broadcast news cantonese timit respectively. timit language model phone-level model. bengali cantonese language models particularly small trained using provided audio transcripts. broadcast news model small well containing million -grams. acoustic features representing acoustic frames context real-valued dense vectors. shift used adjacent frames cantonese bengali broadcast news datasets standard -dimensional speaker-adapted representation used state labels obtained forced alignment using gmm/hmm system. timit experiments dimensional feature space maximum likelihood linear regression features concatenate neighboring frames either direction total frames features. states clustered using decision trees. broadcast news states. timit dataset context-independent labels corresponding beginning middle phonemes. datasets number training points signiﬁcantly exceeds typical machine learning tasks tackled kernel methods. particular training sets contain millions frames. additionally large number output classes datasets presents scalability challenge given size kernel models scales linearly number output classes table token error rate feed predictions acoustic models correspond probability distributions phonetic states rest pipeline calculate misalignment decoder’s outputs ground-truth transcriptions. bengali measure error terms word error rate cantonese character error rate timit phone error rate term token error rate refer dataset corresponding metric. kernel models trained either laplacian gaussian sparse gaussian kernel. kernel models typically hyperparameters kernel bandwidth number random projections initial learning rate optimization procedure. rule thumb good values kernel bandwidths range times median pairwise distances data. various numbers random features ranging using random features leads better approximation kernel function well powerful models though diminishing returns number features increases. sparse gaussian kernel additionally hyperparameter speciﬁes sparsity random projection vector experiments dnns tune hyperparameters related architecture optimization. includes number layers number hidden units layer learning rate. perform epoch layer-wise discriminative pre-training train entire network jointly using sgd. hidden layers generally best setting dnns results present paper setting. additionally dnns tanh activation function. vary number hidden units layer kernel models stochastic gradient descent optimization algorithm mini-batch size samples. heldout tune hyperparameters learning rate decay scheme described monitors performance heldout order decide decay learning rate. method divides learning rate half epoch heldout cross-entropy doesn’t improve least additionally heldout cross-entropy gets worse reverts model back state beginning epoch. instead using heldout cross-entropy experiments heldout erll order decide decay learning rate. mentioned section effective reducing number parameters models impose low-rank constraint output parameter matrix; refer linear bottleneck bottlenecks size bengali cantonese timit respectively. train models without technique; exception unable train kernel models without bottleneck size memory constraints gpus. suggested glorot bengio here dout refer dimensionality input output layer respectively. kernel models initialize random projection matrix discussed section initialize parameter matrix zero matrix. using linear bottleneck decompose parameter matrix initialize resulting matrices randomly like dnns. ultimately training examples random features selected. thus iteration feature selection equivalent computational cost fraction epoch iterations feature selection iteration select random features. thus total computational cost incur feature selection equivalent approximately seven epochs training babel data sets. broadcast news dataset corresponds cost gaussian kernel take median squared distances large number random pairs training examples. laplacian kernel distances instead. sparse gaussian kernel median squared distances randomly chosen sub-vectors size random pairs training points. table kernel results table shows results kernel experiments using either laplacian gaussian sparse gaussian kernels. ‘nt’ speciﬁes tricks used training speciﬁes linear bottleneck used parameter matrix; speciﬁes entropy regularized loss used learning rate decay ‘+fs’ speciﬁes feature selection used experiments row. best result bold. table results table shows results dnns hidden units layer. ‘nt’ speciﬁes tricks used training speciﬁes linear bottleneck used output parameter matrix; speciﬁes entropy regularized loss used learning rate decay best result language bold. section report results experiments comparing kernel methods deep neural networks tasks. report results datasets using various combinations methods discussed previously. kernel methods train models without linear bottlenecks without using erll determine learning rate decay. kernel methods additionally train models without using feature selection. experiments three kernels random features datasets expect timit able random features mentioned previous section experiments train models hidden layers using tanh activation function using either hidden units layer. focus comparing performance methods terms also report results metrics. unless speciﬁed otherwise results development cross-entropy entropy classiﬁcation error erll results heldout set. tables show results kernel models respectively across datasets. many things notice results. within kernel models incorporating linear bottleneck brings large drops across board. performing feature selection generally improves well; improves considerably laplacian kernel modestly sparse gaussian kernel. gaussian kernel typically helps though several instances feature selection hurts second using heldout erll determine decay learning rate helps kernel models attain lower values. next without using feature selection sparse gaussian kernel best performance across board. include feature selection performs comparably laplacian kernel feature selection. interesting note without using feature selection gaussian kernel generally better laplacian kernel; however feature selection laplacian kernel surpasses gaussian kernel general kernel function performed best across majority settings sparse gaussian kernel. models linear bottlenecks almost always lower values though cases effect ter. using erll determine decay learning rate generally helps lower values dnns cases actually hurts dnns hidden units typically attain best results though couple datasets matched narrowly beaten models. terms heldout cross-entropy classiﬁcation error kernels dnns model across metrics. performed similarly kernels outperforming dnns cantonese timit dnns beat kernels bengali bn-. terms average heldout entropy models dnns consistently conﬁdent predictions kernels. signiﬁcantly observe best development results kernel models quite comparable; cantonese timit kernel models outperform dnns absolute whereas bengali better respectively. discuss results test sets. first order avoid overﬁtting test dataset performed test evaluations kernel models performed best terms development ter. ﬁnal table thus contains test results collected. relative performance kernel models similar development results dnns performing better bengali kernels performing better cantonese timit. direct comparison include table test results best kernel models huang mentioned section features labels data partitions decoding script huang thus results directly comparable. achieve absolute improvement kernel model relative huang performs theirs. furthermore beat kernel kernel beats ter. linear bottleneck typically causes large drops average entropy kernel models strong consistent effect cross-entropy. dnns bottleneck typically causes increases cross-entropy relatively modest decreases entropy. using erll determine learning rate decay typically causes increases cross-entropy decreases entropy decrease entropy typically larger increase crossentropy. result erll typically lower models method feature selection typically results large drops cross-entropy especially laplacian sparse gaussian kernels effect entropy quite small. thus helps lower heldout erll across board well vast majority cases. illustrate importance number random features ﬁnal performance model. purpose trained number different models dataset using trained models using different kernels without feature selection. used linear bottleneck size models used heldout crossentropy determine learning rate decay. figure show increasing number features dramatically improves performance learned model terms cross-entropy ter; diminishing returns however small improvements increasing furthermore size dashed solid lines indicates importance feature selection attaining strong performance. large laplacian kernel modest sparse gaussian kernel relatively insigniﬁcant gaussian kernel. figure performance kernel acoustic models dataset function number random features used. results reported terms heldout cross-entropy well development ter. dashed lines signify feature selection performed solid lines mean not. color shape markers indicate kernel used. explore dynamics feature selection process. method guarantee feature selected iteration selected next. figure plot fraction features selected iteration actually remain model iterations. show results cantonese plots datasets qualitatively similar. nearly iterations kernels half selected features survive ﬁnal model. instance laplacian kernel features selected iteration survive remaining rounds selection. comparison also plot expected fraction features selected iteration would survive selected features iteration chosen uniformly random pool. since dt/t expected fraction iteration exponentially small ﬁxed example expected survival rate approximately combinations input features. consider ﬁnal matrix random vectors |θ|··· rd×d random feature selection. coarse measure much inﬂuence input feature ﬁnal feature relative weight i-th figure dpij |θij| normalization term. strong periodic effect function input feature number. reason stems acoustic features generated. recall features concatenation nine -dimensional acoustic feature vectors nine audio frames. examination feature pipeline kingsbury reveals features ordered measure discriminative quality thus expected features value useful others; indeed evident plot. note effect exists extremely weak gaussian kernel. believe gaussian random vectors likely important mention things regarding ways performance kernel models could improved investigated length work. kernel methods given optimization convex bottleneck used would possible stronger convergence guarantees using stochastic average gradient algorithm instead training fact cantonese bengali attained strong recognition performance. unfortunately challenging scale algorithm larger tasks since requires storing every training example previous gradient loss function example. ﬁxed gradient information stored simply storing training example vector however still takes storage quite expensive millions training examples thousands output classes unfortunately bottleneck introduced optimization problem non-convex must also store full gradients thus making memory requirement large. result scalability reasons well details regarding experiments feature selection work used erll model selection criterion additionally instead training large kernel models jointly trained blocks random features combined models logit averaging consistency across experiments used kernel experiments. additionally investigate sequence training techniques kernel methods leaving future work. often improves recognition performance additionally various deep architectures long short term memory networks well numerous training techniques dropout batch normalization improve performance neural networks. intention paper provide comparison kernel methods strong baseline provide exhaustive comparison possible deep learning architectures optimization methods. paper explore performance kernel methods large-scale tasks leveraging kernel approximation technique rahimi recht propose methods lead large improvements performance kernel acoustic models. show using linear bottleneck decompose parameter matrix kernel models leads signiﬁcant improvements well. replicate ﬁndings four different datasets including broadcast news timit benchmark tasks. linear bottleneck well learning rate decay method also typically improve performance acoustic models. using methods conjunction kernel methods attain comparable values dnns across four test sets; cantonese timit kernel models outperform dnns absolute respectively whereas bengali better future work interested number questions develop techniques feature selection learning kernel function effectively? kernel methods robust dnns different types input much performance kernel models improve using sequence training methods? sequence kernels used improve recognition performance kernel acoustic models manner analogous lstms give improvements dnns? kernel methods compete well dnns domains outside speech recognition? broadly speaking biggest limitations kernel methods overcome? research supported intelligence advanced research projects activity department defense u.s. army research laboratory contract number wnf--c-. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. disclaimer views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied iarpa dod/arl u.s. government. additionally partially supported provost graduate fellowship. partially supported iis- google research award alfred. sloan research fellowship award award wnf---. a.b. partially supported grant cper nord-pas calais/feder data advanced data science technologies appendix include tables comparing models trained terms different metrics notation tables kernel models ‘nt’ speciﬁes tricks used training speciﬁes linear bottleneck used output parameter matrix speciﬁes entropy regularized loss used learning rate decay kernel models ‘+fs’ speciﬁes feature selection performed corresponding row. best result metric language bold.", "year": 2017}