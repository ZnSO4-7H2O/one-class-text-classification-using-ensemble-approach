{"title": "Playing FPS Games with Deep Reinforcement Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in AI agents of the game as well as humans in deathmatch scenarios.", "text": "play atari games. foerster consider multi-agent scenario deep distributed recurrent neural networks communicate different agent order solve riddles. recurrent neural networks effective scenarios partially observable states ability remember information arbitrarily long amount time. previous methods usually applied environments hardly resemble real world. paper tackle task playing first-person-shooting game environment. task much challenging playing atari games involves wide variety skills navigating collecting items recognizing ﬁghting enemies etc. furthermore states partially observable agent navigates environment ﬁrst-person perspective makes task suitable real-world robotics applications. paper present ai-agent playing deathmatches games using pixels screen. agent divides problem phases navigation action uses separate networks phase game. furthermore agent infers high-level game information advances deep reinforcement learning allowed autonomous agents perform well atari games often outperforming humans using pixels make decisions. however games take place environments fully observable agent. paper present ﬁrst architecture tackle environments ﬁrst-person shooter games involve partially observable states. typically deep reinforcement learning methods utilize visual input training. present method augment models exploit game feature information presence enemies items training phase. model trained simultaneously learn features along minimizing q-learning objective shown dramatically improve training speed performance agent. architecture also modularized allow different models independently trained different phases game. show proposed architecture substantially outperforms built-in agents game well average humans deathmatch scenarios. deep reinforcement learning proved successful mastering human-level control policies wide variety tasks object recognition visual attention high-dimensional robot control solving physics-based control problems particular deep qnetworks shown effective playing atari games recently defeating world-class players however limitation applications assumption full knowledge current state environment usually true real-world scenarios. case partially observable states learning agent needs remember previous states order select optimal actions. recently attempts handle partially observable states deep reinforcement learning introducing recurrency deep q-networks. example hausknecht stone deep recurrent neural network particularly long-shortterm-memory network learn q-function instead performing q-learning updates online fashion popular experience replay break correlation successive samples. time steps agent experiences stored replay memory q-learning updates done batches experiences randomly sampled memory. every training step next action generated using \u0001-greedy strategy probability next action selected randomly probability according network best action. practice common start progressively decay model assumes step agent receives full observation environment opposed games like atari games actually rarely return full observation since still contain hidden variables current screen buffer usually enough infer good sequence actions. partially observable environments agent receives observation environment usually enough infer full state system. game like doom agent ﬁeld view limited centered around position obviously falls category. deal environments hausknecht stone introduced deep recurrent q-networks extra input returned network previous step represents hidden state agent. recurrent neural network like lstm implemented normal model that. case lstm estimate model built drqn architecture. presence enemies screen decide current phase improve performance. also introduce method co-training game features turned critical guiding convolutional layers network detect enemies. show co-training signiﬁcantly improves training speed performance model. evaluate model different tasks adapted visual doom competition using developed kempka gives direct access doom game engine allows synchronously send commands game agent receive inputs current state game. show proposed architecture substantially outperforms built-in agents game well humans deathmatch scenarios demonstrate importance component architecture. deep q-networks reinforcement learning deals learning policy agent interacting unknown environment. step agent observes current state environment decides action according policy observes reward signal goal agent policy maximizes expected discounted rewards time game terminates discount factor determines importance future rewards. q-function given policy deﬁned expected return executing action state common function approximator estimate action-value function particular uses neural network parametrized idea obtain estimate q-function current policy close optimal q-function deﬁned highest return expect achieve following strategy figure illustration architecture model. input image given convolutional layers. output convolutional layers split streams. ﬁrst ﬂattens output feeds lstm drqn model. second projects extra hidden layer ﬁnal layer representing game feature. training game features q-learning objectives trained jointly. ﬁrst approach solving problem baseline drqn model. although model achieved good performance relatively simple scenarios perform well deathmatch tasks. resulting agents ﬁring will hoping enemy come lines ﬁre. giving penalty using ammo help small penalty agents would keep ﬁring would never ﬁre. game feature augmentation reason agents able accurately detect enemies. vizdoom environment gives access internal variables generated game engine. modiﬁed game engine returns every frame information visible entities. therefore step network receives frame well boolean value entity indicating whether entity appears frame although internal information available test time exploited training. modiﬁed drqn architecture incorporate information make sensitive game features. initial model output convolutional neural network given lstm predicts score action based current frame hidden state. added fully-connected layers size connected output number game features want detect. training time cost network combination normal drqn cost cross-entropy loss. note lstm takes input output never directly provided game features. illustration architecture presented figure although game information available used indicator presence enemies current frame. adding game feature dramatically improved performance model every scenario tried. figure shows performance drqn without game features. explored architectures incorporate game features using separate network make predictions reinjecting predicted features lstm achieve results better initial baseline suggesting sharing convolutional layers decisive performance model. jointly training drqn model game feature detection allows kernels convolutional layers capture relevant information game. experiments takes hours model reach optimal enemy detection accuracy that lstm given features often contain information presence enemy positions resulting accelerated training. augmenting drqn model game features straightforward. however method applied easily model. indeed important aspect model sharing convolution ﬁlters predicting game features q-learning objective. drqn perfectly adapted setting since network takes input single frame predict visible speciﬁc frame. however model network receives frames time step predict whether features appear last frame only independently content previous frames. convolutional layers perform well setting even dropout never obtained enemy detection accuracy using model. reward shaping score deathmatch scenario deﬁned number frags i.e. number kills minus number suicides. reward based score replay table extremely sparse w.r.t state-action pairs non-zero rewards makes difﬁcult agent learn favorable actions. moreover rewards extremely delayed usually result speciﬁc action getting positive reward requires agent explore enemy accurately shoot slow projectile rocket. delay reward makes difﬁcult agent learn actions responsible reward. tackle problem sparse replay table delayed rewards introduce reward shaping i.e. modiﬁcation reward function include small intermediate rewards speed learning process addition positive reward kills negative rewards suicides introduce following intermediate rewards shaping reward function action network positive reward object pickup approach agent receives screen input every frames number frames skipped step. action decided network repeated skipped frames. higher frame-skip rate accelerates training hurt performance. typically aiming enemy sometimes requires rotate degrees impossible frame skip rate high even human players agent repeat rotate action many times ultimately rotate intended frame skip turned best trade-off. sequential updates perform drqn updates different approach presented hausknecht stone sequence observations randomly sampled replay memory instead updating action-states sequence consider ones provided enough history. indeed ﬁrst states sequence estimated almost non-existent figure updates lstm. scores actions taken states updated. first four states provide accurate hidden state lstm last state provide target state deathmatch task typically divided phases involves exploring collect items enemies consists ﬁghting enemies call phases navigation action phases. networks work together trained speciﬁc phase game naturally lead better overall performance. current models allow combination different networks optimized different tasks. however current phase game determined predicting whether enemy visible current frame inferred directly game features present proposed model architecture. various advantages splitting task phases training different network phase. first makes architecture modular allows different models trained tested independently phase. networks trained parallel makes training much faster compared training single network whole task. furthermore navigation phase requires three actions dramatically reduces number state-action pairs required learn q-function makes training much faster importantly using networks also mitigates camper behavior i.e. tendency stay area wait enemies exhibited agent tried train single drqn deathmatch task. trained different networks agent. used drqn augmented game features action network simple navigation network. during evaluation action network called step. enemies detected current frame agent ammo left navigation network called decide next action. otherwise decision given action network. results table demonstrate effectiveness navigation network improving performance agent. figure plot score action network limited deathmatch function training time without dropout without game features different number updates lstm. table comparison human players agent. single player scenario humans agent playing bots separate games. multiplayer scenario agent human playing game. prevent problem errors states o...oh minimum history size state updated backpropagated network. errors states oh+..on− backpropagated used create target action-state. illustration updating process presented figure experiments minimum history size perform updates states. figure shows importance selecting appropriate number updates. increasing number updates leads high correlation sampled frames violating random sampling policy decreasing number updates makes difﬁcult network converge good policy. hyperparameters networks trained using rmsprop algorithm minibatches size network weights updated every steps experiences sampled average times training replay memory contained million recent frames. discount factor used \u0001-greedy policy training linearly decreased ﬁrst million steps ﬁxed different screen resolutions game lead different ﬁeld view. particular resolution provides degree ﬁeld view resolution doom degree ﬁeld view order maximize agent game awareness used resolution resized although faster model obtained lower performance using grayscale images decided colors experiments. scenario vizdoom platform conduct experiments evaluate methods deathmatch scenario. scenario agent plays built-in doom bots ﬁnal score number frags i.e. number bots killed agent minus number suicides committed. consider variations scenario adapted vizdoom competition limited deathmatch known map. agent trained evaluated available weapon rocket launcher. agents gather health packs ammo. full deathmatch unknown maps. agent trained tested different maps. agent starts pistol pick different weapons around well gather health packs ammo. maps training maps testing. randomize textures maps training improved generalizability model. limited deathmatch task ideal demonstrating model design effectiveness chose hyperparameters training time signiﬁcantly lower full deathmatch task. order demonstrate generalizability model full deathmatch task show model also works effectively unknown maps. evaluation metrics evaluation deathmatch scenarios kill death ratio scoring metric. since ratio susceptible camper behavior minimize deaths also report number kills determine agent able explore enemies. addition these also table performance agent in-built game bots without navigation. agent evaluated minutes map. performance full deathmatch task averaged train maps test maps. report total number objects gathered total number deaths total number suicides suicides caused agent shoots close itself weapon blast radius like rocket launcher. since suicides counted deaths provide good penalizing score agent shooting arbitrarily. results analysis demonstrations navigation deathmatch known unknown maps available here. arnold agent trained using proposed action-navigation architecture placed second tracks visual doom competition highest ratio navigation network enhancement. scores tasks without navigation presented table agent evaluated minutes maps results averaged full deathmatch maps. scenarios total number objects picked dramatically increases navigation well ratio. full deathmatch agent starts pistol relatively difﬁcult kill enemies. therefore picking weapons ammo much important full deathmatch explains larger improvement ratio scenario. improvement limited deathmatch scenario limited relatively small since many bots navigating crucial agents. however agent able pick three times many objects health packs ammo navigation. able heal regularly agent decreased number deaths improved ratio. note scores across different tasks comparable difference sizes number objects different maps. performance test maps better training maps necessarily surprising given maps look different. particular test maps contain less stairs differences level usually difﬁcult network handle since train look down. comparison human players. table shows agent outperforms human players single player multiplayer scenarios. single player scenario human players agent play separately bots limited deathmatch three minutes. multiplayer scenario human players agent play minutes. human scores averaged human players scenarios. note suicide rate humans particularly high indicating difﬁcult humans accurately limited reaction time. game features. detecting enemies critical agent’s performance trivial task enemies appear various distances different angles different environments. including game features training resulted signiﬁcant improvement performance model shown figure hours training best score network without game features less network game features able achieve maximum score another advantage using game features gives immediate feedback quality features given convolutional network. enemy detection accuracy lstm receive relevant information presence enemies frame qlearning network struggle learn good policy. enemy detection accuracy takes hours converge training whole model takes week. since enemy detection accuracy correlates ﬁnal model performance architecture allows quickly tune hyperparameters without training complete model. instance enemy detection accuracy withdropout quickly converged respectively allowed infer dropout crucial effective performance model. figure supports inference using dropout layer signiﬁcantly improves performance action network limited deathmatch. explained section game features surprisingly don’t improve results used input used co-training. suggests cotraining might useful application even independent image classiﬁcation tasks like cifar. mcpartland gallagher tastan sukthankar divide tasks navigation combat games present reinforcement learning approaches using game-engine information. koutn´ık previously applied recurrent neural network learn torcs racing video game pixels only. kempka previously applied vanilla simpler scenarios within doom provide empirical study effect changing number skipped frames training testing performance dqn. paper presented complete architecture playing deathmatch scenarios games. introduced method augment drqn model high-level game information modularized architecture incorporate independent networks responsible different phases game. methods lead dramatic improvements standard drqn model applied complicated tasks like deathmatch. showed proposed model able outperform built-in bots well human players demonstrated generalizability model unknown maps. moreover methods complementary recent improvements could easily combined dueling architectures prioritized replay would like acknowledge sandeep subramanian kanthashree mysore sathyendra valuable comments suggestions. thank students carnegie mellon university useful feedback helping testing system. finally thank zdoom community help utilizing doom game engine.", "year": 2016}