{"title": "A Neural Transfer Function for a Smooth and Differentiable Transition  Between Additive and Multiplicative Interactions", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Existing approaches to combine both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. This leads either to an inefficient distribution of computational resources or an extensive increase in the computational complexity of the training procedure.  We present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiablely adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure.", "text": "existing approaches combine additive multiplicative neural units either ﬁxed assignment operations require discrete optimization determine function neuron perform. leads either ineﬃcient distribution computational resources extensive increase computational complexity training procedure. present novel parameterizable transfer function based mathematical concept non-integer functional iteration allows operation neuron performs smoothly importantly diﬀerentiablely adjusted addition multiplication. allows decision addition multiplication integrated standard backpropagation training procedure. commonplace artiﬁcial neural networks value neuron given weighted inputs propagated non-linear transfer function. illustration consider simple neural network multidimensional input multivariate output. input layer called outputs value neuron showed additive anns least hidden layer sigmoidal transfer function able approximate function arbitrarily well given suﬃcient number hidden units. even though additive universal function approximator guarantee approximate function eﬃciently. architecture good match particular problem large number neurons required obtain acceptable results. proposed alternative neural unit weighted summation replaced product input raised power determined corresponding weight. value product unit given taken element-wise. since general incoming values negative complex exponential logarithm used. often non-linearity applied output product unit. typical choice transfer function sigmoid function approximation thereof. matrix multiplication used jointly compute values neurons layer eﬃciently; types neurons combined hybrid summation-multiplication network. poses problem distribute additive multiplicai.e. determine tive units network whether speciﬁc neuron additive multiplicative unit obtain best results. simple solution stack alternating layers additive product units optionally additional connections skip product layer additive layer receives inputs product layer additive layer beneath drawback approach resulting uniform distribution product units hardly ideal. adaptive approach learn function neural unit provided training data. however since addition multiplication diﬀerent operations obvious determine best operation training network using standard neural network optimization methods backpropagation. iterative algorithm determine optimal allocation could following structure initialization randomly choose operation neuron performs. train network minimizing error function evaluate performance validation set. based performance determine allocation using discrete optimization algorithm iterate process satisfactory performance achieved. drawback method computational complexity; evaluate allocation operations whole network must trained takes minutes hours moderately sized problems. propose alternative approach distinction additive multiplicative neurons discrete continuous diﬀerentiable. hence optimal distribution additive multiplicative units determined standard gradient-based optimization. approach organized follows first introduce non-integer iterates exponential function real complex domains. iterates smoothly interpolate addition multiplication finally show interpolation integrated implemented neural networks. equation cannot used deﬁne functional iteration non-integer thus order calculate non-integer iterations function alternative deﬁnition. sought generalization also extend additive property seen generalization functional iteration non-integer iterates. easily veriﬁed composition property holds. hence understand function function gives exponential function applied itself. called functional square root motivated necessity evaluate logarithm negative arguments derive solution abel’s equation complex exponential function. applying substitution derive solution examine behavior around ﬁxed points. ﬁxed point function point property exponential function inﬁnite number ﬁxed points. select ﬁxed point closest real axis upper complex half plane. since contraction mapping according banach ﬁxed-point theorem ﬁxed point found starting arbitrary given respectively. since injective think mapping complex plane called z-plane another complex plane called χ-plane. operation calculating exponential number z-plane corresponds complex multiplication factor χ-plane. illustrated fig. samples shown fig. neuron behaves like additive neuron computes product inputs. dependence parameter neuron calculation corresponds network neuron layer separate outputs neuron following layer fig. compared conventional neural nets architecture additional real-valued parameter neuron also poses signiﬁcant increase computational complexity necessity separate outputs. since corresponds architecture shown fig. interpolation parameter split pre-transfer-function part ˆnyi post-transferfunction part ˜nxj since ˆnyi ˜nxj tied together network free implement arbitrary combinations iterates exponential function. compared conventional neural nets neuron additional parameters namely ˆnyi ˜nxj however asymptotic computational complexity network unchanged. fact architecture corresponds conventional additive neural deﬁned neuron-dependent parameterizable transfer function. neuron transfer function given figure interpolation addition multiplication using iterates exponential function either calculated using abel’s equation schr¨oder’s equation cases interpolated values exceed range bepositive arguments iterates based either solution abel’s equation schr¨oder’s equation however also want deal negative arguments must iterates based schr¨oder’s equation since real logarithm deﬁned positive arguments. exemplary addiplication shown fig. interpolations produced methods monotonic functions w.r.t. interpolation parameter cases local maxima exist; however interpolation based schr¨oder’s equation higher extrema case also general well known existence local extrema pose problem gradient-based optimizers. consequently implementation existing neural network frameworks possible replacing standard sigmoidal transfer function function optionally using complex weight matrix. output pattern circularly shifted right grid cells. neural hidden layers solve problem employing fourier shift theorem. ﬁrst hidden layer additive second multiplicative; output layer additive. neurons linear transfer functions. ﬁrst hidden layer computes input pattern shift mount second hidden layer applies fourier shift theorem output layer computes inverse shifted pattern. proposed method continuously diﬀerentiably interpolate addition multiplication showed integrated neural networks replacing standard sigmoidal transfer function parameterizable transfer function. paper presented mathematical formulation concepts showed integrate neural networks. interpolation technique based noninteger iterates exponential function calculated using abel’s schr¨oder’s functional equations. chose method ﬁrst explorations mathematically sound property calculated iterates form abelian group functional composition. however results non-monotonic interpolation addition multiplication lead challenging optimization landscape training. therefore ad-hoc interpolations monotonic transition addition multiplication. method introduces additional realvalued parameters neuron transfer function. using suitable ﬁxed transfer function might allow absorb parameters back bias. speciﬁc implementation details proposed work drawbacks believe neurons implement operations beyond addition areas application neural computation.", "year": 2015}