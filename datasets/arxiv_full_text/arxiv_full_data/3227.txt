{"title": "Analysis and Optimization of Loss Functions for Multiclass, Top-k, and  Multilabel Classification", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Top-k error is currently a popular performance measure on large scale image classification benchmarks such as ImageNet and Places. Despite its wide acceptance, our understanding of this metric is limited as most of the previous research is focused on its special case, the top-1 error. In this work, we explore two directions that shed more light on the top-k error. First, we provide an in-depth analysis of established and recently proposed single-label multiclass methods along with a detailed account of efficient optimization algorithms for them. Our results indicate that the softmax loss and the smooth multiclass SVM are surprisingly competitive in top-k error uniformly across all k, which can be explained by our analysis of multiclass top-k calibration. Further improvements for a specific k are possible with a number of proposed top-k loss functions. Second, we use the top-k methods to explore the transition from multiclass to multilabel learning. In particular, we find that it is possible to obtain effective multilabel classifiers on Pascal VOC using a single label per image for training, while the gap between multiclass and multilabel methods on MS COCO is more significant. Finally, our contribution of efficient algorithms for training with the considered top-k and multilabel loss functions is of independent interest.", "text": "abstract—top-k error currently popular performance measure large scale image classiﬁcation benchmarks imagenet places. despite wide acceptance understanding metric limited previous research focused special case top- error. work explore directions shed light top-k error. first provide in-depth analysis established recently proposed single-label multiclass methods along detailed account efﬁcient optimization algorithms them. results indicate softmax loss smooth multiclass surprisingly competitive top-k error uniformly across explained analysis multiclass top-k calibration. improvements speciﬁc possible number proposed top-k loss functions. second top-k methods explore transition multiclass multilabel learning. particular possible obtain effective multilabel classiﬁers pascal using single label image training multiclass multilabel methods coco signiﬁcant. finally contribution efﬁcient algorithms training considered top-k multilabel loss functions independent interest. likely grow terms sample size well number classes. simply collecting data relatively straightforward exercise obtaining high quality ground truth annotation hard. even annotation list image level tags collecting consistent exhaustive list labels every image requires signiﬁcant effort. instead existing benchmarks often offer single label image albeit images inherently multilabel. increased number classes leads ambiguity labels classes start overlap exhibit hierarchical structure. issue illustrated figure difﬁcult even humans guess ground truth label correctly ﬁrst attempt allowing guesses instead leads call top-k error main subjects work. previous research focused minimizing top- error consider mainly interested cases achieving small top-k error simultaneously; minimization speciﬁc top-k error. goals pursued ﬁrst part paper concerned single label multiclass classiﬁcation. propose extensions established multiclass loss functions address top-k error minimization derive appropriate optimization schemes based stochastic dual coordinate ascent analyze multiclass methods calibrated top-k error perform extensive lapin schiele computer vision multimodal computing group planck institute informatics saarbrücken saarland germany. e-mail {mlapin schiele}mpi-inf.mpg.de. hein department mathematics computer science fig. class ambiguity single label places labels valley pasture mountain; resort chalet sky. note multiple labels apply image guesses required guess ground truth label correctly. empirical evaluation better understand beneﬁts limitations. earlier version work appeared moving forward top-k classiﬁcation natural transition step multiclass learning single label training example multilabel learning complete relevant labels. multilabel learning forms second part work introduce smoothed version multilabel loss contribute novel projection algorithms efﬁcient optimization multilabel losses sdca framework. furthermore compare multiclass top-k multilabel methods novel experimental setting want quantify utility multilabel annotation. speciﬁcally want understand possible obtain effective multilabel classiﬁers single label annotation. contributions work follows. provide overview related work establish connections number related research directions. particular point intimate link introduce learning problem multiclass multilabel classiﬁcation discuss respective performance metrics. also propose novel loss functions minimizing top-k error novel smooth multilabel loss. brief summary methods consider given table introduce notion top-k calibration analyze multiclass methods calibrated top-k error. particular highlight softmax loss uniformly top-k calibrated develop efﬁcient optimization schemes based sdca framework. speciﬁcally contribute algorithms computing proximal maps used train classiﬁers speciﬁed multiclass top-k multilabel loss functions. perform experiments multiclass benchmarks including imagenet places datasets. evaluation reveals particular softmax loss proposed smooth svmmulti loss competitive uniformly top-k errors improvements speciﬁc obtained top-k losses. evaluate multilabel methods datasets following smooth multilabel shows particularly encouraging results. next svmml perform experiments pascal microsoft coco train multiclass top-k methods using single label prominent object image compare multilabel performance test data multilabel release implementation sdca-based solvers training models loss functions considered work. also publish code corresponding proximal maps independent interest. related work section place work broad context related research directions. first draw connections general problem learning rank. mainly studied context information search retrieval clear ties multiclass multilabel classiﬁcation. second brieﬂy review related results consistency classiﬁcation calibration. form basis theoretical analysis top-k calibration. next focus technical side including optimization method algorithms efﬁcient computation proximal operators. finally consider multiclass multilabel image classiﬁcation main running examples paper. learning rank. learning rank supervised learning problem arises whenever structure output space admits partial order classic example ranking information retrieval e.g. recent review. there feature vector computed every query every document task learn model ranks relevant documents given query irrelevant ones. three main approaches recognized within framework pointwise pairwise listwise approach. pointwise methods cast problem predicting document relevance regression classiﬁcation problem. instead pairwise approach focused predicting relative order documents finally listwise methods attempt optimize given performance measure directly full list documents propose loss function predicted ground truth lists different ranking main interest work label ranking generalizes basic binary classiﬁcation problem multiclass multilabel even hierarchical classiﬁcation survey. link settings established consider queries examples documents class labels. main contrast however employed loss functions performance evaluation test time related work general family convex loss functions ranking classiﬁcation introduced usunier loss functions consider member family. another example wsabie learns joint embedding model optimizing approximation loss top-k classiﬁcation setting directly related label ranking task place ground truth label labels measured prediction scores. alternative approach suggested structured learning aggregate outputs pretrained one-vs-all binary classiﬁers directly predict labels labels missing annotation modelled latent variables. line work pursued task predicting items also considered frame problem maximizing submodular reward function. probabilistic model ranking top-k classiﬁcation proposed metric learning train nearest neighbor model. interesting setting related top-k classiﬁcation learning positive unlabeled data absence label imply negative label also learning label noise label ranking closely related multilabel classiﬁcation consider later paper ranking ranking objectives also considered training convolutional architectures notably loss triplets consideres positive negative examples. many recent works focus ranked list however mainly interested search retrieval number relevant documents exceeds users willing consider. setting suggests different traderecall precision compared setting relevant labels. correspondingly reﬂected performance evaluation mentioned above. consistency calibration. classiﬁcation discrete prediction problem minimizing expected error known computationally hard. instead common minimize surrogate loss leads efﬁcient learning algorithms. important question however whether minimizers expected surrogate loss also minimize expected error. loss functions property called calibrated consistent respect given discrete loss. consistency binary classiﬁcation well understood signiﬁcant progress made analysis multiclass multilabel ranking methods. work optimization. facilitate experimental evaluation proposed loss functions also implement corresponding optimization routines. choose stochastic dual coordinate ascent framework ease implementation strong convergence guarantees possibility compute certiﬁcates optimality duality gap. describe general sdca algorithm implement analysis limited scalar loss functions regularization suitable binary problems. recent work extends analysis vector valued smooth functions general strongly convex regularizers better suited multiclass multilabel loss functions. detailed comparison recent coordinate descent algorithms given following main step optimization algorithm updates dual variables computing projection generally proximal operator proximal operators consider equivalently expressed instances continuous nonlinear resource allocation problem long research history recent survey. related setting euclidean projection onto unit simplex -ball computed approximately bisection time exactly breakpoint searching variable ﬁxing former done time simple implementation based sorting time efﬁcient median ﬁnding algorithm. work choose variable ﬁxing scheme require sorting easy implement. although complexity pathological inputs elements growing exponentially observed complexity practice linear competitive breakpoint searching algorithms exist efﬁcient projection algorithms optimizing hinge loss descendants situation complicated logistic regression binary multiclass. exists analytical solution update logistic loss suggest formula binary case computes approximate update closed form. multiclass logistic loss optimized spams toolbox implements fista alternative optimization methods considered also propose two-level coordinate descent method multiclass case. different works propose follow closely variable ﬁxing scheme used training lambert function resulting entropic proximal map. runtime compares favourably spams show image classiﬁcation. multiclass multilabel image classiﬁcation main applications consider work evaluate proposed loss functions. employ relatively simple image recognition pipeline following feature vectors extracted convolutional neural network vggnet resnet used train linear classiﬁer different loss functions. convnets pre-trained large scale imagenet notation. consider classiﬁcation problems predeﬁned classes. begin multiclass classiﬁcation every example exactly label later generalize multilabel setting example associated labels work classiﬁer function induces ranking class labels prediction y∈y. linear case predictor form parameter learned. stack individual parameters weight matrix rd×m focus linear classiﬁers exposition experiments loss functions formulated general setting kernel trick employed construct nonlinear decision surfaces. fact number experiments kernel well. test time prediction depends evaluation metric generally involves sorting producing top-k highest scoring class labels multiclass setting predicting labels score certain threshold multilabel classiﬁcation. come back performance metrics shortly. denote permutations unless stated otherwise reorders components vector descending order aπm. therefore example maxj necessary make clear vector sorted writing mean sort πk}. also otherwise; introduce shorthand conditional probability finally obtained removing y-th coordinate consider -regularized objectives work multiclass loss regularization parameter classiﬁer training amounts solving minw binary multilabel classiﬁcation problems differ loss multiclass. standard performance measure classiﬁcation problems zero-one loss simply counts number classiﬁcation mistakes metric well understood inspired popular surrogate losses hinge loss naturally becomes stringent number classes increases. alternative standard zero-one error allow guesses instead one. formally top-k zero-one loss count mistake ground truth label scores class labels. note recover standard zero-one error. top-k accuracy deﬁned minus top-k error performance full test sample computed mean across test examples. dataset large number object categories relatively little variation scale location central object. scene recognition also vggnet-like architecture trained places dataset. despite differences benchmarks image representations learned convnets large datasets observed transfer well follow scheme single-label experiments e.g. recognizing birds ﬂowers using network trained imagenet transferring knowledge scene recognition however moving multi-label classiﬁcation pascal microsoft coco need account increased variation scale object placement. earlier works ignore explicit search object location require bounding annotation recent results indicate effective classiﬁers images multiple objects cluttered scenes trained weak image-level annotation explicitly searching multiple scales locations multilabel setup follows closely pipeline exceptions detailed choosing loss function want consider several aspects. first basic level loss function depends available annotation performance metric interested e.g. distinguish multiclass multilabel losses work. next fundamental factors control statistical computational behavior learning. computational reasons work convex surrogate losses rather performance metric directly. context relevant distinction nonsmooth lipschitz functions smooth functions strongly convex conjugates lead faster convergence rates. statistical perspective important understand surrogate loss classiﬁcation calibrated attractive asymptotic property leads bayes consistent classiﬁers. finally exploit duality introduce modiﬁcations conjugates existing functions desirable effects primal loss rest section covers technical background used later paper. discuss notation introduce multiclass multilabel classiﬁcation recall standard approaches classiﬁcation introduce recently proposed methods top-k error minimization. discuss multiclass multilabel performance evaluation measures used later experiments. review established multiclass approaches introduce novel top-k loss functions; also recall moreau-yosida regularization smoothing technique compute convex conjugates sdca optimization. discuss multilabel classiﬁcation methods introduce smooth multilabel compute corresponding convex conjugates. enhance readability defer proofs appendix. ranking class labels given image similar ranking documents user query information retrieval many established metrics popular measure relevant discussion precision-at-k fraction relevant items within retrieved although measure makes perfect sense |yi| i.e. many relevant documents possibly want examine useful correct labels image relevant labels list starts decrease increases. better alternative multilabel setting complementary measure recall-at-k deﬁned measures fraction relevant labels list. note natural generalization topk error multilabel setting coincides multiclass metric whenever singleton. partition based. contrast ranking evaluation partition based measures assess quality actual multilabel prediction requires cut-off threshold several threshold selection strategies proposed literature setting constant threshold prior experiments selecting threshold posteriori matching label cardinality tuning threshold validation learning regression function bypassing threshold selection altogether introducing calibration label experimented options discussed predicted primitives deﬁned performance measure based binary confusion matrix depending averaging occurs following three groups metrics recognized. instance-averaging. binary metrics computed averages labels averaged across examples following consider score binary metric three types averaging. also report multilabel accuracy subset accuracy hamming loss deﬁned respectively multiclass methods section switch performance evaluation test time quality classiﬁer measured during training. particular introduce loss functions used established multiclass methods well novel loss functions optimizing top-k error ova. multiclass problem often solved using one-vs-all reduction independent binary classiﬁcation problems. every class trained versus rest yields classiﬁers {fy}y∈y. typically classiﬁer trained convex margin-based loss function simplifying notation consider hinge logistic losses correspond logistic regression methods respectively. multiclass. alternative scheme multiclass loss directly. multiclass losses consider depend pairwise differences ground truth score scores loss functions family additionally require margin interpreted distance label space simplify notation vectors deﬁned given pair multiclass loss softmax loss common multiclass problems. latter particularly popular deep architectures svmmulti also competitive large-scale image classiﬁcation note conjugates top-k losses coincide equal conjugate svmmulti loss exception effective domains respectively. becomes evident effective domain conjugate feasible dual variables. therefore move svmmulti top-k svmβ top-k svmα introduce constraints dual variables thus limiting extent single training example inﬂuence classiﬁer. smooth top-k svm. apply smoothing technique introduced top-k svmα. smoothing top-k svmβ done similarly proposition smoothing parameter. smooth top-k hinge loss conjugate identity matrix y-th column y-th standard basis vector dimensional vector ones. follows deﬁγ nition fact written proj∆α projc known result says projc closed convex also highlight smooth multiclass penalize mistakes discover convexity losses leads phenomena errk) happens example creates bias working rigid function classes linear classiﬁers. next introduce loss functions modiﬁcations losses goal alleviating phenomenon. top-k svm. recently introduced top-k multiclass modiﬁcations multiclass hinge loss proposed. ﬁrst version motivated directly top-k error second version falls general family ranking losses introduced earlier usunier top-k losses reorders components descending order. show top-k svmα offers tighter upper bound top-k error top-k svmβ. however losses perform similarly experiments small advantage top-k svmβ settings. therefore distinction important simply refer top-k hinge top-k loss. note reduce svmmulti top-k losses smooth implications optimization top-k calibration following employed moreau-yosida regularization obtain smoothed version binary hinge loss applied technique introduced smooth top-k svm. convex conjugate classical result convex analysis states conjugate strongly convex function lipschitz smooth gradient therefore indeed smooth function. simplex differ functional form linear function svmmulti negative entropy lrmulti. motivated top-k directly top-k error change compared svmmulti effective domain conjugate loss. suggests general construct novel losses speciﬁc properties taking conjugate existing loss function modifying effective domain enforces desired properties. motivation comes interpretation dual variables forces every training example pushes decision surface direction given ground truth label. therefore reducing feasible limit maximal contribution given training example. top-k entropy. hinted above ﬁrst construct conjugate top-k entropy loss taking conjugate lrmulti replacing take conjugate obtain primal loss top-k ent. version constructed using truncated top-k entropy. major limitation softmax loss top-k error optimization cannot ignore highest scoring predictions. lead situation loss high even though top-k error zero. that rewrite lrmulti loss function convex sequence monotonically non-increasing implies convex ranking based losses weight highest scoring classiﬁers would like less weight them. drop ﬁrst highest scoring predictions sacriﬁcing convexity loss deﬁne truncated top-k entropy loss follows indexes corresponding smallest components )j=y. loss seen smooth version top-k error small whenever top-k error zero. show synthetic experiment advantage discarding highest scoring classiﬁer top-k enttr becomes apparent. multilabel methods section introduce natural extensions classic multiclass methods discussed setting ground truth labels example focus loss functions produce ranking labels optimize multilabel loss simpliﬁed notation complete overview multilabel classiﬁcation methods given binary relevance binary relevance standard one-vs-all scheme applied multilabel classiﬁcation. default baseline direct multilabel methods consider possible correlations labels. method also known multiclass multilabel perceptron separation ranking loss contrasted another svmmulti extension ranksvm elisseeff weston optimizes pairwise ranking loss note svmml consider ranksvm avoid expensive enumeration possible labellings considering pairwise label ranking. principled large margin approach accounts possible label interactions structured output prediction multilabel conjugate. here compute convex conjugate svmml loss used later deﬁne smooth multilabel svm. note svmml loss depends partitioning every given pair. reﬂected deﬁnition below effective domain conjugate multiclass setting singleton therefore degrees freedom recover unit simplex true multilabel setting hand freedom distribute weight across classes smooth top-k analytic formula smoothed loss. however compute optimize within framework solving euclidean projection problem onto call bipartite simplex. convenient modiﬁcation above another write optimal top-k error j=k+ naturally leads optimal prediction strategy according ranking descending order. however description top-k bayes optimal classiﬁer reveals optimality given better understood partitioning rather ranking labels split rest without preference ranking either subset. hand want classifer top-k bayes optimal simultaneously proper ranking according necessary sufﬁcient. top-k calibration. optimization zero-one loss top-k error leads hard combinatorial problems. instead tackling combinatorial problem directly alternative convex surrogate loss upper bounds discrete error. mild conditions loss function optimal classiﬁer surrogate yields bayes optimal solution zero-one loss. loss functions called classiﬁcation calibrated known statistical learning theory necessary condition classiﬁer universally bayes consistent introduce notion calibration top-k error. deﬁnition multiclass loss function called top-k calibrated possible data generating measures loss top-k calibrated implies even limit inﬁnite data obtain classiﬁer bayes optimal top-k error lemma thus important property even though asymptotic nature. next analyse multiclass classiﬁcation methods covered top-k calibrated. multiclass top-k calibration section consider top-k calibration standard scheme established multiclass classiﬁcation methods proposed top-k enttr loss. first state condition scheme uniformly top-k calibrated corresponds standard zeroone loss simultaneously. condition given terms bayes optimal classiﬁer corresponding binary problems respect given loss function e.g. hinge logistic losses. loss nice generalization smooth multiclass loss svmmulti naturally recover latter singleton. extend variable ﬁxing algorithm obtain efﬁcient method compute euclidean projections onto multilabel cross-entropy. here discuss extension lrmulti loss multilabel learning. softmax function model distribution class labels recovers well-known multinomial logistic regression maximum entropy models. conjugates multilabel losses svmml lrml longer share effective domain case multiclass losses. however still recover conjugate lrmulti loss singleton. bayes optimality top-k calibration section devoted theoretical analysis multiclass losses terms top-k performance. establish best top-k error bayes sense determine classiﬁer achieves deﬁne notion top-k calibration investigate loss functions possess property. furthermore convexity softmax multiclass hinge losses leads phenomena errk) discussed issue motivated modiﬁcations losses top-k error. next show proposed top-k losses also top-k calibrated. optimization framework section mainly devoted efﬁcient optimization multiclass multilabel methods within stochastic dual coordinate ascent framework shalev-shwartz zhang core reason efﬁciency optimization scheme ability formulate variable updates terms projections onto effective domain conjugate loss which turn solved time faster. projections fall broad area nonlinear resource allocation already large selection specialized algorithms. example algorithm kiwiel svmmulti top-k svmβ contribute analogous algorithms remaining losses. particular propose entropic projection algorithm based lambert function lrmulti loss variable ﬁxing algorithm projecting onto bipartite simplex svmml. also discuss proposed loss functions closed-form expression evaluated efﬁciently perform runtime comparison fista using spams optimization toolbox state primal fenchel dual optimization problems introduce lambert function. consider sdca update steps loss computation multiclass methods well present runtime evaluation experiments. cover multilabel optimization present algorithm euclidean projection onto bipartite simplex. brieﬂy recall main facts sdca framework fenchel duality lambert function primal dual problems. rd×n matrix training examples corresponding gram matrix rd×m matrix primal variables rm×n matrix dual variables regularization parameter. primal fenchel dual objective functions given lemma reduction top-k calibrated bayes optimal function convex margin-based loss strictly monotonically increasing function every class proof. bayes optimal classiﬁer binary problem corresponding form hinge loss calibrated since corresponding binary classiﬁers piecewise constant subject degenerate cases result arbitrary rankings classes. surprisingly smoothing technique based moreauyosida regularization makes smoothed loss attractive optimization side also terms top-k calibration. here show smooth binary hinge loss fulﬁlls conditions lemma leads top-k calibrated scheme. alternative scheme binary losses multiclass loss directly. first consider multiclass hinge loss svmmulti known calibrated top- error show top-k calibrated tewari bartlett provide general framework study classiﬁcation calibration applicable large family multiclass methods. however characterization calibration derived terms properties convex hull might difﬁcult verify practice. contrast proofs propositions straightforward based direct derivation corresponding bayes optimal classiﬁers svmmulti lrmulti losses respectively. implicit reason top-k calibration schemes softmax loss estimate probabilities bayes optimal classiﬁer. loss functions allow called proper. refer references therein detailed discussion. established logistic regression softmax loss top-k calibrated interested deﬁning loss functions top-k error? reason calibration asymptotic property since bayes optimal functions obtained pointwise minimization every picture changes linear classiﬁers since obviously cannot minimized independently provided figure directly equation behavior changes dramatically depending whether large positive large negative number. ﬁrst case linear part dominates logarithm function approximately linear; better approximation t−log second case function behaves like exponent this write ete−x note therefore compute approximations initial points order householder method single iteration method already sufﬁcient full ﬂoat precision iterations needed double makes function attractive tool computing entropic projections. multiclass methods section cover optimization multiclass methods within sdca framework. discuss efﬁciently compute smoothed losses introduced conjugation closed-form expression. finally evaluate sdca convergence terms runtime show smoothing moreau-yosida regularization leads signiﬁcant improvements speed. primal objective computed duality could conceivably omitted certiﬁcate optimality required. next focus updates computed different multiclass methods. sdca update svmova lrova. sdca updates binary hinge logistic losses covered highlight svmova update closed-form expression leads scalable training linear svms implemented liblinear although svmmulti also covered different algorithm based sorting case distinction first solve easier continuous quadratic knapsack problem using variable ﬁxing algorithm kiwiel require sorting. corresponds enforcing equality constraint simplex generally already gives optimal solution. computation also fast observe linear time complexity practice shown figure remaining hard cases however fall back sorting scheme similar experience performing case distinction seemed offer signiﬁcant time savings. here consider update step smooth top-k svmα loss. nonsmooth version directly recovered setting update top-k svmβ derived similarly using instead show performing update step equivalent projecting certain vector computed prediction scores onto effective domain top-k simplex added regularization biases solution orthogonal convex conjugate interpreted multilabel loss. sdca proceeds sampling dual variable corresponds training example modifying achieve maximal increase dual objective keeping dual variables ﬁxed. several sampling strategies used e.g. simple scheme indexes randomly shufﬂed every epoch ai’s updated sequentially. algorithm terminates relative duality d)/p falls pre-deﬁned computational budget exhausted case still estimate suboptimality duality gap. since algorithm operates entirely dual variables prediction scores directly applicable training linear well nonlinear classiﬁers often case experiments training linear classiﬁer less expensive maintain primal variables compute products case whenever updated perform rank- update turns every update step maxai equivalent proximal operator certain function seen projection onto effective domain lambert function. lambert function deﬁned inverse mapping wew. widely used many ﬁelds computer science often recognized nonlinear equations involving functions. taking logarithms sides deﬁning equation therefore given equation form directly solve closed-form crux problem function transcendental like logarithm exponent. exist highly optimized implementations latter argue done lambert function. fact already work topic also employ implementation. solve using algorithm computing projection onto top-k simplex introduced minor modiﬁcation similarly update step top-k svmβ loss solved using continuous quadratic knapsack problem discuss supplement smooth top-k hinge losses converge signiﬁcantly faster nonsmooth variants show scaling experiments below. explained theoretical results convergence rate sdca. also similar observations smoothed binary hinge loss. sdca update top-k ent. finally derive optimizaalgorithm instance variable ﬁxing scheme following steps partition variables disjoint sets compute auxiliary variable optimality conditions; compute values variables using verify constraints violated constraints computed solution otherwise examine next partitioning. clearly must hold consider degenerate fall back case. therefore primarily interested partitions monotonicity optimality conditions show always corresponds largest elements vector projected. hence start empty indexes largest bj’s solution found. solve using newton’s method order householder’s method faster convergence rate. latter particularly attractive since always assumed empty i.e. lrmulti loss often also empty general top-k loss. methods require derivatives note means derivatives come additional cost. finally note deﬁnition. here discuss evaluate smoothed losses closed-form expression primal loss. recall smooth top-k svmα euclidean projection proj∆α onto describe algorithm compute projection special case i.e. svmmulti loss algorithm particularly efﬁcient exhibits essentially linear scaling practice. moreover since need products exploit special structure min{max{l avoid explicit computation procedure done top-k svmβ note similar similar variable ﬁxing scheme described above. however problem much easier auxiliary variables computed directly without solve nonlinear system computation involve function. proposition solution sets deﬁned given fig. scaling projection algorithms used sdca optimization. sdca convergence lrmulti svmmulti methods imagenet dataset. sdca fista implemented spams toolbox. smooth svmmulti efﬁciency outlined approach optimizing top-k loss crucially depends fast computation sdca update. implementation able scale large datasets show next. runtime evaluation. first highlight efﬁciency algorithm computing euclidean projection onto top-k simplex used particular optimization svmmulti loss. scaling plot given figure shows results experiment following sample points normal distribution solve projection problems using algorithm kiwiel using method projecting onto different values report total time taken single intel xeon .ghz processor. also observed scaling essentially linear problem dimension makes method applicable problems large number classes. next figure compare wall-clock training time svmmulti smoothed svmmulti lrmulti objectives. plot relative duality validation accuracy versus time best performing models imagenet benchmark. obtain substantial improvement convergence rate smooth compared nonsmooth baseline. moreover svmmulti top- accuracy saturates passes training data justiﬁes fairly loose stopping criterion lrmulti loss cost epoch signiﬁcantly higher compared svmmulti difﬁculty solving suggests smooth top- svmα loss offer competitive performance lower training cost. finally also compare implementation lrmulti spams optimization toolbox provides efﬁcient implementation fista note rate convergence sdca competitive fista noticeably better conclude approach training lrmulti model competitive state-of-the-art faster computation lead speedup. section covers optimization multilabel objectives introduced first reduce computation sdca update step evaluation smoothed loss problem computing euclidean prosvmml jection onto called bipartite simplex next contribute novel variable ﬁxing algorithm computing projection. finally discuss sdca optimization multilabel cross-entropy loss lrml. mization smoothed svmml nonsmooth counterpart recovered setting proposition respectively svmml loss conjugate proposition dual variables corresponding training pair updated y∈yi make remarks regarding optimization multilabel svm. first update step involves exactly projection used proposition deﬁne smoothed svmml loss difference vectors projected radius bipartite simplex. therefore projection algorithm optimization well computing loss. second even though svmml reduces svmmulti singleton derivation smoothed loss projection algorithm proposed bipartite simplex substantially different proposed multiclass setting. notably treatment dimensions symmetric. sdca update lrml. finally discuss optimization multilabel cross-entropy loss lrml. show corresponding sdca update step equivalent certain entropic projection problem propose tackle using function introduced above. proposition respectively lrml loss conjugate proposition dual variables corresponding training pair updated y∈yi order householder’s method solve similar top-k loss above. solving nonlinear equation main computational challenge updating dual variables. however procedure require iteration index partitions generally faster optimization top-k loss. experiments section provides broad array experiments different datasets comparing multiclass multilabel performance loss functions look different aspects empirical evaluation performance synthetic real data handcrafted features features extracted convnet targeting speciﬁc performance measure generally competitive range metrics. show synthetic data top-k enttr loss targeting speciﬁcally top- error outperforms competing methods large margin. focus evaluating top-k performance multiclass methods real-world benchmark datasets including imagenet places. cover multilabel classiﬁcation groups experiments comparative study following popular multilabel datasets; image classiﬁcation pascal coco novel setting contrasting multiclass top-k multilabel methods. projb. below propose efﬁcient variable ﬁxing algorithm compute euclidean projection onto also note trick used top-k svmα exploit special form projection avoid explicit computation problem considered shalev-shwartz singer proposed breakpoint searching algorithm based sorting well formulated root ﬁnding problem solved bisection. next contribute novel variable ﬁxing algorithm inspired algorithm kiwiel continuous quadratic knapsack problem initialization. deﬁne sets solve independent subproblems using algorithm runtime evaluation. also compare runtime proposed variable ﬁxing algorithm sorting based algorithm perform comparison code available. furthermore algorithms consider exact method approximate runtime dependent required precision. experimental setup above results reported table multiclass experiments goal section provide extensive empirical evaluation loss functions terms top-k performance. compare multiclass top-k methods datasets ranging size problem domain granularity detailed statistics given table solvers. liblinear one-vs-all baselines svmova lrova; code top-k svm. extended latter support smooth top-k svmγ top-k losses. multiclass baselines svmmulti lrmulti correspond respectively top- top- ent. nonconvex top-k enttr lrmulti solution initial point perform gradient descent line search cross-validate hyper-parameters range extending optimal value boundary. features. aloi letter news datasets features provided libsvm datasets. aloi randomly split data equally sized training test sets preserving class distributions. letter dataset comes separate validation model selection only. news reduce dimensionality sparse features preserving non-singular components. discussion. results given table make several interesting observations. first schemes perform quite similar multiclass approaches conﬁrms earlier observations schemes performed worse aloi letter. thus generally recommend multiclass losses instead schemes. top- svmα test accuracy top- enttr test accuracy fig. synthetic data visualization top- top- predictions top- svmα optimizes top- error increases top- error. top- enttr ignores top- mistakes optimizes directly top- error. section demonstrate synthetic experiment proposed top- losses outperform top- losses optimal top- performance. dataset three classes shown inner circle figure sampling scheme. first generate samples subdivided segments. segments unit length except segment length sample segments according following distribution class class class finally data rescaled mapped onto unit circle. samples different classes plotted next better visibility signiﬁcant class overlap. visualize top-/ predictions colored circles outside black circle. sample points training/validation/test tune range results shown table column provide results model optimizes corresponding top-k accuracy. first note top- baselines perform similar top- performance except svmmulti top- show better results. next top- losses improve top- accuracy improvement signiﬁcant nonconvex top- enttr loss close optimal solution dataset. top- enttr provides tight bound top- error ignores top- errors loss. unfortunately similar signiﬁcant improvements observed real-world datasets tried. table top-k accuracy various datasets. ﬁrst line reference performance dataset reports top- accuracy except numbers aligned top-k. compare one-vs-all multiclass baselines top-k svmα well proposed smooth top-k svmα comparing softmax loss multiclass clear winner top- performance softmax consistently outperforms multiclass top-k performance might strong property softmax top-k calibrated note trend uniform across datasets particular also ones features coming convnet. smooth top-k top-k entropy losses perform slightly better softmax compares speciﬁc top-k errors. however good performance truncated top-k entropy loss synthetic data transfer real world datasets. table top-k accuracy reported caffe large scale datasets ﬁne-tuning approximately epoch places epochs imagenet. ﬁrst line reference performance ﬁne-tuning. fine-tuning experiments. also performed number ﬁne-tuning experiments original network trained epochs smooth top-k hinge truncated top-k entropy losses. motivation full end-to-end training would beneﬁcial compared training classiﬁer. results reported table note setting slightly different feature extraction step matconvnet non-regularized bias term caffe top-k speciﬁc losses able improve performance compared reference model that places smooth top- loss achieves best top-.. performance. however experiments also observed similar improvements ﬁne-tuning standard softmax loss achieves best performance imagenet training beyond epochs change results signiﬁcantly. conclusion. safe choice multiclass problems seems lrmulti loss yields reasonably good results top-k errors. competitive alternative smooth svmmulti loss faster train wants optimize directly top-k error multilabel experiments section threefold. first establish competitive performance multilabel classiﬁcation methods comparing methods extensive experimental study madjarov multilabel benchmark datasets varying scale complexity. next discuss interesting learning setting top-k classiﬁcation methods emerge transition step multiclass multilabel approaches. finally evaluate multiclass top-k multilabel classiﬁcation methods pascal challenging microsoft coco image classiﬁcation benchmarks. multilabel classiﬁcation. here seek establish solid baseline evaluate implementation multilabel svmml smooth svmml lrml methods. follow work madjarov provide clear description evaluation protocol extensive experimental comparison multilabel classiﬁcation methods datasets reporting performance metrics. limit comparison best performing methods study namely random forest predicting clustering trees hierarchy multilabel classiﬁers binary relevance method using svmova. report results datasets issue published train/test splits remaining benchmark. datasets vary greatly size label cardinality seen basic statistics table details datasets found table multilabel classiﬁcation. best performing methods study madjarov compared multilabel methods baselines rf-pct random forest predicting clustering trees homer hierarchy multilabel classiﬁers binary relevance method using svmova. homer methods marked kernel. following cut-off threshold methods chosen cross validation. deﬁnition). following madjarov choose matching label cardinality training test data. fast easy compute approach drawbacks instance transductive learning method requires re-computation every time test data changes; choice tuned performance measure likely suboptimal. experiments observed generally comparable slightly lower results compared selected validation discussed next. instead koyejo recently showed consistent classiﬁer obtained computes optimizing given performance measure hold-out validation set. distinct values would need considered limit search grid values. following -fold cross-validation select kernel parameter threshold described above. rather large ﬁne-grained grids smoothing parameter always tables present experimental results. report performance metrics previously introduced tune hyper-parameters metric individually. metrics except rank loss hamming loss given percents. since rf-pct method kernel also report results linear kernel methods middle section table. overall experimental results indicate competitive performance methods across datasets evaluation measures. speciﬁcally highlight smooth svmml kernel yields best performance cases. largest datasets bookmarks delicious previous methods even struggled complete training able achieve signiﬁcant performance improvements rank loss well partition-based measures. finally note previous methods show rather large variability performance three multilabel methods tend stable show results concentrated around best performing method cases. multiclass multilabel. collecting ground truth annotation hard. even annotation simply image level providing consistent exhaustive list labels table pascal classiﬁcation results. evaluation multiclass top-k multilabel classiﬁcation methods. methods multiclass section single label image methods multilabel section annotated labels. please section multiclass multilabel details learning setting. every image training would require signiﬁcant effort. much easier provide weaker form annotation single prominent object tagged. interesting question whether still possible train multilabel classiﬁers multiclass annotation. large performance compared methods trained full multilabel annotation? following explore setting answer questions above. also note top-k classiﬁcation emerges naturally intermediate step multiclass multilabel learning. recall top-k loss functions operate multiclass setting single label example label hard guess correctly ﬁrst attempt. could imagine example actually associated labels single label revealed annotation. therefore also interesting top-k loss functions offer advantage classic multiclass losses setting. evaluate multiclass top-k multilabel loss functions common task choose multilabel image classiﬁcation benchmarks pascal microsoft coco. multilabel methods trained using full image level annotation multiclass top-k methods trained using single label image. datasets offer object level bounding annotations used estimate relative sizes objects scene. multiclass training keep label largest object proxy estimating prominent object image. methods evaluated using full annotation test time. note except pruning training labels bounding boxes anywhere training testing. experimental setup. images training testing pascal training testing coco validation set. split training data half parameter tuning re-train full testing. tune regularization parameter range top-k parameter range partition-based measures also tune threshold range equally spaced points. range chosen observing distribution computed matching label cardinality training test data. hyper-parameters tuned method performance metric individually. isolate effect loss functions classiﬁer training feature learning follow classic approach extracting features pre-processing step train classiﬁers ﬁxed image representation. implementation sdca based solvers methods considered section. offers strong convergence guarantees convexity objective duality stopping criterion. feature extraction pipeline fairly common follows steps outlined compute multiple feature vectors image. every original image resized isotropically smallest side equal pixels horizontal ﬂips added total images scales. matconvnet apply resnet- model pre-trained imagenet. extract features pool layer obtain feature vectors dimension image pascal reduce computational costs coco increase stride layer yields feature vectors image total training examples. unlike compute additional global descriptor also perform normalization. preliminary experiments showed advantage decided keep pipeline close original resnet network. table coco multilabel classiﬁcation results. methods multiclass section single label image methods multilabel section annotated labels. please section multiclass multilabel details learning setting details evaluation measures. every feature vector mapped region original image. training simply replicate image labels effectively increasing size training set. test time obtain single ranking class labels image pooling scores class. follow basic setup note improvement possible sophisticated aggregation information different image regions pascal here discuss results presented tables start ﬁrst table reports standard evaluation measure mean first compare top- top-k classiﬁcation methods. before although differences small consistent improvements three groups lrmulti top-k svmmulti top-k svmβ best top- method svmmulti svmmulti outperformed top-k svmβ reporting best multiclass result map. next look performance multiclass multilabel settings. best achieved multilabel svmml exploits full annotation boost performance. however suggests non-trivial trade-off additional annotation effort resulting classiﬁcation performance. limitation results relatively label cardinality labels image. picture changes coco label cardinality labels image. comparing smooth nonsmooth losses nonsmooth loss functions tend perform better dataset. moreover seems perform signiﬁcantly better softmax. somewhat surprising result observed previously e.g. r-cnn detector deeply-supervised cnns even though comparison svm. finally note current state classiﬁcation results reported svmml matches exactly result lssvm-max operates setting closest terms image representation learning architecture. proposed prsvm method performs additional inference achieves map. multiscale orderless pooling directly comparable setting yields map. performing inference extracted image regions report around additionally exploiting bounding annotations boosts performance established performance measure pascal datasets evaluate well method captures inter-class correlations since computed class independently. address limitation also report number multilabel performance metrics table best performing method multiclass category top-k svmβ improvement baseline svmmulti pronounced. furthermore smooth svmml clearly outperforms nonsmooth counterpart also signiﬁcantly increasing multiclass multilabel methods. coco. table presents results coco benchmark. general trend similar observed top-k methods tend outperform top- multiclass baselines outperformed multilabel methods exploit full annotation. however differences methods meaningful dataset. particular smooth top-k svmβ achieves improvement svmmulti multilabel svmml boosts performance improvement highlights value multilabel annotation even though result subject bias label selection procedure multiclass methods small objects repesented well. class imbalance could also reason relatively poor performance svmmulti lrmulti methods experiments. current state classiﬁcation results coco reported comparable architecture achieved performing inference multiple regions image exploiting bounding annotations boosted performance map. looking multilabel evaluation measures also make interesting observations. first rank loss seems correlate well performance measures good since metric loss functions designed optimize. second strong performance suggests single guess generally sufﬁcient guess correct label. however high class imbalance result impressive humbled performance even attempts sufﬁce guess relevant labels. difﬁculty properly ranking less represented classes also highlighted relatively accuracy subset accuracy results although latter metric stringent large scale benchmark. conclusion done extensive experimental study multiclass top-k multilabel performance optimization. observed softmax loss smooth hinge loss competitive across top-k errors considered primary candidates practice. top-k loss functions improve results especially targeting particular top-k error performance measure training examples multilabel nature. latter transition multiclass multilabel classiﬁcation indicates effective multilabel classiﬁers trained single label annotations. results also show classical multilabel competitive pascal however proposed smooth multilabel outperforms competing methods metrics pascal metrics coco. finally would like highlight optimization schemes top-k top-k svmγ svmml include softmax loss multiclass multilabel special cases. russakovsky deng krause satheesh huang karpathy khosla bernstein berg fei-fei imagenet large scale visual recognition challenge zhou lapedriza xiao torralba oliva learning deep features scene recognition using places database nips t.-y. maire belongie hays perona ramanan dollár zitnick microsoft coco common objects context eccv xiao hays ehinger oliva torralba database large-scale scene recognition abbey cvpr shalev-shwartz zhang stochastic dual coordinate ascent methods regularized loss minimization jmlr vol. analysis insights cvpr crammer singer family additive online algorithms category ranking jmlr vol. shalev-shwartz zhang accelerated proximal stochastic dual coordinate ascent regularized loss minimization math. prog. madjarov kocev gjorgjevikj džeroski extensive experimental comparison methods multi-label learning pattern recognition vol. everingham gool williams winn zisserman pascal visual object classes challenge ijcv vol. tsochantaridis joachims hofmann altun large margin methods structured interdependent output variables jmlr usunier buffoni gallinari ranking ordered weighted pairwise classiﬁcation icml weston bengio usunier wsabie scaling large vocabulary image annotation ijcai shimada nagahara r.-i. taniguchi image annotation incomplete labelling modelling image speciﬁc structured loss ieej electr electr vol. ross zhou bagnell learning policies contextual submodular prediction icml gong leung toshev ioffe deep convolutional ranking multilabel image annotation arxiv. wang song leung rosenberg wang philbin chen learning ﬁne-grained image similarity deep ranking cvpr based hashing multi-label image retrieval cvpr agarwal inﬁnite push support vector ranking algorithm directly optimizes accuracy absolute list boyd cortes mohri radovanovic accuracy nips patriksson strömberg algorithms continuous nonlinear resource allocation problem implementations numerical studies ejor vol. efﬁcient euclidean projections linear time icml sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional networks arxiv. r.-w. zhao chen j.-m. y.-g. jiang regional gating neural networks multi-label image classiﬁcation bmvc dembczynski cheng hüllermeier bayes optimal multilabel classiﬁcation probabilistic classiﬁer chains icml read pfahringer holmes frank classiﬁer chains multi-label classiﬁcation ecml appendix proofs proof proposition proof. take convex conjugate top-k hinge loss derived obtain γ-strongly convex conregularizer jugate loss need work dimensional vectors y-th coordinate removed. primal loss obtained convex conjugate /γ-smooth known result convex analysis derive formula compute based euclidean projection onto top-k simplex. deﬁnition fukushima precise fast computation lambert wfunctions without transcendental function evaluations journal computational applied mathematics vol. tsoumakas katakis vlahavas effective efﬁcient multilabel classiﬁcation domains large number labels ecml/pkdd workshop mining multidimensional data katakis tsoumakas vlahavas multilabel text classiﬁcation automated suggestion ecml pkdd discovery challenge vol. note cannot satisfy conditions choice dual variables therefore implies constraint might active note however view active either dimensional problem. consider case constraint active below. show hinge top-k calibrated construct example problem classes note every class bayes optimal binary classiﬁer hence predicted ranking labels arbitrary produce bayes optimal top-k error. maximal corresponds taking largest conditional probabilities yields bayes optimal top-k error since relative order within {pτj irrelevant top-k error classiﬁer sets coincide bayes optimal. note assumed w.l.o.g. clear pτk+ likely classes rest. general ties resolved arbitrarily long guarantee largest components correspond classes yield maximal follows multiclass hinge loss classiﬁcation calibrated maxy∈y bayes optimal classiﬁer reduces constant. moreover even loss top-k calibrated predicted order remaining classes need optimal. solutions independent problems conditions fulﬁlled have therefore solution original problem otherwise optimal least variables must increase. therefore eliminate leads", "year": 2016}