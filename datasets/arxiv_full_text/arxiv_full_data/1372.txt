{"title": "On Study of the Binarized Deep Neural Network for Image Classification", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "Recently, the deep neural network (derived from the artificial neural network) has attracted many researchers' attention by its outstanding performance. However, since this network requires high-performance GPUs and large storage, it is very hard to use it on individual devices. In order to improve the deep neural network, many trials have been made by refining the network structure or training strategy. Unlike those trials, in this paper, we focused on the basic propagation function of the artificial neural network and proposed the binarized deep neural network. This network is a pure binary system, in which all the values and calculations are binarized. As a result, our network can save a lot of computational resource and storage. Therefore, it is possible to use it on various devices. Moreover, the experimental results proved the feasibility of the proposed network.", "text": "network became possible. result research deep neural networks emerged attracted attention. example geoff hinton’s deep belief nets yann lecun ciresan’s study deep convolutional neural networks alex graves’s deep recurrent neural networks features summarized follows. large-scale network model usually large structure including millions neurons connections. consequently computational cost extremely high thus experiments done gpus. better performance archived state-of-theart results various tasks competitions brought breakthroughs ﬁelds handwritten character recognition image classiﬁcation speech recognition lack theoretical explanation although achieved much better performance conventional methods convincing explanation success. example well known trick training could improve performance signiﬁcantly hard explain effect theoretically. recently deep neural network attracted many researchers’ attention outstanding performance. however since network requires high-performance gpus large storage hard individual devices. order improve deep neural network many trials made reﬁning network structure training strategy. unlike trials paper focused basic propagation function artiﬁcial neural network proposed binarized deep neural network. network pure binary system values calculations binarized. result network save computational resource storage. therefore possible various devices. moreover experimental results proved feasibility proposed network. research artiﬁcial neural networks began years proposed warren mcculloch walter pitts donald hebb frank rosenblatt especially two-layer network introduced pattern recognition. figure shows basic function neuron higher layer calculated neurons prior layer connecting weights. however solution network training backpropagation algorithm created paul werbos james mcclelland introduced simulation natural neural process usage artiﬁcial intelligence research became popular. since then successfully applied image classiﬁcation character recognition face recognition speech recognition moreover theoretical explanation ann’s success also given kurt hornik hand order pursue higher recognition rate several optimization methods training proposed dropout dropconnect methods reduce overﬁtting problem signiﬁcantly. hand understanding neural network emerges extend ability dnn. example goodfellow’s recent work digit string recognition output layer trained show digit number recognition result digit. this model able recognize digit string directly without segmentation process. work extended single character recognition character string recognition. inspired work change basic framework possibilities. paper focus basic function make suitable computers. usually seen simulation natural neural process thus neurons weights real number. values represent electric signal generated neural cells. however models often realized computers. know computer process data based binary value like words basic neural cell computer generates binary signals. inspired observation proposed type neural network binarized deep neural network bdnn neurons weights binary value; time calculation basic function bdnn also boolean. actually researches binary neural network corresponding training algorithms nevertheless kind neural network quite different bdnn. although input output binary neural network binary values weights real number. therefore calculation binary neural network different conventional neural network. contrast bdnn pure binary system variables operations binarized less storage request since weights bdnn binary value thus store weight. result save large network small storage. contrast must least bits store real number weight conventional neural network. higher speed basic calculation boolean calculation. calculation efﬁcient. since bdnn uses boolean calculation thus processing speed easily optimized cpu. consequently possi clear network response bdnn quite easy observe response neurons weights propagation process. neuron weight different kinds status. helpful design proper learning strategy bdnn. summary binary variables boolean operations bdnn able reasonable computational resource storage. although performance bdnn comparable conventional deep neural network scale potential improved future. paper organized follows. section principles bdnn introduced well hybrid binarized deep neural network non-binary input data. section training method proposed bdnn hybrid-bdnn. section comparison experiments bdnn conventional analyzed. last section conclusion part. section recognition process bdnn introduced. mentioned above binary variables boolean operations used process. moreover order bdnn non-binary input data also introduced hybrid-bdnn contained conventional neural network part bdnn part. shown fig. basic function build network complicated structure. therefore bdnn actually deﬁnition basic function. like conventional basic function bdnn neural network structure built. usually basic function neural network contain different types calculation linear nonlinear calculations. example shown fig. conventional neural network linear calculation inner product input neurons corresponding weights nonlinear calculation conventional neural network activation function pooling consequently deﬁnition basic function bdnn also deﬁned linear nonlinear calculations. boolean binary operations several derived operations truth table operations shown table obviously exclusive equivalence chosen results balanced bdnn chose equivalence linear calculation. represent represent result equivalence equal multiplication real number. convenience treat binary values real numbers corresponding real number operation instead boolean operation shown fig. assume input neurons bdnn corresponding weights output neuron mentioned above linear calculation bdnn second nonlinear calculation bdnn deﬁned follows count number calculation return larger number. assume basic function bdnn linear nonlinear calculation deﬁned above figure shows example calculation inputs corresponding weights then boolean operation linear calculation results obtained. finally since results thus value output please note although written real number function denotes boolean operation equivalence counting operation. forward propagation bdnn realized. certain input binary data bdnn calculate corresponding output also binary. shown fig. hybrid-bdnn contains three parts normal neural network part transition part bdnn part. lower layers hybrid-bdnn normal neural network part connected input data higher layers bdnn part generates result. transition part single layer normal neural network part bdnn part different neural networks combined. basic function normal neural network part conventional networks basic funtion bdnn introduced above. consequently forward propagation conducted normal neural network bdnn part. remaining problem transition part. deﬁne basic function transition part conduct forward propagation whole hybridbdnn. shown fig. assume input neurons transition part output neurons. mentioned above belongs normal neural network part thus real number. belongs bdnn part binary values. denote basic function transition part take example corresponding weights obtain summary bdnn suitable binary input data classiﬁcation binary image characters. input data non-binary hybrid-bdnn used. please note better many layers normal neural network part otherwise computation speed hybrid-bdnn slowed much. experiments used layer normal neural network part. usually gradient descent algorithm seen training method neural networks. training bdnn hybrid-bdnn also used algorithm. however compared conventional neural network bdnn obviously different properties thus directly algorithm bdnn training. order solve problem approximation conversion applied bdnn make suitable gradient descent training. conventional gradient descent training iteration weights adjusted small value nevertheless since weights bdnn binarized thus hard adjust weights way. hence order apply gradient descent algorithm training bdnn however point using real number must keep forward propagation result binarized network; otherwise training meaningless. therefore order satisfy request basic function training deﬁned. first assume input neurons weights output bdnn; corresponding real numbers returns larger number. accordingly basic function positive values negative ones positive value; otherwise negative. consequently obtain gradient descent algorithm applied train bdnn. although approximation used following experimental results showed that bdnn could trained well method. shown fig. backpropagation applied normal neural network part bdnn part. however order apply backpropagation hybridbdnn still need consider backpropagation transition part. first bdnn training training transition part real numbers used instead binary values. assume corresponding real number function real number version function then order keep forward propagation result satisfy following equation clearly inputs training bdnn forward propagation real number neuron training equals corresponding binary neuron bdnn including output neurons. result train bdnn real numbers. training need convert real number weights binary values trained bdnn obtained. structures also tested comparison. since basic training method bdnn order make fair comparison training optimization techniques conventional neural network used. example experiments learning rate ﬁxed simple hyperbolic tangent function used activation function. moreover datasets mnist cifar used experiments. know mnist dataset handwritten digits thus suitable binary data classiﬁcation test. contrast cifar used nonbinary data hybrid-bdnn tested. dataset training data used train networks; test data used validation data. training ﬁnished iteration lowest error rate test data seen ﬁnal result. early stages introduced three-layer network widely used classiﬁcation tasks. shown fig. network three different layers input layer hidden layer output layer. input layer contains input data output layer generates classiﬁcation results. besides layers fully connected. since network structure classic bdnn structure ﬁrst tested. experiments three-layer bdnn binary data mnist used. original images mnist grayscale used simple binarization method convert grayscale images binary. training data mnist used train network test data mnist used validation data. clearly training method bdnn proposed paper close conventional training method. thus possible borrow training techniques conventional neural networks bdnn dropout dropconnect. besides several special training techniques necessary bdnn. first training must keep number time. output deﬁned. result either positive number negative number. consequently following experiments bdnn neuron number always number. second training conventional neural network error output determined difference between output neuron ground truth. however training bdnn error determined difference also sign value. assume error certain output neuron corresponding ground truth error calculation given means output neuron ground truth positive negative error output otherwise error calculated conventional neural network. actually according output neuron sign ground truth output seen correct. technique essential bdnn training without training converge. different network structures studied classical three-layer network convolutional neural network besides bdnn hybrid-bdnn conventional neural networks experimental results shown table hand using proposed training method ﬁnally bdnn converged training dataset test dataset classiﬁcation rate around result feasibility bdnn proved. hand seen conventional structure much lower error rate bdnn structure. bdnn fully trained proposed method design speciﬁc training method bdnn performance improved lot. another possible reason complexity network. since binary neurons used bdnn complexity lower conventional though structure. result bdnn structure indicates that scale bdnn increased performance becomes better. deep learning research structure widely used image classiﬁcation. therefore experiments also tested structure. shown fig. used structure layers. ﬁrst layer input data followed layers feature maps last layers fully connected neurons used output layer. cnns convolutional operation feature size decreased pooling. nevertheless since hard realize pooling operation binary values thus feature size controlled skipping every pixel convolutional operation. structure layer input image size layer contains feature maps size obtained using kernels size layer contains feature maps size obtained using kernels size layer contains neurons obtained using kernels size layer contains neurons output. structure layer input image size layer contains feature maps size obtained using kernels size layer contains feature maps size obtained using kernels size layer contains neurons obtained using kernels size layer contains neurons output. structure layer input image size layer contains feature maps size obtained using kernels size layer contains feature maps size obtained using kernels size layer contains neurons obtained using kernels size layer contains neurons output. structure used bdnn tested mnist. structure used hybrid-bdnn tested cifar therefore structure hybrid-bdnn input layer following kernels real numbers rest network bdnn part. first error rates mnist shown table clearly using deeper structure performance bdnn improved. meanwhile normal structure still much better result. structure feature maps used result bdnn became better. also shows potential bdnn larger scales. second error rates cifar shown table cifar database color images different kinds objects. experiment color images converted grayscale used. since database difﬁcult error rates networks high though largest network structure used. another observation although training error rate hybrid-bdnn much lower test error rate lower. indicates possible hybrid-bdnn non-binary data classiﬁcation larger network structure high complexity necessary reducing training error hybrid-bdnn. bdnn ﬁrst tried brand binarized propagation function neural networks. moreover special gradient descent training method also proposed bdnn. experimental results proved able bdnn like conventional dnn. besides changing network structure training strategy trial provide thought improving extend usage. computational limitation didn’t test bdnn large scale paper. therefore performance bdnn comparable stateof-the-art results. future develop based training program train bdnn larger scale. various network structures also studied deep fully connected neural network recurrent neural network moreover optimization forward propagation bdnn studied. work started beginning based work patent applied november application number patent also submitted paper cvpr rejected. however still believe work valuable bdnn promising solution low-performance device deep learning models.", "year": 2016}