{"title": "Supervised Dictionary Learning by a Variational Bayesian Group Sparse  Nonnegative Matrix Factorization", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Nonnegative matrix factorization (NMF) with group sparsity constraints is formulated as a probabilistic graphical model and, assuming some observed data have been generated by the model, a feasible variational Bayesian algorithm is derived for learning model parameters. When used in a supervised learning scenario, NMF is most often utilized as an unsupervised feature extractor followed by classification in the obtained feature subspace. Having mapped the class labels to a more general concept of groups which underlie sparsity of the coefficients, what the proposed group sparse NMF model allows is incorporating class label information to find low dimensional label-driven dictionaries which not only aim to represent the data faithfully, but are also suitable for class discrimination. Experiments performed in face recognition and facial expression recognition domains point to advantages of classification in such label-driven feature subspaces over classification in feature subspaces obtained in an unsupervised manner.", "text": "abstract— nonnegative matrix factorization group sparsity constraints formulated probabilistic graphical model assuming observed data generated model feasible variational bayesian algorithm derived learning model parameters. used supervised learning scenario often utilized unsupervised feature extractor followed classification obtained feature subspace. mapped class labels general concept groups underlie sparsity coefficients proposed group sparse model allows incorporating class label information find dimensional label-driven dictionaries represent data faithfully also suitable class discrimination. experiments performed face recognition facial expression recognition domains point advantages classification label-driven feature subspaces classification feature subspaces obtained unsupervised manner. ince appearance seminal paper become popular data decomposition technique succesful applications still growing number fields data nonnegative pixel intensities computer vision amplitude spectra audio signal analysis signal analysis term counts document clustering problems item ratings collaborative filtering. aims decompositions nonnegative matrices. throughout paper columnwise dictionary features organized columnwise matrix coefficients projected onto dictionary assumptions data effectively achieved way. although decomposition nonunique general able produce strictly additive decompositions perceived part-based adding additional bias model different sparsity promoting regularizers proposed divergence-based also include higher order data descriptions many variants developed e.g. local locality constraints non-smooth regularization sparse localized features smoothness constraints graph regularized manifold regularized recently alternative formulations probabilistic framework developed allowing closely related work paper formulations extensions group sparsity constraints. addressing classification problem classify different tasks performed different subjects divergence based mixed-norm regularization imposed dictionary elements taskrelated features close possible separatedly features reﬂect task-independent characteristics required possible proposed. another divergence based algorithm group sparsity penalizations coefficient matrix proposed generative model enforcing groups features previously mentioned properties trained using variational bayesian approach. variant itakura-saito divergence direct group-sparsity enforcing penalization coefficient matrix succesfully applied blind audio source separation. recently generative model trained markov chain monte carlo proposed separate features groups common bases individual bases laplacian scale mixture distributions priors groups applied blind music source separation. work presented paper come probabilistic modeling track research group sparsity constraints exponential scale mixture distributions imposed coefficient matrix directly rather seeking group sparsity constraining groups features comparatively noted algorithms bayesian type. although aims impose group sparsity common individual features resemblance presented model bears laplacian scale mixture prior groups uses metropolis-hastings algorithm parameter estimation choices distributions hierarchical model. group sparse presented paper however engineered allows efficient learning performed using mean-field variational bayesian methodology deterministic features explicit variational bound calculation based upon model comparison selection made commonly utilized scenarios unsupervised learning decompositions suitable clustering supervised learning used unsupervised feature extractor followed classification obtained feature space. cases incorporate label information. applications presented paper focused latter case instead ignoring data labeling proposed group sparse model setup label-driven attempt bring potential information labeling find feature subspaces classification. remaining scenario semisupervised learning applications divergence based algorithms developed also possessing ability include label information. noise distributions equivalent algorithms different corresponding divergences. precisely estimation probabilistic gaussian poissonian gamma noise correspond minimization euclidean kullback-leibler itakura-saito divergences case maximum-aposteriori estimation exponential prior factor corresponds sparsity promoting regularization. anconnection probabilistic modeling worth noting probabilistic latent semantic analysis expectation-maximization algorithm equivalent kl-nmf algorithm using multiplicative updates apart estimators bayesian methods successfully employed efficient parameter learning conditional distribution denotes hidden variables model observed variables model parameters instrumental variational approximation quantified kullbacknote that distribution mixing variable discrete expression collapses discrete mixture exponentials specific shape parameters. exponential distribution obtained truncation laplacian distribution follows exponential scale mixture special case laplacian scale mixture distribution proposed learning algorithm layed matrix form computationally efficient matrix operations hyperparameters variational parameters model organized matrices according table table respectively. feasibility prioritized choice suitable priors gets narrowed down. suggests conjugacy desireable consideration. conjugateexponential models used variational bayesian interested reader referred also variational family general conjugate model would suggest analytical expressions appropriate optimization. categorical mixture selector variable. according exponentially distributed different scale parameters =k<. different selections multiple variables exist organized groups {...s} l_‘{...s}. reciprocals variables representing mean values exponential distributions mixture interpreted continuous indicators large averaged outcomes group inverse gamma probability density functions masses density functions tuned concentrated small values. priors constraints minority groups expected exponential distributions significantly large mean values suitable describe group sparse processes consistently probabilistic setup. specifically seen later alternating updates hidden variables model require which putting summation simple algebraic manipulation compactly become cases algorithms cannot include information straightforwardly therefore done fully unsupervised manner exactly comparative advantage proposed algorithm classification problems lies. weight role quality decomposition obtained dictionary learning stage sense class-to-class discriminative information able bring rather role classifier simple classifier choice classification stage -nearest neighbor cosine distance metric. performance quantifiers obtained runs -fold crossvalidation different random restarts algorithms; pass dictionary learning algorithms performed training only followed obtaining representation test feature space linear least squares nonnegativity constraints parameter optimization done using parameter sweeps using previously mentioned crossvalidation scheme obtain performance measures; criterion parameter selection chosen highest crossvalidated estimate maximal accuracy restarts random initializations. found best parameters algorithms reported accuracies crossvalidated estimates maximal accuracy face recognition jaffe facial expression recognition. yale dataset consists grayscale images subject subjects image different facial expression configuration center-light w/glasses happy left-light w/no glasses normal right-light sleepy surprised wink. dataset consists different images distinct subjects taken different times varying lightning facial expressions facial details jaffe dataset collection images facial expressions angry disgust fear happy neutral surprise posed japanese female models. results compared classification methods based related algorithms also available relevant published results datasets different approaches. matlab/octave implementation algorithm well scripts used generate results available author's homepage received upon request. images yale dataset used experiments prepared media laboratory aligned rotation centering manually determined locations eyes cropped. additionaly specifically paper downsampling images factor alleviate computational load pixelwise masking applied remove background torso hair. finally histograms masked images equalized. case dataset faces taken different angles centering attempted. images downsampled factor histogram equalization. images jaffe dataset first roughly aligned congealing resized factor masked leaving pixels roughly correspond locations faces followed histogram equalization. datasets images vectorized image column vector input matrix factorized. examples preprocessed images shown fig. classification method consists three consecutive stages image preprocessing stage dictionary learning stage classification stage. preprocessing dependent dataset described detail preceeding subsection. dictionary learning stage dictionary obtained proposed group sparse algorithm standard sparse algorithms; case probabilistic group sparsity algorithm class labels taken account umber iterations family algorithms case data projected space significant principal components crossvalidated estimate accuracy reported. decomposition algorithms used experiments comparison referred short principal component analysis nmf_kl kl-divergence includes weighted penalty term encourage sparsity right matrix different perspective structural pattern labels recognized using quantifiers fig. where rather presented directly accross samples norm coefficients accumulated accross samples label normalized cardinalities corresponding labels. furthermore fig. diagrams type reciprocal classification performance illustrated fig. fig. fig. yale jaffe datasets respectively. yale increases accuracy improves hitting peak begins deterio. qualitatively similar behavior dataset denotes number groups. g‹›ﬁ hyperprior rows contain gqu⇀ indices bias corresponding =fk<.f‘˝ towards larger values hierarchically propagates related hidden factors ?ff‘˝ giving elements sparse prior large expected value small expected value othcoefficients significantly large samples belonging single group only. presented experiments number representative coefficients chosen equally distributed among groups i.e. group number representative features bound. behavior prior yale dataset eyeballed fig. fig. fig. correspond visualize qualitatively typical mappings feature space representative several chosen parameter setups. specifically enforced prior representative features label; note dataset labels totals features. number samples training fig. presents samples feature space heatmaps samples sorted according labels coefficients according cumulative norm label averaged number samples label. baseline example nmf_vb decomposition presented fig. observed magnitude difference degree mixing specified features controlled. group sparse decompositions obtained prior would bias distinct features prevalent accross specific groups depicted fig. resembles result concatenating decompositions obtained label separately i.e. label group features groupwise strictly separated terms mixing represented fig. extremes sweet spot obtaining representation spaces good discriminative properties found case relates fig. indicative justifiedness line reasoning fact fig. obtained using parameters yielded best dataset shown table improvements still observed lesser extent. community classification problem dataset known easier side alone gives high accuracy. regarding algorithms concluded sparsity constrains sufficient give performance high quality leaving little space improvement group sparsity constraints. similar results jaffe dataset presented table improvement using gsnmf less marginal compared results table seen case nmf_gs conjunction classifier output classification results level discrete wavelet transform linear discriminant analysis used find features followed classification using support vector machines different kernel choices. somewhat lower accuracies reported experimental setup used consists processing samples gabor filtering sampling fiducial points followed features finalized classifier uses gabor filtering features twolayered perceptron label discrimination. authors classifier based gaussian processes original pixel space. xplanation effect that jaffe dataset droop caused restrictive mixing resulting decompositions subjects different expressions exhibit hardly common features i.e. expression-independent subject-specific information shared groups consequently features forced holistic extract expressions exclusively effect behind behavior yale dataset however yale dataset distances subjects different expressions configuration smaller distances different subjects expressions under configuration allowing satisfactory class discrimination even extremely holistic features remind reader goal yale dataset subject recognition regardless different expressions configuration jaffe face expression recognition regardless subject making experimental results yale dataset summarized table compared classification using nmf_kl nmf_vb improvement performance turned significant classification performed feature subspaces obtained nmf_gs. showed significantly higher average peak performance higher average performance smaller variance also. probabilistic formulation group sparsity constraints layed efficient variational bayesian algorithm approximate learning model parameters. shown prevalence specific features accross groups degree mixing groups controlled. identity-mapped class labels general notion groups presented model utilized supervised feature extractor face recognition facial expression recognition applications beneficial effects decomposition subspaces classification performance observed. everal reported results datasets known author unfortunately little results evaluated nonuniformly accross publications. practical point view however even though peak performance classification nmf_gs admirable problem priori selection nmf_gs decomposition bound produce peak remains open. problem characteristic nmf_gs also methods dependency decompositions initial values. even though variational bayes methodology allows calculation variational bound model comparisons made based upon presented experiments variational bound found uncorrelated classification accuracy attributed fact classifier stands outside bayesian framework i.e. objective directly connected classification embedded probabilistic model. still solution always remains evaluate classification performance separate validation optimaliity indicator determine dictionary select. zhang cheng learning spatially localized parts-based representation proceedings ieee computer society conference computer vision pattern recognition cvpr vol. uture work presented subject includes pursuing modifications group sparse formulation would work semi-supervised settings. model ideally allow efficient inference learning labels unlabeled data avoiding sampling techniques execution speed possible. comparison geometry-based gabor-waveletsbased facial expression recognition using multi-layer perceptron proceedings third ieee international conference automatic face gesture recognition ivek received b.s. electrical engineering faculty electrical engineering computing university zagreb zagreb croatia joined ruđer boškovic institute current position research assistant computational intelligence methods measurement systems project. currently pursuing ph.d. electronics faculty electrical engineering croatia. computing bfccbc=exp−log) a==; wgxxg=exp−.a+loga−g v−log g>v>; a=gv loga=+logv; ℋ=−+logv+g+log; abcdcefgh=wgxxg; >hefcbxfgh a.⋮a*c .⋮*=mg a*=c ℋ=−log−∑aglogg ffoℎhde a.⋮a*|>.⋮>p=exp>.−⋮>p−log >q>; a*=⋮− a.⋮a*=−log++fcodedlog log*m log*)=explog m=.⋮p ℋ=−∑ −.∗exp|o.∗|o∗exp|{+exp|o∗exp|{.∗|{. exp|o∗exp|{ po.⁄ −yq∗w.∗y{+logyq∗w−n{.∗logp{+{.∗log {++log{ ∗yq+nq−.∗|q−nq.∗logpq−lognq −q∗q+logq+q+logq number columns denoted. fig. controlling prevalences features labels yale dataset hinton diagrams normalized norm coefficient matrix accumulated accross labels corresponding features overlayed nmf_gs increasing parameter controlling prevalences features labels jaffe dataset hinton diagrams normalized norm coefficient matrix accumulated accross labels corresponding features overlayed nmf_gs increasing parameter", "year": 2014}