{"title": "Demystifying Parallel and Distributed Deep Learning: An In-Depth  Concurrency Analysis", "tag": ["cs.LG", "cs.CV", "cs.DC", "cs.NE"], "abstract": "Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. Specifically, we present trends in DNN architectures and the resulting implications on parallelization strategies. We discuss the different types of concurrency in DNNs; synchronous and asynchronous stochastic gradient descent; distributed system architectures; communication schemes; and performance modeling. Based on these approaches, we extrapolate potential directions for parallelism in deep learning.", "text": "deep neural networks becoming important tool modern computing applications. accelerating training major challenge techniques range distributed algorithms low-level circuit design. survey describe problem theoretical perspective followed approaches parallelization. specifically present trends architectures resulting implications parallelization strategies. discuss different types concurrency dnns; synchronous asynchronous stochastic gradient descent; distributed system architectures; communication schemes; performance modeling. based approaches extrapolate potential directions parallelism deep learning. concepts general reference surveys overviews; computing methodologies neural networks; distributed computing methodologies; parallel computing methodologies; machine learning; introduction machine learning particular deep learning field rapidly taking variety aspects daily lives. core deep learning lies deep neural network construct inspired interconnected nature human brain. trained properly expressiveness dnns provides accurate solutions problems previously thought unsolvable simply observing large amounts data. deep learning successfully implemented plethora subjects ranging image classification speech recognition medical diagnosis autonomous driving defeating human players complex games since neural networks attracted attention machine learning community however dnns rose prominence available computational power permitted training workstations exploiting inherent parallelism. consequently first successful modern alexnet managed outperform existing datasets increase size dnns complexity computational intensity memory demands deep learning increase proportionally. training competitive accuracy today essentially requires cluster machines high-performance computing architectures. harness computational power available systems different aspects training inference dnns modified increase underlying concurrency. survey discuss variety topics context parallelism distribution spanning vectorization efficient supercomputers. particular present parallelism strategies evaluation implementations thereof well extensions training algorithms systems targeted supporting distributed environments. provide comparative measures approaches analyze concurrency average parallelism using work-depth model related surveys surveys field deep learning focus applications deep learning neural networks history scaling deep learning hardware architectures dnns neural networks three surveys describe dnns origins deep learning methodologies historical perspective well discuss potential capabilities dnns w.r.t. learnable functions representational power. three surveys also describe optimization methods applied regularization techniques detail. another paper discusses scaling deep learning various perspectives focusing models optimization algorithms datasets. paper also overviews aspects distributed computing including asynchronous sparse communication. surveys hardware architectures mostly focus computational side training rather optimization. includes recent survey reviews computation techniques layers mapping computations hardware exploiting inherent parallelism. survey also includes discussion data representation reduction reduce overall memory bandwidth within hardware. surveys discuss accelerators traditional neural networks fpgas deep learning section defines terminology algorithms used deep neural networks section presents dnns detail elaborating different layers analyzing inherent section describes layers presented section computed differently expose section explores analyzes main approaches shared-memory parallelism section provides overview distributed-memory training describing modifications training algorithms techniques reduce communication system implementations. section gives concluding remarks extrapolates potential directions field. paper surveys works obtained recursively tracking relevant bibliography seminal papers field dating back year include additional papers resulting keyword searches google scholar arxiv. quadratic increase deep learning papers latter source works included. full list categorized papers survey found online. terminology algorithms section establishes theory naming conventions material presented survey. first discuss classes supervised unsupervised reinforcement learning problems followed relevant foundations parallel programming. https//scholar.google.com/ https//www.arxiv.org/ https//spcl.inf.ethz.ch/research/parallel_programming/distdl/ data probability distribution training dataset model parameters. iteration model function ground-truth label per-sample loss function gradient parameter update rule. function loss gradient parameters iteration supervised learning machine learning supervised learning process optimizing function labeled samples that given sample function would return value approximates label. assumed dataset other unobserved samples sampled probability distribution. throughout survey refer operators probability expectation random variables; denotes random variable sampled probability distribution ez∼d denotes expected value random variable notations used section summarized table formally given probability distribution data random variable domain construct samples from label domain hypothesis class containing functions wish minimize loss function represents true label practice common class functions defined vector parameters order define parameter space. example represent n-dimensional hyperplane separates samples classes coefficients. deep neural networks define multiple layers namely parameter layer index work consider types supervised learning problems sample loss functions derived classification regression. former goal identify class sample likely belongs e.g. inferring type animal appears image. latter type goal find relation domains predicting values samples instance regression problem might predict future temperature region given past observations. minimization purposes sample loss function continuous differentiable. regression problems possible loss functions squared difference hand classification problems simplest definition loss binary loss i.e. otherwise. however definition match continuity differentiability criteria. resolve issue prominent multi-class classification problems define probability distribution inferred class types instead single label. loss function computes difference predicted distribution true label distribution. normalize model output softmax function typically used cross-entropy loss seen generalization logistic regression introduces continuous loss function multi-class classification. minimizing loss function performed using different approaches iterative methods meta-heuristics prominent method machine learning iterative gradient descent. however since full never observed necessary obtain unbiased estimator gradient. observe ez∼d thus expectation perform gradient descent randomly sampled data iteration applying stochastic gradient descent algorithm iteratively optimizes parameters defined sequence {w}t using samples dataset assumed sampled proven converge rate convex functions lipschitz-continuous bounded gradient ill-posed nature machine learning problems selection important reflect final loss value. choice initial weights originate random values informed decisions pre-trained weights methodology called transfer learning deep learning recent works state optimization space riddled saddle points assume value affect final loss. practice however improper initialization adverse effect loss values networks become deeper line denotes number steps typically real-world instances constant number steps fixed period time desired accuracy achieved. line samples random elements dataset provide unbiased loss estimator. practice implemented shuffling dataset loop processing permutation entirely step called epoch. full training procedure usually consists tens hundreds epochs element sampled gradient loss function respect weights computed deep neural networks gradient obtained respect layer using backpropagation obtained gradient used updating current weights using weight update rule weight update rules. weight update rule denoted algorithm defined function gradient previous weight values current iteration table summarizes popular functions used training. table basic update rule usдd represents learning rate. controls much gradient values overall affect next estimate iterative nonlinear optimization methods finding correct considerable part computation machine learning problems customary iteration-based weight update rule ualr decreases time bound modification size avoid local divergence. popular weight update rules include momentum uses difference current previous weights overcome local minima natural motion recent update rules rmsprop adam first second moments gradient order adapt learning rate per-weight enhancing sparser updates others. factors learning rate symbols found table called hyper-parameters optimization process begins. table represent momentum decay rate first second moment decay rate hyper-parameters respectively. obtain best results hyper-parameters must tuned performed value sweeps meta-optimization multitude hyper-parameters reliance upon considered problematic part community unsupervised reinforcement learning classes machine learning unsupervised reinforcement learning. first class dataset labeled exist) training typically results different objective functions intended infer structure unlabeled data. second class refers tasks environment observed given points time training optimizes action policy function maximize reward observer. context deep learning unsupervised learning useful applications auto-encoders generative adversarial networks auto-encoders constructed neural networks receive sample input output value close possible. training networks possible instance feed samples artificially-added noise optimize network return original sample recent development machine learning. employ deep neural networks generate realistic data simultaneously training networks. first network trained distinguish real dataset samples fake generated samples second network trained generate samples similar dataset possible. class reinforcement learning utilizes dnns different purposes defining policy functions reward functions. training algorithms differ supervised unsupervised learning using methods deep learning details algorithms scope survey parallelization techniques similar. parallel computer architecture continue brief overview parallel hardware architectures used execute learning problems practice. roughly classified single-machine multi-machine systems. single-machine parallelism. parallelism ubiquitous today’s computer architecture internally chip form pipelining out-of-order execution well exposed programmer form multi-core multi-socket systems. multi-core systems long tradition programmed either multiple processes multiple threads both. main difference multiprocess parallel programming forces programmer consider distribution data first-class concern multi-threaded programming allows programmer reason parallelism leaving data shuffling hardware system general-purpose cpus optimized general workloads ranging event-driven desktop applications datacenter server tasks machine learning tasks often compute intensive making similar traditional high-performance computing applications. thus large learning workloads perform well accelerated systems general purpose graphics processing units field-programmable gate arrays used field decade now. devices focus compute throughput specializing architecture utilize high data parallelism workloads. later learning researchers utilize accelerators gpus fpgas computations. emphasize main technique acceleration exploit inherent parallelism learning workloads. reviewed papers papers present empirical results provide details hardware setup. fig. shows summary machine architectures used research papers years. clear trend towards gpus dominate publications beginning however even accelerated nodes sufficient large computational workload. fig. illustrated quickly growing multi-node parallelism works. shows that beginning distributed-memory architectures accelerators gpus become default option machine learning scales today. multi-machine parallelism. training large-scale models compute-intensive task. thus single machines often capable finish task desired time-frame. accelerate computation further computation distributed across multiple machines connected network. important metrics interconnection network latency bandwidth message-rate. different network technologies provide different performance. example modern ethernet infiniband provide high bandwidth infiniband significantly lower latencies higher message rates. special-purpose interconnection networks achieve higher performance three metrics. network communication remains generally slower intra-machine communication. fig. shows breakdown number nodes used deep learning research years. started high large-scale distbelief reduced slightly introduction powerful accelerators quick rise since advent large-scale deep learning. reviewed papers make distributed-memory systems provide details hardware setup. observe large-scale setups similar machines commonplace essential today’s training. parallel programming programming techniques implement parallel learning algorithms parallel computers depend target architecture. range simple threaded implementations openmp single machines. accelerators usually programmed special languages nvidia’s cuda multiple machines distributed memory either simple communication mechanisms tcp/ip remote direct memory access distributed memory machines also convenient libraries message passing interface apache spark. level library focused providing portable performance spark higher-level framework focuses programmer productivity. fig. shows breakdown different communication mechanisms specified papers using multi-node parallelism. shows community quickly recognized deep learning similar characteristics large-scale applications. thus beginning established interface became de-facto portable communication standard distributed deep learning. parallel algorithms briefly discuss concepts parallel computing needed understand parallel machine learning. every computation computer modeled directed acyclic graph vertices computations edges data dependencies computational parallelism graph characterized main parameters graph’s work corresponds total number vertices graph’s depth number vertices longest path dag. parameters allow characterize computational complexity parallel system. example assuming process operation time unit time needed process graph single processor time needed process graph infinite number processes average parallelism computation often good number processes execute graph with. furthermore show execution time processors bounded min{w/p operations learning modeled operations tensors operations highly dataparallel summations introduce dependencies. thus focus parallel reduction operations following. reduction apply series binary operators combine values single value e.g. operation associative change application changes linear-depth line-like graph shown fig. logarithmic-depth tree graph shown fig. simple show work depth reducing numbers respectively. deep learning often needs reduce large tables independent parameters return result processes. called allreduce specification multi-machine environments tables distributed across machines participate overall reduction operation. relatively bandwidth machines operation often critical distributed learning. analyze algorithms simplified logp model ignore injection rate limitations makes similar simple model models point-to-point latency network models cost byte number networked machines. based model above simple show lower bound reduction time simplified model. furthermore element table sent least once second lower bound represents size single data value number values sent. bound strengthened γmg/p disallow redundant computations several practical algorithms exist parallel allreduce operation different environments best algorithm depends system number processes message size. refer chan hoefler moor survey collective algorithms. here summarize algorithms rediscovered context parallel learning. simplest algorithm combine trees summing values process similar fig. broadcasting values back processes; complexity ttree log. algorithm inefficient optimized simple butterfly pattern reducing time tbfly log. butterfly algorithm efficient small large small simple linear pipeline splits message segments bandwidth-optimal performs well practice even though linear component tpipe ranges could rabenseifner’s algorithm combines reduce-scatter gather running time trabe γmg/p. algorithm achieves lower bound harder implement tune. communication problems needed convolutions pooling illustrated fig. exhibit high spatial locality strict neighbor interactions. optimized using well-known techniques stencil computations neighborhood collectives optimized remote memory access programming general exploring different low-level communication message scheduling topology mapping strategies well-known field could significantly speed communication distributed deep learning. minibatch performing common decrease number weight updates computing sample loss minibatches averaging gradient respect subsets data minibatches represent trade-off traditional proven converge drawing sample time batch methods make entire dataset iteration. minibatch shown algorithm optimizes training loss respect groups samples dataset compute gradient apply widely-used backpropagation method sampled minibatch setting minibatch size complex optimization space merit affects statistical accuracy hardware efficiency. illustrated fig. minibatches small harness inherent concurrency evaluation loss function; large quality training decays increased beyond certain point. behavior empirically shown larger minibatch sizes fig. therefore typical size minibatch lies orders past minibatch sizes deep learning increase beyond samples memory constraints accuracy degradation. however networks smaller memory footprint adaptive choice learning rates definition warmup phase became possible train dnns minibatch sizes samples without significant loss accuracy. recent works even treat minibatch size adaptive hyper-parameter increasing training progresses overall works increase upper bound feasible minibatch sizes remove deep neural networks describe anatomy deep neural network fig. scales single layer composition layers deep network rest section describe popular layer types properties followed computational description deep networks backpropagation algorithm. then study several examples popular neural networks detail highlighting computational trends driven definition. neurons basic building block deep neural network neuron. modeled brain artificial neuron accumulates signals neurons connected synapses. activation function applied accumulated value adds nonlinearity network determines signal neuron fires neighbors. feed-forward neural networks neurons grouped layers strictly connected neurons subsequent layers. contrast recurrent neural networks allows back-connections within layer. feed-forward layers. neural network layers implemented weighted sums using synapses weights. activations implemented different functions sigmoid softmax hyperbolic tangents rectified linear units variants thereof color images used input usually represented -dimensional tensor sized n×c×h×w shown fig. number images minibatch image contains channels layer disregards spatial locality image tensor dimensions flattened typical constructions number channels well width height image change layer layer using operators defined below. denote input output channels layer cout respectively. fully connected layer defined group neurons weight matrix per-layer trainable bias vector inner product usually implemented multiplication addition works operators similarity layers neural network fully connected. sparsely connecting neurons sharing weights beneficial reducing number parameters; case popular convolutional layer. convolutional layer every tensor convolved cout kernels size cin×ky×kx base formula minibatch given dimensions cout accounting size convolution consider cases kernel image bounds. note formula omits various extensions operator variable stride padding dilation modifies accessed indices inner loops called convolution kernel kernel size convolutional layers computationally-intensive dnns layer types prominently used networks. layers pooling batch normalization layers. former reduces input tensor width height dimensions performing operation contiguous sub-regions reduced dimensions maximum average given goal layer reduce size tensor sub-sampling emphasizing important features. applying subsequent convolutions kernel size sub-sampled tensor enables learning high-level features correspond larger regions original data. batch normalization example layer creates interdependencies samples minibatch. role center samples around zero mean variance which according authors reduces internal covariate shift. given following transformation recurrent layers. recurrent neural networks enable connections layer’s output inputs. connections create state neurons retaining persistent information network allowing process data sequences instead single tensor. denote input tensor time point represents hidden data time-point carried next time-point. despite initial success layers found tend forget information quickly address issue long-short term memory units redesign structure recurrent connection resemble memory cells. several variants lstm exist gated recurrent unit simplifies lstm gates reduce number parameters. deep networks according definition fully connected layer expressiveness shallow neural network limited separating hyperplane skewed nonlinear activation function. composing layers another create deep networks approximate arbitrarily complex continuous functions. exact class expressible function currently open problem results show neural network depth reduce breadth requirements exponentially additional layer. deep neural network represented function composition e.g. ℓ))) function layer described section vector represents layer weights addition direct composition might reuse output values layer multiple subsequent layers forming shortcut connections computation loss gradient necessary performed repeatedly applying chain rule process commonly referred backpropagation. shown fig. process obtaining performed steps. first computed forward evaluation computing layer dependencies topological ordering. computing loss information propagated backward network computing gradients note layers maintain mutable parameters thus always computed. terms concurrency work-depth model formulate costs computing forward evaluation backpropagation different layer types. table shows work performed layer asymptotically dominates maximal operation dependency path logarithmic parameters. result reaffirms state practice parallelism plays major part feasibility evaluating training dnns. opposed feed-forward networks rnns contain self-connections thus cannot trained backpropagation alone. popular solve issue applying backpropagation time unrolls recurrent layer certain amount sequence length using weights time-point. creates larger feed-forward network trained usual means. addition loss function common practice regularize objective function order promote generalization avoid exploding gradient values. several ways regularize training process. first possible apply tikhonov regularization adding normalized parameter values separate term objective function i.e. ∥cx∥ normalization matrix either norm used. second gradient clipping applied normalize derivatives larger threshold multiplying ∥∇ℓ∥ technique proven useful training rnns handling asynchronous updates distributed deep learning case studies understand successful neural architectures orchestrate aforementioned layers discuss five influential networks highlight trends characteristics past years. networks listed table achieved state-of-the-art performance upon publication. table summarizes networks computational characteristics achieved test accuracy imagenet cifar- datasets. experimentation period different types neural network structures researched methods optimize developed. neural network community converged deep feedforward networks research growth period yielded networks larger sizes operations attempt solve increasingly complex problems. trend supported advent gpus large computational resources however over-parameterization leads overfitting since resulting networks large consumer devices efforts decrease resource usage started around research since focused increasing expressiveness mostly producing deeper networks also reducing number parameters operations required forward-evaluate given network. reducing memory increasing energy efficiency resource conservation trend aims move neural processing user i.e. embedded mobile devices. time smaller networks faster prototype require less information communicate training distributed platforms. lenet first successful convolutional neural network designed identify hand-written digits mnist dataset shown fig. lenet- takes single-channel input performs series convolutions subsamples filtered images max-pooling. convolution-pooling layer sequence occurs again followed fully connected layers final fully connected softmax layer produce results. alexnet alexnet architecture winner imagenet large-scale visual recognition competition. yielding nearly twofold increase accuracy preceding state-of-the-art network played major role current state machine learning. similar lenet alexnet contains series convolution-pooling layers followed fully connected layers. however also uses sequences convolutions local response normalization layers. major factors success alexnet data augmentation network regularization training. googlenet opposed large number parameters alexnet googlenet architecture employs smaller series convolutions organized repeating modules. inspired network-in-network architecture modules invoke ×cin convolutions modules increase expressiveness increasing depth time dimensionality reduction modules essentially trading breadth depth. resnet trend dnns becoming deeper narrower continued successful networks like comprising layers convolutions. however networks increased depth successful training become harder. authors resnet address depth issue training slightly different inter-layer interaction instead composing layers described section every convolutional module would input output residuals implemented shortcut identity connections network. system trains layers respect residuals instead original values according authors solves inherent degradation accuracy networks become deeper. densenet following success resnets densenets increase number connections layers. opposed module identity shortcut denselyconnected blocks concatenate layer’s outputs inputs next layers similar according authors type connection induces better gradient propagation features subsequent layers required strictly high-level. rather since layer also receives inputs previous level features constructed low-level information resulting high-level outputs. practical result half parameters required operations densenets achieve similar results resnets. depth increased layers densenets reduce validation error factor cifar- compared resnets. comes cost increasing number parameters almost order magnitude. specifically resnets achieve error parameters whereas densenets achieve error parameters. outlook. summary recent architectures heavily base convolutional operations deep become narrower time. known direction next breakthrough take; efforts compression recent architectures seem focus decreasing number parameters operations maintaining accuracy increasing nevertheless still necessary vast computational resources train networks alone find networks automatically. layer computation given neural network layers operate -dimensional tensors high locality operations several opportunities parallelizing layer execution. cases computations directly parallelized. however order expose parallelism layer types computations reshaped. below list efforts model performance followed concurrency analysis three popular layer types. performance modeling even work depth models hard estimate runtime single layer alone entire network. fig. presents measurements performance cublassgemm highly-tuned matrix multiplication implementation nvidia cublas library core layer types described below. figure shows matrix dimensions modified performance change linearly practice system internally chooses implementations operation left-hand side figure depicts segmentation. shall show wide variety approaches implement convolutional layers wherein principle applies. spite observation works still manage approximate runtime given performance modeling. using values figure lookup table possible predict time compute backpropagate minibatches various sizes error even clusters gpus asynchronous communication achieved cpus distributed environment using similar approach intel xeon accelerators strictly training time estimation paleo derives performance model operation counts alone pervasive cnns uses performance modeling select networks decreased accuracy match real-time requirements users. fully connected layers described section fully connected layer expressed modeled matrix-matrix multiplication weights neuron values efficient linear algebra libraries cublas used. blas general matrix-matrix multiplication present variety methods optimize implementations fully connected layers. particular paper shows efficient loop construction vectorization blocking unrolling batching. paper also demonstrates weights quantized fixed-point math instead floating point. convolution convolutions constitute majority computations involved training inference dnns. such research community industry invested considerable efforts optimizing computation platforms. fig. depicts convolution methods detailed below table summarizes work depth characteristics convolution layer computed directly equation states fully utilize resources vector processors many-core architectures geared towards many parallel multiplication-accumulation operations. however possible increase utilization introducing data redundancy basis transformation. first algorithmic change proposed convolutional layers well-known technique transform discrete convolution matrix multiplication using toeplitz matrices first occurrence unrolling convolutions cnns found cpus gpus used training method popularized consists reshaping images minibatch tensors matrices. matrix contains unrolled patch would usually convolved generating redundant information convolution kernels stored matrix column represents unrolled kernel multiplying matrices results matrix contains convolved tensor format reshaped subsequent operations. note operation generalized tensors converting single matrix multiplication. processor-friendly gemm method consumes considerable amount memory thus scalable. practical implementations gemm method cudnn implement implicit gemm toeplitz matrix never materialized. also reported strassen matrix multiplication used underlying computation reducing number operations second method compute convolutions make fourier domain convolution defined element-wise multiplication method data kernels transformed using multiplied inverse applied data formula below denotes fourier transform element-wise multiplication. note single minibatch enough transform reuse results. experimental results shown larger convolution kernels beneficial becomes yielding performance gemm method process patches proportional size kernels. additional optimizations made ifft operations using dnn-specific knowledge process uses decimation-in-frequency decimation-in-time ifft order mitigate bit-reversal instructions; multiple ffts sizes batched together performed warp-level gpu; pre-computation twiddle factors. working dnns fft-based convolution optimized further. authors observed zero-padding convolutional kernels considerably smaller images mostly consist zeros. thus pruned executed transforming kernels reducing number operations turn paper reports speedups cpus gpus respectively. prevalent method used today perform convolutions winograd’s algorithm minimal filtering first proposed method modifies original algorithm multiple filters performing following computation tile since number operations winograd convolutions grows quadratically filter size convolution decomposed tiled small convolutions method strictly used small kernels additionally magnitude elements expression table lists concurrency characteristics aforementioned convolution implementations using work-depth model. table method exhibits different behavior average parallelism determined kernel size image size coincides experimental results show one-size-fits-all convolution method. also work depth metrics always sufficient reason absolute performance direct imcol methods exhibit concurrency characteristics even though imcol faster many cases high processor utilization memory reuse opportunities. data layout also plays role convolution performance. assert convolution pooling layers computed faster transposing data n×c×h×w tensors c×h×w paper reports performance increase state-of-the-art single layer full paper reports speedup even case transposing data computation upon inputting tensor layer. primitive libraries cudnn mkldnn provide variety convolution methods data layouts. order assist users choice algorithm libraries provide functions choose best-performing algorithm given tensor sizes memory constraints. internally libraries methods pick fastest one. recurrent units complex gate systems occur within units contain multiple operations incurs small matrix multiplication element-wise operation. reason layers traditionally implemented series high-level operations gemms. however acceleration layers parallelism possible. moreover since units usually chained together types concurrency considered within layer consecutive layers. appleyard describe several optimizations implemented gpus. basic optimization performed fusing computations function saving intermediate results scratch-pad memory. reduces kernel scheduling overhead conserves round-trips global memory orders magnitude slower scratch-pad memory. optimizations include pre-transposition matrices enabling concurrent execution independent recurrent units different multi-processors gpu. inter-layer optimizations paper implements pipelining stacked unit computations immediately starting propagate memory consumption perspective dynamic programming proposed rnns order balance caching intermediate results recomputing forward inference backpropagation. long sequences algorithm conserves memory standard bptt adding time iteration. persistent rnns optimization addresses limitations utilization small minibatch sizes long sequences inputs. caching weights standard units registers optimize memory round-trips timesteps training order registers scheduled requires kernels execute layers persistent performing global synchronization circumventing normal programming model. approach attains speedup previous state-of-the-art minibatch sizes performing order multiple tflop/s per-gpu even though execute gemm operations loads memory multi-processor. additionally approach reduces total memory footprint rnns allowing users stack layers using resources. shared-memory parallelism high average parallelism neural networks harnessed compute individual layers efficiently also evaluate whole network concurrently respect different dimensions. owing minibatches breadth layers depth possible partition forward evaluation backpropagation phases among parallel processors. below discuss three prominent partitioning strategies illustrated fig. summarized table partitioning input samples network structure layer data parallelism training data computed minibatches size layers independent respect straightforward approach parallelization partition work minibatch samples different processors. method called data parallelism dates back first practical implementations artificial neural networks minibatch size. minibatches dnns initially driven data parallelism. multiple vector accelerator microprocessors used parallelizing error backpropagation neural network training. support data parallelism paper presents version delayed gradient updates call bunch mode gradient updated several times prior updating weights. method essentially equivalent minibatch sgd. data parallelism partitions given minibatch images inference among multiple computational resources apart batch normalization layers operate single sample time forward evaluation backpropagation almost completely independent. weight update phase however results partitions averaged obtain gradient w.r.t. minibatch potentially induces allreduce operation. furthermore partitioning method memory parameters accessible participating devices means that e.g. need duplicated among gpus. earliest occurrences mapping data parallel computations gpus performed paper focuses problem training deep belief networks mapping unsupervised training procedure gpus running minibatch sgd. paper shows speedup training restricted boltzmann machines. today data parallelism supported vast majority deep learning frameworks using single multiple gpus cluster multi-gpu nodes. scaling data parallelism naturally limited minibatch size. applying various modifications recent works successfully managed increase batch size samples samples even without losing accuracy. thus issue severe claimed prior works bottleneck hinders scaling data parallelism however layer requires full synchronization point upon invocation. since layers recur multiple times architectures costly. thus popular implementations follow approach driven small subsets minibatch normalized independently. least samples scheduled processor synchronization point local turn increases scaling. another approach problem define different layer altogether. weight normalization proposes separate parameter norm directionality re-parameterization. weights defined represents weight magnitude normalized direction layer essentially reduces depth operator removing inter-dependencies within minibatch. according authors approaches. additional approaches data parallelism proposed literature. parallelsgd times parallel dividing dataset among processors. convergence instances resulting weights aggregated averaged obtain resembling ensemble learning. parallelsgd well deep learning implementations designed mapreduce programming paradigm. using mapreduce easy parallel tasks onto multiple processors well distributed environments e.g. clusters. prior works scaling potential application mapreduce studied variety machine learning problems including promoting need shift single-processor learning distributed memory systems. mapreduce model successful deep learning first generality hindered dnn-specific optimizations. therefore current implementations make high-performance communication interfaces implement features asynchronous execution pipelining sparse communication fine-grained parallelism computational resources latter minibatches fragmented sub-batches hybrid cpu-gpu inference well memory footprint reduction. model parallelism second partitioning strategy training model parallelism strategy divides work according neurons layer namely dimensions -dimensional tensor. case sample minibatch copied processors different parts computed different processors conserve memory incurs additional communication every layer. major differentiating point model parallelism data parallelism trade-off accuracy scaling minibatch size change. nevertheless architecture creates layer interdependencies which turn generate communication determines overall performance. fully connected layers instance incur all-to-all communication neurons connect neurons next layer. another method proposed reducing communication fully connected layers cannon’s matrix multiplication algorithm modified dnns paper reports cannon’s algorithm produces better efficiency speedups simple partitioning small-scale multi-layer fully connected networks. cnns using model parallelism convolutional layers relatively inefficient. samples partitioned across processors convolutional layer would obtain results processors compute convolution operation sums channels. mitigate problem locally connected networks introduced. still performing convolutions lcns define multiple local filters region enabling partitioning dimensions incur all-to-all communication. using lcns model parallelism work presented managed outperform size running nodes -node multi-gpu cluster. behaving cnns i.e. weight sharing apart spatial image boundaries training communication-bound scaling achieved. successfully applying techniques cnns requires fine-grained control parallelism shall show section unfortunately weight sharing important part cnns contributing memory footprint reduction well improving generalization thus standard convolutional layers seen frequently lcns. second form model parallelism replication elements. treenets authors study ensembles dnns propose mid-point ensembles training single model certain layer creates junction multiple copies network trained paper defines ensemble-aware loss functions backpropagation techniques regularize training process. training process turn parallelized across network copies assigning copy different processor. results presented paper three datasets indicate treenets essentially train ensemble expert dnns. layers specific processors pipelining viewed form data parallelism since elements processed network parallel also model parallelism since length pipeline determined structure. first form pipelining used overlap forward evaluation backpropagation weight updates. scheme widely used practice increases utilization mitigating processor idle time. finer granularity neural network architectures designed around principle overlapping layer computations case deep stacking networks dsns step computes different fully connected layer data. however results previous steps concatenated layer inputs enables layer partially computed parallel relaxed data dependencies. layer partitioning several advantages multi-processor pipeline data model parallelism need store parameters processors forward evaluation backpropagation fixed number communication points processors source destination processors always known. moreover since processors always compute layers weights remain cached decrease memory round-trips. disadvantages pipelining however data arrive specific rate order fully utilize system latency proportional number processors incurred. following section discuss well-known implementations layer partitioning distbelief project adam combine advantages pipelining data model parallelism. alexnet computations performed convolutional layers parameters belong fully connected layers. mapping alexnet multi-gpu node using data model parallelism separately best reported speedup gpus successful example hybrid scheme applies data parallelism convolutional layer model parallelism fully connected part using hybrid approach speedup achieved gpus less accuracy loss. results also reaffirmed hybrid implementations speedup achieved gpus using approach. ampnet asynchronous implementation training cpus. defining intermediate representation neural networks includes control-flow constructs model parallelism achieved. particular internal fine-grained parallel tasks within layers identified scheduled asynchronously. additionally asynchronous execution dynamic control flow enables pipelining tasks forward evaluation backpropagation weight update main advantage ampnet recurrent tree-based gated-graph neural networks exhibit heterogeneous characteristics i.e. variable length sample dynamic control flow paper shows speedups tensorflow framework. lastly distbelief distributed deep learning system combines three parallelism strategies. implementation training performed multiple model replicas simultaneously replica trained different samples within replica distributed according neurons layer according different layers project adam extends upon ideas distbelief exhibits types parallelism. however project adam pipelining restricted different cores node. distributed deep learning distributing training process among multiple nodes share memory take many forms. discussed training algorithms copy up-to-date value directly visible processors. distributed environments multiple instances running independently thus overall algorithm adapted. distribution schemes deep learning categorized along three axes model consistency parameter distribution distributed training; fig. table summarize applied techniques optimizations. model consistency denote training algorithms up-to-date observed everyone consistent model methods directly dividing computations among nodes creates distributed form data parallelism nodes communicate updates others fetching minibatch. support distributed data parallel modify algorithm changing lines read weights parameter store centralized decentralized incurs substantial overhead overall system hinders training scaling. recent works relax synchronization restriction creating inconsistent model result training agent time contains copy weights denoted called staleness well-known instance inconsistent hogwild algorithm allows training agents read write parameters will overwriting existing values. hogwild proven converge sparse learning problems updates modify small subsets based foundations distributed asynchronous proof imposes lipschitz continuous differentiability strong convexity converges long staleness i.e. maximal number iterations reading writing bounded hogwild algorithm originally designed shared-memory architectures since extended distributed-memory systems still attains convergence deep learning problems. mitigate interference effect overwriting step implementation transfers gradient instead training agents. asymptotically lack synchronization hogwild gradientcommunicating variants admits optimal convergence rate participating nodes well linear scaling every agent train almost independently. provide correctness guarantees spite asynchrony stale-synchronous parallelism proposes compromise consistent inconsistent models. gradient staleness enforced bounded performing global synchronization step maximal staleness reached nodes. approach works especially well heterogeneous environments lagging agents kept check. distributed asynchronous processing additional advantage adding removing nodes on-the-fly allowing users resources introduce node redundancy remove straggling nodes practical implementations prominently-used model consistency approaches synchronous nodes allreduce operation still scales nearly linearly; asynchronous/ssp larger clusters heterogeneous environments centralization choice designing centralized decentralized network architecture training depends multiple factors including network topology bandwidth communication latency parameter update frequency desired fault tolerance. centralized network architecture would typically include parameter server infrastructure consist specialized nodes; whereas decentralized architecture would rely allreduce communicate parameter updates among nodes. trade-off using either distribution scheme modeled communication cost global update. allreduce operation implemented efficiently different message sizes nodes scheme requires training agent send receive information to/from nodes. thus network routes used terms communication operation equivalent reduce-then-broadcast implementation allreduce taking ttree time. however keep track global view training procedure performing gradient averaging place enabling asynchronous operation training agents. this turn allows nodes communicate less information performing computations well increases fault tolerance dynamic spin-up removal nodes training. infrastructure abstract concept necessarily represented physical server. sharded parameter servers divide responsibility multiple nodes containing piece data. conjunction model parallelism layer pipelining alleviates congestion shown fig. portion model replica transmits gradients receives weights different shard. hierarchical parameter servers alleviate resource contention assigning training agents leaves propagating weights gradients specific agent groups global parameter store. rudra also studies tradeoff allowed staleness number agents minibatch size showing performs better requires adapting learning rate accordingly. infrastructure beneficial performance also fault tolerance. simplest form fault tolerance machine learning checkpoint/restart periodically synchronized persisted non-volatile data store performed locally popular deep learning frameworks globally frameworks poseidon besides checkpoints fault tolerance distributed deep learning first tackled distbelief system training resilience increased introducing computational redundancy training agents well replicating parameter server shards. former agent constructed multiple physical nodes distbelief hybrid parallelism assigned multiple times separate groups nodes. allocating redundant agents enables handling slow faulty replicas cancelling work upon completion faster counterpart. latter resilience technique distbelief project adam parameters replicated persisted non-volatile memory using dedicated manager seen fig. project adam increases resilience distributed training using separate communication endpoints replication using paxos consensus nodes. applying weight updates distributed environment another issue addressed. section establish popular weight rules first-order respect required gradients such centralized decentralized schemes perform weight updates storing last gradient parameter values. since gpus commonly used training dnns frameworks geeps implement specialized acceleratorbased training agents. particular geeps incorporates additional components general including cpu-gpu memory management components weight updates. addition reducing local memory copies infrastructures enable reducing amount information communicated network. distbelief agents send opposed hogwild transmits directions. allows aggregate information multiple agents rather overwriting enables better compression project adam extends behavior utilizing fact compute-capable node offload computation favor communicating less. particular project adam implements different weight update protocols. convolutional layers weights sparse gradients communicated directly. however fully connected layers output previous layer cin×n error cout×n transmitted instead computed therefore nodes communication modified cout significantly smaller balances load agents normally under-utilized dogwild uses low-level unreliable communication gradients network settings former work models distributed clusters heterogeneous computing resources proposes distributed algorithms based stale-synchronous parallelism. specifically decoupling global local learning rates unstable convergence caused stragglers mitigated. latter work gaia acknowledges fact training geo-distributed i.e. originating different locations proposes hierarchical infrastructure synchronizes significant updates data centers. support this approximate synchronous parallel model defined proven retain convergence empirically shown converge faster googlenet treating system homogeneous. decentralized setting load balancing achieved using asynchronous training. however performing parameter exchange cannot allreduce operation incurs global synchronization. approach inconsistent decentralized parameter update gossip algorithms node communicates fixed number random nodes high probability communicating loдm time-steps number nodes data disseminated rest nodes. strong consistency required distributed deep learning method shown marginal success attaining convergence faster performance allreduce nodes. larger parameter gradient compression distributed algorithm requires all-to-all communication operate correctly. discussed above reducing number messages possible. here discuss reducing size message. general ways conserve communication bandwidth distributed deep learning compressing parameters efficient data representations avoiding sending unnecessary information altogether resulting communication sparse data structures. methods former category orthogonal network infrastructure methods applied latter category differ implemented using centralized decentralized topologies. quantization. prominent data representation gradient compression quantization i.e. mapping continuous information buckets represent sets values shown distributions parameter gradient values narrowly dispersed thus methods effective representing working range reduce number bits parameter. method successfully utilized deep learning training inference values quantized post-training papers quantize gradients binary ternary values still attaining convergence marginally reduced accuracy. quantization commonly performed reducing floating-point dynamic range particular methods represent ieee -bit floatingpoint values fewer bits. already applied inference hardware first successful instance reduced precision training performed ieee -bit float values evaluated paper quantized training work out-of-the-box lossy compression reduced precision. rather depends rounding parameters preserves expected value parameters. forms quantization extend upon ideas. qsgd generalizes stochastic rounding stochastic quantization proposes multi-level gradient quantization schemes. deep compression also employs lossless huffman coding increase storage efficiency without impairing convergence. binarized neural networks ternary weight networks terngrad dorefa-net quantize networks binary parameters ternary parameters ternary gradients binary parameters+ternary gradients respectively. bnns terngrad stochastic rounding lower input representation. lastly flexpoint computes mantissa portion floating-point values fixed-point math sharing exponent part among multiple parameters/gradients. accommodate changes exponents predictive analysis used estimating subsequent values. essential convergence lossy quantization local gradient accumulation. particularly distributed environments updates inconsistent important carry quantization error next gradient accumulating error avoid drift. idea originates sigma-delta modulation proven successful many cases. deep gradient compression extends idea correcting momentum well decreasing loss accuracy become non-negligible even resulting minor accuracy increase. sparsification. training dnns exhibit sparse gradients parameter updates. primarily large number parameters necessarily change once; layers convolutional layer optimization process improve accuracy certain convolution kernels. therefore full gradient necessary retain convergence various methods leverage feature proposed. first application gradient sparsification prunes gradient values using static threshold element sent. results show speedup nodes even reduction error. authors achieved compression ratio non-convolutional dnn. subsequent works propose relative adaptive thresholds transmit important gradients based absolute value. counter accuracy loss result sparsification works suggest condition gradient values changing architecture adding various normalization layers whereas others propose local gradient clipping warm-up training. centralized setting distributing sparse gradients straightforward sparse messages sent training agents however implementing necessary allreduce decentralized setting simple agent contribute different nonzero indices gradient. kylix implements sparse allreduce steps first exchanging indices data. desirable systems sparsity pattern node change deep learning gradient indices differ iteration. sparcml targets specifics deep learning explicitly supporting arbitrarily changing indices framework sparse allreduce operations. sparcml combines sending top-k significant indices quantization supports sparse vectors heterogeneous sizes. system switches sparse dense representation techniques. section discuss project adam sending activations errors instead parameters decreasing overall footprint fully connected layers favor redundant computation poseidon framework extends idea transmitting decomposed outer products generalizing concept fields machine learning sufficient factor broadcasting activations sent rather broadcast training agents local recomposition. work best centralized topologies recomposing gradients decentralized environment causes agent process additional outer products step number agents. however authors claim cost recomposition negligible compared communication. since decomposed weights additive opposed gradients incurs all-to-all communication training agents. overcome scalability issues paper suggests partial broadcasting nodes communicate predetermined subset nodes. trading gradient update latency bandwidth paper shows convergence still attained equating delayed updates stale gradients different approach reduce memory footprints design specifically purpose works make memory-efficient layers techniques mostly applied convolutions train networks devices fpgas mobile phones. applied techniques include layers constructed multitude convolutions reshaping applying tucker decomposition convolution tensors separable convolutions papers show dnns decrease size evaluation time exhibiting minor reduction accuracy. model consolidation section discuss parameter consistency spectrum distributed deep learning. cases parameter updates highly infrequent thus precautions must taken received values different methods consolidate results. particular rather running data-parallel multiple nodes distributed deep learning achieved assigning training agents different copies combining resulting models either post-training several times training. latter seen generalization inconsistent view former entirely change training inference processes depending method. multiple instances trained separately dataset overall prediction average predictions ensemble members i.e. ensemble learning used extensively machine learning deep learning form boosting typically increases overall accuracy single model. thus routinely applied machine learning competitions ilsvrc industrial applications. distributed training ensembles completely parallel process requiring communication agents. however works treenets combine ensemble learning custom loss functions promote diversity ensemble members. given ensembles consume factor memory compute power another posttraining model consolidation technique reduce size using knowledge distillation scheme training performed steps first step large network ensemble trained normally; second step trains single neural network mimic output large ensemble. results show second network easier train ensemble labeled dataset attaining word error rate ensemble dnns. model averaging. another technique consolidating models model averaging methods separately instances different machines aggregating parameters every iterations methods proven converge applying stale-synchronous leads higher overall accuracy. overcome accuracy degradation result infrequent averaging sophisticated consolidation methods include elastic averaging natural gradient descent easgd based centralized environment extending direct averaging using elastic forces training agents’ view ps’s view allows agents explore increasing possible distance agent average also allows communicate sparsely respect time easgd reported outperform distbelief downpour method terms accuracy shown tolerant terms update delay used successfully practice communication reduction works natural gradient descent also used deal diverging parameters different agents ng-sgd modifies define learning rate matrices approximating inverse fisher matrix thus natural gradients. averaging agent parameters every samples algorithm allows agents gradually diverge synchronize less traditional sgd. ng-sgd successfully used kaldi speech recognition toolkit method converges faster higher accuracy sgd. distributed settings algorithms also inspected w.r.t. fault tolerance. krum byzantine fault-tolerant algorithm allowing byzantine training agents. particular paper shows gradient aggregation rule based linear combination cannot sustain single byzantine agent. combining specific gradients krum able overcome adversarial gradient inputs network. optimization algorithms architecture search training deep learning nonlinear optimization algorithms exhibit concurrency used sgd’s stead furthermore possible parameter search. supervised learning either viewed stochastic optimization process uses minibatch samples time expressed batch optimization problem entire dataset necessary obtain gradients descent. batch optimization used deep learning since inception dnns using firstsecond-order methods levenberg-marquardt conjugate gradient l-bfgs. although considerably computationally expensive several advantages approaches including increased concurrency better theoretical convergence guarantees worth noting large-minibatch training gaining traction recent years represents middle ground batch methods. methods beneficial inherent concurrency stochasticity which despite sublinear rate convergence works well practice. distributed deep learning batch methods hessian-free second-order optimization initially favored apparent scalability compared traditional however following successful distbelief implementation inconsistent distributed l-bfgs found quadratic increase batch methods memory communication computations high dimensionality desirable stochastic methods achieve results scale better. overcome issue stochastic variants l-bfgs proposed proven converge linearly optimization algorithms applied deep learning attempt reduce variance incurred random sampling alternating direction method multipliers skip backpropagation altogether neumann series expansion approximate hessian matrix scaling large minibatch sizes accuracy loss substantial computational overhead. gradient-free evolutionary algorithms also employed deep learning distributed deep learning examples include genetic algorithms neuro-evolution particleswarm optimization apart recombination/evolution steps training behavior similar ensemble learning thus algorithms amenable parallelism traditional gradient descent. show rest section gradient-independent nature algorithms enable meta-optimization hyper-parameters architectures. hyper-parameter search. multitude hyper-parameters adverse effect resulting accuracy hinders research efforts techniques machine learning. recently prominent method hyperparameter search perform parameter sweeps since method increases overall time exponentially number hyper-parameters effectiveness limited availability computing power. several methods expand beyond simple parameter sweeps making educated guesses tuning hyper-parameters training. former class methods include bayesian optimization predictive analysis learning curves employs predictive analysis grid searches every predetermined number minutes modify momentum hyper-parameter controlling local gradient staleness. paper shows distributed environments controlling synchronous node-group size training increase accuracy performance. yellowfin uses local gradient curvature variance tune momentum working especially well lstm-based models asynchronous environments performing faster adam optimizer metaheuristic optimization algorithms inherently integrate hyper-parameter tuning training thus used dnns. methods include particle swarm optimization based deep learning codeepneat modification neat algorithm simultaneously searches hyper-parameter architecture configurations methods scale almost linearly abundance independent computations. lastly population-based training uses reinforcement learning approach explore exploit hyper-parameter space. particular training agent independently samples information agents every iterations. information used select best configuration hyper-parameters turn perturbed continue learning. creates decentralized topology communication nondeterministic scale better number training agents increases. architecture search. like feature engineering deep learning manually crafting architectures naturally limited human resourcefulness creativity. limitation promoted recent rise research automated neural architecture search. architecture search categorized three distinct approaches discrete search-space traversal reinforcement learning evolutionary algorithms generally sst-based search methods rely defining finite states explore traversing sets. result concurrency search depends number points search space given time. examples methods include deeparchitect proposes definition language allows programmers explicitly define space using options; pnasnet searches networks ordered increasing complexity using search algorithm based conserving half evaluated models compared equivalent approach smash assesses optimality candidate networks using another maps given architecture weights testing. many recent architectures exhibit self-similarity repeating sub-units observation leveraged dramatically reduce number explored architectures composing networks hierarchically modules basic blocks seen fig. approach used successfully community creating candidates modules units rl-based architecture search uses accuracy resulting network reward function whereas modifications hyper-parameters actions. neural architecture search parameters layer modified number layers fixed. sharded ps-based distributed system conjunction policy gradient optimization used training. examples include metaqnn blockqnn operate similarly q-learning optimization; enas significantly reduces computational time sharing parameters across children dnns evolutionary algorithms advantageous architecture search function optimized using methods. hyperneat first successfully applied deep learning used training weights architecture time; codeepneat defines variant neat algorithm optimize hyper-parameters architecture using self-similarity feature dnns optimizing blueprints composed modules. genetic cnns uses genetic algorithms encoding connections binary genes training population dnns every time-step using final accuracy fitness function. highly amenable parallelism successfully used large-scale training nodes used titan large-scale evolution also uses defines specific mutations applied. large-scale evolution outperforms existing rl-based methods terms accuracy well terms scalability entire population parallel however general case requires synchronous reductive communication timesteps selection fittest candidates. overcome issue paper employs tournament selection performs pairwise comparisons population members. additional architecture search methods include multi-level hierarchical representations dnns implement asynchronous distributed tournament selection specialized mutation. regularized evolution extends tournament selection removing oldest sample population iteration thus regularizing optimization process. amoebanets outperform existing methods including manually engineered dnns rl-based searches current state-of-the-art computer vision error imagenet error cifar-. concluding remarks world deep learning brimming concurrency. nearly every aspect training computation convolution meta-optimization architectures inherently parallel. even aspect sequential consistency requirements reduced robustness nonlinear optimization increase concurrency still attaining reasonable accuracy better. paper give overview many aspects respective approaches documented literature provide concurrency analysis using model possible. hard predict future holds highly active field research engineering advancing fast pace producing systems utilize increasing computational resources architectures continually improve state-of-the-art various classification regression problems. below highlight potential directions future research parallel distributed deep learning. research progresses architectures becoming deeper interconnected consecutive non-consecutive layers apart accuracy considerable effort devoted reducing memory footprint number operations order successfully inference mobile devices. also means post-training compression likely researched further training compressible networks desirable. since mobile hardware limited memory capacity energy efficient specialized computational hardware frequently proposed trend nvidia tensor cores tensor processing unit asics fpgas even neuromorphic computing handling sparsity focus asics advances recurrent networks attention learning indicate training inference hardware would also need work efficiently variable-length inputs. computing individual layers highly optimized today thus current research oriented towards inter-layer whole-dnn optimization. tensorflow tensor comprehensions compile entire neural network graphs once performing variety transformations optimize execution time achieving speedup manually tuned individual layers. expect research continue direction point evaluation close optimal terms operations shared-memory optimizations. techniques applied distributed deep learning converging point standard programming interface designed. future ecosystems ease.ml make definition training scheme easier hiding low-level infrastructure setup. combining increasing support cloud systems elastic training latest developments evolutionary algorithms adaptive financially-viable optimization methods rising prominence. finally deep learning used solve increasingly complex problems routing algorithms hierarchical task combination research towards artificial general intelligence focusing multi-purpose networks creates unexplored opportunities model parallelism different training algorithms. searching adequate multi-purpose networks beyond ingenuity human team meta-optimization progressive training increase usability quality; parameter sweeps manual architecture engineering become obsolete. supporting claim fact current state-of-the-art computer vision result automated architecture search. references martín abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg mané rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda viégas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems. http//tensorflow.org/ software available tensorflow.org. alekh agarwal john duchi. distributed delayed stochastic optimization. advances neural information processing systems shawe-taylor zemel bartlett pereira weinberger curran associates inc. http//papers.nips.cc/paper/-distributed-delayed-stochastic-optimization.pdf akopyan sawada cassidy alvarez-icaza arthur merolla imam nakamura datta taba beakes brezzo kuang manohar risk jackson modha. truenorth design tool flow million neuron programmable neurosynaptic chip. ieee transactions computer-aided design integrated circuits systems https//doi.org/./tcad.. alistarh demjan grubic jerry ryota tomioka milan vojnovic. qsgd communication-efficient gradient quantization encoding. advances neural information processing systems guyon luxburg bengio wallach fergus vishwanathan garnett curran associates inc. http//papers.nips.cc/paper/-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf dario amodei sundaram ananthanarayanan rishita anubhai jingliang eric battenberg carl case jared casper bryan catanzaro qiang cheng guoliang chen chen jingdong chen zhijie chen mike chrzanowski adam coates greg diamos ding niandong erich elsen jesse engel weiwei fang linxi christopher fougner liang caixia gong awni hannun tony lappi johannes bing jiang billy patrick legresley libby junjie yang weigao xiangang dongpeng sharan narang andrew sherjil ozair yiping peng ryan prenger sheng qian zongfeng quan jonathan raiman vinay sanjeev satheesh david seetapun shubho sengupta kavya srinet anuroop sriram haiyuan tang liliang tang chong wang jidong wang kaifu wang wang zhijian wang zhiqian wang shuang likai xiao dani yogatama yuan zhan zhenyao zhu. deep speech end-to-end speech recognition english mandarin. proceedings international conference machine learning maria florina balcan kilian weinberger vol. pmlr york york http//proceedings.mlr.press/v/amodei.html nimar arora robert blumofe greg plaxton. thread scheduling multiprogrammed multiprocessors. proceedings tenth annual symposium parallel algorithms architectures york https//doi.org/./. jimmy rich caruana. deep nets really need deep? advances neural information processing systems ghahramani welling cortes lawrence weinberger curran associates inc. http//papers.nips.cc/paper/-do-deep-nets-really-need-to-be-deep.pdf bowen baker otkrist gupta nikhil naik ramesh raskar. designing neural network architectures using reinforcement learning. international conference learning representations https//arxiv.org/abs/. bowen baker otkrist gupta ramesh raskar nikhil naik. practical neural network performance prediction belli hoefler. notified access extending remote memory access programming models producerconsumer synchronization. proceedings ieee international parallel distributed processing symposium ieee. ben-nun levy amnon barak rubin. memory access patterns missing piece multi-gpu puzzle. proceedings international conference high performance computing networking storage analysis article pages. yoshua bengio. deep learning representations looking forward. statistical language speech processing first international conference slsp tarragona spain july proceedings adrian-horia dediu carlos martín-vide ruslan mitkov bianca truthe springer berlin heidelberg berlin heidelberg https //doi.org/./----_ yoshua bengio pascal lamblin popovici hugo larochelle. greedy layer-wise training deep networks. advances neural information processing systems schölkopf platt hoffman press http//papers.nips.cc/paper/-greedy-layer-wise-training-of-deep-networks.pdf peva blanchard mahdi mhamdi rachid guerraoui julien stainer. machine learning adversaries byzantine tolerant gradient descent. advances neural information processing systems guyon luxburg bengio wallach fergus vishwanathan garnett curran associates inc. http //papers.nips.cc/paper/-machine-learning-with-adversaries-byzantine-tolerant-gradient-descent.pdf mariusz bojarski davide testa daniel dworakowski bernhard firner beat flepp prasoon goyal lawrence jackel mathew monfort muller jiakai zhang zhang jake zhao karol zieba. learning self-driving cars. corr abs/. arxiv. http//arxiv.org/abs/. boyd ghosh prabhakar shah. gossip algorithms design analysis applications. proceedings ieee annual joint conference ieee computer communications societies. vol. vol. https //doi.org/./infcom.. stephen boyd neal parikh eric borja peleato jonathan eckstein. distributed optimization statistical learning alternating direction method multipliers. found. trends mach. learn. https//doi.org/./ ernie chan marcel heimlich purkayastha robert geijn. collective communication theory practice experience research articles. concurr. comput. pract. exper. https //doi.org/./cpe.v chan jaitly vinyals. listen attend spell neural network large vocabulary conversational speech recognition. ieee international conference acoustics speech signal processing https//doi.org/./icassp.. kumar chellapilla sidd puri patrice simard. high performance convolutional neural networks document processing. tenth international workshop frontiers handwriting recognition lorette université rennes suvisoft baule https//hal.inria.fr/inria- http//www.suvisoft.com. c.-y. chen choi brand agrawal zhang gopalakrishnan. adacomp adaptive residual gradient compression data-parallel distributed training. corr abs/. arxiv. http//arxiv.org/abs/ chen huo. scalable training deep learning machines incremental block training intra-block parallel optimization blockwise model-update filtering. ieee international conference acoustics speech signal processing https//doi.org/./icassp.. tianshi chen zidong ninghui wang chengyong yunji chen olivier temam. diannao small-footprint high-throughput accelerator ubiquitous machine-learning. proceedings international conference architectural support programming languages operating systems york https//doi.org/./. yunpeng chen jianan huaxin xiao xiaojie shuicheng jiashi feng. dual path networks. advances neural information processing systems guyon luxburg bengio wallach fergus vishwanathan garnett curran associates inc. http//papers.nips.cc/paper/-dual-path-networks.pdf sharan chetlur cliff woolley philippe vandermersch jonathan cohen john tran bryan catanzaro evan shelhamer. cudnn efficient primitives deep learning. corr abs/. arxiv. http//arxiv.org/abs/ trishul chilimbi yutaka suzue johnson apacible karthik kalyanaraman. project adam building efficient scalable deep learning training system. usenix symposium operating systems design implementation usenix association broomfield https//www.usenix.org/conference/osdi/technical-sessions/ presentation/chilimbi kyunghyun bart merriënboer çağlar gülçehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder–decoder statistical machine translation. proceedings conference empirical methods natural language processing association computational linguistics doha qatar http//www.aclweb.org/anthology/d- cheng-tao sang yuanyuan gary bradski kunle olukotun andrew map-reduce machine learning multicore. advances neural information processing systems schölkopf platt hoffman press http//papers.nips.cc/paper/-map-reduce-for-machine-learning-on-multicore. chung sainath ramabhadran picheny gunnels austel chauhari kingsbury. parallel deep neural network training data blue gene/q. ieee transactions parallel distributed systems https//doi.org/./tpds.. cireşan alessandro giusti luca gambardella jürgen schmidhuber. mitosis detection breast cancer histology images deep neural networks. medical image computing computer-assisted intervention miccai kensaku mori ichiro sakuma yoshinobu sato christian barillot nassir navab springer berlin heidelberg berlin heidelberg adam coates brody huval wang david andrew bryan catanzaro. deep learning cots systems. proceedings international conference international conference machine learning volume jmlr.org iii––iii–. nadav cohen sharir amnon shashua. expressive power deep learning tensor analysis. annual conference learning theory vitaly feldman alexander rakhlin ohad shamir vol. pmlr columbia university york york http jason cong bingjun xiao. minimizing computation convolutional neural networks. artificial neural networks machine learning icann international conference artificial neural networks hamburg germany september proceedings stefan wermter cornelius weber włodzisław duch timo honkela petia koprinkova-hristova sven magg günther palm alessandro villa springer international publishing cham https//doi.org/./----_ matthieu courbariaux yoshua bengio jean-pierre david. binaryconnect training deep neural networks binary weights propagations. proceedings international conference neural information processing systems volume press cambridge http//dl.acm.org/citation.cfm?id=. henggang zhang gregory ganger phillip gibbons eric xing. geeps scalable deep learning distributed gpus gpu-specialized parameter server. proceedings eleventh european conference computer systems york article pages. https//doi.org/./. david culler richard karp david patterson abhijit sahay klaus erik schauser eunice santos ramesh subramonian thorsten eicken. logp towards realistic model parallel computation. proceedings fourth sigplan symposium principles practice parallel programming york https//doi.org/./. jeffrey dean greg corrado rajat monga chen matthieu devin quoc mark marc’aurelio ranzato andrew senior paul tucker yang andrew large scale distributed deep networks. proceedings international conference neural information processing systems volume curran associates inc. http//dl.acm.org/citation.cfm?id=. olivier delalleau yoshua bengio. shallow deep sum-product networks. advances neural information processing systems shawe-taylor zemel bartlett pereira weinberger curran associates inc. http//papers.nips.cc/paper/-shallow-vs-deep-sum-product-networks.pdf deng platt. scalable stacking learning building deep architectures. ieee international conference acoustics speech signal processing https//doi.org/./icassp.. dettmers. -bit approximations parallelism deep learning. corr abs/. arxiv. greg diamos shubho sengupta bryan catanzaro mike chrzanowski adam coates erich elsen jesse engel awni hannun sanjeev satheesh. persistent rnns stashing recurrent weights on-chip. proceedings international conference machine learning maria florina balcan kilian weinberger vol. pmlr york york http//proceedings.mlr.press/v/diamos.html thomas dietterich. ensemble methods machine learning. proceedings first international workshop multiple classifier systems springer-verlag london http//dl.acm.org/citation.cfm?id=. drezner amnon barak. asynchronous algorithm scattering information active nodes multicomputer system. parallel distrib. comput. https//doi.org/./-- dryden moon jacobs essen. communication quantization data-parallel training deep neural networks. workshop machine learning environments https//doi.org/. /mlhpc.. farber asanovic. parallel neural network training multi-spert. proceedings international conference algorithms architectures parallel processing. https//doi.org/./icapp.. kevin frans jonathan chen pieter abbeel john schulman. meta learning shared hierarchies. corr alex gaunt matthew johnson maik riechert daniel tarlow ryota tomioka dimitrios vytiniotis webster. ampnet asynchronous model-parallel training dynamic neural networks. corr abs/. arxiv. http//arxiv.org/abs/. xavier glorot yoshua bengio. understanding difficulty training deep feedforward neural networks. proceedings thirteenth international conference artificial intelligence statistics whye mike titterington vol. pmlr chia laguna resort sardinia italy goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems ghahramani welling cortes lawrence weinberger curran associates inc. google. tensorflow overview. https//www.tensorflow.org/performance/xla priya goyal piotr dollár ross girshick pieter noordhuis lukasz wesolowski aapo kyrola andrew tulloch yangqing kaiming accurate large minibatch training imagenet hour. corr abs/. http//arxiv.org/abs/. alex graves greg wayne malcolm reynolds harley danihelka agnieszka grabska-barwińska sergio gómez colmenarejo edward grefenstette tiago ramalho john agapiou hybrid computing using neural network dynamic external memory. nature audrunas gruslys remi munos danihelka marc lanctot alex graves. memory-efficient backadvances neural information processing systems sugiyama propagation time. luxburg guyon garnett curran associates inc. http//papers.nips.cc/paper/ -memory-efficient-backpropagation-through-time.pdf suyog gupta ankur agrawal kailash gopalakrishnan pritish narayanan. deep learning limited numerical precision. proceedings international conference machine learning francis bach david blei vol. pmlr lille france gupta zhang wang. model accuracy runtime tradeoff distributed deep learning systematic study. ieee international conference data mining https//doi.org/./icdm.. stefan hadjis zhang ioannis mitliagkas christopher omnivore optimizer multi-device deep song huizi william dally. deep compression compressing deep neural network pruning trained quantization huffman coding. international conference learning representations http //arxiv.org/abs/. kaiming xiangyu zhang shaoqing jian sun. delving deep rectifiers surpassing human-level performance imagenet classification. proceedings ieee international conference computer vision ieee computer society washington https//doi.org/./iccv.. qirong james cipar henggang seunghak phillip gibbons garth gibson gregory ganger eric xing. effective distributed stale synchronous parallel parameter server. proceedings international conference neural information processing systems volume curran associates inc. hoefler schneider. optimization principles collective neighborhood communications. proceedings international conference high performance computing networking storage analysis. ieee computer society press generalization large batch training neural networks. tion processing systems wanathan garnett -train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks.pdf andrew howard menglong chen dmitry kalenichenko weijun wang tobias weyand marco andreetto hartwig adam. mobilenets efficient convolutional neural networks mobile vision applications. corr abs/. arxiv. http//arxiv.org/abs/. kevin hsieh aaron harlap nandita vijaykumar dimitris konomis gregory ganger phillip gibbons onur mutlu. gaia geo-distributed machine learning approaching speeds. proceedings usenix conference networked systems design implementation usenix association berkeley itay hubara matthieu courbariaux daniel soudry el-yaniv yoshua bengio. quantized neural networks training neural networks precision weights activations. corr abs/. arxiv. http//arxiv.org/abs/. forrest iandola matthew moskewicz khalid ashraf song william dally kurt keutzer. squeezenet alexnet-level accuracy fewer parameters model size. corr abs/. arxiv. http//arxiv.org/abs/. forrest iandola matthew moskewicz khalid ashraf kurt keutzer. firecaffe near-linear acceleration deep neural network training compute clusters. ieee conference computer vision pattern recognition abs/. arxiv. http//arxiv.org/abs/. intel. intel math kernel library. reference manual. intel corporation. intel. mkl-dnn. https//.org/mkl-dnn sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. proceedings international conference international conference machine learning volume jmlr.org jaderberg valentin dalibard simon osindero wojciech czarnecki jeff donahue razavi oriol vinyals green iain dunning karen simonyan chrisantha fernando koray kavukcuoglu. population based training yangqing evan shelhamer jeff donahue sergey karayev jonathan long ross girshick sergio guadarrama trevor darrell. caffe convolutional architecture fast feature embedding. proceedings international conference multimedia. jiawei jiang zhang lele heterogeneity-aware distributed parameter servers. proceedings international conference management data york https//doi.org/./. melvin johnson mike schuster quoc maxim krikun yonghui zhifeng chen nikhil thorat fernanda viégas martin wattenberg greg corrado macduff hughes jeffrey dean. google’s multilingual neural machine translation system enabling zero-shot translation. corr abs/. arxiv. http //arxiv.org/abs/. accelerating stochastic gradient descent using predictive variance readvances neural information processing systems burges bottou welling http//papers.nips.cc/paper/ norman jouppi cliff young nishant patil david patterson gaurav agrawal raminder bajwa sarah bates suresh bhatia boden borchers rick boyle pierre-luc cantin clifford chao chris clark jeremy coriell mike daley matt jeffrey dean gelb tara vazir ghaemmaghami rajendra gottipati william gulland robert hagmann richard doug hogberg john robert hundt hurt julian ibarz aaron jaffey alek jaworski alexander kaplan harshit khaitan daniel killebrew andy koch naveen kumar steve lacy james laudon james diemthu chris leary zhuyuan kyle lucke alan lundin gordon mackean adriana maggiore maire mahony kieran miller rahul nagarajan ravi narayanaswami kathy thomas norrie mark omernick narayana penukonda andy phelps jonathan ross matt ross amir salek emad samadiani chris severn gregory sizikov matthew snelham souter steinberg andy swing mercedes gregory thorson tian horia toma erick tuttle vijay vasudevan richard walter walter wang eric wilcox hyun yoon. in-datacenter performance analysis tensor processing unit. proceedings annual international symposium computer architecture york https//doi.org/./. janis keuper franz-josef pfreundt. asynchronous parallel stochastic gradient descent numeric core scalable distributed machine learning algorithms. proceedings workshop machine learning high-performance computing environments york article pages. https//doi.org/./. hanjoo jaehong park jaehee jang sungroh yoon. deepspark spark-based deep learning supporting asynchronous updates caffe compatibility. corr abs/. arxiv. http//arxiv.org/abs/. yong-deok eunhyeok park sungjoo taelim choi yang dongjun shin. compression deep convolutional neural networks fast power mobile applications. international conference learning representations https//arxiv.org/abs/. köster tristan webb wang marcel nassar arjun bansal william constable oguz elibol stewflexadvances guyon luxburg bengio wallach fergus http//papers.nips.cc/paper/ hall luke hornof amir khosrowshahi carey kloss ruby naveen rao. point adaptive numerical format neural vishwanathan garnett -flexpoint-an-adaptive-numerical-format-for-efficient-training-of-deep-neural-networks.pdf alex krizhevsky ilya sutskever geoffrey hinton. imagenet classification deep convolutional neural networks. proceedings international conference neural information processing systems volume curran associates inc. thorsten kurth jian zhang nadathur satish evan racah ioannis mitliagkas mostofa patwary tareq malas narayanan sundaram wahid bhimji mikhail smorkalov jack deslippe mikhail shiryaev srinivas sridharan prabhat pradeep dubey. deep learning supervised semi-supervised classification scientific data. proceedings international conference high performance computing networking storage analysis york article pages. https//doi.org/./. quoc jiquan ngiam adam coates abhik lahiri bobby prochnow andrew optimization methods deep learning. proceedings international conference international conference machine learning omnipress http//dl.acm.org/citation.cfm?id=. quoc marc’aurelio ranzato rajat monga matthieu devin chen greg corrado jeff dean andrew building high-level features using large scale unsupervised learning. proceedings international coference international conference machine learning omnipress lecun boser denker henderson howard hubbard jackel. backpropagation applied handwritten code recognition. neural comput. https//doi.org/./neco.... yann lecun léon bottou yoshua bengio patrick haffner. gradient-based learning applied document yann lecun corinna cortes. mnist database handwritten digits. http//yann.lecun.com/exdb/mnist. stefan senthil purushwalkam michael cogswell david crandall dhruv batra. heads better training diverse ensemble deep networks. corr abs/. http//arxiv.org/abs/. chao yang feng srimat chakradhar huiyang zhou. optimizing memory efficiency deep convolutional neural networks gpus. proceedings international conference high performance computing networking storage analysis ieee press piscataway article pages. http//dl.acm.org/ citation.cfm?id=. david andersen park alexander smola ahmed vanja josifovski james long eugene shekita bor-yiing scaling distributed machine learning parameter server. proceedings usenix conference operating systems design implementation usenix association berkeley xiangru lian yijun huang yuncheng liu. asynchronous parallel stochastic gradient nonconvex optimization. proceedings international conference neural information processing systems volume press cambridge http//dl.acm.org/citation.cfm?id=. xiangru lian zhang huan zhang cho-jui hsieh zhang liu. decentralized algorithms outperform centralized algorithms? case study decentralized parallel stochastic gradient descent. advances neural information processing systems guyon luxburg bengio wallach fergus vishwanathan garnett curran associates inc. wang dally. deep gradient compression reducing communication bandwidth distributed training. proceedings international conference learning representations https//arxiv.org/abs/. hanxiao karen simonyan oriol vinyals chrisantha fernando koray kavukcuoglu. hierarchical representations efficient architecture search. international conference learning representations https//arxiv.org/abs/. pablo ribalta lorenzo jakub nalepa luciano sanchez ramos josé ranilla pastor. hyper-parameter selection deep neural networks using parallel particle swarm optimization. proceedings genetic evolutionary computation conference companion york https//doi.org/./. james martens. deep learning hessian-free optimization. proceedings international conference international conference machine learning omnipress http//dl.acm.org/citation.cfm?id= message passing interface forum. message-passing interface standard version yajie miao zhang florian metze. distributed learning multilingual feature extractors using gpus. interspeech annual conference international speech communication association singapore september risto miikkulainen jason liang elliot meyerson aditya rawal fink olivier francon bala raju hormoz shahrzad arshak navruzyan nigel duffy babak hodjat. evolving deep neural networks. corr abs/. arxiv. http//arxiv.org/abs/. volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. proceedings international conference machine learning maria florina balcan kilian weinberger vol. pmlr york york http//proceedings.mlr.press/v/ mniha.html volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature http//dx.doi.org/./nature philipp moritz robert nishihara michael jordan. linearly-convergent stochastic l-bfgs algorithm. proceedings international conference artificial intelligence statistics arthur gretton christian robert vol. pmlr cadiz spain http//proceedings.mlr. press/v/moritz.html muller gunzinger. neural simulation parallel computers. neural networks ieee world congress computational intelligence. ieee international conference vol. vol.. https//doi.org/. /icnn.. maryam najafabadi flavio villanustre taghi khoshgoftaar naeem seliya randall wald edin muharemagic. deep learning applications challenges data analytics. journal data https//doi.org/./s--- jiquan ngiam zhenghao chen daniel chia pang quoc andrew tiled convolutional neural networks. advances neural information processing systems lafferty williams shawe-taylor zemel culotta curran associates inc. http//papers.nips.cc/paper/ -tiled-convolutional-neural-networks.pdf nocedal wright. numerical optimization. springer york. cyprien noel simon osindero. dogwild-distributed hogwild gpu. nips workshop distributed eriko nurvitadhi ganesh venkatesh jaewoong debbie marr randy huang jason hock yeong liew krishnan srivatsan duncan moss suchit subhaschandra boudoukh. fpgas beat gpus accelerating next-generation deep neural networks?. proceedings acm/sigda international symposium fieldprogrammable gate arrays york https//doi.org/./. christopher olah. understanding lstm networks. http//colah.github.io/posts/--understanding-lstms oyama nomura sato nishimura tamatsu matsuoka. predicting statistics asynchronous parameters large-scale distributed deep learning system supercomputers. ieee international conference data https//doi.org/./bigdata.. paddlepaddle. elastic deep learning. https//github.com/paddlepaddle/cloud/tree/develop/doc/edl thomas paine hailin jianchao yang thomas huang. asynchronous stochastic gradient descent speed neural network training. corr abs/. arxiv. http//arxiv.org/abs/. yang. survey transfer learning. ieee transactions knowledge data engineering razvan pascanu tomas mikolov yoshua bengio. difficulty training recurrent neural networks. proceedings international conference international conference machine learning volume jmlr.org iii––iii–. http//dl.acm.org/citation.cfm?id=. petroski such madhavan conti lehman stanley clune. deep neuroevolution genetic algorithms competitive alternative training deep neural networks reinforcement learning. corr abs/. arxiv. http//arxiv.org/abs/. rajat raina anand madhavan andrew large-scale deep unsupervised learning using graphics processors. proceedings annual international conference machine learning york https//doi.org/./. mohammad rastegari vicente ordonez joseph redmon farhadi. xnor-net imagenet classification using binary convolutional neural networks. corr abs/. arxiv. http//arxiv.org/abs/. real aggarwal huang regularized evolution image classifier architecture search. corr esteban real sherry moore andrew selle saurabh saxena yutaka leon suematsu quoc alexey kurakin. large-scale evolution image classifiers. proceedings international conference machine learning doina precup whye vol. pmlr international convention centre sydney australia http//proceedings.mlr.press/v/reala.html benjamin recht christopher stephen wright feng niu. hogwild lock-free approach parallelizing stochastic gradient descent. advances neural information processing systems shawe-taylor zemel bartlett pereira weinberger curran associates inc. http//papers.nips.cc/paper/ -hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.pdf christopher zhang kunle olukotun christopher taming wild unified analysis wild -style algorithms. proceedings international conference neural information processing systems volume press cambridge salimans diederik kingma. weight normalization simple reparameterization accelerate trainadvances neural information processing systems sugiyama http//papers.nips.cc/paper/ deep neural networks. luxburg guyon garnett curran associates inc. -weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf frank seide jasha droppo gang dong -bit stochastic gradient descent application data-parallel distributed training speech dnns interspeech https//www.microsoft.com/en-us/research/ publication/-bit-stochastic-gradient-descent-and-application-to-data-parallel-distributed-training-of-speech-dnns/ seide droppo parallelizability stochastic gradient descent speech dnns. ieee international conference acoustics speech signal processing https//doi.org/./ icassp.. reza shokri vitaly shmatikov. privacy-preserving deep learning. proceedings sigsac conference computer communications security york https//doi.org/ david silver julian schrittwieser karen simonyan ioannis antonoglou huang arthur guez thomas hubert lucas baker matthew adrian bolton mastering game without human knowledge. nature song chen towards pervasive user satisfactory across microarchitectures. ieee international symposium high performance computer architecture https//doi.org/./ hpca.. christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. computer vision pattern recognition http//arxiv.org/abs/. gavin taylor ryan burmeister zheng bharat singh ankit patel goldstein. training neural networks without gradients scalable admm approach. corr abs/. arxiv. http//arxiv.org/abs/ tikhonov. numerical methods solution ill-posed problems. vol. springer. tsitsiklis bertsekas athans. distributed asynchronous deterministic stochastic gradient optimization nicolas vasilache jeff johnson michaël mathieu soumith chintala serkan piantino yann lecun. fast convolutional nets fbfft performance evaluation. international conference learning representations http//arxiv.org/abs/. vasilache zinenko theodoridis goyal devito moses verdoolaege adams cohen. tensor comprehensions framework-agnostic high-performance machine learning abstractions. corr abs/. arxiv. http//arxiv.org/abs/. andré viebke suejb memeti sabri pllana ajith abraham. chaos parallelization scheme training convolutional neural networks intel xeon phi. journal supercomputing https//doi.org/./ s---x cong feng chunpeng yandan wang yiran chen terngrad ternary gradients reduce communication distributed deep learning. advances neural information processing systems guyon luxburg bengio wallach fergus vishwanathan garnett curran associates inc. http //papers.nips.cc/paper/-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf pengtao zhou qirong abhimanu kumar yaoliang eric xing. lighter-communication distributed machine learning sufficient factor broadcasting. proceedings thirty-second conference uncertainty artificial intelligence auai press arlington virginia united states http//dl.acm.org/ citation.cfm?id=. kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation visual attention. corr abs/. arxiv. http//arxiv.org/abs/. feng olatunji ruwase yuxiong trishul chilimbi. performance modeling scalability optimization distributed deep learning systems. proceedings sigkdd international conference knowledge discovery data mining york https//doi.org/./. yang aydin buluç james demmel. scaling deep learning knights landing clusters. proceedings international conference high performance computing networking storage analysis york article pages. https//doi.org/./. steven young derek rose travis johnston william heller thomas karnowski thomas potok robert patton gabriel perdue jonathan miller. evolving deep networks using hpc. proceedings machine learning environments york article pages. https//doi.org/./. fisher vladlen koltun. multi-scale context aggregation dilated convolutions. international conference zhang zhiting jinliang pengtao gunhee qirong eric xing. poseidon system architecture efficient gpu-based deep learning multiple machines. corr abs/. arxiv. http//arxiv.org/abs/. zhang zeyu zheng shizhen qirong xiaodan liang zhiting jinliang pengtao eric xing. poseidon efficient communication architecture distributed deep learning clusters. usenix annual technical conference usenix association santa clara https //www.usenix.org/conference/atc/technical-sessions/presentation/zhang sixin zhang anna choromanska yann lecun. deep learning elastic averaging sgd. proceedings international conference neural information processing systems volume press cambridge http//dl.acm.org/citation.cfm?id=. zhang zhang chen chen. cambricon-x accelerator sparse neural networks. annual ieee/acm international symposium microarchitecture https//doi.org/./micro.. zhang zhang zheng asynchronous stochastic gradient descent training. ieee international conference acoustics speech signal processing. https//doi.org/./icassp.. zhang suyog gupta xiangru lian liu. staleness-aware async-sgd distributed deep learning. proceedings twenty-fifth international joint conference artificial intelligence aaai press http//dl.acm.org/citation.cfm?id=. back-propagation algorithm connection machine cm-. tion processing systems touretzky morgan-kaufmann -an-efficient-implementation-of-the-back-propagation-algorithm-on-the-connection-machine-cm-.pdf shuchang zhou zekun xinyu zhou yuxin yuheng zou. dorefa-net training bitwidth convolutional neural networks bitwidth gradients. corr abs/. arxiv. http //arxiv.org/abs/. martin zinkevich markus weimer alex smola lihong parallelized stochastic gradient descent. proceedings international conference neural information processing systems volume curran associates inc. zlateski seung. znni maximizing inference throughput convolutional networks cpus gpus. international conference high performance computing networking storage analysis. https//doi.org/./sc.. layer computation formulas preamble loss function computing tensor input tensor using layer function parameters overall following three functions computed transformed inputs kernels reshaped tensors obtained batching refer slice tensor. practical implementations reshaped matrices transform point-wise multiplication formula batched complex matrix-matrix multiplication. assuming i.e. output tile size convolution input tile size. neighboring tiles overlap total number tiles ⌈h/m⌉ /m⌉. rα×m rα×α rα×r winograd minimal filtering matrices. algorithm", "year": 2018}