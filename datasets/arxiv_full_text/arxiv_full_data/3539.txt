{"title": "BPGrad: Towards Global Optimality in Deep Learning via Branch and  Pruning", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Understanding the global optimality in deep learning (DL) has been attracting more and more attention recently. Conventional DL solvers, however, have not been developed intentionally to seek for such global optimality. In this paper we propose a novel approximation algorithm, BPGrad, towards optimizing deep models globally via branch and pruning. Our BPGrad algorithm is based on the assumption of Lipschitz continuity in DL, and as a result it can adaptively determine the step size for current gradient given the history of previous updates, wherein theoretically no smaller steps can achieve the global optimality. We prove that, by repeating such branch-and-pruning procedure, we can locate the global optimality within finite iterations. Empirically an efficient solver based on BPGrad for DL is proposed as well, and it outperforms conventional DL solvers such as Adagrad, Adadelta, RMSProp, and Adam in the tasks of object recognition, detection, and segmentation.", "text": "figure illustration bpgrad works black denotes solution iterations directed dotted lines denote current gradients dotted circles denote regions wherein solutions achieving global optimality bpgrad automatically estimate scales regions based function evaluation solutions lipschitz continuity assumption. global optimality always desirable preferred optimization. locating global optimality deep learning however extremely challenging high non-convexity thus conventional solvers e.g. stochastic gradient descent adagrad adadelta rmsprop adam intentionally developed purpose best knowledge. alternatively different regularization techniques applied smooth objective functions solvers converge geometrically wider ﬂatter regions parameter space good model solutions exist solutions necessarily global optimum. inspired techniques global optimization nonconvex functions propose novel approximation algorithm bpgrad ability locating global optimality branch pruning well-known algorithm developed searching global solutions nonconvex optimization problems. basic idea effectively gradually shrink lower upper bounds global optimum efﬁciently branching pruning parameter space. fig. illustrates optimization procedure bpgrad. order branch prune space assume objective functions lipschitz continuous approximated lipschitz functions. motivated facts lipschitz continuity provides natural estimate lower upper bounds global optimum used also serve regularization needed smoothen objective functions returned solutions generalize well. understanding global optimality deep learning attracting attention recently. conventional solvers however developed intentionally seek global optimality. paper propose novel approximation algorithm bpgrad towards optimizing deep models globally branch pruning. bpgrad algorithm based assumption lipschitz continuity result adaptively determine step size current gradient given history previous updates wherein theoretically smaller steps achieve global optimality. prove that repeating branch-and-pruning procedure locate global optimality within ﬁnite iterations. empirically efﬁcient solver based bpgrad proposed well outperforms conventional solvers adagrad adadelta rmsprop adam tasks object recognition detection segmentation. deep learning demonstrated successfully many different research areas image classiﬁcation speech recognition natural language processing general empirical success stems mainly better network architectures larger mount training data better learning algorithms however theoretical understanding success applications still remains elusive. recently researchers start understand perspective optimization optimality learned models proved certain conditions critical points learned deep models actually achieve global optimality even though optimization deep learning highly nonconvex. theoretical results partially explain deep models work well practice. fig. illustrate functionality lipschitz continuity regularization noisy narrower deeper valley smoothed wider shallower valley preserved. regularization behavior prevent algorithms stuck local minima. also advocated demonstrated crucial order achieve good generalization learned models several recent works sense bpgrad algorithm/solver essentially aims locate global optimality smoothed objective functions bpgrad generate solutions along directions gradients based estimated regions wherein global optimum exist theoretically repeating branch-and-pruning procedure bpgrad locate global optimum. empirically high demand computation well footprint memory running bpgrad inspires develop efﬁcient solver approximate bpgrad towards global optimization. contributions main contributions work propose novel approximation algorithm bpgrad intent locating global optimum best knowledge approach ﬁrst algorithmic attempt towards global optimization empirically propose novel efﬁcient solver based bpgrad reduce requirement computation well footprint memory. provide theoretical empirical justiﬁcation solver towards preserving theoretical properties bpgrad. demonstrate solver outperforms conventional solvers applications object recognition detection segmentation. related work global optimality empirical loss minimization problem learning deep models highly dimensional nonconvex potentially numerous local minima saddle points. blum rivest showed difﬁcult global optima worst case even learning simple -node neural network np-complete. spite difﬁculties optimizing deep models researchers attempted provide empirical well theoretical justiﬁcation success models w.r.t. global optimality learning. zhang empirically demonstrated sufﬁciently over-parametrized networks trained stochastic gradient descent reach global optimality. choromanska studied loss surface multilayer networks using spin-glass model showed many large-size decoupled networks exists band many local optima whose objective values small close global optimum. brutzkus globerson showed gradient descent converges global optimum polynomial time shallow neural network hidden layer convolutional structure relu activation function. kawaguchi proved error landscape local minima optimization linear deep neural networks. extended results proposed sufﬁcient necessary conditions critical point global minimum. haeffele vidal suggested critical balance degrees positive homogeneity network mapping regularization function prevent non-optimal local minima loss surface neural networks. nguyen hein argued almost local minima global optimal fully connected wide neural networks whose number hidden neurons layer larger training points. soudry carmon employed smoothed analysis techniques provide theoretical guarantee highly nonconvex loss functions multilayer networks easily optimized using local gradient descent updates. hand voroninski provided theoretical properties problem enforcing priors provided generative deep neural networks empirical risk minimization establishing favorable global geometry. solvers widely used solver simplicity whose learning rate predeﬁned. general suffers slow convergence thus learning rate needs carefully tuned. improve efﬁciency several solvers adaptive learning rates proposed including adagrad adadelta rmsprop adam solvers integrate advantages stochastic batch methods small mini-batches used estimate diagonal second-order information heuristically. solvers capability escaping saddle points often yield faster convergence empirically. speciﬁcally adagrad well suited dealing sparse data adapts learning rate parameters performing smaller updates frequent parameters larger updates infrequent parameters. however suffers shrinking learning rate motivates adadelta rmsprop adam. adadelta accumulates squared gradients ﬁxed values rather time adagrad rmsprop updates parameters based rescaled gradients adam based estimated mean variance gradients. recently mukkamala proposed variants rmsprop adagrad logarithmic regret bounds. convention ours though properties global optimality attractive know however optimization perspective algorithm shares similarities recent work global optimization general lipschitz functions uniform sampler utilized maximize lower bound maximizer subject lipschitz conditions. convergence properties w.h.p. derived. contrast approach considers estimating lower upper bounds global optimum employs gradients guidance effectively sample parameter space pruning. convergence proved show algorithm terminate within ﬁnite iterations. empirical solver perspective solver shares similarities recent work improving using feedback objective function. speciﬁcally tracks relative changes objective function running average uses adaptively tune learning rate sgd. theoretical analysis however provided justiﬁcation. contrast solver feedback object function determine learning rate adaptively based rescaled distance feedback current lower bound estimation. theoretical well empirical justiﬁcations established. denote parameters neural network pair data sample associated label nonconvex prediction function represented network objective function training network lipschitz constant gradient parameters denotes normalized gradient global minimum -norm operator vectors. denotes expectation data pairs denotes loss function measuring difference ground-truth labels predicted labels given data samples denotes regularizer parameters. particularly assume that algorithm lower upper bound estimation consider situation samples x··· exist evaluation function lipschitz constant whose global minimum reached sample based simple algebra obtain provides tractable upper bound intractable lower bound unfortunately global minimum. intractability comes fact unknown thus makes lower bound unusable empirically. address problem propose novel tractable estimator mini=··· estimator intentionally introduces upper bound shrunk either decreasing upper bound increasing proved thm. parameter space fully covered samples {xi} estimator become lower bound based estimators propose novel approximation algorithm bpgrad towards global optimization branch pruning. show alg. predeﬁned constant controls precision solution. branch inner loop alg. conducts branch operation split parameter space recursively sampling. towards goal need mapping parameter space bounds. considering lower bound propose sampling based previous samples x··· satisﬁes therefore lower bound estimator higher global minimum safely remove points without evaluation. however becomes smaller risk missing global solutions. address issue propose outer loop alg. increase lower bound drawing samples decrease upper bound later. theoretical analysis theorem whenever holds samples generated alg. satisﬁes proof. since global minimum always holds mini=··· suppose holds mini=··· holds well. would exist least point left sampling contradicting condition complete proof. improve sampling efﬁciency decreasing objective propose sampling along directions gradients small distortion. though gradients encode local structures functions high dimensional space good indicators locating local minima speciﬁcally propose minimization problem generating samples s.t. predeﬁne constant controlling trade-off distortion step size condition objective aims generate sample small distortion anchor point whose step size small well locality property gradients along direction gradient. note reasonable objective functions also utilized sampling purpose long condition satisﬁed. efﬁcient sampling objectives investigated future work. pruning fact speciﬁes samples generated outside union balls deﬁned previous samples. precisely describe requirement introduce concept removable solution space work follows deﬁnition deﬁne denoted mini=··· speciﬁes region wherein function evaluations points cannot smaller lower bound estimator conditioning lipschitz continuity assumption. figure illustration difference sampling using using solid blue lines denote function black dotted lines denote sampling paths starting triangle surrounded blue dotted lines denotes sample. suffers stuck locally avoid locality based rps. summary list bpgrad solver alg. modifying alg. sake fast sampling well memory footprint risk stuck local regions. fig. illustrates scenarios example. sampling method falls loop consider history samples current one. contrast sampling method able keep generating samples avoiding previous samples computation storage expected. theoretical analysis theorem computed using satisﬁes holds discussion thm. cor. imply solver prefers sampling parameter space along path towards single direction roughly speaking. however gradients conventional backpropagation little guarantee satisfy lack constraints learning. though bpgrad algorithm nice theoretical properties global optimization directly applying alg. deep learning incur following problems limit empirical usage address problem practice manually maximum iterations inner outer loops alg. address problem make extra assumptions simplify branching/sampling procedure based follows hand momentum well-known technique deep learning dampen oscillations gradients accelerate directions curvature. therefore solver alg. involves momentum compensate drawbacks backpropagation better approximation alg. section discuss feasibility assumptions reducing computation storage well preserving properties towards global optimization deep learning. utilize matconvnet testbed solver alg. train default networks matconvnet mnist cifar respectively using default parameters without explicit mention. also mnist cifar default. justiﬁcation purpose epochs dataset iterations epoch mnist cifar respectively. experimental details please refer sec. essentially assumption made support three simplify objective assumption usually holds deep learning high dimensionality. therefore focus empirical justiﬁcation assumptions feasibility justify this collect ηt’s running alg. datasets plot fig. overall numbers indeed sufﬁciently small local update based gradients decreases increase iterations general. behavior expected objective supposed decrease well w.r.t. iterations. value beginning datasets induced mainly different l’s. feasibility justify this show evidences fig. plot left-hand side righthand side based returned alg. subﬁgures right values always smaller correspondingly. contrast remaining subﬁgures left values always bigger correspondingly. observations appear robust across different datasets irrelevant parameter determines radius balls i.e. step sizes gradients. momentum parameter related directions gradients updating models appear factor make samples solver satisfy also supports claims thm. cor. relation model update gradient order satisfy evidences provided sec. given evidences hypothesize assumption hold empirically using sufﬁciently large values demonstrate generalization bpgrad solver test applications object recognition detection segmentation training deep convolutional neural networks utilize matconvnet testbed employ demo code well default network architectures different tasks. since solver ability determining learning rates adaptively compare another four widely used solvers adaptive learning rates namely adagrad adadelta rmsprop adam. tune parameters solvers achieve best performance can. mnist digital dataset consists training images test images classes labeled images resolution pixels. cifar- dataset consists training images test images object classes image resolution pixels. follow default implementation train individual similar lenet- dataset. details network architectures please refer demo code. speciﬁcally solvers train networks epochs mnist cifar respectively mini-batch size weight decay momentum addition initial weights networks feeding order mini-batches fair comparison. global learning rate mnist adagrad rmsprop adam. cifar works much better counterpart achieving lower training objectives well lower top- error test time. provides evidence support importance satisfying solver search good solutions toward global optimality. overall solver performs best mnist slightly inferior cifar test time although terms training objective achieves competitive performance mnist best cifar. hypothesize behavior comes effect regularization lipschitz continuity. however solver decrease objectives much faster competitors ﬁrst epochs. observation reﬂects superior ability solver determining adaptive learning rates gradients. especially cifar also compare extra solver based implementation. proposed recent related work improves adam feedbacks objective function tested cifar well. solver much reliable performing consistently epochs. dataset contains training images validation images among object classes. following demo code train alexnet scratch using different solvers. perform training epochs mini-batch size weight decay momentum default learning rates competitors. solver show comparison results fig. evident solver works best training test time. namely converges faster achieve lower objective well lower top- error validation dataset. terms numbers lower second best rmsprop epoch listed table solver parameters typically depend numbers mini-batches epochs respectively. empirically seems work well thus default experiments. accordingly default product numbers minibatches epochs. also parameter lipschitz constant quite robust w.r.t. performance indicating heavily tuning parameter unnecessary practice. demonstrate this compare training objectives solver varying fig. highlight differences crop show results ﬁrst four epochs note remaining results similar behavior. mnist varies corresponding curves clustered similarly cifar decide mnist cifar respectively solver. following fast rcnn demo code conduct solver comparison pascal dataset object classes using selective search default object proposal approach. solvers train network epochs using images trainval test using images test set. weight decay momentum respectively default learning rates competitors. compare adadelta cannot obtain reasonable performance heavy parameter tuning. solver show training comparison fig. test results table though training losses inferior adam case solver works well adam test time average achieving best classes. demonstrates suitability solver training deep models object detection. following work semantic segmentation based fully convolutional networks train fcn-s per-pixel multinomial logistic loss validate standard metric mean pixel intersection union pixel accuracy mean accuracy. solvers conduct training epochs momentum weight decay pascal segmentation set. adagrad rmsprop adam default parameters able achieve best performance. adadelta tune parameters global learning rate rmsprop adagrad adam. adadelta require global learning rate. solver table case solver similar learning behavior adagrad achieves best performance test time. smaller ﬂuctuation epochs validation dataset demonstrates superior reliability solver compared competitors. taking observations account believe solver ability learning robust deep models object segmentation. paper propose novel approximation algorithm namely bpgrad towards searching global optimality branch pruning based lipschitz continuity assumption. basic idea keep generating samples parameter space outside removable parameter space lipschitz continuity provides estimate lower upper bounds global optimality also serves regularization further smooth objective functions theoretically prove conditions bpgrad algorithm converge global optimality within ﬁnite iterations. empirically order avoid high demand computation well storage bpgrad propose efﬁcient solver. theoretical empirical justiﬁcation preserving properties bpgrad provided. demonstrate superiority solver several conventional solvers object recognition detection segmentation.", "year": 2017}