{"title": "Fast k-Nearest Neighbour Search via Prioritized DCI", "tag": ["cs.LG", "cs.AI", "cs.DS", "cs.IR", "stat.ML"], "abstract": "Most exact methods for k-nearest neighbour search suffer from the curse of dimensionality; that is, their query times exhibit exponential dependence on either the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing (DCI) offers a promising way of circumventing the curse and successfully reduces the dependence of query time on intrinsic dimensionality from exponential to sublinear. In this paper, we propose a variant of DCI, which we call Prioritized DCI, and show a remarkable improvement in the dependence of query time on intrinsic dimensionality. In particular, a linear increase in intrinsic dimensionality, or equivalently, an exponential increase in the number of points near a query, can be mostly counteracted with just a linear increase in space. We also demonstrate empirically that Prioritized DCI significantly outperforms prior methods. In particular, relative to Locality-Sensitive Hashing (LSH), Prioritized DCI reduces the number of distance evaluations by a factor of 14 to 116 and the memory consumption by a factor of 21.", "text": "exact methods k-nearest neighbour search suffer curse dimensionality; query times exhibit exponential dependence either ambient intrinsic dimensionality. dynamic continuous indexing offers promising circumventing curse successfully reduces dependence query time intrinsic dimensionality exponential sublinear. paper propose variant call prioritized show remarkable improvement dependence query time particular linear intrinsic dimensionality. increase intrinsic dimensionality equivalently exponential increase number points near query mostly counteracted linear increase space. also demonstrate empirically prioritized signiﬁcantly outperforms prior methods. particular relative locality-sensitive hashing prioritized reduces number distance evaluations factor memory consumption factor method k-nearest neighbours fundamental building block many machine learning algorithms also broad applications beyond artiﬁcial intelligence including statistics bioinformatics database systems e.g. consequently since problem nearest neighbour search ﬁrst posed minsky papert decades intrigued artiﬁcial intelligence theoretical computer science communities alike. unfortunately myriad efforts devising efﬁcient algorithms encountered recurring obstacle curse dimensionality describes phenomenon query time complexity depending exponentially dimensionality. result even datasets moderately high dimensionality practitioners often resort na¨ıve exhaustive search. notions dimensionality commonly considered. familiar notion ambient dimensionality refers dimensionality space data points embedded hand intrinsic dimensionality characterizes intrinsic properties data measures rate number points inside ball grows function radius. precisely dataset intrinsic dimension ball radius contains points. intuitively data points uniformly distributed manifold intrinsic dimensionality roughly dimensionality manifold. existing methods suffer form curse dimensionality. early methods like trees r-trees query times grow exponentially ambient dimensionality. later methods overcame exponential dependence ambient dimensionality able escape exponential dependence intrinsic dimensionality. indeed since linear increase intrinsic dimensionality results exponential increase number points near query problem seems fundamentally hard intrinsic dimensionality high. recently malik proposed approach known dynamic continuous indexing successfully reduces dependence intrinsic dimensionality exponential sublinear thereby making high-dimensional nearest neighbour search practical. observation difﬁculties encountered many existing methods including trees locality-sensitive hashing arise reliance space partitioning popular divideand-conquer strategy. works partitioning vector space discrete cells maintaining data structure keeps track points lying cell. query time methods simply look contents cell containing query possibly adjacent cells perform brute-force search points lying cells. works well low-dimensional settings would work high dimensions? several limitations approach high-dimensional space identiﬁed first volume space grows exponentially dimensionality either number volumes cells must grow exponentially. second discretization space essentially limits ﬁeld view algorithm unaware points adjacent cells. especially problematic query lies near cell boundary could points adjacent cells much closer query. third dimensionality increases surface area grows faster volume; result points increasingly likely near cell boundaries. fourth dataset exhibits varying density across space choosing good partitioning non-trivial. furthermore chosen partitioning ﬁxed cannot adapt changes density arising updates dataset. light observations built idea avoiding partitioning vector space. instead constructs number indices imposes ordering data points. index constructed points similar ranks associated ordering nearby along certain random direction. indices combined allow retrieval points close query along multiple random directions. paper propose variant assigns priority index used determine index process upcoming iteration. reason refer algorithm prioritized dci. simple change results signiﬁcant improvement dependence query time intrinsic dimensionality. specifically show remarkable result linear increase intrinsic dimensionality could mean exponential increase number points near query mostly counteracted corresponding linear increase number indices. words prioritized make dataset high intrinsic dimensionality seem aleasy dataset intrinsic dimensionality linear increase space. knowledge exact method cope high intrinsic dimensionality; prioritized represents ﬁrst method also demonstrate empirically prioritized signiﬁcantly outperforms prior methods. particular compared achieves -fold reduction number distance evaluations -fold reduction memory usage. vast literature algorithms nearest neighbour search. divided categories exact algorithms approximate algorithms. early exact algorithms deterministic store points treebased data structures. examples include trees r-trees x-trees divide vector space hierarchy half-spaces hyper-rectangles voronoi polygons keep track points cell. query times logarithmic size dataset exhibit exponential dependence ambient dimensionality. different method partitions space intersecting multiple hyperplanes. effectively trades space time achieves polynomial query time ambient dimensionality cost exponential space complexity ambient dimensionality. figure visualization query time complexities various exact algorithms function intrinsic dimensionality curve represents example class similar query time complexities. algorithms fall particular class shown next corresponding curve. avoid poor performance worst-case conﬁgurations data exact randomized algorithms proposed. spill trees trees virtual spill trees extend ideas behind trees randomizing orientations hyperplanes partition space halfspaces node tree. randomization enables avoid exponential dependence ambient dimensionality query times still scale exponentially intrinsic dimensionality. whereas methods rely space partitioning algorithms proposed utilize local search strategies. methods start random point look neighbourhood current point point closer query original iteration. like space partitioningbased approaches query time scales exponentially intrinsic dimensionality. table query time complexities various algorithms search. ambient dimensionality intrinsic dimensionality dataset size approximation ratio denoted visualization growth various time complexities function intrinsic dimensionality shown figure query times exhibit undesirable dependence space complexities quadratic size dataset making impractical large datasets. different class algorithms performs search coarse-to-ﬁne manner. examples include navigating nets cover trees rank cover trees maintain sets subsampled data points different levels granularity descend hierarchy neighbourhoods decreasing radii around query. unfortunately query times methods scale exponentially intrinsic dimensionality. difﬁculties devising efﬁcient algorithms exact version problem extensive work approximate algorithms. approximate setting returning point whose distance query within factor distance query true nearest neighbour acceptable. many strategies employed approximate algorithms. methods based tree-based space partitioning local search developed; like many exact algorithms query times also scale exponentially ambient dimensionality. locality-sensitive hashing partitions space regular cells whose shapes implicitly deﬁned choice hash function. achieves query time using space ambient dimensionality dataset size large euclidean space though dependence intrinsic dimensionality made explicit. practice performance degrades datasets large variations density uneven distribution points across cells. consequently various data-dependent hashing schemes proposed unlike data-independent hashing schemes however allow dynamic updates dataset. related approach decomposes space mutually orthogonal axis-aligned subspaces independently partitions subspace. query time linear dataset size known guarantee probability correctness exact approximate setting. different approach projects data lower dimensional space approximately preserves approximate nearest neighbour relationships applies approximate algorithms like trees projected data. query time also linear ambient dimensionality sublinear dataset size. unlike uses space linear dataset size cost longer query time lsh. unfortunately query time exponential intrinsic dimensionality. work closely related dynamic continuous indexing exact randomized algorithm euclidean space whose query time linear ambient dimensionality sublinear dataset size sublinear intrinsic dimensionality uses space linear dataset size. rather partitioning vector space uses multiple global one-dimensional indices orders data points along certain random direction combines indices points near query along multiple random directions. proposed algorithm builds ideas introduced achieves signiﬁcant improvement dependence intrinsic dimensionality. constructs data structure consisting multiple composite indices data points turn consists number simple indices. simple index orders data points according projections along particular random direction. given query every composite index algorithm ﬁnds points near query every constituent simple index known candidate points adds known candidate set. true distances query every candidate point evaluated ones among closconcretely simple index associated random direction stores projections every data point along direction. implemented using standard data structures maintain one-dimensional ordered sequences elements like self-balancing binary search trees skip lists query time algorithm projects query along projection directions associated simple index ﬁnds position query would inserted simple index takes logarithmic time. iterates over visits data points simple index order distances query projection takes constant time iteration. iterates keeps track many times data point visited across simple indices composite index. data point visited every constituent simple index added candidate said retrieved composite index. algorithm data structure construction procedure require dataset points number simple indices constitute composite index number composite indices function construct number appealing properties compared methods based space partitioning. points visited rank rather location space performs well datasets large variations data density. naturally skips sparse regions space concentrates dense regions space. since construction data structure depend dataset algorithm supports dynamic updates dataset able automatically adapt changes data density. furthermore data points represented indices continuous values without discretized granularity discretization need chosen construction time. consequently data structure support queries varying desired levels accuracy allows different speed-vs-accuracy trade-off prioritized differs standard order points different simple indices visited. standard algorithm cycles constituent simple indices composite index regular intervals visits exactly point simple index pass. prioritized algorithm assigns priority constituent simple index; iteration visits upcoming point simple index highest priority updates priority iteration. priority simple index negative absolute difference query projection next data point projection index. algorithm k-nearest neighbour querying procedure require query point binary search trees/skip lists associated projection vectors {}j∈l∈ number points retrieve number points visit composite index function query}jl intuitively ensures data points visited order distances query projection. data points retrieved composite index visited constituent simple indices data time spent evaluating true distances candidate points query. therefore need number candidate points must retrieved ensure algorithm succeeds high probability. derive upper bound failure probability given number candidate points. algorithm fails sufﬁciently many distant points retrieved composite index true k-nearest neighbours. decompose event multiple events event particular distant point retrieved true k-nearest neighbours. since points retrieved order maximum projected distance event happens maximum projected distance distant point less true k-nearest neighbour. start ﬁnding upper bound probability event. simplify notation initially consider displacement vectors query data point relationships projected distances triplets points translate relationships projected lengths pairs displacement vectors. start examining event vector random one-dimensional projection satisﬁes geometric constraint. upper bound probability combinations events occur related failure probability algorithm. points retrieved order maximum distances query along multiple projection directions. since distance projection forms lower bound true distance maximum projected distance approaches true distance number projection directions increases. hence limit number simple indices approaches inﬁnity data points retrieved ideal order order true distances query. construction querying procedures prioritized presented formally algorithms ensure algorithm retrieves exact analynearest neighbours high probability next section shows choose −m/d denotes intrinsic dimensionality. though assumes worst-case conﬁguration data points overly conservative practice; parameters chosen cross-validation. summarize time space complexities prioritized table notably ﬁrst term query complexity dominates ambient dimensionality large favourable dependence intrinsic dimensionality query complexity standard dci. particular linear increase intrinsic dimensionality corresponds exponential increase expansion rate mitigated linear increase number simple indices suggests prioritized better handle datasets high intrinsic dimensionality standard conﬁrmed empirical evidence later paper. analyze time space complexities prioritized derive stopping condition algorithm. algorithm uses standard data structures analysis construction time insertion time deletion time space complexity straightforward. hence section focuses mostly analyzing query time. indexed i.e. i⊆|i|=k happen i⊆|i|=k rewritten as|t ˜etl quantity ˜etl disjoint ˜etj ˜etj |tj| ˜etj |tj| follows |tj| ˜etj ˜etl hypothesis ˜etl therefore ˜etl and|t apply results analyze speciﬁc properties algorithm. convenience instead working directly intrinsic dimensionality analyze query time terms related quantity global relative sparsity deﬁned reproduce deﬁnition completeness. deﬁnition given dataset points within ball radius around point dataset global relative sparsity |bp| |bp| |bp| global relative sparsity related expansion rate intrinsic dimensionality following dataset global relative sparsity )-expansion intrinsic dimensionality derive upper bounds probability true k-nearest neighbours missing candidate points retrieved given composite index expressed terms respectively. results inform chosen ensure querying procedure returns correct results high probability. results follow {p}n closest point query proofs found supplementary material. lemma consider points order retrieved composite index consists simple indices. least points true k-nearest neighbours retrieved lemma consider point projections composite index consists simple indices order visited. probability point projections true k-nearest neighbours visited true k-nearest neighbours retrieved n−mk lemma dataset global relative sparconsistsity simple indices probability candidate points retrieved composite index include true k-nearest neighbours constant lemma dataset global relative sparsity consist∈ simple indices −log probability candidate points retrieved composite index include true k-nearest neighbours constant theorem dataset global relative sparsity −log algorithm returns correct k-nearest neighbours probability least found choice sufﬁces ensure correctness high probability derive bound query time guarantees correctness. analyze time complexity construction insertion deletion space complexity. proofs following found supplementary material. theorem given number simple indices algorithm takes time remk trieve k-nearest neighbours query time denotes intrinsic dimensionality. theorem given number simple indices algorithm takes time preprocess data points construction time. theorem algorithm requires time insert data point time delete data point. theorem algorithm requires space addition space used store data. high-dimensional settings. operates under approximate setting performance metric interest close returned points query rather whether true k-nearest neighbours. algorithms evaluated terms time would need achieve varying levels approximation quality. evaluation performed datasets cifar mnist cifar- consists colour images types objects natural scenes mnist consists grayscale images handwritten digits. images cifar- size three colour channels images mnist size single colour channel. reshape image vector whose entries represent pixel intensities different locations colour channels image. vector dimensionality cifar- mnist. note dimensionalities consideration much higher typically used evaluate prior methods. purposes nearest neighbour search mnist challenging dataset cifar-. images mnist concentrated around modes; consequently data points form dense clusters leading higher intrinsic dimensionality. hand images cifar- diverse data points dispersed space. intuitively much harder closest digit query among digits category plausible near neighbours similar natural image among natural images similar appearance. later results show algorithms need fewer distance evaluations achieve level approximation quality cifar mnist. evaluate performance algorithms using crossvalidation randomly choose different splits query data points. split consists points dataset serve queries remainder designated data points. algorithm retrieve nearest neighbours varying levels approximation quality report mean performance standard deviation splits. approximation quality measured using approximation ratio deﬁned ratio radius ball containing true k-nearest neighbours radius ball containing approximate knearest neighbours returned algorithm. closer approximation ratio higher approximation quality. high dimensions time taken compute true distances query candidate points dominates query time number distance evaluafigure comparison number distance evaluations needed different algorithms achieve varying levels approximation quality cifar- mnist. curve represents mean folds shaded area represents standard deviation. lower values better. close-up view ﬁgure used hashes table tables found achieve best approximation quality given memory constraints. product quantization used data-independent codebook entries algorithm supports dynamic updates. standard used hyparameter settings used prioritized used different settings matches hyperparameter settings standard another uses less space plot number distance evaluations algorithm requires achieve desired level approximation ratio figure shown cifar- hyperparameter setting used standard prioritized requires fewer distance evaluations standard fewer distance evaluations product quantization fewer distance evaluations achieve levels approximation quality represents -fold reduction number distance evaluations relative average. space-efﬁcient hyperparameter setting prioritized achieves -fold reduction compared lsh. mnist hyperparameter setting used standard prioritized requires fewer distance evaluations standard fewer distance evaluations product quantization fewer distance evaluations represents -fold reduction relative average. space-efﬁcient hyperparameter setting prioritized achieves -fold reduction compared lsh. standard lsh. shown figure supplementary material compared prioritized uses less space cifar- less space mnist hyperparameter settings used standard dci. represents -fold reduction memory consumption cifar- -fold reduction mnist. space-efﬁcient hyperparameter setting prioritized uses less space cifar- less space mnist relative represents -fold reduction cifar- -fold reduction mnist. paper presented exact randomized algorithm k-nearest neighbour search refer prioritized dci. showed prioritized achieves signiﬁcant improvement terms dependence query time complexity intrinsic dimensionality compared standard dci. speciﬁcally prioritized large extent counteract linear increase intrinsic dimensionality equivalently exponential increase number points near query using linear increase number simple indices. empirical results validated effectiveness prioritized practice demonstrating advantages prioritized prior methods terms speed memory usage. references anagnostopoulos evangelos emiris ioannis psarros ioannis. low-quality dimension reduction high-dimensional approximate nearest neighbor. international symposium computational geometry andoni alexandr indyk piotr. near-optimal hashing algorithms approximate nearest neighbor high dimensions. foundations computer science focs’. annual ieee symposium ieee andoni alexandr razenshteyn ilya. optimal datadependent hashing approximate near neighbors. proceedings forty-seventh annual symposium theory computing arya sunil mount david netanyahu nathan silverman ruth angela optimal algorithm approximate nearest neighbor searching ﬁxed dimensions. journal berchtold stefan ertl bernhard keim daniel kriegel seidl thomas. fast nearest neighbor search high-dimensional space. data engineering proceedings. international conference ieee biau g´erard chazal fr´ed´eric cohen-steiner david devroye rodriguez carlos weighted knearest neighbor density estimate geometric inference. electronic journal statistics datar mayur immorlica nicole indyk piotr mirrokni vahab locality-sensitive hashing scheme based p-stable distributions. proceedings twentieth annual symposium computational geometry houle michael nett michael. rank-based similarity search reducing dimensional dependence. pattern analysis machine intelligence ieee transactions indyk piotr motwani rajeev. approximate nearest neighbors towards removing curse dimensionality. proceedings thirtieth annual symposium theory computing j´egou herv´e douze matthijs schmid cordelia. product quantization nearest neighbor search. pattern analysis machine intelligence ieee transactions krauthgamer robert james navigating nets simple algorithms proximity search. proceedings fifteenth annual acm-siam symposium discrete algorithms society industrial applied mathematics ting moore andrew yang gray alexander investigation practical approximate nearest neighbor algorithms. advances neural information processing systems paulev´e lo¨ıc j´egou herv´e amsaleg laurent. locality sensitive hashing comparison hash function types querying mechanisms. pattern recognition letters extraneous points must silly extraneous points. therefore probability extraneous points upper bounded probability silly extraneous points. since points retrieved composite index order increasing maximum projected distance query pair points retrieved maxj ujl|} maxj ujl|} {ujl}m projection directions associated constituent simple indices composite index. i=k+ {ujl}j∈ obtain upper bound probability i=k+ size points subset maxj ujl|} words probability points k-nearest neighbours whose maximum projected distances greater distance k-nearest neighbours query maxj ujl|} since maxj ujl|} contained event maxj ujl|} also upper bound probability points k-nearest neighbours whose maximum projected distances exceed k-nearest neighbours deﬁnition probability silly extraneous points. since probability less probability extraneous points upper bound also applies probability. lemma consider point projections composite index consists simple indices order visited. probability point projections true k-nearest neighbours visited true k-nearest neighbours retrieved n−mk proof. points true k-nearest neighbours retrieved referred extraneous points divided categories reasonable silly. extraneous point reasonable k-nearest neighbours silly otherwise. proof. projections points true k-nearest neighbours visited k-nearest neighbours retrieved referred extraneous projections divided categories reasonable silly. extraneous projection reasonable k-nearest neighbours silly otherwise. extraneous projections must silly extraneous projections since could reasonable extraneous projections. therefore probability extraneous projections upper bounded probability silly extraneous projections. since point projections visited order increasing projected distance query extraneous silly projection must closer query projection maximum projection k-nearest neighbour. obtain upper bound probability point projections k-nearest neighbours whose distances respective query projections greater true distance between query k-nearest neighbour maximum projected distances true distances also upper bound probability silly extraneous projections. since probability less probability extraneous projections upper bound also applies probability. lemma dataset global relative sparsity consist∈ simple indices probability candidate points retrieved composite index include true k-nearest neighbours constant proof. refer true k-nearest neighbours among ﬁrst points retrieved composite index true positives false negatives. additionally refer points true k-nearest neighbours among ﬁrst points retrieved false positives. true k-nearest neighbours among ﬁrst candidate points must least false negative true positives. consequently must least false positives. upper bound probability existence false positives terms global relative sparsity apply lemma followed lemma conclude probability event true k-nearest neighbours among ﬁrst candidate points contained event false positives former upper bounded quantity. choose make strictly less proof. refer projections true k-nearest neighbours among ﬁrst visited point projections true positives false negatives. additionally refer projections points true k-nearest neighbours among ﬁrst visited point projections false positives. k-nearest neighbour among candidate points retrieved projections must among ﬁrst visited point projections. must least false negative implying true positives. consequently must least false positives. upper bound probability existence false positives terms global relative sparsity apply lemma followed lemma conclude probability event true k-nearest neighbour missing candidate points contained event false positives former upper bounded quantity. choose −log make strictly less proof. given composite index lemma true k-nearest neighbours missed constant −log probability constant choosing probability therefore min{α algorithm fail composite indices must miss k-nearest neighbours. since composite index constructed independently algorithm fails probability must succeed probability least since min{α makes theorem given number simple indices algorithm takes proof. computing projections query point along ujl’s takes time since constant. searching binary search trees/skip lists tjl’s takes time. total number point projections visited −log γ)). determining next point visit requires popping pushing priority queue takes total time spent visiting points time −log γ)). total number candidate points retrieved γ)). true distances computed every candidate point computation γ)). closest points query among candidate points using selection algorithm like quickselect takes time average. since time visiting points computing distances dominates entire algorithm takes −log time. substituting yields desired expression. proof. computing projections points along ujl’s takes time since constant. inserting points self-balancing binary search trees/skip lists takes time. proof. order insert data point need compute projection along ujl’s insert binary search tree skip list. computing projections takes time inserting corresponding selfbalancing binary search trees skip lists takes time. order delete data point simply remove projections binary search trees skip lists takes time.", "year": 2017}