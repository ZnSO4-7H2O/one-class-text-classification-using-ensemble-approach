{"title": "Reinforcement Learning with Deep Energy-Based Policies", "tag": ["cs.LG", "cs.AI"], "abstract": "We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.", "text": "stochastic policies desirable exploration exploration typically attained heuristically example injecting noise initializing stochastic policy high entropy cases might actually prefer learn stochastic behaviors. paper explore potential reasons this exploration presence multimodal objectives compositionality attained pretraining. beneﬁts include robustness face uncertain dynamics imitation learning improved convergence computational properties multi-modality also application real robot tasks demonstrated however order learn policies must deﬁne objective promotes stochasticity. cases stochastic policy actually optimal solution? discussed prior work stochastic policy emerges optimal answer consider connection optimal control probabilistic inference multiple instantiations framework typically include cost reward function additional factor factor graph infer optimal conditional distribution actions conditioned states. solution shown optimize entropy-augmented reinforcement learning objective correspond solution maximum entropy learning problem intuitively framing control inference produces policies capture single deterministic behavior lowest cost entire range low-cost behaviors explicitly maximizing entropy corresponding policy. instead learning best perform task resulting policies learn ways performing task. apparent policies might preferred learn ways given task might performed resulting policy serve good initialization ﬁnetuning speciﬁc behavior better exploration mechanism seeking best mode multimodal reward landscape; robust behavior propose method learning expressive energy-based policies continuous states actions feasible tabular domains before. apply method learning maximum entropy policies resulting algorithm called soft q-learning expresses optimal policy boltzmann distribution. recently proposed amortized stein variational gradient descent learn stochastic sampling network approximates samples distribution. beneﬁts proposed algorithm include improved exploration compositionality allows transferring skills tasks conﬁrm simulated experiments swimming walking robots. also draw connection actorcritic methods viewed performing approximate inference corresponding energy-based model. introduction deep reinforcement learning emerged promising direction autonomous acquisition complex behaviors ability process complex sensory input acquire elaborate behavior skills using general-purpose neural network representations deep reinforcement learning methods used optimize deterministic stochastic policies. however deep methods operate conventional deterministic notion optimality optimal solution least full observability always deterministic policy although berkeley department elecuc berketrical engineering computer sciences international openai department mathematics computer science institute. haoran tang <hrtangmath.berkeley.edu> tuomas haarnoja <haarnojaberkeley.edu>. algorithms probabilistic inference. experimental evaluation explore potential applications approach. first demonstrate improved exploration performance tasks multi-modal reward landscapes conventional deterministic unimodal methods high risk falling suboptimal local optima. second explore method used provide degree compositionality reinforcement learning showing stochastic energy-based policies serve much better initialization learning skills either random policies policies pretrained conventional maximum reward objectives. preliminaries section deﬁne reinforcement learning problem addressing brieﬂy summarize maximum entropy policy search objective. also present useful identities build algorithm presented section address learning maximum entropy policies approximate inference reinforcement learning continuous action spaces. reinforcement learning problem deﬁned policy search inﬁnite-horizon markov decision process consists tuple state space action space assumed continuous state transition probability represents probability density next state given current state action environment emits reward transition abbreviate simplify notation. also denote state state-action marginals trajectory distribution induced policy goal learn policy deﬁne standard reinforcement learning objective terms quantities maximum entropy augments reward entropy term optimal policy aims maximize entropy visited state e∼ρπ +αh)] maxent maxπ optional convenient parameter used determine relative importance entropy reward. optimization problems type explored number prior works maximum entropy inverse approximate inference using message passing ψ-learning g-learning well recent proposals deep generally operate either simple tabular representations difﬁcult apply continuous high-dimensional domains employ simple parametric representation policy distribution conditional gaussian. therefore although policy optimized perform desired skill many different ways resulting distribution typically limited terms representational power even parameters distribution represented expressive function approximator neural network. extend framework maximum entropy policy search arbitrary policy distributions? paper borrow idea energy-based models turn reveals intriguing connection q-learning actor-critic algorithms probabilistic inference. method formulate stochastic policy energy-based model energy function corresponding soft q-function obtained optimizing maximum entropy objective. highdimensional continuous spaces sampling policy general becomes intractable. borrow recent literature ebms devise approximate sampling procedure based training separate sampling network optimized produce unbiased samples policy ebm. sampling network used updating action selection. parlance reinforcement learning sampling network actor actor-critic algorithm. reveals intriguing connection entropy regularized actorcritic algorithms viewed approximate q-learning methods actor serving role approximate sampler intractable posterior. explore connection paper course discuss connections popular deep methods deterministic policy gradient normalized advantage functions principal contribution work tractable efﬁcient algorithm optimizing arbitrary multimodal stochastic policies represented energy-based models well discussion relates method recent ziebart covered detail section note objective differs qualitatively behavior boltzmann exploration greedily maximize entropy current time step explicitly optimize policies reach states high entropy future. distinction crucial since maximum entropy objective shown maximize entropy entire trajectory distribution policy greedy boltzmann exploration approach discuss section maximum entropy formulation number beneﬁts improved exploration multimodal problems better pretraining later adaptation. wish extend either conventional maximum entropy objective inﬁnite horizon problems convenient also introduce discount factor ensure expected rewards ﬁnite. context policy search algorithms discount factor actually somewhat nuanced choice writing precise objective optimized using discount factor non-trivial defer full derivation discounted objective appendix since unwieldy write explicitly discount following derivations ﬁnal algorithm. soft value functions energy-based models optimizing maximum entropy objective provides framework training stochastic policies must still choose representation policies. choices prior work include discrete multinomial distributions gaussian distributions however want general class distributions represent complex multimodal behaviors instead using general energy-based policies form energy function could represented example deep neural network. universal function approximator represent distribution close connection energy-based models soft versions value functions q-functions theorem soft q-function deﬁned theorem connects maximum entropy objective energy-based models qsoft acts vsoft serves log-partition negative energy function. standard q-function value function relate q-function value function future state soft bellman equation theorem soft q-function satisﬁes soft bellman equation soft bellman equation generalization conventional equation recover standard equation causes approach hard maximum actions. next section discuss identities derive q-learning style algorithm learning maximum entropy policies make practical arbitrary q-function representations approximate inference procedure. section present proposed reinforcement learning algorithm based soft q-function described previous section implemented tractable stochastic gradient descent procedure approximate sampling. ﬁrst describe general case soft q-learning present inference procedure makes tractable deep neural network representations high-dimensional continuous state action spaces. process relate qlearning procedure inference energy-based models actor-critic algorithms. soft q-iteration obtain solution iteratively updating estimates soft. leads ﬁxed-point iteration resembles q-iteration theorem soft q-iteration. qsoft vsoft qsoft))da<∞ stochastic optimization problem solved approximately using stochastic gradient descent using sampled states actions. sampling distributions arbitrary typically real samples rollouts current policy convenient choice uniform distribution. however choice scale poorly high dimensions. better choice current policy produces unbiased estimate soft value conﬁrmed substitution. overall procedure yields iterative approach optimizes q-values summarize section desired generate action samples estimating soft value function. since form policy general sampling intractable. therefore approximate sampling procedure discussed following section. section describe approximately sample soft q-function. existing approaches sample energy-based distributions generally fall categories methods markov chain monte carlo based sampling methods learn stochastic sampling network trained output approximate samples target distribution since sampling mcmc tractable inference must performed online sampling network based stein variational gradient descent amortized svgd amortized svgd several intriguing properties first provides stochastic sampling network query extremely fast sample generation. second shown converge accurate estimate posterior distribution ebm. third resulting algorithm show later strongly resembles actor-critic algorithm provides simple computationally efﬁcient implementation sheds light connection algorithm prior actor-critic methods. formally want learn state-conditioned stochastic neural network parametrized maps noise samples drawn normal gaussian arbitrary distribution unbiased action samples target corresponding soft. denote induced distribution actions want parameters induced distribution refer updates soft bellman backup operator acts soft value function denote maximum entropy policy recovered iteratively applying operator until convergence. however several practicalities need considered order make algorithm. first soft bellman backup cannot performed exactly continuous large state action spaces second sampling energy-based model intractable general. address challenges following sections. soft q-learning section discusses bellman backup theorem implemented practical algorithm uses ﬁnite samples environment resulting method similar q-learning. since soft bellman backup contraction optimal value function ﬁxed point bellman backup optimizing q-function soft bellman error minimized states actions. procedure still intractable integral inﬁnite states actions express stochastic optimization leads stochastic gradient descent update procedure. model soft q-function function approximator parameters denote arbitrary distribution action space. second noting identity ex∼q strictly positive density function express soft q-iteration equivalent form minimizing suppose perturb independent samples appropriate directions induced divergence reduced. stein variational gradient descent provides greedy directions functional eat∼πφ kernel function precise optimal direction reproducing kernel hilbert space thus strictly speaking gradient turns explained assumption chain rule backpropagate stein variational gradient policy network according gradient-based optimization method learn optimal sampling network parameters. sampling network viewed actor actor-critic algorithm. discuss connection section ﬁrst summarize complete maximum entropy policy learning algorithm. algorithm summary summarize propose soft q-learning algorithm learning maximum entropy policies continuous domains. algorithm proceeds alternating collecting experience environment updating soft q-function sampling network parameters. experience stored replay memory buffer standard deep q-learning parameters updated using random minibatches memory. soft q-function updates delayed version target values optimization adam optimizer empirical estimates gradients denote exact formulae used compute gradient estimates deferred appendix also discusses implementation details summarize overview soft q-learning algorithm collect experience sample action using sample next state environment save experience replay memory st+)} sample minibatch replay memory t+)}n update soft q-function parameters sample {a}m compute empirical soft values soft compute empirical gradient ˆ∇θjq update according ˆ∇θjq using adam. update policy sample {ξ}m compute actions compute using empirical estimate compute empiricial estimate ˆ∇φjπ. update according ˆ∇φjπ using adam. case linear-quadratic systems mean maximum entropy policy exactly optimal deterministic policy exploited construct practical path planning methods based iterative linearization probabilistic inference techniques discrete state spaces maximum entropy policy obtained exactly. explored context linearly solvable mdps case inverse reinforcement learning maxent continuous systems continuous time path integral control studies maximum entropy policies maximum entropy planning contrast prior methods work focused extending maximum entropy policy search framework high-dimensional continuous spaces highly multimodal objectives expressive general-purpose energy functions represented deep neural networks. number related methods also used maximum entropy policy optimization intermediate step optimizing policies standard expected reward objective among these work rawlik resembles also makes temporal difference style update soft q-function. however unlike prior work focus general-purpose energy functions approximate sampling rather analytically normalizable distributions. recent work also considers entropy regularized objective though entropy policy parameters sampled actions. thus resulting policy represent arbitrarily complex multi-modal distribution single parameter. form sampler resembles stochastic networks proposed recent work hierarchical learning however prior work uses task-speciﬁc reward bonus system encourage stochastic behavior approach derived optimizing general maximum entropy objective. closely related concept maximum entropy policies boltzmann exploration uses exponential standard q-function probability action number prior works also explored representing policies energy-based models q-value obtained energy model restricted boltzmann machine although methods closely related knowledge extended case deep network models made extensive approximate inference techniques demonstrated complex continuous tasks. recently o’donoghue drew connection between boltzmann exploration entropy-regularized policy gradient though theoretical framework differs maximum entropy policy search unlike full maximum entropy framework approach o’donoghue optimizes maximizing entropy current time step rather planning visiting future states entropy maximized. prior method also demonstrate learning complex multimodal policies continuous action spaces. although motivate method q-learning structure resembles actor-critic algorithm. particularly instructive observe connection approach deep deterministic policy gradient method updates qfunction critic according bellman updates backpropagates q-value gradient actor similarly nfqca actor update differs addition term. indeed without term actor would estimate maximum posteriori action rather capturing entire distribution. suggests intriguing connection method ddpg simply modify ddpg critic updates estimate soft q-values recover variant method. furthermore connection allows cast ddpg simply approximate q-learning method actor serves role approximate maximizer. helps explain good performance ddpg off-policy data. also make connection method policy gradients. appendix show policy gradient policy represented energy-based model closely corresponds update soft q-learning. similar derivation presented concurrent work experiments experiments answer following questions soft q-learning method accurately capture multi-modal policy distribution? soft q-learning energy-based policies exploration complex tasks require tracking multiple modes? maximum entropy policy serve good initialization ﬁnetuning different tasks compared pretraining standard deterministic objective? compare algorithm ddpg shown achieve better sample efﬁciency continuous control problems consider recent techniques reinforce trpo comparison particularly interesting since discussed section ddpg closely corresponds deterministic maximum posteriori variant method. detailed experimental setup found appendix videos experiments example source code available online. cessful learn represent multi-modal behavior designed simple multi-goal environment agent point mass trying reach four symmetrically placed goals. reward deﬁned mixture gaussians means placed goal positions. optimal strategy arbitrary goal optimal maximum entropy policy able choose four goals random. ﬁnal policy obtained method illustrated figure q-values indeed complex shapes unimodal convex bimodal stochastic policy samples actions closely following energy landscape hence learning diverse trajectories lead four goals. comparison policy trained ddpg randomly commits single goal. figure illustration multi-goal environment. left trajectories policy learned method axes correspond positions agent initialized origin. goals depicted dots level curves show reward. right q-values three selected states depicted level curves axes correspond velocity bounded actions sampled policy shown blue stars. note that regions goals method chooses multimodal actions. learning multi-modal policies exploration though environments clear multi-modal reward landscape multi-goal example multimodality prevalent variety tasks. example chess player might various strategies settling seems effective agent navigating maze need various paths ﬁnding exit. learning process often best keep trying multiple available options agent conﬁdent best however deep algorithms continuous control typically unimodal action distributions well suited capture multi-modality. consequence algorithms prematurely commit mode converge suboptimal behavior. evaluate maximum entropy policies might exploration constructed simulated continuous control environments tracking multiple modes important success. ﬁrst experiment uses simulated swimming snake receives reward equal speed along x-axis either forward backward. however swimmer swims enough forward crosses ﬁnish line receives larger reward. therefore best learning strategy explore directions bonus reward discovered commit swimming forward. illustrated figure appendix method able recover strategy keeping track modes ﬁnish line discovered. stochastic policies eventually commit swimfigure comparison soft q-learning ddpg swimmer snake task quadrupedal robot maze task. shows maximum traveled forward distance since beginning training several runs algorithm; large reward crossing ﬁnish line. shows method able reach distance goal faster consistently. different lines show minimum distance goal since beginning training. domains runs method cross threshold line acquiring optimal strategy runs ddpg not. ming forward. deterministic ddpg method shown comparison commits mode prematurely policies converging forward motion choosing suboptimal backward mode. second experiment studies complex task continuous range equally good options prior discovery sparse reward goal. task quadrupedal robot needs path maze target position reward function gaussian centered target. agent choose either upper lower passage appear identical ﬁrst upper passage blocked barrier. similar swimmer experiment optimal strategy requires exploring directions choosing better one. figure compares performance ddpg method. curves show minimum distance target achieved threshold equals minimum possible distance robot chooses upper passage. therefore successful exploration means reaching threshold. policies trained method manage succeed policies trained ddpg converge choosing lower passage. figure quadrupedal robot trained walk random directions empty pretraining environment ﬁnetuned variety tasks including wide narrow u-shaped hallway accelerating training complex tasks standard accelerate deep neural network training task-speciﬁc initialization network trained task used initialization another task. ﬁrst task might something highly general classifying large image dataset second task might speciﬁc ﬁnegrained classiﬁcation small dataset. pretraining also explored context however near-optimal policies often near-deterministic makes poor initializers tasks. section explore energybased policies trained fairly broad objectives produce initializer quickly learning speciﬁc tasks. demonstrate variant quadrupedal robot task. pretraining phase involves learning locomote arbitrary direction reward simply equals speed center mass. resulting policy moves agent quickly randomly chosen direction. overhead plot center mass traces shown illustrate this. pretraining similar ways recent work modulated controllers hierarchical models however contrast prior works require taskspeciﬁc high-level goal generator reward. figure also shows variety test environments used ﬁnetune running policy speciﬁc task. hallway environments agent receives reward walls block sideways motion optimal solution requires learning particular direction. narrow hallways require choosing speciﬁc direction also allow agent walls funnel itself. u-shaped maze requires agent learn curved trajectory order arrive target reward given gaussian bump target location. figure performance downstream task ﬁne-tuning training scratch x-axis shows training iterations. y-axis shows average discounted return. solid lines average values random seeds. shaded regions correspond standard deviation. learn behaviors test environments quickly training policy ddpg random initialization shown figure also evaluated alternative pretraining method based deterministic policies learned ddpg. however deterministic pretraining chooses arbitrary consistent direction training environment providing poor initialization ﬁnetuning speciﬁc task shown results plots. discussion future work presented method learning stochastic energybased policies approximate inference stein variational gradient descent approach viewed type soft q-learning method additional contribution using approximate inference obtain complex multimodal policies. sampling network trained part svgd also viewed tking role actor actor-critic algorithm. experimental results show method effectively capture complex multi-modal behavior problems ranging point mass tasks complex torque control simulated walking swimming robots. applications training stochastic policies include improved exploration case multimodal objectives compositionality pretraining general-purpose stochastic policies efﬁciently ﬁnetuned task-speciﬁc behaviors. work explores potential applications energy-based policies approximate inference exciting avenue future work would study capability represent complex behavioral repertoires potential composability. context linearly solvable mdps several prior works shown policies trained different tasks composed create optimal policies prior works explored simple tractable representations method could used extend results complex highly multi-modal deep neural network models making suitable composable control complex high-dimensional systems humanoid robots. composability could used future work create huge variety near-optimal skills collection energy-based policy building blocks. thank qiang insightful discussion svgd thank vitchyr pong shane help implementing ddpg. haoran tang tuomas haarnoja supported berkeley deep drive. elfwing otsuka uchibe doya freeenergy based reinforcement learning vision-based navigation high-dimensional sensory inputs. int. conf. neural information processing springer rawlik toussaint vijayakumar stochastic optimal control reinforcement learning approximate inference. proceedings robotics science systems viii silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot dieleman grewe nham kalchbrenner sutskever lillicrap leach kavukcuoglu graepel hassabis mastering game deep neural networks tree search. nature issn article. appendix present proofs theorems allow show soft q-learning leads policy improvement respect maximum entropy objective. first deﬁne slightly nuanced version maximum entropy objective allows incorporate discount factor. deﬁnition complicated fact that using discount factor policy gradient methods typically discount state distribution rewards. sense discounted policy gradients typically optimize true discounted objective. instead optimize average reward discount serving reduce variance discussed thomas however purposes derivation deﬁne objective optimized discount factor objective corresponds maximizing discounted expected reward entropy future states originating every state-action tuple weighted probability current policy. note objective still takes account entropy policy future states contrast greedy objectives boltzmann exploration approach proposed o’donoghue objective function expected discounted rewards policy improvement theorem describes policies improved monotonically. similar theorem derive maximum entropy objective theorem given policy deﬁne policy soft improves monotonically. certain regularity conditions converges obviously since non-optimal policy improved optimal policy must satisfy energy-based form. therefore proven theorem show entropy-regularized policy gradient viewed performing soft q-learning maximum-entropy objective. first suppose parametrize stochastic policy choose soft qstqat recover policy gradient note choice using empirical estimate soft advantage rather soft q-value makes target independent soft value convergence soft approximates soft q-value additive constant. resulting policy still correct since boltzmann distribution independent constant shift energy function. implementation computing policy update explain full detail policy update direction ˆ∇φjπ algorithm computed. reuse indices section different meaning body paper sake providing cleaner presentation. expectations appear amortized svgd places. first svgd approximates optimal descent direction equation empirical average samples similarly svgd approximates expectation partition function. general class policies trivially transform given distribution exponential form deﬁning energy write entropy-regularized policy gradient follows computing density sampled actions equation states soft value computed sampling distribution optimal. direct solution obtain actions sampling network non-singular probability practice jacobian usually singular beginning training sampler fully trained. simple solution begin uniform action sampling switch later reasonable since untrained sampler unlikely produce better samples estimating partition function anyway. throughout experiments following parameters ddpg soft q-learning. q-values updated using adam learning rate ddpg policy soft q-learning sampling network adam learning rate algorithm uses replay pool size million. training start replay pool least samples. every mini-batch size training iteration consists time steps q-values policy sampling network trained every time step. experiments epochs except multi-goal task uses epochs ﬁne-tuning tasks trained epochs. q-value policy sampling network neural networks comprised hidden layers hidden units layer relu nonlinearity. ddpg soft q-learning additional noise improve exploration. parameters addition found updating target parameters frequently destabilize training. therefore freeze target parameters every time steps copy current network parameters target networks directly soft q-learning uses action samples compute policy update except multigoal experiment uses number additional action samples compute soft value kernel radial basis function written equal median pairwise distance sampled actions note step size changes dynamically depending state suggested ﬁne-tuning tasks anneal entropy coefﬁcient quickly order improve performance since goal ﬁnetuning recover near-deterministic policy ﬁne-tuning task. particular annealed log-linearly within epochs ﬁne-tuning. moreover samples ﬁxed {ξi}kξ reduced linearly within epochs. figure forward swimming distance achieved policy. policy unique random seed. training iteration distance line ﬁnish line. blue shaded region bounded maximum minimum distance plot shows method able explore equally well directions commits better one. figure plot shows trajectories quadrupedal robot maximum entropy pretraining. robot diverse behavior explores multiple directions. four columns correspond entropy coefﬁcients respectively. different rows correspond policies trained different random seeds. axes show coordinates center-of-mass. decreases training process focuses high rewards therefore exploring training ground extensively. however also tends produce less diverse behavior. therefore trajectories concentrated fourth column.", "year": 2017}