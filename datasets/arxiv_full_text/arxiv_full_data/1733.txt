{"title": "Semi-supervised Convolutional Neural Networks for Text Categorization  via Region Embedding", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "This paper presents a new semi-supervised framework with convolutional neural networks (CNNs) for text categorization. Unlike the previous approaches that rely on word embeddings, our method learns embeddings of small text regions from unlabeled data for integration into a supervised CNN. The proposed scheme for embedding learning is based on the idea of two-view semi-supervised learning, which is intended to be useful for the task of interest even though the training is done on unlabeled data. Our models achieve better results than previous approaches on sentiment classification and topic classification tasks.", "text": "paper presents semi-supervised framework convolutional neural networks text categorization. unlike previous approaches rely word embeddings method learns embeddings small text regions unlabeled data integration supervised cnn. proposed scheme embedding learning based idea two-view semi-supervised learning intended useful task interest even though training done unlabeled data. models achieve better results previous approaches sentiment classiﬁcation topic classiﬁcation tasks. convolutional neural networks neural networks make internal structure data structure image data convolution layers computation unit responds small region input data text gaining attention used systems tagging entity search sentence modeling make structure text data. since originally developed image data ﬁxed-sized low-dimensional dense without modiﬁcation cannot applied text documents variable-sized highdimensional sparse represented sequences one-hot vectors. many studies text therefore words sentences ﬁrst converted low-dimensional word vectors. word vectors often obtained method additional large corpus typically done fashion similar language modeling though many variations word vectors obtained form semi-supervised learning leaves following questions. effective text purely supervised setting without unlabeled data? unlabeled data effectively using general word vector learning methods? recent study addressed text categorization showed without word vector layer feasible also beneﬁcial aided unlabeled data. address also text categorization building propose semi-supervised framework learns embeddings small text regions unlabeled data supervised cnn. essence described later convert small regions data feature vectors upper layers; words training convolution layer learns embedding small regions data. term ‘embedding’ loosely mean structure-preserving function particular function generates low-dimensional features preserve predictive structure. applies directly high-dimensional one-hot vectors leads directly learning embedding small text regions eliminating extra layer word vector conversion. direct learning region embedding noted merit higher accuracy simpler system supervised word vector-based word vectors randomly initialized trained part training. moreover performance best rivaled exceeded previous best results benchmark datasets. motivated ﬁnding seek effective unlabeled data text categorization direct learning embeddings text regions. semi-supervised framework learns region embedding unlabeled data uses produce additional input supervised region embedding trained labeled data. speciﬁcally unlabeled data learn tv-embeddings text region task predicting surrounding context. according theoretical ﬁnding tv-embedding desirable properties ideal conditions relations views labels. reality ideal conditions perfectly consider guidance designing tasks tv-embedding learning. consider several types tv-embedding learning task trained unlabeled data; e.g. task predict presence concepts relevant intended task context indirectly labeled data task. thus seek learn tv-embeddings useful speciﬁcally task interest. contrast previous word vector/embedding learning methods typically produce word embedding general purposes aspects words captured. sense goal region embedding learning text regions high-level concepts relevant task. cannot done word embedding learning since individual words isolation primitive correspond high-level concepts. example easy conveys positive sentiment isolation not. show models tv-embeddings outperform previous best results sentiment classiﬁcation topic classiﬁcation. moreover direct comparison conﬁrms region tv-embeddings provide compact effective representations regions task interest obtained manipulation word embedding. feed-forward network equipped convolution layers interleaved pooling layers. convolution layer consists computation units responds small region input small regions collectively cover entire data. computation unit associated region input computes input region vector represents region. weight matrix rm×q bias vector shared units layer learned training. input document represented one-hot vectors therefore call one-hot cnn; either concatenation one-hot vectors bag-ofword vector bag-of-n-gram vector e.g. region love representation loses word order within region robust data sparsity enables large region size speeds training fewer parameters. mainly embedding learning unlabeled data. called seq-cnn bow-cnn. region size stride meta-parameters. note used tiny three-word vocabulary vector examples save space vocabulary typical applications could much larger. componentwise non-linear function vector component). thus computation unit generates m-dimensional vector number weight vectors neurons. words convolution layer embodies embedding text regions produces m-dim vector text region. essence region embedding uses co-presence absence words region input produce predictive features e.g. presence easy absence predictive indicator turned large feature value negative weight positive weights three words formal argument found appendix. m-dim vectors text regions document aggregated pooling layer either component-wise maximum average used layer features classiﬁcation. focused convolution layer; details consulted. shown one-hot effective text categorization essence direct learning embedding text regions aided options input region vector representation. along line propose semi-supervised learning framework learns embedding text regions unlabeled data integrates learned embedding supervised training. ﬁrst step learn embedding following property. deﬁnition function tv-embedding w.r.t. exists function tv-embedding view deﬁnition preserves everything required predict another view trained unlabeled data. motivation tvembedding theoretical ﬁnding that essentially tv-embedded feature vector useful purpose classiﬁcation ideal conditions. conditions essentially state exists hidden concepts views labels classiﬁcation task related concepts concepts might example pricey handy hard sentiment classiﬁcation product reviews. reality ideal conditions completely consider guidance design tv-embedding learning accordingly. tv-embedding learning related two-view feature learning learn linear embedding unlabeled data tasks predicting word features associated surrounding words. studies were however limited linear embedding. related method learns word embedding left context right context maximally correlate terms canonical correlation analysis. share studies general idea using relations views focus nonlinear learning region embeddings useful task interest resulting methods different. important difference tv-embedding learning co-training involve label guessing thus avoiding risk label contamination. used stacked denoising auto-encoder extract features invariant across domains sentiment classiﬁcation unlabeled data. fully-connected neural networks underperformed cnns base model task interest assume convolution layer region size note however restriction convolution layer merely simplifying description. propose semi-supervised framework following steps. tv-embedding learning train neural network predict context region size convolution layer generates feature vectors text region size classiﬁer layer. convolution layer embodies tv-embedding transfer supervised learning model next step. final supervised learning integrate learned tv-embedding tv-embedded regions used additional input convolution layer. train ﬁnal model labeled data. create task unlabeled data predict context region size deﬁned convolution layer. correspondence deﬁnition tv-embeddings helps consider sub-task assigns label text region instead ultimate task categorizing entire document. sensible makes predictions building small regions. document good acting plot figure clues predicting label plot plot context good acting trained predict i.e. approximate deﬁnition functions embodied convolution layer layer respectively. given document text region indexed convolution layer computes except superscript indicate entities belong layer uses features prediction. learned training. input region vector representation goal learn embedding text regions shared text regions every location. context used tv-embedding learning prediction target thus representation context determined optimize ﬁnal outcome without worrying cost prediction time. guidance conditions relationships views mentioned above; ideally views related relevant concepts. consider following types target/context representation. unsupervised target straightforward vector encoding context/target vectors text regions left right distinguish left right target vector |-dimensional vocabulary |-dimensional. potential problem encoding adjacent regions often syntactic relations typically irrelevant task therefore undesirable. simple remedy found effective vocabulary control context remove function words target vocabulary. partially-supervised target another context representation consider partially supervised sense uses labeled data. first train labeled data intended task apply unlabeled data. discard predictions retain internal output convolution layer m-dimensional vector text region number neurons. m-dimensional vectors represent context. shown examples dimension vectors roughly represents concepts relevant task e.g. ‘desire recommend product’ ‘report faulty product’ therefore advantage representation obvious noise since context represented concepts relevant task. disadvantage good supervised produced perfect particular relevant concepts would missed appear labeled data. final supervised learning integration tv-embeddings supervised tv-embedding obtained unlabeled data produce additional input convolution layer replacing with deﬁned i.e. output tv-embedding applied region. train model labeled data task; update weights bias top-layer parameters designated loss function minimized labeled training data. either ﬁxed updated ﬁne-tuning work simplicity. note takes tv-embedded region input also embedding text regions; call supervised embedding trained labeled data distinguish tv-embeddings. tv-embeddings improve supervised embedding. note naturally extended accommodate multiple tv-embeddings that example types tv-embedding obtained unsupervised target partially-supervised target used once lead performance improvement complement other shown later. data used three datasets used imdb elec summarized table imdb comes unlabeled set. facilitate comparison previous studies used union training unlabeled data. elec consists amazon reviews electronics products. unlabeled data chose reviews data source disjoint training test sets reviewed products disjoint test set. -way classiﬁcation second-level topics unlabeled data chosen disjoint training test sets. multi-label categorization topics since ofﬁcial lyrl split task divides entire corpus training test used entire test unlabeled data implementation used one-layer models found effective base models namely seq-cnn imdb/elec bow-cnn rcv. tv-embedding training miniij αij− goes regions represents target regions model output. weights balance loss originating presence absence words speed training eliminating negative examples similar negative sampling experiment unsupervised target vectors adjacent regions left right retaining frequent words vocabulary control; sentiment classiﬁcation function words removed topic classiﬁcation numbers stop-words provided removed. note words removed target vocabulary. produce partially-supervised target ﬁrst trained supervised models neurons applied trained convolution layer unlabeled data generate -dimensional vectors region. rest implementation follows i.e. supervised models minimized square loss regularization optional dropout rectiﬁer; response normalization performed; optimization done sgd. model selection tested methods tuning meta-parameters done testing models held-out portion training data models re-trained chosen meta-parameters using entire training data. overview conﬁrming effectiveness models comparison supervised report performances relies word vectors pre-trained large corpus besides comparing performance approaches whole also interest compare usefulness learned unlabeled data; therefore show performs integrate word vectors base model one-hot cnns experiments also test word vectors trained wordvec unlabeled data compare models standard semi-supervised methods transductive co-training previous best results literature comparisons models outperform others. particular region tvembeddings shown compact effective region embeddings obtained simple manipulation word embeddings supports approach using region embedding instead word embedding. table error rates comparison models constrained neurons. parentheses around error rates indicate co-training meta-parameters tuned test data. tv-embeddings tested three types tv-embedding summarized table ﬁrst thing note cnns outperform supervised counterpart conﬁrms effectiveness framework propose. table meaningful comparison cnns constrained exactly convolution layer cnn) neurons. best-performing supervised cnns within constraints seq-cnn imdb elec bow-cnn rcv. also served base models complex supervised cnns reviewed later. sentiment classiﬁcation region size chosen model selection models larger supervised cnn. indicates unlabeled data enabled effective larger regions predictive might suffer data sparsity supervised settings. ‘unsup-tv.’ uses bag-of-n-gram vector initially represent region thus retains word order partially within region. used individually unsup-tv. outperform tv-embeddings instead found contributed error reduction combined others implies learned unlabeled data predictive information embeddings missed. best performances obtained using three types tv-embeddings according error rates improved nearly compared supervised result three tv-embeddings different strengths complementing other. figure region tv-embeddings wordvec word embeddings. trained unlabeled data. x-axis dimensionality additional input supervised region embedding. region word. shown uses google news word vectors input competitive number sentence classiﬁcation tasks. vectors trained authors wordvec large google news corpus argued vectors useful various tasks serving ‘universal feature extractors’. tested equipped three convolution layers different region sizes max-pooling using vectors input. although used neurons layer changed match models neurons. models clearly outperform models relatively large differences. comparison embeddings besides comparing performance approaches whole also interest compare usefulness learned unlabeled data. purpose experimented integration word embedding base models using methods; takes concatenation takes average word vectors words region. provide additional input supervised embedding regions place comparison produce region embedding word embedding replace region tv-embedding. show results types word embeddings word embedding word embeddings trained wordvec software unlabeled data i.e. data used tv-embedding learning others note figure plots error rates relation dimensionality produced additional input; smaller dimensionality advantage faster training/prediction. results ﬁrst region tv-embedding useful tasks tested word embeddings since models tv-embedding clearly outperform models word embedding. word vector concatenations much higher dimensionality shown ﬁgure still underperformed -dim region tv-embedding. second since region tv-embedding takes form vector columns correspond words therefore columns whose corresponding words region. based that might wonder simply average word vectors obtained existing tool wordvec instead. suboptimal performances average’ tells idea. attribute fact region embeddings learn predictiveness co-presence absence words region; region embedding expressive averaging word vectors. thus effective compact region embedding cannot trivially obtained word embedding. particular effectiveness combination three tv-embeddings stands out. additionally mechanism using information unlabeled data effective since cnns outperform cnns model one-hot vectors compensate potential information loss embedding learned unlabeled data. this well region-vs-word embedding major difference model model. standard semi-supervised methods many standard semi-supervised methods applicable require vectors input. tested tsvm bag-of-{}-gram vectors using svmlight. tsvm underperformed supervised three datasets note feasibility used frequent n-grams tsvm experiments thus showing results also vocabulary comparison though datasets performance improved n-grams computational cost tsvm turned high taking several days even vocabulary. since co-training meta-learner used cnn. random split vocabulary split ﬁrst last half document tested. reduce computational burden report best co-training performances obtained optimizing meta-parameters including stop test data. even unfair advantage cotraining co-training clearly underperformed models. results demonstrate difﬁculty effectively using unlabeled data tasks given size labeled data relatively large. comparison previous best results compare models previous best results imdb best model three tv-embeddings outperforms previous best results nearly models single tv-embed. also perform better previous results. since elec relatively dataset aware previous semi-supervised results. performance better best supervised complex network architecture three convolution-pooling pairs parallel compare benchmark results tested model multi-label task lyrl split categories assigned document. model outperforms best best supervised paper proposed semi-supervised framework text categorization learns embeddings text regions unlabeled data labeled data. discussed section region embedding trained learn predictiveness co-presence absence words region. contrast word embedding trained represent individual words isolation. thus region embedding expressive simple averaging word vectors spite seeming similarity. comparison embeddings conﬁrmed advantage; region tvembeddings trained speciﬁcally task interest effective tested word embeddings. using models able achieve higher performances previous studies sentiment classiﬁcation topic classiﬁcation. suppose observe views input target label interest ﬁnite discrete sets. assumption assume exists hidden states conditionally independent given rank matrix |h|. theorem consider tv-embedding w.r.t. assumption exists function consider tv-embedding w.r.t. then assumption exists function rank consider matrix also regard matrix matrix matrix equation obtain b)−ba. consider matrix b)−b. know elements correspond function therefore relationship implies exists function text region embedding function maps region text numerical vector. particular form region embedding consider takes either sequential representation text region input. precisely consider language vocabulary word language taken represented dimensional vector referred one-hot-vector representation. vector components represents vocabulary entry. vector representation value component corresponding word value zeros elsewhere. text region size sequence words word represented dimensional vector concatenation vector representations words section main text. call representation seq-representation. alternative bow-representation main text. possible text regions size seq-representation consider embeddings text region form embedding matrix bias vector learned training training objective depends task. following particular form region embedding referred retex vectors produced retex results retex referred retex vectors. goal region embedding learning high-level concepts low-dimensional vectors. said main text cannot done word embedding learning since word embedding embeds individual words isolation primitive correspond high-level concepts. example easy conveys positive sentiment isolation not. analysis representation power retex show unlike word embeddings retex model high-level concepts using co-presence absence words region similar traditional m-grams efﬁcient/robust. first show real-valued function deﬁned exists retex function expressed terms linear function retex vectors. property often referred universal approximation literature proposition consider real-valued function deﬁned exists embedding matrix bias vector vector proof. denote entry corresponding i-th j-th column. assume element represented dimensional vector ones given speciﬁc indexes j-th component one. create indicator function. −|si| denotes i-th component follows wi·x wi·x otherwise. manner create every member follows proof essentially constructs indicator functions m-grams maps corresponding function values. thus representation power retex least good m-grams powerful word embeddings spite seeming similarity form. however well known traditional m-gram-based approaches assign vector dimension m-gram suffer data sparsity problem m-gram useful seen training data. retex clear advantages. show similar m-grams similar lower-dimensional vectors helps learning task interest. also expressive traditional m-gram-based approaches co-presence also absence words single dimension. properties lead robustness data sparsity. ﬁrst introduce deﬁnition simple concept. deﬁnition consider seq-representation. high level semantic concept called simple deﬁned follows. word groups signs. deﬁne i-th word either belongs next proposition illustrates points stating retex ability represent simple concept single dimension. contrast construction proof proposition dimension could represent m-gram. proposition indicator function simple concept embedded dimension using retex. proof. consider text region vector seq-representation contains |dimensional segments i-th segment represents i-th position text region. i-th segment vector zeros except components following proposition shows retex embed concepts unions simple concepts low-dimensional vectors. proposition union simple concepts exists function linear function q-dimensional retex vectors proof. rows constructed proof proposition function desired property. note much smaller number m-grams concept proposition shows retex ability simultaneously make word similarity fact words occur context reduce embedding dimension. word embedding model word similarity model context. m-gram-based approaches model context cannot model word similarity means concept/context expressed large number individual m-grams leading data sparsity problem. thus representation power retex exceeds single-word embedding traditional m-gram-based approaches.", "year": 2015}