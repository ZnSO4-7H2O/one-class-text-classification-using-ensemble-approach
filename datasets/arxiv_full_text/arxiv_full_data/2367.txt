{"title": "The Bayesian Structural EM Algorithm", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In recent years there has been a flurry of works on learning Bayesian networks from data. One of the hard problems in this area is how to effectively learn the structure of a belief network from incomplete data- that is, in the presence of missing values or hidden variables. In a recent paper, I introduced an algorithm called Structural EM that combines the standard Expectation Maximization (EM) algorithm, which optimizes parameters, with structure search for model selection. That algorithm learns networks based on penalized likelihood scores, which include the BIC/MDL score and various approximations to the Bayesian score. In this paper, I extend Structural EM to deal directly with Bayesian model selection. I prove the convergence of the resulting algorithm and show how to apply it for learning a large class of probabilistic models, including Bayesian networks and some variants thereof.", "text": "efficiently follows algorithm fixed paramet­ model hence call method structural name ms-em used.) roughly speaking performs structural ture parameters). parameters current ture. former case standard later \"structural\" penalized bic/mdl score procedure maxima. integrate network. gral cannot solved closed form. current learn incomplete either stochastic approximate former methods tend computationally latter laplace unimodal function algorithm ture search model selection. networks clude bic/mdl score various bayesian score. deal directly convergence apply learning including introduction belief networks bility distributions. choice uncertamty successfully gines optimal lief networks process. terest current ture parameters data record describes work. unfortunately recently incomplete. exclusively used adjusting network structure. considered technology data contains belief allow pnncii?l�d data. however incomplete time require complete data concise s�ruct'lre cruci.al hidden variables approximate adapt algorithm results standard whether approximation search space network framework propose provements experimental works learned networks directed random variables encodes following conditional variable independent second component local models local model maps possible probability rameterized unique preliminaries section cludes various discuss data problems raised factored models start notation. variable denote specific variables assignments boldface denoted explanation explanations willing models model parameterized vector defines probability possible data sets underlying shorthand context.) models zero measure disjoint events. rithms described algo­ form models factored model parametric family defines joint probability mhem) variables model separable parameters e{'f cross product words legal parameterization factors assumption straightforward model. moreover locally example lief networks ues. standard networks table contains networks local models product lipa vector contains parameters andlipa case write joint probability ti�=l flpa multinets cision graphs sentations class models factored separable probability product factor requires changing model. thus every combination results distribution. thus approximate posteriori models highest data sufficient since would expect posterior weight models. learning complete data data complete assigns exploit make assumptions parameters parameters factors assumptions parameter assumption tion depend accumulated data. thus evaluate summary data form accumulated statistics. example complete problem multinomial dirichlet multinomial distribution hyperparameters values space models model factor subtree tors model remain unchanged. justifies decide root tree. learning incomplete data models incomplete learning learning complete fact posterior independent data longer product parameters score thus search model space requires expensive searching large space possible search becomes infeasible-the large amount computation model. thus although investigations bayesian experiments model. sake clarity take values easily continu­ likelihood functions since normalizing models compare. seen values previous efficiently. usually compute following mh). estimate seen previous section true class factored also assume given particular model perform predictive seen mod­ efficiently compute predictions thus choose model maximizes provably marginal theorem also implies them-step m*-step next issue compute probability ments according procedure need however discussed above incomplete evaluate efficiently. problem using approximation want compute expectation learn parameters standard imation computation parameters either zero. example multinomial factor lspa assigned procedure search method structures. models interested class chow trees algorithms construct idea within approach em.) cases must resort procedure search procedure factored algorithm. factored-bayesian-sem above need estimate distribution score factor many models many models nential family expected easily simple approximation elements -;;nv;. thus size expected marginalize another grows hessian nomial factors safely linear hope provide conditions full version side dimension ical integration techniques dimension proximation. well-behaved unimodal laplace's tion would work well. perform laplace's tion case need find maximum point above flogr*cpdni \"truncated't truncation goes evaluate integral called tegration procedures particularly suitable encoded quite described below evaluation smaller tations count simplify mean variance also prior count event. finally minimal maximal values take data. consider approximation values -for using range single edge change structural convergence. procedure cedure uses cheeseman-stutz structures version five perturbations. tried moves similar somewhat procedure assignments previous procedure completed learned using standard interpreted tural analysis singh's limiting behavior using completed datasets tation score. however estimates structures datasets. need merge learned networks. hidden meila network chow tree. exploit collect although procedure proach local maximum. multinets using ining approximations algorithm that terminology factored-bayesian-sem instance using linear approximation applied efficient method caching expected statistics answer variables queries pass training directly telligent approach learn models hidden variable etc. select clearly model learned tial structure research examines cedure constraint-based approaches learn variables search. acknowledgments grateful kevin murphy parr stuart russell useful discussions anonymous referee gate appropriateness detail. work done international. research supported grant number daah-l- grant number nooo restriction focuses learning want committee prediction. mation ensure commit particulars ports models. attempt ture models mixture network. nonetheless larger data better address references abramowitz beinlich binder kanazawa. adap tive probabilistic machine learning boutilier mixture. however buntine. trees. hand stats single directly cheeseman advances knowledge discovery data mining chickering heckerman. efficient approximations learning chickering cooper herskovits. bayesian optimal statistical decisions degroot. dempster laird friedman. learning bayesian friedman goldszmidt. appeared geiger heckerman. geiger heckerman heckerman. learning graphical models heckerman bacchus. lauritzen. mackay. ensemble meila nips pearl. probabilistic rubin. inference missing saul jaakkola singh. learning bayesian spirtes thiesson alternativ bayesian bayesian committee turns variant learn bayesian bayesian structural stage bayesian model weighted iteration expected treatment topic current additional computation large number computations main bottleneck domains. standard evaluating instances. attractive sample however requires noise estimation algorithm. possible type learning", "year": 2013}