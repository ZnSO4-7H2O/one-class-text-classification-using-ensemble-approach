{"title": "dna2vec: Consistent vector representations of variable-length k-mers", "tag": ["q-bio.QM", "cs.CL", "cs.LG", "stat.ML"], "abstract": "One of the ubiquitous representation of long DNA sequence is dividing it into shorter k-mer components. Unfortunately, the straightforward vector encoding of k-mer as a one-hot vector is vulnerable to the curse of dimensionality. Worse yet, the distance between any pair of one-hot vectors is equidistant. This is particularly problematic when applying the latest machine learning algorithms to solve problems in biological sequence analysis. In this paper, we propose a novel method to train distributed representations of variable-length k-mers. Our method is based on the popular word embedding model word2vec, which is trained on a shallow two-layer neural network. Our experiments provide evidence that the summing of dna2vec vectors is akin to nucleotides concatenation. We also demonstrate that there is correlation between Needleman-Wunsch similarity score and cosine similarity of dna2vec vectors.", "text": "ubiquitous representation long sequence dividing shorter k-mer components. unfortunately straightforward vector encoding k-mer one-hot vector vulnerable curse dimensionality. worse distance pair one-hot vectors equidistant. particularly problematic applying latest machine learning algorithms solve problems biological sequence analysis. paper propose novel method train distributed representations variable-length k-mers. method based popular word embedding model wordvec trained shallow two-layer neural network. experiments provide evidence summing dnavec vectors akin nucleotides concatenation. also demonstrate correlation needleman-wunsch similarity score cosine similarity dnavec vectors. usage k-mer representation popular approach analyzing long sequence fragments. k-mer representation simple understand compute. unfortunately straightforward vector encoding one-hot vector vulnerable curse dimensionality. speciﬁcally one-hot vector dimension exponential length example -mer needs vector dimension problematic applying latest machine learning algorithms solve problems biological sequence analysis fact tools prefer lower-dimensional continuous vectors input worse distance arbitrary pair one-hot vectors equidistant even though atggc closer atggg cacga. natural language processing research community long tradition using bag-of-words one-hot vector dimension equal vocabulary size. recently explosion using word embeddings inputs machine learning algorithms especially deep learning community word embeddings vectors real numbers distributed representations words. popular training technique word embeddings wordvec consists using -layer neural network trained current word surrounding context words reconstruction context words loosely inspired linguistic concept distributional hypothesis states words appear context similar meaning deep learning algorithms applied word embeddings dramatic improvements areas machine translation summarization sentiment analysis image captioning fascinating properties wordvec vector arithmetic solve semantic linguistic analogies showed vec. particular analogy task manking woman??? interpreted ﬁnding word closest cosine distance. furthermore showed analogy works past-tense relation paper present novel method compute distributed representations variable-length k-mers. k-mers consistent across diﬀerent lengths i.e. embedding vector space. continuous vector space dimensions. training method shallow two-layer neural network dnavec based wordvec. biovec seqvec also applied wordvec technique biological sequences. although techniques used two-layer neural network train embedding technique generalization variable-length presented method commonly known needleman-wunsch algorithm computing similarity k-mers using dynamic programming scoring global alignments. dynamic programming nature algorithm makes algorithm slow quadratic time complexity length sequence. section show cosine distance words angular distance related needleman-wunsch distance corresponding k-mers. section provide evidence nucleotide concatenation analogy constructed dnavec arithmetic. main contribution work includes variable-length k-mer embedding model experimental evidence shows arithmetic dnavec vectors akin nucleotides concatenation relationship needleman-wunsch alignment cosine similarity dnavec vectors nucleotide concatenation analogy constructed dnavec arithmetic. separate genome long non-overlapping fragments convert long fragments overlapping variable-length k-mers unsupervised training aggregate embedding model using two-layer neural network decompose aggregated model k-mer lengths. fragment genome sequence based characters experiments using dataset fragments typically couple thousand nucleotides. introduce entropy randomly choose fragment’s reverse-complement. given sequence convert sequence overlapping ﬁxed length k-mer sliding window length across example convert tagactgtc -mers {tagac agact gactg actgt ctgtc}. variable-length case sample discrete uniform distribution uniform determine size window. example sample k-mers could {taga gact ctgtc}. shallow two-layer neural train aggregate k-mer embedding. method based wordvec wordvec algorithm options continuous bag-of-words skip-gram. cbow predicts targeted word given context skip-gram predicts context given targeted word. wordvec homepage claims skip-gram slower train cbow skip-gram better infrequent words. skip-gram experiments. dnavec algorithm trained predicting context surrounding given targeted k-mer. context adjacent k-mers surrounding targeted k-mer. example context k-mer gact would {taga ctgtc} previous example section experiments paper used context size targeted word amounts predicting total k-mers. training either negative sampling hierarchy softmax typically used optimize update procedure words. used negative sampling experiments. stage decompose aggregated model k-mer lengths decompose aggregate model k-mer length form khigh klow models. decomposition useful searching nearest neighbors discuss section dnavec trained human assembly speciﬁcally downloaded http//hgdownload.cse.ucsc.edu/downloads.htmlhuman. excluded chromosomes well mitochondrial unlocalized sequences. found summing dnavec embeddings related concatenating k-mers. table investigated hypothesis adding dnavec embeddings arbitrary k-mers examining whether vector sum’s neighbors overlap string concatenation. column results tallied using equation columns used nnearestneighbors section experiment string concatenation come ends. example following condition would marked success table k-mers concatenation dnavec addition. took samples operand. example ﬁrst aggregated summing dnavec vectors individual pairs arbitrary -mer observing whether string concatenation overlaps vector sum’s n-nearest -mer neighbors. needleman-wunsch similarity score paper computed using biopython’s align.globalxx function used match score mismatch penalty figure provided evidence edit distance arbitrary k-mers correlated cosine distance corresponding dnavec vectors. sampled pairs -mers needleman-wunsch score level plot needleman-wunsch similarity score dnavec cosine similarity. figure compared needleman-wunsch similarity distribution k-mer nearest dnavec neighbor distribution random k-mers. speciﬁcally sampled -mers found nearest neighbor using equation computed needleman-wunsch score pair. null distribution sampled pairs random -mers. thus found evidence dnavec nearest-neighbor exhibits alignment similarity. experimented types nucleotide concatenation analogy strong weak concatenations. given k-mers length deﬁne strong concatenation splicing nucleotides k-mers. example -mer -nucleotides snippet would figure boxplot needleman-wunsch score dnavec cosine similarity. lower upper hinges quartiles respectively. spearman’s rank correlation coeﬃcient figure global alignment score distribution nearest-neighbor. nearest-neighbor distribution generated computing needleman-wunsch score -mer nearest neighbor. null distribution computing score random -mers. experimental samples generated randomly sampling k-mers equal length nucleotide snippet concatenation. strong weak concatenation experiments randomly selected either splice. table shows summary experimental results types nucleotide concatenations. particularly accuracy weak concatenation analogy -mer -nucleotides snippet considering deﬁned section note considering -nearest neighbors relatively small comparing space possible -mers merely possible -mers possible -mers. conﬁrm whether arithmetic actually extending k-mer snippet oppose similarity comparison compared analogy results scrambled-snippet experiments concatenated diﬀerent random snippets answer case. expected vector arithmetic signiﬁcantly favoring correct matching snippet diﬀerent random snippet figure table table analogy experiment. analyzed types analogies weak strong concatenation. samples randomly generated type. comparison generated samples using scrambled-snippet sampling strategy. make code data available https//pnpnpn.github.io/dnavec/ upon publication. two-layer neural network training method described section implemented using gensim framework used gensim’s wordvec class parameters window= speciﬁed usage skipgram model half-size context window respectively. trained dnavec vectors used paper dimension size since window sliding step section stochastic terms variable-length could essentially generate training data looping complete genomic sequence data multiple passes called epochs. dnavec model used paper trained epochs. training step took days using gensim parameter workers= quad-core intel xeon memory. figure cumulative mass analogy experiment -mer snippet. samples generated strong-concatenation analogy setup. compared another samples using scrambledsnippet sampling procedure. work presented novel method training distributed representations k-mers. demonstrated dnavec embeddings represent variable-length k-mers consistent fashion nucleotide concatenation experiments. provided experimental evidence showing arithmetic dnavec vectors akin nucleotides concatenation. also showed needleman-wunsch similarity score arbitrary k-mers correlated cosine distance corresponding dnavec vectors. future work fact many machine learning algorithms require ﬁxed-length continuous vectors input explore application dnavec machine learning techniques biological sequence analysis. merriënboer gulcehre bahdanau bougares schwenk bengio learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv.. harris distributional structure. word convolutional neural networks sentence classiﬁcation. arxiv preprint arxiv.. kimothi soni biyani hogan distributed representations biological sequence mikolov sutskever chen corrado dean distributed representations words phrases compositionality. advances neural information processing systems pages řehůřek sojka software framework topic modelling large corpora. proceedings lrec workshop challenges frameworks pages valletta malta. elra. http//is.muni.cz/publication//en. rosenbloom armstrong barber casper clawson diekhans dreszer fujita guruvadoo haeussler ucsc genome browser database update. nucleic acids research d–d. turian ratinov bengio word representations simple general method semi-supervised learning. proceedings annual meeting association computational linguistics pages association computational linguistics.", "year": 2017}