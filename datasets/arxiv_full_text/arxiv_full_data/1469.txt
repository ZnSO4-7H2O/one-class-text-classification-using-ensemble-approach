{"title": "A Joint Speaker-Listener-Reinforcer Model for Referring Expressions", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "Referring expressions are natural language constructions used to identify particular objects within a scene. In this paper, we propose a unified framework for the tasks of referring expression comprehension and generation. Our model is composed of three modules: speaker, listener, and reinforcer. The speaker generates referring expressions, the listener comprehends referring expressions, and the reinforcer introduces a reward function to guide sampling of more discriminative expressions. The listener-speaker modules are trained jointly in an end-to-end learning framework, allowing the modules to be aware of one another during learning while also benefiting from the discriminative reinforcer's feedback. We demonstrate that this unified framework and training achieves state-of-the-art results for both comprehension and generation on three referring expression datasets. Project and demo page: https://vision.cs.unc.edu/refer", "text": "referring expressions natural language constructions used identify particular objects within scene. paper propose uniﬁed framework tasks referring expression comprehension generation. model composed three modules speaker listener reinforcer. speaker generates referring expressions listener comprehends referring expressions reinforcer introduces reward function guide sampling discriminative expressions. listenerspeaker modules trained jointly end-to-end learning framework allowing modules aware another learning also beneﬁting discriminative reinforcer’s feedback. demonstrate uniﬁed framework training achieves state-of-theart results comprehension generation three referring expression datasets. project demo page https//vision.cs.unc.edu/refer. people often referring expressions everyday discourse unambiguously identify indicate particular objects within physical environment. example might point person crowd referring blue shirt might someone pass table. examples pragmatic interaction people first speaker must generate expression given target object surrounding world context. second listener must interpret comprehend expression object environment. therefore paper propose endto-end trained listener-speaker framework models behaviors jointly. goal corresponds gricean maxim manner tries clear brief orderly possible avoiding obscurity ambiguity. avoiding ambiguity important generated expression easily uniquely mapped target object. example pens table long short asking would ambiguous asking long would better. reinforcer module incorporated using reinforcement learning inspired behavioral psychology says agents operating environment take actions maximize expected cumulative reward. case reward takes form discriminative classiﬁer trained reward speaker generating less ambiguous expressions. within realm referring expressions tasks computationally modeled mimicking listener speaker roles. referring expression generation requires algorithm generate referring expression given target object visual scene shown fig. referring expression comprehension requires algorithm localize object/region image described given referring expression shown fig. referring expression generation task studied since many early works space focused relatively limited datasets using synthesized images objects artiﬁcial scenes limited sets real-world objects simpliﬁed environments recently research focus shifted complex natural image datasets expanded include referring expression comprehension task well real-world interactions robotics reason become feasible several large-scale datasets collected scale deep learning models applied. kazemzadeh introduced ﬁrst large-scale dataset natural images imageclef dataset using interactive game later expanded imfigure example comprehension results using full model three datasets. green shows ground-truth region blue shows correct comprehension based detected regions. ages mscoco dataset addition collected google’s dataset also based mscoco images non-interactive setting resulting complex lengthy expressions. paper focus evaluations three recent datasets collected mscoco images recent neural approaches referring expression generation comprehension tasks roughly split types. ﬁrst type uses cnn-lstm encoderdecoder generative model generate sentences given encoded target object. careful design visual representation target object model generate unambiguous expressions here cnn-lstm models referring expression target object easily converted bayes’ rule used address comprehension task selecting largest posterior probability. second type approach uses joint-embedding model projects visual representation target object semantic representation expression common space learns distance metric. generation comprehension performed embedding target object embedding space retrieving closest expression space. type approach typically achieves better comprehension performance cnnlstm model previously applied referring expression comprehension task. recent work also used encoder-decoder model embedding model referring expression generation abstract images ofﬂine listener reranks speaker’s output. paper propose uniﬁed model jointly learns cnn-lstm speaker embedding-based listener models generation comprehension tasks. additionally discriminative reward-based reinforcer guide sampling discriminative expressions improve ﬁnal system. instead working independently speaker listener reinforcer interact other resulting improved performance generation comprehension tasks. results evaluated three standard large-scale datasets verify proposed listener-speaker-reinforcer model signiﬁcantly outperforms state-of-the-art comprehension task generation task advanced cognition capabilities several tasks emerged evaluation applications including image captioning visual question answering referring expression generation/comprehension. image captioning image captioning generate sentence describing general content image. recent approaches deep learning address problem. perhaps common architecture cnnlstm model generates sentence conditioned visual information image. paper related work glstm uses semantics guide caption generation. step beyond image captioning locate regions described captions visual genome collected captions dense regions image used dense-captioning tasks many approaches achieving quite good results captioning challenge caption system output image task dependent. therefore movement toward focused tasks visual question answering referring expression generation comprehension involve speciﬁc regions/objects within image referring expression datasets studied many years linguistics natural language processing mainly focused small artiﬁcial datasets. kazemzadeh introduced ﬁrst large-scale dataset refclef using real-world natural images dataset collected two-player game ﬁrst player writes referring expression given indicated target object. second player shown image expression click correct object described expression. click lies within target object region sides points roles switch. using game interace authors collected refcoco refcoco+ datasets mscoco images refcoco refcoco+ datasets contain referred objects referring expressions average. main difference refcoco refcoco+ refcoco+ players forbidden using absolute location words e.g. left therefore focusing referring expression purely appearance-based descriptions. addition also collected referring expression dataset refcocog using mscoco images non-interactive framework. expressions similar mscoco captions longer complex time constraint non-interactive data collection setting. dataset refcocog objects expressions object average. referring expression comprehension generation referring expressions associated tasks comprehension generation. comprehension task requires system select region described given referring expression. address problem model looks object maximizing probability. people also modeling directly using embedding model learns minimize distance paired object sentence embedding space. generation task asks system compose expression speciﬁed object within image. previous work used rulebased approaches generate expressions ﬁxed grammar pattern recent work followed cnnlstm structure generate expressions speaker listener typically considerred formulate tasks speaker model used referring expression generation listener used comprehension. golland proposed optimal speaker based listener’s response. followed idea suppressing ambiguity listener modeling related work uses speaker model generate expressions abstract images uses ofﬂine listener rerank speaker’s output. compared above model jointly trains speaker listener modules additional reinforcer module encourage unambiguous speaker expression generation. moreover models generation comprehension tasks three natural image datasets. model speaker listener reinforcer during training speaker listener trained jointly beneﬁt reinforcer. reward function reinforcer differentiable incorporated training using reinforcement learning policy gradient algorithm. speaker speaker module follow previous state-ofthe-art cnn-lstm framework. here pre-trained model used deﬁne visual representation target object visual context. then long-short term memory used generate likely expression given visual representation. improved quantitative performance visual comparison model speaker here visual representation includes target object context location/size features visual comparison features. speciﬁcally target object representation modeled features pre-trained network global context modeled features extracted vgg-fc layer entire image. location/size representation target object modeled dimensional vector encoding locations figure framework speaker cnn-lstm model generates referring expression target object. listener joint-embedding model learned minimize distance paired object expression representations. addition reinforcer module helps improve speaker sampling discriminative expressions training. model jointly trained loss functions generation loss embedding loss reward loss thereby improving performance comprehension generation tasks. within image generalize idea incorporate triplet hinge losses composed positive match negative matches. given positive match sample contrastive pair expression describing object pair object image optimize following max-margin loss joint-embedding model mimick listener’s behaviour. purpose embedding model encode visual information target object semantic information referring expression joint embedding space embeds vectors visually semantically related closer together space. referring expression comprehension task given referring expression representation listener embeds joint space selects closest object embedding space predicted target object. referring expressions often relate object objects type within image comparisons tend quite important differentiation. comparison features composed parts appearance similaroi−oj oi−oj number objects chosen comparisons location size similarity concatenating difference compared object δlij xtl]ij ﬁnal visual representation target object concatenation features followed fully-connected layer fusing together joint feature lstm referring expression generation. training minimize negative log-likelihood thing left choose reward function encourages speaker sample less ambiguous expressions. illustrated fig. reinforcer module learns reward function using paired objects expressions. visual representation target object another lstm encode expression representation. rather using mlps encode view listener concatenate views feed together learn logistic regression score trained cross-entropy loss reward function computes match score input object expression. score reward signal eqn. sampled expression target object pairs. training reward function ﬁxed assist joint speaker-listener system. joint model subsection describe speciﬁcs three modules integrated joint framework listener notice visual vector embedding space learned capture neighbourhood vectors referring expressions thus making aware listener’s knowledge. therefore take embedded vector additional input speaker encodes listener based information. fig. concatenation jointly encode standard visual representation target object listener-aware representation feed speaker. besides concatenation elementwise product compact bilinear pooling also applied training sample triplets speaker listener make word embedding speaker listener shared reduce number parameters. reinforcer module sentence sampling using speaker’s lstm shown right fig. within mini-batch sampled expressions target objects reward function obtain reward values. comprehension task test time could either speaker listener select target object given input expression. using listener would embed input expression learned embedding space select closest object predicted target. using speaker would generate expressions object within image select object whose generated model lstm encode input referring expression visual representation speaker encode target object mlps normalization layers following view object expression. composed fully connected layers relu nonlinearities them serving transform object view expression view common embedding space. inner-product normalized representations computed similarity score space. listener force similarity target object referring expression pairs applying hinge loss triplets consist positive match negative matches note listener model limited particular triplet-based model. example computes similarity score every object given referring expression minimizes cross entropy softmax knowing target object could also applied here. reinforcer besides using ground-truth pairs target object referring expression training speaker also reinforcement learning guide speaker toward generating less ambiguous expressions reinforcer module composed discriminative reward function performs non-differentiable policy gradient update speaker. speciﬁcally given softmax output speaker’s lstm sample words according categorical distribution time step resulting complete expression sampling <end> token. sampling operation non-differentiable know whether expression ambiguous feed reward function. therefore policy gradient reinforcement learning update speaker’s parameters. here goal maximize reward expectation distribution parameterized speaker i.e. according policy gradient algorithm expression best matches input expression. therefore utilize modules ensembling speaker listener predictions together pick probable object given referring expression. perform experiments three referring expression datasets refcoco refcoco+ refcocog three datasets collected mscoco images several differences refcoco refcoco+ collected using interactive game interface refcocog collected noninteractive setting contains longer expressions refcocog contains average objects type images refcoco refcoco average refcoco+ disallowed absolute location words referring expressions. overall refcoco expressions objects images refcoco+ expressions objects images refcocog expressions objects images. additionally dataset provided dataset splits evaluation. refcoco refcoco+ provide person object splits evaluation. images containing multiple people testa images containing multiple objects categories testb. refcocog authors divide dataset randomly partitioning objects training testing splits. thus image appear splits. training validation splits released dataset hyperparamters cross-validated refcoco train models refcocog. training either speaker listener perform comprehension task. speaker models feed every ground-truth object region within given image speaker select probable region expression comprehension listener result i.e. argmaxoi directly compute similarity score proposal/object expression pick object highest probability. evaluation compute intersection-over-union comprehended region ground-truth object. score predicted region greater consider correct comprehension. demonstrate beneﬁts module ablation studies lines table lines table three datasets. shows results adding module training. models speaker listener highlight surprisingly using speaker alone already achieves state-of-art results joint training. adding listener improves performance previous state-of-art results. generation task ﬁrst speaker generate multiple expressions object beam search. listener rerank expressions select least ambiguous expression similar fully utilize listener’s power generation propose consider cross comprehension well diversity expressions minimizing potential ﬁrst term second term unary potential measure well target object generated expression match using speaker listener modules respectively also used third term unary potential measures likelihood generated sentence describing objects image. pairwise potential penalize sentences generated different objects expressions every object image jointly generated. compared previous model attempted language generation referring expressions together constraints eqn. explicit overall works better reduce ambiguity generated expressions. optimize model using adam initial learning rate halved every iterations batch size word embedding size hidden state size lstm also output fig. -dimensional. avoid overﬁtting apply dropout ratio linear transformation layers listener. also regularize word-embedding output layers lstm speaker using dropout ratio listener previous state-of-art baseline speaker speaker+listener speaker+reinforcer speaker+listener+reinforcer baseline+mmi speaker+mmi speaker+listener+mmi speaker+reinforcer+mmi speaker+listener+reinforcer+mmi listener previous state-of-art baseline speaker speaker+listener speaker+reinforcer speaker+listener+reinforcer baseline+mmi speaker+mmi speaker+listener+mmi speaker+reinforcer+mmi speaker+listener+reinforcer+mmi table ablation study using speaker module comprehension task half shows performance given ground truth bounding boxes objects bottom half performance using automatic object detectors select potential objects. adding listener reinforcer modules speaker increases performance. listener previous state-of-art speaker+listener speaker+listener+reinforcer speaker+listener+reinforcer speaker+listener+mmi speaker+listener+reinforcer+mmi speaker+listener+reinforcer+mmi listener previous state-of-art speaker+listener speaker+listener+reinforcer speaker+listener+reinforcer speaker+listener+mmi speaker+listener+reinforcer+mmi speaker+listener+reinforcer+mmi table ablation study using listener ensembled listener+speaker modules comprehension task half shows performance given ground truth bounding boxes objects bottom half performance using automatic object detectors select potential objects. jointly training speaker improves listener’s performance adding reinforcer module ensembled speaker+listener prediction performs best. module used comprehension bold. example speaker+listener means speaker module joint model comprehension task speaker+listener means listener module figure example comprehension results based detection. green shows ground-truth region blue shows correct comprehension using speaker+listener+reinforcer+mmi model shows incorrect comprehension. rows show correct comprehensions bottom rows show incorrect ones. speaker+tie baseline+mmi speaker+mmi speaker+listener+mmi speaker+reinforcer+mmi speaker+listener+reinforcer+mmi baseline+mmi+rerank speaker+mmi+rerank speaker+listener+mmi+rerank speaker+reinforcer+mmi+rerank speaker+listener+reinforcer+mmi+rerank following previous work show results trained trained speakers. compare models baseline model pure listener model previous state-of-art results achieved trained outperform mmi. also speaker improved joint training listener module incorporating reinforcer module. ranking speaker learned joint training able outperform pure listener around three datasets already achieves state-of-art performance comprehension task. second show evaluations using variations listener module ensembled listener+speaker modules comprehension task table note listener generally works better speaker comprehension task indicating deterministic joint-embedding model suitable task speaker model similar results observed reinforcer module seems effective speaker still joint training always brings additional discriminative beneﬁts listener module resulting improved performance ensembling speaker listener together achieves best results overall. experiments analyze comprehension performance given ground truth bounding boxes potential comprehension objects algorithm must select objects described. provides analysis comprehension performance independent particular object detection method. additionally also show results using object detector automatically select regions consideration comprehension bottom half table detection algorithm current state detector effectiveness speed trained subset coco train+val dataset removing images test splits refcoco refcoco+ validation split refcocog. empirically select conﬁdence threshold detection output. performance drops somewhat strong dependence visdif model detection overall improvements brought module consistent using ground-truth objects showin robustness joint model. fig. shows comprehension results using full model. generation task evaluate variations speaker module. evaluating generation simple comprehension. image captioning bleu rouge meteor cider common automatic metrics widely used standard evaluations. show automatic evaluation using meteor cider metrics generation table +rerank denotes models incorporating reranking mechanism global optimization objects computer cider robustly collect expressions objects test sets refcoco refcoco+ obtaining expressions respectively average object. refcocog original expressions released dataset limited still show performance completeness. choose speaker+tie model reference learns expression generation together achieves state-of-art performance. generally speaker jointly learned models achieves higher scores single speaker under metrics across datasets. improvements observed settings without +rerank +rerank. addition since previous work found metrics always agree well human judgments referring expressions also human evaluation objects refcoco refcoco+. turkers click referred object given generated expression. results shown table. results indicate ablated beneﬁts brought module ultimately +rerank joint model achieves best performance. fig. compare generated expressions using speaker module different models. show joint expression generation using full model +rerank fig. observed expressions every target object considered together meant relevant target object irrelevant objects. conclusion demonstrated effectiveness uniﬁed framework referring expression generation comprehension. model consists speaker listener modules trained jointly improve performance reinforcer module help produce less ambiguous expressions. experiments indicate model outperforms state comprehension generation multiple datasets evaluation metrics.", "year": 2016}