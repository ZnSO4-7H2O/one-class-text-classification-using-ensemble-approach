{"title": "Dynamic Network Surgery for Efficient DNNs", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of $\\bm{108}\\times$ and $\\bm{17.7}\\times$ respectively, proving that it outperforms the recent pruning method by considerable margins. Code and some models are available at https://github.com/yiwenguo/Dynamic-Network-Surgery.", "text": "deep learning become ubiquitous technology improve machine intelligence. however existing deep models structurally complex making difﬁcult deployed mobile platforms limited computational power. paper propose novel network compression method called dynamic network surgery remarkably reduce network complexity making on-the-ﬂy connection pruning. unlike previous methods accomplish task greedy properly incorporate connection splicing whole process avoid incorrect pruning make continual network maintenance. effectiveness method proved experiments. without accuracy loss method efﬁciently compress number parameters lenet- alexnet factor respectively proving outperforms recent pruning method considerable margins. code models available https//github.com/yiwenguo/dynamic-network-surgery. family brain inspired models deep neural networks substantially advanced variety artiﬁcial intelligence tasks including image classiﬁcation natural language processing speech recognition face recognition. despite tremendous successes recently designed networks tend stacked layers thus learnable parameters. instance alexnet designed krizhevsky million parameters ilsvrc classiﬁcation competition times lecun’s conventional model alone much complex models like vggnet since parameters means storage requirement ﬂoating-point operations increases difﬁculty applying dnns mobile platforms limited memory processing units. moreover battery capacity another bottleneck although models normally require vast number parameters guarantee superior performance signiﬁcant redundancies reported parameterizations therefore proper strategy possible compress models without signiﬁcantly losing prediction accuracies. among existing methods network pruning appears outstanding surprising ability accuracy loss prevention. instance recently propose make \"lossless\" compression deleting unimportant parameters retraining remaining ones somehow similar surgery process. however complex interconnections among hidden neurons parameter importance change dramatically network surgery begins. leads main issues well). ﬁrst issue possibility irretrievable network damage. since pruned connections chance come back incorrect pruning cause ∗this work done yiwen intern intel labs china supervised anbang severe accuracy loss. consequence compression rate must suppressed avoid loss. another issue learning inefﬁciency. paper several iterations alternate pruning retraining necessary fair compression rate alexnet retraining process consists millions iterations time consuming. paper attempt address issues pursue compression limit pruning method. speciﬁc propose sever redundant connections means continual network maintenance call dynamic network surgery. proposed method involves operations pruning splicing conducted different purposes. apparently pruning operation made compress network models pruning incorrect pruning responsible accuracy loss. order compensate unexpected loss properly incorporate splicing operation network surgery thus enabling connection recovery pruned connections found important time. operations integrated together updating parameter importance whenever necessary making method dynamic. fact strategies help make whole process ﬂexible. beneﬁcial better approach compression limit also improve learning efﬁciency validated section method pruning splicing naturally constitute circular procedure dynamically divide network connections categories akin synthesis excitatory inhibitory neurotransmitter human nervous systems rest paper structured follows. section introduce related methods compression brieﬂy discussing merits demerits. section highlight intuition dynamic network surgery introduce implementation details. section experimentally analyses method section draws conclusions. figure pipeline dynamic network surgery al.’s method using alexnet example. needs iterations fair compression rate method runs iterations yield signiﬁcantly better result comparable prediction accuracy. order make models portable variety methods proposed. vanhoucke analyse effectiveness data layout batching usage intel ﬁxed-point instructions making speedup cpus. mathieu explore fast fourier transforms gpus improve speed cnns performing convolution calculations frequency domain. alternative category methods resorts matrix decomposition. denil propose approximate parameter matrices appropriately constructed low-rank decompositions. method achieves speedup convolutional layer drop prediction accuracy. following similar ideas subsequent methods provide signiﬁcant speedups although matrix decomposition beneﬁcial compression speedup methods normally incur severe accuracy loss high compression requirement. vector quantization possible compress dnns. gong explore several methods point effectiveness product quantization. hashnet proposed chen handles network compression grouping parameters hash buckets. trained standard backpropagation procedure able make substantial storage savings. recently proposed binaryconnect binarized neural networks able compress dnns factor noticeable accuracy loss sort inevitable. paper follows idea network pruning. starts early work lecun al.’s makes second derivatives loss function balance training loss model complexity. extension hassibi stork propose take non-diagonal elements hessian matrix consideration producing compression results less accuracy loss. spite theoretical optimization methods suffer high computational complexity tackling large networks regardless accuracy drop. recently explore magnitude-based pruning conjunction retraining report promising compression results without accuracy loss. also validated sparse matrix-vector multiplication accelerated certain hardware design making efﬁcient traditional calculations drawback al.’s method mostly potential risk irretrievable network damage learning inefﬁciency. research network pruning partly inspired effective compress dnns also makes assumption network structure. particular branch methods naturally combined many methods introduced above reduce network complexity. fact already tested combinations obtained excellent results. section highlight intuition method present implementation details. order simplify explanations talk convolutional layers fully connected layers. however claimed pruning method also applied layer types long underlying mathematical operations inner products vector spaces. first clarify notations paper. suppose model represented denotes matrix connection weights layer. fully connected layers p-dimensional input q-dimensional output size simply convolutional layers learnable kernels unfold coefﬁcients kernel vector concatenate matrix. order represent sparse model part connections pruned away binary matrix entries indicating states network connections i.e. whether currently pruned not. therefore additional matrices considered mask matrices. since goal network pruning desired sparse model shall learnt dense reference. apparently abandon unimportant parameters keep important ones. however parameter importance certain network extremely difﬁcult measure mutual inﬂuences mutual activations among interconnected neurons. network connection redundant existence others soon become crucial others removed. therefore appropriate conduct learning process continually maintain network structure. taking layer example propose solve following optimization problem network loss function indicates hadamard product operator consists entry indices matrix discriminative function satisﬁes parameter seems crucial current layer otherwise. function designed base prior knowledge constrain feasible region simplify original np-hard problem. sake topic conciseness leave discussions function section problem solved alternately updating stochastic gradient descent method introduced following paragraphs. since binary matrix determined constraints need investigate update scheme inspired method lagrange multipliers gradient descent give following scheme updating indicates positive learning rate. worth mentioning update important parameters also ones corresponding zero entries considered unimportant ineffective decrease network loss. strategy beneﬁcial improve ﬂexibility method enables splicing improperly pruned connections. partial derivatives formula calculated chain rule randomly chosen minibatch samples. matrix updated shall applied re-calculate whole network activations loss function gradient. repeat steps iteratively sparse model able produce excellent accuracy. procedure summarized algorithm note that dynamic property method shown aspects. hand pruning operations performed whenever existing connections seem become unimportant. hand mistakenly pruned connections shall re-established appear important. latter operation plays dual role network pruning thus called \"network splicing\" paper. pruning splicing constitute circular procedure constantly updating connection weights setting different entries analogical synthesis excitatory inhibitory neurotransmitter human nervous system figure overview method method pipeline found figure candidates ﬁnally found absolute value input best choice claimed parameters relatively small magnitude temporarily pruned others large magnitude kept spliced iteration algorithm obviously threshold values signiﬁcant impact ﬁnal compression rate. certain layer single threshold based average absolute value variance connection weights. however improve robustness method thresholds importing small margin equation parameters range function outputs corresponding entries means parameters neither pruned spliced current iteration. considering algorithm complicated standard backpropagation method shall take steps boost convergence. first suggest slowing pruning splicing frequencies operations lead network structure change. done triggering update scheme stochastically probability rather constantly. function shall monotonically non-increasing satisfy prolonged decrease probability even zero i.e. pruning splicing conducted longer. another possible reason slow convergence vanishing gradient problem. since large percentage connections pruned away network structure become much simpler probably even much \"thinner\" utilizing method. thus loss function derivatives likely small especially reference model deep. resolve problem pruning convolutional layers fully connected layers separately dynamic still somehow similar section experimentally analyse proposed method apply popular network models. fair comparison easy reproduction reference models trained implementation caffe package .prototxt ﬁles provided community. also follow default experimental settings method including training batch size base learning rate learning policy maximal number training iterations. reference models obtained directly apply method reduce model complexity. brief summary compression results shown table begin with consider experiment synthetic data preliminary testify effectiveness method visualize compression quality. exclusive-or problem good option. nonlinear classiﬁcation problem illustrated figure experiment turn original problem complicated figure gaussian noises mixed original data order classify samples design network model illustrated left part figure consists connections weight learned. sigmoid function chosen activation function hidden output neurons. twenty thousand samples randomly generated experiment half used training samples rest test samples. iterations learning three-layer neural network achieves prediction error rate weight matrix network connections input hidden neurons found figure apparently ﬁrst last share similar elements means hidden neurons functioning similarly. hence appropriate model compression reference even though large. iterations reference model compressed right side figure connection weights masks shown figure grey green patches stand entries equal corresponding connections shall kept. particular green ones indicate connections mistakenly pruned beginning spliced surgery. patches indicate corresponding connections permanently pruned end. compressed model prediction error rate slightly better reference model even though parameters zero. note that remaining hidden neurons three different logic gates altogether make classiﬁer. however pruning operations conducted initial parameter magnitude probably four hidden neurons ﬁnally kept obviously optimal compression result. addition reduce impact gaussian noises enlarge margin positive negative samples current model compressed hidden neuron pruned method. carefully explained mechanism behind method preliminarily testiﬁed effectiveness. following subsections test method three popular models make quantitative comparisons network compression methods. mnist database handwritten digits widely used experimentally evaluate machine learning methods. test method network models lenet- lenet-. lenet- conventional model consists learnable layers including convolutional layers fully connected layers. designed lecun document recognition. parameters learned train model iterations obtain prediction error rate lenet-- described classical feedforward neural network three fully connected layers learnable parameters. also trained iterations following learning policy lenet-. well trained lenet-- model achieves error rate proposed method able compress models. batch size learning rate learning policy reference training processes except maximal number iterations properly increased. results shown table convergence network parameters lenet- lenet-- reduced factor respectively means less network connections kept prediction accuracies good slightly better. better demonstrate advantage method make layer-by-layer comparisons compression results al.’s table best knowledge method effective pruning method learning inefﬁciency concern. however method still achieves least times compression improvement method. besides signiﬁcant advantage al.’s models compressed models also undoubtedly much faster theirs. without data augmentation obtain reference model well-learned parameters iterations training perform network surgery alexnet consists learnable layers considered deep. prune convolutional layers fully connected layers separately previously discussed section training batch size base learning rate learning policy still keep reference training process. iterations convolutional layers iterations fully connected layers means iterations total test phase center crop test compressed model validation set. table compares result method others. four compared models built applying al.’s method adaptive fastfood transform method compared \"lossless\" methods method achieves best result terms compression rate. besides acceptable number epochs prediction error rate model comparable even better models compressed better references. order make detailed comparisons compare percentage remaining parameters compressed model al.’s since achieve second best compression rate. shown table method compresses parameters almost every single layer alexnet means storage requirement number flops better reduced compared besides learning process also much efﬁcient thus considerable less epochs needed paper investigated compressing dnns proposed novel method called dynamic network surgery. unlike previous methods conduct pruning retraining alternately method incorporates connection splicing surgery implements whole process dynamic way. utilizing method parameters models deleted prediction accuracy decrease. experimental results show method compresses number parameters lenet- alexnet factor respectively superior recent pruning method considerable margins. besides learning efﬁciency method also better thus less epochs needed.", "year": 2016}