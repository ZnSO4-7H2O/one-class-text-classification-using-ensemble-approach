{"title": "Grounded Language Learning in a Simulated 3D World", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "We are increasingly surrounded by artificially intelligent technology that takes decisions and executes actions on our behalf. This creates a pressing need for general means to communicate with, instruct and guide artificial agents, with human language the most compelling means for such communication. To achieve this in a scalable fashion, agents must be able to relate language to the world and to actions; that is, their understanding of language must be grounded and embodied. However, learning grounded language is a notoriously challenging problem in artificial intelligence research. Here we present an agent that learns to interpret language in a simulated 3D environment where it is rewarded for the successful execution of written instructions. Trained via a combination of reinforcement and unsupervised learning, and beginning with minimal prior knowledge, the agent learns to relate linguistic symbols to emergent perceptual representations of its physical surroundings and to pertinent sequences of actions. The agent's comprehension of language extends beyond its prior experience, enabling it to apply familiar language to unfamiliar situations and to interpret entirely novel instructions. Moreover, the speed with which this agent learns new words increases as its semantic knowledge grows. This facility for generalising and bootstrapping semantic knowledge indicates the potential of the present approach for reconciling ambiguous natural language with the complexity of the physical world.", "text": "increasingly surrounded artiﬁcially intelligent technology takes decisions executes actions behalf. creates pressing need general means communicate with instruct guide artiﬁcial agents human language compelling means communication. achieve scalable fashion agents must able relate language world actions; understanding language must grounded embodied. however learning grounded language notoriously challenging problem artiﬁcial intelligence research. present agent learns interpret language simulated environment rewarded successful execution written instructions. trained combination reinforcement unsupervised learning beginning minimal prior knowledge agent learns relate linguistic symbols emergent perceptual representations physical surroundings pertinent sequences actions. agent’s comprehension language extends beyond prior experience enabling apply familiar language unfamiliar situations interpret entirely novel instructions. moreover speed agent learns words increases semantic knowledge grows. facility generalising bootstrapping semantic knowledge indicates potential present approach reconciling ambiguous natural language complexity physical world. endowing machines ability relate language physical world longstanding challenge development artiﬁcial intelligence. situated intelligent technology becomes ubiquitous development computational approaches understanding grounded language become critical human-ai interaction. beginning winograd early attempts ground language understanding physical world constrained reliance laborious hard-coding linguistic physical rules. modern devices voice control appear competent suﬀer limitation language understanding components mostly rule-based generalise scale beyond programmed domains. work presents novel paradigm simulating language learning understanding. approach diﬀers conventional computational language learning learning understanding take place respect continuous situated environment. simultaneously beyond rule-based approaches situated language understanding paradigm requires agents learn end-to-end grounding linguistic expressions context using language complete tasks given pixel-level visual input. initial experiments presented paper take place extended version deepmind environment agents tasked ﬁnding picking objects based textual description task. paradigm outlined gives rise large number possible learning tasks even simple setup object retrieval presents challenges conventional machine learning approaches. critically language learning contingent combination reinforcement unsupervised learning. combining techniques agents learn connect words phrases emergent representations visible surroundings embodied experience. show semantic knowledge acquired process generalises respect situations language. agents exhibit zeroshot comprehension novel instructions speed acquire words accelerates semantic knowledge grows. further employing curriculum training regime train single agent execute phrasal instructions pertaining multiple tasks requiring distinct action policies well lexical semantic object knowledge. learning semantic grounding without prior knowledge notoriously diﬃcult given limitless possible referents linguistic expression learner must discover correlations stream level inputs relate correlations actions linguistic expressions retain relationships memory. perhaps unsurprisingly systems attempt learn language grounding artiﬁcial agents respect environments simpler continuous noisy sensory experience encountered humans idea programming computers understand relate language simulated physical environment pioneered seminal work winograd shrdlu system programmed understand user generated language containing small number words predicates execute corresponding actions questions requesting information. initially impressive system required syntax semantics word hard coded priori thus unable learn concepts actions. rule-based approaches language understanding come considered brittle scale full complexities natural language. since early work research language grounding taken place across number disciplines primarily robotics computer vision computational linguistics. research natural language processing computer vision pointed importance cross modal approaches grounded concept learning. instance shown learnt concept representation spaces faithfully reﬂect human semantic semantic parsing pursued ﬁeld natural language processing predominantly focussed building compositional mapping natural language formal semantic representations grounded database knowledge graph focus direction work compositional mapping abstract modalities natural language logical form grounding usually discrete high level. contrast work presented paper focus learning ground language level perception actions. siskind represents early attempt ground language perception seeking link objects events stick-ﬁgure animations language. broadly seen precursor recent work mapping language actions video similar modalities similar vein work pentland applies machine learning aspects grounded language learning connecting speech text input images videos even robotic controllers. systems consisted modular pipelines machine learning used optimise individual components complementing hard-coded representations input data. within robotics interest using language facilitate human-robot communication part necessary devise mechanisms grounding perceptible environment language general amount actual learning prior works heavily constrained either extensive hand-written grammars mechanisms support grounding simpliﬁcation terms setup environment. related work focuses language grounding perspective humanmachine communication diﬀerence approaches work language grounded highly structured environments opposed continuous perceptible input learning environment provides. ﬁeld computer vision image classiﬁcation interpreted aligning visual data semantic lexical concepts. moreover neural networks eﬀectively image video representations classiﬁcation networks human-written image captions. mappings also yield plausible descriptions visual scenes observed training however unlike approach captioning models typically learn visual linguistic processing representation ﬁxed datasets part separate independent optimisations. moreover model grounding linguistic symbols actions visual stimuli constantly change based exploration policy agent. idea reinforcement-style learning could play role language learning considered decades recently however agents controlled deep neural nets trained solve tasks environments. language learning agents build approaches algorithms agent architecture auxiliary unsupervised objectives speciﬁc multi-modal learning task. recently-proposed frameworks interactive language learning involve unimodal settings conduct language learning experiments integrated language channel simulated world environment agent perceives surroundings constant stream continuous visual input textual instruction. perceives world actively controlling sees movement visual ﬁeld exploration surroundings. specify general conﬁguration layouts possible objects environment together form language instructions describe agent obtain rewards. high-level conﬁguration simulations customisable precise world experienced agent chosen random billions possibilities corresponding diﬀerent instantiations objects colours surface patterns relative positions overall layout world. illustrate setup consider simple environment comprising connected rooms containing objects. train agent understand simple referring expressions environment could conﬁgured issue instruction form pick episode. training agent experiences multiple episodes shape colour pattern objects diﬀering accordance instruction. thus instruction pick pink striped ladder environment might contain random positions pink striped ladder entirely pink ladder pink striped chair blue striped hairbrush important emphasise complexity learning challenge faced agent even simple reference task this. obtain positive rewards across multiple training episodes agent must learn eﬃciently explore environment inspect candidate objects simultaneously learning meanings multi-word expressions pertain visual features diﬀerent objects also construct complex tasks pertaining characteristics human language understanding generalisation linguistic predicates novel objects productive composition words short phrases interpret unfamiliar instructions grounding language relations actions well concrete objects. agent consists four inter-connected modules optimised single neural network. time step visual input encoded convolutional vision module recurrent language module encodes instruction string mixing module determines signals combined passed two-layer lstm action module hidden state upper lstm policy function computes probability distribution figure example agent begins position immediately receives instruction pick object next green object. explores two-room layout viewing objects relative positions retrieving object best conforms instruction. exploration selection behaviour emerges entirely reward-driven learning preprogrammed. training task this billions possible episodes agent experience containing diﬀerent objects diﬀerent positions across diﬀerent room layouts. agent’s primary objective policy maximizes expected dist= λtrt]. apply advantage actor critic algorithm optimize policy softmax multinomial distribution parametrized agent’s network—towards higher discounted returns. parameters updated according rmsprop update rule share single parameter vector across asynchronous threads. conﬁguration oﬀers suitable trade-oﬀ increased speed loss accuracy asynchronous updates importantly early simulation results revealed initial design learn solve even comparably simple tasks setup. described thus agent learn comparatively infrequent object selection rewards without exploiting stream potentially useful perceptual feedback available time step exploring environment. address endowing agent ways learn unsupervised manner immediate surroundings means auto-regressive objectives applied concurrently reward-based learning involve predicting modelling aspects agent’s surroundings temporal autoencoding temporal autoencoder auxiliary task designed illicit intuitions agent perceptible world change consequence actions. objective predict visual environment conditioned prior visual input action implementation reuses standard visual module combines representation embedded representation combined representation passed deconvolutional network predict vt+. well providing means ﬁne-tune visual system auxiliary task results additional training action-policy network since action representations shared policy network language prediction strengthen ability agent reconcile visual linguistic modalities design word prediction objective estimates instruction words given visual observation using model parameters shared network also serve make behaviour trained agents interpretable agent emits words considers best describe currently observing. auxiliary networks optimised mini-batch gradient descent based mean-squared error negative-log-likelihood respectively. also experimented reward prediction value replay additional auxiliary tasks stabilise reinforcement based training evaluating agent constructed tasks designed test capacity cope various challenges inherent language learning understanding. ﬁrst test ability eﬃciently acquire varied vocabulary words pertaining physically observable aspects environment. examine whether agent combine lexical knowledge interpret familiar unfamiliar word combinations analysis includes phrases whose meaning dependent word order cases agent must induce re-use lexical knowledge directly phrases. finally test agent’s ability learn less concrete aspects language including instructions referring relational concepts phrases referring actions behaviours. ﬁrst experiment explored eﬀect auxiliary objectives ability agent acquire vocabulary diﬀerent concrete words training consisted multiple episodes single room containing objects. episode time agent spawned position equidistant objects received single-word instruction unambiguously referred objects. picked incorrect object. episode began immediately object selected agent selected either object steps. objects instructions sampled random full factors available simulation environment. trained replicas agent conﬁguration ﬁxed hyperparameters figure unsupervised learning auxiliary prediction objectives facilitates word learning. learning curves vocabulary acquisition task. agent situated single room faced objects must select object correctly matches textual instruction. total diﬀerent words used instructions training referring either shape colours relative size relative shade surface pattern target object. reward prediction value replay language prediction temporal autoencoder. data show mean conﬁdence bands across best hyperparameter settings sampled random ranges speciﬁed appendix. training episodes counts individual levels seen training. shown figure relying reinforcement learning alone agent exhibited learning even millions training episodes. fastest learning exhibited agent applying temporal auto-encoding language prediction conjunction value replay reward prediction. results demonstrate auto-regressive objectives extract information critical language learning perceptible environment even explicit reinforcement available. figure word learning much faster words already known rate agents learned vocabulary shape words measured agents three conditions. condition agent prior knowledge shapes names outside training data used here. second condition agent prior knowledge shape words outside target vocabulary third condition agent trained scratch. agents used auxiliary objectives. data show mean conﬁdence bands across best hyperparameter settings condition sampled random ranges speciﬁed appendix exhibit lexical knowledge agent must learn various skills capacities independent speciﬁcs particular language instruction. include awareness objects distinct ﬂoors walls; capacity sense ways objects diﬀer; ability look move direction. addition agent must infer solving solution tasks always contingent visual linguistic input without prior programming explicit teaching importance inter-modal interaction. given complexity learning challenge perhaps unsurprising agent requires thousands training episodes evidence word learning emerges. establish importance ‘pre-linguistic’ learning compared speed vocabulary acquisition agents diﬀerent degrees prior knowledge. training consisted instructions twenty shape terms banana cherries ﬂower fork fridge hammer knife pincer plant saxophone shoe spoon tennis-racket tomato tree wine-glass zebra. agent prior knowledge trained advance remaining twenty shapes full environment. agent minimal prior knowledge trained terms ball regimes advanced training stopped agent reached average reward across episodes. agent prior knowledge began learning directly training set. comparison presented figure demonstrates much initial learning agent trained scratch involves acquiring visual motor rather expressly linguistic capabilities. agent already knowing words learned words notably faster rate agent trained scratch. moreover speed word learning appeared accelerate words learned. shows acquisition words supported general-purpose motor-skills perception also existing lexical semantic knowledge. words agent able bootstrap existing semantic knowledge enable acquisition semantic knowledge. important facets natural language understanding ability compose meanings known words interpret otherwise unfamiliar phrases ability generalise linguistic knowledge learned setting make sense situations. examine capacities agent trained settings experience constrained training simultaneously learned training tested performance agent situations outside colour-shape composition experiment training instructions either unigrams bigrams. possible unigrams shape colour terms listed appendix possible bigrams colour-shape combination except containing shapes lolly ladder pencil suitcase colours magenta grey purple test instructions consisted possible bigrams excluded training set. training episode target object rendered match instruction confounding object correspond bigrams test set. similarly test episode target object confounding object corresponded bigrams test instructions. constraints ensured agent could interpret test instructions excluding objects terms seen training set. colour-shape decomposition composition experiment similar design colour-shape composition experiment. test tasks identical possible training instructions consisted bigram instructions colour-shape composition training set. achieve chance performance test agent must therefore isolate aspects world correspond constituent words bigram instructions build interpretation novel bigrams using constituent concepts. relative size relative shade experiments designed test genersmaller larger ality agents’ representation relational concepts according instruction. training episodes involved target confounding objects whose shape either ball balloon cake cassette chair guitar hairbrush hat. test episodes involved objects whose shape either lolly ladder pencil toothbrush. relative shade experiment followed design agent presented objects possibly diﬀering shape diﬀered shade colouring training colours green blue cyan yellow pink brown orange. test colours magenta grey purple. trained colour shape unigrams together limited number colourshape bigrams agent naturally understood additional colour-shape bigrams familiar constituent words. moreover ability productively compose known words interpret novel phrases contingent explicit training words isolation. exposed bigram phrases training agent inferred constituent lexical concepts reapplied concepts novel combinations test time. indeed condition agent learned generalise fewer training instances apparently simpler composition case. explained fact episodes involving bigram instructions convey greater information content latter condition avails agent information training episode. critically agent’s ability decompose phrases constituent lexical concepts reﬂects ability essential humanlike language learning naturalistic environments since linguistic stimuli rarely contain words isolation. another requirement linguistic generalisation ability extend category terms beyond speciﬁc exemplars concepts learned capacity also observed agent; trained relational concepts larger smaller context particular shapes colour-shape bigrams blue ladder. agents periodically tested remaining bigrams without updating parameters. decomposition-composition regime without training unigram descriptors. lighter darker agents trained interpret terms lighter darker applied colours tested terms context diﬀerent colours. relative size agents trained interpret terms larger smaller applied shapes tested terms context diﬀerent shapes. data show mean across best randomly sampled hyperparameter settings condition. appendix hyperparameter ranges exact train/test stimuli. naturally applied novel shapes almost perfect accuracy. contrast ability generalise lighter darker unfamiliar colours signiﬁcantly chance less perfect. particularly diﬃcult infer mapping corresponding lighter darker colour space small number examples observed training. taken together instances generalisation demonstrate agent simply ground language hard coded features environment pixel activations speciﬁc action sequences rather learns ground meaning abstract semantic representations. practically results also suggest artiﬁcial agents necessarily exposed ﬁnite training regimes ultimately come exhibit productivity characteristic human language understanding. consequence agent’s facility re-using acquired knowledge learning potential train agent complex language tasks exposure curriculum levels. figure shows example successful application curriculum applied task selecting object based ﬂoor colour room located also applied curriculum train agent range multi-word referring instructions form pick represents string consisting either single noun adjective noun adjectives noun latter cases also possible generic term ‘object’ place shape term. case training episode involved object coincided instruction number distractors not. learning curves ‘referring expression agent’ illustrated figure language typically used refer actions behaviours much objects entities. test ability agents ground words corresponding procedures trained single agent follow instructions pertaining three dissociable tasks. constructed tasks using two-room world ﬂoor colourings object properties sampled random. environment selection task involved instructions form pick object pick denotes colour term. next task involved instructions form pick object next object refer objects. finally room task involved instructions form pick room referred colour ﬂoor target room. next room task employed large degrees ambiguity i.e. given next level contain several objects constellation would located next figure curriculum learning necessary solving complex tasks. agent learn retrieve object particular room instructed four-lesson training curriculum required. lesson involved complex layout wider selection objects words solved agent successfully solved previous lesson. schematic layout vocabulary scope lesson shown training curves lesson. initial position agent varies randomly training among locations marked position four possible objects among positions marked white diamond. data show mean across best randomly sampled hyperparameter settings condition. figure learning curve referring expression agent. trained agent able select correct object two-object setup described using compositional expression. ability transfers complex environments larger number confounding objects. figure multi-task learning eﬃcient curriculum steps. single agent learn solve number diﬀerent tasks following two-lesson training curriculum. diﬀerent tasks cannot distinguished based visual information alone require agent language input identify task question. previously curriculum required achieve best possible agent performance tasks trained scratch agent learned solve three types task single room colour ﬂoor used proxy diﬀerent room. however unable achieve learning larger layout distinct rooms separated corridor. agent trained single room transferred larger environment continued learning eventually able solve diﬃcult task. learning tasks agent demonstrates ability ground language referring single objects also sequences actions plans inter-entity relationships. moreover mastering next room tasks agent exhibits sensitivity critical facet many natural languages namely dependence utterance meaning word order. ability solve complex tasks curriculum training emphasises generality emergent semantic representations acquired agent allowing transfer learning scenario related complex environment. artiﬁcial agent capable relating natural languages physical world would transform everyday interactions humans technology. taken important step towards goal describing agent learns execute large number multi-word instructions simulated three-dimensional world pre-programming hard-coded knowledge. agent learns simple language making predictions world language occurs discovering combinations words perceptual cues action decisions result positive outcomes. knowledge distributed across language vision policy networks pertains modiﬁers relational concepts actions well concrete objects. semantic representations enable agent productively interpret novel word combinations apply known relations modiﬁers unfamiliar objects re-use knowledge pertinent concepts already process acquiring concepts. simulations focus language outcomes relevant machine learning general sense. particular agent exhibits active multi-modal concept induction ability transfer learning apply knowledge representations unfamiliar settings facility learning multiple distinct tasks eﬀective synthesis unsupervised reinforcement learning. time learning agent reﬂects various eﬀects characteristic human development rapidly accelerating rates vocabulary growth ability learn rewarded interactions predictions world natural tendency generalise re-use semantic knowledge improved outcomes learning moderated curricula taken together contributions open many avenues future investigations language learning learning generally humans artiﬁcial agents.", "year": 2017}