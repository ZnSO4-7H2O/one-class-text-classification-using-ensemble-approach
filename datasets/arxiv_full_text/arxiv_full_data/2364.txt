{"title": "Data Analysis with Bayesian Networks: A Bootstrap Approach", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In recent years there has been significant progress in algorithms and methods for inducing Bayesian networks from data. However, in complex data analysis problems, we need to go beyond being satisfied with inducing networks with high scores. We need to provide confidence measures on features of these networks: Is the existence of an edge between two nodes warranted? Is the Markov blanket of a given node robust? Can we say something about the ordering of the variables? We should be able to address these questions, even when the amount of data is not enough to induce a high scoring network. In this paper we propose Efron's Bootstrap as a computationally efficient approach for answering these questions. In addition, we propose to use these confidence measures to induce better structures from the data, and to detect the presence of latent variables.", "text": "recent years significant progress bayesian networks data. however com­ plex data analysis need yond satisfied high scores. need provide confidence mea­ sures features exis­ tence edge nodes warranted? markov blanket given node robust? something variables? questions enough induce high scoring network. paper propose efron's bootstrap tationally questions. confidence measures induce better structures data detect presence latent variables. la�t decade great deal research focused learning bayesian networks data concentrated exceptions computationally methods efficient missing data. cently issue hidden variables main concern line work induction high scoring networks flects well network fits data. bayesian network however also contains information ploit information even situations translated breakthroughs mea�ure expression levels thousands experiment data generated experiments data�ets thousands able today contain hundreds instances. can­ expect learn detailed set. however data sets clearly mation. example would like induce correlation causation levels gene \"cause\" suppression challenge data \"noise\" genuine corre­ lations causations correlations. examine determine fidence various structural features networks induce data sets. consider proach methodology efron addressing strap computer-ba�ed accuracy statistics inference. establishing confidence interpreted portant given feature actually ultimately second notion akin a�sessment support particular ture. latter idea nicely separates data shortcomings interpretation methods introduced confidence closed fonn expressions edge appears network). bootstrap general conditions consistent defined simple expression application bootstrap mea�ure confidence inferences felsenstein uncertainty specify phylogenetic bayesian networks known network explicit model report prelimi­ nary results high confidence estimates existence experiments feature interest. features ways first includes features duced models markov neighborhood pdags present method ba�ed literature. log-loss given network. monitors check deviation predictions data. heckerman present approach bayesian considerations edge part underlying confidence estimation similar spirit investigated basis approach algorithmic implementation ther explored propose bootstrap bayesian estimate completeness graph encodes joint probability random variables fonnally bayesian work pair first component namely directed respond random variables edges represent direct dependencies graph encodes following statements descendants given dence features features bayesian network imply exactly ments. characterization network lence classes studied results papers establish nectivity variables rection arcs. resull� alence partially denotes undirected members cla�s contain contain structure equivalent receive score. work structures convert bootstrap network structure corresponds precedes general network letters probability network ture among possible data�ets size sampled induction procedure consistent converge expect grows larger give confidence holds close general conditions bootstrap converges ically. isfied consistency parametric sistency. however requires bootstrap parameters). designed verify convergence bootstrap features tested currently ough theoretical text bayesian network induction. next section test extend bootstrap timates expressing belong generating test bootstrap erated features tures generating bootstrap confidence node belonging blanket node high document repre­ denoting sented instance variable boolean variables newsgroup stop words) frequent words itives\" restarts search. search terminated negatives\" fixed number restarts. \"true negatives\" report pre­ numbers first three categories figures diction three type features alarm gene text domains respectively. ported numbers averaged estimates non-parametric thresholding alarm domain setting itives rea�onable hand text domain setting positives predictions false positives. lower threshold stage source phenomena. example prediction variables ilarly ordering dicted ba�ed bootstrap la�t observation range\" orderings direction. reliably indicates cestors others although relation different feature less sensitive variables. might ea�ily estimated performed simple test samples learned bayesian networks in-degree one. networks ea�y learn take variables. account pairwise shows tradeoff curves non-parametric strap using networks trees. tree­ ba�ed estimates false negatives) pect partially source network domain. common idea learning prior knowledge. particular knowledge structures reduce search space thus improve speed mducuon importantly network. commonly used prior information constraints certain arcs. section explore following bootstrap samples collected types precedes constraints. first estimate require learned confidence higher network respect order. disallow learn­ networks ancestor confidence markov neighborhood smaller disallow intuition procedure learn network original restrict given constraints. different networks induced procedure. port error generating terms log-likelihood a�signed networks. approach finding record struc­ tures examined ones. structures sensitive greedy hill-climbing collect candidates greedy hill-climbing problem since risk getting estimates con­ fidence ba�ed bia�ed sample structures. avoiding problem extensive mcmc simulation representative approach non-parametric domain. comparison similar agree high confidence features. since high confidence features thus bayesian reweight­ bootstrap networks would still assign ma�s. however examine lower thresholds ferences visible paper proposes methodology dence features bootstrap studied bootstrap particular examine important assesses model. experiments generating sions first bootstrap estimates worthy; high confidence estimations positives. neighborhood ables robust features edges pdag. third conclusions established ca�es data sets small model induced. results adaptive data analysis qualitative domain. finally reliable signaling ering variables great interest. markov blanket variables clique variables definitely markov blanket would indicative given reliability results approach. work done friedman abra­ wyner university berke­ ley. thank group berkeley mosix group hebrew university computational moises goldszmidt supported edge ba�es program contract", "year": 2013}