{"title": "DPPred: An Effective Prediction Framework with Concise Discriminative  Patterns", "tag": ["cs.LG", "cs.AI"], "abstract": "In the literature, two series of models have been proposed to address prediction problems including classification and regression. Simple models, such as generalized linear models, have ordinary performance but strong interpretability on a set of simple features. The other series, including tree-based models, organize numerical, categorical and high dimensional features into a comprehensive structure with rich interpretable information in the data.  In this paper, we propose a novel Discriminative Pattern-based Prediction framework (DPPred) to accomplish the prediction tasks by taking their advantages of both effectiveness and interpretability. Specifically, DPPred adopts the concise discriminative patterns that are on the prefix paths from the root to leaf nodes in the tree-based models. DPPred selects a limited number of the useful discriminative patterns by searching for the most effective pattern combination to fit generalized linear models. Extensive experiments show that in many scenarios, DPPred provides competitive accuracy with the state-of-the-art as well as the valuable interpretability for developers and experts. In particular, taking a clinical application dataset as a case study, our DPPred outperforms the baselines by using only 40 concise discriminative patterns out of a potentially exponentially large set of patterns.", "text": "abstract—in literature series models proposed address prediction problems including classiﬁcation regression. simple models generalized linear models ordinary performance strong interpretability simple features. series including tree-based models organize numerical categorical high dimensional features comprehensive structure rich interpretable information data. paper propose novel discriminative pattern-based prediction framework accomplish prediction tasks taking advantages effectiveness interpretability. speciﬁcally dppred adopts concise discriminative patterns preﬁx paths root leaf nodes tree-based models. dppred selects limited number useful discriminative patterns searching effective pattern combination generalized linear models. extensive experiments show many scenarios dppred provides competitive accuracy state-of-the-art well valuable interpretability developers experts. particular taking clinical application dataset case study dppred outperforms baselines using concise discriminative patterns potentially exponentially large patterns. accuracy interpretability desired goals predictive modeling including classiﬁcation regression. previous work characterized lines. line ordinary performance strong interpretability simple features meets serious bottleneck modeling complex high-order interactions features linear regression logistic regression support vector machine line consists models often studied high accuracy example tree-based models including random forest gradient boosted trees well neural network models model nonlinear relationships high-order combinations different features. however lower interpretability high complexity prevent practitioners deploying practice real-world scientiﬁc medical applications require intuitive understanding features high accuracies practitioners satisﬁed neither line models thus important challenging develop effective prediction framework high interpretability data used preparation article obtained pooled resource open-access clinical trials database. such following organizations individuals within pro-act consortium contributed design implementation pro-act database and/or provided data participate analysis data writing report neurological clinical research institute mgh; northeast consortium; novartis; prizelife; regeneron pharmaceuticals inc.; sanoﬁ; teva pharmaceutical industries ltd. shang jiang tong xiao peng department computer science university illinois urbanachampaign usa. e-mail {shang mjiang wtong jxiao jianpeng hanj}illinois.edu dealing high-order interactions features. many pattern-based models proposed last decade construct high-order patterns large features including association rule-based methods categorical data frequent pattern-based algorithms text data graph data recently novel series models discriminative pattern-based models demonstrated advantages traditional models. prune non-discriminative patterns whole frequent patterns however number discriminative patterns used classiﬁcation regression models still huge select concise discriminative patterns better interpretability still open issue. address challenges paper propose novel discriminative patterns-based learning framework extracts concise discriminative patterns high-order interactions among features accurate classiﬁcation regression. dppred ﬁrst train tree-based models generate large high-order patterns. second explore preﬁx paths root nodes leaf nodes tree-based models discriminative patterns. third compress number discriminative patterns selecting effective pattern combinations generalized linear model high classiﬁcation accuracy small regression error. component fast effective pattern extraction enables strong predictability interpretability dppred. actually worth study also ﬁgure observe patient cluster generates different diagnosis patterns. dppred accurately predicts prognosis systematically identiﬁes clinically-relevant features patient stratiﬁcation interpretable manner. distinct diagnosis patterns signiﬁcantly beneﬁt treatment precision medicine. effectiveness. experimental results several realworld datasets demonstrate dppred comparable even better performances state-ofthe-art models standard tasks classiﬁcation regression. clinical pattern discovery. dppred successfully applied discover patient clusters crucial clinical signals amyotrophic lateral sclerosis disease. remaining paper organized follows. section survey related work. section provide problem deﬁnition preliminary study. section presents proposed dppred framework details algorithms. section reports empirical results synthetic real-world datasets. section shows discovery prognosis analytic patients. section concludes study. related work section review existing methods related dppred including pattern-based classiﬁcation models tree-based models pattern selection approaches. pattern-based classiﬁcation philosophy frequent pattern mining widely adopted study problem pattern-based classiﬁcation. proposed classiﬁcation method cmar based multiple class-association rules extended cpar based predictive association rules besides association rules direct discriminative pattern mining proposed generate effective performance however approaches several serious issues. first huge number frequent patterns leads expensive computational cost pattern generation selection. second number selected patterns still large thousands limits interpretability causes inefﬁciency classiﬁcation model. third models capable address regression tasks. moreover discretization continuous variables depends much parameter tuning generate robust performances. fig. important factors disease found dppred. among important clinical variables dppred discovered dataset prizelife challenge highlighted ones later experimentally veriﬁed extremely high correlations disease columns patient clusters. iments demonstrate dppred achieves comparable even better performance competing traditional tree-based models. besides effectiveness want highlight dppred framework applicable real-world tasks model storage computational cost highly restricted. discovering robust patterns prizelife challenge. apply dppred analyze prognosis perform stratiﬁcation amyotrophic lateral sclerosis patients public dataset dream-phil bowen prediction prizelife challenge dppred makes following achievements. robust discriminative patterns found dppred well interpretable methods including winner cannot interpret performances. note dppred selects concise discriminative patterns involving clinical variables exponentially large models used many times variables. show figure dppred discovers important clinical factors blood urea nitrogen respiratory rate. factors found teams challenge indirect experimental logical evidence recently dong proposed utilize patterns different angle data partitioned based patterns complex models trained independently different partitions although type pattern aided models sheds lights different usage patterns model still lacks interpretablity. tree-based models tree-based models popular classiﬁcation tasks. decision tree boosted tree models explainable quite sensitive training data. traditional ensemble methods using multiple trees random forest gradient boosting decision trees alleviate over-ﬁtting issue. showed global reﬁnement could provide better performance growth pruning processes different trees independent however increased model size multi-tree based models sacriﬁces interpretability. proposed dppred different category models. post-pruning techniques multi-tree based models induce feature spaces. typically encoded tree index list instance binary vector indexed trees vens transferred binary vectors inner product kernel space using support vector machine showed increase classiﬁcation accuracy furthermore pairwise interactions also studied two-layer-tree model accurate classiﬁcation regression though number features reduced pruning dimension newly-created feature space still high large number constructed trees. example many efforts pruning model size pruned random forest still megabytes thus prediction slow support real-time applications. experimental results later show dppred delivers comparable results using discriminative patterns substantially reduced even compared state-ofthe-art models. pattern selection simply selecting patterns highest independent heuristics information gain gini index limited simple tasks redundancy over-ﬁtting problems given labels i.e. types classiﬁcation real numbers regression lasso widely used feature selection tasks well forward selection relatively large number candidate discriminative patterns backward selection suitable problem setting. proposed dppred framework adopts lasso forward selection methods select discriminative patterns. performances compared discussed experimental section. problem formulation prediction task data examples d-dimensional feature space together labels worth noting values example either continuous discrete categorical features transformed several binary dummy indicators assume without loss generality. label either class indicator real number depending speciﬁc task. previous patternbased models e.g. ddpmine patterns extracted categorical values thus able handle continuous variables careful manual discretization tricky often requires prior knowledge data. goal proposed framework dppred learn concise model consists small discriminative patterns training data learns predicts examples accurately possible i.e. predict correct class indicator classiﬁcation tasks predict close true number regression tasks. formally given dataset dppred returns discriminative patterns using generalized linear general loss function mapping function maps original feature vector pattern space using patterns dppred generates pool discriminative patterns within reasonable size selects top-k patterns based learning performance training data using generalized linear learning model. since number selected patterns limited patterns able provide informative interpretability reasonable predictive power. addition coming testing data evaluating small selected discriminative patterns dppred enabled make predictions generalized linear model efﬁciently. deﬁnition first deﬁne series concepts derive discriminative patterns. traditional frequent pattern mining works categorical data itemset data discretization required deal continuous variables. instead roughly discretizing numerical values adopt thresholding boolean function dppred. deﬁnition condition thresholding boolean function speciﬁc feature dimension. condition form indicates speciﬁc dimension threshold value. relational operator condition either dimension features corresponding binary indicators restrict note threshold values dppred speciﬁed users beforehand. previous pattern-based models e.g. ddpmine practitioners discretize values continuous variables prior pattern mining. dppred automatically determines values fig. overview dppred framework. training data multi-tree based model trained discriminative pattern generation. tree preﬁx paths root non-leaf nodes treated discriminative patterns. large pool discriminative patterns generated dppred conducts top-k pattern selection identify informative interpretable patterns. typically small finally trains generalized linear model based pattern space representation. tree model completely based training data without human interventions. example suppose possible condition another example could deﬁne pattern conditions. formally conjunctions concatenate different conditions consistent preﬁx path decision tree represents conjunction conditions nodes along path. deﬁne discriminative patterns follows. deﬁnition discriminative patterns refer patterns strong signals learning tasks given labels data. example pattern high information gain classiﬁcation training data pattern small mean square error regression training data discriminative pattern. example suppose labels generdiscriminative patterns overlapped predictive effects. speciﬁcally discriminative patterns special cases patterns. example previous example patterns indicate positive label. however second pattern encodes subset data points ﬁrst pattern encodes thus provide extra information learning process. common phenomenon shows roughly taking discriminative patterns based independent heuristics wastes budget number patterns linear combination patterns synergistic. therefore dppred selects top-k patterns predictive performance make selected patterns complementary compact. assume training testing data share distribution widely acknowledged classiﬁcation regression problems. case accuracy testing data approaching accuracy training data model able alleviate over-ﬁtting issue. overview dppred figure presents overview dppred framework. first learns constrained multi-tree based model training data. adopting every preﬁx path root tree non-leaf nodes discriminative pattern large pool discriminative patterns ready top-k discriminative pattern selection. different solutions forward selection lasso utilized select top-k discriminative patterns based performances using generalized linear model. solutions shown high accuracies experiments. corresponding linear model selected top-k discriminative patterns adopted make discriminative pattern generation ﬁrst component dppred framework generation high-quality discriminative patterns shown algorithm tree refer instances falling speciﬁc node decision tree. random decision tree introduces randomness bootstrapping training data randomly selecting features splitting values dividing large tree smaller ones. random decision trees generated tree preﬁx paths root non-leaf nodes treated discriminative patterns. predictivity decision trees so-generated patterns highly effective speciﬁc prediction task. note decision tree built different loss functions different tasks could entropy gain classiﬁcation tasks mean square error regression tasks. using discriminative patterns discovered tree models shown algorithm discriminative pattern corresponding binary dimension describing whether instances satisfy pattern not. dimension pattern space equal number discriminative patterns large number generation phase need select limited number patterns thus make pattern space small efﬁcient. also worth mention mapping process able fully parallelized speedup. real-world datasets discriminative patterns frequently emerging length patterns long. speciﬁcally assume number instances satisfying given discriminative pattern least length discriminative patterns returned patterns discriminative ensure prediction accuracy diverse ensure sufﬁcient condition coverage. famous multi-tree based models random forest best addressing requirements treat every preﬁx path root tree nonleaf node discriminative pattern. first distributions labels instances tree always entropy. therefore patterns discriminative training data. second provides many putative patterns various random decision trees trained different bootstrapped datasets. third depth threshold minimum tree size naturally added constraints growth trees. top-k pattern selection large pool discriminative patterns generated top-k selection needs done identify informative interpretable patterns. naive heuristic functions information gain gini index evaluate signiﬁcance different patterns prediction task choose ranked patterns. however effects ranked patterns based simple heuristic scores large portion overlaps thus combination work optimally. therefore achieve best performance complementary patterns propose effective solutions forward selection lasso make decisions based effects pattern combinations instead considering different patterns independently. forward pattern selection instead exhausted search possible combinations discriminative patterns forward selection gradually adds discriminative patterns newly added discriminative pattern best choice time provides efﬁcient approximation exhausted search. speciﬁc ﬁrst discriminative patterns ﬁxed algorithm empirically adds discriminative pattern patterns achieves best training performance generalized linear model shown algorithm mentioned before assuming training testing data distribution using training accuracy reasonable. lasso based pattern selection regularization designed make weight vector sparse tuning nonnegative parameter features non-zero weight require testing examples top-k discriminative patterns generalized linear model return predictions testing instances construct pattern space using algorithm prediction top-k discriminative patterns determined upcoming test instance dppred ﬁrst maps learned pattern space applies pre-trained generalized linear model compute prediction shown algorithm number patterns limited mapping pattern space prediction generalized linear model extremely fast. time complexity analysis build single random decision tree depth threshold minimum tree size assuming numbers random features random partitions small ﬁxed constants time complexity total number instances level tree therefore time complexity generating trees generation step. selection step complexity mainly determined number discriminative patterns induced random decision trees dependent total number non-leaf nodes. maximum depth single tree upper bound number leaf nodes starting tree size number leaf nodes since trees binary trees number leaf nodes number non-leaf nodes. therefore number discriminative patterns bounded min{d solve logistic regression lasso using gradient descent algorithm thus time complexity gradient step linear dimension features number examples. time complexity proportional forward selection used proportional lasso used. assuming numbers iterations converge similar lasso forward selection lasso little efﬁcient forward selection. predicting test instances easily ﬁgure bottleneck mapping instances learned pattern space. therefore batch mode examples considered together time complexity streaming mode instances come time complexity number discriminative patterns algorithm top-k pattern selection forward require training examples discriminative patterns return top-k discriminative patterns generalized linear model mapped binary feature representation pattern space i-th example; weight vector generalized linear model; general loss function logistic loss. ensure patterns non-zero weights pattern space carefully choose value assume exists hidden importance among features. weight feature non-zero given also non-zero smaller binary search algorithm shown algorithm lasso implementation glmnet adopted thesis whose loss function cross entropy. require training examples discriminative patterns small value return top-k discriminative patterns generalized linear model construct pattern space using algorithm experiments section conduct extensive experiments demonstrate interpretability efﬁciency effectiveness proposed dppred framework. ﬁrst introduce experimental settings discuss efﬁciency interpretability give results classiﬁcation regression tasks well parameter analysis. experimental settings subsection presents datasets baseline methods learning tasks experiments. datasets first generate synthetic datasets features demographics test results patients label whether patient disease order demonstrate interpretability dppred. assuming doctors diagnose disease using rules based information veriﬁed whether discriminative patterns selected dppred consistent actual diagnosing rules. several real world classiﬁcation regression datasets machine learning repository used experiments shown table statistics number instances number features. datasets adult hypo sick ratio standard train/test splitting therefore classiﬁcation regression datasets divide datasets train/test unbiased sampling preprocessing. classiﬁcation tasks compare ddpmine datasets including adult hypo sick sonar chess waveform mushroom. ddpmine dppred achieve almost perfect accuracy datasets waveform mushroom datasets omitted. addition performance dppred high-dimensional datasets also investigated since ddpmine performs poorly high-dimensional data. metric regression datasets choose general datasets bike crime well clinical datasets patterns likely present parkinsons. furthermore make errors different datasets comparable min-max normalization adopted scale continuous labels metric rooted mean square error testing data lower rmse means better performance. baseline methods ddpmine previous state-of-the-art discriminative pattern based algorithm. ﬁrst discretizes continuous variables frequent pattern mining algorithm could applied. using frequent discriminative patterns feature space constructed classical classiﬁers could utilized. ddpmine focuses classiﬁcation tasks applicable regression experiments. random forest another baseline method using parameters random forest used dppred except limit depth moreover interested limited-depth random forest model built topk generation step global patterns. tree-based methods capable classiﬁcation regression tasks. expected complex models slightly better performance dppred major contributions dppred concise interpretable patterns instead solely accuracy. make fair comparison decision tree similar number nodes dppred also listed baseline. classiﬁcation regression tasks dppred classiﬁcation tasks default parameter setting regression tasks continuous labels complex discrete class labels classiﬁcation natural incorporate patterns. therefore default setting show results using forward selection lasso select top-k discriminative patterns. deeply study impact parameters number selected discriminative patterns number trees random forest therefore parameters efﬁciency interpretability efﬁciency. test running time linearly proportional model complexity related number patterns model used. experiments ddpmine needs patterns dppred needs indicates signiﬁcant reduction prediction runtime. moreover random forest without constraints contain nodes expensive. although evaluation random forest single testing instance traverse number nodes equals depths different trees always needs traverses experiments. therefore dppred efﬁcient model testing instances compared ddpmine random forest achieving times speedup practice. furthermore dppred could fully parallelized speedup. empirical results presented table interpretability discovery interpretable patterns. generate small medical dataset binary classiﬁcation demonstrate interpretability. patient draw several uniformly sampled features follows apply dppred-f dppred-l dataset. give test accuracy discriminative patterns found dppred-f dppred-l listed below. observe found patterns quite close groundtruth rules. demonstrate selected discriminative patterns provide high-quality explanation effectiveness classiﬁcation ddpmine previous state-of-the-art pattern-based classiﬁcation method outperforms traditional classiﬁcation models including decision tree support vector machine compare dppred ddpmine datasets used ddpmine. results shown table dppred-f dppredl always higher accuracy ddpmine. important reason advantage candidate patterns generated tree-based models dppred much discriminative thus effective speciﬁc classiﬁcation task frequent less useful patterns extracted ddpmine. except sick dataset dppred-f highest accuracy dppred-l works best sick dataset. seems dppred-f works little better dppred-l. however results quite close better ddpmine datasets. surprisingly dppred demonstrates even better performance complex model random forest several datasets accuracies datasets still comparable effectiveness pattern selection module select optimal pattern combination instead selecting patterns independently. shows proposed model effective classiﬁcation tasks highly concise interpretable. effectiveness regression since ddpmine applicable regression tasks compare dppred lrf. note methods highly complicated thus preserve limited interpretability. rmse results average differences compared dppred shown table fig. impact top-k patterns classiﬁcation regression tasks. training testing performances almost overlapped datasets. observe small number patterns enough achieve stable performance. because different discrete class labels real valued prediction increases level difﬁculty. although raised number patterns little bag-of-patterns feature representations based small number patterns still limitations predict real value. example different examples constructed pattern space means different predicted values inﬁnite real numbers likely true value example. however worth noting dppred always achieves comparable performance work better similar still demonstrates effectiveness dppred extent model compact interpretable lrf. effectiveness high dimensions interested high-dimensional datasets ddpmine effective large dimensional data. compare ddpmine classiﬁcation datasets whose number dimensions least regression datasets used. dimension original feature space grows reasonable increase depth threshold well number trees involve higher order interactions increase number candidate discriminative patterns. therefore meanwhile dimension mapped pattern space also need increased higher complexity problems. result nomao musk datasets. however kept madelon dataset many features noises. shown table dppred always outperform ddpmine generate comparable results worth noting madelon dataset dppred-f dppred-l outperform signiﬁcantly. stated before madelon highly noisy. result many patterns generated random forest reliable poor test data although discriminative training data. hand dppred compresses patterns keeps discriminative ones thus alleviates problem extent. demonstrates robustness dppred especially features high dimensional noisy. also worth mention training process dppred least times faster ddpmine high dimensional datasets. number discriminative patterns interesting parameter dppred number discriminative patterns used ﬁnal generalized linear model. controls model size generalized linear model used prediction thus affects efﬁciency. default value classiﬁcation tasks regression tasks effectiveness proved previous experiments vary trends training testing accuracies different datasets. three representative classiﬁcation datasets three regression datasets used experiment. fig. impact number trees classiﬁcation tasks. training testing performances almost overlapped. observe small number trees enough achieve stable performance. illustrated figure performance test data always following trend performance training data performance increasing grows classiﬁcation regression tasks discrepancy training test performance signiﬁcant regression tasks reasonable higher complexity problem trends quite similar. addition argue larger difference could caused insufﬁcient size training data curves always overlap bike dataset much bigger two. also worth noting dppred-l performs consistently dppredf especially regression tasks result automatically learned dppred-l manually speciﬁed dppred-f. summary similar trends training test data justiﬁes pattern selection based training accuracy reasonable. real world applications could determined cross validations. although performance becoming better almost time slows much greater default value. true classiﬁcation regression tasks. even larger hurt efﬁciency training process online prediction might introduce overﬁtting issues prediction therefore conclude small enough comprehensive realworld datasets proves proposed dppred compress model tiny size accuracy remains comparable. number trees model another important parameter dppred number trees needed generate large pool discriminative patterns. mentioned before single tree enough generate many patterns thus strong motivation extreme case. default value works well previous experiments thus vary trends training testing accuracies. before three datasets classiﬁcation regression tasks presented experiments. mance much lower others means single decision tree enough diverse patterns pool. trees generally cannot guarantee high coverage effective patterns especially data large dimension high. increasing number trees leads better diversity candidate patterns. according curves easily observe conclude performance remains stable long number trees sufﬁciently large reasonably large enough achieve satisfying result. similar number patterns however many noisy patterns generated becomes large training data better fail characterize testing data harmful generalization model addition trees have larger number pattern candidates generated increase time complexity feature selection. default experiments performs consistently well different data sets. novel marker discovery patient stratification section apply dppred analyze prognosis perform stratiﬁcation patients. unlike diseases many cancers clearly classiﬁed subtypes distinct survival rates signiﬁcant signals identiﬁed explain diverse survival times patients. wide range makes difﬁcult predict disease progression survival suggests rather large underlying disease heterogeneity. exist different subgroups patients unique disease causes prognosis. dataset solve puzzle pooled resource open-access clinical trials platform created prizelife neurological clinical research institute massachusetts general hospital collect data existing completed clinical trials. challenge aimed improving prediction progression rate essentially regression task. participants built models training patients submitted models challenge organizers. organizers models separate leaderboard patients provided feedback model performance participants. several submission-and-feedback cycles months last submissions participants evaluated ranked organizers another separate validation patients. challenge attracted participants received unique algorithms submissionand-feedback leaderboard phase. among them algorithms demonstrated improved accuracy baseline ﬁnal validation data set. best prognosis model developed challenge uses bayesian trees predictive features constructed clinical variables profound success. predicted progression clinical data better clinicians potentially reduce cost future trials -million solution perfect though. uniform model patients thus lacks ability make personalized diagnosis. also hard clinically interpret solution high model complexity. fair comparison dppred trained evaluated mimics challenge. training performed training patients evaluation validation patients. leaderboard patients used merely feature calibration data used challenge consist parts clinical variables actual progression rate available clinical variables patient grouped kinds demographic information vital signs test results family disease history amyotrophic lateral sclerosis functional rating scale detailed description data found supplement variables excluded study units consistent patients. alsfrs quantitative clinical score ranging evaluating functional status patient. consists assessments motor functioning evaluated within range evaluated functions .speech .salivation .swallowing .handwriting .cutting food handling utensils .dressing hygiene .turning adjusting clothes .walking .climbing stairs .respiratory. rate change alsfrs respect time used quantitative measurement progression rate. task predict ∆alsfrs/∆t within months disease onset given clinical variables within ﬁrst months. rmse predicted ∆alsfrs/∆t actual value used evaluate predictive performance. data processing clinical variables patient contain data types static categorical static continuous longitudinal continuous variables. static variables timeindependent longitudinal variables measured multiple times patient likely change time. static categorical variable categories replaced binary features additional indicates whether variable missing. static continuous variable simply continuous feature. longitudinal continuous variable {xt} measured values times measurements ascending order converted continuous features taking statistics {xt} derivative sequence whose element deﬁned statistics xi)/n ﬁrst-measured value last-measured value maximum maxi{xi} minimum mini{xi} standard deviation another statistics taken similarly performing variable conversion separately training leaderboard validation sets features calibrated across sets features completely missing least data sets discarded. number features ﬁnally feed dppred converted clinical variables. task description precision medicine setting assume implicit groupings underlying patients subtypes certain disease. formally deﬁne patient cluster follows. deﬁnition diagnosis-stratiﬁed patient clusters disjoint patient groups patients within group similar different top-k patterns clinical variables across clusters suggests fig. overview prognosis analysis stratiﬁcation patients. starting training data dppred ﬁrst applied discover global patterns. second classical clustering algorithm utilized detect diagnosis-stratiﬁed patient clusters constructed pattern space. third local patterns explored dppred within certain patient cluster. combining global local patterns concise uniﬁed generalized linear model ready testing data. using patients training instances. global patterns expected capture general properties speciﬁc task also hopefully detect implicit groups patients. example suppose disease different subtypes expect global patterns handle general diagnosis others help clinicians partition patients subtypes. deﬁnition local patterns top-kl patterns using patients single patient clusteras training instances. within different patient clusters patients different root causes thus need different diagnoses treatments. therefore motivated discover local patterns. application task ﬁrst discover global patterns patients ﬁgure patient clusters well local patterns patient cluster. goal demonstrate dppred accurately predict prognosis also systematically identify clinically-relevant features patient stratiﬁcation interpretable manner facilitate personalized diagnosis therapy. fig. testing rmse dream-phil bowen prediction prizelife challenge. dppred obtained predictive performance comparable solution away rmse solution comparable top-ranked algorithms also complicated interpretable better baseline rmse. linear combination discriminative patterns trained dppred includes clinical variables forming small subset variables. predict generalized linear model. utilize dppred discover global local patterns. since regression task similar previous experiments therefore patient patterns. patient clustering making analogy bag-of-words bag-of-patterns adopt latent dirichlet allocation algorithm speciﬁcally observing global patterns patients order detect patient clusters design generative process patterns incorporating patient clusters latent variables. first assume patterns particular patient clusterfollow multinomial distribution random variable draws prior dirichlet distribution. inspired bag-of-words making analogies words documents patterns patients represent observed patterns patient patterns. therefore generative process treated process lda. results discussion dppred obtained predictive performance comparable solution gives interpretable discriminative patterns shown figure dppred patient clusters achieves rmse validation data away rmse solution comparable top-ranked algorithms also complicated interpretable better baseline rmse linear combination discriminative patterns trained dppred includes clinical variables total small subset available variables. frequent clinical variable list reveals importance blood urea nitrogen respiratory rate among important features reported teams organizers challenge. variables list agree well challenge ﬁndings. examples include critical role onset delta mouth-related alsfrs assessments vital capacity. high degree consistency challenge results proves reliability dppred newly reported important variables highlights power feature selection dppred shed light research. reasons take newly discovered important clinical variables seriously designing future studies. experimentally shown level elevated minocycline drug delay progression applied therefore correlation level progression rate likely true. respiratory rate reﬂects respiratory muscle functioning thus related .respiratory assessments alsfrs. since importance .respiratory reported several among teams challenge also dppred surprising respiratory rate also list. interestingly dppred algorithm among simultaneously selects respiratory rate .respiratory. another point worth mentioning distinct local patterns patient cluster displayed figure indicating different diagnosis patterns across patient clusters. example mouth-functioning-related scores important overall locally cluster blood pressure important patient clusters plays less signiﬁcant role cluster distinct diagnosis patterns personalized medicine also shed light mechanism underlying heterogeneity treatment als. demonstrate predictive performance stratiﬁcation also trained dppred model without clustering rmse worse. results indicate dppred accurately predicts prognosis also systematically identiﬁes clinically-relevant features patient stratiﬁcation interpretable manner facilitate personalized diagnosis therapy. paper propose effective concise discriminative pattern-based prediction framework address classiﬁcation regression problems provide high interpretability small number discriminative patterns. speciﬁcally dppred ﬁrst trains constrained multi-tree model using training data extracts preﬁx paths root nodes non-leaf nodes trees candidate discriminative patterns. size discriminative patterns compressed selecting effective pattern combinations according predictive performance generalized linear model. instead selecting patterns independently using heuristics dppred ﬁnds best combination using forward selection lasso avoids overlapping effect similar patterns. extensive experiments demonstrate dppred able model high-order interactions present small number interpretable patterns help human experts understand data. dppred provides comparable even better performance state-of-the-art model ddpmine random forest model classiﬁcation regression. dppred successfully applied discover patient clusters crucial clinical signals amyotrophic lateral sclerosis disease. cheng c.-w. hsu. discriminative frequent pattern analysis effective classiﬁcation. data engineering icde ieee international conference pages ieee cong k.-l. tung mining top-k covering rule groups gene expression data. proceedings sigmod international conference management data pages derksen keselman. backward forward stepwise automated subset selection algorithms frequency obtaining authentic noise variables. british journal mathematical statistical psychology zhang cheng verscheure. direct mining discriminative essential frequent patterns model-based search tree. proceedings sigkdd international conference knowledge discovery data mining pages ganjisaffar caruana lopes. bagging gradientboosted trees high precision variance ranking models. proceedings international sigir conference research development information retrieval pages gordon moore gelinas qualls meister werner mendoza mass kushner miller. placebo-controlled phase i/ii studies minocycline amyotrophic lateral sclerosis. neurology ¨uffner zach norel hawe schoenfeld wang fang mackey hardiman crowdsourced analysis clinical trial data predict amyotrophic lateral sclerosis progression. nature biotechnology leslie eskin noble. spectrum kernel string kernel protein classiﬁcation. paciﬁc symposium biocomputing volume pages world scientiﬁc pei. cmar accurate efﬁcient classiﬁcation based multiple class-association rules. data mining icdm proceedings ieee international conference pages ieee caruana gehrke hooker. accurate intelligible models pairwise interactions. proceedings sigkdd international conference knowledge discovery data mining pages moosmann triggs jurie. fast discriminative visual codebooks using randomized clustering forests. twentieth annual conference neural information processing systems pages press wang karypis. harmony efﬁciently mining best rules classiﬁcation. proceedings siam international conference data mining volume pages siam han. cpar classiﬁcation based predictive association rules. proceedings siam international conference data mining volume pages siam stavrovskaya drozda sarang hartley gullans minocycline inhibits cytochrome release delays progression amyotrophic lateral sclerosis mice. nature jingbo shang received b.e. degree department computer science engineering shanghai jiao tong university china. ph.d. candidate university illinois urbana-champaign supervised prof. jiawei han. main research interests include large-scale data mining data mining well constructing mining heterogeneous information network massive text corpora. meng jiang received b.e. degree ph.d. degree department computer science technology tsinghua university. postdoctoral research associate university illinois urbanachampaign. visited carnegie mellon university published papers data-driven behavioral analytics recommendation prediction suspicious behavior detection conferences journals relevant ﬁeld ieee tkde sigkdd aaai cikm ieee icdm. best paper ﬁnalist sigkdd wenzhu tong received b.e. degree department computer science technology wuhan university china m.s. degree computer science department university illinois urbana-champaign. jinfeng xiao received b.s. degree physics mathematics hong kong university science technology. graduate student computer science university illinois urbana-champaign. current research lies interface machine learning data mining biomedical science. jian peng assistant professor department computer science university illinois urbana-champaign. joining csillinois jian postdoc berger visiting scientist lindquist whitehead institute biomedical research. student obtained ph.d. computer science toyota technological institute chicago received b.s. computer science wuhan university. jian worked protein analysis drs. david heckerman jonathan carlson escience group microsoft research jiawei abel bliss professor department computer science university illinois. researching data mining information network analysis database systems publications. served founding editor-in-chief transactions knowledge discovery data jiawei received sigkdd innovation award ieee computer society technical achievement award ieee computer society wallace mcdowell award daniel drucker eminent faculty award uiuc fellow fellow ieee. currently director information network academic research center supported network science-collaborative technology alliance program u.s. army research lab. co-authored textbook data mining concepts techniques adopted worldwide.", "year": 2016}