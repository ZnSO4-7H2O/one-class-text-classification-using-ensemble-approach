{"title": "Towards A Deeper Geometric, Analytic and Algorithmic Understanding of  Margins", "tag": ["math.OC", "cs.AI", "cs.LG", "math.NA", "stat.ML"], "abstract": "Given a matrix $A$, a linear feasibility problem (of which linear classification is a special case) aims to find a solution to a primal problem $w: A^Tw > \\textbf{0}$ or a certificate for the dual problem which is a probability distribution $p: Ap = \\textbf{0}$. Inspired by the continued importance of \"large-margin classifiers\" in machine learning, this paper studies a condition measure of $A$ called its \\textit{margin} that determines the difficulty of both the above problems. To aid geometrical intuition, we first establish new characterizations of the margin in terms of relevant balls, cones and hulls. Our second contribution is analytical, where we present generalizations of Gordan's theorem, and variants of Hoffman's theorems, both using margins. We end by proving some new results on a classical iterative scheme, the Perceptron, whose convergence rates famously depends on the margin. Our results are relevant for a deeper understanding of margin-based learning and proving convergence rates of iterative schemes, apart from providing a unifying perspective on this vast topic.", "text": "given matrix linear feasibility problem aims solution primal problem certiﬁcate dual problem probability distribution inspired continued importance large-margin classiﬁers machine learning paper studies condition measure called margin determines diﬃculty problems. geometrical intuition ﬁrst establish characterizations margin terms relevant balls cones hulls. second contribution analytical present generalizations gordan’s theorem variants hoﬀman’s theorems using margins. proving results classical iterative scheme perceptron whose convergence rates famously depends margin. results relevant deeper understanding margin-based learning proving convergence rates iterative schemes apart providing unifying perspective vast topic. binary linear classiﬁcation given points labels {+−} classiﬁer said separate given points sign succinctly representing yixi shows problem speciﬁc instance deepen geometric algebraic algorithmic understanding problems tied together concept called margin. geometrically provide intuition ways interpret margin primal dual settings relating various balls cones hulls. analytically prove margin-based versions classical results convex analysis like gordan’s hoﬀman’s theorems. algorithmically give insights classical perceptron algorithm. begin gentle introduction concepts getting details. notation write vectors mean indices distinguish surfaces interiors balls obviously mathematical equations choose denote euclidean balls probability simplex denote linear subspace spanned convex hull conv. lastly deﬁne ball radius similarly deﬁned). deﬁnition margin introduced goﬃn gave several geometric interpretations. since extensively studied notion complexity conditioning problem instance. broadly larger magnitude better conditioned pair feasibility problems easier witnesses feasibility. ever since margin-based algorithms extremely popular growing literature machine learning relevant presently summarize. this separation theorems like farkas’ lemma widely applied algorithm design analysis. later prove generalizations gordan’s theorem using aﬃne-margins. g¨uler hoﬀman rothblum generalize bound norms left right hand sides inequality. later prove theorems similar ﬂavor almost magically turn aﬃne-margin. theorems used proving rates convergence algorithms constant explicitly terms familiar quantity useful. makes right quantity consider especially problem establish geometrical characterizations aﬃne-margin feasible well feasible connect well-known radius theorems. paper’s appetizer. tions gordan’s theorem deal alternatives involving aﬃne-margin either strictly feasible. building intuition further sec. prove several interesting variants hoﬀman’s theorem explicitly involve aﬃne-margin either strictly feasible. paper’s main course. never negative always pick unit vector perpendicular leading zero dot-product every since matter easily inseparable margin always zero rank deﬁnition capture diﬃculty verifying linear infeasibility. problem strictly feasible strictly infeasible respectively. distinction really matters still useful make explicit. think full rank performing would unnecessary dimensions. however often wish perform elementary operations much simpler eigenvector computations. instability compared unfortunately behaviour stable small perturbations conv full-dimensional. speciﬁc strictly feasible perturb vectors small amount vector maintains feasibility change small amount. however strictly infeasible perturb vectors small amount vector maintains infeasibility change large amount. large. vector form even tiny component orthogonal suddenly becomes zero. possible choose vector v⊥/v⊥ lin) makes zero dot-product positive despite instability lack continuity indeed negative aﬃne margin determines rate convergence algorithms particular convergence rate neumann–gilbert algorithm determined much convergence rate perceptron algorithm determined discuss issues detail section section positive margin many known geometric interpretations width feasibility cone also largest ball centered unit sphere inside dual cone example here provide interpretations. remember also closely related particular instance minimum enclosing ball problem. common knowledge connected margins possible explicitly characterize relationship done below. show section perceptron related algorithms introduce later yields sequence iterates converge center distance origin conv zero sequence iterates coverges origin ends unit ball. words conv. inequality suﬃces show prove contrapositive conv since conv closed convex conv exists hyperplane separating conv lin. exists constant particular deﬁnitions equivalent full-dimensional diﬀer full-dimensional especially relevant context inﬁnite dimensional reproducing kernel hilbert spaces could even occur rank. case always zero since full-dimensional ball cannot inside ﬁnite-dimensional highlights role margin measure conditioning linear feasibility systems indeed number far-reaching extensions classical radius theorem latter states euclidean distance square nonwould like make quantitative statements happens either alternatives satisﬁed easily preceding geometrical intuition suggests reﬁnement gordan’s theorem namely theorem below accounts margins. related results previously derived discussed terlaky well todd particular shown part theorem could obtained similarly parts could recovered give succinct simple proof theorem relying proposition proposition theorem could also proven albeit less succinctly separation arguments convex analysis. hoﬀman-style theorems often useful prove convergence rate iterative algorithms characterizing distance current iterate target set. example hoﬀmanlike theorem also proved prove linear convergence rate alternating direction method multipliers prove linear convergence ﬁrst order algorithm calculating \u0001-approximate equilibria zero games. worth pointing hoﬀman whose honor theorem named also author whose proof strategy follow alternate proof theorem appeared overlooked intimate connection hoﬀman constant positive negative margin present theorems below. also provide alternative proof theorem above since proving fact completely diﬀerent angles often yield insights. follow techniques though signiﬁcantly simplify perhaps classical proof style possibly amenable bounds involving margin hence instructive unfamiliar proving sorts bounds. variant called normalized perceptron which point theorem below subgradient method updates worst mistake tracks normalized convex combination ai’s. best known property unnormalized perceptron normalized perceptron algorithm strictly feasible margin ﬁnds solution iterations proved less obvious perceptron actually primal-dual nature stated following result terlaky following statement \u0001-certiﬁcate mean vector prove nontrivial facts normalized perceptron case found published literature case feasible. normalized perceptron produce feasible steps continuing algorithm approach optimal maximizes margin i.e. achieves margin actually true classical perceptron. normalization following theorem needed von-neumann described iterative algorithm solving dual private communication dantzig subsequently analyzed latter published goes name von-neumann’s algorithm optimization circles. independently gilbert described essentially identical algorithm goes name gilbert’s algorithm computational geometry literature. respect independent ﬁndings diﬀerent literatures refer von-neumann-gilbert algorithm. starts point conv loops dual algorithm conjectured von-neumann. though designed epelman freund proved feasible also produces feasible steps hence also primal-dual like perceptron readily follows theorem corollary corollary proposition hold well von-neumann-gilbert algorithm place normalized perceptron algorithm. note relaxed version also lagrangian duals seen eq.. light surprising algorithms similar properties. moreover bach recently pointed strong connection duality subgradient frank-wolfe methods. proof. figure illustrates idea proof. assume otherwise wtwt sets nearest point origin line joining consider nearest point origin line parallel note then hence converge linearly strict infeasibility cannot. nevertheless seen geometrically trying represent center circumscribing inscribing balls conv convex combination input points. paper advance unify understanding margins slew results connections ones. first point correctness using aﬃne margin deriving relation smallest ball enclosing conv largest ball within conv. proved generalizations gordan’s theorem whose statements conjectured using preceding geometrical intuition. using tools derived interesting variants hoﬀman’s theorems explicitly aﬃne margins. ended right seminal introductory paper hoﬀman-like theorems used prove convergence rates stability algorithms. theorems also proof strategies useful regard since hoﬀman-like theorems challenging conjecture prove example). similarly gordan’s theorem used wide array settings optimization giving precedent possible usefulness generalization. lastly large margin classiﬁcation integral machine learning topic seems fundamental unify understanding geometrical analytical algorithmic ideas behind margins.", "year": 2014}