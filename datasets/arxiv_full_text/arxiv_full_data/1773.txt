{"title": "Visualizing textual models with in-text and word-as-pixel highlighting", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "We explore two techniques which use color to make sense of statistical text models. One method uses in-text annotations to illustrate a model's view of particular tokens in particular documents. Another uses a high-level, \"words-as-pixels\" graphic to display an entire corpus. Together, these methods offer both zoomed-in and zoomed-out perspectives into a model's understanding of text. We show how these interconnected methods help diagnose a classifier's poor performance on Twitter slang, and make sense of a topic model on historical political texts.", "text": "figure topic model’s token-level posterior memberships shown in-text annotation word-as-pixel views corpus u.s. presidential state union speeches. speeches concatenated running columns; top-left bottom right demo http//slanglab.cs.umass.edu/topic-animator/ explore techniques color make sense statistical text models. method uses in-text annotations illustrate model’s view particular tokens particular documents. another uses high-level wordsas-pixels graphic display entire corpus. together methods offer zoomed-in zoomed-out perspectives model’s understanding text. show interconnected methods help diagnose classiﬁer’s poor performance twitter slang make sense topic model historical political texts. probabilistic models text core technology natural language processing. models typically link words phrases semantic categories like classes topics. analyze data models need understand method interprets text order perform exploratory conﬁrmatory data analysis error analysis engineering improvements. previous work interpreting understanding text models focused summarizing text semantic category level—for instance showing list probable words particular latent topic visualizing latent states character-level long short-term memory recurrent neural network using tokenlevel in-text annotations help understand machine learned model. many models directly deﬁne random variables token-level sufﬁcient statistics resulting individual tokens clear interpretation terms affect inferences model variables. example document classiﬁcation frequencies words impact posterior probability document class. concretely consider multinomial naive bayes whose generative assumption posits document discrete label document’s tokens independently generated single topic φyd. given learned parameters ument utilize posterior binary classiﬁcation case bias term word weights β..βv logistic regression formulated similarly case features word counts posterior odds βvnv case token-level logits βwt. practice full generative model rarely used text; example least terms excluded stopwords punctuation high frequency ﬁltered feature selection. causes many tokens accounted model thus change posterior. deﬁne cases. issue non-linear transforms word counts thresholding scaling often improve classiﬁcation performance unfortunately correspond uniform per-token impact. person read. system provide insight text model thinking showing user original text automatic in-text annotations describing model’s inferences annotations shown abstractly zoomed-out words-as-pixels view text. demonstrate methods using topic models political speeches language classiﬁcation dialectal twitter. models consider document consists sequence symbols ..nd}. could sequence words sequence characters; refer elements sequence tokens particular model deﬁne token-level visual quantity interest position corresponds interesting value model. values encoded visual attributes displaying original token sequence directly user first consider models deﬁne latent variables token level. example latent dirichlet allocation model text posits document arises weighting topics token latent class indexing word distribution used generate word φztwtθdzt. conventionally describe topic. single token position posterior topic membership breaks compromise document prevalence versus lexical probability; able learn interesting representations since individual documents tend subset topics individual topics tend include subset vocabulary. probability given latent topic although demonstrate method using approach methodology would apply common text models. example supervised sequence models also place tokens semantic categories using token-level variables visualized often done annotations interfaces information extraction. similarly karpathy give excellent demonstration preliminary experiments color emerged effective encoding scheme. color represent multiple dimensions well scalar values. previous research visualization examined effectively encode data color given strengths weaknesses human visual system munzner research results colorbrewer palettes available use. text size another interesting option unfortunately variable sized text often difﬁcult read. boldface italics relatively limited information capacity found underlines visually busy. aligns graph next word tokens.) binary document classiﬁcation scalar-valued diverging semantics negative positive correspond different colors blending white utilize classiﬁer visualizations. color used zoomed-out views well. high aggregation level summarizing topic frequencies across thousands documents time colors used in-text annotations assist interpretation. propose complementary high-level level view— words pixels shown figure here individual tokens represented pixels small squares coloring points laid order within document. arrange left-to-right descending columns mimicking natural reading order many leftto-right languages thus corresponding zoomed-out view original text. useful deﬁne features n-grams instance comes span text form pair; e.g. span corresponds bigram positions using n-gram features longer proper generative model text sequence still widely used setting document’s log-probability w|yd) deﬁned n-grams model. case deﬁne span-level weight similar manner extended logistic regression featurebased classiﬁers well. answers part counterfactual token deleted prediction’s logodds would change −ψt. linguistic features could also visualized using color annotations. example syntactic dependency path sequence alternating tokens directed edges unlike n-gram tokens path necessarily contiguous. tokens colored value word token model’s weights paths whose token includes token. dependency edges shown alongside text could colored similar way. deﬁne visual encoding function select ﬁnal visual attributes represent quantity interest user inspired wilkinson grammar graphics approach data visualization. color background foreground text color. boldface italics. analysis ignores impact n-gram features bridging position would introduced; hand text likely would valid likely text perhaps counterfactual viewpoint limited. publication sections chapters within book multiple documents laid another. allows user certain discourse structures text; least ones captured model. figure visualize corpus u.s. presidential state union speeches using david mimno’s jslda data preprocessing topic model implementation average gibbs samples calculate posteriors quantities. model clearly picks natural local groupings latent topics text. driven part model assumptions encoded data preprocessing since version corpus deﬁnes model documents paragraphs speeches. model assumption topic prevalence expected vary textual locality visualization allows qualitative assessment extent assumption holds posterior inferences. apparent example example large streaks orange correspond detailed discussions budgets common include callouts individual paragraphs strong blue topic prevalence discussion political ideologies regards communism islam develop interface linked views data explorer user click word-as-pixel view show corresponding text passage. demo available http//slanglab.cs.umass.edu/ topic-animator/. step internet text analysis pipeline identify language text written character n-gram models widely used approach task popular open-source langid.py tool uses multinomial naive bayes model. short texts pose challenge language identiﬁcation social media messages also present domain adaptation problem since contain much creative nonstandard language different traditional welledited corpora systems typically trained example langid.py uses wikipedia corpora major source training data. figure tweets assess english classiﬁed non-english; every character position blue indicates log-likelihood weight towards nonenglish language; towards english. language statistically associated neighborhoods containing high populations african-americans. expected emerging sociolinguistic literature social media corpora messages contain rich dialectical language different well-edited genres english. fact even ﬁltering messages containing latin- characters langid.py classiﬁes users’ messages non-english upon inspection nearly english. used in-text highlighting help diagnose model errors assigning character position color reﬂecting n-gram feature weights position example common term lmao ends common sufﬁx portugese identiﬁed classiﬁer. characters common modal verbs non-formal american english cause confusion towards irish gaelic. another result expect issue sparsity short texts. many messages small number ﬁring features partly feature selection process used train langid.py’s models suggesting sparsity level tuned level appropriate longer documents short ones. work stems fundamental aspect text processing natural intuitive grasp full meaning written text simply read believe intext annotation less explored natural choice explaining understanding computer’s view language. present simple methods viewing text models expect many avenues future work. boyd-graber jordan mimno david newman david. care feeding topic models problems diagnoshandbook mixed memtics improvements. bership models applications. press chahuneau victor gimpel kevin routledge bryan scherlis lily smith noah word salad reproceedings lating food prices descriptions. emnlp/conll july gardner m.j. lutes lund hansen walker ringger seppi topic browser interactive tool browsing topic models. nips workshop challenges data visualization jørgensen anna hovy dirk søgaard anders. challenges studying processing dialects social meproceedings workshop noisy userdia. generated text beijing china acl. marco baldwin timothy. cross-domain feature selection language identiﬁcation. proceedings international joint conference natural language processing andrew jordan michael. discriminative generative classiﬁers comparison logistic regression naive bayes. advances neural information processing systems o’connor brendan. mitextexplorer linked brushing mutual information exploratory text data analysis. proceedings workshop interactive language learning visualization interfaces", "year": 2016}