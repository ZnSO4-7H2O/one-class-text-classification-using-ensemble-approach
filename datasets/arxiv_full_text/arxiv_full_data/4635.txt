{"title": "On the Foundations of Adversarial Single-Class Classification", "tag": ["cs.LG", "cs.AI"], "abstract": "Motivated by authentication, intrusion and spam detection applications we consider single-class classification (SCC) as a two-person game between the learner and an adversary. In this game the learner has a sample from a target distribution and the goal is to construct a classifier capable of distinguishing observations from the target distribution from observations emitted from an unknown other distribution. The ideal SCC classifier must guarantee a given tolerance for the false-positive error (false alarm rate) while minimizing the false negative error (intruder pass rate). Viewing SCC as a two-person zero-sum game we identify both deterministic and randomized optimal classification strategies for different game variants. We demonstrate that randomized classification can provide a significant advantage. In the deterministic setting we show how to reduce SCC to two-class classification where in the two-class problem the other class is a synthetically generated distribution. We provide an efficient and practical algorithm for constructing and solving the two class problem. The algorithm distinguishes low density regions of the target distribution and is shown to be consistent.", "text": "training sample learner select rejection function probability learner reject basis knowledge and/or adversary selects attacking distribution deﬁned then example drawn switching start game learner receives tolerance parameter giving maximally allowed false positive rate. rejection function valid false positive rate satisﬁes constraint valid rejection function pr{x|} pr{x|∅} equivalent pr{x|} pr{x}. assumed pr{x} known estimated historical data leaving problem estimating pr{x|} given sample. while technically sample clear point falling outside estimated support taken outlier. simpler methods analyzed devroye wise estimate support closed ball centered radius sequence smoothing parameters. quantile estimation goal becomes support estimation problem low-density rejection. level-set estimation goal approximate t}). course level-set estimation used support estimation taking taking sequence approaches zero clearly level-set estimation approximates low-density gaussian kernel well-calibrated bandwidth produce estimate limn→∞ probability. steinwart hush scovel provide convergence rates using l-svm error measure ∆ln) inﬁnite. extensions inﬁnite support many ﬁnite support results given nisenson simple observation exists zero probabilities thus assume w.l.o.g. deﬁnition distribution distributions. exists distribution possesses property w.r.t. example distribution uniform distribution property w.r.t. since never true. similarly distributions also property w.r.t. then doesn’t possess property w.r.t distributions. unsurprisingly game leaves little opportunity learner. rejection function deﬁne rmin i=dirmin rmin particular rmin minq rmin. choosing imin adversary achieve rmin soft setting minq maximized rejection function equivalent continuous lemma transferring probability hmax would result since transferring probability hmax results making smaller contradicting fact minimizes assume consider cases. ﬁrst case w.l.o.g. deﬁning part contradiction. second case however since greater zero deﬁning part gives contradiction. proof assume contradiction theorem’s statement wrong; exists optimal events w.l.o.g. rename events ﬁrst events. note solution following problem show unique global maximum lagrangian show exists diﬀerent distribution eﬀective support meets equality constraints. therefore conclude contradicting optimality assume then parts lemma thus therefore strict convexity linearity equations lagrangian strictly concave. therefore since extremum point lagrangian function unique global maximum. sub-domain interior point). points. then distribution satisﬁes exact eﬀective support therefore meets equality criteria lagrangian. since unique global maximum contradicting fact optimal. partition event subsets correspond probability level sets re-index subsets deﬁne variables representing rejection rate assigned level sets ri). since -symmetric constant level therefore group level sets probability notation lsm}. cannot level lemma would violated). therefore must belong single level thus {sm} feasible solutions following lemma characterizes optimal distributions therefore adversary’s choice optimal distribution must |l||h|+ rejection rates. rates ρ|l||h|+|m| linear combination variables introduce additional variable represent max-min rejection rate. entails following theorem. solution linear program derivation linear program dependent restriction isn’t vulnerable. contradicts restriction then discussed optimal strategy following lemma shows case anyway thus solution linear program always optimal. proof found appendix points drawn i.i.d. normalized second family figure .are gaussians centered discretized evenly spaced bins range random gaussian selected choosing uniformly range σmin minimum ensuring ﬁrst/last zero probability σmax cumulative probability ﬁrst/last possible non-negative diﬀerentiable convex strictly convex satisﬁes conditions similar stronger conditions required bartlett jordan mcauliﬀe provide necessary suﬃcient conditions convex classiﬁcation-calibrated. note however commonly used utilize hard binary classiﬁer induction algorithm minimizes either hinge loss functions well loss functions deﬁned bartlett algorithm given training sample training examples drawn i.i.d. unknown source distribution given type-i threshold algorithm outputs hard rejection function main idea algorithm lemma proof recall sequence positive numbers limn→∞ limn→∞ deﬁne sequence positive numbers y||∞ g′n}. deﬁne limn→∞ devroye wise show probability measure borel sets whose restriction supp absolutely continuous w.r.t. holds theorem {u′n} sequence probability measures uniform density bounded support limn→∞ deﬁne bayesian binary classiﬁcation problem ﬁrst class distribution second class distribution u′n. classes’ prior probabilities pr{+} pr{−} non-negative diﬀerentiable convex loss function strictly convex easy verify ﬁxed minimum u′nφ′. alternatively ψnφ′. points note min{φ′ solution ciφ′ note therefore order equality occur necessary rewrite ciφ′ |φ′| ci|φ′|. therefore |φ′| |φ′| |φ′| |φ′|. since |φ′| |φ′|. therefore gives |φ′| c|φ′| contradiction. thus |φ′| c|φ′| c|φ′| c|φ′| c|φ′| |φ′|. contradiction. strictly monotonically increasing almost everywhere suppt supp. since constant support implies h∗n. therefore suppt supp t∗n} identical suppt supp v∗n} compact support continuously diﬀerentiable. constant then contain lower-left corners grid cells containing sample points. since decreases lemma remains correct a.s.−→ thus a.s.−→ well. results given robert schapire obtain upper bound ﬁnite sample sizes. represent solution primal problem r∗ii solution dual problem. deﬁne minq∈q since respectively feasible solutions primal dual problems lemma ﬁnite discrete. minq proof contradiction. lemma minq clearly r∗ii feasible solution primal problem thus assume contradiction then must exist minq then clearly deﬁne minq contradiction. therefore using theorem trivial solve dual problem begin with since assume note therefore since optimal solution sets identical theorem intermediate results including theorem correct solving primal problem vacuous). note case therefore thus therefore i=|si|pr∗i therefore note valid solution linear program minimal value achievable. therefore i=|si|pr∗i", "year": 2010}