{"title": "Exploring Deep and Recurrent Architectures for Optimal Control", "tag": ["cs.LG", "cs.AI", "cs.NE", "cs.RO", "cs.SY"], "abstract": "Sophisticated multilayer neural networks have achieved state of the art results on multiple supervised tasks. However, successful applications of such multilayer networks to control have so far been limited largely to the perception portion of the control pipeline. In this paper, we explore the application of deep and recurrent neural networks to a continuous, high-dimensional locomotion task, where the network is used to represent a control policy that maps the state of the system (represented by joint angles) directly to the torques at each joint. By using a recent reinforcement learning algorithm called guided policy search, we can successfully train neural network controllers with thousands of parameters, allowing us to compare a variety of architectures. We discuss the differences between the locomotion control task and previous supervised perception tasks, present experimental results comparing various architectures, and discuss future directions in the application of techniques from deep learning to the problem of optimal control.", "text": "sophisticated multilayer neural networks achieved state results multiple supervised tasks. however successful applications multilayer networks control limited largely perception portion control pipeline. paper explore application deep recurrent neural networks continuous high-dimensional locomotion task network used represent control policy maps state system directly torques joint. using recent reinforcement learning algorithm called guided policy search successfully train neural network controllers thousands parameters allowing compare variety architectures. discuss differences locomotion control task previous supervised perception tasks present experimental results comparing various architectures discuss future directions application techniques deep learning problem optimal control. multilayer neural networks autoencoders convolutional neural networks achieved state results number perception natural language tasks however recent attempts extend methods supervised classiﬁcation tasks control reinforcement learning focused adapting deep component perception using standard prior techniques control example learning controller operates directly camera images direct application deep learning optimal control leaves open question whether control policy itself rather visual processing component beneﬁt deep learning techniques. paper explore possibility comparing several deep architectures including recurrent deep neural networks continuous high-dimensional locomotion task. experiments networks used represent control policy maps joint angles torques applied joint. major stumbling block application powerful models like deep neural networks control shortage effective learning algorithms handle rich policy classes also addressing sorts complex tasks actually beneﬁt increased representational power. although multilayer networks explored eighties nineties methods typically used small controllers relatively simple tasks. early experiments neural network control represented system dynamics policy neural networks gradient policy could propagated backwards time however direct optimization approach produce highly unstable gradients often unsuitable learning nontrivial behaviors. recent reinforcement learning methods instead attempt improve policy using sample rollouts actual system avoids need propagate gradients time however methods still susceptible local optima work best simple linear policies leading popularity compact specialized control-speciﬁc function approximators lack expressive power large neural networks work apply recently developed policy search algorithm effectively train large neural networks difﬁcult tasks using trajectory optimization example demonstrations. algorithm called guided policy search uses trajectory optimization guide policy search regions high reward typically initialized successful example demonstration. policy repeatedly optimized standard non-linear optimization algorithm lbfgs making relatively easy combine method controller class. using algorithm present series experiments compare performance several deep recurrent architectures continuous high-dimensional locomotion task rough terrain. task differs qualitatively previous perception tasks since input network consists relatively small number joint angles output consists comparable number joint torques. domain much lower dimensionality than example natural images also demands much greater precision locomotion balance require precise feedbacks response continuous changes joint angles. evaluate shallow single-layer neural networks soft hard rectiﬁed linear units well multi-layer networks varying numbers hidden units recurrent networks maintain hidden state. preliminary results indicate modest improvement generalization networks recurrent multiple layers also suggest overﬁtting local optima become major problem sophisticated controller architectures. improvements advanced experiments discussed future work. reinforcement learning optimal control applied considerable success ﬁelds robotics computer graphics control complex systems robots virtual characters however many successful applications relied policy classes either hand-engineered domain-speciﬁc else restricted following single trajectory neither approach seems adequate learning sorts rich motion repertoires might needed example robot must execute variety tasks natural environment virtual character must exhibit range behaviors emulate human being. order scale optimal control reinforcement learning wide range tasks need generalpurpose policies represent behavior without extensive hand-engineering. accomplish using expressive policy classes. empirical results domain computer vision suggest multilayer networks learn hierarchical structures data general higher-level concepts captured higher levels hierarchy learning hierarchical structures control reinforcement learning long regonized important research goal potential drastically improve generalization transfer. context motor control generalization might include learning skill context generalizing without additional training contexts could enabled learning commonly applicable control control might accomplish walking serve component hierarchically organized walking controller. evaluation examine modest example generalization attained current methods. unfortunately neural networks also proven notoriously difﬁcult adapt control complex dynamical systems. work recently developed guided policy search algorithm train neural network controllers. compared previous methods approach shown effective training neural network controllers though still struggles local optima appear make deep recurrent architectures. discuss possible solutions future work. guided policy search reinforcement learning algorithm uses trajectory optimization example demonstrations guide policy learning. algorithm optimize parameters parameterized policy speciﬁes distribution actions state dynamical system robot state might include features joint angles velocities actions might consist torques apply joint. desired task deﬁned reward function goal learning algorithm time horizon. guided policy search algorithm maximizes objective using importance-sampled estimate expected return off-policy samples. rudimentary importance-sampled estimator given normalizing constant ensures importance weights distribution denotes sampling distribution. sophisticated regularized estimator used practice discussed previous work optimizing estimate expected return respect policy parameters policy improved. learning complex tasks legged locomotion trajectories receive poor rewards this unless sampling distribution chosen carefully samples receive poor reward optimization unable improve policy. idea guided policy search draw samples known good distribution trajectories constructed using variant differential dynamic programming trajectory optimization algorithm construct distribution trajectories simplest variant used paper begins example demonstration uses construct distribution similar trajectories high reward. algorithm ﬁrst initializes policy supervised learning samples drawn trajectory distribution alternates optimized expected return drawing additional on-policy samples improve return estimate. optimization performed non-linear optimization algorithm. however importance weight normalization constant couples samples making difﬁcult apply minibatch algorithms stochastic gradient descent therefore either lbfgs plain gradient descent experiments. recent variational variant guided policy search eschews importance sampling allows supervised learning algorithm including used however variant currently supports reactive policies depend current state making unsuitable training recurrent neural networks. extending variational handle recurrent neural networks would simplify optimization problem. evaluate variety neural network architectures chose challening bipedal locomotion task. since generalization exciting advantages richer policy representations evaluation centered testing generalization learned policies. task visualized figure requires policy control simulated planar biped walking sloped terrain. slope terrain varies reward function penalizes squared deviation walker’s velocity trunk height target values well squared torques control policy trained varying number example terrains tested different test terrains shown figure terrains generated randomly remain unchanged throughout experiments. learning algorithm initialized example demonstrations generated prior locomotion system practice human demonstration ofﬂine planner might used discussed previous work note although task modeled previous work results directly comparable terrains longer challenging simulator uses accurate contact model example demonstrations different motor noise increased slightly. short task modiﬁed difﬁcult differences various architectures would apparent. figure sections training terrains test terrains overlaid rollouts recurrent neural network controller. lines indicate trajectory trunk. terrains consist randomly chosen segments slopes trials controller fall continued progressing forward. believe metric informative measuring total accumulated reward since practical application would care much falling failing make progress precise speed torso height attained controller. controllers evaluated used neural network vector state features joint torques planar walker degrees freedom consisting joints global orientation global position. state feature vector omits horizontal position prevent controller memorizing pattern slopes subtracts ground height vertical position. feature vector also includes velocity root joints indicator whether foot contact position body segment relative trunk total dimensions. noted type data qualitatively different image audio video data deep neural networks typically evaluated example compared image data state feature vector quite small. hand speciﬁc numerical values vector much important since difference height could mean difference standing upright falling ground. hand speciﬁc brightness pixel often matters primarily relation pixels rather absolute terms. similarly performance controller depends tremendously speciﬁc precise covariance inputs outputs since controller must learn appropriate feedbacks maintain balance. type output signiﬁcantly complex classiﬁcation labels. subject regularization well studied ﬁeld deep learning. popular regularizers sparsity denoising avoid overﬁtting. attempt sparsity penalty tests found decreased performance resulting controller. believe differences structure image data state feature vectors regularizers would effective without modiﬁcation. developing regularization schemes suited optimal control interesting avenue future work. evaluated three different types network architectures single-layer network two-layer network single-layer recurrent network. network varied number hidden units used either soft hard rectiﬁed linear units given log) respectively. since found lbfgs struggled optimize hard rectiﬁed linear units used gradient descent tests. attempt also evaluate networks sigmoidal hidden units found performed figure performance controller training -terrain test set. terrain traversed times producing slightly different results motor noise. score controller fraction trials completed successfully results soft hard units shown separately clarity. results rough terrain locomotion experiments plotted figure legend lists type network followed number hidden units layer. number hidden units shallow networks chosen number parameters would roughly match corresponding deep recurrent network. shallow -unit network parameters -unit -unit deep recurrent networks -unit ones results soft hard units presented separately clarity. trained network example terrains. noted number reﬂect total number samples value estimate samples drawn terrain example trajectory distribution consisting time steps additional samples drawn terrain policy iteration algorithm. although perform cross-validation would valuable addition future study would help average effects local optima imbalances difﬁculty randomly generated terrains. expected generalization improves number training terrains complex networks tend require training terrains achieve good results. number examples increases deep recurrent networks tend outperform shallow ones though drastically always. soft rectiﬁed hidden units deep networks perform best small recurrent network performs best using hard rectiﬁed units. possible explanation second-order lbfgs algorithm train deeper networks effectively piecewise linear hard units cause fewer problems long backpropagation recurrent networks even without lbfgs. aside obvious trends results provide interesting insights issues overﬁtting local optima pertain reinforcement learning learning demonstration. overﬁtting unlike standard supervised learning overﬁtting complicated subject experiment combination reinforcement learning learning demonstration. high level controllers suffer distinct forms overﬁtting overﬁt example demonstration learning poor control resulting poor performance results show forms overﬁtting. hard hidden units larger networks tend overﬁt example demonstrations shown poor performance training terrain example type overﬁtting characterized improvement training terrain performance training examples would seem paradoxical supervised learning. hand results also show clear example overﬁtting training terrain case -unit shallow network hard hidden units examples. surprisingly type overﬁtting apear severely afﬂict recurrent networks might hypothesize would vulnerable memorizing pattern slopes training terrains instead appears correspond simply size network though unclear trend real coincidental. problems could alleviated sort regularization discussed previously standard regularization methods tricky apply tasks. local optima nature complex control tasks present considerable number local optima. stems directly expected reward objective objectives like maximum likelihood generally take expectation ﬁxed data distribution control objective requires expectation respect distribution optimized. means samples probability policy represented objective value regardless reward. restarts used alleviate problem degree appear afﬂict many networks experiment number examples increases. problem characterized decrease training test terrain performance example terrains represents troubling challenge scaling neural network-based optimal control difﬁcult problems. essence challenge that even representational power network sufﬁcient learn good policy increasing number poor local optima make impossible even number examples increased indeﬁnitely. next section discuss future directions address challenge. aside pretraining viewed deal local optima deep networks multilayer networks pretrained autoencoders using sort sparsity denoising regularization. attempt pretrain deep networks sparsity regularizer found harmed performance. suspect related particular statistical properties state feature vectors mentioned previously discussion regularizers. conclusions overall results suggest that even equal number parameters deep recurrent networks least match sometimes outperform shallow networks number example terrains large. difference prominent though also likely local optima overﬁtting hampering larger deep recurrent networks. main lessons learned experiment following context control learning demonstration overﬁtting come ﬂavors local optima pose serious challenge especially complicated neural networks deep recurrent networks appear offer advantages shallow ones though full beneﬁt likely realized. lastly practical lesson many larger networks generalize well number training domains large. often missed standard reinforcement learning experiments tend evaluated domain time. presented experiments compare performance various deep architectures continuous high-dimensional locomotion task. analysis shows modest improvement generalization deep recurrent networks number training domains large though results also indicate networks likely hampered local optima might pose greater challenge optimal control supervised learning. results point number promising directions future work. since local optima appear bigger problem number training domains increases possibility addressing challenge train controllers incrementally starting easy tasks progressing larger larger numbers training domains. however unclear policy falls poor local optima ﬁrst domains would ever recover presented additional ones. another option explore regularization schemes might alleviate local optima. hope possible since poor local optima likely correspond unstable control laws. example rough terrain could traversed successfully controller understands balance could also traversed controller memorized particular pattern slopes. latter controller brittle stochastic regularization schemes like dropout might effective avoiding solutions. another promising direction modify optimization algorithm make like tractable supervised learning objectives. recent followup guided poliy search algorithm accomplishes that using variational approximation modiﬁed policy objective algorithm alternates optimizing policy match trajectory distribution optimizing trajectory distribution achieve high reward match policy. difﬁculty approach trajectory optimization step requires policy differentiable terms system state optimization requires policy reactive preventing used recurrent networks. former challenge could addressed stochastic trajectory optimization algorithm perhaps modeled recent work path integrals optimal control latter could addressed including recurrent hidden unit state trajectory optimization. advantage switching maximum likelihood objective would enable scalable optimization algorithm stochastic gradient descent used. likely prerequisite training large numbers domains would required complex tasks. even experiments average policy optimization training terrains used trajectories time steps length corresponding million forward backward propagations network gradient step.", "year": 2013}