{"title": "Deep Q-learning from Demonstrations", "tag": ["cs.AI", "cs.LG"], "abstract": "Deep reinforcement learning (RL) has achieved several high profile successes in difficult decision-making problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages small sets of demonstration data to massively accelerate the learning process even from relatively small amounts of demonstration data and is able to automatically assess the necessary ratio of demonstration data while learning thanks to a prioritized replay mechanism. DQfD works by combining temporal difference updates with supervised classification of the demonstrator's actions. We show that DQfD has better initial performance than Prioritized Dueling Double Deep Q-Networks (PDD DQN) as it starts with better scores on the first million steps on 41 of 42 games and on average it takes PDD DQN 83 million steps to catch up to DQfD's performance. DQfD learns to out-perform the best demonstration given in 14 of 42 games. In addition, DQfD leverages human demonstrations to achieve state-of-the-art results for 11 games. Finally, we show that DQfD performs better than three related algorithms for incorporating demonstration data into DQN.", "text": "deep reinforcement learning achieved several high proﬁle successes difﬁcult decision-making problems. however algorithms typically require huge amount data reach reasonable performance. fact performance learning extremely poor. acceptable simulator severely limits applicability deep many real-world tasks agent must learn real environment. paper study setting agent access data previous control system. present algorithm deep q-learning demonstrations leverages small sets demonstration data massively accelerate learning process even relatively small amounts demonstration data able automatically assess necessary ratio demonstration data learning thanks prioritized replay mechanism. dqfd works combining temporal difference updates supervised classiﬁcation demonstrator’s actions. show dqfd better initial performance prioritized dueling double deep q-networks starts better scores ﬁrst million steps games average takes million steps catch dqfd’s performance. dqfd learns out-perform best demonstration given games. addition dqfd leverages human demonstrations achieve state-of-the-art results games. finally show dqfd performs better three related algorithms incorporating demonstration data dqn. number sequential decision-making problems control. notable examples include deep model-free q-learning general atari game-playing end-to-end policy search control robot motors model predictive control embeddings strategic policies combined search game defeating human expert important part success approaches leverage recent contributions scalability performance deep learning approach taken builds data previous experience using batch train large convolutional neural networks supervised fashion data. sampling data rather current experience correlation values state distribution bias mitigated leading good control policies. apply algorithms aureal world settings helitonomous recommendation syscopters tems typically algorithms learn good control policies many millions steps poor performance simulation. situation acceptable perfectly accurate simulator; however many real world problems come simulator. instead situations agent must learn real domain real consequences actions requires agent good online performance start learning. accurate simulators difﬁcult problems data system operating previous controller performs reasonably well. work make demonstration data pre-train agent perform well task start learning continue improving self-generated data. enabling learning framework opens possibility applying many real world problems demonstration data common accurate simulators exist. propose deep reinforcement learning algorithm deep q-learning demonstrations leverages even small amounts demonstration data massively accelerate learning. dqfd initially pretrains solely demonstration data using combination temporal difference supervised losses. supervised loss enables algorithm learn imitate demonstrator loss enables learn selfconsistent value function continue learning pre-training agent starts interacting domain learned policy. agent updates network demonstration self-generated data. practice choosing ratio demonstration self-generated data learning critical improve performance algorithm. contributions prioritized replay mechanism automatically control ratio. dqfd out-performs learning using prioritized duelpure double games ﬁrst million steps average takes million steps catch dqfd. addition dqfd out-performs pure imitation learning mean score games out-performs best demonstration given games. dqfd leverages human demonstrations learn state-of-the-art policies games. finally show dqfd performs better incorporating demonstration data dqn. prioritized experience replay modiﬁes agent sample important transitions replay buffer frequently. probability sampling particular transition proportional priority priority |δi| last error calculated transition small positive constant ensure transitions sampled probability. account change distribution updates network weighted importance sampling weights size replay buffer controls amount importance sampling importance sampling full importance sampling annealed linearly adopt standard markov decision process formalism work deﬁned tuple consists states actions reward function transition function discount factor state agent takes action upon taking action agent receives reward reaches state determined probability distribution policy speciﬁes state action agent take. goal agent policy mapping states actions maximizes expected discounted total reward agent’s lifetime. value given state-action pair estimate expected future reward obtained following policy optimal value function provides maximal values states determined solving bellman equation optimal policy argmaxa∈a approximates value function deep neural network outputs action values given state input parameters network. components make work. first uses separate target network copied every steps regular network target q-values stable. second agent adds experiences replay buffer dreplay sampled uniformly perform updates network. imitation learning primarily concerned matching performance demonstrator. popular algorithm dagger iteratively produces policies based polling expert policy outside original state space showing leads no-regret validation data online learning sense. dagger requires expert available training provide additional feedback agent. addition combine imitation reinforcement learning meaning never learn improve beyond expert dqfd can. deeply aggrevated extends dagger work deep neural networks continuous action spaces. require always available expert like dagger does expert must provide value function addition actions. similar dagger deeply aggrevated imitation learning cannot learn improve upon expert. another popular paradigm setup zero-sum game learner chooses policy adversary chooses reward function demonstrations also used inverse optimal control high-dimensional continuous robotic control problems however approaches imitation learning allow learning task rewards. algorithm works scenario rewards given environment used demonstrator. framework appropriately called reinforcement learning expert demonstrations also evaluated setup similar combine classiﬁcation losses batch algorithm model-free setting; differs agent pre-trained demonstration data initially batch self-generated data grows time used experience replay train deep q-networks. addition prioritized replay mechanism used balance amount demonstration data mini-batch. present interesting results showing adding loss supervised classiﬁcation loss improves imitation learning even rewards. similarly motivated work focused real world learning robots thus also concerned on-line performance. similar work pre-train agent demonstration data letting interact task. however supervised learning pre-train algorithm able case pre-training helps learning cart-pole. agent provided entire demonstration input addition current state. demonstration speciﬁes goal state wanted different initial conditions. agent trained target actions demonstrations. setup also uses demonstrations requires distribution tasks different initial conditions goal states agent never learn improve upon demonstrations. alphago takes similar approach work pre-training demonstration data interacting real task. alphago ﬁrst trains policy network dataset million expert actions using supervised learning predict actions taken experts. uses starting point apply policy gradient updates self-play combined planning rollouts. here model available planning focus model-free q-learning case. replay algorithm agent samples replay buffer mixed agent demonstration data similar approach. gains slightly better random agent surpassed alternative approach human checkpoint replay requires ability state environment. algorithm similar samples datasets pre-train agent supervised loss. results show higher scores larger variety games without requiring full access environment. replay buffer spiking another similar approach agent’s replay buffer initialized demonstration data pre-train agent good initial performance keep demonstration data permanently. work closely relates workshop paper presenting accelerated expert trajectories also combining classiﬁcation losses deep q-learning setup. trained agent generate demonstration data games better human data. also guarantees policy used demonstrator represented apprenticeship agent using state input network architecture. cross-entropy classiﬁcation loss rather large margin loss dqfd uses pre-train agent perform well ﬁrst interactions environment. many real-world settings reinforcement learning access data system operated previous controller access accurate simulator system. therefore want agent learn much possible demonstration data running real system. goal pre-training phase learn imitate demonstrator value function satisﬁes bellman equation updated updates agent starts interacting environment. pre-training phase agent samples mini-batches demonstration data updates network applying four losses -step double q-learning loss n-step double q-learning loss supervised large margin classiﬁcation loss regularization loss network weights biases. supervised loss used classiﬁcation demonstrator’s actions q-learning loss ensures network satisﬁes bellman equation used starting point learning. supervised loss critical pre-training effect. since demonstration data necessarily covering narrow part state space taking possible actions many state-actions never taken data ground realistic values. pre-train network q-learning updates towards value next state network would update towards highest ungrounded variables network would propagate values throughq function. large margin classiﬁcation loss margin function positive otherwise. loss forces values actions least margin lower value demonstrator’s action. adding loss grounds values unseen actions reasonable values makes greedy policy induced value function imitate demonstrator. algorithm pre-trained supervised loss would nothing constraining values consecutive states q-network would satisfy bellman equation required improve policy on-line learning. also regularization loss applied weights biases network help prevent over-ﬁtting relatively small demonstration dataset. overall loss used update network combination four losses pre-training phase complete agent starts acting system collecting self-generated data adding replay buffer dreplay. data added replay buffer full agent starts overwriting data buffer. however agent never over-writes demonstration data. proportional prioritized sampling different small positive constants added priorities agent demonstration transitions control relative sampling demonstration versus agent data. losses applied demonstration data phases supervised loss applied self-generated data inputs dreplay initialized demonstration data weights initial behavior network weights target network frequency update target number pre-training gradient updates sample action behavior policy πǫqθ play action observe store dreplay overwriting oldest self-generated transition capacity sample mini-batch transitions dreplay prioritization calculate loss using target network perform gradient descent step update evaluated dqfd arcade learning environment atari games standard benchmark contains many games humans still perform better best learning agents. agent plays atari games down-sampled image game screen converted greyscale agent stacks four frames together state. agent must output possible actions game. agent applies discount factor actions repeated four atari frames. episode initialized no-op actions provide random starting positions. scores reported scores atari game regardless agent representing reward internally. algorithms averaged across four trials full dqfd algorithm human demonstrations learning without demonstration data supervised imitation demonstration data without environment interaction performed informal parameter tuning algorithms atari games used parameters entire games. parameters used algorithms shown appendix. coarse search prioritization n-step return parameters best parameters dqfd dqn. differs dqfd demonstration data pre-training supervised losses regularization losses. included n-step returns provide better baseline comparison dqfd dqn. three algorithms dueling state-advantage convolutional network architecture supervised imitation comparison performed supervised classiﬁcation demonstrator’s actions using cross-entropy loss network architecture regularization used dqfd. imitation algorithm loss. imitation learning learns pre-training additional interactions. experiments randomly selected subset atari games. human player play game between three twelve times. episode played either game terminated minutes. game play logged agent’s state actions rewards terminations. human demonstrations range transitions game. dqfd learns small dataset compared similar work alphago learns million human transitions learns million frames. dqfd’s smaller demonstration dataset makes difﬁcult learn good representation withover-ﬁtting. demonstration scores game shown table appendix. human demonstrator much better games much worse many games found many games human player better trained rewards clipped example private reason select actions reward versus actions reward make reward function used human demonstrator agent consistent used unclipped rewards converted rewards using scale ragent sign log. transformation keeps rewards reasonable scale neural network learn conveying important information relative scale individual rewards. adapted rewards used internally algorithms experiments. results still reported using actual game scores typically done atari literature first show learning curves figure three games hero pitfall road runner. hero human demonstrations enable dqfd pitfall achieve score higher previously published result. videos games available https//www.youtube.com/watch?v=jrwmlayuu. hero dqfd achieves higher score human demonstrations well previously published result. pitfall difﬁcult atari game sparse positive rewards dense negative rewards. previous approach achieved positive rewards game dqfd’s best score game averaged million step period road runner agents typically learn super-human policies score exploit differs greatly human play. demonstrations human maximum score road runner game smallest human demonstrations despite factors dqfd still achieves higher score ﬁrst million steps matches dqn’s performance that. right subplot figure shows ratio often demonstration data sampled versus much would sampled uniform sampling. difﬁcult games like pitfall montezuma’s revenge demonstration data sampled frequently time. games ratio converges near constant level differs game. real world tasks agent must perform well ﬁrst action must learn quickly. dqfd performed better ﬁrst million steps games. addition games dqfd starts higher performance pure imitation learning addition loss helps agent generalize demonstration data better. average surpass performance dqfd million steps task never surpasses mean scores. addition boosting initial performance dqfd able leverage human demonstrations learn better policies difﬁcult atari games. compared dqfd’s scores million steps deep reinforcement learning approaches double prioritized dueling popart dqn+pixelcnn took best million step window averaged seeds dqfd scores. dqfd achieves better scores algorithms games shown table note compare reactor published results human starts compare unreal select best hyper-parameters game. despite fact dqfd still out-performs best unreal results games. count-based exploration designed achieves best results difﬁcult exploration games. sparse reward hard exploration games algorithms dqfd learns better policies four games. dqfd out-performs worst demonstration episode given games learns play better best demonstration episode games amidar atlantis boxing breakout crazy climber defender enduro fishing derby hero james bond kung master pong road runner down. comparison pure imitation learning worse demonstrator’s performance every game. figure on-line scores algorithms games hero pitfall road runner. hero pitfall dqfd leverages human demonstrations achieve higher score previously published result. last plot shows much frequently demonstration data sampled data sampled uniformly different games. figure left plots show on-line rewards dqfd losses removed games montezuma’s revenge q-bert. removing either loss degrades performance algorithm. right plots compare dqfd three algorithms related work section. approaches perform well dqfd particularly montezuma’s revenge. pre-training without supervised loss results network trained towards ungrounded q-learning targets agent starts much lower performance slower improve. removing n-step loss nearly large impact initial performance n-step loss greatly helps learning limited demonstration dataset. table scores games dqfd achieves higher scores previously published deep result using random no-op starts. previous results take best agent best iteration evaluate episodes. dqfd scores best million step window averaged four seeds episodes average. simply replay buffer initially full demonstration data. keeps demonstration data mixes demonstration agent data mini-batch. adet essentially dqfd large margin supervised loss replaced cross-entropy loss. results show three approaches worse dqfd games. supervised loss critical good performance dqfd adet perform much better algorithms. algorithms exact demonstration data used dqfd. included prioritized replay mechanism n-step returns algorithms make strong comparison possible. learning framework presented paper common real world problems controlling data centers autonomous vehicles recommendation systems problems typically accurate simulator available learning must performed real system real consequences. however often data available system operated previous controller. presented algorithm called dqfd takes advantage data accelerate learning real system. ﬁrst pretrains solely demonstration data using combination -step n-step supervised regularization losses reasonable policy good starting point learning task. starts interacting task continues learning sampling selfgenerated data well demonstration data. ratio types data mini-batch automatically controlled prioritized-replay mechanism. shown dqfd gets large boost initial performance compared dqn. dqfd better performance ﬁrst million steps atari games average takes million steps match dqfd’s performance. real world tasks agent never hundreds millions steps learn. also showed dqfd out-performs three algorithms leveraging demonstration data fact dqfd out-performs algorithms makes clear better choice real-world application type demonstration data available. addition early performance boost dqfd able leverage human demonstrations achieve state-ofthe-art results atari games. many games hardest exploration games demonstration data used place smarter exploration. result enables deployment problems intelligent exploration would otherwise required. dqfd achieves results despite small amount demonstration data easily generated minutes gameplay. dqfd receive three orders magnitude interaction data demonstration data. dqfd demonstrates gains achieved adding small amount demonstration data right algorithm. related work comparison shows naively adding small amount data pure deep algorithm provide similar beneﬁt sometimes detrimental. results seem obvious given dqfd access privileged data rewards demonstrations mathematically dissimilar training signals naive approaches combining disastrous results. simply supervised learning human demonstrations successful dqfd learns out-perform best demonstration games. dqfd also outperforms three prior algorithms incorporating demonstration data dqn. argue combination four losses pre-training critical agent learn coherent representation destroyed switch training signals pre-training. even pre-training agent must continue using expert data. particular right sub-ﬁgure figure shows ratio expert data needed grows interaction phase difﬁcult exploration games demonstration data becomes useful agent reaches screens game. shows example demonstration data initially enough provide good performance. learning human demonstrations particularly difﬁcult. games imitation learning unable perfectly classify demonstrator’s actions even demonstration dataset. humans play games differs greatly policy agent would learn using information available agent’s state representation. future work plan measure differences demonstration agent data inform approaches derive value demonstrations. another future direction apply concepts domains continuous actions classiﬁcation loss becomes regression loss. authors would like thank keith anderson chris apps coppin fenton nando freitas chris gamble thore graepel georg ostrovski cosmin paduraru jack amir sadik scholz david silver toby pohlen stepleton ziyu wang many others deepmind insightful discussions code contributions efforts.", "year": 2017}