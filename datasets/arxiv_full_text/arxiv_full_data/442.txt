{"title": "Neural Responding Machine for Short-Text Conversation", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.", "text": "propose neural responding machine neural network-based response generator short-text conversation. takes general encoder-decoder framework formalizes generation response decoding process based latent representation input text encoding decoding realized recurrent neural networks trained large amount one-round conversation data collected microblogging service. empirical study shows generate grammatically correct content-wise appropriate responses input text outperforming state-of-the-arts setting including retrieval-based smt-based models. intelligence problems natural involves language understanding reasoning utilization common sense knowledge. previous works direction mainly focus either rule-based learning-based methods types methods often rely manual effort designing rules automatic training model particular learning algorithm small amount data makes difﬁcult develop extensible open domain conversation system. recently explosive growth microblogging services twitter weibo amount conversation data available tremendously increased. makes data-driven approach attack conversation problem possible. instead multiple rounds conversation task hand referred short-text conversation considers round conversation round formed short texts former input user latter response given computer. research shed light understanding complicated mechanism natural language conversation. previous methods fall categories retrieval-based method statistical machine translation based method basic idea retrieval-based method pick suitable response ranking candidate responses linear non-linear combination various matching features main drawbacks retrieval-based method following smt-based method hand generative. basically treats response generation translation problem model trained parallel corpus post-response pairs. despite generative nature method intrinsically unsuitable response generation responses semantically equivalent posts translation. actually post receive responses completely different content manifested example following ﬁgure paper take probabilistic model address response generation problem propose employing neural encoder-decoder task named neural responding machine neural encoder-decoder model illustrated figure ﬁrst summarizes post vector representation feeds representation decoder generate responses. generalize scheme allow post representation dynamically change generation process following idea originally proposed neural-network-based machine translation automatic alignment. essentially estimates likelihood response given post. clearly estimated probability complex enough represent suitable responses. similar framework used machine translation remarkable success note machine translation task estimate probability target language sentence conditioned source language sentence meaning much easier task considering here. paper demonstrate equipped reasonable amount data yield satisfying estimator responses despite difﬁculty task. main contributions two-folds propose encoder-decoder-based neural network generate response stc; empirically veriﬁed proposed method trained reasonable amount data yield performance better traditional retrieval-based translation-based methods. remainder paper start introducing dataset section elaborate model section followed details training section that report experimental results section section conclude paper. weibo popular twitter-like microblogging service china user post short messages visible public group users following her/him. users make comment published post referred response. like twitter weibo also length limit chinese characters posts responses making post-response pair ideal surrogate short-text conversation. construct million scale dataset ﬁrst crawl hundreds millions post-response pairs clean data similar suggested including removing trivial responses like ﬁltering potential advertisements removing responses ﬁrst ones topic consistency. table shows statistics dataset used work. seen post different responses average. addition semantic post responses another difference general parallel data used traditional translation. basic idea build hidden representation post generate response based shown figure particular illustration encoder converts input sequence high-dimensional hidden representations which along attention signal time context-generator build context input decoder time linearly transformed matrix stimulus generating produce t-th word response neural translation system converts representation source language target language. plays difﬁcult role needs transform representation post rich representation many plausible responses. surprising achieved reasonable level linear transformation space representation validated section show post actually invoke many different responses nrm. role attention signal determine part hidden representation emphasized generation process. noted could ﬁxed time changes dynamically generation response sequence dynamic settings function historically generated subsequence input sequence latent representations details shown later section figure gives graphical model decoder essentially standard language model except conditioned context input generation probability t-th word calculated non-linear activation function transformation often assigned parameters logistic function sophisticated long short-term memory unit recently proposed gated recurrent unit compared ungated logistic function lstm specially designed long term memory store information extended time steps without much decay. work since performs comparably lstm squence modeling less parameters easier train. global scheme figure shows graphical model rnn-encoder related context generator global encoding scheme. hidden state time calculated trivial context generation operation essentially ﬁnal hidden state global representation sentence. strategy taken building intermediate representation machine translation. scheme however drawbacks vectorial summarization entire post often hard obtain lose important details response generation especially dimension hidden state enough. reminder paper global encoding scheme referred nrm-glo. local scheme recently bahdanau graves introduced attention mechanism allows decoder dynamically select linearly combine different parts input sequence weighting factors determine part selected generate word turn function hidden states pictorially shown figure basically attention mechanism models alignment inputs around position output position viewed local matching model. local scheme devised automatic alignment source sentence partial target sentence machine translation. scheme enjoys advantage adaptively focusing important words input text according generated words response. local encoding scheme referred nrm-loc. task nrm-glo summarization entire post nrm-loc adaptively select important words post various suitable responses. since post-response pairs strictly parallel word different context different meanings conjecture global representation nrm-glo provide useful context extracting local context therefore complementary scheme nrm-loc. therefore natural extension combine models concatenating encoded hidden states form extended hidden representation time stamp illustrated figure summarization incorporated provide global context local matching. hybrid method hope local global information introduced generation response. model context generation mechanism denoted nrm-hyb. noticed context generator nrm-hyb evoke different encoding mechanisms global encoder local encoder although combined later forming uniﬁed representation. speciﬁcally last hidden state nrm-glo plays role different last state nrm-loc since responsibility encode entire input sentence. role nrm-glo however tends adequately emphasized training hybrid encoder parameters encoding rnns learned jointly scratch. following trick ﬁrst initialize nrm-hyb parameters nrm-loc nrm-glo trained separately tune parameters encoder along training parameters decoder. learn parameters model maximize likelihood observing original response conditioned post training set. post nrms generate responses using left-to-right beam search beam size stanford chinese word segmenter split posts responses sequences words. although posts responses written language distributions words different number unique words post text response text therefore construct separate vocabularies posts responses using frequent words side covering usage words post response respectively. remaining words replaced special token unk. dimensions hidden states encoder decoder dimensions word-embedding post response model parameters initialized randomly sampling uniform distribution models trained nvidia tesla using stochastic gradient descent algorithm mini-batch. training stage model took weeks. retrieval-based retrieval-based models given post response retrieved post-response pairs repository. models rely three components repository sets feature functions machine learning model combine features. work whole million weibo pairs used repository features ranging simple cosine similarity deep matching models used determine suitability post given post following linear model following ranking strategy pick posts retrieved responses given baseline retriever repository manually label obtain labeled post-response pairs. ranking model parameters based labeled dataset. comparison response considered evaluation process. smt-based smt-based models post-response pairs directly used parallel data training translation model. widely used open-source phrase-based translation modelmoses another parallel data consisting post-response pairs used tune system. authors used modiﬁed model obtain response twitter stimulus. main modiﬁcation replacing standard giza++ word alignment model phrase-pair selection method possible phrasepairs training data considered associated probabilities estimated fisher’s exact test yields performance slightly better default setting. compared retrieval-based methods generated responses smt-based methods often ﬂuency even grammatical problems. work choose moses default settings model. automatic evaluation response generation still open problem. widely accepted evaluation methods translation apply since range suitable responses large practically impossible give reference adequate coverage. also reasonable evaluate perplexity generally used measurement statistical language modeling naturalness response relatedness post well evaluated. therefore resort human judgement similar taken important difference. adopt human annotation compare performance different models. five labelers least three-year experience sina weibo invited human evaluation. responses obtained evaluated models pooled randomly permuted labeler. labelers instructed imagine authors original posts judge whether response appropriate natural input post. three levels assigned response scores figure example post candidate responses human annotation. content post implies football match already started author response still waiting match start. response talks food italy. response widely used response suitable post. response states current score still suitable response speciﬁc scenario. suitable response evidently appropriate natural response post; neutral response suitable response speciﬁc scenario; unsuitable hard impossible scenario response suitable. make annotation task operable suitability generated responses judged follow logic consistency responses logically consistent test post; semantic relevance responses semantically relevant test post; scenario dependence responses depend speciﬁc scenario contradict generality responses general contradict ﬁrst three criteria; ﬁrst three criteria contradicted generated response labeled unsuitable. responses general suitable post speciﬁc scenario labeled neutral. figure shows example labeling results post responses. ﬁrst responses labeled unsuitable logic consistency semantic relevance errors. response depends scenario therefore annotated neutral. test consists posts appear training length chinese words words average. experimental results based human annotation summarized table consisting ratio three categories agreement among labelers model. agreement evaluated fleiss’ kappa statistical measure inter-rater consistency. except smt-based model value agreement range models interpreted fair agreement. smt-based model relatively higher kappa value larger considered moderate agreement since responses generated often ﬂuency grammatical errors making easy reach agreement unsuitable cases. table method performs signiﬁcantly worse retrieval-based models generated responses labeled unsuitable mainly ﬂuency relevance errors. observation conﬁrms intuition dataset post potentially corresponding many responses simply taken parallel corpus model. surprisingly responses generated three labeled suitable neutral means generated responses ﬂuent semantically relevant post. among variants nrm-loc outperforms nrm-glo suggesting dynamically generated context might effective static ﬁxed-length vector entire post consistent observation made machine translation; retrieval-based model similar mean score nrm-glo ratio neutral cases outperforms methods. responses retrieved retrieval-based method actually wrote human suffer grammatical ﬂuency problems combination various feature functions potentially makes sure picked responses semantically relevant test posts. however picked responses customized test posts ratio suitable cases lower three neural generation models. test statistical signiﬁcance friedman test non-parametric test differences several related samples based ranking. table shows average rankings annotations corresponding p-values comparisons different pairs methods. comparison retrieval-based nrm-glo signiﬁcant difference ranking tiny. indicates retrieval-based method comparable nrm-glo method. nrmhyb outperforms methods difference statistically signiﬁcant difference nrm-loc retrieval-based method marginal signiﬁcantly worse retrieval-based nrm-hyb methods. figure shows example responses generated nrms comparable retrieval-based model. intriguing notice three variants give suitable quite distinct responses different perspectives choices words. this conjecture caused architecture variations among models well variations random effects like initialization parameters. another interesting observation forth example retrieval-based method returns response mismatched entity name wenshan actually quite common problem retrieval-based model inconsistency details often render response unsuitable cannot adequately considered matching function employed retrieving responses. contrast observe nrms tend make general response barely generate details. among keep best ﬁrst word. seen responses ﬂuent relevant post still vastly different other validating initial conjecture fueled large rich training corpus could work generator cover modes density estimation. paper explored using encoder-decoder-based neural network system coined name neural responding machine generate responses post. empirical studies conﬁrm newly proposed nrms especially hybrid encoding scheme outperform state-of-the-art retrieval-based smtbased methods. preliminary study also shows generate multiple responses great variety given post. future work would consider adding intention users external signal decoder generate responses speciﬁc goals.", "year": 2015}