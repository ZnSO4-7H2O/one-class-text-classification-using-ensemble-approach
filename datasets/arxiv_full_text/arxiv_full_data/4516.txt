{"title": "A Unified Semi-Supervised Dimensionality Reduction Framework for  Manifold Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "We present a general framework of semi-supervised dimensionality reduction for manifold learning which naturally generalizes existing supervised and unsupervised learning frameworks which apply the spectral decomposition. Algorithms derived under our framework are able to employ both labeled and unlabeled examples and are able to handle complex problems where data form separate clusters of manifolds. Our framework offers simple views, explains relationships among existing frameworks and provides further extensions which can improve existing algorithms. Furthermore, a new semi-supervised kernelization framework called ``KPCA trick'' is proposed to handle non-linear problems.", "text": "present general framework semi-supervised dimensionality reduction manifold learning naturally generalizes existing supervised unsupervised learning frameworks apply spectral decomposition. algorithms derived framework able employ labeled unlabeled examples able handle complex problems data form separate clusters manifolds. framework oﬀers simple views explains relationships among existing frameworks provides extensions improve existing algorithms. furthermore semi-supervised kernelization framework called kpca trick proposed handle non-linear problems. many real-world applications high-dimensional data indeed low-dimensional subspace. goal dimensionality reduction reduce complexity input data desired intrinsic information data preserved. desired information discriminative geometrical fisher discriminant analysis popular method among supervised dimensionality reduction algorithms. denote number classes given training set. provided training examples class linear subspace form several separate clusters i.e. form multi-modality able discover eﬃcient classiﬁcation. recently many works improved algorithm several aspects extended algorithms able discover nice low-dimensional subspace even training examples class separate clusters complicated non-linear manifolds. although extended algorithms work reasonably well considerable number labeled examples required achieve satisﬁable performance. many real-world applications image classiﬁcation page classiﬁcation protein function prediction labeling process costly time consuming; contrast unlabeled examples easily obtained. therefore situations beneﬁcial incorporate information contained paper present general semi-supervised dimensionality reduction framework able employ information labeled unlabeled examples. contributions paper summarized follows. able discover nice low-dimensional subspace even training examples class form separate clusters complicated non-linear manifolds. fact previous supervised algorithms casted instances framework. moreover framework explains previously unclear relationships among existing algorithms simple viewpoint. improves unlabeled examples previous algorithms. experiments show hadamard power operator improves classiﬁcation performance semi-supervised learner derived framework. tral decompositions known viewed special cases framework. moreover empirical evidence shows semi-supervised learners derived framework superior existing learners many standard problems. extended semi-supervised learning setting. contrast standard kernel trick kpca trick require users derive mathematical formulas re-implement kernel version original learner. denote training labeled examples inputs yi}ℓ generated ﬁxed unknown probability distribution corresponding class labels generated py|x. addition labeled examples {xi}ℓ+u i=ℓ+ denote unlabeled examples also generated denote matrix input examples here following previous works supervised setting nearest neighbor algorithm used representing simple classiﬁer mentioned goal. note important special cases problems transductive i=ℓ+ given unlabeled examples. order make unlabeled examples learning process make following so-called manifold assumption tions determine good embedded points are; thus arguments matrix embedded points. orthogonal translational transformations identify embedded points pairwise distances instead individual locations. therefore base objective functions pairwise distances embedded examples. here deﬁne objective functions linear respect pairwise distances dist arbitrary distance function embedded points costs penalize embedded distance points speciﬁcation based label information unlabel information respectively described section restrict consider cases dist squared euclidean distance function i.e. dist kaxi axjk symmetric form abat positive semideﬁnite matrix result general framework indeed generalizes previous frameworks shown section deﬁne rewrite weighted combination objective funtions follows note constraint abat prevents trivial solutions every zero vector. positive deﬁnite matrix solution problem given bottom eigenvectors following generalized eigenvalue problem section present various reasonable approaches specifying cost matrices constraint matrix using label unlabel information. words unlabel information neighborhood information interchangeably paper. figure example data form multi-modal structure. algorithm e.g. imposes condition discover subspace merges clusters altogether. obtained space undesirable data classes mixed together. contrast algorithm imposes condition discover subspace merge clusters nearby examples clusters. conditions imposed classical works fda. however ﬁrst condition restrictive capture manifold multi-modal structures data naturally arise applications. thus ﬁrst condition relaxed follows nearby examples deﬁned using neighborhood information examples stay close original embedded spaces. speciﬁcation nearby examples proven successful discovering manifold multi-modal structure figure explanations. cases also appropriate relax second condition section give three examples cost matrices satisfy conditions three examples recently introduced previous works namely discriminant neighborhood embedding marginal fisher analysis local fisher discriminant analysis diﬀerent presentations motivations uniﬁed general framework. firstly specify nearby examples construct matrices based sensible distance eigi nearest neighbors label eige nearest neighbors diﬀerent labels deﬁne follows points and/or unlabeled within framework relationships among three previous works explained. three methods exploit diﬀerent ideas specifying matrices satisfy desirable conditions embedded space. designed penalize embedded space satisfy condition constraint matrix designed satisfy condition designed penalize embedded space satisfy condition things quite obvious case lfda. lfda constraint matrix designed satisfy condition since elements proportional nevertheless since weights inversely proportional elements small class larger weights elements bigger class i.e. pair small class likely satisfy condition pair bigger class. understand recall also cost meant preserve pairwise distance pair examples lfda tries preserve local geometrical structure pair nearby examples class contrast squeeze nearby examples class single point note recent supervised methods manifold learning also presented interpreted framework diﬀerent speciﬁcations examples local discriminant embedding chen supervised nonlinear local embedding cheng important implication manifold assumption nearby examples likely belong class. hence assumption makes sense design prevents pairs nearby examples stay apart embedded space. among methods extracting neighborhood information deﬁne methods based heat kernel popular. beside using heat kernel methods deﬁning invented details. simplest speciﬁcations nearby examples based heat kernel pair nearby examples penalized diﬀerent costs depended similarity similarity points based euclidean distance input space. incidentally speciﬁcation term interpreted approximation laplace-beltrami operator data manifold. learner employs named locally preserving projection using local scaling method proven eﬃcient previous experiments clustering. speciﬁcation deﬁne local scale point usually convenient speciﬁcation since space possible choices considerably smaller instead proposing another method specify cost matrix present novel method used modify existing cost matrix. matrices equal size elements. recall hadamard product linear parameterization however obtain linear subspace deﬁned learning non-linear subspace accomplished standard kernel trick however applying kernel trick inconvenient since mathematical formulas derived implementation done separately linear implementations. recently chatpatanasiri proposed alternative kernelization framework called kpca trick require user derive mathematical formula re-implement kernelized algorithm. moreover kpca trick framework avoids troublesome problems singularity etc. plemented semi-supervised learning framework. kernel function associated non-linear function hilbert space. denote central idea kpca trick represent ﬁnite-dimensional space dimensionality bounded without loss information. within framework coordinate example computed explicitly example coordinate used input existing semi-supervised learner without re-implementations. note although unable numerically represent inner-product conveniently computed kpca principal component feature space. likewise test point mapped consequently mapped data {ϕi} ﬁnite-dimensional explicitly computed. kpca-trick algorithm consisting three simple steps shown figure semi-supervised learners kernelized simple algorithm. algorithm denote semi-supervised learner outputs best linear within formulation corresponding optimal solution invariant non-singular linear transformation; i.e. optimal solution also optimal solution non-singular rd×d note four choices assign weight axis natural diagonal matrix i.e. normalizes axis equally important diagonal matrix matrices deﬁned subsection algorithms ss-mfa ss-lfda guarantee positive semideﬁnite positive deﬁnite i.e. full-rank. case singular cannot immediately apply solve optimization problems. common solve diﬃculty value guaranteed full-rank instead eq.. since acts role regularizer makes sense regularization parameter speciﬁed section similar settings also used existing algorithms e.g. although matrix optimize optimization problem usually overﬁts given data. possible solution problem apply given data ﬁrst place resulted data dimensionality already described section framework generalizes various existing supervised unsupervised manifold learners kpca trick ﬁeld semi-supervised learning. supervised manifold learners cannot represented framework cost functions linear respect distances among examples. extension algorithms handle semi-supervised learning problems interesting future work. yang present another semi-supervised learning framework solves entirely diﬀerent problems problems considered paper. propose extend unsupervised algorithms isomap laplacian eigenmap cases information exact locations points available. best knowledge currently existing semi-supervised dimensionality reduction frameworks literatures similar goal ours; recently proposed. here subsequently show frameworks restated special cases framework. sugiyama extends lfda algorithm handle semi-supervised learning problem adding objective function objective function lfda described section describe sugiyama al.’s algorithm namely ‘self’ without loss generality assume i=kaxik. sugiyama propose solve followgorithm tries preserve global structure input data convey manifold assumption local structure preserved. therefore input unlabeled data complicated manifold appropriate apply song propose extend another algorithm named maximum margin criterion handle semi-supervised learning problem. idea semi-supervised learning extension similar ss-fda ss-mmc respectively). however ss-fda ss-mmc cannot handle problems data class form manifold several clusters shown figure ss-fda ss-mmc satisfy condition fact ss-fda ss-mmc restated instances framework. this note optimization problem ss-mmc stated figure ﬁrst example. projection axes three algorithms namely lfda presented. circles crosses denote labeled examples small circles small crosses denote unlabeled examples. percentage accuracy unlabeled examples shown top. section explain self ss-fda proposed sugiyama song described enough solve semisupervised learning problems even simple ones shown figure figure figure three dimensionality reduction algorithms lfda performed dataset. multi-modality cannot appropriate projection. since clusters contain data class tries preserve structure clusters also fails. case lfda proper projection since cope multi-modality take account labeled examples. note since ss-fda linearly combined algorithm projection lying projections discovered case ss-fda cannot eﬃcient projection unlike lfda course ss-lfda derived framework. similar argument given warn uncareful self situations. figure four dimensionality reduction algorithms lfda performed dataset. multi-modality cannot appropriate projection. also since labeled examples lfda fails good projection well. case proper projection since cope multi-modality take unlabeled examples account. note since self linearly combined algorithm lfda projection lying projections discovered lfda case self cannot correct projection unlike semi-supervised learner like sslfda derived framework which explained section employs since semi-supervised manifold learner derived framework intuitively thought combination supervised learner unsupervised learner. misunderstand semi-supervised learner cannot discover good subspace neither supervised learner unsupervised learner able discover good subspace. examples also mislead readers way. fact intuition incorrect. here give another example shown figure semi-supervised learner able discover good subspace neither supervised unsupervised counterparts. intuitively semi-supervised learner able exploit useful information labeled unlabeled examples. section classiﬁcation performances algorithms derived framework demonstrated. similar experimental setting previous works results compared them. experiments semi-supervised learners ss-lfda ss-dne derived framework compared relevant existing algorithms lpp* lfda self contrast standard apply hadamard power operator explained section denote lpp* variant applying hadamard power operator. non-linear semi-supervised manifold learning also experimented applying kpca trick algorithm illustrated figure since intention apply best kernel compare eﬃciency semi-supervised kernel learner base supervised kernel learners figure third example semi-supervised learner able good projection. undirected graph corresponding values used ss-lfda. ﬁgure pair examples link graph explains projects data axis shown left ﬁgure; apply label information tries choose projection axis squeezes clusters much possible. note apply local-scaling method specify using nearest neighbor algorithm discovered subspaces classiﬁcation performances experimented learners measured standard datasets shown table ﬁrst datasets obtained repository next datasets mainly designed testing semi-supervised learner obtained http//www.kyb.tuebingen.mpg.de/sslbook/benchmarks.html ﬁnal dataset extended yale standard dataset face recognition task. classiﬁcation performance algorithm measured average test accuracy realizations randomly splitting dataset training testing subsets. three parameters needed tuned order apply semi-supervised learner derived framework regularizer degree hadamard power operator kth-nearest neighbor parameter needed construct cost matrices. make learners satisfy condition described section clear small compared number training examples class experience found semi-supervised learners quite insensitive various small values therefore experiments simply parameters needed tuned. tune parameters cross validation. note needed tuned lpp* needed tuned self. ‘good neighbors’ score shown table sugiyama score simply deﬁned training accuracy nearest neighbor algorithm available data labeled given algorithm. note score used dimensionality reduction algorithm. clariﬁes usefulness unlabeled examples dataset readers. intuitively dataset gets high score unlabeled examples useful since indicates pair examples high penalty cost table details dataset denote numbers input features classes labeled examples unlabeled examples testing examples respectively. denotes transductive setting used small datasets examples labeled given unlabeled examples used testing examples well. determined using prior knowledge denotes target dimensionality dataset. good neighbors denotes quantity measures goodness unlabeled data dataset. belong class. note table scores dataset linear score given input space kernel measures score feature space corresponding nd-degree polynomial kernel. numerical results shown table case table case experiments ss-dne ss-lfda compared classiﬁcation performances unsupervised supervised counterparts lpp* ss-dne lpp* lfda ss-lfda. self also compared ss-lfda related semi-supervised learners originated lfda. algorithms highlighted superior counterpart opponents. results algorithms ss-lfda ss-dne outperform opponents comparisons ﬁrst setting small algorithms outperform opponents comparisons second setting large algorithms outperform opponents comparisons. consequently framework oﬀers semi-supervised learner consistently improves base supervised unsupervised learners. note number labeled examples increases usefulness unlabeled examples decreases. subsequently discuss analyze results dataset details next subsections. ionosphere real-world dataset radar pulses passing ionosphere collected system goose labrador. targets free electrons ionosphere. good radar returns showing evidence type structure ionosphere. returns not. since know true decision boundary ionosphere simply target dimensionality observed non-linearization improve classiﬁcation performance algorithms. table percentage accuracies ss-dne ss-lfda derived framework compared existing algorithms ss-lfda ss-dne highlighted outperform opponents superscripts indicate %-conﬁdence levels one-tailed paired t-test diﬀerences accuracies algorithms best opponents. superscripts denote conﬁdence levels observed lpp* much better dataset therefore unlike self ss-lfda much improves lfda. fact main reason ss-lfda ss-dne lpp* good classiﬁcation performances hadamard power operator. explained figures figures deﬁning nearby examples pair examples link almost every link connects nearby examples class indicates unlabel cost matrix quite accurate nearby examples rarely links. fact ratio good nearby examples total nearby examples nevertheless re-deﬁne nearby examples pairs examples having e.g. ratio reduces shown figure indicates many pairs examples small values diﬀerent classes since algorithm derived framework minimizes cost-weighted average distances every pair examples derivation) beneﬁcial increases cost pair large decreases cost pair small easily seen eﬀect hadamard power operator exactly need. good-nearby-examples ratios applying hadamard power operator illustrated figure notice that applying operator even pairs small values balance artiﬁcial dataset generated model psychological experimental results. example classiﬁed balance scale right left balanced. attributes containing integer values left weight left distance right weight right distance. correct class greater dataset illustrates another using classiﬁcation task. centering covariance matrix examples multiple identity matrix. therefore direction principal component largest variance return random direction hence cannot expect much classiﬁcation performance dataset. thus cannot help self improves much performance lfda sometimes self degrades performance lfda overﬁtting. dataset originates development brain-computer interface single person performed trials imagined movements either left hand right hand trial electroencephalography recorded electrodes. autoregressive model order ﬁtted resulting time series. trial represented total ﬁtted parameters. target dimensionality number classes similar previous datasets ss-lfda ss-dne usually able outperform opponents. again appropriate real-world dataset hence self inferior ss-lfda. benchmark derived famous usps dataset handwritten digit recognition. digit images randomly drawn. digits assigned ﬁrst class others form second class. prevent user employ domain knowledge data example rescaled noise added dimension masked pixel shuﬄed although classes dataset original data presumably form clusters digit. therefore target dimension often ss-lfda ss-dne outperform opponents. nevertheless note ss-lfda ss-dne improve much lfda labeled examples quite enough discriminating data therefore unlabeled examples oﬀer relatively small information semi-supervised learners. face recognition dataset derived extended yale human subjects poses illumination conditions. m-eyale randomly chose subjects images subject original dataset down-sampling example m-eyale consists classes class consists images randomly-chosen subjects. hence separated clusters class able advantage algorithms employing conditions explained section dataset number labeled examples class ﬁxed examples classes observed. since dataset consist clusters target dimensionality clear lpp* performs much better dataset. recall captures maximum-variance directions; nevertheless face recognition task maximum-variance directions discriminant directions directions lighting posing therefore captures totally wrong directions hence degrades performance self lfda. contrast lpp* much better captures local structures dataset discover much better subspaces. thus cooperating lpp* lfda ss-lfda ss-dne able obtain good performances. presented uniﬁed semi-supervised learning framework linear non-linear dimensionality reduction algorithms. advantages framework generalizes existing various supervised unsupervised semi-supervised learning frameworks employing spectral methods. empirical evidences showing satisﬁable performance algorithms derived framework reported standard datasets.", "year": 2008}