{"title": "Learning Probabilistic Programs Using Backpropagation", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Probabilistic modeling enables combining domain knowledge with learning from data, thereby supporting learning from fewer training instances than purely data-driven methods. However, learning probabilistic models is difficult and has not achieved the level of performance of methods such as deep neural networks on many tasks. In this paper, we attempt to address this issue by presenting a method for learning the parameters of a probabilistic program using backpropagation. Our approach opens the possibility to building deep probabilistic programming models that are trained in a similar way to neural networks.", "text": "data thereby supporting learning fewer training instances purely data-driven methods. however learning probabilistic models diﬃcult achieved level performance methods deep neural networks many tasks. paper attempt address issue presenting method learning parameters probabilistic program using backpropagation. approach opens possibility building deep probabilistic programming models trained similar neural networks. biggest attractions probabilistic modeling enables combine domain knowledge learning data. neural networks proven extremely eﬀective learning usually require data. ability incorporate explicit domain knowledge could signiﬁcantly reduce amount data required. another appealing property probabilistic models interpretable often easier explain neural networks. probabilistic programming provides modular composable expressive framework representing probabilistic models. however probabilistic programs generally considered good neural networks learning machines possibly inference learning complex models quite hard. paper attempt address issue providing learn probabilistic programs using back-propagation similar neural networks. hope provide probabilistic programs similar scalability properties neural networks. second beneﬁt provide framework allows entire range possibilities purely knowledgedriven purely data-driven everything between represented reasoned with learned coherent way. tional probability distributions bayesian network known structure incomplete observations using gradient descent. binder also showed parameterized representations cpts noisy-or linear gaussians also showed handle situations parameters used dynamic models. work everything needed learn probabilistic program using gradient descent. unfortunately binder al’s method requires performing inference compute gradient. inference expensive probabilistic models prohibitive inner loop gradient descent algorithm. result method largely ignored learning bayesian networks practitioners using also requires inference step usually requires fewer iterations gradient descent converge. however also considered slow algorithm ultimately unsatisfying. paper describe method learning parameters probabilistic program using gradient descent require inference. required ability generate samples prior distribution program easy. iteration algorithm number samples generated. subset program necessary generation samples expanded. samples approximate representation distribution deﬁned program. loss function compares empirical data distribution distribution formulated. gradient loss respect program parameters computed using reverse-mode automatic diﬀerentiation backpropagation. generative adversarial nets generative neural networks demonstrated considerable success unsupervised learning generative probabilistic models rich data sets. however gans interpretable hard include prior knowledge network unlike probabilistic program. also minor point gans don’t support inference directly inference performed auxiliary network. stan based reverse-mode automatic diﬀerentiation like ours. however stan uses hamiltonian monte carlo whereas back-propagation. stan also requires restricted form model stan requires model diﬀerentiable respect variables model. framework model non-diﬀerentiable even discontinuous respect variables. need diﬀerentiable respect learnable parameters model. makes applicable general-purpose probabilistic programming languages like church. edward probabilistic programming language enables explicit representation inference models. inference models implemented tensor flow enabling many scalability beneﬁts neural networks. main diﬀerence edward approach edward requires inference model written explicitly whereas approach black inference worked automatically. specify inference model advantage enables encode algorithms couldn’t easily derived. however might hard scale complex models would particularly diﬃcult non-experts use. beydin wood neural networks support inference probabilistic programs speciﬁcally help create good proposals sequential importance sampling. work diﬀers learn probabilistic program rather auxiliary network assists inference. naturally possible give ordinary neural networks bayesian interpretation bayesian deep learning variety speciﬁc forms bayesian generative neural network frameworks trainable backpropagation developed deep belief nets deep boltzmann machines deep generative models deep generative stochastic networks contrast approach frameworks deﬁnes speciﬁc kind neural network given structure. contrast general framework applies programs generic probabilistic programming language. start functional probabilistic programming language like church figaro. parameters language designated learned method. example expression might learnable parameter. similarly expression representing conditional probability distribution categorical variables entries conditional probability table could learnable parameters. parameters continuous distributions mean variance gaussian could also learnable. optionally prior-like function provided learnable parameter. prior interpreted regularization function higher values prior preferred. require every construct language three things done sample value expression given arguments; compute conditional density value given arguments; compute derivative conditional density respect parameter. conditional density needs diﬀerentiable respect parameter respect variables model. probabilistic program deﬁnes probability density function values produced program. deﬁne error function program measures loosely speaking well density function models training set. stochastic gradient descent learn parameters program minimize error function. iteration gradient descent generate samples distribution deﬁned program using current parameter values. estimate error function using samples. addition generating speciﬁc samples restrict attention expressions evaluated generate samples rather full expressions evaluated inﬁnite. result sampling data structure call parameterized probabilistic network reverse mode automatic diﬀerentiation compute gradient. result backpropagation algorithm. turns compute derivative exactly need compute derivative conditional density node given ancestor analysis needed determine exactly conditional densities need computed. approximation however assume parents node independent purpose derivative computation results simple eﬃcient algorithm. assume church-like functional probabilistic programming language parameters designated learned method. require primitives language diﬀerentiable respect parameters compute densities derivatives using primitives. goal learning minimize loss function respect parameters. turns traditional loss functions like negative likelihood data respect model don’t work well method natural loss functions deﬁned. probabilistic program create network trainable using backpropagation. call network parameterized probabilistic network consist directed acyclic graph. associated node function diﬀerentiable parameter also given calculate value addition parameters also prior prior thought regularization function rather true prior bayesian sense; indicates much particular value preferred. unspeciﬁed sink graph output node. multiple output nodes. represent output nodes. output node given local error function error assumed additive across diﬀerent values additive across output nodes overall loss network deﬁned expected value error typical formulation loss negative likelihood data given model cannot used decompose additively diﬀerent values instead natural formulation error function negative probability value according data. example parzen windows create training data estimate density point choose sample data deﬁne quires speciﬁc values nodes network. since generative model sample values nodes network. probabilistic programming nodes large potentially inﬁnite given samples instantiate subset nodes long program terminates probability one. forward propagation generate samples probabilistic program. process creating program node representing expression gets evaluated. expression gets evaluated multiple samples represented node. node distinct values generated number samples expression represented evaluated number times value generated. similarly nodes number samples joint value appears. process generation also create topological order nodes according order evaluated. third step holds depend therefore reduced problem computing derivative probability density particular value respect parameters actually generalize slightly providing method compute derivative conditional probability density value variable given speciﬁc value ancestors variable. reduced problem compute partial derivative conditional probability density. however variable variables might conditionally independent given therefore introduce notion sepset sepset given nodes variables conditionally independent given rewriting last line reduce complexity algorithm performing multiplication separately initial summation. reduced problem computing partial derivative conditional probability density value variable given speciﬁc value ancestors recurse. done equation computed times. according equation cost computing equation computes every output node every sample total cost computing equation cost computing equations cost computing equation question many times equations computed. seem trivial work graph theoretic property. hypothesize it’s exponential size largest sepset analysis needed. total number times overall cost performing step gradient descent complexity computing gradient exactly high approximation scheme smaller sepsets. extreme version empty sepsets assuming parents independent purpose computing gradient. need compute derivatives unconditional pdfs. proceeding equation compute equations computed times cost each. therefore total cost algorithm iteration gradient descent since number output nodes total number nodes summarized thus complexity linear number nodes network number samples taken number training instances used iteration maximum number parents node. note although scheme assumes parents independent purposes backpropagation dependencies parents taken account generating samples. approximation scheme perform practical problems open question. similar frameworks approximate inference graphical models like mini-buckets could also consider whole range approximations limit size sepsets allowed. discuss possibility here. important questions method whether provide similar scalability learning deep neural networks. particular issues solved deep nets vanishing gradient network becomes deep convergence learning local minima. hope sampled training batches help local minima problem. vanishing gradient problem question sensitivity output network parameters early network. early variable critical output sensitivity high method discover this. also opposed traditional neural networks variables closest data network vanishing gradient problem less signiﬁcant anyway. even model learned using requires inference. inference probabilistic models hard advantage neural networks. however signiﬁcant strides made area last years including using neural networks inference process itself. method orthogonal work inference; beneﬁt advances inference area. might relying inference anyway full bayesian inference learn parameters model inference? experience simultaneously learning parameters high dimensional space perform probabilistic inference underlying model often intractable. parameters learned separately underlying inference often much easier. goal evaluate approach across number dimensions. first natural point comparison stan. stan also uses reverse mode automatic diﬀerentiation purpose inference using hamiltonian monte carlo. also stan implements limited language model primitives need diﬀerentiable model variables parameters. would like compare eﬃciency accuracy learning using approach stan’s also whether approach scale models cannot easily represented stan. stan heavily engineered efﬁciency comparison require eﬀort part create eﬃcient implementation. second natural comparison gans. gans generative neural network models proven highly eﬀective generating data. hope enabling domain knowledge explicitly included models possible train models using less data. ultimately interesting whether possible represent learn kinds deep probabilistic models framework. example imagine functional lda-style model. topic consists probability distribution transformations data structure image graph. generative model consists choosing topic choosing sequence transformations topic applying sequentially starting value using function composition. transformations probabilistic functions parameterized parameters learned using methods paper. general model special case.", "year": 2017}