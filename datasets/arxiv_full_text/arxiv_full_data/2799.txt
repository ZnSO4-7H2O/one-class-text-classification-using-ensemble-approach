{"title": "tau-FPL: Tolerance-Constrained Learning in Linear Time", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Learning a classifier with control on the false-positive rate plays a critical role in many machine learning applications. Existing approaches either introduce prior knowledge dependent label cost or tune parameters based on traditional classifiers, which lack consistency in methodology because they do not strictly adhere to the false-positive rate constraint. In this paper, we propose a novel scoring-thresholding approach, tau-False Positive Learning (tau-FPL) to address this problem. We show the scoring problem which takes the false-positive rate tolerance into accounts can be efficiently solved in linear time, also an out-of-bootstrap thresholding method can transform the learned ranking function into a low false-positive classifier. Both theoretical analysis and experimental results show superior performance of the proposed tau-FPL over existing approaches.", "text": "hongyuan college computing georgia institute technology atlanta school computer science software engineering east china normal university shanghai china learning classiﬁer control false-positive rate plays critical role many machine learning applications. existing approaches either introduce prior knowledge dependent label cost tune parameters based traditional classiﬁers lack consistency methodology strictly adhere false-positive rate constraint. paper propose novel scoring-thresholding approach -false positive learning address problem. show scoring problem takes false-positive rate tolerance accounts eﬃciently solved linear time also out-of-bootstrap thresholding method transform learned ranking function false-positive classiﬁer. theoretical analysis experimental results show superior performance proposed -fpl existing approaches. keywords neyman-pearson classiﬁcation false positive rate control bipartite ranking partial-auc optimization euclidean projection real-world applications spam ﬁltering medical diagnosing loss misclassifying positive instance negative instance rather diﬀerent. instance medical diagnosing misdiagnosing pa∗. preliminary version work appeared proceedings thirty-second aaai conference tient healthy dangerous misclassifying healthy person sick. meanwhile reality often diﬃcult deﬁne accurate cost kinds errors situations desirable keep classiﬁer working small tolerance false-positive rate i.e. allow classiﬁer misclassify larger percent negative instances. traditional classiﬁers trained maximizing classiﬁcation accuracy suitable mismatched goal. literature classiﬁcation constrained false-positive rate known neymanpearson classiﬁcation problem existing approaches roughly grouped several categories. common approach cost-sensitive learning assigns diﬀerent costs diﬀerent classes representatives include cost-sensitive cost-interval cost-sensitive boosting though eﬀective eﬃcient handling diﬀerent misclassiﬁcation costs usually diﬃcult appropriate misclassiﬁcation cost speciﬁc tolerance. another group methods formulates problem constrained optimization problem tolerance explicit constraint methods often need saddle point lagrange function leading time-consuming alternate optimization. moreover surrogate loss often used simplify optimization problem possibly making tolerance constraint satisﬁed practice. third line research scoring-thresholding methods train scoring function ﬁrst threshold meet target tolerance practice scoring function trained either class conditional density estimation bipartite ranking however computing density estimation another diﬃcult problem. also bipartite ranking methods less scalable super-linear training complexity. additionally methods paying special attention positive class. example asymmetric maximizes margin negative samples core positive samples one-class ﬁnds smallest ball enclose positive samples. however incorporate tolerance learning procedure either. paper address tolerance constrained learning problem proposing false positive learning speciﬁcally -fpl scoring-thresholding method. scoring stage explicitly learn ranking function optimizes probability ranking positive instance centroid worst percent negative instances. whereafter shown that help newly proposed euclidean projection algorithm ranking problem solved linear time projected gradient framework. worth noting euclidean projection problem generalization large family projection problems proposed linear-time algorithm based bisection divide-and-conquer three orders faster existing state-of-the-art methods. thresholding stage devise out-of-bootstrap thresholding method transform aforementioned ranking function false-positive classiﬁer. method much less prone overﬁtting compared existing thresholding methods. theoretical analysis experimental results show proposed method achieves superior performance existing approaches. false-positive rate. consider following neyman-pearson classiﬁcation problem aims minimizing false negative rate classiﬁer constraint false positive rate proposition deﬁne j-th largest value multiset ﬂoor function. constrained optimization problem share optimal solution optimal objective value following ranking problem prefers scores positive examples exceed mean scores worst proportion negative examples. optimization equivalent original problem general cases equality also could hold scores worst -proportion negative examples same. tight convex upper bound original minimization problem contrast cost-sensitive classiﬁcation oﬀer insecure lower bound. upper bound also tighter convex relaxation original constrained minimization problem introducing algorithm brieﬂy review related approaches constrained learning show approach seen tighter upper bound meanmaintain linear training complexity. cost sensitive learning alternative approach eliminating constraint approximate introducing asymmetric costs diﬀerent types error classiﬁcation learning framework words cost-sensitive learning methods insecure setting mismatched easily violate constraint make excessive requirements optimal speciﬁed known solving original constrained problem proved np-hard. practice also make continuous approximation constraint tractable cost-sensitive training multi-level approximation makes relationship problem original unclear blocks straightforward theoretical justiﬁcation. standard solution approach relies replacing convex surrogate function obtain convex constrained optimization problem interestingly proposition shows whichever surrogate function hypothesis chosen resulting constrained optimization problem weaker upper bound approach. thus general exact risk convex constrained optimization risk practice prefer tighter approximation since represents original objective better. moreover theorem also achieves tighter bound generalization error rate considering empirical risk ranking-thresholding traditional bipartite ranking methods usually superlinear training complexity. compared them main advantage algorithm comes linear-time conplexity iteration without convergence rate named algorithm -fpl give detailed description works next sections. higher score positive instances centroid percent negative instances. thresholding suitable threshold chosen ﬁnal prediction instance obtained precisely approximating indicator function constraint choosing bounded loss sigmoid ramp loss etc. seems appealing. however bounded functions always bring non-convex property corresponding problems usually np-hard diﬃculties limit eﬃcient training eﬀective theoretical guarantee learned model. simplicity conjugation. steps summarized algorithm iteration ﬁrst update solution gradient objective function project dual solution onto feasible sequel show projection problem eﬃciently solved linear time. practice since smooth also leverage nesterov’s method accelerate convergence algorithm. nesterov’s method achieves convergence rate smooth objective function number iterations. c/k. several eﬃcient methods based median-selecting variable ﬁxing techniques available hand upper bounded constraints automatically satisﬁed omitted. special case studied example achieve complexity. unfortunately none methods directly applied solving generalized case property unﬁxed upper-bound constraint knowledge attempt address problem unﬁxed upper bound solve similar problem based sorting exhaustive search method achieves runtime complexity super-linear even quadratic linearly dependent. contrast proposed method applied aforementioned special cases minor changes remains complexity. notable characteristic method eﬃcient combination bisection divide-and-conquer former oﬀers guarantee worst complexity latter signiﬁcantly reduces large constant factor bisection method. based theorem projection problem solved ﬁnding value three dual variables satisfy linear system. ﬁrst propose basic bisection method guarantees worst time complexity. similar method also used brevity denote i-largest dimension respectively deﬁne function follows main idea leveraging bisection solve system theorem root order make bisection work need three conditions continuous; root eﬃciently bracketed interval; value endpoints interval opposite signs. fortunately based following three lemmas requirements satisﬁed. current compute computing completed well-designed median-selecting algorithm current hand evaluate sign determine bound addition special case checked using lemma linear-time k-largest element selecting algorithm since bound maximum tolerance error. thus worst runtime algorithm furthermore also leverage convexity improve algorithm please refer details related techniques. although bisection solves projections linear time lead slow convergence technique beneﬁts exploiting monotonicity functions stated lemma notice that method also used ﬁnding root arbitary piecewise linear monotone function without requirement convexity. improved method endpoints divide conquer lemma reveals important chain monotonicity dual variables used improve performance baseline method. steps summarized algorithm denote value variable iteration instance emma δt−. implies uncertainty intervals interval shrinking lengths four intervals reduced simultaneously. hand notice indeed piecewise linear function computation value contains comparison discard elements current bound advance iteration reduce expected comparison counts half. complex similar procedure also applied computing functions piecewise linear main cost comparison endpoints. result approximately linear function evenly distributed breakpoints ﬁrst iteration bisection costs time finally computational cost iteration dominated gradient evaluation projection step. since complexity projection step cost computing gradient combining theorem that \u0001-suboptimal solution total computational complexity -fpl table compares computational complexity -fpl staten)/ of-the-art methods. order validation complexity corresponds number hyperparameters. this easy -fpl asymptotically eﬃcient. running multiple rounds make training data. process completed obtain ﬁnal threshold averaging. hand ﬁnal scoring function obtained ways learn scoring function using full training data gather weights learned previous round average them. method combines advantages out-of-bootstrap soft-thresholding techniques accurate error estimation reduced variance little sacriﬁce bias thus setting thresholding near risk area. bility giving positive instances higher score proportion negative instances. using i.e. ex−∼p− measure quality px+∼p+ probability giving positive instances lower score percent negative instances. following theorem bounds empirical loss l¯k. theorem given training data consisting independent instances distribution independent instances distribution optimal solution problem assume have proper probability least theorem implies upper bounded o/m)) probability ranking positive samples percent negative samples also bounded o/m)). approaching inﬁnity would close means case almost ensure thresholding suitable point true-positive rate close moreover observe play diﬀerent roles bound. instance well known largest absolute value gaussian random instances grows log. thus believe growth slightly aﬀects largest centroid top-proportion scores negatives samples. leads conclusion increasing slightly raise signiﬁcant reduce margin target k/n. hand increasing reduce upper bound thus increasing chance ﬁnding positive instances top. control respectively. solve projection problem. comparing method ibis improved bisection algorithm also makes convexity monotonicity. experiments running intel core processor. shown fig. thanks eﬃcient reduction constant factor method outperforms ibis saving almost running time limit case. also solve projection problem proposed using simpliﬁed version method compare method presented whose complexity kn). observe fig. method linear complexity regarding suﬀer growth limit case three-order magnitude faster competitors. proportion negative samples. considering ranking performance independently avoid practical problem mismatching constraint testing always oﬀer optimal threshold. table ranking performance diﬀerent values tolerance number positive/negative instances feature dimensions shown together name dataset. best results shown bold. ’n/a’s denote experiments require week training. bound approximation toppush ranking focus optimizing absolute ranking list also special case model svmpauc general method designed optimizing arbitrary partial-auc. test version algorithms -rank -rank correspond diﬀerent choice learning scheme. intuitively enlarge training phase seen top-down approximation—from upper bound original objective hand reason choosing that roughly speaking average score proportion negative samples close score n-th largest negative sample. hold-out tests carried data train data test set. large datasets instead rounds. round hyper-parameters chosen -fold cross validation grid search scope extended optimal boundary. results. table reports experimental results. note cases proposed method outperforms peer methods. conﬁrms theoretical analysis methods extract capacity model better. toppush highly-competitive case extremely small gradually lose advantage increase. algorithm svmpauc based cutting-plane methods exponential number constraints similar technologies also used many ranking structured prediction methods e.g. structured time complexity kind methods log) found even thousands training samples hard ﬁnish experiments allowed time. section compare performance diﬀerent models jointly learning scoring function threshold training phase i.e. output classiﬁer. evaluate classiﬁer maximum tolerance neyman-pearson score np-score deﬁned rate true-positive rate classiﬁer respectively maximum tolerance. table np-score] real-world datasets diﬀerent values tolerance leftmost column number positive/negative instances feature dimensions dataset. dataset best results shown bold. settings. similar setting classiﬁcation ranking experiments i.e. small scale datasets times stratiﬁed hold-out tests carried out; large datasets instead rounds. comparison baselines include cost-sensitive logistic regression choose surrogate function diﬀerent cs-svm; bias-shifting support vector machine ﬁrst training standard tuning threshold meet speciﬁed false-positive rate; cost-sensitive complete comparison also construct cs-svm out-of-bootstrap thresholding eliminate possible performance gains comes diﬀerent thresholding method focus training algorithm itself. comparing methods hyperparameters selected -fold cross-validation grid search aims minimizing np-score search scope extended optimal value boundary. -fpl ranking stage regularization parameter selected minimize threshold chosen minimize np-score. test variants algorithms -fpl -fpl corresponding diﬀerent choice learning scheme. mentioned previously enlarge seen top-down approximation towards original objective. results. np-score results given table first note methods achieve best performance tests compared various comparing methods. moreover clear even using method select threshold performance cost sensitive method still limited. another observation three algorithms using out-of-bootstrap thresholding eﬃciently control false positive rate constraint. moreover -fpls stable algorithms thus results datasets data size running -fpl ranking algorithm datasets diﬀerent optimal report corresponding training time. up-sampling technology ensures that ﬁxed datasets share optimal regularization parameter thus unique variable ﬁxed data size. figure shows log-log plot training time -fpl versus size training data diﬀerent lines correspond diﬀerent clear training time -fpl indeed linear dependent number training data. consistent theoretical analysis also demonstrate scalability -fpl. proof based turing-reduction following maximum agreement problem maximum agreement problem. deﬁne point {+−} reach agreement given data ...} contains samples maximum clear fact equivalent problem binary classiﬁcation using loss hyperplane hypothesis set. general solving approximately maximizing agreements within constant factor np-hard introduce decision problem version oracle algorithm dp-map solve applying bisection search take maximum value output solution. overall number calling log. naive reduction shows dp-map also np-hard. precisely np-complete problem. lemma determine exists hyperplane reaches least agreements calling once. output minimum value less hyperplane learned exactly corresponds hyperplane reaches enough agreements otherwise hyperplane. thus complete reduction. obtained details. remark problem deﬁnition proof implicitly suppose minimum attained. general cases considering decision problem complete similar reduction omit details simplicity here. notice replacement able keep problems equivalent. since objective convex jointly concave also feasible domain convex; hence satisﬁes strong max-min property swapped. swapping ﬁrst consider inner minimization subproblem diate value theorem must root. another fact piecewise-linear thus root unique roots must located continuous segment slope otherwise exists root located segment whose slope ensure root exists. following conclusions decreasing increasing know increasing thus increasing means convex hand easily check thus moment larger sub-gradient case thus increasing convex σj’s rademacher random variables ﬁrst inequality utilizes contraction property rademacher complexity last follows cauchy-schwarz inequality jensen’s inequality. next bound eﬃciently solved using projected gradient method proposed linear time projection. moreover out-of-bootstrap thresholding applied transform learned ranking model classiﬁer false-positive rate. demonstrate superiority method using theoretical analysis extensive experiments several benchmark datasets. research mainly done ﬁrst author intern idst alibaba. work supported national natural science foundation china nsfc-zhejiang joint fund integration industrialization information program shanghai science technology commission joint research grant proposal overseas chinese scholars", "year": 2018}