{"title": "Effective Use of Word Order for Text Categorization with Convolutional  Neural Networks", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Convolutional neural network (CNN) is a neural network that can make use of the internal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embedding of small text regions for use in classification. In addition to a straightforward adaptation of CNN from image to text, a simple but new variation which employs bag-of-word conversion in the convolution layer is proposed. An extension to combine multiple convolution layers is also explored for higher accuracy. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods.", "text": "convolutional neural network neural network make internal structure data structure image data. paper studies text categorization exploit structure text data accurate prediction. instead using low-dimensional word vectors input often done directly apply high-dimensional text data leads directly learning embedding small text regions classiﬁcation. addition straightforward adaptation image text simple variation employs bag-ofword conversion convolution layer proposed. extension combine multiple convolution layers also explored higher accuracy. experiments demonstrate effectiveness approach comparison state-of-the-art methods. text categorization task automatically assigning pre-deﬁned categories documents written natural languages. several types text categorization studied deals different types documents categories topic categorization detect discussed topics spam detection sentiment classiﬁcation determine sentiment typically product movie reviews. standard approach text categorization represent documents bag-of-word vectors noted loss word order caused bag-of-word vectors particularly problematic sentiment classiﬁcation. simple remedy word bi-grams addition unigrams however word n-grams text categorization general always effective; e.g. topic categorization simply adding phrases n-grams effective beneﬁt word order text categorization take different approach employs convolutional neural networks neural network make internal structure data structure image data convolution layers computation unit responds small region input data apply text categorization make structure document data unit convolution layer responds small region document text since work token-level applications collobert used systems entity search sentence modeling word embedding learning product feature mining notably many studies text ﬁrst layer network converts words sentences word vectors table lookup. word vectors either trained part training ﬁxed learned method additional large corpus. latter form semi-supervised learning study elsewhere. interested effectiveness without additional resources; therefore word vectors trained part network training word vector lookup done. question arises however whether word vector lookup purely supervised setting really useful text categorization. essence convolution layers convert text regions ﬁxed size feature vectors described later. sense word vector learning layer special case convolution layer region size one. size appropriate bi-grams discriminating unigrams? hence take different approach. directly apply high-dimensional one-hot vectors; i.e. directly learn embedding text regions without going word embedding learning. approach made possible solving computational issue efﬁcient handling high-dimensional sparse data turned merits improving accuracy fast training/prediction simplifying system code text publicly available internet. study effectiveness text categorization explain suitable task. types tested seq-cnn straightforward adaptation image text bow-cnn simple variation employs bag-of-word conversion convolution layer. experiments show seqwe term ‘embedding’ loosely mean structurepreserving function particular function generates lowdimensional features preserve predictive structure. implemented image would handle sparse data efﬁciently without efﬁcient handling sparse data convolution high-dimensional one-hot vectors would computationally infeasible. figure convolution layer image. computation unit computes non-linear function small region input image weight matrix bias vector shared units layer. outperforms bow-cnn sentiment classiﬁcation vice versa topic classiﬁcation winner generally outperforms conventional bagof-n-gram vector-based methods well previous models text complex. particular knowledge ﬁrst work successfully used word order improve topic classiﬁcation performance. simple extension combines multiple convolution layers leads improvement. empirical analysis show make effective high-order n-grams conventional methods fail. preliminary image feed-forward neural network convolution layers interleaved pooling layers illustrated figure layer performs classiﬁcation using features generated layers below. convolution layer consists several computation units takes input region vector represents small region input image applies non-linear function typically region vector concatenation forward representation would treat word pixel treat image pixels channels represent pixel |-dimensional one-hot vector. running example suppose vocabulary don’t hate love associate words dimensions vector alphabetical order document love then document vector convolution layer image represent region concatenation pixels makes |-dimensional region vectors region size ﬁxed advance. example example document vector above stride would regions love love represented following vectors rest image; text region vectors converted feature vectors i.e. convolution layer learns embed text regions lowdimensional vector space. call neural convolution layer region representation seq-cnn distinguish bow-cnn described next. pixels region would example -dimensional region number channels three conceptually computation units placed input image entire image collectively covered illustrated figure region stride often small value regions overlap other though stride figure larger region size illustration. distinguishing feature convolution layers weight sharing. given input unit associated region computes region vector representing region location predeﬁned component-wise non-linear activation function vector component). matrix weights vector biases learned training shared computation units layer. weight sharing enables learning useful features irrespective location preserving location useful features appeared. regard output convolution layer ‘image’ output computation unit considered ‘pixel’ channels number weight vectors number neurons. words convolution layer converts image regions m-dim vectors locations regions inherited conversion. output image convolution layer passed pooling layer essentially shrinks image merging neighboring pixels higher layers deal abstract/global information. pooling layer consists pooling units associated small region image. commonly-used merging methods average-pooling max-pooling respectively compute channel-wise average/maximum region. text consider application text data. suppose given document vocabulary requires vector representation data preserves internal locations input. straightpooling unit associated whole text). dynamic k-max pooling sentence modeling extends take largest values function sentence length entire data operation limited max-pooling. pooling differs natural extension standard pooling image max-pooling types applied. multiple pooling units associated different regions layer receive locational information turned useful topic classiﬁcation shown later. bag-of-n-grams traditional methods represent document entirely bag-of-n-gram vector apply classiﬁer model svm. however since high-order n-grams susceptible data sparsity large infeasible also ineffective. also note bag-of-n-gram represents n-gram one-hot vector ignores fact n-grams share constituent words. contrast internally learns embedding text regions useful intended task. consequently large used especially bow-convolution layer turned useful topic classiﬁcation. neuron trained assign large value e.g. love likely assign large value love well even though love never seen training. conﬁrm points empirically later. extension parallel described simplest network architecture pair convolution pooling layers. extended several ways experiments explored parallel large. since dimensionality region vectors determines dimensionality weight vectors high-dimensional region vectors means parameters learn. large model becomes complex and/or training becomes unaffordably expensive even efﬁcient handling sparse data; therefore lower dimensionality lowering vocabulary size and/or region size desirable depending nature task. alternative provide perform bagof-word conversion make region vectors |dimensional instead |-dimensional; e.g. example region vectors would converted whereas size images ﬁxed image applications documents naturally variable-sized therefore ﬁxed stride output convolution layer also variable-sized shown figure given variable-sized output convolution layer standard pooling image would produce variable-sized output passed another convolution layer. produce ﬁxed-sized output required fully-connected layer number pooling units dynamically determine pooling region size data point entire data covered without overlapping. convolution layers parallel illustrated figure idea learn multiple types embedding small text regions complement improve model accuracy. architecture multiple convolution-pooling pairs different region sizes given one-hot vectors input produce feature vectors region; layer takes concatenation produced feature vectors input. ﬁxed activation function rectiﬁer minimized square loss regularization stochastic gradient descent used words appeared frequently training set; thus example seq-cnn region size region vector dimensional. out-of-vocabulary words represented zero vector. bow-cnn speed computation used variable region stride larger stride taken repetition region vectors avoided padding size ﬁxed region size. used techniques commonly used image typically small performance improvements. dropout optionally applied input layer. response normalization case scales output pooling layer location multiplying baseline methods comparison tested linear kernel fully-connected neural networks bag-of-n-gram vectors input. experiment fully-connected neural nets minimized square loss regularization optional dropout activation ﬁxed rectiﬁer. generate bag-ofn-gram vectors topic classiﬁcation ﬁrst component word frequency document scaled unit vectors found always improved performance frequency. sentiment classiﬁcation often done generated binary vectors scaled unit vectors. tested three types bag-of-n-gram traditional vectors component vectors corresponds either uni-gram bi-gram tri-gram words. nb-lm also tested nb-lm ﬁrst appeared nbsvm later small modiﬁcation produced performance exceeds state-of-the-art supervised methods imdb mmrb experimented mmrb version generates binary bag-of-n-gram vectors multiplies component n-gram log/p probabilities estimated using training data logistic regression training. used mmrb’s software modiﬁcation model selection methods hyper-parameters conﬁgurations regularization parameters chosen based performance development data using chosen hyper-parameters models re-trained using training data. data tasks data preprocessing imdb movie reviews imdb dataset benchmark dataset sentiment classiﬁcation. task determine movie reviews positive negative. training test sets consist reviews. preprocessing tokenized text emoticons treated tokens converted characters lower case. elec electronics product reviews elec consists electronic product reviews. part large amazon review dataset chose electronics seemed different movies. following generation imdb chose training test half consists positive reviews half negative regarding rating negative positive reviewed products disjoint training test set. note extract text original data used text section summary section. obtained test reviews training sets various sizes. training test sets available internet. data preprocessing imdb. topic categorization corpus reuters news articles described lyrl topic categories hierarchy document associated topic. performance task known sensitive thresholding strategies algorithms additional models would like test. therefore also experimented single-label categorization assign second-level topics document directly evaluate models. task used documents one-month period test generated various sizes training sets documents earlier dates. data sizes shown table lyrl used concatenation headline text elements. data preprocessing imdb except used stopword list provided lyrl regarded numbers stopwords. table shows error rates comparison baseline methods. ﬁrst thing note datasets best-performing outperforms baseline methods demonstrates effectiveness approach. look details ﬁrst focus convolution layer sentiment classiﬁcation conﬁguration chosen model selection region size stride weight vectors max-pooling pooling unit types cnn; seq-cnn outperforms bow-cnn well baseline methods except one. note small region size max-pooling review contains short phrase conveys strong sentiment review could receive high score irrespective rest review. sensible type conﬁguration effective sentiment classiﬁcation. contrast topic categorization conﬁguration chosen bow-cnn model selection region size variable-stride≥ averagepooling pooling units weight vectors different sentiment classiﬁcation. presumably topic classiﬁcation larger context would predictive short fragments entire document matters location predictive text also matters last point news documents tend crucial sentences beginning. task bow-cnn outperform baseline methods bow-cnn outperforms seq-cnn indicates setting merit fewer parameters larger beneﬁt keeping word order region. turn parallel cnn. imdb seqcnn seq-convolution layers outperforms seq-cnn. neurons exceeds best-performing baseline also best previous supervised result. presume effectiveness seq-cnn indicates length predictive text regions variable. best performance imdb obtained ‘seq-bown-cnn’ equipped three layers parallel seq-convolution layers seq-cnn layer regards entire document region represents region bag-of-n-gram vector input computation unit; particular generated vectors multiplying nb-weights binary vectors motivated good performance nb-lm. third layer bow-convolution layer region variable size takes one-hot vectors n-gram vocabulary input learn document embedding. seq-bown-cnn elec table except regions sizes seqconvolution layers datasets performance improved seq-cnn. results suggest learned three layers distinct enough complement other. effectiveness third layer indicates short word sequences also global context large window useful task; thus inclusion bow-convolution layer ngram vocabulary large ﬁxed region size might even effective providing focused context pursue work. baseline methods comparing baseline methods other sentiment classiﬁcation reducing vocabulary frequent n-grams table error rate comparison bag-of-n-grambased methods. sentiment classiﬁcation imdb elec -way topic categorization indicates frequent n-grams used indicates n-grams used. used frequent words. notably hurt performance even though reduction common practice. error rates clearly improved addition bitri-grams. contrast topic categorization bi-grams slightly improved accuracy reduction vocabulary hurt performance. nb-lm strong imdb poor rcv; effectiveness appears datadependent also observed comparison state-of-the-art results shown table previous best supervised result imdb nb-lm best error rate better nearly reports semisupervised method learns low-dimensional vector representations documents unlabeled data. result directly comparable supervised results additional resource. nevertheless best result rivals result. tested bow-cnn multi-label lyrl. used thresholding strategy lyrl. shown table bow-cnn outperforms lyrl’s best results even though data preprocessing much simpler previous focus sentence classiﬁcation studies relation text categorization. studied ﬁne-tuning pre-trained word vectors produce input parallel cnn. reported performance poor word vectors trained part training tasks also unable outperform baselines type model. also approach system simpler fewer layer need tune dimensionality word vectors meta-parameters word vector learning. kalchbrenner proposed complex modiﬁcations sentence modeling. notably given word vectors convolution feature maps produces region matrix rd×m using provided code found model resource-demanding tasks. imdb elec best error rates obtained training various conﬁgurations memory hours respectively better bow. since excellent performances reported short sentence classiﬁcation presume model optimized short sentences text categorization general. performance dependency training known expensive compared with e.g. linear models linear imdb takes minutes using svmlight high-end intel cpu. nevertheless code training takes minutes datasets shown figure first comparison show n-grams found predictive; i.e. following n-grams assigned largest weights binary features elec negative positive class respectively note that even though also given bitri-grams features chosen binary features mostly uni-grams; furthermore features include bi-grams four tri-grams. means that given size training data still heavily counts uni-grams could ambiguous cannot fully take advantage higher-order ngrams. contrast nb-weights tend promote ngrams larger features assigned largest nb-weights uni- tri-grams. however seen above nb-weights always lead best performance. table show text regions learned seq-cnn predictive elec. convolution layer region size neurons; thus embedding convolution layer produces -dim vector region serves features layer weights assigned vector components. table ni/pi indicates component received i-th highest weight layer negative/positive class respectively. table shows text regions whose embedded vectors large value corresponding component i.e. predictive text regions. note embedded vectors text regions listed close large value component. table also shows proximity embedded vectors tends reﬂect proximity terms relations target classes effect embedding helps classiﬁcation layer. bag-of-n-gram representation n-grams appear training data participate prediction. contrast strength n-grams contribute accurate prediction even appear training data long constituent words input embedding constituent words region. point table show text regions test appear training data either entirely partially bi-grams whose embedded features large values heavily-weighted component thus contributing prediction. many these show small part unacceptably abysmally universally poor hugely disappointed enormously disappointed monumentally frustrating endlessly frustrating best concept ever best ideas ever best ever wholly satisﬁed entirely satisﬁed incredicbly satisﬁed overall impressed awfully pleased exceptionally pleased entirely happy acoustically good blindingly fast certain patterns. noticeable pattern entirely satisﬁed overall impressed. adjectives alone could ambiguous negated. know writer indeed satisﬁed need sequence satisﬁed insertion adverb entirely common. best ever’ another pattern discriminating pair words adjacent other. patterns require tri-grams disambiguation seq-cnn successfully makes even though exact tri-grams seen training result learning e.g. satisﬁed non-negative predictive positive class training. effectively word order bag-of-n-gram-based approaches fail. paper showed provides alternative mechanism effective word order text categorization direct embedding small text regions different traditional bag-of-ngram approach word-vector cnn. parallel framework several types embedding learned combined complement higher accuracy. state-of-the-art performances sentiment classiﬁcation topic classiﬁcation achieved using approach. john blitzer mark dredze fernando pereira. biographies bollywood boom-boxes blenders domain adaptation sentiment classiﬁcation. proceedings acl. ronan collobert jason weston l´eon bottou michael karlen koray kavukcuoglu pavel kuksa. natural language processing scratch. journal machine learning research pang lillian shivakumar vaithyanathan. thumbs sentiment classiﬁcation using machine learning techniques. proceedings conference empirical methods natural language processing pages olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei. imagenet large scale visual recognition challenge. arxiv.. mehran sahami susan dumais david heckerman eric horvitz. bayesian approach ﬁltering junk e-mail. proceedings aaai’ workshop learning text categorization. yelong shen xiaodong jianfeng deng gr´egoire mensnil. latent semantic model convolutional-pooling structure information retrieval. proceedings cikm. christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. arxiv..", "year": 2014}