{"title": "StressedNets: Efficient Feature Representations via Stress-induced  Evolutionary Synthesis of Deep Neural Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "The computational complexity of leveraging deep neural networks for extracting deep feature representations is a significant barrier to its widespread adoption, particularly for use in embedded devices. One particularly promising strategy to addressing the complexity issue is the notion of evolutionary synthesis of deep neural networks, which was demonstrated to successfully produce highly efficient deep neural networks while retaining modeling performance. Here, we further extend upon the evolutionary synthesis strategy for achieving efficient feature extraction via the introduction of a stress-induced evolutionary synthesis framework, where stress signals are imposed upon the synapses of a deep neural network during training to induce stress and steer the synthesis process towards the production of more efficient deep neural networks over successive generations and improved model fidelity at a greater efficiency. The proposed stress-induced evolutionary synthesis approach is evaluated on a variety of different deep neural network architectures (LeNet5, AlexNet, and YOLOv2) on different tasks (object classification and object detection) to synthesize efficient StressedNets over multiple generations. Experimental results demonstrate the efficacy of the proposed framework to synthesize StressedNets with significant improvement in network architecture efficiency (e.g., 40x for AlexNet and 33x for YOLOv2) and speed improvements (e.g., 5.5x inference speed-up for YOLOv2 on an Nvidia Tegra X1 mobile processor).", "text": "abstract—the computational complexity leveraging deep neural networks extracting deep feature representations signiﬁcant barrier widespread adoption particularly embedded devices. particularly promising strategy addressing complexity issue notion evolutionary synthesis deep neural networks demonstrated successfully produce highly efﬁcient deep neural networks retaining modeling performance. here extend upon evolutionary synthesis strategy achieving efﬁcient feature extraction introduction stress-induced evolutionary synthesis framework stress signals imposed upon synapses deep neural network training induce stress steer synthesis process towards production efﬁcient deep neural networks successive generations improved model ﬁdelity greater efﬁciency. proposed stress-induced evolutionary synthesis approach evaluated variety different deep neural network architectures different tasks synthesize efﬁcient stressednets multiple generations. experimental results demonstrate efﬁcacy proposed framework synthesize stressednets signiﬁcant improvement network architecture efﬁciency speed improvements crucial challenging aspect machine learning computer vision modeling performance heavily dependent extracted features. large number approaches proposed tackle challenge still considered open problem. high level feature representation approaches primarily divided main categories hand-crafted feature representations learned feature representations. hand-crafted feature representations quantitative features designed human experts long history machine learning computer vision literature. instance histogram oriented gradient features utilized different problems people detection crowd segmentation object detection scale-invariant feature transform speeded robust features applied wide variety applications image matching object classiﬁcation image registration action recognition addition well-known feature representations wide variety hand-crafted feature representations different applications different edge detection algorithms corner detection texture extraction name few. hand-crafted features designed human experts shown good performance different problems often limited applicable speciﬁc applications highly dependent knowledge human experts. mitigate high dependency knowledge human experts hand craft features particular applications learned feature representations automatically learned directly training data using machine learning approaches learned feature representations typically learned simultaneously training inference approach designed particular task thus learned features considered optimal error minimization sense inference task hand. successful strategies recent years feature learning deep learning feature extraction inference method particular task learned performed within end-to-end learning process. particular deep neural networks popular form deep learning demonstrated tremendous success learning powerful feature representations data leading state-of-the-art performance variety different applications past decade object detection semantic image segmentation image classiﬁcation speed recognition gene sequencing success deep neural networks learning feature representations inﬂuenced important factors. first deep neural network training process formulated end-to-end approach feature extraction inference trained simultaneously based training data. makes learning different layers deep neural networks joint process best possible feature representation optimized training step. approach helps deep neural network optimized inference time well. example convolution layers fully connected layers convolutional neural networks designed play roles feature extractor inference framework respectively. type conﬁguration effective since part compensate modeling deﬁciencies trained together leads efﬁcient feature representation. given rise data deep neural networks learn highly powerful feature representations around data high inference performance. secondly signiﬁcant growth computational power particularly rise parallel computing devices graphics processing units distributed computing systems greatly accelerated training inference speed deep neural networks. example seminal paper krizhevsky described approach enabling training deep neural networks turning point deep neural networks. improvements high-performance computing devices encouraged researchers focus design larger deeper neural networks produce powerful feature representations. despite successes demonstrated design complex deep neural networks main drawbacks approach improvement performance often came expense increased complexity making networks well-suited many applications particular require efﬁcient inference embedded systems considerable computational memory limitations self-driving cars smartphone applications surveillance cameras. example high-performance complex deep neural networks computationally expensive also require large memories store enormous number network parameters. fast data transmission required additionally support expensive computation load large network parametrization. issues associated computational complexity memory complexity bandwidth considered main barriers widespread adoption deep neural networks feature extraction variety operational scenarios applications. tackle challenge computational complexity leveraging deep neural networks learning extracting feature representations strong recent interest towards obtaining efﬁcient deep neural networks capable producing efﬁcient deep features particularly promising strategy addressing complexity issue notion evolutionary synthesis deep neural networks biological processes mimicked within probabilistic framework synthesize progressively efﬁcient deep neural networks generation generation. evolutionary synthesis strategy demonstrated successfully produce highly efﬁcient deep neural networks retaining modeling performance thus enabling efﬁcient powerful deep feature extraction. recent work shaﬁee took inspiration study dias ressler studied inheritance parental traumatic exposure offsprings found environmental stimuli imposed exposed parents here olfactory traumatic exposure mice– strong genetic inﬂuence offsprings conceived time. fascinating effect also showed klosin environmental information induced environmental stresses experienced lifetime elegans transmitted genetically subsequent generations. inspired past stressful experiences passed genetics generation generation shaﬁee mimicked phenomena preliminary work imposing environmental stresses ancestor network training results genetic encoding favoring synthesis even efﬁcient robust offspring networks. preliminary results presented evolutionary synthesis performed alexnet architecture task image classiﬁcation showed considerable promise synthesizing highly efﬁcient deep neural networks strong model accuracy retainment thus motivates extend upon preliminary work fully develop evolutionary synthesis framework built around environmental stress induction. motivated encouraging results extend upon initial ideas presented introduction formalized stress-induced evolutionary synthesis framework stress signals imposed upon synapses deep neural network training induce stress steer synthesis process towards production efﬁcient deep neural networks problem traumatic stresses synapses encoded within prior model. prior model designed distribution synaptic strength exposed parent deep neural network tailored exhibit inherent genetic encodings favor offspring neural networks greater efﬁciency synthesis process thus transmitting environmental information experienced deep neural network generation generation. introduction evaluation large family stressednets based variety different network architectures different types tasks demonstrate generalizability proposed comprehensive evaluation stress-induced evolutionary synthesis using wider variety benchmark datasets demonstrate generalizability proposed approach paper organized follow. section related work achieving efﬁcient deep neural networks presented provide context. section proposed stressed-induced evolutionary synthesis framework formalized explained detailed manner. section proposed stressed-induced evolutionary synthesis framework comprehensively examined evaluated variety different deep neural network architectures different tasks different benchmark datasets synthesize efﬁcient stressednets multiple generations along study effect environmental factors well quantitative qualitative feature analysis. related work prior discussing proposed stressed-induced evolutionary synthesis framework great detail important give context previous related methods achieving efﬁcient deep neural networks deep feature representations obtained. majority methods previous literature achieving efﬁcient deep neural networks grouped main categories methods addressing memory complexity associated deep neural networks methods focusing computational memory complexity issues together. area methods tackling memory complexity lecun addressed issue seminal paper proposing optimal brain damage method synapses pruned based strengths. utilized second-derivative information specify neuron pruned made trade-off number parameters training error. formulated error function effect perturbing parameter vector analytically calculated training. main framework summarized follow initial network architecture chosen network trained obtain reasonable performance. based second derivative saliency value parameter computed parameters lowsaliency value removed network model. proposed approach took advantage information theory select non-important parameters model removed. neural networks considered nonlinear mapping inputs outputs network parameters extract knowledge therefore different information theoretic methods applied area. gong took advantage information-theoretical vector-quantization methods compress parameters network. used k-means clustering weights quantize parameters dense connected layers. examined different quantization algorithms different levels including binarization vector quantization product quantization residual quantization compared terms saving storage requirement deep neural network given preservation modeling accuracy extent. reduce network structure storage requirement proposed combination pruning quantization huffman coding. followed optimal brain damage approach weights smaller weights pre-deﬁned threshold pruned network network trained compensate loss. applied quantization weight sharing approach reduce number required bits store weight network. also performed huffman coding reduce storage based occurrence weights network. extended upon algorithm proposed dynamic network surgery method beside pruning splicing procedure performed. splicing procedure enables connection recovery pruned connections found important. storage demand deep neural networks issue needed resolved however bigger issue computational complexity running time problem deep neural networks processed embedded devices several methods trying address issue. area methods addressing computational memory complexity issues simultaneously low-rank matrix factorization proposed approximate ﬁlter structures convolutional kernels convolutional layers. example jaderberg took advantage low-rank matrix factorization learn separable smaller kernels like separable kernels optimized training network. convolutional kernels approximated based ﬁlter banks horizontal vertical kernels. proposed ﬁlter bank approaches reduces redundancy among ﬁlters approximate smaller kernel playing role bases. approach addition reduce number parameters could decrease computational complexity results decreased running time feed-forward pass network. ioannou proposed training approach network learns small basis ﬁlters scratch low-rank matrix factorization using smaller kernel size addresses running-time issue. learned kernels rectangular spatial domain. conventional squared kernels factorized rectangular horizontal vertical kernels responses linearly combined next layer ﬁlters. process another address network optimization issue. suggested applying regularization techniques learn kernel structures account structured sparsity. introduced regularization approach learning ﬁlter shapes layer depth during training. formulated loss function account structure network well. proposed loss function combination loss data nonstructured regularization every weight network structured sparsity regularization layer. applied group lasso weights zero weights set. proposed approached tends remove less important ﬁlters channels network. variational learning bayesian algorithms techniques proposed formulate model compression network optimization. ullrich took advantage minimum description length variational learning framework neural network compression. enforced sparsity model compression prior distribution training time. molchanov utilized variational dropout approach sparsify neural networks. applied unbounded dropout technique leading sparse neural networks. louizos extended upon proposed method ullrich used hierarchical priors prune neurons instead synapses network. applied sparsity inducing priors hidden units instead individual weights prunes neuron instead synapses network. also utilized posterior uncertainty determines optimal ﬁxed point precision. another promising approach tackling computational memory complexity issues simultaneously evolutionary sythesis framework proposed shaﬁee inspirations evolutionary biology random mutation natural selection heredity leveraged within probabilistic framework synthesize increasingly efﬁcient deep neural networks successive generations resulting learning highly efﬁcient powerful feature representations. previous works explored evolutionary computing methods training generating deep neural networks largely focused accuracy progressively efﬁcient deep neural networks also leveraged classical methods genetic algorithms evolutionary programming differs greatly probabilistic generative framework proposed aspects evolutionary synthesis framework greatly inﬂuencing efﬁciency quality synthesized offspring deep neural networks genetic encoding scheme acts probabilistic ‘dna’ mimic heredity aspect biological evolution. instance shaﬁee wong extended genetic encoding scheme synthesize deep neural networks architectures enable efﬁcient inference parallel computing devices gpus. speciﬁcally proposed genetic encoding scheme promote formation highly sparse sets synaptic clusters thus tailoring hardware architecture gpus execute kernel computing instructions highly methodology here introduce formalize extended evolutionary synthesis framework learning efﬁcient deep feature representations using stress-induced evolutionary synthesis strategy several synapses network exposed stress signals training induce stress within network. imposed stress signals leveraged inducing environmental stresses improves robustness synthesized network architectures facing traumatic changes consequence promotes synthesis stressednets improved model ﬁdelity greater efﬁciency. section ﬁrst review underlying concept behind evolutionary synthesis deep neural networks followed detailed description explanation proposed stress-induced evolutionary synthesis scheme. evolutionary synthesis framework leveraging work ﬁrst proposed shaﬁee progressively efﬁcient deep neural networks synthesized within probabilistic framework multiple generations leveraging processes mimic heredity natural selection random mutation. speciﬁcally architectural traits deep neural network modeled synaptic probability models considered probabilistic ‘dna’ used mimic heredity pass genetic information subsequent generations. offspring deep neural networks diverse network architectures synthesized stochastically based probabilistic ‘dna’ together probabilistic computational environmental factor models encouraging progressively increasing network architecture efﬁciency generations. architecture deep neural network encoded different sets random variables representing existence neurons synapses network. realization random variables binary values determines whether interested neuron synapse realized network architecture not. however possible infer existence neuron given existence ingoing outgoing synapse. main purpose evolutionary synthesis frameworks model optimal probability distribution network architecture time generational manner. generational approach helps probability distribution network architecture account change network model optimal network architecture better way. such process synthesizing deep neural network formulated within evolutionary framework generation better probability distribution introduced based network architecture next generation fig. overview proposed stress-induced evolutionary synthesis framework architectural traits ancestor networks encoded probabilistic ‘dna’ sequences. environmental stresses induced epoch training thus enabling synapses prepared traumatic exposure generation. offspring stressednet synthesized generation based probabilistic ‘dna’ sequences environmental factors random mutation. main steps proposed framework induce stress training process synthesize stressednets based probabilistic ancestor network trained stress proposed framework synthesizes stressednets network architectures efﬁcient compared ancestor. fig. visualization synapse kernel ﬁlter layer convolution layer. kernel structure consisting synapses ﬁlter combination kernels size input channels. number output channels speciﬁed number ﬁlters layer. hierarchical structure shaﬁee decomposed multi-factor probability distribution promote formation synaptic clusters resulting synthesis offspring deep neural networks tailored efﬁcient computation parallel computing systems cluster synapses generation cluster encoded subset synapses network ﬁlter kernel inside ﬁlter examples synaptic clusters genetic encoding scheme synaptic probability model imposed environmental factors. offspring networks trained generation achieve modeling accuracy preserving efﬁciency architectural diversity. environmental factor plays role prior constraints regarding desired network architecture applied synthesis offspring network. approach search domain model optimal architecture reduced result better probability distribution created model optimal network architecture. furthermore synaptic probability model treated genetic encoding network architecture context evolutionary synthesis framework plays main role evolve network architecture survive simulated environment effect genetic encoding synthesizing efﬁcient network architectures crucial. shaﬁee utilized trained weights ancestor network formulate genetic encoding probability distribution. following genetic encoding offspring networks modeled represents trained synaptic strengths network generation based notion desired traits inherited offspring networks related strong synaptic strengths ancestor networks. synaptic strength non-existence represented synapse encoded deep neural network composed different components hierarchical manner synapses smallest components hierarchy. shown figure synapses constructs kernel kernels creates ﬁlter layer consecutively layer network constructed based different ﬁlters. prior model imposed training stage. here prior model encodes imposed stresses synapses training generation. shown figure encode imposed stresses synapses within prior model training deep neural network generation. approach prepares network traumatic change would happen offspring network helps ancestor network transmit stressful experiences probabilistic dna. given goal better promotes synthesis offspring networks effective efﬁcient network architectures provide effective genetic here prior model realized binomial probability distribution strengths subset synapses weakened epoch level training formulated follows qtg− binomial distribution formulating uniformly distributed random numbers based uniform distribution iverson bracket determining whether synapse selected epoch generation ˆwtg− encodes trained synaptic strengths epoch generation binomial distribution qtg− formulated based trained synaptic strengths ˆwtg− epoch bernoulli distribution synapse network containing synapses computed based ˆwtg− normalization factor. factor intra-generational environmental factor applied epoch training weaken strength stochastically selected synapses. factor imposes minor stress deep neural network epoch level. stochastically selected synapses epoch meant less important modeling power deep neural network synapses weakening minimal effect modeling accuracy. however cumulation tiny stress-induced changes shapes distribution synaptic strengths promote formation synaptic probability model taken place. example main speed parallel computing devices gpus synthesized network less number ﬁlter since parallel computing device computes whole ﬁlter time. however different processing devices since processing tasks computed sequentially. encoding important signiﬁcant inﬂuence network architectures offspring deep neural networks addition preserving accuracy. mentioned before genetic encoding scheme previously proposed highly dependent synaptic strengths ancestor deep neural network i.e. wg−; therefore optimizing distribution synaptic strengths promotes optimal genetic encoding favoring synthesis offspring neural networks greater architectural efﬁciency highly desired. inspired improve architectural efﬁciency synthesized offspring deep neural networks propose stress-induced evolutionary synthesis approach stress signals imposed deep neural networks training. stress encoded prior model within probabilistic framework induce environmental stresses better promote synthesis robust networks achieve greater efﬁciency maintaining modeling performance. general idea imposition epoch-level traumatic stresses weaken strengths subset synapses induce environmental stresses network training. here stresses imposed exposed parent deep neural network training inﬂuence distribution synaptic strengths deep neural network favor offspring networks greater architectural efﬁciency. effect transmitted genetically next generation i.e. probabilistic genetic encoding. speciﬁcally induced stresses encourage conﬁgurations enable effective genetic encodings linked synthesized offspring networks greater architectural efﬁciencies. model neural network probabilistic model d-dimensional input network network assigns probability possible output regarding trained synaptic strengths learning process synaptic strengths within deep neural network formulated maximum likelihood estimation given training data favoring synthesis offspring deep neural networks efﬁcient effective network architectures. such stress induced ancestor deep neural network results genetic encodings synthesizing stressednets efﬁcient robust network architectures. experimental results performance proposed stress-induced evolutionary synthesis approach synthesizing deep neural networks even greater efﬁciency retaining modeling accuracy evaluated comprehensive manner variety different deep neural network architectures different tasks. performance proposed approach also compared state-of-the-art methods achieving efﬁcient deep neural networks widely-used benchmark experiment research literature provide context ﬁeld. furthermore effect various environmental factors quality synthesized stressednets investigated parametric analysis. finally comprehensive qualitative quantitative feature analysis performed extracted features within synthesized stressednets. efﬁcacy across network architectures section efﬁcacy generalizability proposed stress-induced evolutionary synthesis framework examined across different network architectures tasks benchmark datasets. lenet ﬁrst experiment proposed approach examined synthesize stressednets based letnet network architecture task image classiﬁcation. original lenet trained mnist dataset mnist image dataset comprises training images test images handwritten digits figure demonstrates examples mnist dataset. stress-induced evolutionary synthesis performed generations error synthesized stressednets within achieved original network. lenet architecture used study implemented caffe platform commonly referred lenet-caffe. architecture comprised convolutional layers ﬁlters size fully connected layers neurons. variant lenet chosen commonly used research literature comparing different methods achieving efﬁcient deep neural networks. table shows efﬁcient network architectures corresponding classiﬁcation errors achieved using state-of-the-art methods different stressednets synthesized proposed stress-induced evolutionary synthesis approach generations respectively. observed network architecture synthesized stressedn fewer weights compared original network architecture achieving modeling accuracy also observed network architecture smaller synthesized stressedn fewer weights original network architecture still achieving modeling accuracy interesting fact stressedn consists four ﬁlters ﬁrst convolutional layer seven ﬁlters second convolutional layer stressedn contains four ﬁlters ﬁrst convolutional layer ﬁlters second convolutional layer. important much computational complexity lies ﬁrst convolutional layers much parameters fully-connected layers signiﬁcant decrease number ﬁlters convolutional layers within synthesized stressednets result signiﬁcant improvement computational speed. taken context performance compared state-of-the-art methods observed stressednets slightly higher error compared methods network architectures comparable number non-zero weights signiﬁcantly fewer ﬁlters convolutional layers. implementation point view parallel processing units gpus hardware accelerators embedded devices heavy computation convolutional layers main bottleneck computational efﬁciency deep neural networks compared fully connected layers. therefore reducing number ﬁlters convolutional layers signiﬁcantly speed computational processing time. alexnet examine efﬁcacy proposed stress-induced evolutionary synthesis approach larger complex deep neural network lenet image classiﬁcation stressnets synthesized alexnet network architecture consists convolutional layers ﬁlters respectively fully connected layers neurons. cifar benchmark dataset used experiment comprises training test natural images different classes equally distributed. account image size cifar- dataset kernel sizes ﬁrst convolutional layers rest stress-induced evolutionary synthesis performed generations error synthesized stressednets within achieved original network. table shows comparison network architecture original network architecture synthesized stressednet network architecture synthesized stressednet smaller original network experiencing classiﬁcation error increase also important note number ﬁlters convolutional layers stressednet fewer compared original network results noticeably reduced computational complexity particularly parallel computing devices. finally observed model size stressednet smaller compared original network beneﬁcial embedded scenarios memory storage limited. yolov main advantages proposed stress-induced evolutionary synthesis framework easily generalizable variety different network architectures different tasks. demonstrate ﬂexibility stressednets synthesized using proposed approach based yolov network architecture task object detection. yolov object detection problem formulated single regression problem bounding coordinates class probabilities computed time. result besides achieving state-of-the-art performance network architecture yolov considerably smaller efﬁcient compared object detection deep neural networks thus considered fastest object detection approaches research literature. despite efﬁcient design still currently investigate efﬁcacy proposed framework synthesizing stressednets perform fast object detection embedded systems yolov network trained kitti benchmark dataset purpose detecting types objects image pedestrian. figure shows examples kitti dataset. object types used experiment kitti benchmark dataset three different task difﬁculty groups easy moderate iii) hard. experiment stress-induced evolutionary synthesis performed generations average precision synthesized stressednets within achieved original network. proposed framework synthesized evolved network architecture generations smaller size original yolov architecture table shows comparison network architecture original network architecture synthesized stressednet clearly observed synthesized stressednet provides comparable results cases outperforms original yolov network terms object types difﬁculty scenarios smaller model size compared original network experiment illustrates over-parameterization issues faced deep neural networks faced limited training data sizes task require full information capacity network. seen proposed framework synthesizes highly efﬁcient deep neural networks possess network architectures sufﬁcient information capacity task hand. generations) compared original network. synthesized stressednet consists number non-zero numbers show number ﬁlters convolutional layers. observed stressednet’s network architecture fewer ﬁlters stressednet examined nvidia tegra mobile processor. seen table observed original yolov network achieved frames second synthesized stressednet achieved frames second effect environmental factors effect different environmental factors different levels enforcement synthesizing efﬁcient network architectures studied section. analyze behavior synthesized stressednets several generations yolov network architecture utilized original ancestor network architecture three different experiments three different environmental factor values using kitti dataset. figure illustrates modeling accuracy stressednets different generations different environmental factors. observed synthesized stressednets produced three tested conﬁgurations provide better accuracy compared original yolov network. however environmental factor decreased resulted stressednet models provide comparable sometimes worse modeling accuracy compared original yolov network. result demonstrates network facing severe conditions cannot survive maintain modeling accuracy. figure shows performance synthesized stressednets based environmental factor generations network architecture synthesized stressednet million fewer parameters outperforming original network figure shows performance synthesized stressednets based environmental factor observed enforcing greater efﬁciency descendent offspring networks stressednets synthesized noticeably higher efﬁciency generation compared last experiment still outperforming original yolov figure shows performance synthesized stressednets based environmental factor observed even aggressive level efﬁciency enforcement trend persists stressednets synthesized noticeably higher efﬁciency generation compared last experiment figure demonstrate performance synthesized stressednet architectures imposed severe environmental factors seen offspring networks enforced severe environments networks cannot maintain modeling accuracies results lose modeling accuracy generations. effect illustrated comparing generation experiment generation experiment seen offspring stressednet experiment provides increase modeling accuracy compared original yolov synthesized stressednet experimental results yolov experiment kitti dataset. network architecture synthesized stressednet compared original network. synthesized stressednet model size smaller original network outperformed original network test scenarios. observed inference speed synthesized stresssednet nvidia tegra mobile processor faster original yolov network. conducted experiments illustrate possible synthesize deep neural networks highly efﬁcient network architectures proposed stressinduced evolutionary synthesis approach achieving modeling accuracies comparable cases higher original network. however important consider choosing smaller environmental factors produces efﬁcient networks less number generations offspring networks cannot retain complete modeling accuracy situations. therefore trade-of processing time synthesizing efﬁcient networks synthesizing best possible efﬁcient network architecture. last experiment study comprehensive feature analysis generated feature representations within stressednets performed qualitatively quantitatively investigate discriminative capacity. qualitatively investigate discriminative capacity generated feature representations within stressednets deep neural network based lenet architecture mentioned section trained mnist dataset stressed-induced evolutionary synthesis performed network. extracted features last fully connected layer stressednets different generations along original network extracted t-sne performed features visualize extracted features t-sne variation stochastic neighbor embedding visualizes highdimensional data giving datapoint location map. algorithm capable capturing much local structure high-dimensional data well optimized much easier compared stateof-the-art visualization algorithms. ability preserve local structure important aspect want study high-dimensional feature representations produced deep neural networks. better visualization samples class mnist dataset selected visualization purposes. network produces different number features based size last fully connected layer; seen table original lenet network produces features stressednets results features respectively. seen figure between-class distances feature representations speciﬁc classes decrease feature domains formed synthesized stressednets still maintain separability among classes. figure shows classes affected least among classes classes dispersed most. better understanding separability classes classiﬁcation conducted test samples based selected training samples table shows classiﬁcation accuracy classes mnist dataset. results show consistent trend classiﬁcation accuracies classiﬁcation accuracies classes almost original network synthesized stressednets larger difference classiﬁcation accuracies took place classes fig. environmental factor analysis. effect different environmental factors studied performing stressed-induced evolutionary synthesis generations using yolov network architecture different factor values. generation demonstrates original network architecture statistics. fig. t-sne visualization extracted features original letnet network compared different stressednets. seen between-class distances decrease feature domains formed stressednets still maintain separability among classes. original lenet network. show effectiveness stressednets demonstrate network architectures shrink feature vectors also creates highly discriminative feature space input data feature sets size generated features within stressednet stressednet extracted well. performance metric conducted compare effectiveness generated features within stressednets. seen table comparison modeling accuracies shows discriminatory power stressednet feature representations higher obtained using applied features extracted original lenet. comparing total classiﬁcation accuracies original letnet stressednet illustrates discriminatory power stressednet feature representations comparable lenet shows information density stressednet features level original lenet architecture. conclusion paper stress-induced evolutionary synthesis framework proposed synthesizing progressively efﬁcient deep neural networks generations. inducing stresses upon networks training stage within evolutionary synthesis framework robustness synthesized networks facing traumatic changes improved consequence promotes synthesis descendant deep neural networks improved model ﬁdelity greater efﬁciency. experimental results across variety different network architectures datasets demonstrate effectiveness generality proposed stress-induced evolutionary synthesis framework synthesizing efﬁcient deep neural networks preserving modeling accuracy makes well-suited embedded applications computational memory resources highly limited. acknowledgments authors would like thank automotive distance control systems gmbh continental canada research chairs program natural sciences engineering research council canada ﬁnancial support. authors also thank nvidia hardware used study nvidia hardware grant program. references dalal triggs histograms oriented gradients human detection computer vision pattern recognition cvpr ieee computer society conference vol. ieee grauman darrell efﬁcient image matching distributions local invariant features computer vision pattern recognition cvpr ieee computer society conference vol. kumar kang doermann unsupervised feature learning framework no-reference image quality assessment computer vision pattern recognition ieee conference ramos nascimento pereira texture extraction evaluation ridgelet wavelet co-occurrence based methods applied mammograms expert systems applications vol. hinton deng dahl a.-r. mohamed jaitly senior vanhoucke nguyen sainath deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal process. mag. shaﬁee wong evolutionary synthesis deep neural networks synaptic cluster-driven genetic encoding nips workshop efﬁcient methods deep neural networks shaﬁee barshan chwyl karg scharfenberger wong learning efﬁcient deep feature representations transgenerational genetic transmission environmental information evolutionary synthesis deep neural networks proceedings ieee conference computer vision pattern recognition white ligomenides gannet genetic algorithm optimizing topology weights neural network design international workshop artiﬁcial neural networks. springer shaﬁee barshan wong evolution groups deeper look synaptic cluster driven evolution deep neural networks arxiv preprint arxiv.", "year": 2018}