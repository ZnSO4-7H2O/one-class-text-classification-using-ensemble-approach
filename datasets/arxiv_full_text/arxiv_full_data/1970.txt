{"title": "Distance-based Confidence Score for Neural Network Classifiers", "tag": ["cs.AI", "cs.CV", "stat.ML"], "abstract": "The reliable measurement of confidence in classifiers' predictions is very important for many applications and is, therefore, an important part of classifier design. Yet, although deep learning has received tremendous attention in recent years, not much progress has been made in quantifying the prediction confidence of neural network classifiers. Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with prohibitive computational costs. In this paper we propose a simple, scalable method to achieve a reliable confidence score, based on the data embedding derived from the penultimate layer of the network. We investigate two ways to achieve desirable embeddings, by using either a distance-based loss or Adversarial Training. We then test the benefits of our method when used for classification error prediction, weighting an ensemble of classifiers, and novelty detection. In all tasks we show significant improvement over traditional, commonly used confidence scores.", "text": "reliable measurement conﬁdence classiﬁers’ predictions important many applications therefore important part classiﬁer design. although deep learning received tremendous attention recent years much progress made quantifying prediction conﬁdence neural network classiﬁers. bayesian models oﬀer mathematically grounded framework reason model uncertainty usually come prohibitive computational costs. paper propose simple scalable method achieve reliable conﬁdence score based data embedding derived penultimate layer network. investigate ways achieve desirable embeddings using either distance-based loss adversarial training. test beneﬁts method used classiﬁcation error prediction weighting ensemble classiﬁers novelty detection. tasks show signiﬁcant improvement traditional commonly used conﬁdence scores. classiﬁcation conﬁdence scores designed measure accuracy model predicting class assignment generative classiﬁcation models probabilistic nature therefore provide conﬁdence scores directly. discriminative models hand direct access probability prediction. instead related non-probabilistic trying evaluate conﬁdence neural network classiﬁers number scores commonly used. strength activated output unit followed softmax normalization closely related ratio activities strongest second strongest units. another entropy output units minimal units equally probable. often however scores provide reliable measure conﬁdence. important reliably measure prediction conﬁdence? various contexts medical diagnosis decision support systems important know prediction conﬁdence order decide upon example conﬁdence certain prediction involvement human expert decision process called for. another important aspect real world applications ability recognize samples belong known classes also improved reliable conﬁdence score. even irrespective application context reliable prediction conﬁdence used boost classiﬁer performance methods self-training ensemble classiﬁcation. context better conﬁdence score improve ﬁnal performance classiﬁer. derivation good conﬁdence score therefore part classiﬁer’s design important component classiﬁers’ design. order derive reliable conﬁdence score classiﬁers focus attention empirical observation concerning neural networks trained classiﬁcation shown demonstrate parallel useful embedding properties. speciﬁcally common practice days treat upstream layers pre-trained network representation layer. layer activation used representing similar objects train simpler classiﬁers perform diﬀerent tasks related identical original task network trained computer vision embeddings commonly obtained training deep network recognition large database embeddings shown provide better semantic representations images number related tasks including classiﬁcation small datasets image annotation structured predictions given semantic representation compute natural multi-class probability distribution described section estimating local density embedding space. estimated density used assign conﬁdence score test point using likelihood belong assigned class. note however commonly used embedding discussed associated network trained classiﬁcation only impede suitability measure conﬁdence reliably. fact training neural networks metric learning often used achieve desirable embeddings schroﬀ hoﬀer ailon tadmor since goal improve probabilistic interpretation embedding essentially based local point density estimation wish modify loss function term penalizes violation pairwise constraints hadsell experiments show modiﬁed network indeed produces better conﬁdence score comparable classiﬁcation performance. surprisingly directly designed purpose show networks trained adversarial examples following adversarial training paradigm also provide suitable embedding conﬁdence score. ﬁrst contribution therefore prediction conﬁdence score based local density estimation embedding space neural network. score computed every network order score achieve superior performance necessary slightly change training procedure. second contribution show suitable embedding achieved either augmenting loss function trained network term penalizes distance-based similarity loss below) using adversarial training. importance latter contribution fold firstly ﬁrst show density image embeddings improved indirect adversarial training perturbations addition improved word embedding quality shown miyato direct adversarial training perturbations. secondly show section adversarial training improves results imposing much lighter burden hyperparameters tune compared distance-based loss. conﬁdence score evaluated comparison scores using following tasks performance binary classiﬁcation task identifying class prediction correct incorrect training ensemble classiﬁers classiﬁer’s prediction weighted conﬁdence score novelty detection conﬁdence used predict whether test point belongs known classes train empirical evaluation method described section using datasets diﬀerent network architectures used previous work using speciﬁc datasets. method achieves signiﬁcant improvement tasks. compared recent method shown improve traditional measures classiﬁcation conﬁdence dropout distance-based score achieves better results also maintaining lower computational costs. bayesian approach seeks compute posterior distribution parameters neural network used estimate prediction uncertainty mackay neal however bayesian neural networks always practical implement computational cost involved typically high. accordance method referred mc-dropout ghahramani proposed dropout test time bayesian approximation neural network providing cheap proxy bayesian neural networks. lakshminarayanan proposed adversarial training improve uncertainty measure entropy score neural network. still basic common conﬁdence scores neural networks derived strength activated output unit rather normalized version conﬁdence score handles better situation class probable entropy normalized network’s output. zaragoza d’alché compared scores well complex ones demonstrating somewhat surprisingly empirical superiority basic methods described previous paragraph. score monotonically related local density similarly labeled train points neighborhood henceforth referred distance score. note intuitively might beneﬁcial scaling factor distance mean distance found deteriorating eﬀect line related work salakhutdinov hinton ways achieve eﬀective embedding mentioned section order achieve eﬀective embedding helps modify training procedure neural network classiﬁer. simplest modiﬁcation augments network’s loss function training additional term. resulting loss function linear combination terms classiﬁcation denoted lclass another pairwise loss embedding denoted ldist. deﬁned follows desirable embedding also achieved adversarial training using fast gradient method suggested goodfellow method given input target neural network parameters adversarial examples generated using step adversarial example generated point batch current parameters network classiﬁcation loss minimized regular adversarial examples. although originally designed improve robustness method ensembles models used improve overall performance ﬁnal classiﬁer many ways train ensemble boosting bagging. also many ways integrate predictions classiﬁers ensemble including average prediction voting discussed bauer kohavi ensemble methods conﬁdence score either weight predictions diﬀerent classiﬁers conﬁdence voting. novelty detection task determine whether test point belongs known class label another problem becomes relevant ever increasing availability large datasets reviews markou singh pimentel recent work vinokurov weinshall task also highly relevant real world applications classiﬁer usually exposed many samples belong known class. note novelty detection quite diﬀerent learning classes examples zero shot learning conﬁdence score based estimation local density induced network points represented using eﬀective embedding created trained network upstream layers. local density point estimated based euclidean distance embedded space point nearest neighbors training set. speciﬁcally denote embedding deﬁned trained neural network classiﬁer. denote k-nearest neighbors training based euclidean distance embedded space {yj}k denote corresponding class labels points probability space constructed assuming likelihood points belong class proportional exponential negative euclidean distance them. accordance local probability point belongs class proportional probability belongs class subset points ldist deﬁned implementation details pairs points denoted training minibatch sampled replacement training points minibatch half many pairs size minibatch. experiments lclass regular cross entropy loss. note also tried distance-based loss functions limit distance points class exactly tadmor however functions produced worse results especially dataset many classes. finally note tried using distance-based loss adversarial training together training network also produced worse results. recent methods shown improve reliability conﬁdence score based entropy mcdropout adversarial training terms computational cost adversarial training increase training time computation additional gradients addition adversarial examples training set. mc-dropout hand change training time increases test time orders magnitude methods complementary approach focus modiﬁcations actual computation network either train test time. done evaluate conﬁdence using entropy score. show experiments adversarial training combined proposed conﬁdence score improves ﬁnal results signiﬁcantly. unlike methods described above mc-dropout adversarial training distance-based conﬁdence score takes existing network computes conﬁdence score network’s embedding output activation. network without adversarial training dropout. loss function network suitably augmented empirical results section show score always improves results entropy score given network. train test computational complexity considering distance-based loss tadmor showed computing distances training neural networks negligible eﬀect training time. alternatively using adversarial training additional computational cost incurred mentioned above hand fewer hyper parameters left tuning. test time method requires carrying embeddings training data also computation nearest neighbors sample. nearest neighbor classiﬁcation studied extensively past years consequently many methods perform either precise approximate k-nn reduced time space complexity recent empirical comparison main methods). experiments using either condensed nearest neighbours density preserving sampling able reduce memory requirements train original size without aﬀecting performance. point additional storage required nearest neighbor step much smaller size networks used classiﬁcation increase space complexity became insigniﬁcant. regards time complexity recent studies shown modern gpu’s used speed nearest neighbor computation orders magnitude hyvönen also showed k-nn approximation recall accomplished times faster compared precise k-nn. combining reductions space time note even large dataset including example images embedded dimensional space computation complexity nearest neighbors test sample requires ﬂoating-point operations. comparable even much faster single forward test sample modern relatively small resnets parameters. thus method scales many ways deﬁne ensembles classiﬁers diﬀerent ways together. focus ensembles obtained using diﬀerent training parameters single training method. speciﬁcally means train several neural networks using random initialization network parameters along random shuﬄing train points. henceforth regular networks refer networks trained classiﬁcation regular cross-entropy loss distance networks refer networks trained loss function deﬁned networks refer networks trained adversarial examples deﬁned ensemble methods diﬀer weigh predictions diﬀerent classiﬁers ensemble. number options common recent review) accordance used comparison experimental evaluation section softmax average simple voting weighted softmax average conﬁdence voting dictator voting evaluate methods weights deﬁned either entropy score distance score deﬁned novelty detection seeks identify points test belong classes present train set. evaluate performance task train network known benchmark dataset augmenting test test points another dataset includes diﬀerent classes. conﬁdence score used diﬀerentiate known unknown samples. binary classiﬁcation task therefore evaluate performance using curves. section empirically evaluate beneﬁts proposed approach comparing performance conﬁdence score alternative existing scores diﬀerent tasks described above. svhn cases commonly done data preprocessed using global contrast normalization whitening. method data augmentation used cifar- svhn svhn also additional labeled images. stl- hand cropping ﬂipping used stl- check robustness method heavy data augmentation. experiments networks used non-linear activation. cifar- stl- used network suggested clevert following architecture denotes convolution layer kernels size stride denotes max-pooling layer window size stride denotes fully connected layer output units. stl- last layer replaced training applied dropout pooling layer last convolution corresponding drop probabilities svhn dataset used following architecture networks trained distance loss batch randomly picked pairs points least batch included pairs points class. margin cases parameter rest training parameters found supplementary material. distance score observed number nearest neighbors could maximum value number samples class train data. also observed smaller numbers often worked note reported results denoted \"state-of-the-art\" datasets often involve heavy augmentation. study order able exhaustive comparisons described below opted un-augmented scenario ﬂexible informative enough purpose comparison diﬀerent methods. therefore numerical results compared empirical studies used similar un-augmented settings. speciﬁcally selected commonly used architectures achieve good performance close results modern resnets ﬂexible enough extensive evaluations. table legend. leftmost column margin entropy denote commonly used conﬁdence scores described section distance denotes proposed method described section second line reg. denotes networks trained entropy loss dist. denotes networks trained distance loss deﬁned denotes networks trained adversarial training deﬁned denotes mc-dropout applied networks normally trained entropy loss. since network trained svhn trained without dropoutmcd applicable. table legend. notations similar described legend table distinction distance denotes regular architecture distance score computed independently network pair using embedding distance denotes hybrid architecture network pair ﬁxed distance network embedding used compute distance score prediction second network pair. mc-dropout proposed ghahramani used dropout following manner. trained network usual computed predictions using dropout test. repeated times test example average activation delivered output. ﬁrst compare performance conﬁdence score binary task evaluating whether network’s predicted classiﬁcation label correct not. results independent actual accuracy note accuracy comparable achieved resnets using augmentation cifar- using regular training data svhn example). results three datasets seen table cases proposed distance score computed suitably trained network achieves signiﬁcant improvement alternative scores even enhanced using either adversarial training mc-dropout. test distance score evaluate performance ensemble networks. results shown table distance score achieves signiﬁcant improvement methods. also note diﬀerence distance score computed distance networks entropy score computed adversarially trained networks much higher compared diﬀerence using network. show section adversarial training typically leads decreased performance using ensemble networks relying entropy score observation supports added value proposed conﬁdence score. ﬁnal note also used hybrid architecture using matched pair classiﬁcation network second distance network. embedding deﬁned distance network used compute distance score predictions ﬁrst classiﬁcation network. surprisingly method achieves figure accuracy using ensemble networks cifar- stl- svnh x-axis denotes number networks ensemble. absolute accuracy shown successful ensemble methods among methods evaluated methods distance score including best performing method diﬀerences accuracy performers baseline method shown using plot standard deviation diﬀerence repetitions. best results cifar- svhn comparable best result stl-. method used later section improve accuracy running ensemble networks. investigation phenomenon lies beyond scope current study. order evaluate improvement performance using conﬁdence score direct integration classiﬁers ensemble used common ways deﬁne integration procedure ways construct ensemble itself. comparisons number networks ensemble remained ﬁxed experiments included following ensemble compositions regular networks distance networks networks networks networks belong kind networks remaining networks belong another kind spanning combinations. described section predictions classiﬁers ensemble integrated using diﬀerent criteria. general found methods distance score including methods used conﬁdence score prediction weighting performed less well simple average softmax activation otherwise best performance obtained using weighted average weights deﬁned distance score variants also checked options obtaining distance score network deﬁned conﬁdence score; light advantage demonstrated hybrid networks shown section pair networks diﬀerent kinds distance score computed using embedding networks pair. mcdropout used section high computational cost. readability combination achieving best performance; combination achieving best performance using adversarial training ensemble variant achieving best performance without using distance score ensemble average using adversarial training without distance score. additional results conditions tested found supplementary material. gain better statistical signiﬁcance experiment repeated least times overlap networks. cifar- stl- fig. shows ensemble accuracy methods mentioned using datasets. clearly seen weighting predictions based distance score improves results signiﬁcantly. best results achieved combining distance networks adversarial networks signiﬁcant improvement ensemble kind networks still note importantly distance score used weight kind networks. since adversarial training always applicable computational cost train time show combination distance networks regular networks also lead signiﬁcant improvement performance using distance score hybrid architecture described section finally note adversarial networks alone achieve poor results using original ensemble average demonstrating value distance score improving performance ensemble adversarial networks alone. finally compare performance diﬀerent conﬁdence scores task novelty detection. task conﬁdence score used decide another binary classiﬁcation problem test example belong classes networks trained rather unknown class? performance binary classiﬁcation task evaluated using corresponding curve conﬁdence score. used contrived datasets evaluate performance task following experimental construction suggested lakshminarayanan ﬁrst experiment trained network stl- dataset tested stl- svhn test sets. second experiment switched between datasets making svhn known dataset stl- novel one. task requires discriminate known novel datasets. comparison computed novelty often does one-class classiﬁer using embeddings. novelty thus computed showed much poorer performance possibly dataset involves many classes therefore results included here. results shown table adversarial training designed handle sort challenge surprisingly best performer. nevertheless proposed conﬁdence score improves results even further demonstrating added value. proposed conﬁdence score multi-class neural network classiﬁers. method proposed compute score scalable simple implement kind neural network. method diﬀerent commonly used methods based measuring point density eﬀective embedding space network thus providing coherent statistical measure distribution network’s predictions. also showed suitable embeddings achieved using either distance-based loss someunexpectedly adversarial training. demonstrated superiority score number tasks. tasks evaluated using number diﬀerent datasets task-appropriate network architectures. tasks proposed method achieved best results compared traditional conﬁdence scores. donahue jeﬀrey anne hendricks lisa guadarrama sergio rohrbach marcus venugopalan subhashini saenko kate darrell trevor. long-term recurrent convolutional networks visual recognition description. proceedings ieee conference computer vision pattern recognition garcia vincent debreuve eric barlaud michel. fast nearest neighbor search using gpu. computer vision pattern recognition workshops cvprw’. ieee computer society conference ieee hadsell raia chopra sumit lecun yann. dimensionality reduction learning invariant mapping. computer vision pattern recognition ieee computer society conference volume ieee kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition hexiang zhou guang-tong deng zhiwei liao zicheng mori greg. learning structured inference neural networks label relations. proceedings ieee conference computer vision pattern recognition hyvönen ville pitkänen teemu tasoulis sotiris jääsaari elias tuomainen risto wang liang corander jukka roos teemu. fast k-nn search. arxiv preprint arxiv. lakshminarayanan balaji pritzel alexander blundell charles. simple scalable predictive uncertainty estimation using deep ensembles. arxiv preprint arxiv. wang xuesong ding shifei. research development neural network ensembles survey. artiﬁcial intelligence review mackay david bayesian methods adaptive models. thesis california institute technology netzer yuval wang coates adam bissacco alessandro andrew reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning volume palatucci mark pomerleau dean hinton geoﬀrey mitchell zero-shot learning semantic output codes. advances neural information processing systems salakhutdinov ruslan hinton geoﬀrey learning nonlinear embedding preserving class neighbourhood structure. aistats volume schroﬀ florian kalenichenko dmitry philbin james. facenet uniﬁed embedding face recognition clustering. proceedings ieee conference computer vision pattern recognition sharif razavian azizpour hossein sullivan josephine carlsson stefan. features oﬀthe-shelf astounding baseline recognition. proceedings ieee conference computer vision pattern recognition workshops srivastava nitish hinton geoﬀrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. journal machine learning research szegedy christian zaremba wojciech sutskever ilya bruna joan erhan dumitru goodfellow fergus rob. intriguing properties neural networks. arxiv preprint arxiv. tadmor oren rosenwein shalev-shwartz shai wexler yonatan shashua amnon. learning metric embedding face recognition using multibatch method. advances neural information processing systems zaragoza hugo d’alché florence. conﬁdence measures neural network classiﬁers. proceedings seventh int. conf. information processing management uncertainty knowlegde based systems", "year": 2017}