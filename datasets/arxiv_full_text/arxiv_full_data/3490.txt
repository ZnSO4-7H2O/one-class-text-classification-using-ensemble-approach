{"title": "Multi-view Low-rank Sparse Subspace Clustering", "tag": ["cs.CV", "cs.LG", "math.OC", "stat.ML"], "abstract": "Most existing approaches address multi-view subspace clustering problem by constructing the affinity matrix on each view separately and afterwards propose how to extend spectral clustering algorithm to handle multi-view data. This paper presents an approach to multi-view subspace clustering that learns a joint subspace representation by constructing affinity matrix shared among all views. Relying on the importance of both low-rank and sparsity constraints in the construction of the affinity matrix, we introduce the objective that balances between the agreement across different views, while at the same time encourages sparsity and low-rankness of the solution. Related low-rank and sparsity constrained optimization problem is for each view solved using the alternating direction method of multipliers. Furthermore, we extend our approach to cluster data drawn from nonlinear subspaces by solving the corresponding problem in a reproducing kernel Hilbert space. The proposed algorithm outperforms state-of-the-art multi-view subspace clustering algorithms on one synthetic and four real-world datasets.", "text": "existing approaches address multi-view subspace clustering problem constructing aﬃnity matrix view separately afterwards propose extend spectral clustering algorithm handle multi-view data. paper presents approach multi-view subspace clustering learns joint subspace representation constructing aﬃnity matrix shared among views. relying importance low-rank sparsity constraints construction aﬃnity matrix introduce objective balances agreement across diﬀerent views time encourages sparsity low-rankness solution. related low-rank sparsity constrained optimization problem view solved using alternating direction method multipliers. furthermore extend approach cluster data drawn nonlinear subspaces solving corresponding problem reproducing kernel hilbert space. proposed algorithm outperforms state-of-the-art multi-view subspace clustering algorithms synthetic four real-world datasets. many real-world machine learning problems data comprised several diﬀerent representations views. example documents available multiple languages diﬀerent descriptors constructed images although individual views suﬃcient perform learning task integrating complementary information diﬀerent views reduce complexity given task multi-view clustering seeks partition data points based multiple representations assuming cluster structure shared across views. combining information diﬀerent views multi-view clustering algorithms attempt achieve accurate cluster assignments simply concatenating features diﬀerent views. practice high-dimensional data often reside low-dimensional subspace. data points single subspace problem ﬁnding basis subspace low-dimensional representation data points. depending constraints imposed lowdimensional representation problem solved using e.g. principal component analysis independent component analysis non-negative matrix factorization hand data points drawn diﬀerent sources union subspaces. assigning subspace cluster solve problem applying standard clustering algorithms k-means however algorithms based assumption data points distributed around centroid often perform well cases data points subspace arbitrarily distributed. example points small distance diﬀerent subspaces still subspace therefore methods rely spatial proximity data points often fail provide satisfactory solution. motivated development subspace clustering algorithms goal subspace clustering identify low-dimensional subspaces cluster membership data points. spectral based methods present approach subspace clustering problem. gained attention recent years competitive results achieve arbitrarily shaped clusters well deﬁned mathematical principles. methods based spectral graph theory represent data points nodes weighted graph. clustering problem solved relaxation min-cut problem graph main challenges spectral based methods construction aﬃnity matrix whose elements deﬁne similarity data points. sparse subspace clustering lowrank subspace clustering among eﬀective methods solve problem. methods rely self-expressiveness property data representing data point linear combination data points. low-rank representation imposes low-rank constraint data representation matrix captures global structure data. low-rank implies data matrix represented small number outer products left right singular vectors weighted corresponding singular values. assumption subspaces independent data sampling suﬃcient guarantees exact clustering. however many real-world datasets assumption overly restrictive assumption data drawn disjoint subspaces would appropriate hand sparse subspace clustering represents data point sparse linear combination points captures local structure data. learning representation matrix interpreted sparse coding however compared sparse coding dictionary learned representation sparse based selfrepresentation property i.e. data matrix stands dictionary. also succeeds data drawn independent subspaces conditions established clustering data drawn disjoint subspaces however theoretical analysis shows possible over-segments subspaces dimensionality data points higher three. experimental results show misclassiﬁes diﬀerent data points ssc. therefore order capture global local structure data necessary combine low-rank sparsity constraints multi-view subspace clustering considered part multi-view multi-modal learning. multi-view learning method learns view generation matrices representation matrix relying assumption data views share representation matrix. multi-view method based canonical correlation analysis extraction two-view ﬁlter-bank-based features image classiﬁcation task. similarly authors rely tensorbased canonical correlation analysis perform multi-view dimensionality reduction. approach used preprocessing step multi-view learning case high-dimensional data. low-rank representation matrix learned view separately learned representation matrices concatenated matrix uniﬁed graph aﬃnity matrix obtained. method relies learning linear projection matrix view separately. high-order distancebased multi-view stochastic learning proposed eﬃciently explore complementary characteristics multi-view features image classiﬁcation. method application oriented towards image reranking assumes multi-view features contained hypergraph laplacians deﬁne diﬀerent modalities. authors propose multi-view matrix completion algorithm handling multi-view features semi-supervised multi-label image classiﬁcation. previous multi-view subspace clustering works address problem constructing aﬃnity matrix view separately extend algorithm handle multi-view data. however since input data often corrupted noise approach lead propagation noise aﬃnity matrices degrade clustering performance. diﬀerent existing approaches propose multi-view spectral clustering framework jointly learns subspace representation constructing single aﬃnity matrix shared multi-view data time encourages low-rank sparsity representation. propose multi-view low-rank sparse subspace clustering algorithms enforce agreement aﬃnity matrices pairs views; aﬃnity matrices towards common centroid. opposed proposed approach deal highly heterogeneous multi-view data coming diﬀerent modalities. present optimization procedure solve convex dual optimization problems using alternating direction method multipliers furthermore propose kernel extension algorithms solving problem reproducing kernel hilbert space experimental results show mlrssc algorithm outperforms state-of-the-art multi-view subspace clustering algorithms several benchmark datasets. additionally evaluate performance novel real-world heterogeneous multi-view dataset biological domain. remainder paper organized follows. section gives brief overview lowrank sparse subspace clustering methods. section introduces novel multi-view subspace clustering algorithms. section present kernelized version proposed algorithms formulating subspace clustering problem rkhs. performance algorithms demonstrated section section concludes paper. throughout paper matrices represented bold capital symbols vectors bold lower-case symbols. denotes frobenius norm matrix. norm denoted absolute values matrix elements; inﬁnity norm maximum absolute element value; nuclear norm singular values matrix. trace operator matrix denoted diag vector diagonal elements matrix. denotes null vector. table summarizes notations used throughout paper. union linear subspaces unknown dimensions. given data points task subspace clustering cluster data points according subspaces belong ﬁrst step construction aﬃnity matrix irn×n whose elements deﬁne similarity data points. ideally aﬃnity matrix block diagonal matrix nonzero distance assigned points representation matrix view centroid representation matrix aﬃnity matrix singular value decomposition data points view mapped high-dimensional feature space gram matrix view norm used tightest convex relaxation quasi-norm counts number nonzero elements solution. constraint diag used avoid trivial solution representing data point linear combination itself. given aﬃnity matrix spectral clustering ﬁnds cluster membership data points applying k-means clustering eigenvectors graph laplacian matrix irn×n computed aﬃnity matrix section present multi-view low-rank sparse subspace clustering algorithm diﬀerent regularization approaches. assume given dataset described views features. objective joint representation matrix balances trade-oﬀ agreement across diﬀerent views time promotes sparsity low-rankness solution. formulate joint objective function enforces representation matrices across diﬀerent views regularized towards common consensus. motivated propose regularization schemes mlrssc algorithm mlrssc based pairwise similarities centroid-based mlrssc. ﬁrst regularization encourages similarity pairs representation matrices. centroid-based approach enforces representations across diﬀerent views towards common centroid. standard spectral clustering algorithm applied jointly inferred aﬃnity matrix. cases prior information view important others dependent view value used across views last term objective introduced encourage similarities pairs representation matrices across views. solve convex optimization problem alternating direction method multipliers admm converges objective composed two-block convex separable problems terms depend observed variable block. update steps repeated convergence maximum number iteration reached. check convergence verifying following constraints iteration obtaining representation matrix view combine taking element-wise average across views. next step algorithm assignment data points corresponding clusters applying spectral clustering algorithm joint aﬃnity matrix |cavg| |cavg|t algorithm summarizes steps pairwise mlrssc. practical reasons initial values {µi} µmax optimizations views. however possible diﬀerent views update {µi} general approach diﬀerent initial values {µi} µmax view signiﬁcantly increases number variables optimization. computational complexity algorithm number iterations number views number data points. experiments maximal algorithm converged maximal number iterations exceeded importantly computational complexity spectral clustering step computational cost proposed representation learning step times higher. addition pairwise mlrssc also introduce objective centroid-based mlrssc enforces view-speciﬁc representations towards common centroid. propose solve following minimization problem objective function minimized alternating minimization cycling views consensus variable. speciﬁcally following steps repeated consensus variable update keeping others ﬁxed update centroid-based mlrssc need combine aﬃnity matrices across views since joint aﬃnity matrix directly computed centroid matrix i.e. |c∗| |c∗|t algorithm summarizes steps centroid-based mlrssc. computational complexity algorithm complexity algorithm spectral decomposition laplacian enables spectral clustering separate data points nonlinear hypersurfaces. however representing data points linear combination data points mlrssc algorithm learns aﬃnity matrix models linear subspace structure data. order recover nonlinear subspaces propose solve mlrssc rkhs implicitly mapping data points high dimensional feature space. deﬁne function maps original input space high dimensional feature space since presented update rules corrupted data pairwise centroid-based mlrssc depend products approaches solved rkhs extended model nonlinear manifold structure. section present results demonstrate eﬀectiveness proposed algorithms. performance measured synthetic three real-world datasets commonly used evaluate performance multi-view algorithms. moreover introduce novel real-world multi-view dataset molecular biology domain. compared mlrssc state-of-theart multi-view subspace clustering algorithms well baselines best single view lrssc feature concatenation lrssc. digit dataset available repository. dataset consists examples handwritten digits extracted dutch utility maps. examples class represented feature sets. following experiments used three feature sets fourier coeﬃcients character shapes proﬁle correlations karhunen-love coeﬃcients. reuters dataset contains features documents available diﬀerent languages translations common categories. documents bag-of-words representation. documents originally written english view translations french german spanish italian four views. randomly sampled documents class resulting dataset documents. -sources dataset news articles dataset collected three online news sources reuters guardian. articles bag-of-words representation. articles used available three sources. article dataset annotated dominant topic class. prokaryotic phyla dataset contains prokaryotic species described heterogeneous multi-view data including textual data diﬀerent genomic representations textual data consists bag-of-words representation documents describing prokaryotic species considered view. experiments genomic representations proteome composition encoded relative frequencies amino acids gene repertoire encoded presence/absence indicators gene families genome. order reduce dimensionality dataset apply principal component analysis three views separately retain principal components explaining variance. species dataset labeled phylum belongs unlike previous datasets dataset unbalanced. frequently occurring cluster contains species smallest cluster contains species. synthetic dataset generated described points generated views data points view generated two-component gaussian mixture models. cluster means covariance matrices view compare pairwise mlrssc centroid-based mlrssc kernel extensions algorithms best performing state-of-the-art multi-view subspace clustering algorithms including co-regularized multi-view spectral clustering robust multi-view spectral clustering convex sparse multi-view spectral clustering moreover also compare mlrssc algorithms lrssc baselines best single view low-rank sparse subspace clustering performs single view lrssc view takes individual view achieves best performance feature concatenation lrrsc concatenates features individual view performs single-view lrssc joint view representation. co-regularized multi-view parameter vary step choose rmsc values tested parameter csmsc chosen parameter algorithms standard deviation gaussian kernel used build similarity matrix median pairwise euclidean distances data points number iterations co-reg converged within less iterations. number iterations csmsc rmsc available source code provided authors. parameters algorithms values based respective source codes provided authors. lrssc mlrssc ﬁrst choose penalty parameter values ﬁxed value constraints also optimize constraints. iteration update ﬁxed till maximal value reached. single-view lrssc low-rank parameter tuned step sparsity parameter consensus parameter tuned step also possible diﬀerent view since prior information importance views view datasets variant adjusted corrupted data except digit dataset. kernel extension mlrssc gaussian kernel optimize standard deviation view separately range times median pairwise euclidean distances data points holding parameters ﬁxed. best sigma pairwise mlrssc also used centroid mlrssc without optimization. maximum number iterations convergence error tolerance linear mlrssc kernel mlrssc. tune parameters algorithm report best performance. compared methods k-means last step algorithm. since k-means depends initial cluster centroid positions yield diﬀerent solution diﬀerent initializations k-means times report means standard deviations performance measures. evaluate clustering performance using diﬀerent metrics precision recall f-score normalized mutual information adjusted rand index metrics higher value indicates better performance. table compares clustering performance mlrssc algorithms four realworld datasets synthetic dataset. results indicate mlrssc consistently outperforms methods terms tested measures. datasets mlrssc improves performance large extent demonstrates importance combined low-rank sparsity constraints. speciﬁcally average mlrssc higher second best method -sources reuters digit prokaryotic synthetic datasets respectively. similar improvements also observed using metrics measuring clustering performance. pairwise centroid-based mlrssc perform comparably except prokaryotic dataset pairwise mlrssc signiﬁcantly better centroid-based mlrssc except recall. comparing linear mlrssc kernel mlrssc linear mlrssc performs better -sources reuters datasets. kernel mlrssc outperforms linear mlrssc digit prokaryotic synthetic datasets although diﬀerence digit dataset signiﬁcant. however comes cost tuning parameters computing kernel. better performance linear mlrppsc -sources reuters datasets suprising since datasets sparse large number features much higher number data points. hand digit prokaryotic especially synthetic datasets dense lower-dimensional feature vectors beneﬁt projection high-dimensional feature space. mlrssc trades-oﬀ low-rank sparsity consensus parameters respectively. section test eﬀect parameters performance mlrssc. experiments sparsity parametar i.e. higher value low-rank parameter leads lower value sparsity parameter vice versa. depends whether problem solved requires exploiting global local structure data. table performance diﬀerent algorithms multi-view datasets. mean standard deviation runs k-means clustering algorithm diﬀerent random initializations reported. reuters digit mlrssc algorithm outperforms second best algorithm regardless choice prokaryotic dataset pairwise mlrssc performs comparably rmsc algorithm insensitive parameter. hand centroid-based mlrssc lags behind dataset respect measure consistently improves performance higher values next vary consensus parameter keep low-rank parameter sparsity parameter ﬁxed. figure shows performance mlrssc respect measure diﬀerent values similarly varying parameter mlrssc performs consistently better algorithms regardless choice again exception centroid-based mlrssc prokaryotic dataset. results prove mlrssc pretty stable regardless choice parameters long parameters chosen appropriate range. figure performance mlrssc w.r.t. measure varying low-rank parameter keeping consensus parameter ﬁxed. sparsity parameter blue line shows best performing algorithm besides mlrssc among algorithms listed table pmlrssc stands pairwise mlrssc cmlrssc centroid-based mlrssc. figure performance mlrssc w.r.t. measure varying consensus parameter keeping low-rank parameter sparsity parameter ﬁxed. blue line shows best performing algorithm besides mlrssc among algorithms listed table order check computational time mlrssc scales increase number data points perform experiments digit dataset compare mlrssc algorithms. computational time depends number iterations convergence conditions. number iterations error tolerance comparing performance algorithms. figure shows computational time averaged runs function number figure average computational time seconds function number data points measured digit dataset. co-reg mlrssc algorithm times pairwise regularization shown similar centroid regularization. diﬀerence co-reg rmsc seen scale algorithms shown together. figure demonstrates behavior convergence conditions pairwise mlrssc. ease illustration errors normalized summed across views. seen four real-world datasets algorithm converges within iterations. centroid mlrssc exhibits similar behavior. figure shows objective function value pairwise centroid mlrrsc respect number iterations. paper proposed multi-view subspace clustering algorithm called multi-view low-rank sparse subspace clustering learns joint subspace representation across views. main property algorithm jointly learn aﬃnity matrix constrained sparsity low-rank. deﬁned optimization problems derived admm-based algorithms pairwise centroid-based regularization schemes. addition extended proposed mlrssc algorithm nonlinear subspaces solving related optimization problem reproducing kernel hilbert space. experimental results multi-view datasets various domains showed proposed algorithms outperforms state-of-the-art multi-view subspace clustering algorithms. high computational complexity presents serious drawback spectral clustering algorithms. future work plan explore improve eﬃciency proposed approach applicable large-scale multi-view problems. moreover consider extend mlrssc algorithm handle incomplete data.", "year": 2017}