{"title": "Attentive Explanations: Justifying Decisions and Pointing to the  Evidence", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "Deep models are the defacto standard in visual decision models due to their impressive performance on a wide array of visual tasks. However, they are frequently seen as opaque and are unable to explain their decisions. In contrast, humans can justify their decisions with natural language and point to the evidence in the visual world which led to their decisions. We postulate that deep models can do this as well and propose our Pointing and Justification (PJ-X) model which can justify its decision with a sentence and point to the evidence by introspecting its decision and explanation process using an attention mechanism. Unfortunately there is no dataset available with reference explanations for visual decision making. We thus collect two datasets in two domains where it is interesting and challenging to explain decisions. First, we extend the visual question answering task to not only provide an answer but also a natural language explanation for the answer. Second, we focus on explaining human activities which is traditionally more challenging than object classification. We extensively evaluate our PJ-X model, both on the justification and pointing tasks, by comparing it to prior models and ablations using both automatic and human evaluations.", "text": "figure attentive explanations given question corresponding image predict answer explain generating natural language justiﬁcation introspecting model attention mechanisms ﬁrst answer second justiﬁcation mediate units deep network performing certain task unable provide explanatory text grounded image. contrast pointing justiﬁcation-based explanation model explicitly multi-modal generating textual justiﬁcations also providing visual attentions decision justiﬁcation respectively generating convincing explanations calls models recognize objects activities attributes highlight visual elements important classiﬁcation decision. produce convincing explanations propose multi-modal explanation system provides explanations verbally pointing. illustrate consider images figure examples question what person doing? asked model correctly answers skiing. though images share common visual elements textual justiﬁcations reﬂect differences imdeep models defacto standard visual decision problems impressive performance wide array visual tasks. however frequently seen opaque unable explain decisions. contrast humans justify decisions natural language point evidence visual world supports decisions. propose method incorporates novel explanation attention mechanism; model trained using textual rationals infers latent attention visually ground explanations. collect novel datasets domains interesting challenging explain decisions. first extend visual question answering task provide answer also visual natural language explanations answer. second focus explaining human activities contemporary activity recognition dataset. extensively evaluate model justiﬁcation pointing tasks comparing prior models ablations using automatic human evaluations. humans surprisingly good explaining decisions even though explanations necessarily align initial reasoning still explaining decisions integral part human communication understanding learning. therefore build models explain decisions something comes naturally humans. explanations take many forms. example humans explain decisions natural language pointing visual evidence. show deep models demonstrate similar competence develop novel multi-modal model textually justiﬁes decisions visually grounds evidence attention mechanisms. previous methods able provide text-only explanation conditioned image context task able visualize active interages justiﬁes answer skiing discussing skis mountain justiﬁes answer skis hill clothing. respect pointing examples vqa-att attention generated model makes decision focuses skis legs revealing visual model relies answering question. however exp-att generated explaining decision made points different evidence discussed textual justiﬁcations. demonstrates model need attend evidence making decision subsequently justifying decision. exp-att allows conﬁrm whether model actually attending discussed items generating textual justiﬁcation comparing vqa-att determine model attends regions making decision explaining decision. following differentiate introspective explanations reﬂect decision process network justiﬁcation explanations discuss evidence supports decision without necessarily reﬂecting neural network decision process reﬂecting explanations given humans. introspective models lead better understanding network decision processes justiﬁcation systems potentially clearer end-users familiar deep models. pj-x model encompasses philosophies. whereas text generated pj-x model directly reﬂect model’s decision process provide straightforward explanations easy understand end-users. including attention activations used decision justiﬁcation processes pj-x also introspective. introspective explanation models illuminate underlying mechanism model’s decision. thus develop introspective explanation models researcher needs access data model itself. contrast justiﬁcation explanation systems discuss evidence supports decision human understandable format. thus believe important access ground truth human justiﬁcations evaluation justiﬁcation systems. dearth datasets include examples humans justify speciﬁc decisions. propose collect complementary explanation datasets challenging vision problems activity recognition visual question answering collect training evaluation data textual justiﬁcations well evaluation data pointing task. grounded explanations. incorporate novel explanatory attention step method allows visually ground explanation text. order generate satisfactory explanations collect datasets include human explanations activity recognition visual question answering. proposed pointing justiﬁcation explanation model outperforms strong baselines. additionally show part model improves slightly challenge winner efﬁcient train test. related work explanations. early textual explanation models span variety applications feedback teaching programs generally template based. recently developed deep network generate natural language justiﬁcations ﬁne-grained object classiﬁer. however unlike model provide multi-modal explanations model trained descriptions rather reference explanations. variety work proposed methods visually explain decisions. methods discriminative visual patches whereas others understand intermediate features important decisions e.g. certain neuron represent. pj-x points visual evidence attention mechanism intuitive convey knowledge important network without requiring domain knowledge. contrast previous work pj-x generates multi-modal explanations form explanatory sentences attention maps pointing visual evidence. discussed section explanation systems either introspective systems justiﬁcation systems. paradigm models like highlight discriminative image attributes without access speciﬁc model considered justiﬁcation explanations whereas models like illuminate inner workings deep networks considered introspective explanations. argue useful; justiﬁcations provide helpful information humans easily digestible format whereas introspective explanations provide insight model’s decision process though harder human unfamiliar deep learning understand. model strives satisfy deﬁnitions; providing textual explanations deﬁnition justiﬁcation explanations whereas visualizing system attends provides introspective explanations. visual question answering attention. initial approaches used full-frame representations recent approaches form spatial attention base method i.e. winner challenge predict figure pointing justiﬁcation architecture generates multi-modal explanation includes textual justiﬁcation points visual evidence. model consists pointing mechanisms answering pointing explaining pointing latent weighting spatially localized image features based question however elementwise product opposed compact bilinear pooling. concurrent work explored element-wise product method however improves performance applying hyperbolic tangent multi-modal pooling whereas improve applying signed square-root normalization. activity recognition. recent work activity recognition still images relies variety cues pose global context however although cues like pose inﬂuence model performance activity recognition models capable indicating factors inﬂuence decision process. contrast explanations reveal parts image important classiﬁcation. goal work justify decision made natural language point evidence decision textual justiﬁcation provided model. deliberately design pointing justiﬁcation model allow training tasks well decision process jointly. speciﬁcally want rely natural language justiﬁcations classiﬁcation labels supervision. design model learn point latent way. pointing rely attention mechanism allows model focus spatial subset visual representation. model ignores spatial visual features attend pointing also allows introspect model. model uses different attentions makes predictions another generates explanations. ﬁrst predict answer given image question. given answer question image generate textual justiﬁcation. cases include latent attention mechanism allows introspect question answer points overview double attention model presented figure learning answer. visual question answering goal predict answer given question image. activity recognition explicit question. thus ignore question equivalent setting question representation vector ones. able introspect answering process want model select area image gives evidence answer. achieved using attention model. rely overall architecture state-of-the-art attention model remove core contribution unit pool multi-modal features. instead propose simpler element-wise multiplication pooling after fully-connected layer embedding visual feature learns alignment visual textual representation. found leads similar performance much faster training. comparison dataset model state-of-the-art model found section detail extract spatial image features last convolutional layer resnet- followed convolutions giving spatial image feature. encode question -layer refer combine spatial image feature using element-wise multiplication followed signed square-root normalization dropout layers convolutions relu between operate spatial feature location propose explanation datasets visual question answering explanation human pose activity explanation summary dataset statistics presented table explanation dataset visual question answering dataset contains open-ended questions images require understanding vision natural language commonsense knowledge answer. dataset consists approximately mscoco images questions image answers question. select images question/answer pairs training pairs validation later divided pairs validation testing. pairs selected based simple heuristics would remove pairs require trivial explanations what color banana? etc. collected explanation data point training explanations data point validation test sets. annotators asked provide proper sentence clause would come proposition because explanations provided image question learning justify. argue generate textual justiﬁcation condition question answer image. instance able explain because vancouver police figure model needs question i.e. people arrest someone? answer i.e. image i.e. vancouver police banner motorcycles. model ﬁrst using second attention mechanism using localized feature input lstm generates explanations. hope uncover parts image contain evidence justiﬁcation. allow model learn attend relevant spatial location based answer image question combine answer feature question-image embedding applying convolutions element-wise multiplication followed signed squareroot normalization dropout resulting multimodal feature ﬂattened attention similarly previous attention step ﬁnally test images). image collected explanations. data annotation asked annotators complete sentence tell person because.. action ground truth activity label. also asked least words avoid mentioning activity class sentence. dataset also comes sentence descriptions provided examples descriptions explanations seen figure ground truth pointing. addition textual justiﬁcation collect attention maps humans vqa-x act-x datasets order evaluate attention model corresponds humans think evidence answer human-annotated attention maps collected amazon mechanical turk segmentation interface opensurfaces project annotators provided image answer asked segment objects and/or regions prominently justify answer. dataset randomly sample images test split image collect attention maps. collected annotations used computing earth mover’s distance evaluate attention maps model several baselines. examples seen figure section detailing experimental setup present model visual question answering results textual justiﬁcation visual pointing tasks. finally provide analyze qualitative results tasks. model training hyperparameters. model pre-trained training achieve state-of-the-art performance predicting answers either freeze ﬁnetune weights prediction model figure act-x dataset contains images dataset activity explanations. collected one-sentence descriptions. explanations task speciﬁc whereas descriptions generic. action explanation dataset human pose dataset contains images extracted videos downloaded youtube. dataset selected images pertain activities resulting images total whereas activity recognition answer embedding size tasks. train models training hyperparameters validation report results test splits detailed section evaluation metrics. evaluate textual results w.r.t bleu- meteor rouge cider spice metrics based degree similarity between generated ground truth sentences. also include human evaluation automatic metrics always reﬂect human preference. randomly choose images test sets vqa-x act-x datasets humans image judge whether generated explanation better than worse than equivalent ground truth explanation report percentage generated explanations equivalent better ground truth human explanations least human judges agree. probability distributions region rank correlation used evaluation metrics. reﬂects minimum amount work must performed transform distribution moving distribution mass. captures notion distance sets distributions instead single points. code compute emd. computing rank correlation follow scale attention maps human attention maps vqa-hat dataset rank pixel values compute correlation ranked lists. model throughout experiments based state-of-the-art model trains evaluates faster main difference models combine different representations create multimodal features. evaluate model using accuracy measure challenge. instead compact bilinear pooling representations model simply embeds encoded image feature using convolutions applies element-wise multiplication image embedding lstm feature. model aims create rich multimodal feature approximating outer product representations model tries learn proper alignment features merged element-wise multiplication creates feature powerful feature. similar merged representation normalized applying signed square root table openended results dataset test-dev. columns indicate accuracy model trained training train+val respectively. model achieves slightly higher accuracy previous challenge winner faster train test time. details compared models. re-implemented state-of-the-art captioning model integrated attention mechanism refer captioning model. model uses images class labels generating textual justiﬁcations. also compare using publicly available code. fair comparison resnet features training extracted entire image. generated sentences conditioned image class predictions. uses discriminative loss enforces generated sentence contain class-speciﬁc information backpropagate policy gradients training language generator thus involves training separate sentence classiﬁer generate rewards. model discriminative loss/policy gradients require deﬁning reward. note trained descriptions. ours descriptions another ablation train model descriptions instead explanations. ours exp-attention similar sense attention mechanism generating explanations however discriminative loss trained explanations instead descriptions. comparing state-of-the-art. pj-x model performs well compared state-of-the-art automatic evaluation metrics human evaluations ours model signiﬁcantly improves ours description model large margin datasets expected descriptions collected task generating explanations demonstrates necessity explanation datasets build explanation models. additionally model outperforms learns generate explanations given description training data. conﬁrms datasets ground truth explanations important textual justiﬁcation generation. ours descriptions performs worse certain metrics compared attributed additional training signals generated discriminative loss policy gradients investigation future work. ablating pj-x model. comparing ours captioning model shows conditioning explanations model decision important. though conditioning answer seems rather helpful act-x seems essential vqa-x sensible single image dataset correspond many different question answer pairs. thus important model access questions answers accurately generate explanation. finally including attention allows build multi-modal explanation model. act-x dataset clear including attention greatly improves textual justiﬁcations. vqa-x dataset ours attention ours comparable. though attention improve scores textual justiﬁcation task vqa-x dataset provide multi-modal explanation provides added insight model’s decision. robustness statistical priors. generated explanations could suffer drawbacks existing image captioning models–the sentences driven priors training data less grounded image. measuring robustness priors ﬁrst report percentage explanations generated model validation exact copies training table percentage duplicates extremely act-x high ratio vqa-x. investigate issue measure model trained descriptions perform. seen left columns table percentage datasets. vqa-x dataset currently explanation triplet act-x descriptions coco datasets least sentences image. postulate model shows robustness statistical priors given training sentences diverse enough. table evaluation textual justiﬁcations. evaluated automatic metrics bleu- meteor rouge cider spice reference sentence human automatic evaluation always explanation. proposed model compares favorably baselines. details compared baselines. compare model following baselines. random point randomly attends single point grid. uniform generates attention uniformly distributed grid. table evaluation pointing rank correlation metric denotes attention used predict answer whereas denotes attention used generate explanations. results standard error less comparing baselines. evaluate attention maps using earth mover’s distance rank correlation vqa-x act-x datasets table table table observe exp-att outperforms baselines performs similarly ans-att vqa-x indicating exp-att aligns well human annotated attentions also model attention used making decision. fact rank correlation ans-att expatt respectively indicating high alignment. act-x exp-att outperforms baselines ans-att indicating regions model attends generating explanation agree regions humans point justifying decision. suggests whereas ans-att attention maps helpful understanding model debugging necessarily best option providing visual evidence agrees human justiﬁcations. figure vqa-x qualitative results given image question model provides answer pointing evidence answer justiﬁcation pointing evidence justiﬁcation pairs. however instead compute rank-correlation metric following datasets. table similar trends metric model outperforms baseline datasets best model rank-correlation metric. section present qualitative results vqax act-x datasets demonstrating model generates high quality sentences attention maps point relevant locations image. vqa-x. figure shows qualitative results vqax dataset. textual justiﬁcations able capture common sense discuss speciﬁc image parts important answering question. example asked holiday explanation model able discuss object represent concept christmas i.e. there christmas tree glowing lights. determining kind animal requires discussing speciﬁc image visually notice attention model able point important visual evidence. example second figure question what room this? visual explanation focuses toilet sink. given pair different image toilet attention model able focus sink reﬂection mirror. moreover supporting initial claims attention leads correct answer attention leads relevant explanation look different e.g. generating four legs long ﬂuffy hair requires looking sheep wider angle. figure shows similar trends act-x dataset. example incorrectly predicting activity power yoga image depicting manual labor explanation because sitting yoga holding yoga pose suggests misclassiﬁed yoga mat. reiterate model justiﬁes predictions fully explain inner-workings deep architectures. however justiﬁcations demonstrate model output intuitive explanations could help unfamiliar deep architectures make sense model predictions. step towards explainable models work introduced novel attentive explanation model capable providing natural language justiﬁcations decisions well pointing evidence. proposed novel explanation datasets collected crowd sourcing visual question answering activity recognition i.e. vqa-x act-x. quantitatively demonstrated attention using reference explanations train model helps achieve high quality explanations. furthermore demonstrated model able point evidence well give natural sentence justiﬁcations similar ones humans give. act-x. figure shows results act-x dataset. textual explanations discuss variety visual cues important correctly classifying activities global context e.g. over grassy lawn mountainous area person-object interaction e.g. pushing lawn mower riding bicycle mowing lawn mountain biking respectively. explanations require determining many multiple cues appropriate justify particular action. model points visual evidence important understanding human activity. example classify mowing lawn second figure model focuses person grass well lawn mower. model also differentiate similar activities based context e.g.mountain biking road biking. additional results various settings. figure figure demonstrate images question/answer pair needed good explanations. also demonstrate explanations generated model visually grounded robust priors existing training data. figure shows explanations different images question/answer pair. importantly explanation text visualizations change reﬂect image content. instance question where picture taken? model explains answer airport pointing discussing planes trucks ﬁrst image pointing discussing baggage carousel second image. figure shows different questions asked images explanations provide information speciﬁc questions. example question sunny? model explains answer mentioning reﬂection pointing water whereas question what person doing? points directly surfer mentions person surfboard. figure shows explanations act-x dataset discuss small details important differentiating similar classes. example explaining kayaking windsurﬁng important mention correct sporting equipment kayak sail instead image context. hand distinguishing bicycling bicycling important discuss image context doing trick wall riding bicycle road. figure figure compare explanations answers action labels correctly incorrectly predicted. addition providing intuition predictions correct explanations frequently justify model makes incorrect predictions. example incorrectly predicting whether stop vqa-att denotes attention maps used predicting answers exp-att denotes attention maps used generating corresponding justiﬁcations. figure vqa-x results image different questions. select results image different pairs show although images same model able answer questions differently generate different explanation accordingly vqa-att denotes attention maps used predicting answers exp-att denotes attention maps used generating corresponding justiﬁcations. figure act-x results similar activities. figure left show results ﬁne-grained activities related windsurﬁng kayaking canoeing observe ﬁne-grained activities correctly predicted explanations match activity image. figure right show results ﬁne-grained activities related bicycling observe ﬁne-grained activities correctly predicted explanations match activity image. act-att denotes attention maps used predicting answers exp-att denotes attention maps used generating corresponding justiﬁcations. figure vqa-x results. denotes ground-truth answer indicates actual prediction made model. figure left show various qualitative results correctly predicted answer observe explanation justiﬁes answer accordingly. figure right show results incorrectly predicted answer observe although answer incorrect model provide visual textual explanations model might failing cases. figure act-x results. denotes ground-truth answer indicates actual prediction made model. figure left show various qualitative results correctly predicted answer observe explanation justiﬁes answer accordingly. figure right show results incorrectly predicted answer observe although answer incorrect model provide visual textual explanations model might failing cases.", "year": 2016}